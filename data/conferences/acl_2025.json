{
  "conference": "ACL 2025",
  "conference_id": "acl_2025",
  "paper_count": 2162,
  "papers": [
    {
      "arxiv_id": "00027388066eae1346d2dc9a646b921e15022f37",
      "title": "From Information to Insight: Leveraging LLMs for Open Aspect-Based Educational Summarization",
      "authors": [
        "Yang Zhong",
        "D. J. Litman"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/00027388066eae1346d2dc9a646b921e15022f37",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.20290",
      "title": "QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions",
      "authors": [
        "Siyin Wang",
        "Wenyi Yu",
        "Xianzhao Chen",
        "Xiaohai Tian",
        "Jun Zhang",
        "Lu Lu",
        "Yu Tsao",
        "Junichi Yamagishi",
        "Yuxuan Wang",
        "Chao Zhang"
      ],
      "abstract": "This paper explores a novel perspective to speech quality assessment by leveraging natural language descriptions, offering richer, more nuanced insights than traditional numerical scoring methods. Natural language feedback provides instructive recommendations and detailed evaluations, yet existing datasets lack the comprehensive annotations needed for this approach. To bridge this gap, we introduce QualiSpeech, a comprehensive low-level speech quality assessment dataset encompassing 11 key aspects and detailed natural language comments that include reasoning and contextual insights. Additionally, we propose the QualiSpeech Benchmark to evaluate the low-level speech understanding capabilities of auditory large language models (LLMs). Experimental results demonstrate that finetuned auditory LLMs can reliably generate detailed descriptions of noise and distortion, effectively identifying their types and temporal characteristics. The results further highlight the potential for incorporating reasoning to enhance the accuracy and reliability of quality assessments. The dataset will be released at https://huggingface.co/datasets/tsinghua-ee/QualiSpeech.",
      "arxiv_url": "https://arxiv.org/abs/2503.20290",
      "pdf_url": "https://arxiv.org/pdf/2503.20290",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.14194",
      "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models",
      "authors": [
        "Xinlin Zhuang",
        "Jiahui Peng",
        "Ren Ma",
        "Yinfan Wang",
        "Tianyi Bai",
        "Xingjian Wei",
        "Jiantao Qiu",
        "Chi Zhang",
        "Ying Qian",
        "Conghui He"
      ],
      "abstract": "The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural language quality assessments, diversity-based filters, and classifier-based approaches, are limited by single-dimensional evaluation or redundancy-focused strategies. To address these gaps, we propose four dimensions to evaluate data quality: professionalism, readability, reasoning, and cleanliness. We further introduce Meta-rater,a multi-dimensional data selection method that integrates these dimensions with existing quality metrics through learned optimal weightings. Meta-rater employs proxy models to train a regression model that predicts validation loss, enabling the identification of optimal combinations of quality scores. Experiments demonstrate that Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23, with advantages that scale to models as large as 7.2B parameters. Our work establishes that holistic, multi-dimensional quality integration significantly outperforms conventional single-dimension approaches, offering a scalable paradigm for enhancing pre-training efficiency and model capability. To advance future research, we release scripts, data, and models at https://github.com/opendatalab/Meta-rater.",
      "arxiv_url": "https://arxiv.org/abs/2504.14194",
      "pdf_url": "https://arxiv.org/pdf/2504.14194",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "003dece9fddb6ae24c87e5d20fec8ff4a108f7ae",
      "title": "Can VLMs Actually See and Read? A Survey on Modality Collapse in Vision-Language Models",
      "authors": [
        "Mong Yuan Sim",
        "Wei Emma Zhang",
        "Xiang Dai",
        "Biaoyan Fang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/003dece9fddb6ae24c87e5d20fec8ff4a108f7ae",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "006c73468323d4b0fa9fa6ec09e8f82f6d777c24",
      "title": "Token-level Preference Self-Alignment Optimization for Multi-style Outline Controllable Generation",
      "authors": [
        "Zihao Li",
        "Xuekong Xu",
        "Ziyao Chen",
        "Lixin Zou",
        "Ethanhjwu Ethanhjwu",
        "Qiang Chen",
        "Chenliang Li"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/006c73468323d4b0fa9fa6ec09e8f82f6d777c24",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.03854",
      "title": "Vision-Language Models Struggle to Align Entities across Modalities",
      "authors": [
        "Inigo Alonso",
        "Ander Salaberria",
        "Gorka Azkune",
        "Jeremy Barnes",
        "Oier López de Lacalle"
      ],
      "abstract": "Cross-modal entity linking refers to the ability to align entities and their attributes across different modalities. While cross-modal entity linking is a fundamental skill needed for real-world applications such as multimodal code generation, fake news detection, or scene understanding, it has not been thoroughly studied in the literature. In this paper, we introduce a new task and benchmark to address this gap. Our benchmark, MATE, consists of 5.5k evaluation instances featuring visual scenes aligned with their textual representations. To evaluate cross-modal entity linking performance, we design a question-answering task that involves retrieving one attribute of an object in one modality based on a unique attribute of that object in another modality. We evaluate state-of-the-art Vision-Language Models (VLMs) and humans on this task, and find that VLMs struggle significantly compared to humans, particularly as the number of objects in the scene increases. Our analysis also shows that, while chain-of-thought prompting can improve VLM performance, models remain far from achieving human-level proficiency. These findings highlight the need for further research in cross-modal entity linking and show that MATE is a strong benchmark to support that progress.",
      "arxiv_url": "https://arxiv.org/abs/2503.03854",
      "pdf_url": "https://arxiv.org/pdf/2503.03854",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19997",
      "title": "Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents",
      "authors": [
        "Tao Wu",
        "Jingyuan Chen",
        "Wang Lin",
        "Mengze Li",
        "Yumeng Zhu",
        "Ang Li",
        "Kun Kuang",
        "Fei Wu"
      ],
      "abstract": "Large language models (LLMs) are revolutionizing education, with LLM-based agents playing a key role in simulating student behavior. A major challenge in student simulation is modeling the diverse learning patterns of students at various cognitive levels. However, current LLMs, typically trained as ``helpful assistants'', target at generating perfect responses. As a result, they struggle to simulate students with diverse cognitive abilities, as they often produce overly advanced answers, missing the natural imperfections that characterize student learning and resulting in unrealistic simulations. To address this issue, we propose a training-free framework for student simulation. We begin by constructing a cognitive prototype for each student using a knowledge graph, which captures their understanding of concepts from past learning records. This prototype is then mapped to new tasks to predict student performance. Next, we simulate student solutions based on these predictions and iteratively refine them using a beam search method to better replicate realistic mistakes. To validate our approach, we construct the \\texttt{Student\\_100} dataset, consisting of $100$ students working on Python programming and $5,000$ learning records. Experimental results show that our method consistently outperforms baseline models, achieving $100\\%$ improvement in simulation accuracy.",
      "arxiv_url": "https://arxiv.org/abs/2505.19997",
      "pdf_url": "https://arxiv.org/pdf/2505.19997",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.16465",
      "title": "OS-Kairos: Adaptive Interaction for MLLM-Powered GUI Agents",
      "authors": [
        "Pengzhou Cheng",
        "Zheng Wu",
        "Zongru Wu",
        "Aston Zhang",
        "Zhuosheng Zhang",
        "Gongshen Liu"
      ],
      "abstract": "Autonomous graphical user interface (GUI) agents powered by multimodal large language models have shown great promise. However, a critical yet underexplored issue persists: over-execution, where the agent executes tasks in a fully autonomous way, without adequate assessment of its action confidence to compromise an adaptive human-agent collaboration. This poses substantial risks in complex scenarios, such as those involving ambiguous user instructions, unexpected interruptions, and environmental hijacks. To address the issue, we introduce OS-Kairos, an adaptive GUI agent capable of predicting confidence levels at each interaction step and efficiently deciding whether to act autonomously or seek human intervention. OS-Kairos is developed through two key mechanisms: (i) collaborative probing that annotates confidence scores at each interaction step; (ii) confidence-driven interaction that leverages these confidence scores to elicit the ability of adaptive interaction. Experimental results show that OS-Kairos substantially outperforms existing models on our curated dataset featuring complex scenarios, as well as on established benchmarks such as AITZ and Meta-GUI, with 24.59\\%$\\sim$87.29\\% improvements in task success rate. OS-Kairos facilitates an adaptive human-agent collaboration, prioritizing effectiveness, generality, scalability, and efficiency for real-world GUI interaction. The dataset and codes are available at https://github.com/Wuzheng02/OS-Kairos.",
      "arxiv_url": "https://arxiv.org/abs/2503.16465",
      "pdf_url": "https://arxiv.org/pdf/2503.16465",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01592",
      "title": "Statement-Tuning Enables Efficient Cross-lingual Generalization in Encoder-only Models",
      "authors": [
        "A. Elshabrawy",
        "Thanh-Nhi Nguyen",
        "Yeeun Kang",
        "Lihan Feng",
        "Annant Jain",
        "Faadil Abdullah Shaikh",
        "Jonibek Mansurov",
        "Mohamed Fazli Mohamed Imam",
        "Jesús-Germán Ortiz-Barajas",
        "Rendi Chevi",
        "Alham Fikri Aji"
      ],
      "abstract": "Large Language Models (LLMs) excel in zero-shot and few-shot tasks, but achieving similar performance with encoder-only models like BERT and RoBERTa has been challenging due to their architecture. However, encoders offer advantages such as lower computational and memory costs. Recent work adapts them for zero-shot generalization using Statement Tuning, which reformulates tasks into finite templates. We extend this approach to multilingual NLP, exploring whether encoders can achieve zero-shot cross-lingual generalization and serve as efficient alternatives to memory-intensive LLMs for low-resource languages. Our results show that state-of-the-art encoder models generalize well across languages, rivaling multilingual LLMs while being more efficient. We also analyze multilingual Statement Tuning dataset design, efficiency gains, and language-specific generalization, contributing to more inclusive and resource-efficient NLP models. We release our code and models.",
      "arxiv_url": "https://arxiv.org/abs/2506.01592",
      "pdf_url": "https://arxiv.org/pdf/2506.01592",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07424",
      "title": "Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models",
      "authors": [
        "Kyeonghyun Kim",
        "Jinhee Jang",
        "Juhwan Choi",
        "Yoonji Lee",
        "Kyohoon Jin",
        "Youngbin Kim"
      ],
      "abstract": "Large language models (LLMs) are renowned for their extensive linguistic knowledge and strong generalization capabilities, but their high computational demands make them unsuitable for resource-constrained environments. In contrast, small language models (SLMs) are computationally efficient but often lack the broad generalization capacity of LLMs. To bridge this gap, we propose PiFi, a novel framework that combines the strengths of both LLMs and SLMs to achieve high performance while maintaining efficiency. PiFi integrates a single frozen layer from an LLM into a SLM and fine-tunes the combined model for specific tasks, boosting performance without a significant increase in computational cost. We show that PiFi delivers consistent performance improvements across a range of natural language processing tasks, including both natural language understanding and generation. Moreover, our findings demonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing generalization to unseen domains and facilitating the transfer of linguistic abilities.",
      "arxiv_url": "https://arxiv.org/abs/2506.07424",
      "pdf_url": "https://arxiv.org/pdf/2506.07424",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "01bd51ca90cf2ae03eb88881c48f96f07b73962d",
      "title": "M2-TabFact: Multi-Document Multi-Modal Fact Verification with Visual and Textual Representations of Tabular Data",
      "authors": [
        "Mingyang Zhou",
        "Lingyu Zhang",
        "Sophia Horng",
        "Maximillian Chen",
        "Kung-Hsiang Huang",
        "Shih-Fu Chang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/01bd51ca90cf2ae03eb88881c48f96f07b73962d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20428",
      "title": "The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project",
      "authors": [
        "Angelina A. Aquino",
        "Lester James Validad Miranda",
        "E. M. Or"
      ],
      "abstract": "This paper presents UD-NewsCrawl, the largest Tagalog treebank to date, containing 15.6k trees manually annotated according to the Universal Dependencies framework. We detail our treebank development process, including data collection, pre-processing, manual annotation, and quality assurance procedures. We provide baseline evaluations using multiple transformer-based models to assess the performance of state-of-the-art dependency parsers on Tagalog. We also highlight challenges in the syntactic analysis of Tagalog given its distinctive grammatical properties, and discuss its implications for the annotation of this treebank. We anticipate that UD-NewsCrawl and our baseline model implementations will serve as valuable resources for advancing computational linguistics research in underrepresented languages like Tagalog.",
      "arxiv_url": "https://arxiv.org/abs/2505.20428",
      "pdf_url": "https://arxiv.org/pdf/2505.20428",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "0203bbab0b813043d2bc783ccb2f5b92a1822fbf",
      "title": "GlyphPattern: An Abstract Pattern Recognition for Vision-Language Models",
      "authors": [
        "Zixuan Wu",
        "Yoolim Kim",
        "C. Anderson"
      ],
      "abstract": "Vision-Language Models (VLMs) building upon the foundation of powerful large language models have made rapid progress in reasoning across visual and textual data. While VLMs perform well on vision tasks that they are trained on, our results highlight key challenges in abstract pattern recognition. We present GlyphPattern, a 954 item dataset that pairs 318 human-written descriptions of visual patterns from 40 writing systems with three visual presentation styles. GlyphPattern evaluates abstract pattern recognition in VLMs, requiring models to understand and judge natural language descriptions of visual patterns. GlyphPattern patterns are drawn from a large-scale cognitive science investigation of human writing systems; as a result, they are rich in spatial reference and compositionality. Our experiments show that GlyphPattern is challenging for state-of-the-art VLMs (GPT-4o achieves only 55% accuracy), with marginal gains from few-shot prompting. Our detailed error analysis reveals challenges at multiple levels, including visual processing, natural language understanding, and pattern generalization.",
      "arxiv_url": "https://www.semanticscholar.org/paper/0203bbab0b813043d2bc783ccb2f5b92a1822fbf",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.07457",
      "title": "LLMs syntactically adapt their language use to their conversational partner",
      "authors": [
        "Florian Kandra",
        "Vera Demberg",
        "Alexander Koller"
      ],
      "abstract": "It has been frequently observed that human speakers align their language use with each other during conversations. In this paper, we study empirically whether large language models (LLMs) exhibit the same behavior of conversational adaptation. We construct a corpus of conversations between LLMs and find that two LLM agents end up making more similar syntactic choices as conversations go on, confirming that modern LLMs adapt their language use to their conversational partners in at least a rudimentary way.",
      "arxiv_url": "https://arxiv.org/abs/2503.07457",
      "pdf_url": "https://arxiv.org/pdf/2503.07457",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11083",
      "title": "Streamlining the Collaborative Chain of Models into A Single Forward Pass in Generation-Based Tasks",
      "authors": [
        "Yuanjie Lyu",
        "Chao Zhang",
        "Yuhao Chen",
        "Yong Chen",
        "Tong Xu"
      ],
      "abstract": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\"Chain of Models\"approach is widely used, where multiple specialized models work sequentially on distinct sub-tasks. This approach is effective but increases resource demands as each model must be deployed separately. Recent advancements attempt to address this by applying prompt tuning, which allows a shared base model to adapt to multiple tasks with minimal parameter changes. However, a key challenge remains: intermediate outputs, passed between models as plain text, require recomputation of hidden states (i.e., Key and Value (KV) states in Transformers) during inference. In this paper, we introduce FTHSS, a novel prompt-tuning method that enables models to share KV hidden states, eliminating redundant forward passes and reducing KV cache storage. By modifying input and attention masks during training, FTHSS allows models to effectively utilize KV hidden states from prior models in both single- and multi-round scenarios. Empirical results on four tasks show that FTHSS matches the performance of traditional model chains while improving inference efficiency.",
      "arxiv_url": "https://arxiv.org/abs/2502.11083",
      "pdf_url": "https://arxiv.org/pdf/2502.11083",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01332",
      "title": "An Empirical Study of Group Conformity in Multi-Agent Systems",
      "authors": [
        "Min Choi",
        "Keonwoo Kim",
        "Sungwon Chae",
        "Sangyeob Baek"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have enabled multi-agent systems that simulate real-world interactions with near-human reasoning. While previous studies have extensively examined biases related to protected attributes such as race, the emergence and propagation of biases on socially contentious issues in multi-agent LLM interactions remain underexplored. This study explores how LLM agents shape public opinion through debates on five contentious topics. By simulating over 2,500 debates, we analyze how initially neutral agents, assigned a centrist disposition, adopt specific stances over time. Statistical analyses reveal significant group conformity mirroring human behavior; LLM agents tend to align with numerically dominant groups or more intelligent agents, exerting a greater influence. These findings underscore the crucial role of agent intelligence in shaping discourse and highlight the risks of bias amplification in online interactions. Our results emphasize the need for policy measures that promote diversity and transparency in LLM-generated discussions to mitigate the risks of bias propagation within anonymous online environments.",
      "arxiv_url": "https://arxiv.org/abs/2506.01332",
      "pdf_url": "https://arxiv.org/pdf/2506.01332",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.05551",
      "title": "FRAME: Boosting LLMs with A Four-Quadrant Multi-Stage Pretraining Strategy",
      "authors": [
        "Xuemiao Zhang",
        "Feiyu Duan",
        "Liangyu Xu",
        "Yongwei Zhou",
        "Sirui Wang",
        "Rongxiang Weng",
        "Jingang Wang",
        "Xunliang Cai"
      ],
      "abstract": "Large language models (LLMs) have significantly advanced human language understanding and generation, with pretraining data quality and organization being crucial to their performance. Multi-stage pretraining is a promising approach, but existing methods often lack quantitative criteria for data partitioning and instead rely on intuitive heuristics. In this paper, we propose the novel Four-quadRAnt Multi-stage prEtraining strategy (FRAME), guided by the established principle of organizing the pretraining process into four stages to achieve significant loss reductions four times. This principle is grounded in two key findings: first, training on high Perplexity (PPL) data followed by low PPL data, and second, training on low PPL difference (PD) data followed by high PD data, both causing the loss to drop significantly twice and performance enhancements. By partitioning data into four quadrants and strategically organizing them, FRAME achieves a remarkable 16.8% average improvement over random across MMLU and CMMLU for the 3B model, effectively boosting LLM performance.",
      "arxiv_url": "https://arxiv.org/abs/2502.05551",
      "pdf_url": "https://arxiv.org/pdf/2502.05551",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12502",
      "title": "Efficient OpAmp Adaptation for Zoom Attention to Golden Contexts",
      "authors": [
        "Haoyuan Wu",
        "Rui Ming",
        "Haisheng Zheng",
        "Zhuolun He",
        "Bei Yu"
      ],
      "abstract": "Large language models (LLMs) have shown significant promise in question-answering (QA) tasks, particularly in retrieval-augmented generation (RAG) scenarios and long-context applications. However, their performance is hindered by noisy reference documents, which often distract from essential information. Despite fine-tuning efforts, Transformer-based architectures struggle to prioritize relevant content. This is evidenced by their tendency to allocate disproportionate attention to irrelevant or later-positioned documents. Recent work proposes the differential attention mechanism to address this issue, but this mechanism is limited by an unsuitable common-mode rejection ratio (CMRR) and high computational costs. Inspired by the operational amplifier (OpAmp), we propose the OpAmp adaptation to address these challenges, which is implemented with adapters efficiently. By integrating the adapter into pre-trained Transformer blocks, our approach enhances focus on the golden context without costly training from scratch. Empirical evaluations on noisy-context benchmarks reveal that our Qwen2.5-OpAmp-72B model, trained with our OpAmp adaptation, surpasses the performance of state-of-the-art LLMs, including DeepSeek-V3 and GPT-4o.",
      "arxiv_url": "https://arxiv.org/abs/2502.12502",
      "pdf_url": "https://arxiv.org/pdf/2502.12502",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "0273998f1af210983beef72691b3fd49588a0a84",
      "title": "Identifying Open Challenges in Language Identification",
      "authors": [
        "Rob van der Goot"
      ],
      "abstract": "Automatic language identification is a core problem of many Natural Language Processing (NLP) pipelines. A wide variety of architectures and benchmarks have been proposed with often near-perfect performance. Although previous studies have focused on certain challenging setups (i.e. cross-domain, short inputs), a systematic comparison is missing. We propose a benchmark that allows us to test for the effect of input size, training data size, domain, number of languages, scripts, and language families on performance. We evaluate five popular models on this benchmark and identify which open challenges remain for this task as well as which architectures achieve robust performance. We find that cross-domain setups are the most challenging (although arguably most relevant), and that number of languages, variety in scripts, and variety in language families have only a small impact on performance. We also contribute practical takeaways: training with 1,000 instances per language and a maximum input length of 100 characters is enough for robust language identification. Based on our findings, we train an accurate (94.41%) multi-domain language identification model on 2,034 languages, for which we also provide an analysis of the remaining errors. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/0273998f1af210983beef72691b3fd49588a0a84",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01475",
      "title": "PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization",
      "authors": [
        "Zouying Cao",
        "Runze Wang",
        "Yifei Yang",
        "Xinbei Ma",
        "Xiaoyong Zhu",
        "Bo Zheng",
        "Hai Zhao"
      ],
      "abstract": "Large Language Model (LLM) agents have demonstrated impressive capabilities in handling complex interactive problems. Existing LLM agents mainly generate natural language plans to guide reasoning, which is verbose and inefficient. NL plans are also tailored to specific tasks and restrict agents' ability to generalize across similar tasks. To this end, we explore pseudocode-style plans (P-code Plan) to capture the structural logic of reasoning. We find that P-code Plan empowers LLM agents with stronger generalization ability and more efficiency. Inspired by this finding, we propose a pseudocode-style Planning Guided Preference Optimization method called PGPO for effective agent learning. With two planning-oriented rewards, PGPO further enhances LLM agents' ability to generate high-quality P-code Plans and subsequent reasoning. Experiments show that PGPO achieves superior performance on representative agent benchmarks and outperforms the current leading baselines. Analyses reveal the advantage of PGPO in reducing action errors and omissions during reasoning.",
      "arxiv_url": "https://arxiv.org/abs/2506.01475",
      "pdf_url": "https://arxiv.org/pdf/2506.01475",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "02b700b70db1831648cfa49f8b5453e380b5471c",
      "title": "AutoAlign: Get Your LLM Aligned with Minimal Annotations",
      "authors": [
        "Xinyu Lu",
        "Dong Xu",
        "Chunkang Zhang",
        "Xinyan Guan",
        "Junxiang Wang",
        "Qingyu Zhang",
        "Pengbo Wang",
        "Yingzhi Mao",
        "Hao Xiang",
        "Xueru Wen",
        "Zichao Li",
        "Yaojie Lu",
        "Hongyu Lin",
        "Le Sun",
        "Xianpei Han"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/02b700b70db1831648cfa49f8b5453e380b5471c",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.15357",
      "title": "Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding",
      "authors": [
        "Elisa Sanchez-Bayona",
        "Rodrigo Agerri"
      ],
      "abstract": "This paper presents a comprehensive evaluation of the capabilities of Large Language Models (LLMs) in metaphor interpretation across multiple datasets, tasks, and prompt configurations. Although metaphor processing has gained significant attention in Natural Language Processing (NLP), previous research has been limited to single-dataset evaluations and specific task settings, often using artificially constructed data through lexical replacement. We address these limitations by conducting extensive experiments using diverse publicly available datasets with inference and metaphor annotations, focusing on Natural Language Inference (NLI) and Question Answering (QA) tasks. The results indicate that LLMs'performance is more influenced by features like lexical overlap and sentence length than by metaphorical content, demonstrating that any alleged emergent abilities of LLMs to understand metaphorical language are the result of a combination of surface-level features, in-context learning, and linguistic knowledge. This work provides critical insights into the current capabilities and limitations of LLMs in processing figurative language, highlighting the need for more realistic evaluation frameworks in metaphor interpretation tasks. Data and code are publicly available.",
      "arxiv_url": "https://arxiv.org/abs/2507.15357",
      "pdf_url": "https://arxiv.org/pdf/2507.15357",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.16389",
      "title": "RiOT: Efficient Prompt Refinement with Residual Optimization Tree",
      "authors": [
        "Chenyi Zhou",
        "Zhengyan Shi",
        "Yuan Yao",
        "Lei Liang",
        "Huajun Chen",
        "Qiang Zhang"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have highlighted their potential across a variety of tasks, but their performance still heavily relies on the design of effective prompts. Existing methods for automatic prompt optimization face two challenges: lack of diversity, limiting the exploration of valuable and innovative directions and semantic drift, where optimizations for one task can degrade performance in others. To address these issues, we propose Residual Optimization Tree (RiOT), a novel framework for automatic prompt optimization. RiOT iteratively refines prompts through text gradients, generating multiple semantically diverse candidates at each step, and selects the best prompt using perplexity. Additionally, RiOT incorporates the text residual connection to mitigate semantic drift by selectively retaining beneficial content across optimization iterations. A tree structure efficiently manages the optimization process, ensuring scalability and flexibility. Extensive experiments across five benchmarks, covering commonsense, mathematical, logical, temporal, and semantic reasoning, demonstrate that RiOT outperforms both previous prompt optimization methods and manual prompting.",
      "arxiv_url": "https://arxiv.org/abs/2506.16389",
      "pdf_url": "https://arxiv.org/pdf/2506.16389",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.15569",
      "title": "SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification",
      "authors": [
        "Chengye Wang",
        "Yifei Shen",
        "Zexi Kuang",
        "Arman Cohan",
        "Yilun Zhao"
      ],
      "abstract": "We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks.",
      "arxiv_url": "https://arxiv.org/abs/2506.15569",
      "pdf_url": "https://arxiv.org/pdf/2506.15569",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-06-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.08603",
      "title": "Unlocking Speech Instruction Data Potential with Query Rewriting",
      "authors": [
        "Yonghua Hei",
        "Yibo Yan",
        "Shuliang Liu",
        "Huiyu Zhou",
        "Linfeng Zhang",
        "Xuming Hu"
      ],
      "abstract": "End-to-end Large Speech Language Models~(\\textbf{LSLMs}) demonstrate strong potential in response latency and speech comprehension capabilities, showcasing general intelligence across speech understanding tasks. However, the ability to follow speech instructions has not been fully realized due to the lack of datasets and heavily biased training tasks. Leveraging the rich ASR datasets, previous approaches have used Large Language Models~(\\textbf{LLMs}) to continue the linguistic information of speech to construct speech instruction datasets. Yet, due to the gap between LLM-generated results and real human responses, the continuation methods further amplify these shortcomings. Given the high costs of collecting and annotating speech instruction datasets by humans, using speech synthesis to construct large-scale speech instruction datasets has become a balanced and robust alternative. Although modern Text-To-Speech~(\\textbf{TTS}) models have achieved near-human-level synthesis quality, it is challenging to appropriately convert out-of-distribution text instruction to speech due to the limitations of the training data distribution in TTS models. To address this issue, we propose a query rewriting framework with multi-LLM knowledge fusion, employing multiple agents to annotate and validate the synthesized speech, making it possible to construct high-quality speech instruction datasets without relying on human annotation. Experiments show that this method can transform text instructions into distributions more suitable for TTS models for speech synthesis through zero-shot rewriting, increasing data usability from 72\\% to 93\\%. It also demonstrates unique advantages in rewriting tasks that require complex knowledge and context-related abilities.",
      "arxiv_url": "https://arxiv.org/abs/2507.08603",
      "pdf_url": "https://arxiv.org/pdf/2507.08603",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.09712",
      "title": "The Structural Safety Generalization Problem",
      "authors": [
        "Julius Broomfield",
        "Tom Gibbs",
        "Ethan Kosak-Hine",
        "George Ingebretsen",
        "Tia Nasir",
        "Jason Zhang",
        "Reihaneh Iranmanesh",
        "Sara Pieri",
        "Reihaneh Rabbany",
        "Kellin Pelrine"
      ],
      "abstract": "LLM jailbreaks are a widespread safety challenge. Given this problem has not yet been tractable, we suggest targeting a key failure mechanism: the failure of safety to generalize across semantically equivalent inputs. We further focus the target by requiring desirable tractability properties of attacks to study: explainability, transferability between models, and transferability between goals. We perform red-teaming within this framework by uncovering new vulnerabilities to multi-turn, multi-image, and translation-based attacks. These attacks are semantically equivalent by our design to their single-turn, single-image, or untranslated counterparts, enabling systematic comparisons; we show that the different structures yield different safety outcomes. We then demonstrate the potential for this framework to enable new defenses by proposing a Structure Rewriting Guardrail, which converts an input to a structure more conducive to safety assessment. This guardrail significantly improves refusal of harmful inputs, without over-refusing benign ones. Thus, by framing this intermediate challenge - more tractable than universal defenses but essential for long-term safety - we highlight a critical milestone for AI safety research.",
      "arxiv_url": "https://arxiv.org/abs/2504.09712",
      "pdf_url": "https://arxiv.org/pdf/2504.09712",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11466",
      "title": "GiFT: Gibbs Fine-Tuning for Code Generation",
      "authors": [
        "Haochen Li",
        "Wanjin Feng",
        "Xin Zhou",
        "Zhiqi Shen"
      ],
      "abstract": "Training Large Language Models (LLMs) with synthetic data is a prevalent practice in code generation. A key approach is self-training, where LLMs are iteratively trained on self-generated correct code snippets. In this case, the self-generated codes are drawn from a conditional distribution, conditioned on a specific seed description. However, the seed description is not the only valid representation that aligns with its intended meaning. With all valid descriptions and codes forming a joint space, codes drawn from the conditional distribution would lead to an underrepresentation of the full description-code space. As such, we propose Gibbs Fine-Tuning (GiFT), a novel self-training method inspired by Gibbs sampling. GiFT allows self-generated data to be drawn from the marginal distribution of the joint space, thereby mitigating the biases inherent in conditional sampling. We provide a theoretical analysis demonstrating the potential benefits of fine-tuning LLMs with code derived from the marginal distribution. Furthermore, we propose a perplexity-based code selection method to mitigate the imbalanced long-tail distribution of the self-generated codes. Empirical evaluation of two LLMs across four datasets demonstrates that GiFT achieves superior performance, particularly on more challenging benchmarks. Source code is available at https://github.com/Alex-HaochenLi/GiFT.",
      "arxiv_url": "https://arxiv.org/abs/2502.11466",
      "pdf_url": "https://arxiv.org/pdf/2502.11466",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16104",
      "title": "Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models",
      "authors": [
        "Yue Li",
        "Xin Yi",
        "Dongsheng Shi",
        "Gerard de Melo",
        "Xiaoling Wang",
        "Linlin Wang"
      ],
      "abstract": "With the increasing size of Large Vision-Language Models (LVLMs), network pruning techniques aimed at compressing models for deployment in resource-constrained environments have garnered significant attention. However, we observe that pruning often leads to a degradation in safety performance. To address this issue, we present a novel and lightweight approach, termed Hierarchical Safety Realignment (HSR). HSR operates by first quantifying the contribution of each attention head to safety, identifying the most critical ones, and then selectively restoring neurons directly within these attention heads that play a pivotal role in maintaining safety. This process hierarchically realigns the safety of pruned LVLMs, progressing from the attention head level to the neuron level. We validate HSR across various models and pruning strategies, consistently achieving notable improvements in safety performance. To our knowledge, this is the first work explicitly focused on restoring safety in LVLMs post-pruning.",
      "arxiv_url": "https://arxiv.org/abs/2505.16104",
      "pdf_url": "https://arxiv.org/pdf/2505.16104",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "032eeb1cdd63e647664e130d6b78c39ec7831900",
      "title": "MARK: Multi-agent Collaboration with Ranking Guidance for Text-attributed Graph Clustering",
      "authors": [
        "Yiwei Fu",
        "Yuxing Zhang",
        "Chunchun Chen",
        "JianwenMa JianwenMa",
        "Quan Yuan",
        "Rong-Cheng Tu",
        "Xinli Huang",
        "Wei Ye",
        "Xiao Luo",
        "Minghua Deng"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/032eeb1cdd63e647664e130d6b78c39ec7831900",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07154",
      "title": "Syntactic Control of Language Models by Posterior Inference",
      "authors": [
        "Vicky Xefteri",
        "Tim Vieira",
        "Ryan Cotterell",
        "Afra Amini"
      ],
      "abstract": "Controlling the syntactic structure of text generated by language models is valuable for applications requiring clarity, stylistic consistency, or interpretability, yet it remains a challenging task. In this paper, we argue that sampling algorithms based on the posterior inference can effectively enforce a target constituency structure during generation. Our approach combines sequential Monte Carlo, which estimates the posterior distribution by sampling from a proposal distribution, with a syntactic tagger that ensures that each generated token aligns with the desired syntactic structure. Our experiments with GPT2 and Llama3-8B models show that with an appropriate proposal distribution, we can improve syntactic accuracy, increasing the F1 score from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both cases without compromising the language model's fluency. These results underscore both the complexity of syntactic control and the effectiveness of sampling algorithms, offering a promising approach for applications where precise control over syntax is essential.",
      "arxiv_url": "https://arxiv.org/abs/2506.07154",
      "pdf_url": "https://arxiv.org/pdf/2506.07154",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.13216",
      "title": "Capability Salience Vector: Fine-grained Alignment of Loss and Capabilities for Downstream Task Scaling Law",
      "authors": [
        "Qiming Ge",
        "Shuhao Xing",
        "Songyang Gao",
        "Yunhua Zhou",
        "Yicheng Zou",
        "Songyang Zhang",
        "Zhi Chen",
        "Hang Yan",
        "Qi Zhang",
        "Qipeng Guo",
        "Kai Chen"
      ],
      "abstract": "Scaling law builds the relationship between training computation and validation loss, enabling researchers to effectively predict the loss trending of models across different levels of computation. However, a gap still remains between validation loss and the model's downstream capabilities, making it untrivial to apply scaling law to direct performance prediction for downstream tasks. The loss typically represents a cumulative penalty for predicted tokens, which are implicitly considered to have equal importance. Nevertheless, our studies have shown evidence that when considering different training data distributions, we cannot directly model the relationship between downstream capability and computation or token loss. To bridge the gap between validation loss and downstream task capabilities, in this work, we introduce Capability Salience Vector, which decomposes the overall loss and assigns different importance weights to tokens to assess a specific meta-capability, aligning the validation loss with downstream task performance in terms of the model's capabilities. Experiments on various popular benchmarks demonstrate that our proposed Capability Salience Vector could significantly improve the predictability of language model performance on downstream tasks.",
      "arxiv_url": "https://arxiv.org/abs/2506.13216",
      "pdf_url": "https://arxiv.org/pdf/2506.13216",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14494",
      "title": "StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following",
      "authors": [
        "Jinnan Li",
        "Jinzhe Li",
        "Yue Wang",
        "Yi Chang",
        "Yuan Wu"
      ],
      "abstract": "Multi-turn instruction following capability constitutes a core competency of large language models (LLMs) in real-world applications. Existing evaluation benchmarks predominantly focus on fine-grained constraint satisfaction and domain-specific capability assessment, yet overlook the crucial structural dependencies between dialogue turns that distinguish multi-turn from single-turn interactions. These structural dependencies not only reflect user intent but also establish an essential second dimension for the instruction following evaluation beyond constraint satisfaction. To address this gap, we propose StructFlowBench, a multi-turn instruction following benchmark with structural flow modeling. The benchmark defines an innovative structural flow framework with six fundamental inter-turn relationships. These relationships introduce novel structural constraints for model evaluation and also serve as generation parameters for creating customized dialogue flows tailored to specific scenarios. Adopting established LLM-based automatic evaluation methodologies, we conduct systematic evaluations of 13 leading open-source and closed-source LLMs. Experimental results reveal significant deficiencies in current models' comprehension of multi-turn dialogue structures. The code is available at https://github.com/MLGroupJLU/StructFlowBench.",
      "arxiv_url": "https://arxiv.org/abs/2502.14494",
      "pdf_url": "https://arxiv.org/pdf/2502.14494",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18557",
      "title": "TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation",
      "authors": [
        "He Zhu",
        "Zhiwen Ruan",
        "Junyou Su",
        "Xingwei He",
        "Yun Chen",
        "Wenjia Zhang",
        "Guanhua Chen"
      ],
      "abstract": "High-quality instruction data is crucial for developing large language models (LLMs), yet existing approaches struggle to effectively control instruction complexity. We present TAG-INSTRUCT, a novel framework that enhances instruction complexity through structured semantic compression and controlled difficulty augmentation. Unlike previous prompt-based methods operating on raw text, TAG-INSTRUCT compresses instructions into a compact tag space and systematically enhances complexity through RL-guided tag expansion. Through extensive experiments, we show that TAG-INSTRUCT outperforms existing instruction complexity augmentation approaches. Our analysis reveals that operating in tag space provides superior controllability and stability across different instruction synthesis frameworks.",
      "arxiv_url": "https://arxiv.org/abs/2505.18557",
      "pdf_url": "https://arxiv.org/pdf/2505.18557",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03690",
      "title": "Robust Preference Optimization via Dynamic Target Margins",
      "authors": [
        "Jie Sun",
        "Junkang Wu",
        "Jiancan Wu",
        "Zhibo Zhu",
        "Xingyu Lu",
        "Jun Zhou",
        "Lintao Ma",
        "Xiang Wang"
      ],
      "abstract": "The alignment of Large Language Models (LLMs) is crucial for ensuring their safety and reliability in practical applications. Direct Preference Optimization (DPO) has emerged as an efficient method that directly optimizes models using preference pairs, significantly reducing resource demands. However, the effectiveness of DPO heavily depends on the data quality, which is frequently compromised by noise. In this work, we propose $\\gamma$-PO, a dynamic target margin preference optimization algorithm that adjust reward margins at the pairwise level. By introducing instance-specific margin calibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those demonstrating higher reward margins) while suppressing potential noise from ambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible with variants of DPO that rely on reward margin between preference pairs. Across benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an average 4.4\\% improvement over other baselines, setting new benchmarks for state-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code changes and has a negligible impact on training efficiency, making it a robust solution for enhancing LLMs alignment. Our codes are available at \\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.",
      "arxiv_url": "https://arxiv.org/abs/2506.03690",
      "pdf_url": "https://arxiv.org/pdf/2506.03690",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "03a35e97a1500ef4235be3c6659b1b560f94c998",
      "title": "Summary Factual Inconsistency Detection Based on LLMs Enhanced by Universal Information Extraction",
      "authors": [
        "Anguo Li",
        "Lei Yu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/03a35e97a1500ef4235be3c6659b1b560f94c998",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.06474",
      "title": "ROGRAG: A Robustly Optimized GraphRAG Framework",
      "authors": [
        "Zhefan Wang",
        "Huanjun Kong",
        "Jie Ying",
        "Wanli Ouyang",
        "Nanqing Dong"
      ],
      "abstract": "Large language models (LLMs) commonly struggle with specialized or emerging topics which are rarely seen in the training corpus. Graph-based retrieval-augmented generation (GraphRAG) addresses this by structuring domain knowledge as a graph for dynamic retrieval. However, existing pipelines involve complex engineering workflows, making it difficult to isolate the impact of individual components. It is also challenging to evaluate the retrieval effectiveness due to the overlap between the pretraining and evaluation datasets. In this work, we introduce ROGRAG, a Robustly Optimized GraphRAG framework. Specifically, we propose a multi-stage retrieval mechanism that integrates dual-level with logic form retrieval methods to improve retrieval robustness without increasing computational cost. To further refine the system, we incorporate various result verification methods and adopt an incremental database construction approach. Through extensive ablation experiments, we rigorously assess the effectiveness of each component. Our implementation includes comparative experiments on SeedBench, where Qwen2.5-7B-Instruct initially underperformed. ROGRAG significantly improves the score from 60.0% to 75.0% and outperforms mainstream methods. Experiments on domain-specific datasets reveal that dual-level retrieval enhances fuzzy matching, while logic form retrieval improves structured reasoning, highlighting the importance of multi-stage retrieval.ROGRAG is released as an open-source resource and supports installation with pip.",
      "arxiv_url": "https://arxiv.org/abs/2503.06474",
      "pdf_url": "https://arxiv.org/pdf/2503.06474",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-03-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.14431",
      "title": "Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes Domains",
      "authors": [
        "Xu Chu",
        "Zhijie Tan",
        "Hanlin Xue",
        "Guanyu Wang",
        "Tong Mo",
        "Weiping Li"
      ],
      "abstract": "Large Language Models (LLMs) are widely applied to downstream domains. However, current LLMs for high-stakes domain tasks, such as financial investment and legal QA, typically generate brief answers without reasoning processes and explanations. This limits users' confidence in making decisions based on their responses. While original CoT shows promise, it lacks self-correction mechanisms during reasoning. This work introduces Domain$o1$s, which enhances LLMs' reasoning capabilities on domain tasks through supervised fine-tuning and tree search. We construct CoT-stock-2k and CoT-legal-2k datasets for fine-tuning models that activate domain-specific reasoning steps based on their judgment. Additionally, we propose Selective Tree Exploration to spontaneously explore solution spaces and sample optimal reasoning paths to improve performance. We also introduce PROOF-Score, a new metric for evaluating domain models' explainability, complementing traditional accuracy metrics with richer assessment dimensions. Extensive experiments on stock investment recommendation and legal reasoning QA tasks demonstrate Domaino1s's leading performance and explainability. Our code is available at https://github.com/Hyalinesky/Domaino1s.",
      "arxiv_url": "https://arxiv.org/abs/2501.14431",
      "pdf_url": "https://arxiv.org/pdf/2501.14431",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.01550",
      "title": "Representation Bending for Large Language Model Safety",
      "authors": [
        "Ashkan Yousefpour",
        "Taeheon Kim",
        "Ryan S. Kwon",
        "Seungbeen Lee",
        "Wonje Jeung",
        "Seungju Han",
        "Alvin Wan",
        "Harrison Ngan",
        "Youngjae Yu",
        "Jonghyun Choi"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as powerful tools, but their inherent safety risks - ranging from harmful content generation to broader societal harms - pose significant challenges. These risks can be amplified by the recent adversarial attacks, fine-tuning vulnerabilities, and the increasing deployment of LLMs in high-stakes environments. Existing safety-enhancing techniques, such as fine-tuning with human feedback or adversarial training, are still vulnerable as they address specific threats and often fail to generalize across unseen attacks, or require manual system-level defenses. This paper introduces RepBend, a novel approach that fundamentally disrupts the representations underlying harmful behaviors in LLMs, offering a scalable solution to enhance (potentially inherent) safety. RepBend brings the idea of activation steering - simple vector arithmetic for steering model's behavior during inference - to loss-based fine-tuning. Through extensive evaluation, RepBend achieves state-of-the-art performance, outperforming prior methods such as Circuit Breaker, RMU, and NPO, with up to 95% reduction in attack success rates across diverse jailbreak benchmarks, all with negligible reduction in model usability and general capabilities.",
      "arxiv_url": "https://arxiv.org/abs/2504.01550",
      "pdf_url": "https://arxiv.org/pdf/2504.01550",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "03f72f79e2687d28e83e58b08f10b1cc8c08b31e",
      "title": "QAEval: Mixture of Evaluators for Question-Answering Task Evaluation",
      "authors": [
        "Tan Yue",
        "Rui Mao",
        "Xuzhao Shi",
        "Shuo Zhan",
        "Zuhao Yang",
        "Dongyan Zhao"
      ],
      "abstract": "Question answering (QA) tasks serve as a key benchmark for evaluating generation systems. Traditional rule-based metrics, such as accuracy and relaxed-accuracy, struggle with open-ended and unstructured responses. LLM-based evaluation methods offer greater flexibility but suffer from sensitivity to instructions, robustness issues, and high computational costs. To overcome these challenges, we introduce QAE-val, a hybrid framework combining rule-based reliability with LLM-based adaptability. QAE-val utilizes two high-quality datasets: QAEx-tract for short-answer extraction and QAScore for scoring model training. By integrating a Mixture of Evaluators model with Dynamic Load Balancing Optimization, QAEval enables accurate, cost-effective QA evaluation. Experimental results show it outperforms models like GPT-4o and Claude-3, achieving 92.3% accuracy with only 0.6B parameters.",
      "arxiv_url": "https://www.semanticscholar.org/paper/03f72f79e2687d28e83e58b08f10b1cc8c08b31e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.18636",
      "title": "SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model",
      "authors": [
        "Xun Liang",
        "Simin Niu",
        "Zhiyu Li",
        "Sensen Zhang",
        "Hanyu Wang",
        "Feiyu Xiong",
        "Jason Zhaoxin Fan",
        "Bo Tang",
        "Shichao Song",
        "Mengwei Wang",
        "Jiawei Yang"
      ],
      "abstract": "The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https://github.com/IAAR-Shanghai/SafeRAG.",
      "arxiv_url": "https://arxiv.org/abs/2501.18636",
      "pdf_url": "https://arxiv.org/pdf/2501.18636",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-01-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00064",
      "title": "Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling",
      "authors": [
        "Jiayi Zeng",
        "Yizhe Feng",
        "Mengliang He",
        "Wenhui Lei",
        "Wei Zhang",
        "Zeming Liu",
        "Xiaoming Shi",
        "Aimin Zhou"
      ],
      "abstract": "Large language models (LLMs) have demonstrated significant advancements in error handling. Current error-handling works are performed in a passive manner, with explicit error-handling instructions. However, in real-world scenarios, explicit error-handling instructions are usually unavailable. In this paper, our work identifies this challenge as how to conduct proactive error handling without explicit error handling instructions. To promote further research, this work introduces a new benchmark, termed Mis-prompt, consisting of four evaluation tasks, an error category taxonomy, and a new evaluation dataset. Furthermore, this work analyzes current LLMs' performance on the benchmark, and the experimental results reveal that current LLMs show poor performance on proactive error handling, and SFT on error handling instances improves LLMs' proactive error handling capabilities. The dataset will be publicly available.",
      "arxiv_url": "https://arxiv.org/abs/2506.00064",
      "pdf_url": "https://arxiv.org/pdf/2506.00064",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.09730",
      "title": "Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem Proving",
      "authors": [
        "Sara Rajaee",
        "Kumar Pratik",
        "Gabriele Cesa",
        "Arash Behboodi"
      ],
      "abstract": "The most promising recent methods for AI reasoning require applying variants of reinforcement learning (RL) either on rolled out trajectories from the LLMs, even for the step-wise rewards, or large quantities of human-annotated trajectory data. The reliance on the rolled-out trajectory renders the compute cost and time prohibitively high. In particular, the correctness of a reasoning trajectory can typically only be judged at its completion, leading to sparse rewards in RL or requiring expensive synthetic data generation in expert iteration-like methods. In this work, we focus on the Automatic Theorem Proving (ATP) task and propose a novel verifier-in-the-loop design, which, unlike existing approaches that leverage feedback on the entire reasoning trajectory, employs an automated verifier to give intermediate feedback at each step of the reasoning process. Using Lean as the verifier, we empirically show that the step-by-step local verification produces a global improvement in the model's reasoning accuracy and efficiency.",
      "arxiv_url": "https://arxiv.org/abs/2503.09730",
      "pdf_url": "https://arxiv.org/pdf/2503.09730",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "0443af04a138bbb537d1bfefa6fb7a7c83a432f8",
      "title": "SocialForge: simulating the social internet to provide realistic training against influence operations",
      "authors": [
        "Ulysse Oliveri",
        "Guillaume Gadek",
        "Alexandre Dey",
        "Benjamin Coste",
        "Damien Lolive",
        "Arnaud Delhay",
        "Bruno Grilheres"
      ],
      "abstract": "Social media platforms have enabled large-scale influence campaigns, impacting democratic processes. To fight against these threats, continuous training is needed. A typical training session is based on a fictive scenario describing key elements which are instantiated into a dedicated platform. Such a platform simulates social networks, which host a huge amount of content aligned with the training scenario. However, directly using Large Language Models to create appropriate content results in low content diversity due to coarse-grained and high-level scenario constraints, which compromises the trainees’ immersion. We address this issue with SocialForge , a system designed to enhance the diversity and realism of the generated content while ensuring its adherence to the original scenario. Specifically, SocialForge refines and augments the initial scenario constraints by generating detailed subnarratives, personas, and events. We assess diversity, realism, and adherence to the scenario through custom evaluation protocol. We also propose an automatic method to detect erroneous constraint generation, ensuring optimal alignment of the content with the scenario. SocialForge has been used in real trainings and in several showcases, with great end-user satisfaction. We release an open-source dataset 1 generated with SocialForge for the research community.",
      "arxiv_url": "https://www.semanticscholar.org/paper/0443af04a138bbb537d1bfefa6fb7a7c83a432f8",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.03340",
      "title": "EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with Neural Knowledge Base of Entity States",
      "authors": [
        "Hainiu Xu",
        "Siya Qi",
        "Jiazheng Li",
        "Yuxiang Zhou",
        "Jinhua Du",
        "C. Catmur",
        "Yulan He"
      ],
      "abstract": "Theory-of-Mind (ToM), the ability to infer others' perceptions and mental states, is fundamental to human interaction but remains challenging for Large Language Models (LLMs). While existing ToM reasoning methods show promise with reasoning via perceptual perspective-taking, they often rely excessively on off-the-shelf LLMs, reducing their efficiency and limiting their applicability to high-order ToM reasoning. To address these issues, we present EnigmaToM, a novel neuro-symbolic framework that enhances ToM reasoning by integrating a Neural Knowledge Base of entity states (Enigma) for (1) a psychology-inspired iterative masking mechanism that facilitates accurate perspective-taking and (2) knowledge injection that elicits key entity information. Enigma generates structured knowledge of entity states to build spatial scene graphs for belief tracking across various ToM orders and enrich events with fine-grained entity state details. Experimental results on ToMi, HiToM, and FANToM benchmarks show that EnigmaToM significantly improves ToM reasoning across LLMs of varying sizes, particularly excelling in high-order reasoning scenarios.",
      "arxiv_url": "https://arxiv.org/abs/2503.03340",
      "pdf_url": "https://arxiv.org/pdf/2503.03340",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "05178fa35a317d916d8d8f94eeb35702960f985a",
      "title": "Detecting and Mitigating Challenges in Zero-Shot Video Summarization with Video LLMs",
      "authors": [
        "Luca Cagliero",
        "Lorenzo Vaiani",
        "Eliana Pastor",
        "Alkis Koudounas",
        "Elena Baralis",
        "Vittorio Mazzia",
        "Sandro Pollastrini",
        "Thomas Gueudré",
        "Manuel Giollo",
        "Daniele Amberti",
        "Yue Wu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/05178fa35a317d916d8d8f94eeb35702960f985a",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12658",
      "title": "R.R.: Unveiling LLM Training Privacy through Recollection and Ranking",
      "authors": [
        "Wenlong Meng",
        "Zhenyuan Guo",
        "Lenan Wu",
        "Chen Gong",
        "Wenyan Liu",
        "Weixian Li",
        "Chengkun Wei",
        "Wenzhi Chen"
      ],
      "abstract": "Large Language Models (LLMs) pose significant privacy risks, potentially leaking training data due to implicit memorization. Existing privacy attacks primarily focus on membership inference attacks (MIAs) or data extraction attacks, but reconstructing specific personally identifiable information (PII) in LLMs' training data remains challenging. In this paper, we propose R.R. (Recollect and Rank), a novel two-step privacy stealing attack that enables attackers to reconstruct PII entities from scrubbed training data where the PII entities have been masked. In the first stage, we introduce a prompt paradigm named recollection, which instructs the LLM to repeat a masked text but fill in masks. Then we can use PII identifiers to extract recollected PII candidates. In the second stage, we design a new criterion to score each PII candidate and rank them. Motivated by membership inference, we leverage the reference model as a calibration to our criterion. Experiments across three popular PII datasets demonstrate that the R.R. achieves better PII identification performance than baselines. These results highlight the vulnerability of LLMs to PII leakage even when training data has been scrubbed. We release our code and datasets at GitHub.",
      "arxiv_url": "https://arxiv.org/abs/2502.12658",
      "pdf_url": "https://arxiv.org/pdf/2502.12658",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20622",
      "title": "SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation",
      "authors": [
        "Ting Xu",
        "Zhichao Huang",
        "Jiankai Sun",
        "Shanbo Cheng",
        "Wai Lam"
      ],
      "abstract": "We present Sequential Policy Optimization for Simultaneous Machine Translation (SeqPO-SiMT), a new policy optimization framework that defines the simultaneous machine translation (SiMT) task as a sequential decision making problem, incorporating a tailored reward to enhance translation quality while reducing latency. In contrast to popular Reinforcement Learning from Human Feedback (RLHF) methods, such as PPO and DPO, which are typically applied in single-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task. This intuitive framework allows the SiMT LLMs to simulate and refine the SiMT process using a tailored reward. We conduct experiments on six datasets from diverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that SeqPO-SiMT consistently achieves significantly higher translation quality with lower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning (SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17 in the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context than offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly rival the offline translation of high-performing LLMs, including Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.",
      "arxiv_url": "https://arxiv.org/abs/2505.20622",
      "pdf_url": "https://arxiv.org/pdf/2505.20622",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.15299",
      "title": "Multi-Hop Question Generation via Dual-Perspective Keyword Guidance",
      "authors": [
        "Maodong Li",
        "Longyin Zhang",
        "Fang Kong"
      ],
      "abstract": "Multi-hop question generation (MQG) aims to generate questions that require synthesizing multiple information snippets from documents to derive target answers. The primary challenge lies in effectively pinpointing crucial information snippets related to question-answer (QA) pairs, typically relying on keywords. However, existing works fail to fully utilize the guiding potential of keywords and neglect to differentiate the distinct roles of question-specific and document-specific keywords. To address this, we define dual-perspective keywords (i.e., question and document keywords) and propose a Dual-Perspective Keyword-Guided (DPKG) framework, which seamlessly integrates keywords into the multi-hop question generation process. We argue that question keywords capture the questioner's intent, whereas document keywords reflect the content related to the QA pair. Functionally, question and document keywords work together to pinpoint essential information snippets in the document, with question keywords required to appear in the generated question. The DPKG framework consists of an expanded transformer encoder and two answer-aware transformer decoders for keyword and question generation, respectively. Extensive experiments demonstrate the effectiveness of our work, showcasing its promising performance and underscoring its significant value in the MQG task.",
      "arxiv_url": "https://arxiv.org/abs/2505.15299",
      "pdf_url": "https://arxiv.org/pdf/2505.15299",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23561",
      "title": "Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models",
      "authors": [
        "Zenghui Yuan",
        "Yangming Xu",
        "Jiawen Shi",
        "Pan Zhou",
        "Lichao Sun"
      ],
      "abstract": "Model merging for Large Language Models (LLMs) directly fuses the parameters of different models finetuned on various tasks, creating a unified model for multi-domain tasks. However, due to potential vulnerabilities in models available on open-source platforms, model merging is susceptible to backdoor attacks. In this paper, we propose Merge Hijacking, the first backdoor attack targeting model merging in LLMs. The attacker constructs a malicious upload model and releases it. Once a victim user merges it with any other models, the resulting merged model inherits the backdoor while maintaining utility across tasks. Merge Hijacking defines two main objectives-effectiveness and utility-and achieves them through four steps. Extensive experiments demonstrate the effectiveness of our attack across different models, merging algorithms, and tasks. Additionally, we show that the attack remains effective even when merging real-world models. Moreover, our attack demonstrates robustness against two inference-time defenses (Paraphrasing and CLEANGEN) and one training-time defense (Fine-pruning).",
      "arxiv_url": "https://arxiv.org/abs/2505.23561",
      "pdf_url": "https://arxiv.org/pdf/2505.23561",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16487",
      "title": "All That Glitters is Not Novel: Plagiarism in AI Generated Research",
      "authors": [
        "Tarun Gupta",
        "Danish Pruthi"
      ],
      "abstract": "Automating scientific research is considered the final frontier of science. Recently, several papers claim autonomous research agents can generate novel research ideas. Amidst the prevailing optimism, we document a critical concern: a considerable fraction of such research documents are smartly plagiarized. Unlike past efforts where experts evaluate the novelty and feasibility of research ideas, we request $13$ experts to operate under a different situational logic: to identify similarities between LLM-generated research documents and existing work. Concerningly, the experts identify $24\\%$ of the $50$ evaluated research documents to be either paraphrased (with one-to-one methodological mapping), or significantly borrowed from existing work. These reported instances are cross-verified by authors of the source papers. The remaining $76\\%$ of documents show varying degrees of similarity with existing work, with only a small fraction appearing completely novel. Problematically, these LLM-generated research documents do not acknowledge original sources, and bypass inbuilt plagiarism detectors. Lastly, through controlled experiments we show that automated plagiarism detectors are inadequate at catching plagiarized ideas from such systems. We recommend a careful assessment of LLM-generated research, and discuss the implications of our findings on academic publishing.",
      "arxiv_url": "https://arxiv.org/abs/2502.16487",
      "pdf_url": "https://arxiv.org/pdf/2502.16487",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11404",
      "title": "ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models",
      "authors": [
        "Hanxing Ding",
        "Shuchang Tao",
        "Liang Pang",
        "Zihao Wei",
        "Jinyang Gao",
        "Bolin Ding",
        "Huawei Shen",
        "Xueqi Chen"
      ],
      "abstract": "Tool learning has emerged as a crucial capability for large language models (LLMs) to solve complex real-world tasks through interaction with external tools. Existing approaches face significant challenges, including reliance on hand-crafted prompts, difficulty in multi-step planning, and lack of precise error diagnosis and reflection mechanisms. We propose ToolCoder, a novel framework that reformulates tool learning as a code generation task. Inspired by software engineering principles, ToolCoder transforms natural language queries into structured Python function scaffold and systematically breaks down tasks with descriptive comments, enabling LLMs to leverage coding paradigms for complex reasoning and planning. It then generates and executes function implementations to obtain final responses. Additionally, ToolCoder stores successfully executed functions in a repository to promote code reuse, while leveraging error traceback mechanisms for systematic debugging, optimizing both execution efficiency and robustness. Experiments demonstrate that ToolCoder achieves superior performance in task completion accuracy and execution reliability compared to existing approaches, establishing the effectiveness of code-centric approaches in tool learning.",
      "arxiv_url": "https://arxiv.org/abs/2502.11404",
      "pdf_url": "https://arxiv.org/pdf/2502.11404",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "05a5fd60686674ac8958db8e2936100d4223ed83",
      "title": "We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?",
      "authors": [
        "Runqi Qiao",
        "Qiuna Tan",
        "Guanting Dong",
        "Minhui Wu",
        "Chong Sun",
        "Xiaoshuai Song",
        "Jiapeng Wang",
        "Zhuoma Gongque",
        "Shanglin Lei",
        "Yifan Zhang",
        "Zhe Wei",
        "Miaoxuan Zhang",
        "Runfeng Qiao",
        "Xiao Zong",
        "Yida Xu",
        "Peiqing Yang",
        "Zhimin Bao",
        "Muxi Diao",
        "Chen Li",
        "Honggang Zhang"
      ],
      "abstract": "Visual mathematical reasoning, as a fundamental visual reasoning ability, has received widespread attention from the Large Multi-modal Models (LMMs) community. Existing benchmarks mainly focus more on the end-to-end performance, but neglect the underlying principles of knowledge acquisition and generalization. Instead, we introduce W E -M ATH , the first benchmark specifically designed to explore the problem-solving principles. We metic-ulously collect 6.5K visual math problems and decompose them into 10.9K step-level questions for evaluation, spanning 5 layers of knowledge granularity and 67 hierarchical knowledge concepts. Specifically, we decompose composite problems into sub-problems according to the required knowledge concepts and introduce a novel four-dimensional metric to hierarchically assess inherent issues in LMMs’ reasoning process. With W E -M ATH , we conduct a thorough evaluation of existing LMMs in visual mathematical reasoning and provide comprehensive analysis and insight for future development. We anticipate that W E -M ATH will open new pathways for advancements in visual mathematical reasoning for LMMs. Data and code are available at https://github.com/We-Math/We-Math.",
      "arxiv_url": "https://www.semanticscholar.org/paper/05a5fd60686674ac8958db8e2936100d4223ed83",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.10442",
      "title": "Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities",
      "authors": [
        "Shivam Chandhok",
        "Wan-Cyuan Fan",
        "Vered Shwartz",
        "Vineeth N. Balasubramanian",
        "Leonid Sigal"
      ],
      "abstract": "Vision-language Models (VLMs) have emerged as general-purpose tools for addressing a variety of complex computer vision problems. Such models have been shown to be highly capable, but, at the same time, lacking some basic visual understanding skills. In this paper, we set out to understand the limitations of SoTA VLMs on fundamental visual tasks by constructing a series of tests that probe which components of design, specifically, may be lacking. Importantly, we go significantly beyond the current benchmarks, which simply measure the final performance of VLM response, by also comparing and contrasting it to the performance of probes trained directly on features obtained from the visual encoder, intermediate vision-language projection and LLM-decoder output. In doing so, we uncover shortcomings in VLMs and make a number of important observations about their capabilities, robustness and how they process visual information. We hope our insights will guide progress in further improving VLMs.",
      "arxiv_url": "https://arxiv.org/abs/2507.10442",
      "pdf_url": "https://arxiv.org/pdf/2507.10442",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16770",
      "title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint",
      "authors": [
        "Qianli Ma",
        "Dongrui Liu",
        "Chen Qian",
        "Linfeng Zhang",
        "Jing Shao"
      ],
      "abstract": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: $\\textbf{neuron misidentification}$ due to simplistic parameter magnitude-based selection, and $\\textbf{cross-task neuron interference}$ during merging. To address these challenges, we propose $\\textbf{LED-Merging}$, a three-stage framework that $\\textbf{L}$ocates task-specific neurons via gradient-based attribution, dynamically $\\textbf{E}$lects critical neurons through multi-model importance fusion, and $\\textbf{D}$isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging effectively reduces harmful response rates, showing a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench, while simultaneously preserving 95\\% of utility performance, such as achieving 52.39\\% accuracy on GSM8K. LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs. Code is available at $\\href{https://github.com/MqLeet/LED-Merging}{GitHub}$.",
      "arxiv_url": "https://arxiv.org/abs/2502.16770",
      "pdf_url": "https://arxiv.org/pdf/2502.16770",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06806",
      "title": "Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification",
      "authors": [
        "Subhendu Khatuya",
        "Shashwat Naidu",
        "Saptarshi Ghosh",
        "Pawan Goyal",
        "Niloy Ganguly"
      ],
      "abstract": "The explosion of textual data has made manual document classification increasingly challenging. To address this, we introduce a robust, efficient domain-agnostic generative model framework for multi-label text classification. Instead of treating labels as mere atomic symbols, our approach utilizes predefined label descriptions and is trained to generate these descriptions based on the input text. During inference, the generated descriptions are matched to the pre-defined labels using a finetuned sentence transformer. We integrate this with a dual-objective loss function, combining cross-entropy loss and cosine similarity of the generated sentences with the predefined target descriptions, ensuring both semantic alignment and accuracy. Our proposed model LAGAMC stands out for its parameter efficiency and versatility across diverse datasets, making it well-suited for practical applications. We demonstrate the effectiveness of our proposed model by achieving new state-of-the-art performances across all evaluated datasets, surpassing several strong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in Macro-F1 compared to the closest baseline across all datasets.",
      "arxiv_url": "https://arxiv.org/abs/2506.06806",
      "pdf_url": "https://arxiv.org/pdf/2506.06806",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "060326641be6455b9129ec027570c04b69397300",
      "title": "M2RC-EVAL: Massively Multilingual Repository-level Code Completion Evaluation",
      "authors": [
        "Jiaheng Liu",
        "Ken Deng",
        "Congnan Liu",
        "Jian Yang",
        "Shukai Liu",
        "He Zhu",
        "Peng Zhao",
        "Linzheng Chai",
        "Yanan Wu",
        "Ke Jin",
        "Ge Zhang",
        "Z. Wang",
        "Guoan Zhang",
        "Yingshui Tan",
        "Bangyu Xiang",
        "Zhaoxiang Zhang",
        "Wenbo Su",
        "Bo Zheng"
      ],
      "abstract": "Repository-level code completion has drawn great attention in software engineering, and several benchmarks have been introduced. However, existing repository-level code completion benchmarks usually focus on a limited number of languages ( < 5), which cannot evaluate the general code intelligence abilities across different languages for existing code Large Language Models (LLMs). Besides, the existing benchmarks usually report overall average scores of different languages, where the fine-grained abilities in different completion scenarios are ignored. Therefore, to facilitate the research of code LLMs in multilingual scenarios, we propose a massively multilingual repository-level code completion benchmark covering 18 programming languages (called M 2 RC -E VAL ), and two types of fine-grained annotations (i.e., bucket-level and semantic-level ) on different completion scenarios are provided, where we obtain these annotations based on the parsed abstract syntax tree. Moreover, we also cu-rate a massively multilingual instruction corpora M 2 RC -I NSTRUCT dataset to improve the repository-level code completion abilities of existing code LLMs. Comprehensive experimental results demonstrate the effectiveness of our M 2 RC -E VAL and M 2 RC -I NSTRUCT .",
      "arxiv_url": "https://www.semanticscholar.org/paper/060326641be6455b9129ec027570c04b69397300",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16880",
      "title": "CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter",
      "authors": [
        "Yepeng Weng",
        "Dianwen Mei",
        "Huishi Qiu",
        "Xujie Chen",
        "Li Liu",
        "Jiang Tian",
        "Zhongchao Shi"
      ],
      "abstract": "Speculative decoding is a powerful technique that accelerates Large Language Model (LLM) inference by leveraging a lightweight speculative draft model. However, existing designs suffers in performance due to misalignment between training and inference. Recent methods have tried to solve this issue by adopting a multi-step training strategy, but the complex inputs of different training steps make it harder for the draft model to converge. To address this, we propose CORAL, a novel framework that improves both accuracy and efficiency in speculative drafting. CORAL introduces Cross-Step Representation Alignment, a method that enhances consistency across multiple training steps, significantly improving speculative drafting performance. Additionally, we identify the LM head as a major bottleneck in the inference speed of the draft model. We introduce a weight-grouping mechanism that selectively activates a subset of LM head parameters during inference, substantially reducing the latency of the draft model. We evaluate CORAL on three LLM families and three benchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming state-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that CORAL effectively mitigates training-inference misalignment and delivers significant speedup for modern LLMs with large vocabularies.",
      "arxiv_url": "https://arxiv.org/abs/2502.16880",
      "pdf_url": "https://arxiv.org/pdf/2502.16880",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11221",
      "title": "PlanGenLLMs: A Modern Survey of LLM Planning Capabilities",
      "authors": [
        "Hui Wei",
        "Zihao Zhang",
        "Shenghua He",
        "Tian Xia",
        "Shijia Pan",
        "Feihu Liu"
      ],
      "abstract": "LLMs have immense potential for generating plans, transforming an initial world state into a desired goal state. A large body of research has explored the use of LLMs for various planning tasks, from web navigation to travel planning and database querying. However, many of these systems are tailored to specific problems, making it challenging to compare them or determine the best approach for new tasks. There is also a lack of clear and consistent evaluation criteria. Our survey aims to offer a comprehensive overview of current LLM planners to fill this gap. It builds on foundational work by Kartam and Wilkins (1990) and examines six key performance criteria: completeness, executability, optimality, representation, generalization, and efficiency. For each, we provide a thorough analysis of representative works and highlight their strengths and weaknesses. Our paper also identifies crucial future directions, making it a valuable resource for both practitioners and newcomers interested in leveraging LLM planning to support agentic workflows.",
      "arxiv_url": "https://arxiv.org/abs/2502.11221",
      "pdf_url": "https://arxiv.org/pdf/2502.11221",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "065ef00342be3a717f32ae5f5ef7dd0f9497ea67",
      "title": "HELIOS: Harmonizing Early Fusion, Late Fusion, and LLM Reasoning for Multi-Granular Table-Text Retrieval",
      "authors": [
        "Sungho Park",
        "Joohyung Yun",
        "Jongwuk Lee",
        "Wook-Shin Han"
      ],
      "abstract": "Table-text retrieval aims to retrieve relevant tables and text to support open-domain question answering. Existing studies use either early or late fusion, but face limitations. Early fusion pre-aligns a table row with its associated passages, forming “stars,\" which often include irrelevant contexts and miss query-dependent relationships. Late fusion retrieves individual nodes, dynamically aligning them, but it risks missing relevant contexts. Both approaches also struggle with advanced reasoning tasks, such as column-wise aggregation and multi-hop reasoning. To address these issues, we pro-pose HELIOS , which combines the strengths of both approaches. First, the edge-based bipartite subgraph retrieval identifies finer-grained edges between table segments and passages, effectively avoiding the inclusion of irrelevant contexts. Then, the query-relevant node expansion identifies the most promising nodes, dynamically retrieving relevant edges to grow the bipartite subgraph, minimizing the risk of missing important contexts. Lastly, the star-based LLM refinement performs logical inference at the star graph level rather than the bipartite subgraph, supporting advanced reasoning tasks. Experimental results show that HELIOS outperforms state-of-the-art models with a significant improvement up to 42.6% and 39.9% in recall and nDCG, respectively, on the OTT-QA benchmark.",
      "arxiv_url": "https://www.semanticscholar.org/paper/065ef00342be3a717f32ae5f5ef7dd0f9497ea67",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03984",
      "title": "Around the World in 24 Hours: Probing LLM Knowledge of Time and Place",
      "authors": [
        "Carolin Holtermann",
        "Paul Röttger",
        "Anne Lauscher"
      ],
      "abstract": "Reasoning over time and space is essential for understanding our world. However, the abilities of language models in this area are largely unexplored as previous work has tested their abilities for logical reasoning in terms of time and space in isolation or only in simple or artificial environments. In this paper, we present the first evaluation of the ability of language models to jointly reason over time and space. To enable our analysis, we create GeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37 time zones. Using GeoTemp, we evaluate eight open chat models of three different model families for different combinations of temporal and geographic knowledge. We find that most models perform well on reasoning tasks involving only temporal knowledge and that overall performance improves with scale. However, performance remains constrained in tasks that require connecting temporal and geographical information. We do not find clear correlations of performance with specific geographic regions. Instead, we find a significant performance increase for location names with low model perplexity, suggesting their repeated occurrence during model training. We further demonstrate that their performance is heavily influenced by prompt formulation - a direct injection of geographical knowledge leads to performance gains, whereas, surprisingly, techniques like chain-of-thought prompting decrease performance on simpler tasks.",
      "arxiv_url": "https://arxiv.org/abs/2506.03984",
      "pdf_url": "https://arxiv.org/pdf/2506.03984",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.01264",
      "title": "ProgCo: Program Helps Self-Correction of Large Language Models",
      "authors": [
        "Xiaoshuai Song",
        "Yanan Wu",
        "Weixun Wang",
        "Jiaheng Liu",
        "Wenbo Su",
        "Bo Zheng"
      ],
      "abstract": "Self-Correction aims to enable large language models (LLMs) to self-verify and self-refine their initial responses without external feedback. However, LLMs often fail to effectively self-verify and generate correct feedback, further misleading refinement and leading to the failure of self-correction, especially in complex reasoning tasks. In this paper, we propose Program-driven Self-Correction (ProgCo). First, program-driven verification (ProgVe) achieves complex verification logic and extensive validation through self-generated, self-executing verification pseudo-programs. Then, program-driven refinement (ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement on both responses and verification programs to mitigate misleading of incorrect feedback in complex reasoning tasks. Experiments on three instruction-following and mathematical benchmarks indicate that ProgCo achieves effective self-correction, and can be further enhance performance when combined with real program tools. We release our code at https://github.com/songxiaoshuai/progco.",
      "arxiv_url": "https://arxiv.org/abs/2501.01264",
      "pdf_url": "https://arxiv.org/pdf/2501.01264",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.17663",
      "title": "Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States",
      "authors": [
        "Yang Xiao",
        "Jiashuo Wang",
        "Qiancheng Xu",
        "Changhe Song",
        "Chunpu Xu",
        "Yi Cheng",
        "Wenjie Li",
        "Pengfei Liu"
      ],
      "abstract": "As Large Language Models (LLMs) increasingly participate in human-AI interactions, evaluating their Theory of Mind (ToM) capabilities - particularly their ability to track dynamic mental states - becomes crucial. While existing benchmarks assess basic ToM abilities, they predominantly focus on static snapshots of mental states, overlooking the temporal evolution that characterizes real-world social interactions. We present \\textsc{DynToM}, a novel benchmark specifically designed to evaluate LLMs' ability to understand and track the temporal progression of mental states across interconnected scenarios. Through a systematic four-step framework, we generate 1,100 social contexts encompassing 5,500 scenarios and 78,100 questions, each validated for realism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs reveals that their average performance underperforms humans by 44.7\\%, with performance degrading significantly when tracking and reasoning about the shift of mental states. This performance gap highlights fundamental limitations in current LLMs' ability to model the dynamic nature of human mental states.",
      "arxiv_url": "https://arxiv.org/abs/2505.17663",
      "pdf_url": "https://arxiv.org/pdf/2505.17663",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19490",
      "title": "Automated CAD Modeling Sequence Generation from Text Descriptions via Transformer-Based Large Language Models",
      "authors": [
        "Jianxing Liao",
        "Junyan Xu",
        "Yatao Sun",
        "Maowen Tang",
        "Sicheng He",
        "Jingxian Liao",
        "Shui Yu",
        "Yun Li",
        "Hongguan Xiao"
      ],
      "abstract": "Designing complex computer-aided design (CAD) models is often time-consuming due to challenges such as computational inefficiency and the difficulty of generating precise models. We propose a novel language-guided framework for industrial design automation to address these issues, integrating large language models (LLMs) with computer-automated design (CAutoD).Through this framework, CAD models are automatically generated from parameters and appearance descriptions, supporting the automation of design tasks during the detailed CAD design phase. Our approach introduces three key innovations: (1) a semi-automated data annotation pipeline that leverages LLMs and vision-language large models (VLLMs) to generate high-quality parameters and appearance descriptions; (2) a Transformer-based CAD generator (TCADGen) that predicts modeling sequences via dual-channel feature aggregation; (3) an enhanced CAD modeling generation model, called CADLLM, that is designed to refine the generated sequences by incorporating the confidence scores from TCADGen. Experimental results demonstrate that the proposed approach outperforms traditional methods in both accuracy and efficiency, providing a powerful tool for automating industrial workflows and generating complex CAD models from textual prompts. The code is available at https://jianxliao.github.io/cadllm-page/",
      "arxiv_url": "https://arxiv.org/abs/2505.19490",
      "pdf_url": "https://arxiv.org/pdf/2505.19490",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.09743",
      "title": "Partial Colexifications Improve Concept Embeddings",
      "authors": [
        "Arne Rubehn",
        "Johann-Mattis List"
      ],
      "abstract": "While the embedding of words has revolutionized the field of Natural Language Processing, the embedding of concepts has received much less attention so far. A dense and meaningful representation of concepts, however, could prove useful for several tasks in computational linguistics, especially those involving cross-linguistic data or sparse data from low resource languages. First methods that have been proposed so far embed concepts from automatically constructed colexification networks. While these approaches depart from automatically inferred polysemies, attested across a larger number of languages, they are restricted to the word level, ignoring lexical relations that would only hold for parts of the words in a given language. Building on recently introduced methods for the inference of partial colexifications, we show how they can be used to improve concept embeddings in meaningful ways. The learned embeddings are evaluated against lexical similarity ratings, recorded instances of semantic shift, and word association data. We show that in all evaluation tasks, the inclusion of partial colexifications lead to improved concept representations and better results. Our results further show that the learned embeddings are able to capture and represent different semantic relationships between concepts.",
      "arxiv_url": "https://arxiv.org/abs/2502.09743",
      "pdf_url": "https://arxiv.org/pdf/2502.09743",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11340",
      "title": "DecompileBench: A Comprehensive Benchmark for Evaluating Decompilers in Real-World Scenarios",
      "authors": [
        "Zeyu Gao",
        "Yuxin Cui",
        "Hao Wang",
        "Siliang Qin",
        "Yuan-Yuan Wang",
        "Bolun Zhang",
        "Chao Zhang"
      ],
      "abstract": "Decompilers are fundamental tools for critical security tasks, from vulnerability discovery to malware analysis, yet their evaluation remains fragmented. Existing approaches primarily focus on syntactic correctness through synthetic micro-benchmarks or subjective human ratings, failing to address real-world requirements for semantic fidelity and analyst usability. We present DecompileBench, the first comprehensive framework that enables effective evaluation of decompilers in reverse engineering workflows through three key components: \\textit{real-world function extraction} (comprising 23,400 functions from 130 real-world programs), \\textit{runtime-aware validation}, and \\textit{automated human-centric assessment} using LLM-as-Judge to quantify the effectiveness of decompilers in reverse engineering workflows. Through a systematic comparison between six industrial-strength decompilers and six recent LLM-powered approaches, we demonstrate that LLM-based methods surpass commercial tools in code understandability despite 52.2% lower functionality correctness. These findings highlight the potential of LLM-based approaches to transform human-centric reverse engineering. We open source \\href{https://github.com/Jennieett/DecompileBench}{DecompileBench} to provide a framework to advance research on decompilers and assist security experts in making informed tool selections based on their specific requirements.",
      "arxiv_url": "https://arxiv.org/abs/2505.11340",
      "pdf_url": "https://arxiv.org/pdf/2505.11340",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04688",
      "title": "MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models",
      "authors": [
        "Gio Paik",
        "Geewook Kim",
        "Jinbae Im"
      ],
      "abstract": "This paper introduces MMRefine, a MultiModal Refinement benchmark designed to evaluate the error refinement capabilities of Multimodal Large Language Models (MLLMs). As the emphasis shifts toward enhancing reasoning during inference, MMRefine provides a framework that evaluates MLLMs' abilities to detect and correct errors across six distinct scenarios beyond just comparing final accuracy before and after refinement. Furthermore, the benchmark analyzes the refinement performance by categorizing errors into six error types. Experiments with various open and closed MLLMs reveal bottlenecks and factors impeding refinement performance, highlighting areas for improvement in effective reasoning enhancement. Our code and dataset are publicly available at https://github.com/naver-ai/MMRefine.",
      "arxiv_url": "https://arxiv.org/abs/2506.04688",
      "pdf_url": "https://arxiv.org/pdf/2506.04688",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "07bfaa2404441550654a56791f38478d3c6a354f",
      "title": "Task Knowledge Injection via Interpolations and Reinstatement for Large Language Model Generalization",
      "authors": [
        "Yukun Zhao",
        "Lingyong Yan",
        "Zhenyang Li",
        "Shuaiqiang Wang",
        "Zhumin Chen",
        "Zhaochun Ren",
        "Dawei Yin"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/07bfaa2404441550654a56791f38478d3c6a354f",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12953",
      "title": "Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text",
      "authors": [
        "Andrei Jarca",
        "Florinel-Alin Croitoru",
        "R. Ionescu"
      ],
      "abstract": "Masked language modeling has become a widely adopted unsupervised technique to pre-train large language models (LLMs). However, the process of selecting tokens for masking is random, and the percentage of masked tokens is typically fixed for the entire training process. In this paper, we propose to adjust the masking ratio and to decide which tokens to mask based on a novel task-informed anti-curriculum learning scheme. First, we harness task-specific knowledge about useful and harmful tokens in order to determine which tokens to mask. Second, we propose a cyclic decaying masking ratio, which corresponds to an anti-curriculum schedule (from hard to easy). We exemplify our novel task-informed anti-curriculum by masking (TIACBM) approach across three diverse downstream tasks: sentiment analysis, text classification by topic, and authorship attribution. Our findings suggest that TIACBM enhances the ability of the model to focus on key task-relevant features, contributing to statistically significant performance gains across tasks. We release our code at https://github.com/JarcaAndrei/TIACBM.",
      "arxiv_url": "https://arxiv.org/abs/2502.12953",
      "pdf_url": "https://arxiv.org/pdf/2502.12953",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.07849",
      "title": "The Invisible Hand: Unveiling Provider Bias in Large Language Models for Code Generation",
      "authors": [
        "Xiaoyu Zhang",
        "Juan Zhai",
        "Shiqing Ma",
        "Qingshuang Bao",
        "Weipeng Jiang",
        "Qian Wang",
        "Chao Shen",
        "Yang Liu"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as the new recommendation engines, surpassing traditional methods in both capability and scope, particularly in code generation. In this paper, we reveal a novel provider bias in LLMs: without explicit directives, these models show systematic preferences for services from specific providers in their recommendations (e.g., favoring Google Cloud over Microsoft Azure). To systematically investigate this bias, we develop an automated pipeline to construct the dataset, incorporating 6 distinct coding task categories and 30 real-world application scenarios. Leveraging this dataset, we conduct the first comprehensive empirical study of provider bias in LLM code generation across seven state-of-the-art LLMs, utilizing approximately 500 million tokens (equivalent to $5,000+ in computational costs). Our findings reveal that LLMs exhibit significant provider preferences, predominantly favoring services from Google and Amazon, and can autonomously modify input code to incorporate their preferred providers without users'requests. Such a bias holds far-reaching implications for market dynamics and societal equilibrium, potentially contributing to digital monopolies. It may also deceive users and violate their expectations, leading to various consequences. We call on the academic community to recognize this emerging issue and develop effective evaluation and mitigation methods to uphold AI security and fairness.",
      "arxiv_url": "https://arxiv.org/abs/2501.07849",
      "pdf_url": "https://arxiv.org/pdf/2501.07849",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.16655",
      "title": "Accelerating Antibiotic Discovery with Large Language Models and Knowledge Graphs",
      "authors": [
        "Maxime Delmas",
        "Magdalena Wysocka",
        "D. Gusicuma",
        "André Freitas"
      ],
      "abstract": "The discovery of novel antibiotics is critical to address the growing antimicrobial resistance (AMR). However, pharmaceutical industries face high costs (over $1 billion), long timelines, and a high failure rate, worsened by the rediscovery of known compounds. We propose an LLM-based pipeline that acts as an alarm system, detecting prior evidence of antibiotic activity to prevent costly rediscoveries. The system integrates organism and chemical literature into a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling, and multi-level evidence classification. We tested the pipeline on a private list of 73 potential antibiotic-producing organisms, disclosing 12 negative hits for evaluation. The results highlight the effectiveness of the pipeline for evidence reviewing, reducing false negatives, and accelerating decision-making. The KG for negative hits and the user interface for interactive exploration will be made publicly available.",
      "arxiv_url": "https://arxiv.org/abs/2503.16655",
      "pdf_url": "https://arxiv.org/pdf/2503.16655",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.14302",
      "title": "Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent",
      "authors": [
        "Xueyang Feng",
        "Jingsen Zhang",
        "Jiakai Tang",
        "Wei Li",
        "Guohao Cai",
        "Xu Chen",
        "Quanyu Dai",
        "Yue Zhu",
        "Zhenhua Dong"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have significantly propelled the development of Conversational Recommendation Agents (CRAs). However, these agents often generate short-sighted responses that fail to sustain user guidance and meet expectations. Although preference optimization has proven effective in aligning LLMs with user expectations, it remains costly and performs poorly in multi-turn dialogue. To address this challenge, we introduce a novel multi-turn preference optimization (MTPO) paradigm ECPO, which leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction throughout multi-turn dialogues, uncovering the underlying causes of dissatisfaction. These causes can be utilized to support targeted optimization of unsatisfactory responses, thereby achieving turn-level preference optimization. ECPO ingeniously eliminates the significant sampling overhead of existing MTPO methods while ensuring the optimization process drives meaningful improvements. To support ECPO, we introduce an LLM-based user simulator, AILO, to simulate user feedback and perform expectation confirmation during conversational recommendations. Experimental results show that ECPO significantly enhances CRA's interaction capabilities, delivering notable improvements in both efficiency and effectiveness over existing MTPO methods.",
      "arxiv_url": "https://arxiv.org/abs/2506.14302",
      "pdf_url": "https://arxiv.org/pdf/2506.14302",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "080bf0e02e72894a93175583bfc318031dc6015c",
      "title": "Logic-Regularized Verifier Elicits Reasoning from LLMs",
      "authors": [
        "Xinyu Wang",
        "Changzhi Sun",
        "Lian Cheng",
        "Yuanbin Wu",
        "Dell Zhang",
        "Xiaoling Wang",
        "Xuelong Li"
      ],
      "abstract": "Verifiers are crucial components for enhancing modern LLMs’ reasoning capability. Typical verifiers require resource-intensive supervised dataset construction, which is costly and faces limitations in data diversity. In this paper, we propose L O V ER , an unsupervised verifier regularized by logical rules. L O V ER treats the verifier as a binary latent variable, utilizing internal activations and enforcing three logical constraints on multiple reasoning paths: negation consistency, intra-group consistency, and inter-group consistency (grouped by the final answer). By incorporating logical rules as priors, L O V ER can leverage unlabeled examples and is directly compatible with any off-the-shelf LLMs. Experiments on 10 datasets demonstrate that L O V ER significantly outperforms unsupervised baselines, achieving performance comparable to the supervised verifier (reaching its 95% level on average). The source code is publicly available at https://github.com/ wangxinyufighting/llm-lover.",
      "arxiv_url": "https://www.semanticscholar.org/paper/080bf0e02e72894a93175583bfc318031dc6015c",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24244",
      "title": "Mamba Knockout for Unraveling Factual Information Flow",
      "authors": [
        "Nir Endy",
        "I. Grosbard",
        "Yuval Ran-milo",
        "Yonatan Slutzky",
        "Itay Tshuva",
        "Raja Giryes"
      ],
      "abstract": "This paper investigates the flow of factual information in Mamba State-Space Model (SSM)-based language models. We rely on theoretical and empirical connections to Transformer-based architectures and their attention mechanisms. Exploiting this relationship, we adapt attentional interpretability techniques originally developed for Transformers--specifically, the Attention Knockout methodology--to both Mamba-1 and Mamba-2. Using them we trace how information is transmitted and localized across tokens and layers, revealing patterns of subject-token information emergence and layer-wise dynamics. Notably, some phenomena vary between mamba models and Transformer based models, while others appear universally across all models inspected--hinting that these may be inherent to LLMs in general. By further leveraging Mamba's structured factorization, we disentangle how distinct\"features\"either enable token-to-token information exchange or enrich individual tokens, thus offering a unified lens to understand Mamba internal operations.",
      "arxiv_url": "https://arxiv.org/abs/2505.24244",
      "pdf_url": "https://arxiv.org/pdf/2505.24244",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11133",
      "title": "MasRouter: Learning to Route LLMs for Multi-Agent Systems",
      "authors": [
        "Yanwei Yue",
        "Gui-Min Zhang",
        "Boyang Liu",
        "Guancheng Wan",
        "Kun Wang",
        "Dawei Cheng",
        "Yiyan Qi"
      ],
      "abstract": "Multi-agent systems (MAS) powered by Large Language Models (LLMs) have been demonstrated to push the boundaries of LLM capabilities, yet they often incur significant costs and face challenges in dynamic LLM selection. Current LLM routing methods effectively reduce overhead in single-agent scenarios by customizing LLM selection for each query, but they overlook the critical decisions regarding collaboration modes and agent roles in MAS. In response to this challenge, we first introduce the problem of Multi-Agent System Routing (MASR), which integrates all components of MAS into a unified routing framework. Toward this goal, we propose MasRouter, the first high-performing, cost-effective, and inductive MASR solution. MasRouter employs collaboration mode determination, role allocation, and LLM routing through a cascaded controller network, progressively constructing a MAS that balances effectiveness and efficiency. Extensive experiments demonstrate that MasRouter is (1) high-performing, achieving a $1.8\\%\\sim8.2\\%$ improvement over the state-of-the-art method on MBPP; (2) economical, reducing overhead by up to $52.07\\%$ compared to SOTA methods on HumanEval; and (3) plug-and-play, seamlessly integrating with mainstream MAS frameworks, reducing overhead by $17.21\\%\\sim28.17\\%$ via customized routing. The code is available at https://github.com/yanweiyue/masrouter.",
      "arxiv_url": "https://arxiv.org/abs/2502.11133",
      "pdf_url": "https://arxiv.org/pdf/2502.11133",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24714",
      "title": "FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation",
      "authors": [
        "Junyu Luo",
        "Zhizhuo Kou",
        "Liming Yang",
        "Xiao Luo",
        "Jinsheng Huang",
        "Zhiping Xiao",
        "Jingshu Peng",
        "Chengzhong Liu",
        "Jiaming Ji",
        "Xuanzhe Liu",
        "Sirui Han",
        "Ming Zhang",
        "Yike Guo"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, in the financial domain, there is a notable lack of effective and specialized multimodal evaluation datasets. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20 annotators and carefully designed validation mechanisms. Additionally, we develop FinScore, an evaluation system incorporating hallucination penalties and multi-dimensional capability assessment to provide an unbiased evaluation. Extensive experimental results demonstrate that even state-of-the-art models like GPT-4o exhibit unsatisfactory performance on FinMME, highlighting its challenging nature. The benchmark exhibits high robustness with prediction variations under different prompts remaining below 1%, demonstrating superior reliability compared to existing datasets. Our dataset and evaluation protocol are available at https://huggingface.co/datasets/luojunyu/FinMME and https://github.com/luo-junyu/FinMME.",
      "arxiv_url": "https://arxiv.org/abs/2505.24714",
      "pdf_url": "https://arxiv.org/pdf/2505.24714",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2509.18719",
      "title": "LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection",
      "authors": [
        "Bo Qu",
        "Zhurong Wang",
        "Daisuke Yagi",
        "Zach Xu",
        "Yang Zhao",
        "Yinan Shan",
        "Frank Zahradnik"
      ],
      "abstract": "This paper presents a novel approach to e-commerce payment fraud detection by integrating reinforcement learning (RL) with Large Language Models (LLMs). By framing transaction risk as a multi-step Markov Decision Process (MDP), RL optimizes risk detection across multiple payment stages. Crafting effective reward functions, essential for RL model success, typically requires significant human expertise due to the complexity and variability in design. LLMs, with their advanced reasoning and coding capabilities, are well-suited to refine these functions, offering improvements over traditional methods. Our approach leverages LLMs to iteratively enhance reward functions, achieving better fraud detection accuracy and demonstrating zero-shot capability. Experiments with real-world data confirm the effectiveness, robustness, and resilience of our LLM-enhanced RL framework through long-term evaluations, underscoring the potential of LLMs in advancing industrial RL applications.",
      "arxiv_url": "https://arxiv.org/abs/2509.18719",
      "pdf_url": "https://arxiv.org/pdf/2509.18719",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-09-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19155",
      "title": "Sparse-to-Dense: A Free Lunch for Lossless Acceleration of Video Understanding in LLMs",
      "authors": [
        "Xuan Zhang",
        "Cunxiao Du",
        "Sicheng Yu",
        "Jiawei Wu",
        "Fengzhuo Zhang",
        "Wei Gao",
        "Qian Liu"
      ],
      "abstract": "Due to the auto-regressive nature of current video large language models (Video-LLMs), the inference latency increases as the input sequence length grows, posing challenges for the efficient processing of video sequences that are usually very long. We observe that during decoding, the attention scores of most tokens in Video-LLMs tend to be sparse and concentrated, with only certain tokens requiring comprehensive full attention. Based on this insight, we introduce Sparse-to-Dense (StD), a novel decoding strategy that integrates two distinct modules: one leveraging sparse top-K attention and the other employing dense full attention. These modules collaborate to accelerate Video-LLMs without loss. The fast (sparse) model speculatively decodes multiple tokens, while the slow (dense) model verifies them in parallel. StD is a tuning-free, plug-and-play solution that achieves up to a 1.94$\\times$ walltime speedup in video processing. It maintains model performance while enabling a seamless transition from a standard Video-LLM to a sparse Video-LLM with minimal code modifications.",
      "arxiv_url": "https://arxiv.org/abs/2505.19155",
      "pdf_url": "https://arxiv.org/pdf/2505.19155",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.09902",
      "title": "PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants",
      "authors": [
        "Zheng Zhao",
        "Clara Vania",
        "Subhradeep Kayal",
        "Naila Khan",
        "Shay B. Cohen",
        "Emine Yilmaz"
      ],
      "abstract": "Large language models (LLMs) have advanced conversational AI assistants. However, systematically evaluating how well these assistants apply personalization--adapting to individual user preferences while completing tasks--remains challenging. Existing personalization benchmarks focus on chit-chat, non-conversational tasks, or narrow domains, failing to capture the complexities of personalized task-oriented assistance. To address this, we introduce PersonaLens, a comprehensive benchmark for evaluating personalization in task-oriented AI assistants. Our benchmark features diverse user profiles equipped with rich preferences and interaction histories, along with two specialized LLM-based agents: a user agent that engages in realistic task-oriented dialogues with AI assistants, and a judge agent that employs the LLM-as-a-Judge paradigm to assess personalization, response quality, and task success. Through extensive experiments with current LLM assistants across diverse tasks, we reveal significant variability in their personalization capabilities, providing crucial insights for advancing conversational AI systems.",
      "arxiv_url": "https://arxiv.org/abs/2506.09902",
      "pdf_url": "https://arxiv.org/pdf/2506.09902",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-06-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15572",
      "title": "DReSD: Dense Retrieval for Speculative Decoding",
      "authors": [
        "Milan Gritta",
        "Huiyin Xue",
        "Gerasimos Lampouras"
      ],
      "abstract": "Speculative decoding (SD) accelerates Large Language Model (LLM) generation by using an efficient draft model to propose the next few tokens, which are verified by the LLM in a single forward call, reducing latency while preserving its outputs. We focus on retrieval-based SD where the draft model retrieves the next tokens from a non-parametric datastore. Sparse retrieval (REST), which operates on the surface form of strings, is currently the dominant paradigm due to its simplicity and scalability. However, its effectiveness is limited due to the usage of short contexts and exact string matching. Instead, we introduce Dense Retrieval for Speculative Decoding (DReSD), a novel framework that uses approximate nearest neighbour search with contextualised token embeddings to retrieve the most semantically relevant token sequences for SD. Extensive experiments show that DReSD achieves (on average) 87% higher acceptance rates, 65% longer accepted tokens and 19% faster generation speeds compared to sparse retrieval (REST).",
      "arxiv_url": "https://arxiv.org/abs/2502.15572",
      "pdf_url": "https://arxiv.org/pdf/2502.15572",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "08f788b96fcec0026c94169f3757d2aeecda394c",
      "title": "L-CiteEval: A Suite for Evaluating Fidelity of Long-context Models",
      "authors": [
        "Zecheng Tang",
        "Keyan Zhou",
        "Juntao Li",
        "Baibei Ji",
        "Jianye Hou",
        "Min Zhang"
      ],
      "abstract": "Long-context models (LCMs) have witnessed remarkable advancements in recent years, facilitating real-world tasks like long-document QA. The success of LCMs is founded on the hypothesis that the model demonstrates strong fidelity , enabling it to respond based on the provided long context rather than relying solely on the intrinsic knowledge acquired during pre-training. Yet, in this paper, we find that open-sourced LCMs are not as faithful as expected. We introduce L-CiteEval , an out-of-the-box suite that can assess both generation quality and fidelity in long-context understanding tasks. It covers 11 tasks with context lengths ranging from 8K to 48K and a corresponding automatic evaluation pipeline. Evaluation of 11 cutting-edge closed-source and open-source LCMs indicates that, while there are minor differences in their generation, open-source models significantly lag behind closed-source counterparts in terms of fidelity. Furthermore, we analyze the benefits of citation generation for LCMs from both the perspective of explicit model output and the internal attention mechanism 1 .",
      "arxiv_url": "https://www.semanticscholar.org/paper/08f788b96fcec0026c94169f3757d2aeecda394c",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04047",
      "title": "On Support Samples of Next Word Prediction",
      "authors": [
        "Yuqian Li",
        "Yupei Du",
        "Yufang Liu",
        "Feifei Feng",
        "Mou Xiao Feng",
        "Yuanbin Wu"
      ],
      "abstract": "Language models excel in various tasks by making complex decisions, yet understanding the rationale behind these decisions remains a challenge. This paper investigates \\emph{data-centric interpretability} in language models, focusing on the next-word prediction task. Using representer theorem, we identify two types of \\emph{support samples}-those that either promote or deter specific predictions. Our findings reveal that being a support sample is an intrinsic property, predictable even before training begins. Additionally, while non-support samples are less influential in direct predictions, they play a critical role in preventing overfitting and shaping generalization and representation learning. Notably, the importance of non-support samples increases in deeper layers, suggesting their significant role in intermediate representation formation. These insights shed light on the interplay between data and model decisions, offering a new dimension to understanding language model behavior and interpretability.",
      "arxiv_url": "https://arxiv.org/abs/2506.04047",
      "pdf_url": "https://arxiv.org/pdf/2506.04047",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "0934347718b26f2c2f1bd123a3f5fc6719e653e8",
      "title": "From Evasion to Concealment: Stealthy Knowledge Unlearning for LLMs",
      "authors": [
        "Tianle Gu",
        "Kexin Huang",
        "Ruilin Luo",
        "Yuanqi Yao",
        "Xiuying Chen",
        "Yujiu Yang",
        "Yan Teng",
        "Yingchun Wang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/0934347718b26f2c2f1bd123a3f5fc6719e653e8",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "0939030d4ad669ebeb91b5ab22418bc1f025744b",
      "title": "Metagent-P: A Neuro-Symbolic Planning Agent with Metacognition for Open Worlds",
      "authors": [
        "Yanfang Zhou",
        "Yuntao Liu",
        "Xiaodong Li",
        "Yongqiang Zhao",
        "Xintong Wang",
        "Jinlong Tian",
        "Zhenyu Li",
        "Xinhai Xu"
      ],
      "abstract": "The challenge of developing agents capable of open-world planning remains fundamental to artificial general intelligence (AGI). While large language models (LLMs) have made progress with their vast world knowledge, their limitations in perception, memory, and reliable reasoning still hinder LLM-based agents from achieving human-level performance in long-term tasks. Drawing inspiration from human cognitive-metacognitive collaboration, we propose Metagent-P , integrating the world knowledge of LLMs, the symbolic reasoning capabilities of cognitive architectures, and the self-reflection characteristic of metacognition to construct a “planning-verification-execution-reflection” framework. Metagent-P improves experience utilization through multimodal memory integration. It uses a neural-symbolic hierarchical representation structure to ensure the plan’s reasoning correctness in advance. Finally, it actively adapts the agent to dynamic environments through monitoring, evaluation, and regulation mechanisms. Experimental results show Metagent-P significantly outperforms current state-of-the-art methods in Minecraft. In long-term tasks, Metagent-P reduces the average replanning counts by 34% and exceeds the average human success rate by 18.96% . Additionally, Metagent-P also demonstrates self-evolution through step-by-step open-world exploration.",
      "arxiv_url": "https://www.semanticscholar.org/paper/0939030d4ad669ebeb91b5ab22418bc1f025744b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.08524",
      "title": "Position-Aware Depth Decay Decoding (D3): Boosting Large Language Model Inference Efficiency",
      "authors": [
        "Siqi Fan",
        "Xuezhi Fang",
        "Xingrun Xing",
        "Peng Han",
        "Shuo Shang",
        "Yequan Wang"
      ],
      "abstract": "Due to the large number of parameters, the inference phase of Large Language Models (LLMs) is resource-intensive. Unlike traditional model compression, which needs retraining, recent dynamic computation methods show that not all components are required for inference, enabling a training-free pipeline. In this paper, we focus on the dynamic depth of LLM generation. A token-position aware layer skipping framework is proposed to save 1.5x times operations efficiently while maintaining performance. We first observed that tokens predicted later have lower perplexity and thus require less computation. Then, we propose a training-free algorithm called Position-Aware Depth Decay Decoding ($D^3$), which leverages a power-law decay function, $\\left\\lfloor L \\times (\\alpha^i) \\right\\rfloor$, to determine the number of layers to retain when generating token $T_i$. Remarkably, without any retraining, the $D^3$ achieves success across a wide range of generation tasks for the first time. Experiments on large language models (\\ie the Llama) with $7 \\sim 70$ billion parameters show that $D^3$ can achieve an average 1.5x speedup compared with the full-inference pipeline while maintaining comparable performance with nearly no performance drop ($<1\\%$) on the GSM8K and BBH benchmarks.",
      "arxiv_url": "https://arxiv.org/abs/2503.08524",
      "pdf_url": "https://arxiv.org/pdf/2503.08524",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.05317",
      "title": "On Synthesizing Data for Context Attribution in Question Answering",
      "authors": [
        "Gorjan Radevski",
        "Kiril Gashteovski",
        "Shahbaz Syed",
        "Christopher Malon",
        "Sebastien Nicolas",
        "Chia-Chien Hung",
        "Timo Sztyler",
        "Verena Heusser",
        "Wiem Ben Rim",
        "Masafumi Enomoto",
        "Kunihiro Takeoka",
        "M. Oyamada",
        "Goran Glavavs",
        "Carolin Lawrence"
      ],
      "abstract": "Question Answering (QA) accounts for a significant portion of LLM usage\"in the wild\". However, LLMs sometimes produce false or misleading responses, also known as\"hallucinations\". Therefore, grounding the generated answers in contextually provided information -- i.e., providing evidence for the generated text -- is paramount for LLMs' trustworthiness. Providing this information is the task of context attribution. In this paper, we systematically study LLM-based approaches for this task, namely we investigate (i) zero-shot inference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic data generated by larger LLMs. Our key contribution is SynQA: a novel generative strategy for synthesizing context attribution data. Given selected context sentences, an LLM generates QA pairs that are supported by these sentences. This leverages LLMs' natural strengths in text generation while ensuring clear attribution paths in the synthetic training data. We show that the attribution data synthesized via SynQA is highly effective for fine-tuning small LMs for context attribution in different QA tasks and domains. Finally, with a user study, we validate the usefulness of small LMs (fine-tuned on synthetic data from SynQA) in context attribution for QA.",
      "arxiv_url": "https://arxiv.org/abs/2504.05317",
      "pdf_url": "https://arxiv.org/pdf/2504.05317",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01916",
      "title": "DNCASR: End-to-End Training for Speaker-Attributed ASR",
      "authors": [
        "Xianrui Zheng",
        "C. Zhang",
        "P. Woodland"
      ],
      "abstract": "This paper introduces DNCASR, a novel end-to-end trainable system designed for joint neural speaker clustering and automatic speech recognition (ASR), enabling speaker-attributed transcription of long multi-party meetings. DNCASR uses two separate encoders to independently encode global speaker characteristics and local waveform information, along with two linked decoders to generate speaker-attributed transcriptions. The use of linked decoders allows the entire system to be jointly trained under a unified loss function. By employing a serialised training approach, DNCASR effectively addresses overlapping speech in real-world meetings, where the link improves the prediction of speaker indices in overlapping segments. Experiments on the AMI-MDM meeting corpus demonstrate that the jointly trained DNCASR outperforms a parallel system that does not have links between the speaker and ASR decoders. Using cpWER to measure the speaker-attributed word error rate, DNCASR achieves a 9.0% relative reduction on the AMI-MDM Eval set.",
      "arxiv_url": "https://arxiv.org/abs/2506.01916",
      "pdf_url": "https://arxiv.org/pdf/2506.01916",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "09f90a7fe63eaff68292ad463838c4ba13ea104c",
      "title": "NBDESCRIB: A Dataset for Text Description Generation from Tables and Code in Jupyter Notebooks with Guidelines",
      "authors": [
        "Xuye Liu",
        "Tengfei Ma",
        "Yimu Wang",
        "Fengjie Wang",
        "Jian Zhao",
        "Sherry Lin",
        "Winthrop F. Gillis",
        "Caleb Weinreb",
        "Ayman Zeine",
        "Samuel C Jones",
        "Emma M Robinson",
        "J. Markowitz",
        "Sandeep Robert",
        "Shuai Lu",
        "Daya Guo",
        "Shuo Ren",
        "Junjie Huang",
        "Alexey Svyatkovskiy",
        "Ambrosio Blanco",
        "Colin B. Clement",
        "Dawn Drain",
        "Daxin Jiang",
        "Duyu Tang",
        "Ge Li",
        "Li-dong Zhou",
        "Linjun Shou",
        "Long Zhou",
        "Michele Tu-fano",
        "Ming Gong",
        "Ming Zhou",
        "Nan Duan",
        "Neel Sun-daresan",
        "Shao Kun Deng",
        "Dehai Min",
        "Nan Hu",
        "Rihui Jin",
        "Nuo Lin",
        "Jiaoyan Chen",
        "Yongrui Chen",
        "Yu Li",
        "Guilin Qi",
        "Michael J. Muller",
        "A. Wang",
        "Steven I. Ross",
        "Justin D. Weisz",
        "Mayank Agarwal",
        "Kartik Tala-madupula",
        "Stephanie Houde",
        "Fernando Martinez",
        "Linyong Nan",
        "Chiachun Hsieh",
        "Ziming Mao",
        "Xi Victoria",
        "Neha Lin",
        "Rui Verma",
        "Wojciech Zhang",
        "Kry ´ sci ´ nski Hailey",
        "Riley Schoelkopf",
        "Kong"
      ],
      "abstract": "Generating cell-level descriptions for Jupyter Notebooks, which is a major resource consisting of codes, tables, and descriptions, has been attracting increasing research attention. However, existing methods for Jupyter Notebooks mostly focus on generating descriptions from code snippets or table outputs independently. On the other side, descriptions should be personalized as users have different purposes in different scenarios while previous work ignored this situation during description generation. In this work, we formulate a new task, personalized description generation with code, tables, and user-written guidelines in Jupyter Note-books. To evaluate this new task, we collect and propose a benchmark, namely NBDESCRIB, containing code, tables, and user-written guidelines as inputs and personalized descriptions as targets. Extensive experiments show that while existing models of text generation are able to generate fluent and readable descriptions, they still struggle to produce factually correct descriptions without user-written guidelines. CodeT5 achieved the highest scores in Orientation (1.27) and Correctness (-0.43) among foundation models in human evaluation, while the ground truth scored higher in Orientation (1.45) and Correctness (1.19). Common error patterns involve misalignment with guidelines, incorrect variable values, omission of important code information, and reasoning errors. Moreover, ablation studies show that adding guidelines significantly enhances performance. both qualitatively and quantitatively.",
      "arxiv_url": "https://www.semanticscholar.org/paper/09f90a7fe63eaff68292ad463838c4ba13ea104c",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Personalization"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18001",
      "title": "Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning",
      "authors": [
        "Xinghao Chen",
        "Zhijing Sun",
        "Wenjin Guo",
        "Miaoran Zhang",
        "Yanjun Chen",
        "Yirong Sun",
        "Hui Su",
        "Yijie Pan",
        "Dietrich Klakow",
        "Wenjie Li",
        "Xiaoyu Shen"
      ],
      "abstract": "Large Language Models (LLMs) excel in reasoning tasks through Chain-of-Thought (CoT) prompting. However, CoT prompting greatly increases computational demands, which has prompted growing interest in distilling CoT capabilities into Small Language Models (SLMs). This study systematically examines the factors influencing CoT distillation, including the choice of granularity, format and teacher model. Through experiments involving four teacher models and seven student models across seven mathematical and commonsense reasoning datasets, we uncover three key findings: (1) Unlike LLMs, SLMs exhibit a non-monotonic relationship with granularity, with stronger models benefiting from finer-grained reasoning and weaker models performing better with simpler CoT supervision; (2) CoT format significantly impacts LLMs but has minimal effect on SLMs, likely due to their reliance on supervised fine-tuning rather than pretraining preferences; (3) Stronger teacher models do NOT always produce better student models, as diversity and complexity in CoT supervision can outweigh accuracy alone. These findings emphasize the need to tailor CoT strategies to specific student model, offering actionable insights for optimizing CoT distillation in SLMs. The code and datasets are available at https://github.com/EIT-NLP/Distilling-CoT-Reasoning.",
      "arxiv_url": "https://arxiv.org/abs/2502.18001",
      "pdf_url": "https://arxiv.org/pdf/2502.18001",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "0a295555faaa8b9dad7e37bc049466b763854fc4",
      "title": "Sheep's Skin, Wolf's Deeds: Are LLMs Ready for Metaphorical Implicit Hate Speech?",
      "authors": [
        "Jingjie Zeng",
        "Liang Yang",
        "Zekun Wang",
        "Yuanyuan Sun",
        "Hongfei Lin"
      ],
      "abstract": "Implicit hate speech has become a significant challenge for online platforms, as it often avoids detection by large language models (LLMs) due to its indirectly expressed hateful intent. This study identifies the limitations of LLMs in detecting implicit hate speech, particularly when disguised as seemingly harmless expressions in a rhetorical device. To address this challenge, we employ a Jailbreaking strategy and Energy-based Constrained Decoding techniques, and design a small model for measuring the energy of metaphorical rhetoric. This approach can lead to LLMs generating metaphorical implicit hate speech. Our research reveals that advanced LLMs, like GPT-4o, frequently misinterpret metaphorical implicit hate speech, and fail to prevent its propagation effectively. Even specialized models, like ShieldGemma and LlamaGuard, demonstrate inadequacies in blocking such content, often misclassifying it as harmless speech. This work points out the vulnerability of current LLMs to implicit hate speech, and emphasizes the improvements to address hate speech threats better.",
      "arxiv_url": "https://www.semanticscholar.org/paper/0a295555faaa8b9dad7e37bc049466b763854fc4",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.13816",
      "title": "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations",
      "authors": [
        "Chenghao Xiao",
        "Hou Pong Chan",
        "Hao Zhang",
        "Mahani Aljunied",
        "Li Bing",
        "N. A. Moubayed",
        "Yu Rong"
      ],
      "abstract": "While understanding the knowledge boundaries of LLMs is crucial to prevent hallucination, research on the knowledge boundaries of LLMs has predominantly focused on English. In this work, we present the first study to analyze how LLMs recognize knowledge boundaries across different languages by probing their internal representations when processing known and unknown questions in multiple languages. Our empirical studies reveal three key findings: 1) LLMs'perceptions of knowledge boundaries are encoded in the middle to middle-upper layers across different languages. 2) Language differences in knowledge boundary perception follow a linear structure, which motivates our proposal of a training-free alignment method that effectively transfers knowledge boundary perception ability across languages, thereby helping reduce hallucination risk in low-resource languages; 3) Fine-tuning on bilingual question pair translation further enhances LLMs'recognition of knowledge boundaries across languages. Given the absence of standard testbeds for cross-lingual knowledge boundary analysis, we construct a multilingual evaluation suite comprising three representative types of knowledge boundary data. Our code and datasets are publicly available at https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.",
      "arxiv_url": "https://arxiv.org/abs/2504.13816",
      "pdf_url": "https://arxiv.org/pdf/2504.13816",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.14856",
      "title": "Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation",
      "authors": [
        "Jiajun Shen",
        "Tong Zhou",
        "Yubo Chen",
        "Delai Qiu",
        "Shengping Liu",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "While hallucinations of large language models could been alleviated through retrieval-augmented generation and citation generation, how the model utilizes internal knowledge is still opaque, and the trustworthiness of its generated answers remains questionable. In this work, we introduce Context-Prior Augmented Citation Generation task, requiring models to generate citations considering both external and internal knowledge while providing trustworthy references, with 5 evaluation metrics focusing on 3 aspects: answer helpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the paradigm for our task, and also design INTRALIGN, an integrated method containing customary data generation and an alignment algorithm. Our experimental results show that our method achieves a better cross-scenario performance with regard to other baselines. Our extended experiments further reveal that retrieval quality, question types, and model knowledge have considerable influence on the trustworthiness in citation generation.",
      "arxiv_url": "https://arxiv.org/abs/2504.14856",
      "pdf_url": "https://arxiv.org/pdf/2504.14856",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-04-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00637",
      "title": "Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics",
      "authors": [
        "Lorenzo Jaime Yu Flores",
        "Ori Ernst",
        "Jackie Chi Kit Cheung"
      ],
      "abstract": "Well-calibrated model confidence scores can improve the usefulness of text generation models. For example, users can be prompted to review predictions with low confidence scores, to prevent models from returning bad or potentially dangerous predictions. However, confidence metrics are not always well calibrated in text generation. One reason is that in generation, there can be many valid answers, which previous methods do not always account for. Hence, a confident model could distribute its output probability among multiple sequences because they are all valid. We propose task-agnostic confidence metrics suited to generation, which rely solely on the probabilities associated with the model outputs without the need for further fine-tuning or heuristics. Using these, we are able to improve the calibration of BART and Flan-T5 on summarization, translation, and QA datasets.",
      "arxiv_url": "https://arxiv.org/abs/2506.00637",
      "pdf_url": "https://arxiv.org/pdf/2506.00637",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01014",
      "title": "Rhythm Controllable and Efficient Zero-Shot Voice Conversion via Shortcut Flow Matching",
      "authors": [
        "Jialong Zuo",
        "Shengpeng Ji",
        "Minghui Fang",
        "Mingze Li",
        "Ziyue Jiang",
        "Xize Cheng",
        "Xiaoda Yang",
        "Feiyang Chen",
        "Xinyu Duan",
        "Zhou Zhao"
      ],
      "abstract": "Zero-Shot Voice Conversion (VC) aims to transform the source speaker's timbre into an arbitrary unseen one while retaining speech content. Most prior work focuses on preserving the source's prosody, while fine-grained timbre information may leak through prosody, and transferring target prosody to synthesized speech is rarely studied. In light of this, we propose R-VC, a rhythm-controllable and efficient zero-shot voice conversion model. R-VC employs data perturbation techniques and discretize source speech into Hubert content tokens, eliminating much content-irrelevant information. By leveraging a Mask Generative Transformer for in-context duration modeling, our model adapts the linguistic content duration to the desired target speaking style, facilitating the transfer of the target speaker's rhythm. Furthermore, R-VC introduces a powerful Diffusion Transformer (DiT) with shortcut flow matching during training, conditioning the network not only on the current noise level but also on the desired step size, enabling high timbre similarity and quality speech generation in fewer sampling steps, even in just two, thus minimizing latency. Experimental results show that R-VC achieves comparable speaker similarity to state-of-the-art VC methods with a smaller dataset, and surpasses them in terms of speech naturalness, intelligibility and style transfer performance.",
      "arxiv_url": "https://arxiv.org/abs/2506.01014",
      "pdf_url": "https://arxiv.org/pdf/2506.01014",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16983",
      "title": "LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding",
      "authors": [
        "Junlong Tong",
        "Jinlan Fu",
        "Zixuan Lin",
        "Yingqi Fan",
        "Anhao Zhao",
        "Hui Su",
        "Xiaoyu Shen"
      ],
      "abstract": "Large Language Models (LLMs) are primarily designed for batch processing. Existing methods for adapting LLMs to streaming rely either on expensive re-encoding or specialized architectures with limited scalability. This work identifies three key mismatches in adapting batch-oriented LLMs to streaming: (1) input-attention, (2) output-attention, and (3) position-ID mismatches. While it is commonly assumed that the latter two mismatches require frequent re-encoding, our analysis reveals that only the input-attention mismatch significantly impacts performance, indicating re-encoding outputs is largely unnecessary. To better understand this discrepancy with the common assumption, we provide the first comprehensive analysis of the impact of position encoding on LLMs in streaming, showing that preserving relative positions within source and target contexts is more critical than maintaining absolute order. Motivated by the above analysis, we introduce a group position encoding paradigm built on batch architectures to enhance consistency between streaming and batch modes. Extensive experiments on cross-lingual and cross-modal tasks demonstrate that our method outperforms existing approaches. Our method requires no architectural modifications, exhibits strong generalization in both streaming and batch modes. The code is available at repository https://github.com/EIT-NLP/StreamingLLM.",
      "arxiv_url": "https://arxiv.org/abs/2505.16983",
      "pdf_url": "https://arxiv.org/pdf/2505.16983",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04396",
      "title": "TableLoRA: Low-rank Adaptation on Table Structure Understanding for Large Language Models",
      "authors": [
        "Xinyi He",
        "Yihao Liu",
        "Mengyu Zhou",
        "Yeye He",
        "Haoyu Dong",
        "Shi Han",
        "Zejian Yuan",
        "Dongmei Zhang"
      ],
      "abstract": "Tabular data are crucial in many fields and their understanding by large language models (LLMs) under high parameter efficiency paradigm is important. However, directly applying parameter-efficient fine-tuning (PEFT) techniques to tabular tasks presents significant challenges, particularly in terms of better table serialization and the representation of two-dimensional structured information within a one-dimensional sequence. To address this, we propose TableLoRA, a module designed to improve LLMs'understanding of table structure during PEFT. It incorporates special tokens for serializing tables with special token encoder and uses 2D LoRA to encode low-rank information on cell positions. Experiments on four tabular-related datasets demonstrate that TableLoRA consistently outperforms vanilla LoRA and surpasses various table encoding methods tested in control experiments. These findings reveal that TableLoRA, as a table-specific LoRA, enhances the ability of LLMs to process tabular data effectively, especially in low-parameter settings, demonstrating its potential as a robust solution for handling table-related tasks.",
      "arxiv_url": "https://arxiv.org/abs/2503.04396",
      "pdf_url": "https://arxiv.org/pdf/2503.04396",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.13125",
      "title": "Generating Plausible Distractors for Multiple-Choice Questions via Student Choice Prediction",
      "authors": [
        "Yooseop Lee",
        "Suin Kim",
        "Yohan Jo"
      ],
      "abstract": "In designing multiple-choice questions (MCQs) in education, creating plausible distractors is crucial for identifying students' misconceptions and gaps in knowledge and accurately assessing their understanding. However, prior studies on distractor generation have not paid sufficient attention to enhancing the difficulty of distractors, resulting in reduced effectiveness of MCQs. This study presents a pipeline for training a model to generate distractors that are more likely to be selected by students. First, we train a pairwise ranker to reason about students' misconceptions and assess the relative plausibility of two distractors. Using this model, we create a dataset of pairwise distractor ranks and then train a distractor generator via Direct Preference Optimization (DPO) to generate more plausible distractors. Experiments on computer science subjects (Python, DB, MLDL) demonstrate that our pairwise ranker effectively identifies students' potential misunderstandings and achieves ranking accuracy comparable to human experts. Furthermore, our distractor generator outperforms several baselines in generating plausible distractors and produces questions with a higher item discrimination index (DI).",
      "arxiv_url": "https://arxiv.org/abs/2501.13125",
      "pdf_url": "https://arxiv.org/pdf/2501.13125",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16182",
      "title": "IPO: Your Language Model is Secretly a Preference Classifier",
      "authors": [
        "Shivank Garg",
        "Ayush Singh",
        "Shweta Singh",
        "Paras Chopra"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. While it enables LLMs to achieve human-level alignment, it often incurs significant computational and financial costs due to its reliance on training external reward models or human-labeled preferences. In this work, we propose Implicit Preference Optimization (IPO), an alternative approach that leverages generative LLMs as preference classifiers, thereby reducing the dependence on external human feedback or reward models to obtain preferences. We conduct a comprehensive evaluation on the preference classification ability of LLMs using RewardBench, assessing models across different sizes, architectures, and training levels to validate our hypothesis. Furthermore, we investigate the self-improvement capabilities of LLMs by generating multiple responses for a given instruction and employing the model itself as a preference classifier for Direct Preference Optimization (DPO)-based training. Our findings demonstrate that models trained through IPO achieve performance comparable to those utilizing state-of-the-art reward models for obtaining preferences.",
      "arxiv_url": "https://arxiv.org/abs/2502.16182",
      "pdf_url": "https://arxiv.org/pdf/2502.16182",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.07244",
      "title": "Can Vision-Language Models Evaluate Handwritten Math?",
      "authors": [
        "Oikantik Nath",
        "Hanani Bathina",
        "Mohammed Safi Ur Rahman Khan",
        "Mitesh M. Khapra"
      ],
      "abstract": "Recent advancements in Vision-Language Models (VLMs) have opened new possibilities in automatic grading of handwritten student responses, particularly in mathematics. However, a comprehensive study to test the ability of VLMs to evaluate and reason over handwritten content remains absent. To address this gap, we introduce FERMAT, a benchmark designed to assess the ability of VLMs to detect, localize and correct errors in handwritten mathematical content. FERMAT spans four key error dimensions - computational, conceptual, notational, and presentation - and comprises over 2,200 handwritten math solutions derived from 609 manually curated problems from grades 7-12 with intentionally introduced perturbations. Using FERMAT we benchmark nine VLMs across three tasks: error detection, localization, and correction. Our results reveal significant shortcomings in current VLMs in reasoning over handwritten text, with Gemini-1.5-Pro achieving the highest error correction rate (77%). We also observed that some models struggle with processing handwritten content, as their accuracy improves when handwritten inputs are replaced with printed text or images. These findings highlight the limitations of current VLMs and reveal new avenues for improvement. We release FERMAT and all the associated resources in the open-source to drive further research.",
      "arxiv_url": "https://arxiv.org/abs/2501.07244",
      "pdf_url": "https://arxiv.org/pdf/2501.07244",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "0c1712c47e6932136fd6ed6a3230db5320ac8a46",
      "title": "REVISE: A Framework for Revising OCRed text in Practical Information Systems with Data Contamination Strategy",
      "authors": [
        "Gyuho Shim",
        "Seongtae Hong",
        "Heu-Jeoung Lim"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/0c1712c47e6932136fd6ed6a3230db5320ac8a46",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00914",
      "title": "How do Transformer Embeddings Represent Compositions? A Functional Analysis",
      "authors": [
        "Aishik Nagar",
        "Ishaan Singh Rawal",
        "Mansi Dhanania",
        "Cheston Tan"
      ],
      "abstract": "Compositionality is a key aspect of human intelligence, essential for reasoning and generalization. While transformer-based models have become the de facto standard for many language modeling tasks, little is known about how they represent compound words, and whether these representations are compositional. In this study, we test compositionality in Mistral, OpenAI Large, and Google embedding models, and compare them with BERT. First, we evaluate compositionality in the representations by examining six diverse models of compositionality (addition, multiplication, dilation, regression, etc.). We find that ridge regression, albeit linear, best accounts for compositionality. Surprisingly, we find that the classic vector addition model performs almost as well as any other model. Next, we verify that most embedding models are highly compositional, while BERT shows much poorer compositionality. We verify and visualize our findings with a synthetic dataset consisting of fully transparent adjective-noun compositions. Overall, we present a thorough investigation of compositionality.",
      "arxiv_url": "https://arxiv.org/abs/2506.00914",
      "pdf_url": "https://arxiv.org/pdf/2506.00914",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17800",
      "title": "Your Language Model May Think Too Rigidly: Achieving Reasoning Consistency with Symmetry-Enhanced Training",
      "authors": [
        "Yi-Fan Yao",
        "Zhepeng Cen",
        "Miao Li",
        "William Jongwon Han",
        "Yuyou Zhang",
        "Emerson Liu",
        "Zuxin Liu",
        "Chuang Gan",
        "Ding Zhao"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities across various tasks. However, even minor variations in query phrasing, despite preserving the underlying semantic meaning, can significantly affect their performance. To address this, we focus on enhancing LLMs' awareness of symmetry in query variations and propose syMmetry-ENhanceD (MEND) Data Augmentation, a data-centric approach that improves the model's ability to extract useful information from context. Unlike existing methods that emphasize reasoning chain augmentation, our approach improves model robustness at the knowledge extraction stage through query augmentations, enabling more data-efficient training and stronger generalization to Out-of-Distribution (OOD) settings. Extensive experiments on both logical and arithmetic reasoning tasks show that MEND enhances reasoning performance across diverse query variations, providing new insight into improving LLM robustness through structured dataset curation.",
      "arxiv_url": "https://arxiv.org/abs/2502.17800",
      "pdf_url": "https://arxiv.org/pdf/2502.17800",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20067",
      "title": "UniCodec: Unified Audio Codec with Single Domain-Adaptive Codebook",
      "authors": [
        "Yidi Jiang",
        "Qian Chen",
        "Shengpeng Ji",
        "Yu Xi",
        "Wen Wang",
        "Chong Zhang",
        "Xianghu Yue",
        "Shiliang Zhang",
        "Haizhou Li"
      ],
      "abstract": "The emergence of audio language models is empowered by neural audio codecs, which establish critical mappings between continuous waveforms and discrete tokens compatible with language model paradigms. The evolutionary trends from multi-layer residual vector quantizer to single-layer quantizer are beneficial for language-autoregressive decoding. However, the capability to handle multi-domain audio signals through a single codebook remains constrained by inter-domain distribution discrepancies. In this work, we introduce UniCodec, a unified audio codec with a single codebook to support multi-domain audio data, including speech, music, and sound. To achieve this, we propose a partitioned domain-adaptive codebook method and domain Mixture-of-Experts strategy to capture the distinct characteristics of each audio domain. Furthermore, to enrich the semantic density of the codec without auxiliary modules, we propose a self-supervised mask prediction modeling approach. Comprehensive objective and subjective evaluations demonstrate that UniCodec achieves excellent audio reconstruction performance across the three audio domains, outperforming existing unified neural codecs with a single codebook, and even surpasses state-of-the-art domain-specific codecs on both acoustic and semantic representation capabilities.",
      "arxiv_url": "https://arxiv.org/abs/2502.20067",
      "pdf_url": "https://arxiv.org/pdf/2502.20067",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.09146",
      "title": "Generative Frame Sampler for Long Video Understanding",
      "authors": [
        "Linli Yao",
        "Haoning Wu",
        "Kun Ouyang",
        "Yuanxing Zhang",
        "Caiming Xiong",
        "Bei Chen",
        "Xu Sun",
        "Junnan Li"
      ],
      "abstract": "Despite recent advances in Video Large Language Models (VideoLLMs), effectively understanding long-form videos remains a significant challenge. Perceiving lengthy videos containing thousands of frames poses substantial computational burden. To mitigate this issue, this paper introduces Generative Frame Sampler (GenS), a plug-and-play module integrated with VideoLLMs to facilitate efficient lengthy video perception. Built upon a lightweight VideoLLM, GenS leverages its inherent vision-language capabilities to identify question-relevant frames. To facilitate effective retrieval, we construct GenS-Video-150K, a large-scale video instruction dataset with dense frame relevance annotations. Extensive experiments demonstrate that GenS consistently boosts the performance of various VideoLLMs, including open-source models (Qwen2-VL-7B, Aria-25B, VILA-40B, LLaVA-Video-7B/72B) and proprietary assistants (GPT-4o, Gemini). When equipped with GenS, open-source VideoLLMs achieve impressive state-of-the-art results on long-form video benchmarks: LLaVA-Video-72B reaches 66.8 (+4.3) on LongVideoBench and 77.0 (+2.7) on MLVU, while Aria obtains 39.2 on HourVideo surpassing the Gemini-1.5-pro by 1.9 points. We will release all datasets and models at https://generative-sampler.github.io.",
      "arxiv_url": "https://arxiv.org/abs/2503.09146",
      "pdf_url": "https://arxiv.org/pdf/2503.09146",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.11582",
      "title": "AskQE: Question Answering as Automatic Evaluation for Machine Translation",
      "authors": [
        "Dayeon Ki",
        "Kevin Duh",
        "Marine Carpuat"
      ],
      "abstract": "How can a monolingual English speaker determine whether an automatic translation in French is good enough to be shared? Existing MT error detection and quality estimation (QE) techniques do not address this practical scenario. We introduce AskQE, a question generation and answering framework designed to detect critical MT errors and provide actionable feedback, helping users decide whether to accept or reject MT outputs even without the knowledge of the target language. Using ContraTICO, a dataset of contrastive synthetic MT errors in the COVID-19 domain, we explore design choices for AskQE and develop an optimized version relying on LLaMA-3 70B and entailed facts to guide question generation. We evaluate the resulting system on the BioMQM dataset of naturally occurring MT errors, where AskQE has higher Kendall's Tau correlation and decision accuracy with human ratings compared to other QE metrics.",
      "arxiv_url": "https://arxiv.org/abs/2504.11582",
      "pdf_url": "https://arxiv.org/pdf/2504.11582",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14778",
      "title": "Harnessing PDF Data for Improving Japanese Large Multimodal Models",
      "authors": [
        "Jeonghun Baek",
        "Akiko Aizawa",
        "Kiyoharu Aizawa"
      ],
      "abstract": "Large Multimodal Models (LMMs) have demonstrated strong performance in English, but their effectiveness in Japanese remains limited due to the lack of high-quality training data. Current Japanese LMMs often rely on translated English datasets, restricting their ability to capture Japan-specific cultural knowledge. To address this, we explore the potential of Japanese PDF data as a training resource, an area that remains largely underutilized. We introduce a fully automated pipeline that leverages pretrained models to extract image-text pairs from PDFs through layout analysis, OCR, and vision-language pairing, removing the need for manual annotation. Additionally, we construct instruction data from extracted image-text pairs to enrich the training data. To evaluate the effectiveness of PDF-derived data, we train Japanese LMMs and assess their performance on the Japanese LMM Benchmark. Our results demonstrate substantial improvements, with performance gains ranging from 2.1% to 13.8% on Heron-Bench. Further analysis highlights the impact of PDF-derived data on various factors, such as model size and language models, reinforcing its value as a multimodal resource for Japanese LMMs.",
      "arxiv_url": "https://arxiv.org/abs/2502.14778",
      "pdf_url": "https://arxiv.org/pdf/2502.14778",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16078",
      "title": "Small Language Models in the Real World: Insights from Industrial Text Classification",
      "authors": [
        "Lujun Li",
        "Lama Sleem",
        "Niccolò Gentile",
        "Geoffrey Nichil",
        "Radu State"
      ],
      "abstract": "With the emergence of ChatGPT, Transformer models have significantly advanced text classification and related tasks. Decoder-only models such as Llama exhibit strong performance and flexibility, yet they suffer from inefficiency on inference due to token-by-token generation, and their effectiveness in text classification tasks heavily depends on prompt quality. Moreover, their substantial GPU resource requirements often limit widespread adoption. Thus, the question of whether smaller language models are capable of effectively handling text classification tasks emerges as a topic of significant interest. However, the selection of appropriate models and methodologies remains largely underexplored. In this paper, we conduct a comprehensive evaluation of prompt engineering and supervised fine-tuning methods for transformer-based text classification. Specifically, we focus on practical industrial scenarios, including email classification, legal document categorization, and the classification of extremely long academic texts. We examine the strengths and limitations of smaller models, with particular attention to both their performance and their efficiency in Video Random-Access Memory (VRAM) utilization, thereby providing valuable insights for the local deployment and application of compact models in industrial settings.",
      "arxiv_url": "https://arxiv.org/abs/2505.16078",
      "pdf_url": "https://arxiv.org/pdf/2505.16078",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.17314",
      "title": "PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights",
      "authors": [
        "Adnan Qidwai",
        "Srija Mukhopadhyay",
        "Prerana Khatiwada",
        "Dan Roth",
        "Vivek Gupta"
      ],
      "abstract": "Accurate and complete product descriptions are crucial for e-commerce, yet seller-provided information often falls short. Customer reviews offer valuable details but are laborious to sift through manually. We present PRAISE: Product Review Attribute Insight Structuring Engine, a novel system that uses Large Language Models (LLMs) to automatically extract, compare, and structure insights from customer reviews and seller descriptions. PRAISE provides users with an intuitive interface to identify missing, contradictory, or partially matching details between these two sources, presenting the discrepancies in a clear, structured format alongside supporting evidence from reviews. This allows sellers to easily enhance their product listings for clarity and persuasiveness, and buyers to better assess product reliability. Our demonstration showcases PRAISE's workflow, its effectiveness in generating actionable structured insights from unstructured reviews, and its potential to significantly improve the quality and trustworthiness of e-commerce product catalogs.",
      "arxiv_url": "https://arxiv.org/abs/2506.17314",
      "pdf_url": "https://arxiv.org/pdf/2506.17314",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15910",
      "title": "Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models",
      "authors": [
        "Zheyuan Liu",
        "Guangyao Dou",
        "Xiangchi Yuan",
        "Chunhui Zhang",
        "Zhaoxuan Tan",
        "Meng Jiang"
      ],
      "abstract": "Generative models such as Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) trained on massive datasets can lead them to memorize and inadvertently reveal sensitive information, raising ethical and privacy concerns. While some prior works have explored this issue in the context of LLMs, it presents a unique challenge for MLLMs due to the entangled nature of knowledge across modalities, making comprehensive unlearning more difficult. To address this challenge, we propose Modality Aware Neuron Unlearning (MANU), a novel unlearning framework for MLLMs designed to selectively clip neurons based on their relative importance to the targeted forget data, curated for different modalities. Specifically, MANU consists of two stages: important neuron selection and selective pruning. The first stage identifies and collects the most influential neurons across modalities relative to the targeted forget knowledge, while the second stage is dedicated to pruning those selected neurons. MANU effectively isolates and removes the neurons that contribute most to the forget data within each modality, while preserving the integrity of retained knowledge. Our experiments conducted across various MLLM architectures illustrate that MANU can achieve a more balanced and comprehensive unlearning in each modality without largely affecting the overall model utility.",
      "arxiv_url": "https://arxiv.org/abs/2502.15910",
      "pdf_url": "https://arxiv.org/pdf/2502.15910",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10981",
      "title": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory",
      "authors": [
        "Yexiang Liu",
        "Zekun Li",
        "Zhi Fang",
        "Nan Xu",
        "Ran He",
        "Tieniu Tan"
      ],
      "abstract": "Recently, scaling test-time compute on Large Language Models (LLM) has garnered wide attention. However, there has been limited investigation of how various reasoning prompting strategies perform as scaling. In this paper, we focus on a standard and realistic scaling setting: majority voting. We systematically conduct experiments on 6 LLMs $\\times$ 8 prompting strategies $\\times$ 6 benchmarks. Experiment results consistently show that as the sampling time and computational overhead increase, complicated prompting strategies with superior initial performance gradually fall behind simple Chain-of-Thought. We analyze this phenomenon and provide theoretical proofs. Additionally, we propose a probabilistic method to efficiently predict scaling performance and identify the best prompting strategy under large sampling times, eliminating the need for resource-intensive inference processes in practical applications. Furthermore, we introduce two ways derived from our theoretical analysis to significantly improve the scaling performance. We hope that our research can promote to re-examine the role of complicated prompting, unleash the potential of simple prompting strategies, and provide new insights for enhancing test-time scaling performance. Code is available at https://github.com/MraDonkey/rethinking_prompting.",
      "arxiv_url": "https://arxiv.org/abs/2505.10981",
      "pdf_url": "https://arxiv.org/pdf/2505.10981",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.13628",
      "title": "Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning",
      "authors": [
        "Nathaniel Krasner",
        "Nicholas Lanuzo",
        "Antonios Anastasopoulos"
      ],
      "abstract": "Multilingual alignment of sentence representations has mostly required bitexts to bridge the gap between languages. We investigate whether visual information can bridge this gap instead. Image caption datasets are very easy to create without requiring multilingual expertise, so this offers a more efficient alternative for low-resource languages. We find that multilingual image-caption alignment can implicitly align the text representations between languages, languages unseen by the encoder in pretraining can be incorporated into this alignment post-hoc, and these aligned representations are usable for cross-lingual Natural Language Understanding (NLU) and bitext retrieval.",
      "arxiv_url": "https://arxiv.org/abs/2505.13628",
      "pdf_url": "https://arxiv.org/pdf/2505.13628",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "0ddff0d4ef1c8ed1fdf68bb50d07563050944808",
      "title": "Disentangling Text and Math in Word Problems: Evidence for the Bidimensional Structure of Large Language Models' Reasoning",
      "authors": [
        "Pedro H. Calais",
        "Gabriel Franco",
        "Zilu Tang",
        "Themistoklis Nikas",
        "Wagner Meira",
        "Evimaria Terzi",
        "Mark Crovella",
        "G. Daroczy",
        "Magdalena Wolska",
        "W. D. Meurers",
        "Hans-Christoph Nuerk. 2015",
        "J Word",
        "Christine DiStefano",
        "Min Zhu",
        "Diana Mîndrila",
        "Nouha Dziri",
        "Ximing Lu",
        "M. Sclar",
        "Xiang Lor-raine",
        "Liwei Li",
        "Bill Yuchen Jiang",
        "Sean Lin",
        "Welleck",
        "Jingxuan Fan",
        "Sarah Martinson",
        "Erik Y. Wang",
        "K. Hausknecht",
        "Jonah Brenner",
        "Danxian Liu",
        "Nianli Peng",
        "Corey Wang",
        "Michael P Brenner. 2024",
        "Bingbin Liu",
        "Sébastien Bubeck",
        "Ronen Eldan",
        "Janard-han Kulkarni",
        "Yuanzhi Li",
        "Anh Nguyen",
        "Rachel Ward",
        "Yi Zhang",
        "Tinygsm",
        "M. S. Locatelli",
        "Matheus Prado",
        "Matheus Igor Joaquim da Silva Costa",
        "Torres Prates"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/0ddff0d4ef1c8ed1fdf68bb50d07563050944808",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16475",
      "title": "ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection",
      "authors": [
        "Jiaqi Li",
        "Xinyi Dong",
        "Yang Liu",
        "Zhizhuo Yang",
        "Quansen Wang",
        "Xiaobo Wang",
        "Songchun Zhu",
        "Zixia Jia",
        "Zilong Zheng"
      ],
      "abstract": "We present a novel pipeline, ReflectEvo, to demonstrate that small language models (SLMs) can enhance meta introspection through reflection learning. This process iteratively generates self-reflection for self-training, fostering a continuous and self-evolving process. Leveraging this pipeline, we construct ReflectEvo-460k, a large-scale, comprehensive, self-generated reflection dataset with broadened instructions and diverse multi-domain tasks. Building upon this dataset, we demonstrate the effectiveness of reflection learning to improve SLMs' reasoning abilities using SFT and DPO with remarkable performance, substantially boosting Llama-3 from 52.4% to 71.2% and Mistral from 44.4% to 71.1%. It validates that ReflectEvo can rival or even surpass the reasoning capability of the three prominent open-sourced models on BIG-bench without distillation from superior models or fine-grained human annotation. We further conduct a deeper analysis of the high quality of self-generated reflections and their impact on error localization and correction. Our work highlights the potential of continuously enhancing the reasoning performance of SLMs through iterative reflection learning in the long run.",
      "arxiv_url": "https://arxiv.org/abs/2505.16475",
      "pdf_url": "https://arxiv.org/pdf/2505.16475",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12052",
      "title": "A Dual-Perspective NLG Meta-Evaluation Framework with Automatic Benchmark and Better Interpretability",
      "authors": [
        "Xinyu Hu",
        "Mingqi Gao",
        "Li Lin",
        "Zhenghan Yu",
        "Xiaojun Wan"
      ],
      "abstract": "In NLG meta-evaluation, evaluation metrics are typically assessed based on their consistency with humans. However, we identify some limitations in traditional NLG meta-evaluation approaches, such as issues in handling human ratings and ambiguous selections of correlation measures, which undermine the effectiveness of meta-evaluation. In this work, we propose a dual-perspective NLG meta-evaluation framework that focuses on different evaluation capabilities, thereby providing better interpretability. In addition, we introduce a method of automatically constructing the corresponding benchmarks without requiring new human annotations. Furthermore, we conduct experiments with 16 representative LLMs as the evaluators based on our proposed framework, comprehensively analyzing their evaluation performance from different perspectives.",
      "arxiv_url": "https://arxiv.org/abs/2502.12052",
      "pdf_url": "https://arxiv.org/pdf/2502.12052",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21859",
      "title": "Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries",
      "authors": [
        "Vishakh Padmakumar",
        "Zichao Wang",
        "David Arbour",
        "Jennifer Healey"
      ],
      "abstract": "While large language models (LLMs) are increasingly capable of handling longer contexts, recent work has demonstrated that they exhibit the\"lost in the middle\"phenomenon (Liu et al., 2024) of unevenly attending to different parts of the provided context. This hinders their ability to cover diverse source material in multi-document summarization, as noted in the DiverseSumm benchmark (Huang et al., 2024). In this work, we contend that principled content selection is a simple way to increase source coverage on this task. As opposed to prompting an LLM to perform the summarization in a single step, we explicitly divide the task into three steps -- (1) reducing document collections to atomic key points, (2) using determinantal point processes (DPP) to perform select key points that prioritize diverse content, and (3) rewriting to the final summary. By combining prompting steps, for extraction and rewriting, with principled techniques, for content selection, we consistently improve source coverage on the DiverseSumm benchmark across various LLMs. Finally, we also show that by incorporating relevance to a provided user intent into the DPP kernel, we can generate personalized summaries that cover relevant source information while retaining coverage.",
      "arxiv_url": "https://arxiv.org/abs/2505.21859",
      "pdf_url": "https://arxiv.org/pdf/2505.21859",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.04661",
      "title": "DRAE: Dynamic Retrieval-Augmented Expert Networks for Lifelong Learning and Task Adaptation in Robotics",
      "authors": [
        "Yayu Long",
        "Kewei Chen",
        "Long Jin",
        "Mingsheng Shang"
      ],
      "abstract": "We introduce Dynamic Retrieval-Augmented Expert Networks (DRAE), a groundbreaking architecture that addresses the challenges of lifelong learning, catastrophic forgetting, and task adaptation by combining the dynamic routing capabilities of Mixture-of-Experts (MoE); leveraging the knowledge-enhancement power of Retrieval-Augmented Generation (RAG); incorporating a novel hierarchical reinforcement learning (RL) framework; and coordinating through ReflexNet-SchemaPlanner-HyperOptima (RSHO).DRAE dynamically routes expert models via a sparse MoE gating mechanism, enabling efficient resource allocation while leveraging external knowledge through parametric retrieval (P-RAG) to augment the learning process. We propose a new RL framework with ReflexNet for low-level task execution, SchemaPlanner for symbolic reasoning, and HyperOptima for long-term context modeling, ensuring continuous adaptation and memory retention. Experimental results show that DRAE significantly outperforms baseline approaches in long-term task retention and knowledge reuse, achieving an average task success rate of 82.5% across a set of dynamic robotic manipulation tasks, compared to 74.2% for traditional MoE models. Furthermore, DRAE maintains an extremely low forgetting rate, outperforming state-of-the-art methods in catastrophic forgetting mitigation. These results demonstrate the effectiveness of our approach in enabling flexible, scalable, and efficient lifelong learning for robotics.",
      "arxiv_url": "https://arxiv.org/abs/2507.04661",
      "pdf_url": "https://arxiv.org/pdf/2507.04661",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-07-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03968",
      "title": "From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding",
      "authors": [
        "Chiwei Zhu",
        "Benfeng Xu",
        "Xiaorui Wang",
        "Zhendong Mao"
      ],
      "abstract": "The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora. Data, models and codes will be available at https://github.com/Ignoramus0817/SynthQuestions.",
      "arxiv_url": "https://arxiv.org/abs/2506.03968",
      "pdf_url": "https://arxiv.org/pdf/2506.03968",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11441",
      "title": "Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning",
      "authors": [
        "Hwan Chang",
        "Hwanhee Lee"
      ],
      "abstract": "Large language models (LLMs) risk retaining unauthorized or sensitive information from their training data, which raises privacy concerns. LLM unlearning seeks to mitigate these risks by selectively removing specified data while maintaining overall model performance. However, most existing work focus on methods to achieve effective forgetting and does not provide a detailed analysis of the retain set, the portion of training data that is not targeted for removal. In this paper, we investigate the effects of unlearning on various subsets of the retain set through a case study on entity unlearning. We introduce the Syntactically Similar Neighbor Set, a group of queries that share similar syntactic structures with the data targeted for removal, and show that this subset suffers the greatest performance drop during unlearning. Moreover, when used for regularization, this set not only preserves performance on syntactically similar queries but also delivers comparable or improved results across other data subsets. Our results highlight that syntactic similarity is a critical factor, potentially more so than domain or entity relationships, in achieving effective and practical LLM unlearning.",
      "arxiv_url": "https://arxiv.org/abs/2502.11441",
      "pdf_url": "https://arxiv.org/pdf/2502.11441",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20581",
      "title": "The Noisy Path from Source to Citation: Measuring How Scholars Engage with Past Research",
      "authors": [
        "Hong Chen",
        "M. Teplitskiy",
        "David Jurgens"
      ],
      "abstract": "Academic citations are widely used for evaluating research and tracing knowledge flows. Such uses typically rely on raw citation counts and neglect variability in citation types. In particular, citations can vary in their fidelity as original knowledge from cited studies may be paraphrased, summarized, or reinterpreted, possibly wrongly, leading to variation in how much information changes from cited to citing paper. In this study, we introduce a computational pipeline to quantify citation fidelity at scale. Using full texts of papers, the pipeline identifies citations in citing papers and the corresponding claims in cited papers, and applies supervised models to measure fidelity at the sentence level. Analyzing a large-scale multi-disciplinary dataset of approximately 13 million citation sentence pairs, we find that citation fidelity is higher when authors cite papers that are 1) more recent and intellectually close, 2) more accessible, and 3) the first author has a lower H-index and the author team is medium-sized. Using a quasi-experiment, we establish the\"telephone effect\"- when citing papers have low fidelity to the original claim, future papers that cite the citing paper and the original have lower fidelity to the original. Our work reveals systematic differences in citation fidelity, underscoring the limitations of analyses that rely on citation quantity alone and the potential for distortion of evidence.",
      "arxiv_url": "https://arxiv.org/abs/2502.20581",
      "pdf_url": "https://arxiv.org/pdf/2502.20581",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12831",
      "title": "Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering",
      "authors": [
        "Zifeng Cheng",
        "Zhonghui Wang",
        "Yuchen Fu",
        "Zhiwei Jiang",
        "Yafeng Yin",
        "Cong Wang",
        "Qing Gu"
      ],
      "abstract": "Extracting sentence embeddings from large language models (LLMs) is a practical direction, as it requires neither additional data nor fine-tuning. Previous studies usually focus on prompt engineering to guide LLMs to encode the core semantic information of the sentence into the embedding of the last token. However, the last token in these methods still encodes an excess of non-essential information, such as stop words, limiting its encoding capacity. To this end, we propose a Contrastive Prompting (CP) method that introduces an extra auxiliary prompt to elicit better sentence embedding. By contrasting with the auxiliary prompt, CP can steer existing prompts to encode the core semantics of the sentence, rather than non-essential information. CP is a plug-and-play inference-time intervention method that can be combined with various prompt-based methods. Extensive experiments on Semantic Textual Similarity (STS) tasks and downstream classification tasks demonstrate that our method can improve the performance of existing prompt-based methods across different LLMs. Our code will be released at https://github.com/zifengcheng/CP.",
      "arxiv_url": "https://arxiv.org/abs/2505.12831",
      "pdf_url": "https://arxiv.org/pdf/2505.12831",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18980",
      "title": "PEToolLLM: Towards Personalized Tool Learning in Large Language Models",
      "authors": [
        "Qiancheng Xu",
        "Yongqing Li",
        "Heming Xia",
        "Fan Liu",
        "Min Yang",
        "Wenjie Li"
      ],
      "abstract": "Tool learning has emerged as a promising direction by extending Large Language Models' (LLMs) capabilities with external tools. Existing tool learning studies primarily focus on the general-purpose tool-use capability, which addresses explicit user requirements in instructions. However, they overlook the importance of personalized tool-use capability, leading to an inability to handle implicit user preferences. To address the limitation, we first formulate the task of personalized tool learning, which integrates user's interaction history towards personalized tool usage. To fill the gap of missing benchmarks, we construct PEToolBench, featuring diverse user preferences reflected in interaction history under three distinct personalized settings, and encompassing a wide range of tool-use scenarios. Moreover, we propose a framework PEToolLLaMA to adapt LLMs to the personalized tool learning task, which is trained through supervised fine-tuning and direct preference optimization. Extensive experiments on PEToolBench demonstrate the superiority of PEToolLLaMA over existing LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2502.18980",
      "pdf_url": "https://arxiv.org/pdf/2502.18980",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14354",
      "title": "Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment",
      "authors": [
        "Moxin Li",
        "Yuantao Zhang",
        "Wenjie Wang",
        "Wentao Shi",
        "Zhuo Liu",
        "Fuli Feng",
        "Tat-Seng Chua"
      ],
      "abstract": "Multi-Objective Alignment (MOA) aims to align LLMs'responses with multiple human preference objectives, with Direct Preference Optimization (DPO) emerging as a prominent approach. However, we find that DPO-based MOA approaches suffer from widespread preference conflicts in the data, where different objectives favor different responses. This results in conflicting optimization directions, hindering the optimization on the Pareto Front. To address this, we propose to construct Pareto-optimal responses to resolve preference conflicts. To efficiently obtain and utilize such responses, we propose a self-improving DPO framework that enables LLMs to self-generate and select Pareto-optimal responses for self-supervised preference alignment. Extensive experiments on two datasets demonstrate the superior Pareto Front achieved by our framework compared to various baselines. Code is available at https://github.com/zyttt-coder/SIPO.",
      "arxiv_url": "https://arxiv.org/abs/2502.14354",
      "pdf_url": "https://arxiv.org/pdf/2502.14354",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02412",
      "title": "SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning",
      "authors": [
        "Zhengyuan Liu",
        "Geyu Lin",
        "Hui Li Tan",
        "Huayun Zhang",
        "Yanfeng Lu",
        "Xiaoxue Gao",
        "Stella Xin Yin",
        "He Sun",
        "Hock Huan Goh",
        "Lung Hsiang Wong",
        "Nancy F. Chen"
      ],
      "abstract": "The integration of generative artificial intelligence into educational applications has enhanced personalized and interactive learning experiences, and it shows strong potential to promote young learners language acquisition. However, it is still challenging to ensure consistent and robust performance across different languages and cultural contexts, and kids-friendly design requires simplified instructions, engaging interactions, and age-appropriate scaffolding to maintain motivation and optimize learning outcomes. In this work, we introduce SingaKids, a dialogic tutor designed to facilitate language learning through picture description tasks. Our system integrates dense image captioning, multilingual dialogic interaction, speech understanding, and engaging speech generation to create an immersive learning environment in four languages: English, Mandarin, Malay, and Tamil. We further improve the system through multilingual pre-training, task-specific tuning, and scaffolding optimization. Empirical studies with elementary school students demonstrate that SingaKids provides effective dialogic teaching, benefiting learners at different performance levels.",
      "arxiv_url": "https://arxiv.org/abs/2506.02412",
      "pdf_url": "https://arxiv.org/pdf/2506.02412",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Personalization"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11297",
      "title": "Probing Subphonemes in Morphology Models",
      "authors": [
        "Gal Astrach",
        "Yuval Pinter"
      ],
      "abstract": "Transformers have achieved state-of-the-art performance in morphological inflection tasks, yet their ability to generalize across languages and morphological rules remains limited. One possible explanation for this behavior can be the degree to which these models are able to capture implicit phenomena at the phonological and subphonemic levels. We introduce a language-agnostic probing method to investigate phonological feature encoding in transformers trained directly on phonemes, and perform it across seven morphologically diverse languages. We show that phonological features which are local, such as final-obstruent devoicing in Turkish, are captured well in phoneme embeddings, whereas long-distance dependencies like vowel harmony are better represented in the transformer's encoder. Finally, we discuss how these findings inform empirical strategies for training morphological models, particularly regarding the role of subphonemic feature acquisition.",
      "arxiv_url": "https://arxiv.org/abs/2505.11297",
      "pdf_url": "https://arxiv.org/pdf/2505.11297",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.10787",
      "title": "Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers",
      "authors": [
        "Yilun Zhao",
        "Chengye Wang",
        "Chuhan Li",
        "Arman Cohan"
      ],
      "abstract": "This paper introduces MISS-QA, the first benchmark specifically designed to evaluate the ability of models to interpret schematic diagrams within scientific literature. MISS-QA comprises 1,500 expert-annotated examples over 465 scientific papers. In this benchmark, models are tasked with interpreting schematic diagrams that illustrate research overviews and answering corresponding information-seeking questions based on the broader context of the paper. We assess the performance of 18 frontier multimodal foundation models, including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant performance gap between these models and human experts on MISS-QA. Our analysis of model performance on unanswerable questions and our detailed error analysis further highlight the strengths and limitations of current models, offering key insights to enhance models in comprehending multimodal scientific literature.",
      "arxiv_url": "https://arxiv.org/abs/2507.10787",
      "pdf_url": "https://arxiv.org/pdf/2507.10787",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.05019",
      "title": "Mixture-of-Personas Language Models for Population Simulation",
      "authors": [
        "Ngoc Bui",
        "Hieu Trung Nguyen",
        "Shantanu Kumar",
        "Julian Theodore",
        "Weikang Qiu",
        "Viet Anh Nguyen",
        "Rex Ying"
      ],
      "abstract": "Advances in Large Language Models (LLMs) paved the way for their emerging applications in various domains, such as human behavior simulations, where LLMs could augment human-generated data in social science research and machine learning model training. However, pretrained LLMs often fail to capture the behavioral diversity of target populations due to the inherent variability across individuals and groups. To address this, we propose \\textit{Mixture of Personas} (MoP), a \\textit{probabilistic} prompting method that aligns the LLM responses with the target population. MoP is a contextual mixture model, where each component is an LM agent characterized by a persona and an exemplar representing subpopulation behaviors. The persona and exemplar are randomly chosen according to the learned mixing weights to elicit diverse LLM responses during simulation. MoP is flexible, requires no model finetuning, and is transferable across base models. Experiments for synthetic data generation show that MoP outperforms competing methods in alignment and diversity metrics.",
      "arxiv_url": "https://arxiv.org/abs/2504.05019",
      "pdf_url": "https://arxiv.org/pdf/2504.05019",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.05622",
      "title": "CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory",
      "authors": [
        "Weichen Zhang",
        "Chen Gao",
        "Shiquan Yu",
        "Ruiying Peng",
        "Baining Zhao",
        "Qian Zhang",
        "Jinqiang Cui",
        "Xinlei Chen",
        "Yong Li"
      ],
      "abstract": "Aerial vision-and-language navigation (VLN), requiring drones to interpret natural language instructions and navigate complex urban environments, emerges as a critical embodied AI challenge that bridges human-robot interaction, 3D spatial reasoning, and real-world deployment. Although existing ground VLN agents achieved notable results in indoor and outdoor settings, they struggle in aerial VLN due to the absence of predefined navigation graphs and the exponentially expanding action space in long-horizon exploration. In this work, we propose \\textbf{CityNavAgent}, a large language model (LLM)-empowered agent that significantly reduces the navigation complexity for urban aerial VLN. Specifically, we design a hierarchical semantic planning module (HSPM) that decomposes the long-horizon task into sub-goals with different semantic levels. The agent reaches the target progressively by achieving sub-goals with different capacities of the LLM. Additionally, a global memory module storing historical trajectories into a topological graph is developed to simplify navigation for visited targets. Extensive benchmark experiments show that our method achieves state-of-the-art performance with significant improvement. Further experiments demonstrate the effectiveness of different modules of CityNavAgent for aerial VLN in continuous city environments. The code is available at \\href{https://github.com/VinceOuti/CityNavAgent}{link}.",
      "arxiv_url": "https://arxiv.org/abs/2505.05622",
      "pdf_url": "https://arxiv.org/pdf/2505.05622",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19294",
      "title": "Towards Reliable Large Audio Language Model",
      "authors": [
        "Ziyang Ma",
        "Xiquan Li",
        "Ya-Zhen Song",
        "Wenxi Chen",
        "Chenpeng Du",
        "Jian Wu",
        "Yuanzhe Chen",
        "Zhuo Chen",
        "Yuping Wang",
        "Yuxuan Wang",
        "Xie Chen"
      ],
      "abstract": "Recent advancements in large audio language models (LALMs) have demonstrated impressive results and promising prospects in universal understanding and reasoning across speech, music, and general sound. However, these models still lack the ability to recognize their knowledge boundaries and refuse to answer questions they don't know proactively. While there have been successful attempts to enhance the reliability of LLMs, reliable LALMs remain largely unexplored. In this paper, we systematically investigate various approaches towards reliable LALMs, including training-free methods such as multi-modal chain-of-thought (MCoT), and training-based methods such as supervised fine-tuning (SFT). Besides, we identify the limitations of previous evaluation metrics and propose a new metric, the Reliability Gain Index (RGI), to assess the effectiveness of different reliable methods. Our findings suggest that both training-free and training-based methods enhance the reliability of LALMs to different extents. Moreover, we find that awareness of reliability is a\"meta ability\", which can be transferred across different audio modalities, although significant structural and content differences exist among sound, music, and speech.",
      "arxiv_url": "https://arxiv.org/abs/2505.19294",
      "pdf_url": "https://arxiv.org/pdf/2505.19294",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.18792",
      "title": "REALM: A Dataset of Real-World LLM Use Cases",
      "authors": [
        "Jingwen Cheng",
        "Kshitish Ghate",
        "Wenyue Hua",
        "W. Wang",
        "Hong Shen",
        "Fei Fang"
      ],
      "abstract": "Large Language Models (LLMs), such as the GPT series, have driven significant industrial applications, leading to economic and societal transformations. However, a comprehensive understanding of their real-world applications remains limited. To address this, we introduce REALM, a dataset of over 94,000 LLM use cases collected from Reddit and news articles. REALM captures two key dimensions: the diverse applications of LLMs and the demographics of their users. It categorizes LLM applications and explores how users' occupations relate to the types of applications they use. By integrating real-world data, REALM offers insights into LLM adoption across different domains, providing a foundation for future research on their evolving societal roles.",
      "arxiv_url": "https://arxiv.org/abs/2503.18792",
      "pdf_url": "https://arxiv.org/pdf/2503.18792",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12898",
      "title": "JEBS: A Fine-grained Biomedical Lexical Simplification Task",
      "authors": [
        "William Xia",
        "Ishita Unde",
        "Brian Ondov",
        "Dina Demner-Fushman"
      ],
      "abstract": "Online medical literature has made health information more available than ever, however, the barrier of complex medical jargon prevents the general public from understanding it. Though parallel and comparable corpora for Biomedical Text Simplification have been introduced, these conflate the many syntactic and lexical operations involved in simplification. To enable more targeted development and evaluation, we present a fine-grained lexical simplification task and dataset, Jargon Explanations for Biomedical Simplification (JEBS, https://github.com/bill-from-ri/JEBS-data ). The JEBS task involves identifying complex terms, classifying how to replace them, and generating replacement text. The JEBS dataset contains 21,595 replacements for 10,314 terms across 400 biomedical abstracts and their manually simplified versions. Additionally, we provide baseline results for a variety of rule-based and transformer-based systems for the three sub-tasks. The JEBS task, data, and baseline results pave the way for development and rigorous evaluation of systems for replacing or explaining complex biomedical terms.",
      "arxiv_url": "https://arxiv.org/abs/2506.12898",
      "pdf_url": "https://arxiv.org/pdf/2506.12898",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10061",
      "title": "Compute Optimal Scaling of Skills: Knowledge vs Reasoning",
      "authors": [
        "Nicholas Roberts",
        "Niladri S. Chatterji",
        "Sharan Narang",
        "Mike Lewis",
        "Dieuwke Hupkes"
      ],
      "abstract": "Scaling laws are a critical component of the LLM development pipeline, most famously as a way to forecast training decisions such as 'compute-optimally' trading-off parameter count and dataset size, alongside a more recent growing list of other crucial decisions. In this work, we ask whether compute-optimal scaling behaviour can be skill-dependent. In particular, we examine knowledge and reasoning-based skills such as knowledge-based QA and code generation, and we answer this question in the affirmative: scaling laws are skill-dependent. Next, to understand whether skill-dependent scaling is an artefact of the pretraining datamix, we conduct an extensive ablation of different datamixes and find that, also when correcting for datamix differences, knowledge and code exhibit fundamental differences in scaling behaviour. We conclude with an analysis of how our findings relate to standard compute-optimal scaling using a validation set, and find that a misspecified validation set can impact compute-optimal parameter count by nearly 50%, depending on its skill composition.",
      "arxiv_url": "https://arxiv.org/abs/2503.10061",
      "pdf_url": "https://arxiv.org/pdf/2503.10061",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01295",
      "title": "CodeArena: A Collective Evaluation Platform for LLM Code Generation",
      "authors": [
        "Mingzhe Du",
        "A. Luu",
        "Bin Ji",
        "Xiaobao Wu",
        "Dong Huang",
        "Terry Yue Zhuo",
        "Qian Liu",
        "See-Kiong Ng"
      ],
      "abstract": "Large Language Models (LLMs) have reshaped code generation by synergizing their exceptional comprehension of natural language and programming syntax, thereby substantially boosting developer productivity. These advancements have prompted numerous efforts to quantitatively evaluate their coding capabilities. However, persistent challenges, such as benchmark leakage, data dissipation, and limited system accessibility, continue to impede a timely and accurate assessment. To address these limitations, we introduce CodeArena, an online evaluation framework tailored for LLM code generation. The key innovation is a collective evaluation mechanism, which dynamically recalibrates individual model scores based on the holistic performance of all participating models, mitigating score biases caused by widespread benchmark leakage. In addition, CodeArena ensures open access to all submitted solutions and test cases and provides automation-friendly APIs to streamline the code evaluation workflow. Our main contributions are: (1) a collective evaluation system for unbiased assessment, (2) a public repository of solutions and test cases, and (3) automation-ready APIs for seamless integration.",
      "arxiv_url": "https://arxiv.org/abs/2503.01295",
      "pdf_url": "https://arxiv.org/pdf/2503.01295",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.12523",
      "title": "Memorization vs. Reasoning: Updating LLMs with New Knowledge",
      "authors": [
        "Aochong Oliver Li",
        "Tanya Goyal"
      ],
      "abstract": "Large language models (LLMs) encode vast amounts of pre-trained knowledge in their parameters, but updating them as real-world information evolves remains a challenge. Existing methodologies and benchmarks primarily target entity substitutions, failing to capture the full breadth of complex real-world dynamics. In this paper, we introduce Knowledge Update Playground (KUP), an automatic pipeline for simulating realistic knowledge updates reflected in an evidence corpora. KUP's evaluation framework includes direct and indirect probes to both test memorization of updated facts and reasoning over them, for any update learning methods. Next, we present a lightweight method called memory conditioned training (MCT), which conditions tokens in the update corpus on self-generated\"memory\"tokens during training. Our strategy encourages LLMs to surface and reason over newly memorized knowledge at inference. Our results on two strong LLMs show that (1) KUP benchmark is highly challenging, with the best CPT models achieving $<2\\%$ in indirect probing setting (reasoning) and (2) MCT training significantly outperforms prior continued pre-training (CPT) baselines, improving direct probing (memorization) results by up to $25.4\\%$.",
      "arxiv_url": "https://arxiv.org/abs/2504.12523",
      "pdf_url": "https://arxiv.org/pdf/2504.12523",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.09351",
      "title": "DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts",
      "authors": [
        "Yuchen Feng",
        "Bowen Shen",
        "Naibin Gu",
        "Jiaxuan Zhao",
        "Peng Fu",
        "Zheng Lin",
        "Weiping Wang"
      ],
      "abstract": "Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture achieve high cost-efficiency by selectively activating a subset of the parameters. Despite the inference efficiency of MoE LLMs, the training of extensive experts from scratch incurs substantial overhead, whereas reconstructing a dense LLM into an MoE LLM significantly reduces the training budget. However, existing reconstruction methods often overlook the diversity among experts, leading to potential redundancy. In this paper, we come up with the observation that a specific LLM exhibits notable diversity after being pruned on different calibration datasets, based on which we present a Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE includes domain affinity mining, pruning-based expert reconstruction, and efficient retraining. Specifically, the reconstruction includes pruning and reassembly of the feed-forward network (FFN) module. After reconstruction, we efficiently retrain the model on routers, experts and normalization modules. We implement DIVE on Llama-style LLMs with open-source training corpora. Experiments show that DIVE achieves training efficiency with minimal accuracy trade-offs, outperforming existing pruning and MoE reconstruction methods with the same number of activated parameters.",
      "arxiv_url": "https://arxiv.org/abs/2506.09351",
      "pdf_url": "https://arxiv.org/pdf/2506.09351",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20757",
      "title": "The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue Agents",
      "authors": [
        "Yihong Tang",
        "Kehai Chen",
        "Xuefeng Bai",
        "Zhengyu Niu",
        "Bo Wang",
        "Jie Liu",
        "Min Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have made remarkable advances in role-playing dialogue agents, demonstrating their utility in character simulations. However, it remains challenging for these agents to balance character portrayal utility with content safety because this essential character simulation often comes with the risk of generating unsafe content. To address this issue, we first conduct a systematic exploration of the safety-utility trade-off across multiple LLMs. Our analysis reveals that risk scenarios created by villain characters and user queries (referred to as risk coupling) contribute to this trade-off. Building on this, we propose a novel Adaptive Dynamic Multi-Preference (ADMP) method, which dynamically adjusts safety-utility preferences based on the degree of risk coupling and guides the model to generate responses biased toward utility or safety. We further introduce Coupling Margin Sampling (CMS) into coupling detection to enhance the model's ability to handle high-risk scenarios. Experimental results demonstrate that our approach improves safety metrics while maintaining utility.",
      "arxiv_url": "https://arxiv.org/abs/2502.20757",
      "pdf_url": "https://arxiv.org/pdf/2502.20757",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.17003",
      "title": "A Survey on Personalized Alignment - The Missing Piece for Large Language Models in Real-World Applications",
      "authors": [
        "Jian Guan",
        "Jun Wu",
        "Jia-Nan Li",
        "Chuanqi Cheng",
        "Wei Wu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their transition to real-world applications reveals a critical limitation: the inability to adapt to individual preferences while maintaining alignment with universal human values. Current alignment techniques adopt a one-size-fits-all approach that fails to accommodate users' diverse backgrounds and needs. This paper presents the first comprehensive survey of personalized alignment-a paradigm that enables LLMs to adapt their behavior within ethical boundaries based on individual preferences. We propose a unified framework comprising preference memory management, personalized generation, and feedback-based alignment, systematically analyzing implementation approaches and evaluating their effectiveness across various scenarios. By examining current techniques, potential risks, and future challenges, this survey provides a structured foundation for developing more adaptable and ethically-aligned LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2503.17003",
      "pdf_url": "https://arxiv.org/pdf/2503.17003",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-03-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11705",
      "title": "LLM Agents Making Agent Tools",
      "authors": [
        "Georg Wölflein",
        "Dyke Ferber",
        "Daniel Truhn",
        "Ognjen Arandjelovi'c",
        "J. Kather"
      ],
      "abstract": "Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains demanding large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, an agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a GitHub URL and short task description, ToolMaker autonomously installs dependencies and generates code to perform the task, using a closed-loop self-correction mechanism for debugging. To evaluate our approach, we introduce a benchmark comprising 15 complex computational tasks spanning various domains with over 100 unit tests to assess correctness and robustness. Our method correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows. Our code and benchmark are publicly available at https://github.com/KatherLab/ToolMaker.",
      "arxiv_url": "https://arxiv.org/abs/2502.11705",
      "pdf_url": "https://arxiv.org/pdf/2502.11705",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "1018bceba7d22baf85c7e5384d223f2a6b12fc15",
      "title": "Proxy-Driven Robust Multimodal Sentiment Analysis with Incomplete Data",
      "authors": [
        "Aoqiang Zhu",
        "Min Hu",
        "Xiaohua Wang",
        "Jiaoyun Yang",
        "Yiming Tang",
        "Ning An"
      ],
      "abstract": "Multimodal Sentiment Analysis (MSA) with incomplete data has gained significant attention recently. Existing studies focus on optimizing model structures to handle modality missing-ness, but models still face challenges in robustness when dealing with uncertain missingness. To this end, we propose a data-centric robust multimodal sentiment analysis method, Proxy-Driven Robust Multimodal Fusion (P-RMF). First, we map unimodal data to the latent space of Gaussian distributions to capture core features and structure, thereby learn stable modality representation. Then, we combine the quantified modality intrinsic uncertainty to learn stable multimodal joint representation (i.e., proxy modality), which is further enhanced through multi-layer dynamic cross-modal injection to increase its diversity. Extensive experimental results show that P-RMF outperforms existing models in noise resistance and achieves state-of-the-art performance on multiple benchmark datasets. Code will be available at https: //github.com/aoqzhu/P-RMF .",
      "arxiv_url": "https://www.semanticscholar.org/paper/1018bceba7d22baf85c7e5384d223f2a6b12fc15",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11514",
      "title": "Investigating Inference-time Scaling for Chain of Multi-modal Thought: A Preliminary Study",
      "authors": [
        "Yujie Lin",
        "Ante Wang",
        "Moye Chen",
        "Jingyao Liu",
        "Hao Liu",
        "Jinsong Su",
        "Xinyan Xiao"
      ],
      "abstract": "Recently, inference-time scaling of chain-of-thought (CoT) has been demonstrated as a promising approach for addressing multi-modal reasoning tasks. While existing studies have predominantly centered on text-based thinking, the integration of both visual and textual modalities within the reasoning process remains unexplored. In this study, we pioneer the exploration of inference-time scaling with multi-modal thought, aiming to bridge this gap. To provide a comprehensive analysis, we systematically investigate popular sampling-based and tree search-based inference-time scaling methods on 10 challenging tasks spanning various domains. Besides, we uniformly adopt a consistency-enhanced verifier to ensure effective guidance for both methods across different thought paradigms. Results show that multi-modal thought promotes better performance against conventional text-only thought, and blending the two types of thought fosters more diverse thinking. Despite these advantages, multi-modal thoughts necessitate higher token consumption for processing richer visual inputs, which raises concerns in practical applications. We hope that our findings on the merits and drawbacks of this research line will inspire future works in the field.",
      "arxiv_url": "https://arxiv.org/abs/2502.11514",
      "pdf_url": "https://arxiv.org/pdf/2502.11514",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "109f70e9b04d8440771aff2afdb389a30c583c22",
      "title": "'No' Matters: Out-of-Distribution Detection in Multimodality Multi-Turn Interactive Dialogue Download PDF",
      "authors": [
        "Rena Gao",
        "Xuetong Wu",
        "Siwen Luo",
        "Caren Han",
        "Feng Liu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/109f70e9b04d8440771aff2afdb389a30c583c22",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18448",
      "title": "Disambiguate First Parse Later: Generating Interpretations for Ambiguity Resolution in Semantic Parsing",
      "authors": [
        "Irina Saparina",
        "Mirella Lapata"
      ],
      "abstract": "Handling ambiguity and underspecification is an important challenge in natural language interfaces, particularly for tasks like text-to-SQL semantic parsing. We propose a modular approach that resolves ambiguity using natural language interpretations before mapping these to logical forms (e.g., SQL queries). Although LLMs excel at parsing unambiguous utterances, they show strong biases for ambiguous ones, typically predicting only preferred interpretations. We constructively exploit this bias to generate an initial set of preferred disambiguations and then apply a specialized infilling model to identify and generate missing interpretations. To train the infilling model, we introduce an annotation method that uses SQL execution to validate different meanings. Our approach improves interpretation coverage and generalizes across datasets with different annotation styles, database structures, and ambiguity types.",
      "arxiv_url": "https://arxiv.org/abs/2502.18448",
      "pdf_url": "https://arxiv.org/pdf/2502.18448",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.21586",
      "title": "Can Vision Language Models Understand Mimed Actions?",
      "authors": [
        "Hyundong Justin Cho",
        "Spencer Lin",
        "Tejas Srinivasan",
        "Michael S. Bernstein",
        "Deuksin Kwon",
        "Natali T. Chavez",
        "Jonathan May"
      ],
      "abstract": "Nonverbal communication (NVC) plays an integral role in human language, but studying NVC in general is challenging because of its broad scope and high variance in interpretation among individuals and cultures. However, mime -- the theatrical technique of suggesting intent using only gesture, expression, and movement -- is a subset of NVC that consists of explicit and embodied actions with much lower human interpretation variance. We argue that a solid understanding of mimed actions is a crucial prerequisite for vision-language models capable of interpreting and commanding more subtle aspects of NVC. Hence, we propose Mime Identification Multimodal Evaluation (MIME), a novel video-based question answering benchmark comprising of 86 mimed actions. Constructed with motion capture data, MIME consists of variations of each action with perturbations applied to the character, background, and viewpoint for evaluating recognition robustness. We find that both open-weight and API-based vision-language models perform significantly worse than humans on MIME, motivating the need for increased research for instilling more robust understanding of human gestures.",
      "arxiv_url": "https://arxiv.org/abs/2506.21586",
      "pdf_url": "https://arxiv.org/pdf/2506.21586",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.05312",
      "title": "Towards Adaptive Memory-Based Optimization for Enhanced Retrieval-Augmented Generation",
      "authors": [
        "Qitao Qin",
        "Yucong Luo",
        "Yihang Lu",
        "Zhibo Chu",
        "Xianwei Meng"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG), by integrating non-parametric knowledge from external knowledge bases into models, has emerged as a promising approach to enhancing response accuracy while mitigating factual errors and hallucinations. This method has been widely applied in tasks such as Question Answering (QA). However, existing RAG methods struggle with open-domain QA tasks because they perform independent retrieval operations and directly incorporate the retrieved information into generation without maintaining a summarizing memory or using adaptive retrieval strategies, leading to noise from redundant information and insufficient information integration. To address these challenges, we propose Adaptive memory-based optimization for enhanced RAG (Amber) for open-domain QA tasks, which comprises an Agent-based Memory Updater, an Adaptive Information Collector, and a Multi-granular Content Filter, working together within an iterative memory updating paradigm. Specifically, Amber integrates and optimizes the language model's memory through a multi-agent collaborative approach, ensuring comprehensive knowledge integration from previous retrieval steps. It dynamically adjusts retrieval queries and decides when to stop retrieval based on the accumulated knowledge, enhancing retrieval efficiency and effectiveness. Additionally, it reduces noise by filtering irrelevant content at multiple levels, retaining essential information to improve overall model performance. We conduct extensive experiments on several open-domain QA datasets, and the results demonstrate the superiority and effectiveness of our method and its components. The source code is available \\footnote{https://anonymous.4open.science/r/Amber-B203/}.",
      "arxiv_url": "https://arxiv.org/abs/2504.05312",
      "pdf_url": "https://arxiv.org/pdf/2504.05312",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13012",
      "title": "Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents",
      "authors": [
        "Chaoran Chen",
        "Bingsheng Yao",
        "Ruishi Zou",
        "Wenyue Hua",
        "Weimin Lyu",
        "T. Li",
        "Dakuo Wang"
      ],
      "abstract": "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature. Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.",
      "arxiv_url": "https://arxiv.org/abs/2502.13012",
      "pdf_url": "https://arxiv.org/pdf/2502.13012",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17173",
      "title": "Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch",
      "authors": [
        "Xueru Wen",
        "Jie Lou",
        "Zichao Li",
        "Yaojie Lu",
        "Xing Yu",
        "Yuqiu Ji",
        "Guohai Xu",
        "Hongyu Lin",
        "Ben He",
        "Xianpei Han",
        "Le Sun",
        "Debing Zhang"
      ],
      "abstract": "Reward models (RMs) are crucial for aligning large language models (LLMs) with human preferences. However, most RM research is centered on English and relies heavily on synthetic resources, which leads to limited and less reliable datasets and benchmarks for Chinese. To address this gap, we introduce CheemsBench, a fully human-annotated RM evaluation benchmark within Chinese contexts, and CheemsPreference, a large-scale and diverse preference dataset annotated through human-machine collaboration to support Chinese RM training. We systematically evaluate open-source discriminative and generative RMs on CheemsBench and observe significant limitations in their ability to capture human preferences in Chinese scenarios. Additionally, based on CheemsPreference, we construct an RM that achieves state-of-the-art performance on CheemsBench, demonstrating the necessity of human supervision in RM training. Our findings reveal that scaled AI-generated data struggles to fully capture human preferences, emphasizing the importance of high-quality human supervision in RM development.",
      "arxiv_url": "https://arxiv.org/abs/2502.17173",
      "pdf_url": "https://arxiv.org/pdf/2502.17173",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.11068",
      "title": "Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models",
      "authors": [
        "Bumjin Park",
        "Jinsil Lee",
        "Jaesik Choi"
      ],
      "abstract": "Large language models (LLMs) are increasingly engaging in moral and ethical reasoning, where criteria for judgment are often unclear, even for humans. While LLM alignment studies cover many areas, one important yet underexplored area is how LLMs make judgments about obligations. This work reveals a strong tendency in LLMs to judge non-obligatory contexts as obligations when prompts are augmented with modal expressions such as must or ought to. We introduce this phenomenon as Deontological Keyword Bias (DKB). We find that LLMs judge over 90\\% of commonsense scenarios as obligations when modal expressions are present. This tendency is consist across various LLM families, question types, and answer formats. To mitigate DKB, we propose a judgment strategy that integrates few-shot examples with reasoning prompts. This study sheds light on how modal expressions, as a form of linguistic framing, influence the normative decisions of LLMs and underscores the importance of addressing such biases to ensure judgment alignment.",
      "arxiv_url": "https://arxiv.org/abs/2506.11068",
      "pdf_url": "https://arxiv.org/pdf/2506.11068",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.12322",
      "title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis",
      "authors": [
        "Xin Gao",
        "Qizhi Pei",
        "Zinan Tang",
        "Yu Li",
        "Honglin Lin",
        "Jiang Wu",
        "Conghui He",
        "Lijun Wu"
      ],
      "abstract": "While data synthesis and distillation are promising strategies to enhance small language models, current approaches heavily rely on Large Language Models (LLMs), which suffer from high computational costs, environmental inefficiency, and potential biases inherited from monolithic architectures. In contrast, smaller LLMs are more accessible and sustainable, but their individual capabilities often fall short in generating high-quality, diverse, and reliable data. Inspired by collaborative human processes (e.g., peer review), we propose a multiple small LLMs involved framework, GRA, that aggregates specialized roles across small LLMs to iterative refinement and quality control typically achieved by a single large LLM. In this collaborative framework, multiple small LLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a peer-review-inspired data synthesis pipeline. The Generator proposes initial data samples, the Reviewer critiques their quality and diversity, and the Adjudicator resolves conflicts to finalize the output. By decomposing the synthesis process into specialized sub-tasks, collaborative small LLMs can achieve data-level parity with large LLM-based distillation. Through experiments across multiple benchmarks, we demonstrate that GRA-produced data matches or exceeds the quality of single large LLM outputs, e.g., Qwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large models for high-quality data synthesis, advocating instead for strategic coordination of smaller agents. Our datasets, models, and code are publicly available at https://github.com/GX-XinGao/GRA.",
      "arxiv_url": "https://arxiv.org/abs/2504.12322",
      "pdf_url": "https://arxiv.org/pdf/2504.12322",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01172",
      "title": "The Inverse Scaling Effect of Pre-Trained Language Model Surprisal Is Not Due to Data Leakage",
      "authors": [
        "Byung-Doh Oh",
        "Hongao Zhu",
        "William Schuler"
      ],
      "abstract": "In psycholinguistic modeling, surprisal from larger pre-trained language models has been shown to be a poorer predictor of naturalistic human reading times. However, it has been speculated that this may be due to data leakage that caused language models to see the text stimuli during training. This paper presents two studies to address this concern at scale. The first study reveals relatively little leakage of five naturalistic reading time corpora in two pre-training datasets in terms of length and frequency of token $n$-gram overlap. The second study replicates the negative relationship between language model size and the fit of surprisal to reading times using models trained on 'leakage-free' data that overlaps only minimally with the reading time corpora. Taken together, this suggests that previous results using language models trained on these corpora are not driven by the effects of data leakage.",
      "arxiv_url": "https://arxiv.org/abs/2506.01172",
      "pdf_url": "https://arxiv.org/pdf/2506.01172",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.08427",
      "title": "Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models",
      "authors": [
        "Jiaxiang Liu",
        "Boxuan Xing",
        "Chenhao Yuan",
        "Chenxiang Zhang",
        "Di Wu",
        "Xiusheng Huang",
        "Haida Yu",
        "Chuhan Lang",
        "Pengfei Cao",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "As large language models (LLMs) continue to advance, there is a growing urgency to enhance the interpretability of their internal knowledge mechanisms. Consequently, many interpretation methods have emerged, aiming to unravel the knowledge mechanisms of LLMs from various perspectives. However, current interpretation methods differ in input data formats and interpreting outputs. The tools integrating these methods are only capable of supporting tasks with specific inputs, significantly constraining their practical applications. To address these challenges, we present an open-source Knowledge Mechanisms Revealer&Interpreter (Know-MRI) designed to analyze the knowledge mechanisms within LLMs systematically. Specifically, we have developed an extensible core module that can automatically match different input data with interpretation methods and consolidate the interpreting outputs. It enables users to freely choose appropriate interpretation methods based on the inputs, making it easier to comprehensively diagnose the model's internal knowledge mechanisms from multiple perspectives. Our code is available at https://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on https://youtu.be/NVWZABJ43Bs.",
      "arxiv_url": "https://arxiv.org/abs/2506.08427",
      "pdf_url": "https://arxiv.org/pdf/2506.08427",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.00777",
      "title": "FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation",
      "authors": [
        "Qianli Wang",
        "Nils Feldhus",
        "Simon Ostermann",
        "Luis-Felipe Villa-Arenas",
        "Sebastian Moller",
        "Vera Schmitt"
      ],
      "abstract": "Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming two state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCF's core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals, which we hope will serve as an important finding for future research in this direction.",
      "arxiv_url": "https://arxiv.org/abs/2501.00777",
      "pdf_url": "https://arxiv.org/pdf/2501.00777",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24196",
      "title": "CLaSp: In-Context Layer Skip for Self-Speculative Decoding",
      "authors": [
        "Longze Chen",
        "Renke Shan",
        "Huiming Wang",
        "Lu Wang",
        "Ziqiang Liu",
        "Run Luo",
        "Jiawei Wang",
        "Hamid Alinejad-Rokny",
        "Min Yang"
      ],
      "abstract": "Speculative decoding (SD) is a promising method for accelerating the decoding process of Large Language Models (LLMs). The efficiency of SD primarily hinges on the consistency between the draft model and the verify model. However, existing drafting approaches typically require additional modules to be trained, which can be challenging to implement and ensure compatibility across various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping strategy for self-speculative decoding. Unlike prior methods, CLaSp does not require additional drafting modules or extra training. Instead, it employs a plug-and-play mechanism by skipping intermediate layers of the verify model to construct a compressed draft model. Specifically, we develop a dynamic programming algorithm that optimizes the layer-skipping process by leveraging the complete hidden states from the last verification stage as an objective. This enables CLaSp to dynamically adjust its layer-skipping strategy after each verification stage, without relying on pre-optimized sets of skipped layers. Experimental results across diverse downstream tasks demonstrate that CLaSp achieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the original distribution of the generated text.",
      "arxiv_url": "https://arxiv.org/abs/2505.24196",
      "pdf_url": "https://arxiv.org/pdf/2505.24196",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01089",
      "title": "Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements",
      "authors": [
        "Metehan Oguz",
        "Yavuz Faruk Bakman",
        "D. Yaldiz"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive performances in tasks related to coreference resolution. However, previous studies mostly assessed LLM performance on coreference resolution with nouns and third person pronouns. This study evaluates LLM performance on coreference resolution with indexical like I, you, here and tomorrow, which come with unique challenges due to their linguistic properties. We present the first study examining how LLMs interpret indexicals in English, releasing the English Indexical Dataset with 1600 multiple-choice questions. We evaluate pioneering LLMs, including GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3. Our results reveal that LLMs exhibit an impressive performance with some indexicals (I), while struggling with others (you, here, tomorrow), and that syntactic cues (e.g. quotation) contribute to LLM performance with some indexicals, while they reduce performance with others. Code and data are available at: https://github.com/metehanoguzz/LLMs-Indexicals-English.",
      "arxiv_url": "https://arxiv.org/abs/2506.01089",
      "pdf_url": "https://arxiv.org/pdf/2506.01089",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.09600",
      "title": "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System",
      "authors": [
        "Jihao Zhao",
        "Zhiyuan Ji",
        "Jason Zhaoxin Fan",
        "Hanyu Wang",
        "Simin Niu",
        "Bo Tang",
        "Feiyu Xiong",
        "Zhiyu Li"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline. This paper initially introduces a dual-metric evaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable the direct quantification of chunking quality. Leveraging this assessment method, we highlight the inherent limitations of traditional and semantic chunking in handling complex contextual nuances, thereby substantiating the necessity of integrating LLMs into chunking process. To address the inherent trade-off between computational efficiency and chunking precision in LLM-based approaches, we devise the granularity-aware Mixture-of-Chunkers (MoC) framework, which consists of a three-stage processing mechanism. Notably, our objective is to guide the chunker towards generating a structured list of chunking regular expressions, which are subsequently employed to extract chunks from the original text. Extensive experiments demonstrate that both our proposed metrics and the MoC framework effectively settle challenges of the chunking task, revealing the chunking kernel while enhancing the performance of the RAG system.",
      "arxiv_url": "https://arxiv.org/abs/2503.09600",
      "pdf_url": "https://arxiv.org/pdf/2503.09600",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-03-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2508.09515",
      "title": "LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation",
      "authors": [
        "Jakub Šmíd",
        "Pavel Přibáň",
        "Pavel Král"
      ],
      "abstract": "Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed sentiment analysis in a target language by transferring knowledge from a source language with available annotated data. Most existing methods depend heavily on often unreliable translation tools to bridge the language gap. In this paper, we propose a new approach that leverages a large language model (LLM) to generate high-quality pseudo-labelled data in the target language without the need for translation tools. First, the framework trains an ABSA model to obtain predictions for unlabelled target language data. Next, LLM is prompted to generate natural sentences that better represent these noisy predictions than the original text. The ABSA model is then further fine-tuned on the resulting pseudo-labelled dataset. We demonstrate the effectiveness of this method across six languages and five backbone models, surpassing previous state-of-the-art translation-based approaches. The proposed framework also supports generative models, and we show that fine-tuned LLMs outperform smaller multilingual models.",
      "arxiv_url": "https://arxiv.org/abs/2508.09515",
      "pdf_url": "https://arxiv.org/pdf/2508.09515",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-08-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17321",
      "title": "Turning Conversations into Workflows: A Framework to Extract and Evaluate Dialog Workflows for Service AI Agents",
      "authors": [
        "Prafulla Kumar Choubey",
        "Xiangyu Peng",
        "Shilpa Bhagavath",
        "Caiming Xiong",
        "Shiva K. Pentyala",
        "Chien-Sheng Wu"
      ],
      "abstract": "Automated service agents require well-structured workflows to provide consistent and accurate responses to customer queries. However, these workflows are often undocumented, and their automatic extraction from conversations remains unexplored. In this work, we present a novel framework for extracting and evaluating dialog workflows from historical interactions. Our extraction process consists of two key stages: (1) a retrieval step to select relevant conversations based on key procedural elements, and (2) a structured workflow generation process using a question-answer-based chain-of-thought (QA-CoT) prompting. To comprehensively assess the quality of extracted workflows, we introduce an automated agent and customer bots simulation framework that measures their effectiveness in resolving customer issues. Extensive experiments on the ABCD and SynthABCD datasets demonstrate that our QA-CoT technique improves workflow extraction by 12.16\\% in average macro accuracy over the baseline. Moreover, our evaluation method closely aligns with human assessments, providing a reliable and scalable framework for future research.",
      "arxiv_url": "https://arxiv.org/abs/2502.17321",
      "pdf_url": "https://arxiv.org/pdf/2502.17321",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00391",
      "title": "SHARE: An SLM-based Hierarchical Action CorREction Assistant for Text-to-SQL",
      "authors": [
        "Ge Qu",
        "Jinyang Li",
        "Bowen Qin",
        "Xiaolong Li",
        "Nan Huo",
        "Chenhao Ma",
        "Reynold Cheng"
      ],
      "abstract": "Current self-correction approaches in text-to-SQL face two critical limitations: 1) Conventional self-correction methods rely on recursive self-calls of LLMs, resulting in multiplicative computational overhead, and 2) LLMs struggle to implement effective error detection and correction for declarative SQL queries, as they fail to demonstrate the underlying reasoning path. In this work, we propose SHARE, an SLM-based Hierarchical Action corREction assistant that enables LLMs to perform more precise error localization and efficient correction. SHARE orchestrates three specialized Small Language Models (SLMs) in a sequential pipeline, where it first transforms declarative SQL queries into stepwise action trajectories that reveal underlying reasoning, followed by a two-phase granular refinement. We further propose a novel hierarchical self-evolution strategy for data-efficient training. Experimental results demonstrate that SHARE effectively enhances self-correction capabilities while proving robust across various LLMs. Furthermore, our comprehensive analysis shows that SHARE maintains strong performance even in low-resource training settings, which is particularly valuable for text-to-SQL applications with data privacy constraints.",
      "arxiv_url": "https://arxiv.org/abs/2506.00391",
      "pdf_url": "https://arxiv.org/pdf/2506.00391",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00491",
      "title": "Optimizing Question Semantic Space for Dynamic Retrieval-Augmented Multi-hop Question Answering",
      "authors": [
        "Linhao Ye",
        "Lang Yu",
        "Zhikai Lei",
        "Qin Chen",
        "Jie Zhou",
        "Liang He"
      ],
      "abstract": "Retrieval-augmented generation (RAG) is usually integrated into large language models (LLMs) to mitigate hallucinations and knowledge obsolescence. Whereas,conventional one-step retrieve-and-read methods are insufficient for multi-hop question answering, facing challenges of retrieval semantic mismatching and the high cost in handling interdependent subquestions. In this paper, we propose Optimizing Question Semantic Space for Dynamic Retrieval-Augmented Multi-hop Question Answering (Q-DREAM). Q-DREAM consists of three key modules: (1) the Question Decomposition Module (QDM), which decomposes multi-hop questions into fine-grained subquestions; (2) the Subquestion Dependency Optimizer Module (SDOM), which models the interdependent relations of subquestions for better understanding; and (3) the Dynamic Passage Retrieval Module (DPRM), which aligns subquestions with relevant passages by optimizing the semantic embeddings. Experimental results across various benchmarks demonstrate that Q-DREAM significantly outperforms existing RAG methods, achieving state-of-the-art performance in both in-domain and out-of-domain settings. Notably, Q-DREAM also improves retrieval efficiency while maintaining high accuracy compared with recent baselines.",
      "arxiv_url": "https://arxiv.org/abs/2506.00491",
      "pdf_url": "https://arxiv.org/pdf/2506.00491",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00805",
      "title": "HSCR: Hierarchical Self-Contrastive Rewarding for Aligning Medical Vision Language Models",
      "authors": [
        "Songtao Jiang",
        "Yan Zhang",
        "Yeying Jin",
        "Zhihang Tang",
        "Yangyang Wu",
        "Yang Feng",
        "Jian Wu",
        "Zuozhu Liu"
      ],
      "abstract": "Medical Vision-Language Models (Med-VLMs) have achieved success across various tasks, yet most existing methods overlook the modality misalignment issue that can lead to untrustworthy responses in clinical settings. In this paper, we propose Hierarchical Self-Contrastive Rewarding (HSCR), a novel approach that addresses two critical challenges in Med-VLM alignment: 1) Cost-effective generation of high-quality preference data; 2) Capturing nuanced and context-aware preferences for improved alignment. HSCR first leverages the inherent capability of Med-VLMs to generate dispreferred responses with higher sampling probability. By analyzing output logit shifts after visual token dropout, we identify modality-coupled tokens that induce misalignment and derive an implicit alignment reward function. This function guides token replacement with hallucinated ones during decoding, producing high-quality dispreferred data. Furthermore, HSCR introduces a multi-level preference optimization strategy, which extends beyond traditional adjacent-level optimization by incorporating nuanced implicit preferences, leveraging relative quality in dispreferred data to capture subtle alignment cues for more precise and context-aware optimization. Extensive experiments across multiple medical tasks, including Med-VQA, medical image captioning and instruction following, demonstrate that HSCR not only enhances zero-shot performance but also significantly improves modality alignment and trustworthiness with just 2,000 training entries.",
      "arxiv_url": "https://arxiv.org/abs/2506.00805",
      "pdf_url": "https://arxiv.org/pdf/2506.00805",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "13486f3020ab4581d1a562e42d34271614a60e40",
      "title": "Task-Specific Information Decomposition for End-to-End Dense Video Captioning",
      "authors": [
        "Zhiyue Liu",
        "Xinru Zhang",
        "Jinyuan Liu"
      ],
      "abstract": "Dense video captioning aims to localize events within input videos and generate concise descriptive texts for each event. Advanced end-to-end methods require both tasks to share the same intermediate features that serve as event queries, thereby enabling the mutual promotion of two tasks. However, relying on shared queries limits the model’s ability to extract task-specific information, as event semantic perception and localization demand distinct perspectives on video understanding. To address this, we propose a decomposed dense video cap-tioning framework that derives localization and captioning queries from event queries, enabling task-specific representations while maintaining inter-task collaboration. Considering the roles of different queries, we design a contrastive semantic optimization strategy that guides localization queries to focus on event-level visual features and captioning queries to align with textual semantics. Besides, only localization information is considered in existing methods for label assignment, failing to ensure the relevance of the selected queries to descriptions. We jointly consider localization and captioning losses to achieve a semantically balanced assignment process. Extensive experiments on the YouCook2 and ActivityNet Cap-tions datasets demonstrate that our framework achieves state-of-the-art performance.",
      "arxiv_url": "https://www.semanticscholar.org/paper/13486f3020ab4581d1a562e42d34271614a60e40",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18588",
      "title": "Safety Alignment via Constrained Knowledge Unlearning",
      "authors": [
        "Zesheng Shi",
        "Yucheng Zhou",
        "Jing Li"
      ],
      "abstract": "Despite significant progress in safety alignment, large language models (LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms have not fully deleted harmful knowledge in LLMs, which allows such attacks to bypass safeguards and produce harmful outputs. To address this challenge, we propose a novel safety alignment strategy, Constrained Knowledge Unlearning (CKU), which focuses on two primary objectives: knowledge localization and retention, and unlearning harmful knowledge. CKU works by scoring neurons in specific multilayer perceptron (MLP) layers to identify a subset U of neurons associated with useful knowledge. During the unlearning process, CKU prunes the gradients of neurons in U to preserve valuable knowledge while effectively mitigating harmful content. Experimental results demonstrate that CKU significantly enhances model safety without compromising overall performance, offering a superior balance between safety and utility compared to existing methods. Additionally, our analysis of neuron knowledge sensitivity across various MLP layers provides valuable insights into the mechanics of safety alignment and model knowledge editing.",
      "arxiv_url": "https://arxiv.org/abs/2505.18588",
      "pdf_url": "https://arxiv.org/pdf/2505.18588",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03523",
      "title": "TokAlign: Efficient Vocabulary Adaptation via Token Alignment",
      "authors": [
        "Chong Li",
        "Jiajun Zhang",
        "Chengqing Zong"
      ],
      "abstract": "Tokenization serves as a foundational step for Large Language Models (LLMs) to process text. In new domains or languages, the inefficiency of the tokenizer will slow down the training and generation of LLM. The mismatch in vocabulary also hinders deep knowledge transfer between LLMs like token-level distillation. To mitigate this gap, we propose an efficient method named TokAlign to replace the vocabulary of LLM from the token co-occurrences view, and further transfer the token-level knowledge between models. It first aligns the source vocabulary to the target one by learning a one-to-one mapping matrix for token IDs. Model parameters, including embeddings, are rearranged and progressively fine-tuned for the new vocabulary. Our method significantly improves multilingual text compression rates and vocabulary initialization for LLMs, decreasing the perplexity from 3.4$\\text{e}^2$ of strong baseline methods to 1.2$\\text{e}^2$ after initialization. Experimental results on models across multiple parameter scales demonstrate the effectiveness and generalization of TokAlign, which costs as few as 5k steps to restore the performance of the vanilla model. After unifying vocabularies between LLMs, token-level distillation can remarkably boost (+4.4% than sentence-level distillation) the base model, costing only 235M tokens.",
      "arxiv_url": "https://arxiv.org/abs/2506.03523",
      "pdf_url": "https://arxiv.org/pdf/2506.03523",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.07228",
      "title": "ConceptCarve: Dynamic Realization of Evidence",
      "authors": [
        "Eylon Caplan",
        "Dan Goldwasser"
      ],
      "abstract": "Finding evidence for human opinion and behavior at scale is a challenging task, often requiring an understanding of sophisticated thought patterns among vast online communities found on social media. For example, studying how gun ownership is related to the perception of Freedom, requires a retrieval system that can operate at scale over social media posts, while dealing with two key challenges: (1) identifying abstract concept instances, (2) which can be instantiated differently across different communities. To address these, we introduce ConceptCarve, an evidence retrieval framework that utilizes traditional retrievers and LLMs to dynamically characterize the search space during retrieval. Our experiments show that ConceptCarve surpasses traditional retrieval systems in finding evidence within a social media community. It also produces an interpretable representation of the evidence for that community, which we use to qualitatively analyze complex thought patterns that manifest differently across the communities.",
      "arxiv_url": "https://arxiv.org/abs/2504.07228",
      "pdf_url": "https://arxiv.org/pdf/2504.07228",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "14390e0a949ff1d5c61c862f624b9e9c468aa99d",
      "title": "Beyond the Average Reader: the Reader Embedding Approach",
      "authors": [
        "Calogero J. Scozzaro",
        "Matteo Delsanto",
        "D. Radicioni"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/14390e0a949ff1d5c61c862f624b9e9c468aa99d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13137",
      "title": "Theorem Prover as a Judge for Synthetic Data Generation",
      "authors": [
        "Joshua Ong Jun Leang",
        "Giwon Hong",
        "Wenda Li",
        "Shay B. Cohen"
      ],
      "abstract": "The demand for synthetic data in mathematical reasoning has increased due to its potential to enhance the mathematical capabilities of large language models (LLMs). However, ensuring the validity of intermediate reasoning steps remains a significant challenge, affecting data quality. While formal verification via theorem provers effectively validates LLM reasoning, the autoformalisation of mathematical proofs remains error-prone. In response, we introduce iterative autoformalisation, an approach that iteratively refines theorem prover formalisation to mitigate errors, thereby increasing the execution rate on the Lean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as a Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to rigorously assess LLM intermediate reasoning, effectively integrating autoformalisation with synthetic data generation. Finally, we present Reinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that replaces human annotation with theorem prover feedback in Reinforcement Learning from Human Feedback (RLHF). Across multiple LLMs, applying TP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving 5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for SVAMP, and 3.55% on Llama-3.1-8B for AQUA.",
      "arxiv_url": "https://arxiv.org/abs/2502.13137",
      "pdf_url": "https://arxiv.org/pdf/2502.13137",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12450",
      "title": "Investigating and Extending Homans' Social Exchange Theory with Large Language Model based Agents",
      "authors": [
        "Lei Wang",
        "Zheqing Zhang",
        "Xu Chen"
      ],
      "abstract": "Homans' Social Exchange Theory (SET) is widely recognized as a basic framework for understanding the formation and emergence of human civilizations and social structures. In social science, this theory is typically studied based on simple simulation experiments or real-world human studies, both of which either lack realism or are too expensive to control. In artificial intelligence, recent advances in large language models (LLMs) have shown promising capabilities in simulating human behaviors. Inspired by these insights, we adopt an interdisciplinary research perspective and propose using LLM-based agents to study Homans' SET. Specifically, we construct a virtual society composed of three LLM agents and have them engage in a social exchange game to observe their behaviors. Through extensive experiments, we found that Homans' SET is well validated in our agent society, demonstrating the consistency between the agent and human behaviors. Building on this foundation, we intentionally alter the settings of the agent society to extend the traditional Homans' SET, making it more comprehensive and detailed. To the best of our knowledge, this paper marks the first step in studying Homans' SET with LLM-based agents. More importantly, it introduces a novel and feasible research paradigm that bridges the fields of social science and computer science through LLM-based agents. Code is available at https://github.com/Paitesanshi/SET.",
      "arxiv_url": "https://arxiv.org/abs/2502.12450",
      "pdf_url": "https://arxiv.org/pdf/2502.12450",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12665",
      "title": "A2ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization",
      "authors": [
        "Junhui He",
        "Junna Xing",
        "Nan Wang",
        "Rui Xu",
        "Shangyu Wu",
        "Peng Zhou",
        "Qiang Liu",
        "C. Xue",
        "Qingan Li"
      ],
      "abstract": "Long context large language models (LLMs) pose significant challenges for efficient serving due to the large memory footprint and high access overhead of KV cache. Retrieval-based KV cache reduction methods can mitigate these challenges, typically by offloading the complete KV cache to CPU and retrieving necessary tokens on demand during inference. However, these methods still suffer from unsatisfactory accuracy degradation and extra retrieval overhead. To address these limitations, this paper proposes A$^2$ATS, a novel retrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate approximation of attention scores by applying the vector quantization technique to key states, thereby enabling efficient and precise retrieval of the top-K tokens. First, we propose Windowed Rotary Position Embedding, which decouples the positional dependency from query and key states after position embedding. Then, we propose query-aware vector quantization that optimizes the objective of attention score approximation directly. Finally, we design the heterogeneous inference architecture for KV cache offloading, enabling long context serving with larger batch sizes. Experimental results demonstrate that A$^2$ATS can achieve a lower performance degradation with similar or lower overhead compared to existing methods, thereby increasing long context serving throughput by up to $2.7 \\times$.",
      "arxiv_url": "https://arxiv.org/abs/2502.12665",
      "pdf_url": "https://arxiv.org/pdf/2502.12665",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12342",
      "title": "REAL-MM-RAG: A Real-World Multi-Modal Retrieval Benchmark",
      "authors": [
        "Navve Wasserman",
        "Roi Pony",
        "O. Naparstek",
        "Adi Raz Goldfarb",
        "Eli Schwartz",
        "Udi Barzelay",
        "Leonid Karlinsky"
      ],
      "abstract": "Accurate multi-modal document retrieval is crucial for Retrieval-Augmented Generation (RAG), yet existing benchmarks do not fully capture real-world challenges with their current design. We introduce REAL-MM-RAG, an automatically generated benchmark designed to address four key properties essential for real-world retrieval: (i) multi-modal documents, (ii) enhanced difficulty, (iii) Realistic-RAG queries and (iv) accurate labeling. Additionally, we propose a multi-difficulty-level scheme based on query rephrasing to evaluate models' semantic understanding beyond keyword matching. Our benchmark reveals significant model weaknesses, particularly in handling table-heavy documents and robustness to query rephrasing. To mitigate these shortcomings, we curate a rephrased training set and introduce a new finance-focused, table-heavy dataset. Fine-tuning on these datasets enables models to achieve state-of-the-art retrieval performance on REAL-MM-RAG benchmark. Our work offers a better way to evaluate and improve retrieval in multi-modal RAG systems while also providing training data and models that address current limitations.",
      "arxiv_url": "https://arxiv.org/abs/2502.12342",
      "pdf_url": "https://arxiv.org/pdf/2502.12342",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Multi-Modal RAG",
        "RAG"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11256",
      "title": "Unveiling Environmental Impacts of Large Language Model Serving: A Functional Unit View",
      "authors": [
        "Yanran Wu",
        "Inez Hua",
        "Yi Ding"
      ],
      "abstract": "Large language models (LLMs) offer powerful capabilities but come with significant environmental impact, particularly in carbon emissions. Existing studies benchmark carbon emissions but lack a standardized basis for comparison across different model configurations. To address this, we introduce the concept of functional unit (FU) as a standardized basis and develop FUEL, the first FU-based framework for evaluating LLM serving's environmental impact. Through three case studies, we uncover key insights and trade-offs in reducing carbon emissions by optimizing model size, quantization strategy, and hardware choice, paving the way for more sustainable LLM serving. The code is available at https://github.com/jojacola/FUEL.",
      "arxiv_url": "https://arxiv.org/abs/2502.11256",
      "pdf_url": "https://arxiv.org/pdf/2502.11256",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12597",
      "title": "Automatic Expert Discovery in LLM Upcycling via Sparse Interpolated Mixture-of-Experts",
      "authors": [
        "Shengzhuang Chen",
        "Ying Wei",
        "Jonathan Richard Schwarz"
      ],
      "abstract": "We present Sparse Interpolated Mixture-of-Experts (SIMoE) instruction-tuning, an end-to-end algorithm designed to fine-tune a dense pre-trained Large Language Model (LLM) into a MoE-style model that possesses capabilities in multiple specialized domains. During instruction-tuning, SIMoE automatically identifies multiple specialized experts under a specified sparsity constraint, with each expert representing a structurally sparse subset of the seed LLM's parameters that correspond to domain-specific knowledge within the data. SIMoE simultaneously learns an input-dependent expert merging strategy via a router network, leveraging rich cross-expert knowledge for superior downstream generalization that surpasses existing baselines. Empirically, SIMoE consistently achieves state-of-the-art performance on common instruction-tuning benchmarks while maintaining an optimal performance-compute trade-off compared to all baselines.",
      "arxiv_url": "https://arxiv.org/abs/2506.12597",
      "pdf_url": "https://arxiv.org/pdf/2506.12597",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.07719",
      "title": "Entailed Between the Lines: Incorporating Implication into NLI",
      "authors": [
        "Shreya Havaldar",
        "Hamidreza Alvari",
        "Alex Fabrikant",
        "John Palowitch",
        "Mohammad Javad Hosseini",
        "S. Buthpitiya"
      ],
      "abstract": "Much of human communication depends on implication, conveying meaning beyond literal words to express a wider range of thoughts, intentions, and feelings. For models to better understand and facilitate human communication, they must be responsive to the text's implicit meaning. We focus on Natural Language Inference (NLI), a core tool for many language tasks, and find that state-of-the-art NLI models and datasets struggle to recognize a range of cases where entailment is implied, rather than explicit from the text. We formalize implied entailment as an extension of the NLI task and introduce the Implied NLI dataset (INLI) to help today's LLMs both recognize a broader variety of implied entailments and to distinguish between implicit and explicit entailment. We show how LLMs fine-tuned on INLI understand implied entailment and can generalize this understanding across datasets and domains.",
      "arxiv_url": "https://arxiv.org/abs/2501.07719",
      "pdf_url": "https://arxiv.org/pdf/2501.07719",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04603",
      "title": "A MISMATCHED Benchmark for Scientific Natural Language Inference",
      "authors": [
        "Firoz Shaik",
        "Mobashir Sadat",
        "Nikita Gautam",
        "Doina Caragea",
        "Cornelia Caragea"
      ],
      "abstract": "Scientific Natural Language Inference (NLI) is the task of predicting the semantic relation between a pair of sentences extracted from research articles. Existing datasets for this task are derived from various computer science (CS) domains, whereas non-CS domains are completely ignored. In this paper, we introduce a novel evaluation benchmark for scientific NLI, called MISMATCHED. The new MISMATCHED benchmark covers three non-CS domains-PSYCHOLOGY, ENGINEERING, and PUBLIC HEALTH, and contains 2,700 human annotated sentence pairs. We establish strong baselines on MISMATCHED using both Pre-trained Small Language Models (SLMs) and Large Language Models (LLMs). Our best performing baseline shows a Macro F1 of only 78.17% illustrating the substantial headroom for future improvements. In addition to introducing the MISMATCHED benchmark, we show that incorporating sentence pairs having an implicit scientific NLI relation between them in model training improves their performance on scientific NLI. We make our dataset and code publicly available on GitHub.",
      "arxiv_url": "https://arxiv.org/abs/2506.04603",
      "pdf_url": "https://arxiv.org/pdf/2506.04603",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.05617",
      "title": "Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching",
      "authors": [
        "Mingzhe Li",
        "Jing Xiang",
        "Qishen Zhang",
        "Kaiyang Wan",
        "Xiuying Chen"
      ],
      "abstract": "Knowledge distillation typically involves transferring knowledge from a Large Language Model (LLM) to a Smaller Language Model (SLM). However, in tasks such as text matching, fine-tuned smaller models often yield more effective domain-specific representations, as they focus on optimizing the similarity of input pairs. To leverage both the specialized strengths of small models and the rich semantic understanding of LLMs, we introduce a flipped knowledge distillation paradigm, where LLM learns from SLM. Specifically, we address the architectural gap between decoder-only LLMs and smaller encoder-based models by reinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder generates compressed representations, while the decoder maps them to the output space. During training, the encoder produces representations and their similarities, which are then aligned with the similarity scores produced by the teacher, using our proposed Margin-aware Contrastive Learning (MCL) approach. The MCL ensures accurate similarity for both positive and negative pairs, and adaptively handles the internal differences within positive and negative samples. Our paradigm requires only a reasonably good-performing SLM, allowing the LLM to achieve improved performance. Experiments on financial and healthcare benchmarks, as well as real-world applications, confirm its effectiveness, and the model has been fully deployed in an online environment.",
      "arxiv_url": "https://arxiv.org/abs/2507.05617",
      "pdf_url": "https://arxiv.org/pdf/2507.05617",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18136",
      "title": "Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection",
      "authors": [
        "Mykola Trokhymovych",
        "Lydia Pintscher",
        "Ricardo Baeza-Yates",
        "Diego Sáez-Trumper"
      ],
      "abstract": "We introduce a next-generation vandalism detection system for Wikidata, one of the largest open-source structured knowledge bases on the Web. Wikidata is highly complex: its items incorporate an ever-expanding universe of factual triples and multilingual texts. While edits can alter both structured and textual content, our approach converts all edits into a single space using a method we call Graph2Text. This allows for evaluating all content changes for potential vandalism using a single multilingual language model. This unified approach improves coverage and simplifies maintenance. Experiments demonstrate that our solution outperforms the current production system. Additionally, we are releasing the code under an open license along with a large dataset of various human-generated knowledge alterations, enabling further research.",
      "arxiv_url": "https://arxiv.org/abs/2505.18136",
      "pdf_url": "https://arxiv.org/pdf/2505.18136",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "1503e6b0f0157b5a03e852dde614a316a45167e2",
      "title": "Compositional Syntactico-SemBanking for English as a Second or Foreign Language",
      "authors": [
        "Wenxi Li",
        "Xihao Wang",
        "Weiwei Sun"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/1503e6b0f0157b5a03e852dde614a316a45167e2",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "15090be86a20837e64e33651281a46bfc6cf038d",
      "title": "BadWindtunnel: Defending Backdoor in High-noise Simulated Training with Confidence Variance",
      "authors": [
        "Ruyi Zhang",
        "Songlei Jian",
        "Yusong Tan",
        "Heng Gao",
        "Haifang Zhou",
        "Kai Lu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/15090be86a20837e64e33651281a46bfc6cf038d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.17849",
      "title": "Dynamic and Generalizable Process Reward Modeling",
      "authors": [
        "Zhangyue Yin",
        "Qiushi Sun",
        "Zhiyuan Zeng",
        "Qinyuan Cheng",
        "Xipeng Qiu",
        "Xuanjing Huang"
      ],
      "abstract": "Process Reward Models (PRMs) are crucial for guiding Large Language Models (LLMs) in complex scenarios by providing dense reward signals. However, existing PRMs primarily rely on heuristic approaches, which struggle with cross-domain generalization. While LLM-as-judge has been proposed to provide generalized rewards, current research has focused mainly on feedback results, overlooking the meaningful guidance embedded within the text. Additionally, static and coarse-grained evaluation criteria struggle to adapt to complex process supervision. To tackle these challenges, we propose Dynamic and Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to capture and store fine-grained, multi-dimensional reward criteria. DG-PRM dynamically selects reward signals for step-wise reward scoring. To handle multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation to identify discriminative positive and negative pairs. Experimental results show that DG-PRM achieves stunning performance on prevailing benchmarks, significantly boosting model performance across tasks with dense rewards. Further analysis reveals that DG-PRM adapts well to out-of-distribution scenarios, demonstrating exceptional generalizability.",
      "arxiv_url": "https://arxiv.org/abs/2507.17849",
      "pdf_url": "https://arxiv.org/pdf/2507.17849",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13544",
      "title": "From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap for Text Length Control via MARKERGEN",
      "authors": [
        "Peiwen Yuan",
        "Chuyi Tan",
        "Shaoxiong Feng",
        "Yiwei Li",
        "Xinglin Wang",
        "Yueqi Zhang",
        "Jiayi Shi",
        "Boyuan Pan",
        "Yao Hu",
        "Kan Li"
      ],
      "abstract": "Despite the rapid progress of large language models (LLMs), their length-controllable text generation (LCTG) ability remains below expectations, posing a major limitation for practical applications. Existing methods mainly focus on end-to-end training to reinforce adherence to length constraints. However, the lack of decomposition and targeted enhancement of LCTG sub-abilities restricts further progress. To bridge this gap, we conduct a bottom-up decomposition of LCTG sub-abilities with human patterns as reference and perform a detailed error analysis. On this basis, we propose MarkerGen, a simple-yet-effective plug-and-play approach that:(1) mitigates LLM fundamental deficiencies via external tool integration;(2) conducts explicit length modeling with dynamically inserted markers;(3) employs a three-stage generation scheme to better align length constraints while maintaining content quality. Comprehensive experiments demonstrate that MarkerGen significantly improves LCTG across various settings, exhibiting outstanding effectiveness and generalizability.",
      "arxiv_url": "https://arxiv.org/abs/2502.13544",
      "pdf_url": "https://arxiv.org/pdf/2502.13544",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "156153f8db085e402e75d655fd5aca195e333170",
      "title": "TableCoder: Table Extraction from Text via Reliable Code Generation",
      "authors": [
        "Haoyu Dong",
        "Yue Hu",
        "Huailiang Peng",
        "Yanan Cao"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/156153f8db085e402e75d655fd5aca195e333170",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.19193",
      "title": "Browsing Lost Unformed Recollections: A Benchmark for Tip-of-the-Tongue Search and Reasoning",
      "authors": [
        "Sky CH-Wang",
        "Darshan Deshpande",
        "S. Muresan",
        "Anand Kannappan",
        "Rebecca Qian"
      ],
      "abstract": "We introduce Browsing Lost Unformed Recollections, a tip-of-the-tongue known-item search and reasoning benchmark for general AI assistants. BLUR introduces a set of 573 real-world validated questions that demand searching and reasoning across multi-modal and multilingual inputs, as well as proficient tool use, in order to excel on. Humans easily ace these questions (scoring on average 98%), while the best-performing system scores around 56%. To facilitate progress toward addressing this challenging and aspirational use case for general AI assistants, we release 350 questions through a public leaderboard, retain the answers to 250 of them, and have the rest as a private test set.",
      "arxiv_url": "https://arxiv.org/abs/2503.19193",
      "pdf_url": "https://arxiv.org/pdf/2503.19193",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2025-03-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21784",
      "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation",
      "authors": [
        "Tharindu Kumarage",
        "Ninareh Mehrabi",
        "Anil Ramakrishna",
        "Xinyan Zhao",
        "Richard Zemel",
        "Kai-Wei Chang",
        "A. Galstyan",
        "Rahul Gupta",
        "Charith Peris"
      ],
      "abstract": "Safety reasoning is a recent paradigm where LLMs reason over safety policies before generating responses, thereby mitigating limitations in existing safety measures such as overrefusal and jailbreak vulnerabilities. However, implementing this paradigm is challenging due to the resource-intensive process of creating high-quality policy-embedded chain-ofthought (CoT) datasets while ensuring reasoning remains accurate and free from hallucinations or policy conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation for Safety Reasoning, a novel data generation recipe that leverages multi-agent deliberation to iteratively expand reasoning on safety policies. A data refiner stage in AIDSAFE ensures high-quality outputs by eliminating repetitive, redundant, and deceptive thoughts. AIDSAFEgenerated CoTs provide a strong foundation for supervised fine-tuning (SFT)-based safety training. Additionally, to address the need of preference data in alignment stages, such as DPO training, we introduce a supplemental recipe that uses belief augmentation to create distinct selected and rejected CoT samples. Our evaluations demonstrate that AIDSAFEgenerated CoTs achieve superior policy adherence and reasoning quality. Consequently, we show that fine-tuning open-source LLMs on these CoTs can significantly improve safety generalization and jailbreak robustness while maintaining acceptable utility and over-refusal accuracy. AIDSAFE-generated CoT datasets can be found here.",
      "arxiv_url": "https://arxiv.org/abs/2505.21784",
      "pdf_url": "https://arxiv.org/pdf/2505.21784",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "15ba2022589d2e1b5dc2dbedf02208b2f1b93b6e",
      "title": "Advancing SMoE for Continuous Domain Adaptation of MLLMs: Adaptive Router and Domain-Specific Loss",
      "authors": [
        "Liang Zhang",
        "Ziyao Lu",
        "Fandong Meng",
        "Hui Li",
        "Jie Zhou",
        "Jinsong Su"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/15ba2022589d2e1b5dc2dbedf02208b2f1b93b6e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16061",
      "title": "Internal and External Impacts of Natural Language Processing Papers",
      "authors": [
        "Yu Zhang"
      ],
      "abstract": "We investigate the impacts of NLP research published in top-tier conferences (i.e., ACL, EMNLP, and NAACL) from 1979 to 2024. By analyzing citations from research articles and external sources such as patents, media, and policy documents, we examine how different NLP topics are consumed both within the academic community and by the broader public. Our findings reveal that language modeling has the widest internal and external influence, while linguistic foundations have lower impacts. We also observe that internal and external impacts generally align, but topics like ethics, bias, and fairness show significant attention in policy documents with much fewer academic citations. Additionally, external domains exhibit distinct preferences, with patents focusing on practical NLP applications and media and policy documents engaging more with the societal implications of NLP models.",
      "arxiv_url": "https://arxiv.org/abs/2505.16061",
      "pdf_url": "https://arxiv.org/pdf/2505.16061",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "15f4c61b2c4d4ff5708df1f54d1ec212528966d5",
      "title": "Modeling Uncertainty in Composed Image Retrieval via Probabilistic Embeddings",
      "authors": [
        "Haomiao Tang",
        "Jinpeng Wang",
        "Yuang Peng",
        "G. MEng",
        "Ruisheng Luo",
        "Bin Chen",
        "Long Chen",
        "Yaowei Wang",
        "Shutao Xia"
      ],
      "abstract": "Composed Image Retrieval (CIR) enables users to search for images using multimodal queries that combine text and reference images. While metric learning methods have shown promise, they rely on deterministic point embeddings that fail to capture the inherent uncertainty in the input data, in which user intentions may be imprecisely specified or open to multiple interpretations. We address this challenge by refor-mulating CIR through our proposed Co mposed P robabilistic E mbedding (C O PE) framework, which represents both queries and targets as Gaussian distributions in latent space rather than fixed points. Through careful design of probabilistic distance metrics and hierarchical learning objectives, C O PE explicitly captures uncertainty at both instance and feature levels, enabling more flexible, nuanced, and robust matching that can handle polysemy and ambiguity in search intentions. Extensive experiments across multiple benchmarks demonstrate that C O PE effectively quantifies both quality and semantic uncertainties within Com-posed Image Retrieval, achieving state-of-the-art performance on recall rate. Code: https: //github.com/tanghme0w/ACL25-CoPE .",
      "arxiv_url": "https://www.semanticscholar.org/paper/15f4c61b2c4d4ff5708df1f54d1ec212528966d5",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13383",
      "title": "MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification",
      "authors": [
        "Linzhuang Sun",
        "Hao Liang",
        "Jingxuan Wei",
        "Bihui Yu",
        "Tianpeng Li",
        "Fan Yang",
        "Zenan Zhou",
        "Wentao Zhang"
      ],
      "abstract": "According to the Test-Time Scaling, the integration of External Slow-Thinking with the Verify mechanism has been demonstrated to enhance multi-round reasoning in large language models (LLMs). However, in the multimodal (MM) domain, there is still a lack of a strong MM-Verifier. In this paper, we introduce MM-Verifier and MM-Reasoner to enhance multimodal reasoning through longer inference and more robust verification. First, we propose a two-step MM verification data synthesis method, which combines a simulation-based tree search with verification and uses rejection sampling to generate high-quality Chain-of-Thought (COT) data. This data is then used to fine-tune the verification model, MM-Verifier. Additionally, we present a more efficient method for synthesizing MMCOT data, bridging the gap between text-based and multimodal reasoning. The synthesized data is used to fine-tune MM-Reasoner. Our MM-Verifier outperforms all larger models on the MathCheck, MathVista, and MathVerse benchmarks. Moreover, MM-Reasoner demonstrates strong effectiveness and scalability, with performance improving as data size increases. Finally, our approach achieves strong performance when combining MM-Reasoner and MM-Verifier, reaching an accuracy of 65.3 on MathVista, surpassing GPT-4o (63.8) with 12 rollouts.",
      "arxiv_url": "https://arxiv.org/abs/2502.13383",
      "pdf_url": "https://arxiv.org/pdf/2502.13383",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "16346663d55dce1faaf71d3ecb2b1ad095179605",
      "title": "EasyEA: Large Language Model is All You Need in Entity Alignment Between Knowledge Graphs",
      "authors": [
        "Jingwei Cheng",
        "Chenglong Lu",
        "Linyan Yang",
        "Guoqing Chen",
        "Fu Zhang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/16346663d55dce1faaf71d3ecb2b1ad095179605",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18583",
      "title": "The Silent Saboteur: Imperceptible Adversarial Attacks against Black-Box Retrieval-Augmented Generation Systems",
      "authors": [
        "Hongru Song",
        "Yuansan Liu",
        "Ruqing Zhang",
        "Jiafeng Guo",
        "Jianming Lv",
        "M. D. Rijke",
        "Xueqi Cheng"
      ],
      "abstract": "We explore adversarial attacks against retrieval-augmented generation (RAG) systems to identify their vulnerabilities. We focus on generating human-imperceptible adversarial examples and introduce a novel imperceptible retrieve-to-generate attack against RAG. This task aims to find imperceptible perturbations that retrieve a target document, originally excluded from the initial top-$k$ candidate set, in order to influence the final answer generation. To address this task, we propose ReGENT, a reinforcement learning-based framework that tracks interactions between the attacker and the target RAG and continuously refines attack strategies based on relevance-generation-naturalness rewards. Experiments on newly constructed factual and non-factual question-answering benchmarks demonstrate that ReGENT significantly outperforms existing attack methods in misleading RAG systems with small imperceptible text perturbations.",
      "arxiv_url": "https://arxiv.org/abs/2505.18583",
      "pdf_url": "https://arxiv.org/pdf/2505.18583",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-05-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "16a5e92e5cdfb189a63033cfe0c03aa5add957c8",
      "title": "Beyond Generation: Leveraging LLM Creativity to Overcome Label Bias in Classification",
      "authors": [
        "Xiaoyue Wang",
        "Xin Liu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/16a5e92e5cdfb189a63033cfe0c03aa5add957c8",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "16b37ac5d7eb6b33916f1ba7004bbcc60a3e4ef0",
      "title": "T²DR: A Two-Tier Deficiency-Resistant Framework for Incomplete Multimodal Learning",
      "authors": [
        "Han Lin",
        "Xiu Tang",
        "Huan Li",
        "Wenxue Cao",
        "Sai Wu",
        "Chang Yao",
        "Lidan Shou",
        "Gang Chen"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/16b37ac5d7eb6b33916f1ba7004bbcc60a3e4ef0",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "16d4af9a38f778d7d3c90e4743940baa51399726",
      "title": "Supervised and Unsupervised Probing of Shortcut Learning: Case Study on the Emergence and Evolution of Syntactic Heuristics in BERT",
      "authors": [
        "Elke Vandermeerschen",
        "Miryam de Lhoneux"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/16d4af9a38f778d7d3c90e4743940baa51399726",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "16e2c1e988b13329e3b7fc2dbbf854f9414721d7",
      "title": "RATIONALYST: Pre-training Process-Supervision for Improving Reasoning",
      "authors": [
        "Dongwei Jiang",
        "Guoxuan Wang",
        "Yining Lu",
        "Andrew Wang",
        "Jingyu (Jack) Zhang",
        "Chuyu Liu",
        "Benjamin Van Durme",
        "Daniel Khashabi"
      ],
      "abstract": "The reasoning steps generated by LLMs might be incomplete, as they mimic logical leaps common in everyday communication found in their pre-training data: underlying rationales are frequently left implicit (unstated). To address this challenge, we introduce R ATIONALYST , a model for process-supervision of reasoning based on pre-training on a vast collection of rationale annotations extracted from unlabeled data. We extract 79k rationales from web-scale unlabelled dataset (the Pile) and a combination of reasoning datasets with minimal human intervention. This web-scale pre-training for reasoning allows R ATIONALYST to consistently generalize across diverse reasoning tasks, including mathematical, commonsense, scientific, and logical reasoning. Fine-tuned from LLaMa-3-8B, R ATIONALYST improves the accuracy of reasoning by an average of 3.9% on 7 representative reasoning benchmarks. It also demonstrates superior performance compared to significantly larger verifiers like GPT-4 and similarly sized models fine-tuned on matching training sets. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/16e2c1e988b13329e3b7fc2dbbf854f9414721d7",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "1725cdec0bce2e71dcc3ba824dd711481b5a8a7f",
      "title": "RolePlot: A Systematic Framework for Evaluating and Enhancing the Plot-Progression Capabilities of Role-Playing Agents",
      "authors": [
        "Pinyi Zhang",
        "Siyu An",
        "Lingfeng Qiao",
        "Yifei Yu",
        "Jingyang Chen",
        "Jie Wang",
        "Di Yin",
        "Xing Sun",
        "Kai Zhang"
      ],
      "abstract": "Role-playing agents (RPAs) are garnering increasing interests as a novel form of conversational AI. While previous research has predominantly concentrated on their ability to por-tray specified characters, we argue from a user-centered perspective that RPAs’ capability to advance the plot requires substantial improvements to deliver more engaging interaction. To bridge this gap, we propose RolePlot , a role-playing framework specifically designed to evaluate and enhance the plot-progression capabilities of RPAs. RolePlot begins by constructing a plot-progression dataset extended from human-written literary scripts and specially designed synthetic data, followed by narrative theory-driven manual annotation and automated labeling validated through human verification. We then exploit the over-parameterized embedding space of LLMs to detect a “trigger subspace” that identifies dialogue segments cat-alyzing plot transitions. When user’s inputs align with this subspace, we explicitly prompt RPAs to advance the plot. For evaluation, we simulate User-RPA interactions and track both the conversation longevity (measured in dialogue turns before disengagement) and users’ arousal levels across different stages. Empiri-cally, our method improves RPAs’ capability to time plot developments, and more importantly, yielding a significant increase in conversation",
      "arxiv_url": "https://www.semanticscholar.org/paper/1725cdec0bce2e71dcc3ba824dd711481b5a8a7f",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21043",
      "title": "Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction",
      "authors": [
        "Sam O’Connor Russell",
        "Naomi Harte"
      ],
      "abstract": "Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs) facilitate naturalistic human-robot interaction, yet most rely solely on speech. We introduce MM-VAP, a multimodal PTTM which combines speech with visual cues including facial expression, head pose and gaze. We find that it outperforms the state-of-the-art audio-only in videoconferencing interactions (84% vs. 79% hold/shift prediction accuracy). Unlike prior work which aggregates all holds and shifts, we group by duration of silence between turns. This reveals that through the inclusion of visual features, MM-VAP outperforms a state-of-the-art audio-only turn-taking model across all durations of speaker transitions. We conduct a detailed ablation study, which reveals that facial expression features contribute the most to model performance. Thus, our working hypothesis is that when interlocutors can see one another, visual cues are vital for turn-taking and must therefore be included for accurate turn-taking prediction. We additionally validate the suitability of automatic speech alignment for PTTM training using telephone speech. This work represents the first comprehensive analysis of multimodal PTTMs. We discuss implications for future work and make all code publicly available.",
      "arxiv_url": "https://arxiv.org/abs/2505.21043",
      "pdf_url": "https://arxiv.org/pdf/2505.21043",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "17642d2fbacd13ec1845a1ec690f80959d9e9be3",
      "title": "ScanEZ: Integrating Cognitive Models with Self-Supervised Learning for Spatiotemporal Scanpath Prediction",
      "authors": [
        "Ekta Sood",
        "Prajit Dhar",
        "Enrica Troiano",
        "Rosy Southwell",
        "Sidney K. DMello"
      ],
      "abstract": "Accurately predicting human scanpaths during reading is vital for diverse fields and down-stream tasks, from educational technologies to automatic question answering. To date, however, progress in this direction remains limited by scarce gaze data. We overcome the issue with ScanEZ, a self-supervised framework grounded in cognitive models of reading. ScanEZ jointly models the spatial and temporal dimensions of scanpaths by leveraging synthetic data and a 3-D gaze objective inspired by masked language modeling. With this framework, we provide evidence that two key factors in scanpath prediction during reading are: the use of masked modeling of both spatial and temporal patterns of eye movements, and cognitive model simulations as an inductive bias to kick-start training. Our approach achieves state-of-the-art results on established datasets (e.g., up to 31.4% negative log-likelihood improvement on CELER L1), and proves portable across different experimental conditions.",
      "arxiv_url": "https://www.semanticscholar.org/paper/17642d2fbacd13ec1845a1ec690f80959d9e9be3",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02391",
      "title": "Consultant Decoding: Yet Another Synergistic Mechanism",
      "authors": [
        "Chuanghao Ding",
        "Jiaping Wang",
        "Ziqing Yang",
        "Xiaoliang Wang",
        "Dahua Lin",
        "Cam-Tu Nguyen",
        "Fei Tan"
      ],
      "abstract": "The synergistic mechanism based on Speculative Decoding (SD) has garnered considerable attention as a simple yet effective approach for accelerating the inference of large language models (LLMs). Nonetheless, the high rejection rates require repeated LLMs calls to validate draft tokens, undermining the overall efficiency gain of SD. In this work, we revisit existing verification mechanisms and propose a novel synergetic mechanism Consultant Decoding (CD). Unlike SD, which relies on a metric derived from importance sampling for verification, CD verifies candidate drafts using token-level likelihoods computed solely by the LLM. CD achieves up to a 2.5-fold increase in inference speed compared to the target model, while maintaining comparable generation quality (around 100% of the target model's performance). Interestingly, this is achieved by combining models whose parameter sizes differ by two orders of magnitude. In addition, CD reduces the call frequency of the large target model to below 10%, particularly in more demanding tasks. CD's performance was even found to surpass that of the large target model, which theoretically represents the upper bound for speculative decoding.",
      "arxiv_url": "https://arxiv.org/abs/2506.02391",
      "pdf_url": "https://arxiv.org/pdf/2506.02391",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03541",
      "title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement",
      "authors": [
        "Xiaofeng Zhou",
        "Heyan Huang",
        "Lizi Liao"
      ],
      "abstract": "Large Language Models (LLMs) continue to set new standards in knowledge-intensive and complex reasoning tasks, yet their high computational demands limit widespread adoption. While distilling large models into smaller ones offers a sustainable solution, current techniques--such as static knowledge distillation, resource-intensive reinforcement learning from human feedback, or limited self-reflection--struggle to yield substantial and lasting performance gains. In this paper, we present a novel Debate and Reflect (D&R) framework that orchestrates multi-turn debates between smaller models and stronger teacher models, eliciting actionable feedback (e.g., error analysis, corrective strategies) to guide student models. Further, we introduce Tree-structured Direct Preference Optimization (T-DPO) to efficiently leverage these debate logs, organizing interactions into a hierarchical format for effective training. Empirical evaluations across diverse NLP benchmarks demonstrate that our approach significantly improves smaller-model accuracy, robustness, and generalization, outperforming conventional baselines by a large margin.",
      "arxiv_url": "https://arxiv.org/abs/2506.03541",
      "pdf_url": "https://arxiv.org/pdf/2506.03541",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20569",
      "title": "Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models",
      "authors": [
        "Jihoon Lee",
        "Min Song"
      ],
      "abstract": "Despite significant advancements in Large Vision-Language Models, Object Hallucination (OH) remains a persistent challenge. Building upon prior studies on contrastive decoding that address this issue without requiring additional model training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an advanced method to suppress OH. RVCD leverages both negative and positive images at the logit level, explicitly referencing AI-generated images designed to represent a single concept. Our approach demonstrates substantial improvements over existing decoding-based methods.",
      "arxiv_url": "https://arxiv.org/abs/2505.20569",
      "pdf_url": "https://arxiv.org/pdf/2505.20569",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.09081",
      "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning",
      "authors": [
        "Prabhat Pandey",
        "R. Swaminathan",
        "Vijay Girish",
        "Arunasish Sen",
        "†. JianXie",
        "Grant P. Strimel",
        "Andreas Schwarz",
        "Rosana Ardila",
        "Megan Branson",
        "Kelly Davis",
        "Michael Henretty",
        "Michael Kohler",
        "Joshua Meyer",
        "Reuben Morais",
        "Lindsay Saunders",
        "F. M. Tyers",
        "Gregor Weber",
        "Jean Carletta",
        "Simone Ashby",
        "S. Bourban",
        "Mike Flynn",
        "M. Guillemot",
        "Thomas Hain",
        "J. Kadlec",
        "Vasilis Karaiskos",
        "Wessel Kraaij",
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zheng-Fei Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao",
        "Daniel Tompkins",
        "Wanxiang Che",
        "Yu-An Chung",
        "Yu Zhang",
        "Wei Han",
        "Chung-Cheng Chiu",
        "James Qin",
        "Ruoming Pang",
        "Yonghui Wu",
        "W2v-bert",
        "C. Cieri",
        "David Miller",
        "Alexis Conneau",
        "Min Ma",
        "Simran Khanuja",
        "Daniel Galvez",
        "G. Diamos",
        "Juan Ciro",
        "Juan Felipe Cerón",
        "Keith Achorn",
        "Anjali Gopi",
        "David Kanter",
        "Maximilian Lam",
        "Mark Mazumder",
        "Vijay Janapa",
        "Edward J. Hu",
        "Yelong Shen",
        "Zeyuan Phillip Wallis",
        "Chien-yu Huang",
        "Wei-Chih Chen",
        "Shu-Wen Yang",
        "Andy T. Liu",
        "Chen-An Li",
        "Yu-Xiang Lin",
        "Wei-Cheng Tseng",
        "Anuj Diwan",
        "Yi-Jen Shih",
        "Jiatong Shi",
        "KeXiang Lu",
        "Shi Wang",
        "Chi-Yuan Hsiao",
        "Chun-Yi Kuan",
        "Haibin Wu",
        "Siddhant Arora",
        "Kai-Wei Chang",
        "Yifan Peng"
      ],
      "abstract": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset designed for instruction fine-tuning and pre-training of speech-text large language models (LLMs). SIFT-50M is built from publicly available speech corpora, which collectively contain 14K hours of speech, and leverages LLMs along with off-the-shelf expert models. The dataset spans five languages, encompassing a diverse range of speech understanding as well as controllable speech generation instructions. Using SIFT-50M, we train SIFT-LLM, which outperforms existing speech-text LLMs on instruction-following benchmarks while achieving competitive performance on foundational speech tasks. To support further research, we also introduce EvalSIFT, a benchmark dataset specifically designed to evaluate the instruction-following capabilities of speech-text LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2504.09081",
      "pdf_url": "https://arxiv.org/pdf/2504.09081",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.11095",
      "title": "Persistent Homology of Topic Networks for the Prediction of Reader Curiosity",
      "authors": [
        "Manuel D. S. Hopp",
        "Vincent Labatut",
        "Arthur Amalvy",
        "Richard Dufour",
        "Hannah Stone",
        "Hayley K. Jach",
        "Kou Murayama"
      ],
      "abstract": "Reader curiosity, the drive to seek information, is crucial for textual engagement, yet remains relatively underexplored in NLP. Building on Loewenstein's Information Gap Theory, we introduce a framework that models reader curiosity by quantifying semantic information gaps within a text's semantic structure. Our approach leverages BERTopic-inspired topic modeling and persistent homology to analyze the evolving topology (connected components, cycles, voids) of a dynamic semantic network derived from text segments, treating these features as proxies for information gaps. To empirically evaluate this pipeline, we collect reader curiosity ratings from participants (n = 49) as they read S. Collins's''The Hunger Games''novel. We then use the topological features from our pipeline as independent variables to predict these ratings, and experimentally show that they significantly improve curiosity prediction compared to a baseline model (73% vs. 30% explained deviance), validating our approach. This pipeline offers a new computational method for analyzing text structure and its relation to reader engagement.",
      "arxiv_url": "https://arxiv.org/abs/2506.11095",
      "pdf_url": "https://arxiv.org/pdf/2506.11095",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12143",
      "title": "Small Models Struggle to Learn from Strong Reasoners",
      "authors": [
        "Yuetai Li",
        "Xiang Yue",
        "Zhangchen Xu",
        "Fengqing Jiang",
        "Luyao Niu",
        "Bill Yuchen Lin",
        "Bhaskar Ramasubramanian",
        "Radha Poovendran"
      ],
      "abstract": "Large language models (LLMs) excel in complex reasoning tasks, and distilling their reasoning capabilities into smaller models has shown promise. However, we uncover an interesting phenomenon, which we term the Small Model Learnability Gap: small models ($\\leq$3B parameters) do not consistently benefit from long chain-of-thought (CoT) reasoning or distillation from larger models. Instead, they perform better when fine-tuned on shorter, simpler reasoning chains that better align with their intrinsic learning capacity. To address this, we propose Mix Distillation, a simple yet effective strategy that balances reasoning complexity by combining long and short CoT examples or reasoning from both larger and smaller models. Our experiments demonstrate that Mix Distillation significantly improves small model reasoning performance compared to training on either data alone. These findings highlight the limitations of direct strong model distillation and underscore the importance of adapting reasoning complexity for effective reasoning capability transfer.",
      "arxiv_url": "https://arxiv.org/abs/2502.12143",
      "pdf_url": "https://arxiv.org/pdf/2502.12143",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.02000",
      "title": "Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System",
      "authors": [
        "Yongsen Zheng",
        "Zongxuan Xie",
        "Guohua Wang",
        "Ziyao Liu",
        "Liang Lin",
        "Kwok-Yan Lam"
      ],
      "abstract": "Unfairness is a well-known challenge in Recommender Systems (RSs), often resulting in biased outcomes that disadvantage users or items based on attributes such as gender, race, age, or popularity. Although some approaches have started to improve fairness recommendation in offline or static contexts, the issue of unfairness often exacerbates over time, leading to significant problems like the Matthew effect, filter bubbles, and echo chambers. To address these challenges, we proposed a novel framework, Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System (HyFairCRS), aiming to promote multi-interest diversity fairness in dynamic and interactive Conversational Recommender Systems (CRSs). HyFairCRS first captures a wide range of user interests by establishing diverse hypergraphs through contrastive learning. These interests are then utilized in conversations to generate informative responses and ensure fair item predictions within the dynamic user-system feedback loop. Experiments on two CRS-based datasets show that HyFairCRS achieves a new state-of-the-art performance while effectively alleviating unfairness. Our code is available at https://github.com/zysensmile/HyFairCRS.",
      "arxiv_url": "https://arxiv.org/abs/2507.02000",
      "pdf_url": "https://arxiv.org/pdf/2507.02000",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "182c72cf74b9f47e352b699bd4bbb53b12088cda",
      "title": "SPECTRA: Faster Large Language Model Inference with Optimized Internal and External Speculation",
      "authors": [
        "Nguyen-Khang Le",
        "Truong Do",
        "Le-Minh Nguyen"
      ],
      "abstract": "Inference with modern Large Language Models (LLMs) is both computationally expensive and time-consuming. Speculative decoding has emerged as a promising solution, but existing approaches face key limitations: training-based methods require a draft model that is challenging to obtain and lacks generalizability, while training-free methods offer limited speedup gains. In this work, we present S PECTRA , a novel framework for accelerating LLM inference without the need for additional training or modification to the original LLM. S PECTRA introduces two new techniques for efficiently utilizing internal and external speculation, each outperforming corresponding state-of-the-art (SOTA) methods independently. When combined, these techniques achieve up to a 4.08x speedup across various benchmarks and LLM architectures, significantly surpassing existing training-free approaches. The implementation of S PECTRA is publicly available.",
      "arxiv_url": "https://www.semanticscholar.org/paper/182c72cf74b9f47e352b699bd4bbb53b12088cda",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "183d271e7dea50b57f756da03a4fc9373894d4ed",
      "title": "-Stance: A Large-Scale Real World Dataset of Stances in Legal Argumentation",
      "authors": [
        "Ankita Gupta",
        "Douglas Rice",
        "Brendan T. O'Connor"
      ],
      "abstract": "We present 𝛿 - Stance , a large-scale dataset of stances involved in legal argumentation. 𝛿 - Stance contains stance-annotated argument pairs, semi-automatically mined from millions of examples of U.S. judges citing precedent in context using citation signals. The dataset aims to facilitate work on the legal argument stance classification task , which involves assessing whether a case summary strengthens or weak-ens a legal argument (polarity) and to what extent (intensity). To assess the complexity of this task, we evaluate various existing NLP methods, including zero-shot prompting proprietary large language models (LLMs), and supervised fine-tuning of smaller open-weight language models (LMs) on 𝛿 - Stance . Our findings reveal that although prompting proprietary LLMs can help predict stance polarity, supervised model fine-tuning on 𝛿 - Stance is necessary to distinguish intensity. We further find that alternative strategies such as domain-specific pretraining and zero-shot prompting using masked LMs remain insufficient. Beyond our dataset’s utility for the legal domain, we further find that fine-tuning small LMs on 𝛿 - Stance improves their performance in other domains. Finally, we study how temporal changes in signal definition can impact model performance, highlighting the importance of careful data curation for down-stream tasks by considering the historical and sociocultural context. We publish the associated dataset 1 to foster further research on legal argument reasoning.",
      "arxiv_url": "https://www.semanticscholar.org/paper/183d271e7dea50b57f756da03a4fc9373894d4ed",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.18491",
      "title": "MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering",
      "authors": [
        "Shuo Yang",
        "Siwen Luo",
        "S. Han",
        "Eduard Hovy"
      ],
      "abstract": "Visual Question Answering (VQA) requires reasoning across visual and textual modalities, yet Large Vision-Language Models (LVLMs) often lack integrated commonsense knowledge, limiting their robustness in real-world scenarios. To address this, we introduce MAGIC-VQA, a novel framework that enhances VQA by systematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs a three-stage process: (1) Explicit Knowledge Integration from external sources, (2) By-Type Post-Processing for contextual refinement, and (3) Implicit Knowledge Augmentation using a Graph Neural Network (GNN) for structured reasoning. While GNNs bring greater depth to structured inference, they enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key gap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating the need for extensive pre-training or complex prompt tuning. Our framework achieves state-of-the-art performance on benchmark datasets, significantly improving commonsense reasoning in VQA.",
      "arxiv_url": "https://arxiv.org/abs/2503.18491",
      "pdf_url": "https://arxiv.org/pdf/2503.18491",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.02503",
      "title": "Continual Gradient Low-Rank Projection Fine-Tuning for LLMs",
      "authors": [
        "Chenxu Wang",
        "Yilin Lyu",
        "Zicheng Sun",
        "Liping Jing"
      ],
      "abstract": "Continual fine-tuning of Large Language Models (LLMs) is hampered by the trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA) offers efficiency but constrains the model's ability to learn new tasks and transfer knowledge due to its low-rank nature and reliance on explicit parameter constraints. We propose GORP (Gradient LOw Rank Projection) for Continual Learning, a novel training strategy that overcomes these limitations by synergistically combining full and low-rank parameters and jointly updating within a unified low-rank gradient subspace. GORP expands the optimization space while preserving efficiency and mitigating catastrophic forgetting. Extensive experiments on continual learning benchmarks demonstrate GORP's superior performance compared to existing state-of-the-art approaches. Code is available at https://github.com/Wcxwcxw/GORP.",
      "arxiv_url": "https://arxiv.org/abs/2507.02503",
      "pdf_url": "https://arxiv.org/pdf/2507.02503",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11276",
      "title": "The Rotary Position Embedding May Cause Dimension Inefficiency in Attention Heads for Long-Distance Retrieval",
      "authors": [
        "Ting-Rui Chiang",
        "Dani Yogatama"
      ],
      "abstract": "The Rotary Position Embedding (RoPE) is widely used in the attention heads of many large language models (LLM). It rotates dimensions in the query and the key vectors by different angles according to their positions in the input sequence. For long context modeling, the range of positions may vary a lot, and thus RoPE rotates some dimensions by a great range of angles. We hypothesize that the wide range of rotation angles may prevent LLMs from utilizing those dimensions. To validate this hypothesis, we present a controlled experiment showing that applying RoPE causes low utility of certain dimensions. Our analyses on three LLMs also indicate that these dimensions do not help LLMs do long-context question answering.",
      "arxiv_url": "https://arxiv.org/abs/2502.11276",
      "pdf_url": "https://arxiv.org/pdf/2502.11276",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12859",
      "title": "Re-identification of De-identified Documents with Autoregressive Infilling",
      "authors": [
        "Lucas Georges Gabriel Charpentier",
        "Pierre Lison"
      ],
      "abstract": "Documents revealing sensitive information about individuals must typically be de-identified. This de-identification is often done by masking all mentions of personally identifiable information (PII), thereby making it more difficult to uncover the identity of the person(s) in question. To investigate the robustness of de-identification methods, we present a novel, RAG-inspired approach that attempts the reverse process of re-identification based on a database of documents representing background knowledge. Given a text in which personal identifiers have been masked, the re-identification proceeds in two steps. A retriever first selects from the background knowledge passages deemed relevant for the re-identification. Those passages are then provided to an infilling model which seeks to infer the original content of each text span. This process is repeated until all masked spans are replaced. We evaluate the re-identification on three datasets (Wikipedia biographies, court rulings and clinical notes). Results show that (1) as many as 80% of de-identified text spans can be successfully recovered and (2) the re-identification accuracy increases along with the level of background knowledge.",
      "arxiv_url": "https://arxiv.org/abs/2505.12859",
      "pdf_url": "https://arxiv.org/pdf/2505.12859",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.08409",
      "title": "FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion",
      "authors": [
        "Fred Xu",
        "Song Jiang",
        "Zijie Huang",
        "Xiao Luo",
        "Shichang Zhang",
        "Yuanzhou Chen",
        "Yizhou Sun"
      ],
      "abstract": "Taxonomy Expansion, which models complex concepts and their relations, can be formulated as a set representation learning task. The generalization of set, fuzzy set, incorporates uncertainty and measures the information within a semantic concept, making it suitable for concept modeling. Existing works usually model sets as vectors or geometric objects such as boxes, which are not closed under set operations. In this work, we propose a sound and efficient formulation of set representation learning based on its volume approximation as a fuzzy set. The resulting embedding framework, Fuzzy Set Embedding (FUSE), satisfies all set operations and compactly approximates the underlying fuzzy set, hence preserving information while being efficient to learn, relying on minimum neural architecture. We empirically demonstrate the power of FUSE on the task of taxonomy expansion, where FUSE achieves remarkable improvements up to 23% compared with existing baselines. Our work marks the first attempt to understand and efficiently compute the embeddings of fuzzy sets.",
      "arxiv_url": "https://arxiv.org/abs/2506.08409",
      "pdf_url": "https://arxiv.org/pdf/2506.08409",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19361",
      "title": "Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?",
      "authors": [
        "Yancheng He",
        "Shilong Li",
        "Jiaheng Liu",
        "Weixun Wang",
        "Xingyuan Bu",
        "Ge Zhang",
        "Z. Peng",
        "Zhaoxiang Zhang",
        "Zhicheng Zheng",
        "Wenbo Su",
        "Bo Zheng"
      ],
      "abstract": "Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique abilities of existing LLMs on these long CoTs, we introduce the DeltaBench, including the generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the ability to detect errors in long CoT reasoning. Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models. Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process, which aims to investigate the boundaries and limitations of existing PRMs and critic models. Finally, we hope that DeltaBench could guide developers to better understand the long CoT reasoning abilities of their models.",
      "arxiv_url": "https://arxiv.org/abs/2502.19361",
      "pdf_url": "https://arxiv.org/pdf/2502.19361",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "197ebd8895aeb5cb06e0a4f93483210c7f20d53b",
      "title": "A Parameter-Efficient and Fine-Grained Prompt Learning for Vision-Language Models",
      "authors": [
        "Yongbin Guo",
        "Shuzhen Li",
        "Zhulin Liu",
        "Tong Zhang",
        "C. L. P. Chen"
      ],
      "abstract": "Current vision-language models (VLMs) understand complex vision-text tasks by extracting overall semantic information from large-scale cross-modal associations. However, extracting from large-scale cross-modal associations often smooths out semantic details and requires large computations, limiting multi-modal fine-grained understanding performance and efficiency. To address this issue, this paper proposes a detail-oriented prompt learning (DoPL) method for vision-language models to implement fine-grained multi-modal semantic alignment with merely 0.25M trainable parameters. According to the low-entropy information concentration theory, DoPL explores shared interest tokens from text-vision correlations and transforms them into alignment weights to enhance text prompt and vision prompt via detail-oriented prompt generation. It effectively guides the current frozen layer to extract fine-grained text-vision alignment cues. Furthermore, DoPL constructs detail-oriented prompt generation for each frozen layer to implement layer-by-layer localization of fine-grained semantic alignment, achieving precise understanding in complex vision-text tasks. DoPL performs well in parameter-efficient fine-grained semantic alignment with only 0.12% tunable parameters for vision-language models. The state-of-the-art results over the previous parameter-efficient fine-tuning methods and full fine-tuning approaches on six benchmarks demonstrate the effectiveness and efficiency of DoPL in complex multi-modal tasks.",
      "arxiv_url": "https://www.semanticscholar.org/paper/197ebd8895aeb5cb06e0a4f93483210c7f20d53b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01682",
      "title": "GRNFormer: A Biologically-Guided Framework for Integrating Gene Regulatory Networks into RNA Foundation Models",
      "authors": [
        "Mufan Qiu",
        "Xinyu Hu",
        "Fengwei Zhan",
        "Sukwon Yun",
        "Jie Peng",
        "Ruichen Zhang",
        "B. Kailkhura",
        "Jiekun Yang",
        "Tianlong Chen"
      ],
      "abstract": "Foundation models for single-cell RNA sequencing (scRNA-seq) have shown promising capabilities in capturing gene expression patterns. However, current approaches face critical limitations: they ignore biological prior knowledge encoded in gene regulatory relationships and fail to leverage multi-omics signals that could provide complementary regulatory insights. In this paper, we propose GRNFormer, a new framework that systematically integrates multi-scale Gene Regulatory Networks (GRNs) inferred from multi-omics data into RNA foundation model training. Our framework introduces two key innovations. First, we introduce a pipeline for constructing hierarchical GRNs that capture regulatory relationships at both cell-type-specific and cell-specific resolutions. Second, we design a structure-aware integration framework that addresses the information asymmetry in GRNs through two technical advances: (1) A graph topological adapter using multi-head cross-attention to weight regulatory relationships dynamically, and (2) a novel edge perturbation strategy that perturb GRNs with biologically-informed co-expression links to augment graph neural network training. Comprehensive experiments have been conducted on three representative downstream tasks across multiple model architectures to demonstrate the effectiveness of GRNFormer. It achieves consistent improvements over state-of-the-art (SoTA) baselines: $3.6\\%$ increase in drug response prediction correlation, $9.6\\%$ improvement in single-cell drug classification AUC, and $1.1\\%$ average gain in gene perturbation prediction accuracy.",
      "arxiv_url": "https://arxiv.org/abs/2503.01682",
      "pdf_url": "https://arxiv.org/pdf/2503.01682",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12057",
      "title": "Culture is Not Trivia: Sociocultural Theory for Cultural NLP",
      "authors": [
        "Naitian Zhou",
        "David Bamman",
        "Isaac L. Bleaman"
      ],
      "abstract": "The field of cultural NLP has recently experienced rapid growth, driven by a pressing need to ensure that language technologies are effective and safe across a pluralistic user base. This work has largely progressed without a shared conception of culture, instead choosing to rely on a wide array of cultural proxies. However, this leads to a number of recurring limitations: coarse national boundaries fail to capture nuanced differences that lay within them, limited coverage restricts datasets to only a subset of usually highly-represented cultures, and a lack of dynamicity results in static cultural benchmarks that do not change as culture evolves. In this position paper, we argue that these methodological limitations are symptomatic of a theoretical gap. We draw on a well-developed theory of culture from sociocultural linguistics to fill this gap by 1) demonstrating in a case study how it can clarify methodological constraints and affordances, 2) offering theoretically-motivated paths forward to achieving cultural competence, and 3) arguing that localization is a more useful framing for the goals of much current work in cultural NLP.",
      "arxiv_url": "https://arxiv.org/abs/2502.12057",
      "pdf_url": "https://arxiv.org/pdf/2502.12057",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "19c97a6561680782f0fc8150ea1fbe7b6f14b07c",
      "title": "First-AID: the first Annotation Interface for grounded Dialogues",
      "authors": [
        "Stefano Menini",
        "Daniel Russo",
        "Alessio Palmero Aprosio",
        "Marco Guerini"
      ],
      "abstract": "The swift advancement of Large Language Models (LLMs) has led to their widespread use across various tasks and domains, demonstrating remarkable generalization capabilities. However, achieving optimal performance in specialized tasks often requires fine-tuning LLMs with task-specific resources. The creation of high-quality, human-annotated datasets for this purpose is challenging due to financial constraints and the limited availability of human experts. To address these limitations, we propose First-AID, a novel human-in-the-loop (HITL) data collection framework for the knowledge-driven generation of synthetic di-alogues using LLM prompting. In particular, our framework implements different strategies of data collection that require different user intervention during dialogue generation to reduce post-editing efforts and enhance the quality of generated dialogues. We also evaluated First-AID on misinformation and hate countering dialogues collection, demonstrating (1) its potential for efficient and high-quality data generation and (2) its adaptability to different practical constraints thanks to the three data collection strategies.",
      "arxiv_url": "https://www.semanticscholar.org/paper/19c97a6561680782f0fc8150ea1fbe7b6f14b07c",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.05952",
      "title": "Scalable Vision Language Model Training via High Quality Data Curation",
      "authors": [
        "Hongyuan Dong",
        "Zijian Kang",
        "Weijie Yin",
        "Xiao Liang",
        "Chao Feng",
        "Jiao Ran"
      ],
      "abstract": "In this paper, we introduce SAIL-VL (ScAlable Vision Language Model TraIning via High QuaLity Data Curation), an open-source vision language model (VLM) series achieving state-of-the-art (SOTA) performance in 2B and 8B parameters. The following three key improvements contribute to SAIL-VL's leading performance: (1) Scalable high-quality visual understanding data construction: We implement a data construction pipeline to enable hundred-million-scale high-quality recaption data annotation. The resulted dataset SAIL-Caption is validated to be of the highest data quality compared with opensource datasets. (2) Scalable Pretraining with High-Quality Visual Understanding Data: We scale SAIL-VL's pretraining budget up to 655B tokens and show that even a 2B VLM benefits from scaled up training data sizes, exhibiting logarithmic data size scaling laws in benchmark performance. (3) Scalable SFT via data quantity and complexity scaling: We curate a high-quality SFT dataset collection with leading data quantity scaling effectiveness and demonstrate that training with progressively higher-complexity data surpasses baseline one-stage training by a large margin. SAIL-VL series models achieve the highest average score in 18 widely used VLM benchmarks in our evaluation, with the 2B model takes the top position over VLMs of comparable sizes on OpenCompass 2024 (https://rank.opencompass.org.cn/leaderboard-multimodal), demonstrating robust visual comprehension abilities. SAIL-VL series models are released at HuggingFace (https://huggingface.co/BytedanceDouyinContent).",
      "arxiv_url": "https://arxiv.org/abs/2501.05952",
      "pdf_url": "https://arxiv.org/pdf/2501.05952",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23630",
      "title": "GeNRe: A French Gender-Neutral Rewriting System Using Collective Nouns",
      "authors": [
        "Enzo Doyen",
        "Amalia Todirascu-Courtier"
      ],
      "abstract": "A significant portion of the textual data used in the field of Natural Language Processing (NLP) exhibits gender biases, particularly due to the use of masculine generics (masculine words that are supposed to refer to mixed groups of men and women), which can perpetuate and amplify stereotypes. Gender rewriting, an NLP task that involves automatically detecting and replacing gendered forms with neutral or opposite forms (e.g., from masculine to feminine), can be employed to mitigate these biases. While such systems have been developed in a number of languages (English, Arabic, Portuguese, German, French), automatic use of gender neutralization techniques (as opposed to inclusive or gender-switching techniques) has only been studied for English. This paper presents GeNRe, the very first French gender-neutral rewriting system using collective nouns, which are gender-fixed in French. We introduce a rule-based system (RBS) tailored for the French language alongside two fine-tuned language models trained on data generated by our RBS. We also explore the use of instruct-based models to enhance the performance of our other systems and find that Claude 3 Opus combined with our dictionary achieves results close to our RBS. Through this contribution, we hope to promote the advancement of gender bias mitigation techniques in NLP for French.",
      "arxiv_url": "https://arxiv.org/abs/2505.23630",
      "pdf_url": "https://arxiv.org/pdf/2505.23630",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12494",
      "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented Generation",
      "authors": [
        "Zhuocheng Zhang",
        "Yang Feng",
        "Min Zhang"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large language model applications, with numerous existing frameworks offering a wide range of functionalities to facilitate the development of RAG systems. However, we have identified several persistent challenges in these frameworks, including difficulties in algorithm reproduction and sharing, lack of new techniques, and high system overhead. To address these limitations, we introduce \\textbf{FlexRAG}, an open-source framework specifically designed for research and prototyping. FlexRAG supports text-based, multimodal, and network-based RAG, providing comprehensive lifecycle support alongside efficient asynchronous processing and persistent caching capabilities. By offering a robust and flexible solution, FlexRAG enables researchers to rapidly develop, deploy, and share advanced RAG systems. Our toolkit and resources are available at \\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
      "arxiv_url": "https://arxiv.org/abs/2506.12494",
      "pdf_url": "https://arxiv.org/pdf/2506.12494",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-06-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.02398",
      "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for Long Behavior Sequence",
      "authors": [
        "Yunxiao Shi",
        "Wujiang Xu",
        "Zeqi Zhang",
        "Xing Zi",
        "Qiang Wu",
        "Min Xu"
      ],
      "abstract": "User profile embedded in the prompt template of personalized recommendation agents play a crucial role in shaping their decision-making process. High-quality user profiles are essential for aligning agent behavior with real user interests. Typically, these profiles are constructed by leveraging LLMs for user profile modeling (LLM-UM). However, this process faces several challenges: (1) LLMs struggle with long user behaviors due to context length limitations and performance degradation. (2) Existing methods often extract only partial segments from full historical behavior sequence, inevitably discarding diverse user interests embedded in the omitted content, leading to incomplete modeling and suboptimal profiling. (3) User profiling is often tightly coupled with the inference context, requiring online processing, which introduces significant latency overhead. In this paper, we propose PersonaX, an agent-agnostic LLM-UM framework to address these challenges. It augments downstream recommendation agents to achieve better recommendation performance and inference efficiency. PersonaX (a) segments complete historical behaviors into clustered groups, (b) selects multiple sub behavior sequences (SBS) with a balance of prototypicality and diversity to form a high quality core set, (c) performs offline multi-persona profiling to capture diverse user interests and generate fine grained, cached textual personas, and (d) decouples user profiling from online inference, enabling profile retrieval instead of real time generation. Extensive experiments demonstrate its effectiveness: using only 30 to 50% of behavioral data (sequence length 480), PersonaX enhances AgentCF by 3 to 11% and Agent4Rec by 10 to 50%. As a scalable and model-agnostic LLM-UM solution, PersonaX sets a new benchmark in scalable user modeling.",
      "arxiv_url": "https://arxiv.org/abs/2503.02398",
      "pdf_url": "https://arxiv.org/pdf/2503.02398",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-03-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04618",
      "title": "Better Process Supervision with Bi-directional Rewarding Signals",
      "authors": [
        "Wenxiang Chen",
        "Wei He",
        "Zhiheng Xi",
        "Honglin Guo",
        "Boyang Hong",
        "Jiazheng Zhang",
        "Rui Zheng",
        "Nijun Li",
        "Tao Gui",
        "Yun Li",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "Process supervision, i.e., evaluating each step, is critical for complex large language model (LLM) reasoning and test-time searching with increased inference compute. Existing approaches, represented by process reward models (PRMs), primarily focus on rewarding signals up to the current step, exhibiting a one-directional nature and lacking a mechanism to model the distance to the final target. To address this problem, we draw inspiration from the A* algorithm, which states that an effective supervisory signal should simultaneously consider the incurred cost and the estimated cost for reaching the target. Building on this key insight, we introduce BiRM, a novel process supervision model that not only evaluates the correctness of previous steps but also models the probability of future success. We conduct extensive experiments on mathematical reasoning tasks and demonstrate that BiRM provides more precise evaluations of LLM reasoning steps, achieving an improvement of 3.1% on Gaokao2023 over PRM under the Best-of-N sampling method. Besides, in search-based strategies, BiRM provides more comprehensive guidance and outperforms ORM by 5.0% and PRM by 3.8% respectively on MATH-500.",
      "arxiv_url": "https://arxiv.org/abs/2503.04618",
      "pdf_url": "https://arxiv.org/pdf/2503.04618",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "1a7451079158d57c72e71e35457d90acde9f00a1",
      "title": "SARA: Salience-Aware Reinforced Adaptive Decoding for Large Language Models in Abstractive Summarization",
      "authors": [
        "Nayu Liu",
        "Junnan Zhu",
        "Yiming Ma",
        "Zhicong Lu",
        "Wenlei Xu",
        "Yong Yang",
        "Jiang Zhong",
        "Kaiwen Wei"
      ],
      "abstract": "LLMs have improved the fluency and informativeness of abstractive summarization but remain prone to hallucinations, where generated content deviates from the source document. Recent PMI decoding strategies mitigate over-reliance on prior knowledge by comparing output probabilities with and without source documents, effectively enhancing contextual utilization and improving faithfulness. However, existing strategies often neglect the explicit use of salient contextual information and rely on static hyperparameters to fix the balance between contextual and prior knowledge, limiting their flexibility. In this work, we propose S alience-A ware R einforced A daptive decoding (SARA), which incorporates salient information and allows the model to adaptively determine reliance on the source document’s context, salient context, and the model’s prior knowledge based on pointwise mutual information. Moreover, a tokenwise adaptive decoding mechanism via reinforcement learning is proposed in SARA to dynamically adjust the contributions of context and prior knowledge at each decoding timestep. Experiments on CNN/DM, WikiHow, and NYT50 datasets show that SARA consistently improves the quality and faithfulness of summaries across various LLM backbones without modifying their weights.",
      "arxiv_url": "https://www.semanticscholar.org/paper/1a7451079158d57c72e71e35457d90acde9f00a1",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.02295",
      "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection",
      "authors": [
        "Yachao Zhao",
        "Bo Wang",
        "Yan Wang"
      ],
      "abstract": "Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content. While extensive research has investigated biases in LLMs, prior work has predominantly focused on explicit bias, with minimal attention to implicit bias and the relation between these two forms of bias. This paper presents a systematic framework grounded in social psychology theories to investigate and compare explicit and implicit biases in LLMs. We propose a novel self-reflection-based evaluation framework that operates in two phases: first measuring implicit bias through simulated psychological assessment methods, then evaluating explicit bias by prompting LLMs to analyze their own generated content. Through extensive experiments on advanced LLMs across multiple social dimensions, we demonstrate that LLMs exhibit a substantial inconsistency between explicit and implicit biases: while explicit bias manifests as mild stereotypes, implicit bias exhibits strong stereotypes. We further investigate the underlying factors contributing to this explicit-implicit bias inconsistency, examining the effects of training data scale, model size, and alignment techniques. Experimental results indicate that while explicit bias declines with increased training data and model size, implicit bias exhibits a contrasting upward trend. Moreover, contemporary alignment methods effectively suppress explicit bias but show limited efficacy in mitigating implicit bias.",
      "arxiv_url": "https://arxiv.org/abs/2501.02295",
      "pdf_url": "https://arxiv.org/pdf/2501.02295",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.15675",
      "title": "P3: Prompts Promote Prompting",
      "authors": [
        "Xinyu Zhang",
        "Yuanquan Hu",
        "Fangchao Liu",
        "Zhicheng Dou"
      ],
      "abstract": "Current large language model (LLM) applications often employ multi-component prompts, comprising both system and user prompts, to guide model behaviors. While recent advancements have demonstrated the efficacy of automatically optimizing either the system or user prompt to boost performance, such unilateral approaches often yield suboptimal outcomes due to the interdependent nature of these components. In this work, we introduce P3, a novel self-improvement framework that concurrently optimizes both system and user prompts through an iterative process. The offline optimized prompts are further leveraged to promote online prompting by performing query-dependent prompt optimization. Extensive experiments on general tasks (e.g., Arena-hard and Alpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3 achieves superior performance in the realm of automatic prompt optimization. Our results highlight the effectiveness of a holistic optimization strategy in enhancing LLM performance across diverse domains.",
      "arxiv_url": "https://arxiv.org/abs/2507.15675",
      "pdf_url": "https://arxiv.org/pdf/2507.15675",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03994",
      "title": "Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era",
      "authors": [
        "Dan Oneaţă",
        "Desmond Elliott",
        "Stella Frank"
      ],
      "abstract": "Human learning and conceptual representation is grounded in sensorimotor experience, in contrast to state-of-the-art foundation models. In this paper, we investigate how well such large-scale models, trained on vast quantities of data, represent the semantic feature norms of concrete object concepts, e.g. a ROSE is red, smells sweet, and is a flower. More specifically, we use probing tasks to test which properties of objects these models are aware of. We evaluate image encoders trained on image data alone, as well as multimodally-trained image encoders and language-only models, on predicting an extended denser version of the classic McRae norms and the newer Binder dataset of attribute ratings. We find that multimodal image encoders slightly outperform language-only approaches, and that image-only encoders perform comparably to the language models, even on non-visual attributes that are classified as\"encyclopedic\"or\"function\". These results offer new insights into what can be learned from pure unimodal learning, and the complementarity of the modalities.",
      "arxiv_url": "https://arxiv.org/abs/2506.03994",
      "pdf_url": "https://arxiv.org/pdf/2506.03994",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "1b49e8199795a4a6d95a7840fc0b06d6d5e3917b",
      "title": "NeuronMerge: Merging Models via Functional Neuron Groups",
      "authors": [
        "Wangyun Gu",
        "Qianghua Gao",
        "Li-Xin Zhang",
        "Xu Shen",
        "Jieping Ye"
      ],
      "abstract": "Model merging techniques like task arithmetic, which combines model parameters through weighted averaging, have proven effective. However, the success of task arithmetic relies on the linearity between model weight differences and output feature changes, which is often lacking in conventional fine-tuned models. In this work, we employ neuron description methods to analyze and classify neurons based on their functionalities. We theoretically demonstrate that grouping Multi-Layer Perceptron (MLP) neurons by functionality enhances model linearity. Building on this, we propose a neuron-based task arithmetic merging method that consistently improves performance across various tasks and model scales. Our approach is complementary to existing merging techniques, achieving superior results in merging models fine-tuned on fundamental tasks like Math, Code and Translation.",
      "arxiv_url": "https://www.semanticscholar.org/paper/1b49e8199795a4a6d95a7840fc0b06d6d5e3917b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "1b5b80c8054adfe13092c8a8a4b523e8d6260db9",
      "title": "Impartial Multi-task Representation Learning via Variance-invariant Probabilistic Decoding",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "abstract": "Multi-task learning (MTL) enhances efficiency by sharing representations across tasks, but task dissimilarities often cause partial learning, where some tasks dominate while others are neglected. Existing methods mainly focus on balancing loss or gradients but fail to fundamentally address this issue due to the representation discrepancy in latent space. In this paper, we propose variance-invariant probabilistic decoding for multi-task learning (VIP-MTL), a framework that ensures impartial learning by harmonizing representation spaces across tasks. VIP-MTL decodes shared representations into task-specific probabilistic distributions and applies variance normalization to constrain these distributions to a consistent scale. Experiments on two language benchmarks show that VIP-MTL outperforms 12 representative methods under the same multi-task settings, especially in heterogeneous task combinations and data-constrained scenarios. Further analysis shows that VIP-MTL is robust to sampling distributions, efficient on optimization process, and scale-invariant to task losses. Additionally, the learned task-specific representations are more informative, enhancing the language understanding abilities of pre-trained language models under the multi-task paradigm.",
      "arxiv_url": "https://www.semanticscholar.org/paper/1b5b80c8054adfe13092c8a8a4b523e8d6260db9",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.09265",
      "title": "Perspective Transition of Large Language Models for Solving Subjective Tasks",
      "authors": [
        "Xiaolong Wang",
        "Yuan Zhang",
        "Ziyue Wang",
        "Yuzhuang Xu",
        "Fuwen Luo",
        "Yile Wang",
        "Peng Li",
        "Yang Liu"
      ],
      "abstract": "Large language models (LLMs) have revolutionized the field of natural language processing, enabling remarkable progress in various tasks. Different from objective tasks such as commonsense reasoning and arithmetic question-answering, the performance of LLMs on subjective tasks is still limited, where the perspective on the specific problem plays crucial roles for better interpreting the context and giving proper response. For example, in certain scenarios, LLMs may perform better when answering from an expert role perspective, potentially eliciting their relevant domain knowledge. In contrast, in some scenarios, LLMs may provide more accurate responses when answering from a third-person standpoint, enabling a more comprehensive understanding of the problem and potentially mitigating inherent biases. In this paper, we propose Reasoning through Perspective Transition (RPT), a method based on in-context learning that enables LLMs to dynamically select among direct, role, and third-person perspectives for the best way to solve corresponding subjective problem. Through extensive experiments on totally 12 subjective tasks by using both closed-source and open-source LLMs including GPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single fixed perspective based methods such as chain-of-thought prompting and expert prompting, highlights the intricate ways that LLMs can adapt their perspectives to provide nuanced and contextually appropriate responses for different problems.",
      "arxiv_url": "https://arxiv.org/abs/2501.09265",
      "pdf_url": "https://arxiv.org/pdf/2501.09265",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "1b78bedf7b1bd852d89170ca559ff19505bdc07f",
      "title": "Design Choices for Extending the Context Length of Visual Language Models",
      "authors": [
        "Mukai Li",
        "Lei Li",
        "Shansan Gong",
        "Qi Liu"
      ],
      "abstract": "Visual Language Models (VLMs) demonstrate impressive capabilities in processing multi-modal inputs, yet applications such as visual agents, which require handling multiple images and high-resolution videos, demand enhanced long-range modeling. Moreover, existing open-source VLMs lack systematic exploration into extending their context length, and commercial models often provide limited details. To tackle this, we aim to establish an effective so-lution that enhances long context performance of VLMs while preserving their capacities in short context scenarios. Towards this goal, we make the best design choice through extensive experiment settings from data curation to context window extending and utilizing: (1) we analyze data sources and length distributions to construct ETVLM - a data recipe to balance the performance across scenarios; (2) we examine existing position extending meth-ods, identify their limitations and propose M-RoPE++ as an enhanced approach; we also choose to solely instruction-tune the backbone with mixed-source data; (3) we discuss how to better utilize extended context windows and propose hybrid-resolution training. Built on the Qwen-VL series model, we propose G I - RAFFE , which is effectively extended to 128K lengths. Evaluated on extensive long context VLM benchmarks such as VideoMME and Viusal Haystacks, our G IRAFFE achieves state-of-the-art performance among similarly sized open-source long VLMs and is competitive with commercial model GPT-4V. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/1b78bedf7b1bd852d89170ca559ff19505bdc07f",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14361",
      "title": "Retrieval-Augmented Process Reward Model for Generalizable Mathematical Reasoning",
      "authors": [
        "Jiachen Zhu",
        "Congmin Zheng",
        "Jianghao Lin",
        "Kounianhua Du",
        "Ying Wen",
        "Yong Yu",
        "Jun Wang",
        "Weinan Zhang"
      ],
      "abstract": "While large language models (LLMs) have significantly advanced mathematical reasoning, Process Reward Models (PRMs) have been developed to evaluate the logical validity of reasoning steps. However, PRMs still struggle with out-of-distribution (OOD) challenges. This paper identifies key OOD issues, including step OOD, caused by differences in reasoning patterns across model types and sizes, and question OOD, which arises from dataset shifts between training data and real-world problems. To address these issues, we introduce Retrieval-Augmented Process Reward Model (RetrievalPRM), a novel framework designed to tackle these OOD issues. By utilizing a two-stage retrieval-enhanced mechanism, RetrievalPRM retrieves semantically similar questions and steps as a warmup, enhancing PRM's ability to evaluate target steps and improving generalization and reasoning consistency across different models and problem types. Our extensive experiments demonstrate that RetrievalPRM outperforms existing baselines across multiple real-world datasets. Our open-source contributions include a retrieval-enhanced dataset, a tuning framework for PRM training, and the RetrievalPRM model, establishing a new standard for PRM performance.",
      "arxiv_url": "https://arxiv.org/abs/2502.14361",
      "pdf_url": "https://arxiv.org/pdf/2502.14361",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.14434",
      "title": "Unifying Streaming and Non-streaming Zipformer-based ASR",
      "authors": [
        "Bidisha Sharma",
        "K. Durai",
        "Shankar Venkatesan",
        "Jeena Prakash",
        "Shashi Kumar",
        "Malolan Chetlur",
        "A. Stolcke"
      ],
      "abstract": "There has been increasing interest in unifying streaming and non-streaming automatic speech recognition (ASR) models to reduce development, training, and deployment costs. We present a unified framework that trains a single end-to-end ASR model for both streaming and non-streaming applications, leveraging future context information. We propose to use dynamic right-context through the chunked attention masking in the training of zipformer-based ASR models. We demonstrate that using right-context is more effective in zipformer models compared to other conformer models due to its multi-scale nature. We analyze the effect of varying the number of right-context frames on accuracy and latency of the streaming ASR models. We use Librispeech and large in-house conversational datasets to train different versions of streaming and non-streaming models and evaluate them in a production grade server-client setup across diverse testsets of different domains. The proposed strategy reduces word error by relative 7.9\\% with a small degradation in user-perceived latency. By adding more right-context frames, we are able to achieve streaming performance close to that of non-streaming models. Our approach also allows flexible control of the latency-accuracy tradeoff according to customers requirements.",
      "arxiv_url": "https://arxiv.org/abs/2506.14434",
      "pdf_url": "https://arxiv.org/pdf/2506.14434",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.19298",
      "title": "AndroidGen: Building an Android Language Agent under Data Scarcity",
      "authors": [
        "Hanyu Lai",
        "Junjie Gao",
        "Xiao Liu",
        "Yifan Xu",
        "Shudan Zhang",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "abstract": "Large language models have opened up a world of possibilities for various NLP tasks, sparking optimism for the future. Despite their potential, LLMs have yet to be widely used as agents on real mobile devices. The main challenge is the need for high-quality data sources. Time constraints and labor intensity often hinder human annotation. On the other hand, existing LLMs exhibit inadequate completion rates and need a robust data filtration strategy. Given these challenges, we develop a framework called AndroidGen to enhance the capabilities of LLM-based agents under data scarcity. In addition, we leverage AndroidGen to collect trajectories given human tasks and train open-source LLMs on these trajectories to develop an open-source mobile agent without manually labeled trajectories. We extensively evaluate AndroidGen with AndroidWorld, AitW, and various popular applications, demonstrating its improvements and revealing potential areas for future improvement. Code, model, and data are available at https://github.com/THUDM/AndroidGen.",
      "arxiv_url": "https://arxiv.org/abs/2504.19298",
      "pdf_url": "https://arxiv.org/pdf/2504.19298",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06238",
      "title": "Explaining Matters: Leveraging Definitions and Semantic Expansion for Sexism Detection",
      "authors": [
        "Sahrish Khan",
        "Arshad Jhumka",
        "Gabriele Pergola"
      ],
      "abstract": "The detection of sexism in online content remains an open problem, as harmful language disproportionately affects women and marginalized groups. While automated systems for sexism detection have been developed, they still face two key challenges: data sparsity and the nuanced nature of sexist language. Even in large, well-curated datasets like the Explainable Detection of Online Sexism (EDOS), severe class imbalance hinders model generalization. Additionally, the overlapping and ambiguous boundaries of fine-grained categories introduce substantial annotator disagreement, reflecting the difficulty of interpreting nuanced expressions of sexism. To address these challenges, we propose two prompt-based data augmentation techniques: Definition-based Data Augmentation (DDA), which leverages category-specific definitions to generate semantically-aligned synthetic examples, and Contextual Semantic Expansion (CSE), which targets systematic model errors by enriching examples with task-specific semantic features. To further improve reliability in fine-grained classification, we introduce an ensemble strategy that resolves prediction ties by aggregating complementary perspectives from multiple language models. Our experimental evaluation on the EDOS dataset demonstrates state-of-the-art performance across all tasks, with notable improvements of macro F1 by 1.5 points for binary classification (Task A) and 4.1 points for fine-grained classification (Task C).",
      "arxiv_url": "https://arxiv.org/abs/2506.06238",
      "pdf_url": "https://arxiv.org/pdf/2506.06238",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.00691",
      "title": "To Code or not to Code? Adaptive Tool Integration for Math Language Models via Expectation-Maximization",
      "authors": [
        "Haozhe Wang",
        "Long Li",
        "Chao Qu",
        "Weidi Xu",
        "Fengming Zhu",
        "Wei Chu",
        "Fangzhen Lin"
      ],
      "abstract": "Recent advances in mathematical problem-solving with language models (LMs) integrate chain-of-thought (CoT) reasoning and code execution to harness their complementary strengths. However, existing hybrid frameworks exhibit a critical limitation: they depend on externally dictated instructions or rigid code-integration templates, lacking metacognitive awareness -- the capacity to dynamically evaluate intrinsic capabilities and autonomously determine when and how to integrate tools. This rigidity motivates our study of autonomous code integration, enabling models to adapt tool-usage strategies as their reasoning abilities evolve during training. While reinforcement learning (RL) shows promise for boosting LLM reasoning at scale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning autonomous code integration due to inadequate exploration of the vast combinatorial space of CoT-code interleaving patterns. To address this challenge, we propose a novel Expectation-Maximization (EM) framework that synergizes structured exploration (E-step) with off-policy RL optimization (M-step), creating a self-reinforcing cycle between metacognitive tool-use decisions and evolving capabilities. Experiments reveal our method achieves superior results through improved exploration. Notably, our 7B model improves over 11% on MATH500 and 9.4% on AIME without o1-like CoT.",
      "arxiv_url": "https://arxiv.org/abs/2502.00691",
      "pdf_url": "https://arxiv.org/pdf/2502.00691",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.06582",
      "title": "ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting",
      "authors": [
        "Steven H. Wang",
        "Maksim Zubkov",
        "Kexin Fan",
        "Sarah Harrell",
        "Yuyang Sun",
        "Wei Chen",
        "Andreas Lindhardt Plesner",
        "R. Wattenhofer"
      ],
      "abstract": "Information retrieval, specifically contract clause retrieval, is foundational to contract drafting because lawyers rarely draft contracts from scratch; instead, they locate and revise the most relevant precedent. We introduce the Atticus Clause Retrieval Dataset (ACORD), the first retrieval benchmark for contract drafting fully annotated by experts. ACORD focuses on complex contract clauses such as Limitation of Liability, Indemnification, Change of Control, and Most Favored Nation. It includes 114 queries and over 126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars. The task is to find the most relevant precedent clauses to a query. The bi-encoder retriever paired with pointwise LLMs re-rankers shows promising results. However, substantial improvements are still needed to effectively manage the complex legal work typically undertaken by lawyers. As the first retrieval benchmark for contract drafting annotated by experts, ACORD can serve as a valuable IR benchmark for the NLP community.",
      "arxiv_url": "https://arxiv.org/abs/2501.06582",
      "pdf_url": "https://arxiv.org/pdf/2501.06582",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.06689",
      "title": "DependEval: Benchmarking LLMs for Repository Dependency Understanding",
      "authors": [
        "Junjia Du",
        "Yadi Liu",
        "Hongcheng Guo",
        "Jiawei Wang",
        "Haojian Huang",
        "Yunyi Ni",
        "Zhoujun Li"
      ],
      "abstract": "While large language models (LLMs) have shown considerable promise in code generation, real-world software development demands advanced repository-level reasoning. This includes understanding dependencies, project structures, and managing multi-file changes. However, the ability of LLMs to effectively comprehend and handle complex code repositories has yet to be fully explored. To address challenges, we introduce a hierarchical benchmark designed to evaluate repository dependency understanding (DependEval). Benchmark is based on 15,576 repositories collected from real-world websites. It evaluates models on three core tasks: Dependency Recognition, Repository Construction, and Multi-file Editing, across 8 programming languages from actual code repositories. Our evaluation of over 25 LLMs reveals substantial performance gaps and provides valuable insights into repository-level code understanding.",
      "arxiv_url": "https://arxiv.org/abs/2503.06689",
      "pdf_url": "https://arxiv.org/pdf/2503.06689",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "1d143d8bdc7d986f3740da4ba100a7c2771b5d4f",
      "title": "NarGINA: Towards Accurate and Interpretable Children's Narrative Ability Assessment via Narrative Graphs",
      "authors": [
        "Jun Zhong",
        "Longwei Xu",
        "Li Kong",
        "Xianzhuo Li",
        "Dandan Liang",
        "Junsheng Zhou"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/1d143d8bdc7d986f3740da4ba100a7c2771b5d4f",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04796",
      "title": "Optimizing Multi-Hop Document Retrieval Through Intermediate Representations",
      "authors": [
        "Jiaen Lin",
        "Jingyu Liu"
      ],
      "abstract": "Retrieval-augmented generation (RAG) encounters challenges when addressing complex queries, particularly multi-hop questions. While several methods tackle multi-hop queries by iteratively generating internal queries and retrieving external documents, these approaches are computationally expensive. In this paper, we identify a three-stage information processing pattern in LLMs during layer-by-layer reasoning, consisting of extraction, processing, and subsequent extraction steps. This observation suggests that the representations in intermediate layers contain richer information compared to those in other layers. Building on this insight, we propose Layer-wise RAG (L-RAG). Unlike prior methods that focus on generating new internal queries, L-RAG leverages intermediate representations from the middle layers, which capture next-hop information, to retrieve external knowledge. L-RAG achieves performance comparable to multi-step approaches while maintaining inference overhead similar to that of standard RAG. Experimental results show that L-RAG outperforms existing RAG methods on open-domain multi-hop question-answering datasets, including MuSiQue, HotpotQA, and 2WikiMultiHopQA. The code is available in https://github.com/Olive-2019/L-RAG",
      "arxiv_url": "https://arxiv.org/abs/2503.04796",
      "pdf_url": "https://arxiv.org/pdf/2503.04796",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-03-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.09957",
      "title": "FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation based on Knowledge Graphs",
      "authors": [
        "Zengyi Gao",
        "Yukun Cao",
        "Hairu Wang",
        "Ao Ke",
        "Yuan Feng",
        "Xike Xie",
        "S. K. Zhou"
      ],
      "abstract": "To mitigate the hallucination and knowledge deficiency in large language models (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) has shown promising potential by utilizing KGs as external resource to enhance LLMs reasoning. However, existing KG-RAG approaches struggle with a trade-off between flexibility and retrieval quality. Modular methods prioritize flexibility by avoiding the use of KG-fine-tuned models during retrieval, leading to fixed retrieval strategies and suboptimal retrieval quality. Conversely, coupled methods embed KG information within models to improve retrieval quality, but at the expense of flexibility. In this paper, we propose a novel flexible modular KG-RAG framework, termed FRAG, which synergizes the advantages of both approaches. FRAG estimates the hop range of reasoning paths based solely on the query and classify it as either simple or complex. To match the complexity of the query, tailored pipelines are applied to ensure efficient and accurate reasoning path retrieval, thus fostering the final reasoning process. By using the query text instead of the KG to infer the structural information of reasoning paths and employing adaptable retrieval strategies, FRAG improves retrieval quality while maintaining flexibility. Moreover, FRAG does not require extra LLMs fine-tuning or calls, significantly boosting efficiency and conserving resources. Extensive experiments show that FRAG achieves state-of-the-art performance with high efficiency and low resource consumption.",
      "arxiv_url": "https://arxiv.org/abs/2501.09957",
      "pdf_url": "https://arxiv.org/pdf/2501.09957",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-01-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11962",
      "title": "CrafText Benchmark: Advancing Instruction Following in Complex Multimodal Open-Ended World",
      "authors": [
        "Z. Volovikova",
        "G. Gorbov",
        "Petr Kuderov",
        "Aleksandr Panov",
        "Alexey Skrynnik"
      ],
      "abstract": "Following instructions in real-world conditions requires the ability to adapt to the world's volatility and entanglement: the environment is dynamic and unpredictable, instructions can be linguistically complex with diverse vocabulary, and the number of possible goals an agent may encounter is vast. Despite extensive research in this area, most studies are conducted in static environments with simple instructions and a limited vocabulary, making it difficult to assess agent performance in more diverse and challenging settings. To address this gap, we introduce CrafText, a benchmark for evaluating instruction following in a multimodal environment with diverse instructions and dynamic interactions. CrafText includes 3,924 instructions with 3,423 unique words, covering Localization, Conditional, Building, and Achievement tasks. Additionally, we propose an evaluation protocol that measures an agent's ability to generalize to novel instruction formulations and dynamically evolving task configurations, providing a rigorous test of both linguistic understanding and adaptive decision-making.",
      "arxiv_url": "https://arxiv.org/abs/2505.11962",
      "pdf_url": "https://arxiv.org/pdf/2505.11962",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.17650",
      "title": "Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking?",
      "authors": [
        "Chengda Lu",
        "Xiaoyu Fan",
        "Yu Huang",
        "Rongwu Xu",
        "Jijie Li",
        "Wei Xu"
      ],
      "abstract": "Jailbreak attacks have been observed to largely fail against recent reasoning models enhanced by Chain-of-Thought (CoT) reasoning. However, the underlying mechanism remains underexplored, and relying solely on reasoning capacity may raise security concerns. In this paper, we try to answer the question: Does CoT reasoning really reduce harmfulness from jailbreaking? Through rigorous theoretical analysis, we demonstrate that CoT reasoning has dual effects on jailbreaking harmfulness. Based on the theoretical insights, we propose a novel jailbreak method, FicDetail, whose practical performance validates our theoretical findings.",
      "arxiv_url": "https://arxiv.org/abs/2505.17650",
      "pdf_url": "https://arxiv.org/pdf/2505.17650",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2510.24014",
      "title": "Text2DB: Integration-Aware Information Extraction with Large Language Model Agents",
      "authors": [
        "Yizhu Jiao",
        "Sha Li",
        "Sizhe Zhou",
        "Heng Ji",
        "Jiawei Han"
      ],
      "abstract": "The task of information extraction (IE) is to extract structured knowledge from text. However, it is often not straightforward to utilize IE output due to the mismatch between the IE ontology and the downstream application needs. We propose a new formulation of IE TEXT2DB that emphasizes the integration of IE output and the target database (or knowledge base). Given a user instruction, a document set, and a database, our task requires the model to update the database with values from the document set to satisfy the user instruction. This task requires understanding user instructions for what to extract and adapting to the given DB/KB schema for how to extract on the fly. To evaluate this new task, we introduce a new benchmark featuring common demands such as data infilling, row population, and column addition. In addition, we propose an LLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer component that interacts with the database, the Planner component that generates a code-based plan with calls to IE models, and the Analyzer component that provides feedback regarding code quality before execution. Experiments show that OPAL can successfully adapt to diverse database schemas by generating different code plans and calling the required IE models. We also highlight difficult cases such as dealing with large databases with complex dependencies and extraction hallucination, which we believe deserve further investigation. Source code: https://github.com/yzjiao/Text2DB",
      "arxiv_url": "https://arxiv.org/abs/2510.24014",
      "pdf_url": "https://arxiv.org/pdf/2510.24014",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-10-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23295",
      "title": "How Does Response Length Affect Long-Form Factuality",
      "authors": [
        "James Xu Zhao",
        "Jimmy Z.J. Liu",
        "Bryan Hooi",
        "S. Ng"
      ],
      "abstract": "Large language models (LLMs) are widely used for long-form text generation. However, factual errors in the responses would undermine their reliability. Despite growing attention to LLM factuality, the effect of response length on factuality remains underexplored. In this work, we systematically investigate this relationship by first introducing an automatic and bi-level long-form factuality evaluation framework, which achieves high agreement with human annotations while being cost-effective. Using this framework, we conduct controlled experiments and find that longer responses exhibit lower factual precision, confirming the presence of length bias. To explain this phenomenon, we empirically examine three hypotheses: error propagation, long context, and facts exhaustion. Our results reveal that facts exhaustion, where the model gradually exhausts more reliable knowledge, is the primary cause of factual degradation, rather than the other two hypotheses.",
      "arxiv_url": "https://arxiv.org/abs/2505.23295",
      "pdf_url": "https://arxiv.org/pdf/2505.23295",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04232",
      "title": "TGEA: An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models",
      "authors": [
        "Jie He",
        "Bo Peng",
        "Yi Liao",
        "Qun Liu",
        "Deyi Xiong"
      ],
      "abstract": "In order to deeply understand the capability of pretrained language models in text generation and conduct a diagnostic evaluation, we propose TGEA, an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models (PLMs). We use carefully selected prompt words to guide GPT-2 to generate candidate sentences, from which we select 47K for error annotation. Crowdsourced workers manually check each of these sentences and detect 12k erroneous sentences. We create an error taxonomy to cover 24 types of errors occurring in these erroneous sentences according to the nature of errors with respect to linguistics and knowledge (e.g., common sense). For each erroneous span in PLM-generated sentences, we also detect another span that is closely associated with it. Each error is hence manually labeled with comprehensive annotations, including the span of the error, the associated span, minimal correction to the error, the type of the error, and rationale behind the error. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. This is the first dataset with comprehensive annotations for PLM-generated texts, which facilitates the diagnostic evaluation of PLM-based text generation. Furthermore, we use TGEA as a benchmark dataset and propose a series of automatic diagnosis tasks, including error detection, error type classification, associated span detection, error rationale generation, to further promote future study on the automatic error detection and correction on texts generated by pretrained language models.",
      "arxiv_url": "https://arxiv.org/abs/2503.04232",
      "pdf_url": "https://aclanthology.org/2021.acl-long.469.pdf",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "1deec7ae5e3efaa4ec4f6541ed5bd3c057d500c6",
      "title": "Prediction-Augmented Generation for Automatic Diagnosis Tasks",
      "authors": [
        "Changyu Ju",
        "Dong-Ho Lee"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/1deec7ae5e3efaa4ec4f6541ed5bd3c057d500c6",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07296",
      "title": "HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval",
      "authors": [
        "Arian Askari",
        "Emmanouil Stergiadis",
        "Ilya Gusev",
        "Moran Beladev"
      ],
      "abstract": "We present HotelMatch-LLM, a multimodal dense retrieval model for the travel domain that enables natural language property search, addressing the limitations of traditional travel search engines which require users to start with a destination and editing search parameters. HotelMatch-LLM features three key innovations: (1) Domain-specific multi-task optimization with three novel retrieval, visual, and language modeling objectives; (2) Asymmetrical dense retrieval architecture combining a small language model (SLM) for efficient online query processing and a large language model (LLM) for embedding hotel data; and (3) Extensive image processing to handle all property image galleries. Experiments on four diverse test sets show HotelMatch-LLM significantly outperforms state-of-the-art models, including VISTA and MARVEL. Specifically, on the test set -- main query type -- we achieve 0.681 for HotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our analysis highlights the impact of our multi-task optimization, the generalizability of HotelMatch-LLM across LLM architectures, and its scalability for processing large image galleries.",
      "arxiv_url": "https://arxiv.org/abs/2506.07296",
      "pdf_url": "https://arxiv.org/pdf/2506.07296",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "1e46088f358da3343096020391264d30437714e5",
      "title": "ProMALex: Progressive Modular Adapters for Multi-Jurisdictional Legal Language Modeling",
      "authors": [
        "Santosh T.Y.S.S",
        "M. Elganayni"
      ],
      "abstract": "This paper addresses the challenge of adapting language models to the jurisdiction-specific nature of legal corpora. Existing approaches—training separate models for each jurisdiction or using a single shared model—either fail to leverage common legal principles beneficial for low-resource settings or risk negative interference from conflicting ju-risdictional interpretations. To overcome these limitations, we propose a parameter-efficient framework ProMALex, that first derives hierarchical relationships across jurisdictions and progressively inserts adapter modules across model layers based on jurisdictional similarity. This design allows modules in lower layers to be shared across jurisdictions, capturing common legal principles, while higher layers specialize through jurisdiction-specific adapters. Experimental results on two legal language modeling benchmarks demonstrate that Pro-MALex outperforms both fully shared and jurisdiction-specific models.",
      "arxiv_url": "https://www.semanticscholar.org/paper/1e46088f358da3343096020391264d30437714e5",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06808",
      "title": "Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events",
      "authors": [
        "James A. Michaelov",
        "Reeka Estacio",
        "Zhien Zhang",
        "Benjamin Bergen"
      ],
      "abstract": "Can language models reliably predict that possible events are more likely than merely improbable ones? By teasing apart possibility, typicality, and contextual relatedness, we show that despite the results of previous work, language models' ability to do this is far from robust. In fact, under certain conditions, all models tested - including Llama 3, Gemma 2, and Mistral NeMo - perform at worse-than-chance level, assigning higher probabilities to impossible sentences such as 'the car was given a parking ticket by the brake' than to merely unlikely sentences such as 'the car was given a parking ticket by the explorer'.",
      "arxiv_url": "https://arxiv.org/abs/2506.06808",
      "pdf_url": "https://arxiv.org/pdf/2506.06808",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.22978",
      "title": "A Systematic Study of Compositional Syntactic Transformer Language Models",
      "authors": [
        "Yida Zhao",
        "Hao Xve",
        "Xiang Hu",
        "Kewei Tu"
      ],
      "abstract": "Syntactic language models (SLMs) enhance Transformers by incorporating syntactic biases through the modeling of linearized syntactic parse trees alongside surface sentences. This paper focuses on compositional SLMs that are based on constituency parse trees and contain explicit bottom-up composition of constituent representations. We identify key aspects of design choices in existing compositional SLMs and propose a unified framework encompassing both existing models and novel variants. We conduct a comprehensive empirical evaluation of all the variants in our framework across language modeling, syntactic generalization, summarization, dialogue, and inference efficiency. Based on the experimental results, we make multiple recommendations on the design of compositional SLMs. Our code is released at https://github.com/zhaoyd1/compositional_SLMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.22978",
      "pdf_url": "https://arxiv.org/pdf/2506.22978",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "1e74db1c01062b3a351262b93c007934ad6fe424",
      "title": "Exploring Knowledge Filtering for Retrieval-Augmented Discriminative Tasks",
      "authors": [
        "Minjie Qiang",
        "Zhongqing Wang",
        "Xiaoyi Bao",
        "Haoyuan Ma",
        "Shoushan Li",
        "Guodong Zhou"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/1e74db1c01062b3a351262b93c007934ad6fe424",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.01059",
      "title": "Dynamic Attention-Guided Context Decoding for Mitigating Context Faithfulness Hallucinations in Large Language Models",
      "authors": [
        "Yanwen Huang",
        "Yong Zhang",
        "Ning Cheng",
        "Zhitao Li",
        "Shaojun Wang",
        "Jing Xiao"
      ],
      "abstract": "Large language models (LLMs) often exhibit Context Faithfulness Hallucinations, where outputs deviate from retrieved information due to incomplete context integration. Our analysis reveals a strong correlation between token-level uncertainty and hallucinations. We hypothesize that attention mechanisms inherently encode context utilization signals, supported by probing analysis. Based on these insights, we propose Dynamic Attention-Guided Context Decoding (DAGCD), a lightweight framework that leverages attention distributions and uncertainty signals in a single-pass decoding. Experiments on open-book QA datasets demonstrate DAGCD's effectiveness, yielding significant improvements in faithfulness and robustness while preserving computational efficiency.",
      "arxiv_url": "https://arxiv.org/abs/2501.01059",
      "pdf_url": "https://arxiv.org/pdf/2501.01059",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01237",
      "title": "Polishing Every Facet of the GEM: Testing Linguistic Competence of LLMs and Humans in Korean",
      "authors": [
        "SungHo Kim",
        "Nayeon Kim",
        "Taehee Jeon",
        "SangKeun Lee"
      ],
      "abstract": "We introduce the $\\underline{Ko}rean \\underline{G}rammar \\underline{E}valuation Bench\\underline{M}ark (KoGEM)$, designed to assess the linguistic competence of LLMs and humans in Korean. KoGEM consists of 1.5k multiple-choice QA pairs covering five main categories and 16 subcategories. The zero-shot evaluation of 27 LLMs of various sizes and types reveals that while LLMs perform remarkably well on straightforward tasks requiring primarily definitional knowledge, they struggle with tasks that demand the integration of real-world experiential knowledge, such as phonological rules and pronunciation. Furthermore, our in-depth analysis suggests that incorporating such experiential knowledge could enhance the linguistic competence of LLMs. With KoGEM, we not only highlight the limitations of current LLMs in linguistic competence but also uncover hidden facets of LLMs in linguistic competence, paving the way for enhancing comprehensive language understanding. Our code and dataset are available at: https://github.com/SungHo3268/KoGEM.",
      "arxiv_url": "https://arxiv.org/abs/2506.01237",
      "pdf_url": "https://arxiv.org/pdf/2506.01237",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2511.17153",
      "title": "LangMark: A Multilingual Dataset for Automatic Post-Editing",
      "authors": [
        "Diego Velazquez",
        "Mikaela Grace",
        "Konstantinos Karageorgos",
        "Lawrence Carin",
        "Aaron Schliem",
        "Dimitrios Zaikis",
        "Roger Wechsler"
      ],
      "abstract": "Automatic post-editing (APE) aims to correct errors in machine-translated text, enhancing translation quality, while reducing the need for human intervention. Despite advances in neural machine translation (NMT), the development of effective APE systems has been hindered by the lack of large-scale multilingual datasets specifically tailored to NMT outputs. To address this gap, we present and release LangMark, a new human-annotated multilingual APE dataset for English translation to seven languages: Brazilian Portuguese, French, German, Italian, Japanese, Russian, and Spanish. The dataset has 206,983 triplets, with each triplet consisting of a source segment, its NMT output, and a human post-edited translation. Annotated by expert human linguists, our dataset offers both linguistic diversity and scale. Leveraging this dataset, we empirically show that Large Language Models (LLMs) with few-shot prompting can effectively perform APE, improving upon leading commercial and even proprietary machine translation systems. We believe that this new resource will facilitate the future development and evaluation of APE systems.",
      "arxiv_url": "https://arxiv.org/abs/2511.17153",
      "pdf_url": "https://arxiv.org/pdf/2511.17153",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-11-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.23056",
      "title": "Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning",
      "authors": [
        "Zhuang Xiang",
        "Bin Wu",
        "Jiyu Cui",
        "Kehua Feng",
        "Xiaotong Li",
        "Huabin Xing",
        "Keyan Ding",
        "Qiang Zhang",
        "Huajun Chen"
      ],
      "abstract": "Molecular structure elucidation involves deducing a molecule's structure from various types of spectral data, which is crucial in chemical experimental analysis. While large language models (LLMs) have shown remarkable proficiency in analyzing and reasoning through complex tasks, they still encounter substantial challenges in molecular structure elucidation. We identify that these challenges largely stem from LLMs'limited grasp of specialized chemical knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search for test-time scaling as a plugin. Specifically, we construct an external molecular substructure knowledge base to extend the LLMs'coverage of the chemical structure space. Furthermore, we design a specialized molecule-spectrum scorer to act as a reward model for the reasoning process, addressing the issue of inaccurate solution evaluation in LLMs. Experimental results show that our approach significantly boosts performance, particularly gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is available at https://github.com/HICAI-ZJU/K-MSE.",
      "arxiv_url": "https://arxiv.org/abs/2506.23056",
      "pdf_url": "https://arxiv.org/pdf/2506.23056",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.13975",
      "title": "Navigating Rifts in Human-LLM Grounding: Study and Benchmark",
      "authors": [
        "Omar Shaikh",
        "Hussein Mozannar",
        "Gagan Bansal",
        "Adam Fourney",
        "Eric Horvitz"
      ],
      "abstract": "Language models excel at following instructions but often struggle with the collaborative aspects of conversation that humans naturally employ. This limitation in grounding -- the process by which conversation participants establish mutual understanding -- can lead to outcomes ranging from frustrated users to serious consequences in high-stakes scenarios. To systematically study grounding challenges in human-LLM interactions, we analyze logs from three human-assistant datasets: WildChat, MultiWOZ, and Bing Chat. We develop a taxonomy of grounding acts and build models to annotate and forecast grounding behavior. Our findings reveal significant differences in human-human and human-LLM grounding: LLMs were three times less likely to initiate clarification and sixteen times less likely to provide follow-up requests than humans. Additionally, we find that early grounding failures predict later interaction breakdowns. Building on these insights, we introduce Rifts, a benchmark derived from publicly available LLM interaction data containing situations where LLMs fail to initiate grounding. We note that current frontier models perform poorly on Rifts, highlighting the need to reconsider how we train and prompt LLMs for human interaction. To this end, we develop a preliminary intervention aimed at mitigating grounding failures.",
      "arxiv_url": "https://arxiv.org/abs/2503.13975",
      "pdf_url": "https://arxiv.org/pdf/2503.13975",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04800",
      "title": "HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated Information on Retrieval-Augmented Generation",
      "authors": [
        "Ouyang Jie",
        "Tingyue Pan",
        "Mingyue Cheng",
        "Ruiran Yan",
        "Yucong Luo",
        "Jiaying Lin",
        "Qi Liu"
      ],
      "abstract": "While Retrieval-Augmented Generation (RAG) has emerged as an effective approach for addressing the knowledge outdating problem in Large Language Models (LLMs), it still faces a critical challenge: the prevalence of outdated information in knowledge bases. Current research primarily focuses on incorporating up-to-date information, yet the impact of outdated information coexisting in retrieval sources remains inadequately addressed. To bridge this gap, we introduce HoH, the first benchmark specifically designed to evaluate the impact of outdated information on RAG. Our benchmark leverages token-level diff algorithms combined with LLM pipelines to efficiently create a large-scale QA dataset that accurately captures the evolution of temporal knowledge in real-world facts. Through comprehensive experiments, we reveal that outdated information significantly degrades RAG performance in two critical ways: (1) it substantially reduces response accuracy by distracting models from correct information, and (2) it can mislead models into generating potentially harmful outputs, even when current information is available. Current RAG approaches struggle with both retrieval and generation aspects when handling outdated information. These findings highlight the urgent need for innovative solutions to address the temporal challenges in RAG. Our code and data are available at: https://github.com/0russwest0/HoH.",
      "arxiv_url": "https://arxiv.org/abs/2503.04800",
      "pdf_url": "https://arxiv.org/pdf/2503.04800",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00823",
      "title": "Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in LLMs Across Logical Transformations and Question Answering Tasks",
      "authors": [
        "Yuntai Bao",
        "Xuhong Zhang",
        "Tianyu Du",
        "Xinkui Zhao",
        "Zhengwen Feng",
        "Hao Peng",
        "Jianwei Yin"
      ],
      "abstract": "Large language models (LLMs) are trained on extensive datasets that encapsulate substantial world knowledge. However, their outputs often include confidently stated inaccuracies. Earlier works suggest that LLMs encode truthfulness as a distinct linear feature, termed the\"truth direction\", which can classify truthfulness reliably. We address several open questions about the truth direction: (i) whether LLMs universally exhibit consistent truth directions; (ii) whether sophisticated probing techniques are necessary to identify truth directions; and (iii) how the truth direction generalizes across diverse contexts. Our findings reveal that not all LLMs exhibit consistent truth directions, with stronger representations observed in more capable models, particularly in the context of logical negation. Additionally, we demonstrate that truthfulness probes trained on declarative atomic statements can generalize effectively to logical transformations, question-answering tasks, in-context learning, and external knowledge sources. Finally, we explore the practical application of truthfulness probes in selective question-answering, illustrating their potential to improve user trust in LLM outputs. These results advance our understanding of truth directions and provide new insights into the internal representations of LLM beliefs. Our code is public at https://github.com/colored-dye/truthfulness_probe_generalization",
      "arxiv_url": "https://arxiv.org/abs/2506.00823",
      "pdf_url": "https://arxiv.org/pdf/2506.00823",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02572",
      "title": "HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference",
      "authors": [
        "Ping Gong",
        "Jiawei Yi",
        "Shengnan Wang",
        "Juncheng Zhang",
        "Zewen Jin",
        "Ouxiang Zhou",
        "Ruibo Liu",
        "Guanbin Xu",
        "Youhui Bai",
        "Bowen Ye",
        "Kun Yuan",
        "Tong Yang",
        "Gong Zhang",
        "Renhai Chen",
        "Feng Wu",
        "Cheng Li"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as a pivotal research area, yet the attention module remains a critical bottleneck in LLM inference, even with techniques like KVCache to mitigate redundant computations. While various top-$k$ attention mechanisms have been proposed to accelerate LLM inference by exploiting the inherent sparsity of attention, they often struggled to strike a balance between efficiency and accuracy. In this paper, we introduce HATA (Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates low-overhead learning-to-hash techniques into the Top-$k$ attention process. Different from the existing top-k attention methods which are devoted to seeking an absolute estimation of qk score, typically with a great cost, HATA maps queries and keys into binary hash codes, and acquires the relative qk score order with a quite low cost, which is sufficient for realizing top-k attention. Extensive experiments demonstrate that HATA achieves up to 7.2$\\times$ speedup compared to vanilla full attention while maintaining model accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention methods in both accuracy and efficiency across multiple mainstream LLM models and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA.",
      "arxiv_url": "https://arxiv.org/abs/2506.02572",
      "pdf_url": "https://arxiv.org/pdf/2506.02572",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.09601",
      "title": "CoT-Valve: Length-Compressible Chain-of-Thought Tuning",
      "authors": [
        "Xinyin Ma",
        "Guangnian Wan",
        "Runpeng Yu",
        "Gongfan Fang",
        "Xinchao Wang"
      ],
      "abstract": "Chain-of-Thought significantly enhances a model's reasoning capability, but it also comes with a considerable increase in inference costs due to long chains. With the observation that the reasoning path can be easily compressed under easy tasks but struggle on hard tasks, we explore the feasibility of elastically controlling the length of reasoning paths with only one model, thereby reducing the inference overhead of reasoning models dynamically based on task difficulty. We introduce a new tuning and inference strategy named CoT-Valve, designed to allow models to generate reasoning chains of varying lengths. To achieve this, we propose to identify a direction in the parameter space that, when manipulated, can effectively control the length of generated CoT. Moreover, we show that this property is valuable for compressing the reasoning chain. We construct datasets with chains from long to short for the same questions and explore two enhanced strategies for CoT-Valve: (1) a precise length-compressible CoT tuning method, and (2) a progressive chain length compression approach. Our experiments show that CoT-Valve successfully enables controllability and compressibility of the chain and shows better performance than the prompt-based control. We applied this method to QwQ-32B-Preview, reducing reasoning chains on GSM8K from 741 to 225 tokens with a minor performance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with only one additional incorrect answer.",
      "arxiv_url": "https://arxiv.org/abs/2502.09601",
      "pdf_url": "https://arxiv.org/pdf/2502.09601",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06698",
      "title": "Contextual Experience Replay for Self-Improvement of Language Agents",
      "authors": [
        "Yitao Liu",
        "Chenglei Si",
        "Karthik R. Narasimhan",
        "Shunyu Yao"
      ],
      "abstract": "Large language model (LLM) agents have been applied to sequential decision-making tasks such as web navigation, but without any environment-specific experiences, they often fail in these complex tasks. Moreover, current LLM agents are not designed to continually learn from past experiences during inference time, which could be crucial for them to gain these environment-specific experiences. To address this, we propose Contextual Experience Replay (CER), a training-free framework to enable efficient self-improvement for language agents in their context window. Specifically, CER accumulates and synthesizes past experiences into a dynamic memory buffer. These experiences encompass environment dynamics and common decision-making patterns, allowing the agents to retrieve and augment themselves with relevant knowledge in new tasks, enhancing their adaptability in complex environments. We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena, CER also gets a competitive average success rate of 36.7%, relatively improving the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a comprehensive analysis on it to prove its efficiency, validity and understand it better.",
      "arxiv_url": "https://arxiv.org/abs/2506.06698",
      "pdf_url": "https://arxiv.org/pdf/2506.06698",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-06-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12060",
      "title": "Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement",
      "authors": [
        "Peng Ding",
        "Jun Kuang",
        "Zongyu Wang",
        "Xuezhi Cao",
        "Xunliang Cai",
        "Jiajun Chen",
        "Shujian Huang"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive capabilities across various tasks but remain vulnerable to meticulously crafted jailbreak attacks. In this paper, we identify a critical safety gap: while LLMs are adept at detecting jailbreak prompts, they often produce unsafe responses when directly processing these inputs. Inspired by this insight, we propose SAGE (Self-Aware Guard Enhancement), a training-free defense strategy designed to align LLMs' strong safety discrimination performance with their relatively weaker safety generation ability. SAGE consists of two core components: a Discriminative Analysis Module and a Discriminative Response Module, enhancing resilience against sophisticated jailbreak attempts through flexible safety discrimination instructions. Extensive experiments demonstrate SAGE's effectiveness and robustness across various open-source and closed-source LLMs of different sizes and architectures, achieving an average 99% defense success rate against numerous complex and covert jailbreak methods while maintaining helpfulness on general benchmarks. We further conduct mechanistic interpretability analysis through hidden states and attention distributions, revealing the underlying mechanisms of this detection-generation discrepancy. Our work thus contributes to developing future LLMs with coherent safety awareness and generation behavior. Our code and datasets are publicly available at https://github.com/NJUNLP/SAGE.",
      "arxiv_url": "https://arxiv.org/abs/2505.12060",
      "pdf_url": "https://arxiv.org/pdf/2505.12060",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16580",
      "title": "Can Indirect Prompt Injection Attacks Be Detected and Removed?",
      "authors": [
        "Yulin Chen",
        "Haoran Li",
        "Yuan Sui",
        "Yufei He",
        "Yue Liu",
        "Yangqiu Song",
        "Bryan Hooi"
      ],
      "abstract": "Prompt injection attacks manipulate large language models (LLMs) by misleading them to deviate from the original input instructions and execute maliciously injected instructions, because of their instruction-following capabilities and inability to distinguish between the original input instructions and maliciously injected instructions. To defend against such attacks, recent studies have developed various detection mechanisms. If we restrict ourselves specifically to works which perform detection rather than direct defense, most of them focus on direct prompt injection attacks, while there are few works for the indirect scenario, where injected instructions are indirectly from external tools, such as a search engine. Moreover, current works mainly investigate injection detection methods and pay less attention to the post-processing method that aims to mitigate the injection after detection. In this paper, we investigate the feasibility of detecting and removing indirect prompt injection attacks, and we construct a benchmark dataset for evaluation. For detection, we assess the performance of existing LLMs and open-source detection models, and we further train detection models using our crafted training datasets. For removal, we evaluate two intuitive methods: (1) the segmentation removal method, which segments the injected document and removes parts containing injected instructions, and (2) the extraction removal method, which trains an extraction model to identify and remove injected instructions.",
      "arxiv_url": "https://arxiv.org/abs/2502.16580",
      "pdf_url": "https://arxiv.org/pdf/2502.16580",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2508.17153",
      "title": "Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models",
      "authors": [
        "Tharindu Madusanka",
        "Ian Pratt-Hartmann",
        "R. Batista-Navarro"
      ],
      "abstract": "Efforts to apply transformer-based language models (TLMs) to the problem of reasoning in natural language have enjoyed ever-increasing success in recent years. The most fundamental task in this area to which nearly all others can be reduced is that of determining satisfiability. However, from a logical point of view, satisfiability problems vary along various dimensions, which may affect TLMs'ability to learn how to solve them. The problem instances of satisfiability in natural language can belong to different computational complexity classes depending on the language fragment in which they are expressed. Although prior research has explored the problem of natural language satisfiability, the above-mentioned point has not been discussed adequately. Hence, we investigate how problem instances from varying computational complexity classes and having different grammatical constructs impact TLMs'ability to learn rules of inference. Furthermore, to faithfully evaluate TLMs, we conduct an empirical study to explore the distribution of satisfiability problems.",
      "arxiv_url": "https://arxiv.org/abs/2508.17153",
      "pdf_url": "https://arxiv.org/pdf/2508.17153",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-08-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01673",
      "title": "GRAM: Generative Recommendation via Semantic-aware Multi-granular Late Fusion",
      "authors": [
        "Sunkyung Lee",
        "Minjin Choi",
        "Eunseong Choi",
        "Hye-young Kim",
        "Jongwuk Lee"
      ],
      "abstract": "Generative recommendation is an emerging paradigm that leverages the extensive knowledge of large language models by formulating recommendations into a text-to-text generation task. However, existing studies face two key limitations in (i) incorporating implicit item relationships and (ii) utilizing rich yet lengthy item information. To address these challenges, we propose a Generative Recommender via semantic-Aware Multi-granular late fusion (GRAM), introducing two synergistic innovations. First, we design semantic-to-lexical translation to encode implicit hierarchical and collaborative item relationships into the vocabulary space of LLMs. Second, we present multi-granular late fusion to integrate rich semantics efficiently with minimal information loss. It employs separate encoders for multi-granular prompts, delaying the fusion until the decoding stage. Experiments on four benchmark datasets show that GRAM outperforms eight state-of-the-art generative recommendation models, achieving significant improvements of 11.5-16.0% in Recall@5 and 5.3-13.6% in NDCG@5. The source code is available at https://github.com/skleee/GRAM.",
      "arxiv_url": "https://arxiv.org/abs/2506.01673",
      "pdf_url": "https://arxiv.org/pdf/2506.01673",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2508.11388",
      "title": "Model Interpretability and Rationale Extraction by Input Mask Optimization",
      "authors": [
        "M. Brinner",
        "Sina Zarrieß"
      ],
      "abstract": "Concurrent to the rapid progress in the development of neural-network based models in areas like natural language processing and computer vision, the need for creating explanations for the predictions of these black-box models has risen steadily. We propose a new method to generate extractive explanations for predictions made by neural networks, that is based on masking parts of the input which the model does not consider to be indicative of the respective class. The masking is done using gradient-based optimization combined with a new regularization scheme that enforces sufficiency, comprehensiveness and compactness of the generated explanation, three properties that are known to be desirable from the related field of rationale extraction in natural language processing. In this way, we bridge the gap between model interpretability and rationale extraction, thereby proving that the latter of which can be performed without training a specialized model, only on the basis of a trained classifier. We further apply the same method to image inputs and obtain high quality explanations for image classifications, which indicates that the conditions proposed for rationale extraction in natural language processing are more broadly applicable to different input types.",
      "arxiv_url": "https://arxiv.org/abs/2508.11388",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.867.pdf",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-08-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11901",
      "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity",
      "authors": [
        "Dylan Zhang",
        "Justin Wang",
        "Tianran Sun"
      ],
      "abstract": "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.",
      "arxiv_url": "https://arxiv.org/abs/2502.11901",
      "pdf_url": "https://arxiv.org/pdf/2502.11901",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18291",
      "title": "InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning",
      "authors": [
        "Zifu Wan",
        "Yaqi Xie",
        "Ce Zhang",
        "Zhiqiu Lin",
        "Zihan Wang",
        "Simon Stepputtis",
        "Deva Ramanan",
        "Katia P. Sycara"
      ],
      "abstract": "Large multimodal foundation models, particularly in the domains of language and vision, have significantly advanced various tasks, including robotics, autonomous driving, information retrieval, and grounding. However, many of these models perceive objects as indivisible, overlooking the components that constitute them. Understanding these components and their associated affordances provides valuable insights into an object's functionality, which is fundamental for performing a wide range of tasks. In this work, we introduce a novel real-world benchmark, InstructPart, comprising hand-labeled part segmentation annotations and task-oriented instructions to evaluate the performance of current models in understanding and executing part-level tasks within everyday contexts. Through our experiments, we demonstrate that task-oriented part segmentation remains a challenging problem, even for state-of-the-art Vision-Language Models (VLMs). In addition to our benchmark, we introduce a simple baseline that achieves a twofold performance improvement through fine-tuning with our dataset. With our dataset and benchmark, we aim to facilitate research on task-oriented part segmentation and enhance the applicability of VLMs across various domains, including robotics, virtual reality, information retrieval, and other related fields. Project website: https://zifuwan.github.io/InstructPart/.",
      "arxiv_url": "https://arxiv.org/abs/2505.18291",
      "pdf_url": "https://arxiv.org/pdf/2505.18291",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.06517",
      "title": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers",
      "authors": [
        "Zicong Tang",
        "Luohe Shi",
        "Z. Li",
        "Baoyuan Qi",
        "Guoming Liu",
        "Lefei Zhang",
        "Ping Wang"
      ],
      "abstract": "Large Language Models (LLMs) have achieved impressive accomplishments in recent years. However, the increasing memory consumption of KV cache has possessed a significant challenge to the inference system. Eviction methods have revealed the inherent redundancy within the KV cache, demonstrating its potential for reduction, particularly in deeper layers. However, KV cache reduction for shallower layers has been found to be insufficient. Based on our observation that, the KV cache exhibits a high degree of similarity. Based on this observation, we proposed a novel KV cache reduction method, SpindleKV, which balances both shallow and deep layers. For deep layers, we employ an attention weight based eviction method, while for shallow layers, we apply a codebook based replacement approach which is learnt by similarity and merging policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma faced by other attention based eviction methods. Experiments on two common benchmarks with three different LLMs shown that SpindleKV obtained better KV cache reduction effect compared to baseline methods, while preserving similar or even better model performance.",
      "arxiv_url": "https://arxiv.org/abs/2507.06517",
      "pdf_url": "https://arxiv.org/pdf/2507.06517",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03593",
      "title": "Is linguistically-motivated data augmentation worth it?",
      "authors": [
        "Ray Groshan",
        "Michael Ginn",
        "Alexis Palmer"
      ],
      "abstract": "Data augmentation, a widely-employed technique for addressing data scarcity, involves generating synthetic data examples which are then used to augment available training data. Researchers have seen surprising success from simple methods, such as random perturbations from natural examples, where models seem to benefit even from data with nonsense words, or data that doesn't conform to the rules of the language. A second line of research produces synthetic data that does in fact follow all linguistic constraints; these methods require some linguistic expertise and are generally more challenging to implement. No previous work has done a systematic, empirical comparison of both linguistically-naive and linguistically-motivated data augmentation strategies, leaving uncertainty about whether the additional time and effort of linguistically-motivated data augmentation work in fact yields better downstream performance. In this work, we conduct a careful and comprehensive comparison of augmentation strategies (both linguistically-naive and linguistically-motivated) for two low-resource languages with different morphological properties, Uspanteko and Arapaho. We evaluate the effectiveness of many different strategies and their combinations across two important sequence-to-sequence tasks for low-resource languages: machine translation and interlinear glossing. We find that linguistically-motivated strategies can have benefits over naive approaches, but only when the new examples they produce are not significantly unlike the training data distribution.",
      "arxiv_url": "https://arxiv.org/abs/2506.03593",
      "pdf_url": "https://arxiv.org/pdf/2506.03593",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2125ddbee130f7cba5056f9b538287deb717f534",
      "title": "Harnessing Whisper for Prosodic Stress Analysis",
      "authors": [
        "Samuel S. Sohn",
        "Sten Knutsen",
        "K. Stromswold"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/2125ddbee130f7cba5056f9b538287deb717f534",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.17693",
      "title": "Zero-Shot Conversational Stance Detection: Dataset and Approaches",
      "authors": [
        "Yuzhe Ding",
        "Kang He",
        "Bobo Li",
        "Limin Zheng",
        "Haijun He",
        "Fei Li",
        "Chong Teng",
        "Dong-Hong Ji"
      ],
      "abstract": "Stance detection, which aims to identify public opinion towards specific targets using social media data, is an important yet challenging task. With the increasing number of online debates among social media users, conversational stance detection has become a crucial research area. However, existing conversational stance detection datasets are restricted to a limited set of specific targets, which constrains the effectiveness of stance detection models when encountering a large number of unseen targets in real-world applications. To bridge this gap, we manually curate a large-scale, high-quality zero-shot conversational stance detection dataset, named ZS-CSD, comprising 280 targets across two distinct target types. Leveraging the ZS-CSD dataset, we propose SITPCL, a speaker interaction and target-aware prototypical contrastive learning model, and establish the benchmark performance in the zero-shot setting. Experimental results demonstrate that our proposed SITPCL model achieves state-of-the-art performance in zero-shot conversational stance detection. Notably, the SITPCL model attains only an F1-macro score of 43.81%, highlighting the persistent challenges in zero-shot conversational stance detection.",
      "arxiv_url": "https://arxiv.org/abs/2506.17693",
      "pdf_url": "https://arxiv.org/pdf/2506.17693",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00253",
      "title": "Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race",
      "authors": [
        "Lihao Sun",
        "Chengzhi Mao",
        "Valentin Hofmann",
        "Xuechunzi Bai"
      ],
      "abstract": "Although value-aligned language models (LMs) appear unbiased in explicit bias evaluations, they often exhibit stereotypes in implicit word association tasks, raising concerns about their fair usage. We investigate the mechanisms behind this discrepancy and find that alignment surprisingly amplifies implicit bias in model outputs. Specifically, we show that aligned LMs, unlike their unaligned counterparts, overlook racial concepts in early internal representations when the context is ambiguous. Not representing race likely fails to activate safety guardrails, leading to unintended biases. Inspired by this insight, we propose a new bias mitigation strategy that works by incentivizing the representation of racial concepts in the early model layers. In contrast to conventional mitigation methods of machine unlearning, our interventions find that steering the model to be more aware of racial concepts effectively mitigates implicit bias. Similar to race blindness in humans, ignoring racial nuances can inadvertently perpetuate subtle biases in LMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.00253",
      "pdf_url": "https://arxiv.org/pdf/2506.00253",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21926",
      "title": "Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning",
      "authors": [
        "Yin Hua",
        "Zhiqiang Liu",
        "Mingyang Chen",
        "Zheng Fang",
        "Chi-Man Wong",
        "Lingxiao Li",
        "C. Vong",
        "Hua-zeng Chen",
        "Wen Zhang"
      ],
      "abstract": "In natural language processing (NLP) and computer vision (CV), the successful application of foundation models across diverse tasks has demonstrated their remarkable potential. However, despite the rich structural and textual information embedded in knowledge graphs (KGs), existing research of foundation model for KG has primarily focused on their structural aspects, with most efforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This limitation has hindered progress in addressing more challenging out-of-KG tasks. In this paper, we introduce MERRY, a foundation model for general knowledge graph reasoning, and investigate its performance across two task categories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG question answering, KGQA). We not only utilize the structural information, but also the textual information in KGs. Specifically, we propose a multi-perspective Conditional Message Passing (CMP) encoding architecture to bridge the gap between textual and structural modalities, enabling their seamless integration. Additionally, we introduce a dynamic residual fusion module to selectively retain relevant textual information and a flexible edge scoring mechanism to adapt to diverse downstream tasks. Comprehensive evaluations on 28 datasets demonstrate that MERRY outperforms existing baselines in most scenarios, showcasing strong reasoning capabilities within KGs and excellent generalization to out-of-KG tasks such as KGQA.",
      "arxiv_url": "https://arxiv.org/abs/2505.21926",
      "pdf_url": "https://arxiv.org/pdf/2505.21926",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.21910",
      "title": "AutoMixer: Checkpoint Artifacts as Automatic Data Mixers",
      "authors": [
        "Ernie Chang",
        "Yang Li",
        "Patrick Huber",
        "David Kant",
        "Yangyang Shi",
        "Vikas Chandra"
      ],
      "abstract": "In language model training, it is desirable to equip models with capabilities from various tasks. However, it is not clear how to directly obtain the right data mixtures for these capabilities as the relationship between data and tasks is difficult to be modeled. In this work, we observe that checkpoint models exhibit emerging capabilities at different points in the training trajectory. Often, the training process saves checkpoints as artifacts that are under-utilized as a source of in-training data signals. We identify these artifact models based on their respective capabilities on the benchmarks and leverage them as data mixers by using their aggregated first-order influence approximation over source data. We demonstrated on eight reasoning benchmarks that the proposed framework shows significant improvements in the pretraining setting, with performance improvements of up to 1.93%. Overall, this shows the potential of checkpoint models to enhance data quality and optimize data mixtures.",
      "arxiv_url": "https://arxiv.org/abs/2506.21910",
      "pdf_url": "https://arxiv.org/pdf/2506.21910",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.14900",
      "title": "From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment",
      "authors": [
        "Chongxuan Huang",
        "Yongshi Ye",
        "Biao Fu",
        "Qifeng Su",
        "Xiaodong Shi"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable multilingual capabilities, however, how to evaluate cross-lingual alignment remains underexplored. Existing alignment benchmarks primarily focus on sentence embeddings, but prior research has shown that neural models tend to induce a non-smooth representation space, which impact of semantic alignment evaluation on low-resource languages. Inspired by neuroscientific findings that similar information activates overlapping neuronal regions, we propose a novel Neuron State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a lignment capabilities of LLMs, which offers a more semantically grounded approach to assess cross-lingual alignment. We evaluate NeuronXA on several prominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two transfer tasks and three multilingual benchmarks. The results demonstrate that with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation of 0.9556 with downstream tasks performance and 0.8514 with transferability. These findings demonstrate NeuronXA's effectiveness in assessing both cross-lingual alignment and transferability, even with a small dataset. This highlights its potential to advance cross-lingual alignment research and to improve the semantic understanding of multilingual LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2507.14900",
      "pdf_url": "https://arxiv.org/pdf/2507.14900",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "21e5b6b7b42cc97ffb2e12200e3c811a24ff6eeb",
      "title": "MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct",
      "authors": [
        "Run Luo",
        "Haonan Zhang",
        "Longze Chen",
        "Ting-En Lin",
        "Xiong Liu",
        "Yuchuan Wu",
        "Min Yang",
        "Yongbin Li",
        "Minzheng Wang",
        "Pengpeng Zeng",
        "Lianli Gao",
        "Hengtao Shen",
        "Yunshui Li",
        "Hamid Alinejad-Rokny",
        "Xiaobo Xia",
        "Jingkuan Song",
        "Fei Huang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/21e5b6b7b42cc97ffb2e12200e3c811a24ff6eeb",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24362",
      "title": "Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion",
      "authors": [
        "Anum Afzal",
        "Florian Matthes",
        "Gal Chechik",
        "Yftah Ziser"
      ],
      "abstract": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well \\emph{even before a single token is generated}, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits.",
      "arxiv_url": "https://arxiv.org/abs/2505.24362",
      "pdf_url": "https://arxiv.org/pdf/2505.24362",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16133",
      "title": "HASH-RAG: Bridging Deep Hashing with Retriever for Efficient, Fine Retrieval and Augmented Generation",
      "authors": [
        "Jinyu Guo",
        "Xunlei Chen",
        "Qiyang Xia",
        "Zhaokun Wang",
        "Jie Ou",
        "Libo Qin",
        "Shunyu Yao",
        "Wenhong Tian"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) encounters efficiency challenges when scaling to massive knowledge bases while preserving contextual relevance. We propose Hash-RAG, a framework that integrates deep hashing techniques with systematic optimizations to address these limitations. Our queries directly learn binary hash codes from knowledgebase code, eliminating intermediate feature extraction steps, and significantly reducing storage and computational overhead. Building upon this hash-based efficient retrieval framework, we establish the foundation for fine-grained chunking. Consequently, we design a Prompt-Guided Chunk-to-Context (PGCC) module that leverages retrieved hash-indexed propositions and their original document segments through prompt engineering to enhance the LLM's contextual awareness. Experimental evaluations on NQ, TriviaQA, and HotpotQA datasets demonstrate that our approach achieves a 90% reduction in retrieval time compared to conventional methods while maintaining considerate recall performance. Additionally, The proposed system outperforms retrieval/non-retrieval baselines by 1.4-4.3% in EM scores.",
      "arxiv_url": "https://arxiv.org/abs/2505.16133",
      "pdf_url": "https://arxiv.org/pdf/2505.16133",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19647",
      "title": "Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation",
      "authors": [
        "Xiaochuan Liu",
        "Rui-jie Song",
        "Xiting Wang",
        "Xu Chen"
      ],
      "abstract": "Automatic related work generation (RWG) can save people's time and effort when writing a draft of related work section (RWS) for further revision. However, existing methods for RWG always suffer from shallow comprehension due to taking the limited portions of references papers as input and isolated explanation for each reference due to ineffective capturing the relationships among them. To address these issues, we focus on full-text-based RWG task and propose a novel multi-agent framework. Our framework consists of three agents: a selector that decides which section of the papers is going to read next, a reader that digests the selected section and updates a shared working memory, and a writer that generates RWS based on the final curated memory. To better capture the relationships among references, we also propose two graph-aware strategies for selector, enabling to optimize the reading order with constrains of the graph structure. Extensive experiments demonstrate that our framework consistently improves performance across three base models and various input configurations. The graph-aware selectors outperform alternative selectors, achieving state-of-the-art results. The code and data are available at https://github.com/1190200817/Full_Text_RWG.",
      "arxiv_url": "https://arxiv.org/abs/2505.19647",
      "pdf_url": "https://arxiv.org/pdf/2505.19647",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "22907701f8bc885d4ada4d0bfb770d89881c5985",
      "title": "Do LLMs Understand Dialogues? A Case Study on Dialogue Acts",
      "authors": [
        "A. Qamar",
        "Jonathan Tong",
        "Ruihong Huang"
      ],
      "abstract": "Recent advancements in NLP, largely driven by Large Language Models (LLMs), have significantly improved performance on an array of tasks. However, Dialogue Act (DA) classification remains challenging, particularly in the fine-grained 50-class, multiparty setting. This paper investigates the root causes of LLMs’ poor performance in DA classification through a linguistically motivated analysis. We identify three key pre-tasks essential for accurate DA prediction: Turn Management , Communica-tive Function Identification , and Dialogue Structure Prediction . Our experiments reveal that LLMs struggle with these fundamental tasks, often failing to outperform simple rule-based baselines. Additionally, we establish a strong empirical correlation between errors in these pre-tasks and DA classification failures. A human study further highlights the significant gap between LLM and human-level dialogue understanding. These findings indicate that LLMs’ shortcomings in dialogue comprehension hinder their ability to accurately predict DAs, highlighting the need for improved dialogue-aware training approaches.",
      "arxiv_url": "https://www.semanticscholar.org/paper/22907701f8bc885d4ada4d0bfb770d89881c5985",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "22ac583f29d629fd2dfeb99fc90b8ad5c99b8c10",
      "title": "The Lies Characters Tell: Utilizing Large Language Models to Normalize Adversarial Unicode Perturbations",
      "authors": [
        "Portia Cooper",
        "Eduardo Blanco",
        "Mihai Surdeanu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/22ac583f29d629fd2dfeb99fc90b8ad5c99b8c10",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11078",
      "title": "DEEPER Insight into Your User: Directed Persona Refinement for Dynamic Persona Modeling",
      "authors": [
        "Aili Chen",
        "Chengyu Du",
        "Jiangjie Chen",
        "Jinghan Xu",
        "Yikai Zhang",
        "Siyu Yuan",
        "Zulong Chen",
        "Liangyue Li",
        "Yanghua Xiao"
      ],
      "abstract": "To advance personalized applications such as recommendation systems and user behavior prediction, recent research increasingly adopts large language models (LLMs) for human -readable persona modeling. In dynamic real -world scenarios, effective persona modeling necessitates leveraging streaming behavior data to continually optimize user personas. However, existing methods -whether regenerating personas or incrementally extending them with new behaviors -often fail to achieve sustained improvements in persona quality or future behavior prediction accuracy. To address this, we propose DEEPER, a novel approach for dynamic persona modeling that enables continual persona optimization. Specifically, we enhance the model's direction -search capability through an iterative reinforcement learning framework, allowing it to automatically identify effective update directions and optimize personas using discrepancies between user behaviors and model predictions. Extensive experiments on dynamic persona modeling involving 4800 users across 10 domains highlight the superior persona optimization capabilities of DEEPER, delivering an impressive 32.2% average reduction in user behavior prediction error over four update rounds -outperforming the best baseline by a remarkable 22.92%.",
      "arxiv_url": "https://arxiv.org/abs/2502.11078",
      "pdf_url": "https://arxiv.org/pdf/2502.11078",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.09749",
      "title": "Enhancing Lexicon-Based Text Embeddings with Large Language Models",
      "authors": [
        "Yibin Lei",
        "Tao Shen",
        "Yu Cao",
        "Andrew Yates"
      ],
      "abstract": "Recent large language models (LLMs) have demonstrated exceptional performance on general-purpose text embedding tasks. While dense embeddings have dominated related research, we introduce the first Lexicon-based EmbeddiNgS (LENS) leveraging LLMs that achieve competitive performance on these tasks. Regarding the inherent tokenization redundancy issue and unidirectional attention limitations in traditional causal LLMs, LENS consolidates the vocabulary space through token embedding clustering, and investigates bidirectional attention and various pooling strategies. Specifically, LENS simplifies lexicon matching by assigning each dimension to a specific token cluster, where semantically similar tokens are grouped together, and unlocking the full potential of LLMs through bidirectional attention. Extensive experiments demonstrate that LENS outperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB), delivering compact feature representations that match the sizes of dense counterparts. Notably, combining LENSE with dense embeddings achieves state-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).",
      "arxiv_url": "https://arxiv.org/abs/2501.09749",
      "pdf_url": "https://arxiv.org/pdf/2501.09749",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01877",
      "title": "When Should Dense Retrievers Be Updated in Evolving Corpora? Detecting Out-of-Distribution Corpora Using GradNormIR",
      "authors": [
        "Dayoon Ko",
        "Jinyoung Kim",
        "Sohyeon Kim",
        "Jin-Hwa Kim",
        "Jaehoon Lee",
        "Seonghak Song",
        "Minyoung Lee",
        "Gunhee Kim"
      ],
      "abstract": "Dense retrievers encode texts into embeddings to efficiently retrieve relevant documents from large databases in response to user queries. However, real-world corpora continually evolve, leading to a shift from the original training distribution of the retriever. Without timely updates or retraining, indexing newly emerging documents can degrade retrieval performance for future queries. Thus, identifying when a dense retriever requires an update is critical for maintaining robust retrieval systems. In this paper, we propose a novel task of predicting whether a corpus is out-of-distribution (OOD) relative to a dense retriever before indexing. Addressing this task allows us to proactively manage retriever updates, preventing potential retrieval failures. We introduce GradNormIR, an unsupervised approach that leverages gradient norms to detect OOD corpora effectively. Experiments on the BEIR benchmark demonstrate that GradNormIR enables timely updates of dense retrievers in evolving document collections, significantly enhancing retrieval robustness and efficiency.",
      "arxiv_url": "https://arxiv.org/abs/2506.01877",
      "pdf_url": "https://arxiv.org/pdf/2506.01877",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "22de954d4609282e701a7840bf3c73349aacc97e",
      "title": "CiteLab: Developing and Diagnosing LLM Citation Generation Workflows via the Human-LLM Interaction",
      "authors": [
        "Jiajun Shen",
        "Tong Zhou",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/22de954d4609282e701a7840bf3c73349aacc97e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13347",
      "title": "Craw4LLM: Efficient Web Crawling for LLM Pretraining",
      "authors": [
        "Shi Yu",
        "Zhiyuan Liu",
        "Chenyan Xiong"
      ],
      "abstract": "Web crawl is a main source of large language models'(LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality. This paper presents Craw4LLM, an efficient web crawling method that explores the web graph based on the preference of LLM pretraining. Specifically, it leverages the influence of a webpage in LLM pretraining as the priority score of the web crawler's scheduler, replacing the standard graph connectivity based priority. Our experiments on a web graph containing 900 million webpages from a commercial search engine's index demonstrate the efficiency of Craw4LLM in obtaining high-quality pretraining data. With just 21% URLs crawled, LLMs pretrained on Craw4LLM data reach the same downstream performances of previous crawls, significantly reducing the crawling waste and alleviating the burdens on websites. Our code is publicly available at https://github.com/cxcscmu/Craw4LLM.",
      "arxiv_url": "https://arxiv.org/abs/2502.13347",
      "pdf_url": "https://arxiv.org/pdf/2502.13347",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.00982",
      "title": "Are LLMs effective psychological assessors? Leveraging adaptive RAG for interpretable mental health screening through psychometric practice",
      "authors": [
        "Federico Ravenda",
        "Seyed Ali Bahrainian",
        "A. Raballo",
        "Antonietta Mira",
        "Noriko Kando"
      ],
      "abstract": "In psychological practices, standardized questionnaires serve as essential tools for assessing mental health through structured, clinically-validated questions (i.e., items). While social media platforms offer rich data for mental health screening, computational approaches often bypass these established clinical assessment tools in favor of black-box classification. We propose a novel questionnaire-guided screening framework that bridges psychological practice and computational methods through adaptive Retrieval-Augmented Generation (\\textit{aRAG}). Our approach links unstructured social media content and standardized clinical assessments by retrieving relevant posts for each questionnaire item and using Large Language Models (LLMs) to complete validated psychological instruments. Our findings demonstrate two key advantages of questionnaire-guided screening: First, when completing the Beck Depression Inventory-II (BDI-II), our approach matches or outperforms state-of-the-art performance on Reddit-based benchmarks without requiring training data. Second, we show that guiding LLMs through standardized questionnaires can yield superior results compared to directly prompting them for depression screening, while also providing a more interpretable assessment by linking model outputs to clinically validated diagnostic criteria. Additionally, we show, as a proof-of-concept, how our questionnaire-based methodology can be extended to other mental conditions' screening, highlighting the promising role of LLMs as psychological assessors.",
      "arxiv_url": "https://arxiv.org/abs/2501.00982",
      "pdf_url": "https://arxiv.org/pdf/2501.00982",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-01-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04629",
      "title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing",
      "authors": [
        "Xiangchao Yan",
        "Shiyang Feng",
        "Jiakang Yuan",
        "Renqiu Xia",
        "Bin Wang",
        "Bo Zhang",
        "Lei Bai"
      ],
      "abstract": "Survey paper plays a crucial role in scientific research, especially given the rapid growth of research publications. Recently, researchers have begun using LLMs to automate survey generation for better efficiency. However, the quality gap between LLM-generated surveys and those written by human remains significant, particularly in terms of outline quality and citation accuracy. To close these gaps, we introduce SurveyForge, which first generates the outline by analyzing the logical structure of human-written outlines and referring to the retrieved domain-related articles. Subsequently, leveraging high-quality papers retrieved from memory by our scholar navigation agent, SurveyForge can automatically generate and refine the content of the generated article. Moreover, to achieve a comprehensive evaluation, we construct SurveyBench, which includes 100 human-written survey papers for win-rate comparison and assesses AI-generated survey papers across three dimensions: reference, outline, and content quality. Experiments demonstrate that SurveyForge can outperform previous works such as AutoSurvey.",
      "arxiv_url": "https://arxiv.org/abs/2503.04629",
      "pdf_url": "https://arxiv.org/pdf/2503.04629",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.18626",
      "title": "The TIP of the Iceberg: Revealing a Hidden Class of Task-In-Prompt Adversarial Attacks on LLMs",
      "authors": [
        "Sergey Berezin",
        "R. Farahbakhsh",
        "Noel Crespi"
      ],
      "abstract": "We present a novel class of jailbreak adversarial attacks on LLMs, termed Task-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks (e.g., cipher decoding, riddles, code execution) into the model's prompt to indirectly generate prohibited inputs. To systematically assess the effectiveness of these attacks, we introduce the PHRYGE benchmark. We demonstrate that our techniques successfully circumvent safeguards in six state-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings highlight critical weaknesses in current LLM safety alignments and underscore the urgent need for more sophisticated defence strategies. Warning: this paper contains examples of unethical inquiries used solely for research purposes.",
      "arxiv_url": "https://arxiv.org/abs/2501.18626",
      "pdf_url": "https://arxiv.org/pdf/2501.18626",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.17061",
      "title": "Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models",
      "authors": [
        "Xinlong Chen",
        "Yuanxing Zhang",
        "Qiang Liu",
        "Jun Wu",
        "Fuzheng Zhang",
        "Tieniu Tan"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have exhibited impressive capabilities across various visual tasks, yet they remain hindered by the persistent challenge of hallucinations. To address this critical issue, we propose Mixture of Decoding (MoD), a novel approach for hallucination mitigation that dynamically adapts decoding strategies by evaluating the correctness of the model's attention on image tokens. Specifically, MoD measures the consistency between outputs generated from the original image tokens and those derived from the model's attended image tokens, to distinguish the correctness aforementioned. If the outputs are consistent, indicating correct attention, MoD employs a complementary strategy to amplify critical information. Conversely, if the outputs are inconsistent, suggesting erroneous attention, MoD utilizes a contrastive strategy to suppress misleading information. Extensive experiments demonstrate that MoD significantly outperforms existing decoding methods across multiple mainstream benchmarks, effectively mitigating hallucinations in LVLMs. The code is available at https://github.com/xlchen0205/MoD.",
      "arxiv_url": "https://arxiv.org/abs/2505.17061",
      "pdf_url": "https://arxiv.org/pdf/2505.17061",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00928",
      "title": "Deep Temporal Reasoning in Video Language Models: A Cross-Linguistic Evaluation of Action Duration and Completion through Perfect Times",
      "authors": [
        "Olga Loginova",
        "Sof'ia Ortega Loguinova"
      ],
      "abstract": "Human perception of events is intrinsically tied to distinguishing between completed (perfect and telic) and ongoing (durative) actions, a process mediated by both linguistic structure and visual cues. In this work, we introduce the \\textbf{Perfect Times} dataset, a novel, quadrilingual (English, Italian, Russian, and Japanese) multiple-choice question-answering benchmark designed to assess video-language models (VLMs) on temporal reasoning. By pairing everyday activity videos with event completion labels and perfectivity-tailored distractors, our dataset probes whether models truly comprehend temporal dynamics or merely latch onto superficial markers. Experimental results indicate that state-of-the-art models, despite their success on text-based tasks, struggle to mirror human-like temporal and causal reasoning grounded in video. This study underscores the necessity of integrating deep multimodal cues to capture the nuances of action duration and completion within temporal and causal video dynamics, setting a new standard for evaluating and advancing temporal reasoning in VLMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.00928",
      "pdf_url": "https://arxiv.org/pdf/2506.00928",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11469",
      "title": "If Attention Serves as a Cognitive Model of Human Memory Retrieval, What is the Plausible Memory Representation?",
      "authors": [
        "Ryosuke Yoshida",
        "Shinnosuke Isono",
        "Kohei Kajikawa",
        "Taiga Someya",
        "Yushi Sugimito",
        "Yohei Oseki"
      ],
      "abstract": "Recent work in computational psycholinguistics has revealed intriguing parallels between attention mechanisms and human memory retrieval, focusing primarily on vanilla Transformers that operate on token-level representations. However, computational psycholinguistic research has also established that syntactic structures provide compelling explanations for human sentence processing that token-level factors cannot fully account for. In this paper, we investigate whether the attention mechanism of Transformer Grammar (TG), which uniquely operates on syntactic structures as representational units, can serve as a cognitive model of human memory retrieval, using Normalized Attention Entropy (NAE) as a linking hypothesis between models and humans. Our experiments demonstrate that TG's attention achieves superior predictive power for self-paced reading times compared to vanilla Transformer's, with further analyses revealing independent contributions from both models. These findings suggest that human sentence processing involves dual memory representations -- one based on syntactic structures and another on token sequences -- with attention serving as the general memory retrieval algorithm, while highlighting the importance of incorporating syntactic structures as representational units.",
      "arxiv_url": "https://arxiv.org/abs/2502.11469",
      "pdf_url": "https://arxiv.org/pdf/2502.11469",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "23876c7c61fd8f32a53690ac442bcef543be1e1b",
      "title": "HyKGE: A Hypothesis Knowledge Graph Enhanced RAG Framework for Accurate and Reliable Medical LLMs Responses",
      "authors": [
        "Xinke Jiang",
        "Ruizhe Zhang",
        "Yongxin Xu",
        "Rihong Qiu",
        "Yue Fang",
        "Zhiyuan Wang",
        "Jinyi Tang",
        "Hongxin Ding",
        "Xu Chu",
        "Junfeng Zhao",
        "Yasha Wang"
      ],
      "abstract": "In this paper, we investigate the retrieval-augmented generation (RAG) based on Knowledge Graphs (KGs) to improve the accuracy and reliability of Large Language Models (LLMs). Recent approaches suffer from insufficient and repetitive knowledge retrieval, tedious and time-consuming query parsing, and monotonous knowledge utilization. To this end, we develop a Hy pothesis K nowledge G raph E nhanced ( HyKGE ) framework, which leverages LLMs’ powerful reasoning capacity to compensate for the incompleteness of user queries, optimizes the interaction process with LLMs, and provides diverse retrieved knowledge. Specifically, HyKGE explores the zero-shot capability and the rich knowledge of LLMs with Hypothesis Outputs to extend feasible exploration directions in the KGs, as well as the carefully curated prompt to enhance the density and efficiency of LLMs’ responses. Furthermore,",
      "arxiv_url": "https://www.semanticscholar.org/paper/23876c7c61fd8f32a53690ac442bcef543be1e1b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22096",
      "title": "Knowledge Base Construction for Knowledge-Augmented Text-to-SQL",
      "authors": [
        "Jinheon Baek",
        "Horst Samulowitz",
        "Oktie Hassanzadeh",
        "D. Subramanian",
        "Sola S. Shirai",
        "A. Gliozzo",
        "D. Bhattacharjya"
      ],
      "abstract": "Text-to-SQL aims to translate natural language queries into SQL statements, which is practical as it enables anyone to easily retrieve the desired information from databases. Recently, many existing approaches tackle this problem with Large Language Models (LLMs), leveraging their strong capability in understanding user queries and generating corresponding SQL code. Yet, the parametric knowledge in LLMs might be limited to covering all the diverse and domain-specific queries that require grounding in various database schemas, which makes generated SQLs less accurate oftentimes. To tackle this, we propose constructing the knowledge base for text-to-SQL, a foundational source of knowledge, from which we retrieve and generate the necessary knowledge for given queries. In particular, unlike existing approaches that either manually annotate knowledge or generate only a few pieces of knowledge for each query, our knowledge base is comprehensive, which is constructed based on a combination of all the available questions and their associated database schemas along with their relevant knowledge, and can be reused for unseen databases from different datasets and domains. We validate our approach on multiple text-to-SQL datasets, considering both the overlapping and non-overlapping database scenarios, where it outperforms relevant baselines substantially.",
      "arxiv_url": "https://arxiv.org/abs/2505.22096",
      "pdf_url": "https://arxiv.org/pdf/2505.22096",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11089",
      "title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention",
      "authors": [
        "Jingyang Yuan",
        "Huazuo Gao",
        "Damai Dai",
        "Junyu Luo",
        "Liang Zhao",
        "Zhengyan Zhang",
        "Zhenda Xie",
        "Y. X. Wei",
        "Lean Wang",
        "Zhiping Xiao",
        "Yuqing Wang",
        "C. Ruan",
        "Ming Zhang",
        "W. Liang",
        "Wangding Zeng"
      ],
      "abstract": "Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.",
      "arxiv_url": "https://arxiv.org/abs/2502.11089",
      "pdf_url": "https://arxiv.org/pdf/2502.11089",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17204",
      "title": "Order Matters: Investigate the Position Bias in Multi-constraint Instruction Following",
      "authors": [
        "Jie Zeng",
        "Qi He",
        "Qingyu Ren",
        "Jiaqing Liang",
        "Yanghua Xiao",
        "Weikang Zhou",
        "Zeye Sun",
        "Fei Yu"
      ],
      "abstract": "Real-world instructions with multiple constraints pose a significant challenge to existing large language models (LLMs). An observation is that the LLMs exhibit dramatic performance fluctuation when disturbing the order of the incorporated constraints. Yet, none of the existing works has systematically investigated this position bias problem in the field of multi-constraint instruction following. To bridge this gap, we design a probing task where we quantitatively measure the difficulty distribution of the constraints by a novel Difficulty Distribution Index (CDDI). Through the experimental results, we find that LLMs are more performant when presented with the constraints in a ``hard-to-easy'' order. This preference can be generalized to LLMs with different architecture or different sizes of parameters. Additionally, we conduct an explanation study, providing an intuitive insight into the correlation between the LLM's attention and constraint orders. Our code and dataset are publicly available at https://github.com/meowpass/PBIF.",
      "arxiv_url": "https://arxiv.org/abs/2502.17204",
      "pdf_url": "https://arxiv.org/pdf/2502.17204",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04642",
      "title": "TaDA: Training-free recipe for Decoding with Adaptive KV Cache Compression and Mean-centering",
      "authors": [
        "Vinay Joshi",
        "P. Brahma",
        "Zicheng Liu",
        "E. Barsoum"
      ],
      "abstract": "The key-value (KV) cache in transformer models is a critical component for efficient decoding or inference, yet its memory demands scale poorly with sequence length, posing a major challenge for scalable deployment of large language models. Among several approaches to KV cache compression, quantization of key and value activations has been widely explored. Most KV cache quantization methods still need to manage sparse and noncontiguous outliers separately. To address this, we introduce TaDA, a training-free recipe for KV cache compression with quantization precision that adapts to error sensitivity across layers and a mean centering to eliminate separate outlier handling. Our approach yields substantial accuracy improvements for multiple models supporting various context lengths. Moreover, our approach does not need to separately manage outlier elements -- a persistent hurdle in most traditional quantization methods. Experiments on standard benchmarks demonstrate that our technique reduces KV cache memory footprint to 27% of the original 16-bit baseline while achieving comparable accuracy. Our method paves the way for scalable and high-performance reasoning in language models by potentially enabling inference for longer context length models, reasoning models, and longer chain of thoughts.",
      "arxiv_url": "https://arxiv.org/abs/2506.04642",
      "pdf_url": "https://arxiv.org/pdf/2506.04642",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18878",
      "title": "Learning to Generate Structured Output with Schema Reinforcement Learning",
      "authors": [
        "Ya-Ting Lu",
        "Haolun Li",
        "Xin Cong",
        "Zhong Zhang",
        "Yesai Wu",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Fangming Liu",
        "Maosong Sun"
      ],
      "abstract": "This study investigates the structured generation capabilities of large language models (LLMs), focusing on producing valid JSON outputs against a given schema. Despite the widespread use of JSON in integrating language models with programs, there is a lack of comprehensive analysis and benchmarking of these capabilities. We explore various aspects of JSON generation, such as structure understanding, escaping, and natural language description, to determine how to assess and enable LLMs to generate valid responses. Building upon this, we propose SchemaBench features around 40K different JSON schemas to obtain and assess models' abilities in generating valid JSON. We find that the latest LLMs are still struggling to generate a valid JSON string. Moreover, we demonstrate that incorporating reinforcement learning with a Fine-grained Schema Validator can further enhance models' understanding of JSON schema, leading to improved performance. Our models demonstrate significant improvement in both generating JSON outputs and downstream tasks.",
      "arxiv_url": "https://arxiv.org/abs/2502.18878",
      "pdf_url": "https://arxiv.org/pdf/2502.18878",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11393",
      "title": "HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning",
      "authors": [
        "Xiaoyuan Li",
        "Moxin Li",
        "Rui Men",
        "Yichang Zhang",
        "Keqin Bao",
        "Wenjie Wang",
        "Fuli Feng",
        "Dayiheng Liu",
        "Junyang Lin"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable capabilities in commonsense reasoning; however, some variations in questions can trigger incorrect responses. Do these models truly understand commonsense knowledge, or just memorize expression patterns? To investigate this question, we present the first extensive robustness evaluation of LLMs in commonsense reasoning. We introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200 cases, by designing and compiling seven types of question variants. To construct this benchmark, we propose a two-stage method to develop Chinese HellaSwag, a finely annotated dataset comprising 12,000 instances across 56 categories. We conduct extensive experiments on 41 representative LLMs, revealing that these LLMs are far from robust in commonsense reasoning. Furthermore, this robustness varies depending on the language in which the LLM is tested. This work establishes a high-quality evaluation benchmark, with extensive experiments offering valuable insights to the community in commonsense reasoning for LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2502.11393",
      "pdf_url": "https://arxiv.org/pdf/2502.11393",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.10085",
      "title": "Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning",
      "authors": [
        "Chenxi Huang",
        "Shaotian Yan",
        "Liang Xie",
        "Binbin Lin",
        "Sinan Fan",
        "Yue Xin",
        "Deng Cai",
        "Chen Shen",
        "Jieping Ye"
      ],
      "abstract": "Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient Fine-Tuning (PEFT) method, has attracted widespread attention for significantly improving parameter efficiency by editing representation space alone. In this work, we investigate applying ReFT to complex reasoning tasks. However, directly using the native ReFT method, which modifies fixed representations at the beginning and end of each layer, yields suboptimal performance, as these fixed-position representations have uncertain impact on the outputs. We observe that, in complex reasoning tasks, there often exist certain critical representations. These representations either integrate significant information from preceding layers or regulate subsequent layer representations. Through layer-by-layer propagation, they exert a substantial influence on the final output. Naturally, fine-tuning these critical representations has the potential to greatly enhance reasoning performance. Building upon these insights, we propose Critical Representation Fine-Tuning (CRFT), a novel method that identifies and optimizes these critical representations through information flow analysis. CRFT operates within a supervised learning framework, dynamically optimizing critical representations in a low-rank linear subspace while freezing the base model. The effectiveness and efficiency of our method are validated across eight benchmarks for arithmetic and commonsense reasoning, using LLaMA and Mistral model families. Furthermore, our method also adapts effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work highlights the untapped potential of representation-level optimization for CoT reasoning, offering a lightweight yet powerful alternative to traditional PEFT methods.",
      "arxiv_url": "https://arxiv.org/abs/2507.10085",
      "pdf_url": "https://arxiv.org/pdf/2507.10085",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17878",
      "title": "Towards Enhanced Immersion and Agency for LLM-based Interactive Drama",
      "authors": [
        "Hongqiu Wu",
        "Weiqi Wu",
        "Tianyang Xu",
        "Jiameng Zhang",
        "Hai Zhao"
      ],
      "abstract": "LLM-based Interactive Drama is a novel AI-based dialogue scenario, where the user (i.e. the player) plays the role of a character in the story, has conversations with characters played by LLM agents, and experiences an unfolding story. This paper begins with understanding interactive drama from two aspects: Immersion, the player's feeling of being present in the story, and Agency, the player's ability to influence the story world. Both are crucial to creating an enjoyable interactive experience, while they have been underexplored in previous work. To enhance these two aspects, we first propose Playwriting-guided Generation, a novel method that helps LLMs craft dramatic stories with substantially improved structures and narrative quality. Additionally, we introduce Plot-based Reflection for LLM agents to refine their reactions to align with the player's intentions. Our evaluation relies on human judgment to assess the gains of our methods in terms of immersion and agency.",
      "arxiv_url": "https://arxiv.org/abs/2502.17878",
      "pdf_url": "https://arxiv.org/pdf/2502.17878",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "24e2d7f5be6be64f384b7b894fdaf4b2b225187b",
      "title": "Sign2Vis: Automated Data Visualization from Sign Language",
      "authors": [
        "Yao Wan",
        "Yang Wu",
        "Zhen Li",
        "Guobiao Zhang",
        "Hongyu Zhang",
        "Zhou Zhao",
        "Hai Jin",
        "April Wang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/24e2d7f5be6be64f384b7b894fdaf4b2b225187b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.03124",
      "title": "PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models",
      "authors": [
        "Mingyang Song",
        "Zhao-yu Su",
        "Xiaoye Qu",
        "Jiawei Zhou",
        "Yu Cheng"
      ],
      "abstract": "Process-level Reward Models (PRMs) are crucial for complex reasoning and decision-making tasks, where each intermediate step plays an important role in the reasoning process. Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios. However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs'performance systematically. To address this gap, we introduce PRMBench, a process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including simplicity, soundness, and sensitivity. In our experiments on 15 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs. These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research. We hope PRMBench can be a robust bench for advancing research on PRM evaluation and development.",
      "arxiv_url": "https://arxiv.org/abs/2501.03124",
      "pdf_url": "https://arxiv.org/pdf/2501.03124",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.03561",
      "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement",
      "authors": [
        "Runnan Fang",
        "Xiaobin Wang",
        "Yuan Liang",
        "Shuofei Qiao",
        "Jialong Wu",
        "Zekun Xi",
        "Ningyu Zhang",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Huajun Chen"
      ],
      "abstract": "In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.",
      "arxiv_url": "https://arxiv.org/abs/2504.03561",
      "pdf_url": "https://arxiv.org/pdf/2504.03561",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-04-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18874",
      "title": "Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework",
      "authors": [
        "Kaishuai Xu",
        "Tiezheng Yu",
        "Wenjun Hou",
        "Yi Cheng",
        "Liangyou Li",
        "Xin Jiang",
        "Lifeng Shang",
        "Qun Liu",
        "Wenjie Li"
      ],
      "abstract": "Large Language Models (LLMs) are being used more and more extensively for automated evaluation in various scenarios. Previous studies have attempted to fine-tune open-source LLMs to replicate the evaluation explanations and judgments of powerful proprietary models, such as GPT-4. However, these methods are largely limited to text-based analyses under predefined general criteria, resulting in reduced adaptability for unseen instructions and demonstrating instability in evaluating adherence to quantitative and structural constraints. To address these limitations, we propose a novel evaluation framework, ARJudge, that adaptively formulates evaluation criteria and synthesizes both text-based and code-driven analyses to evaluate LLM responses. ARJudge consists of two components: a fine-tuned Analyzer that generates multi-faceted evaluation analyses and a tuning-free Refiner that combines and refines all analyses to make the final judgment. We construct a Composite Analysis Corpus that integrates tasks for evaluation criteria generation alongside text-based and code-driven analysis generation to train the Analyzer. Our results demonstrate that ARJudge outperforms existing fine-tuned evaluators in effectiveness and robustness. Furthermore, it demonstrates the importance of multi-faceted evaluation and code-driven analyses in enhancing evaluation capabilities.",
      "arxiv_url": "https://arxiv.org/abs/2502.18874",
      "pdf_url": "https://arxiv.org/pdf/2502.18874",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20322",
      "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms",
      "authors": [
        "Mengru Wang",
        "Ziwen Xu",
        "Shengyu Mao",
        "Shumin Deng",
        "Zhaopeng Tu",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.",
      "arxiv_url": "https://arxiv.org/abs/2505.20322",
      "pdf_url": "https://arxiv.org/pdf/2505.20322",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10789",
      "title": "Data Caricatures: On the Representation of African American Language in Pretraining Corpora",
      "authors": [
        "Nicholas Deas",
        "Blake Vente",
        "Amith Ananthram",
        "Jessica A. Grieser",
        "D. Patton",
        "Shana Kleiner",
        "James Shepard",
        "Kathleen McKeown"
      ],
      "abstract": "With a combination of quantitative experiments, human judgments, and qualitative analyses, we evaluate the quantity and quality of African American Language (AAL) representation in 12 predominantly English, open-source pretraining corpora. We specifically focus on the sources, variation, and naturalness of included AAL texts representing the AAL-speaking community. We find that AAL is underrepresented in all evaluated pretraining corpora compared to US demographics, constituting as few as 0.007% and at most 0.18% of documents. We also find that more than 25% of AAL texts in C4 may be perceived as inappropriate for LLMs to generate and to reinforce harmful stereotypes. Finally, we find that most automated filters are more likely to conserve White Mainstream English (WME) texts over AAL in pretraining corpora.",
      "arxiv_url": "https://arxiv.org/abs/2503.10789",
      "pdf_url": "https://arxiv.org/pdf/2503.10789",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.07784",
      "title": "Domain Regeneration: How well do LLMs match syntactic properties of text domains?",
      "authors": [
        "Da Ju",
        "Hagen Blix",
        "Adina Williams"
      ],
      "abstract": "Recent improvement in large language model performance have, in all likelihood, been accompanied by improvement in how well they can approximate the distribution of their training data. In this work, we explore the following question: which properties of text domains do LLMs faithfully approximate, and how well do they do so? Applying observational approaches familiar from corpus linguistics, we prompt a commonly used, opensource LLM to regenerate text from two domains of permissively licensed English text which are often contained in LLM training data -- Wikipedia and news text. This regeneration paradigm allows us to investigate whether LLMs can faithfully match the original human text domains in a fairly semantically-controlled setting. We investigate varying levels of syntactic abstraction, from more simple properties like sentence length, and article readability, to more complex and higher order properties such as dependency tag distribution, parse depth, and parse complexity. We find that the majority of the regenerated distributions show a shifted mean, a lower standard deviation, and a reduction of the long tail, as compared to the human originals.",
      "arxiv_url": "https://arxiv.org/abs/2505.07784",
      "pdf_url": "https://arxiv.org/pdf/2505.07784",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2637ce4b80a5fa4463d528c3816c790a61ee76cc",
      "title": "V-Oracle: Making Progressive Reasoning in Deciphering Oracle Bones for You and Me",
      "authors": [
        "Runqi Qiao",
        "Qiuna Tan",
        "Guanting Dong",
        "MinhuiWu MinhuiWu",
        "Jiapeng Wang",
        "Yifan Zhang",
        "Zhuoma Gongque",
        "Chong Sun",
        "Yida Xu",
        "Yadong Xue",
        "Ye Tian",
        "Zhimin Bao",
        "Lan Yang",
        "Chen Li",
        "Honggang Zhang"
      ],
      "abstract": "Oracle Bone Script (OBS) is a vital treasure of human civilization, rich in insights from ancient societies. However, the evolution of written language over millennia complicates its decipherment. In this paper, we propose V-Oracle , an innovative framework that utilizes Large Multi-modal Models (LMMs) for interpreting OBS. V-Oracle applies principles of pictographic character formation and frames the task as a visual question-answering (VQA) problem, establishing a multi-step reasoning chain. It proposes a multi-dimensional data augmentation for synthesizing high-quality OBS samples, and also implements a multi-phase oracle alignment tuning to improve LMMs’ visual reasoning capabilities. More-over, to bridge the evaluation gap in the OBS field, we further introduce Oracle-Bench , a comprehensive benchmark that emphasizes process-oriented assessment and incorporates both standard and out-of-distribution setups for realistic evaluation. Extensive experimental re-sults can demonstrate the effectiveness of our method in providing quantitative analyses and superior deciphering capability.",
      "arxiv_url": "https://www.semanticscholar.org/paper/2637ce4b80a5fa4463d528c3816c790a61ee76cc",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.19993",
      "title": "CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems",
      "authors": [
        "Haochen Zhang",
        "Tianyi Zhang",
        "Junze Yin",
        "Oren Gal",
        "Anshumali Shrivastava",
        "Vladimir Braverman"
      ],
      "abstract": "Recommender systems play a pivotal role in providing relevant content to users. With the rapid development of large language models (LLMs), researchers have begun utilizing LLMs to build more powerful recommender systems. However, existing approaches that focus on aligning LLMs with recommendation tasks do not fully leverage their sequential information processing capabilities, leading to suboptimal performance. In this paper, we propose a novel system called compressed vocabulary expansion (CoVE). In CoVE, each item is assigned a unique ID within the expanded vocabulary. Our framework effectively capitalizes on sequence understanding abilities of LLMs, significantly enhancing their performance on recommendation tasks. Additionally, we compress the embedding layer, making CoVE practical for large-scale industrial applications. The effectiveness and performance of CoVE are demonstrated through comprehensive experiments on multiple recommendation datasets and comparisons with prior works. Our code can be found at https://github.com/HaochenZhang717/CoVE-official-Repo.",
      "arxiv_url": "https://arxiv.org/abs/2506.19993",
      "pdf_url": "https://arxiv.org/pdf/2506.19993",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.21821",
      "title": "Physics: Benchmarking Foundation Models on University-Level Physics Problem Solving",
      "authors": [
        "Kaiyue Feng",
        "Yilun Zhao",
        "Yixin Liu",
        "Tianyu Yang",
        "Chen Zhao",
        "John Sous",
        "Arman Cohan"
      ],
      "abstract": "We introduce PHYSICS, a comprehensive benchmark for university-level physics problem solving. It contains 1297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. We develop a robust automated evaluation system for precise and reliable validation. Our evaluation of leading foundation models reveals substantial limitations. Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems. Through comprehensive error analysis, exploration of diverse prompting strategies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify key areas for improvement, laying the foundation for future advancements.",
      "arxiv_url": "https://arxiv.org/abs/2503.21821",
      "pdf_url": "https://arxiv.org/pdf/2503.21821",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-03-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11357",
      "title": "Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents",
      "authors": [
        "Vardaan Pahuja",
        "Yadong Lu",
        "Corby Rosset",
        "Boyu Gou",
        "Arindam Mitra",
        "Spencer Whitehead",
        "Yu Su",
        "Ahmed Awadallah"
      ],
      "abstract": "Recent success in large multimodal models (LMMs) has sparked promising applications of agents capable of autonomously completing complex web tasks. While open-source LMM agents have made significant advances in offline evaluation benchmarks, their performance still falls substantially short of human-level capabilities in more realistic online settings. A key bottleneck is the lack of diverse and large-scale trajectory-level datasets across various domains, which are expensive to collect. In this paper, we address this challenge by developing a scalable recipe to synthesize the largest and most diverse trajectory-level dataset to date, containing over 94K successful multimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and 33M web elements. In particular, we leverage extensive web exploration and refinement to obtain diverse task intents. The average cost is 28 cents per successful trajectory, making it affordable to a wide range of users in the community. Leveraging this dataset, we train Explorer, a multimodal web agent, and demonstrate strong performance on both offline and online web agent benchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++. Additionally, our experiments highlight data scaling as a key driver for improving web agent capabilities. We hope this study makes state-of-the-art LMM-based agent research at a larger scale more accessible.",
      "arxiv_url": "https://arxiv.org/abs/2502.11357",
      "pdf_url": "https://arxiv.org/pdf/2502.11357",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.15501",
      "title": "ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution",
      "authors": [
        "Alexandru Coca",
        "Mark Gaynor",
        "Zhenxing Zhang",
        "Jianpeng Cheng",
        "Bo-Hsiang Tseng",
        "Peter Boothroyd",
        "Héctor Martínez Alonso",
        "Diarmuid Ó Séaghdha",
        "Anders Johannsen"
      ],
      "abstract": "This work evaluates the potential of large language models (LLMs) to power digital assistants capable of complex action execution. These assistants rely on pre-trained programming knowledge to execute multi-step goals by composing objects and functions defined in assistant libraries into action execution programs. To achieve this, we develop ASPERA, a framework comprising an assistant library simulation and a human-assisted LLM data generation engine. Our engine allows developers to guide LLM generation of high-quality tasks consisting of complex user queries, simulation state and corresponding validation programs, tackling data availability and evaluation robustness challenges. Alongside the framework we release Asper-Bench, an evaluation dataset of 250 challenging tasks generated using ASPERA, which we use to show that program generation grounded in custom assistant libraries is a significant challenge to LLMs compared to dependency-free code generation.",
      "arxiv_url": "https://arxiv.org/abs/2507.15501",
      "pdf_url": "https://arxiv.org/pdf/2507.15501",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.18202",
      "title": "Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection",
      "authors": [
        "San Kim",
        "Jonghwi Kim",
        "Yejin Jeon",
        "Gary Lee"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by providing external knowledge for accurate and up-to-date responses. However, this reliance on external sources exposes a security risk, attackers can inject poisoned documents into the knowledge base to steer the generation process toward harmful or misleading outputs. In this paper, we propose Gradient-based Masked Token Probability (GMTP), a novel defense method to detect and filter out adversarially crafted documents. Specifically, GMTP identifies high-impact tokens by examining gradients of the retriever's similarity function. These key tokens are then masked, and their probabilities are checked via a Masked Language Model (MLM). Since injected tokens typically exhibit markedly low masked-token probabilities, this enables GMTP to easily detect malicious documents and achieve high-precision filtering. Experiments demonstrate that GMTP is able to eliminate over 90% of poisoned content while retaining relevant documents, thus maintaining robust retrieval and generation performance across diverse datasets and adversarial settings.",
      "arxiv_url": "https://arxiv.org/abs/2507.18202",
      "pdf_url": "https://arxiv.org/pdf/2507.18202",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-07-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.20199",
      "title": "Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains",
      "authors": [
        "Juntian Zhang",
        "Chuanqi Cheng",
        "Yuhan Liu",
        "Wei Liu",
        "Jian Luan",
        "Rui Yan"
      ],
      "abstract": "Vision-language models (VLMs) achieve remarkable success in single-image tasks. However, real-world scenarios often involve intricate multi-image inputs, leading to a notable performance decline as models struggle to disentangle critical information scattered across complex visual features. In this work, we propose Focus-Centric Visual Chain, a novel paradigm that enhances VLMs'perception, comprehension, and reasoning abilities in multi-image scenarios. To facilitate this paradigm, we propose Focus-Centric Data Synthesis, a scalable bottom-up approach for synthesizing high-quality data with elaborate reasoning paths. Through this approach, We construct VISC-150K, a large-scale dataset with reasoning data in the form of Focus-Centric Visual Chain, specifically designed for multi-image tasks. Experimental results on seven multi-image benchmarks demonstrate that our method achieves average performance gains of 3.16% and 2.24% across two distinct model architectures, without compromising the general vision-language capabilities. our study represents a significant step toward more robust and capable vision-language systems that can handle complex visual scenarios.",
      "arxiv_url": "https://arxiv.org/abs/2504.20199",
      "pdf_url": "https://arxiv.org/pdf/2504.20199",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11427",
      "title": "Mergenetic: a Simple Evolutionary Model Merging Library",
      "authors": [
        "Adrian Robert Minut",
        "Tommaso Mencattini",
        "Andrea Santilli",
        "Donato Crisostomi",
        "E. Rodolà"
      ],
      "abstract": "Model merging allows combining the capabilities of existing models into a new one - post hoc, without additional training. This has made it increasingly popular thanks to its low cost and the availability of libraries that support merging on consumer GPUs. Recent work shows that pairing merging with evolutionary algorithms can boost performance, but no framework currently supports flexible experimentation with such strategies in language models. We introduce Mergenetic, an open-source library for evolutionary model merging. Mergenetic enables easy composition of merging methods and evolutionary algorithms while incorporating lightweight fitness estimators to reduce evaluation costs. We describe its design and demonstrate that Mergenetic produces competitive results across tasks and languages using modest hardware.",
      "arxiv_url": "https://arxiv.org/abs/2505.11427",
      "pdf_url": "https://arxiv.org/pdf/2505.11427",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20258",
      "title": "LLM as a Broken Telephone: Iterative Generation Distorts Information",
      "authors": [
        "Amr Mohamed",
        "Mingmeng Geng",
        "M. Vazirgiannis",
        "Guokan Shang"
      ],
      "abstract": "As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs. Inspired by the\"broken telephone\"effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation. Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.",
      "arxiv_url": "https://arxiv.org/abs/2502.20258",
      "pdf_url": "https://arxiv.org/pdf/2502.20258",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.12835",
      "title": "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back Home",
      "authors": [
        "Viktor Moskvoretskii",
        "M. Lysyuk",
        "M. Salnikov",
        "Nikolay Ivanov",
        "Sergey Pletenev",
        "Daria Galimzianova",
        "Nikita Krayko",
        "Vasily Konovalov",
        "Irina Nikishina",
        "Alexander Panchenko"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) improves correctness of Question Answering (QA) and addresses hallucinations in Large Language Models (LLMs), yet greatly increase computational costs. Besides, RAG is not always needed as may introduce irrelevant information. Recent adaptive retrieval methods integrate LLMs' intrinsic knowledge with external information appealing to LLM self-knowledge, but they often neglect efficiency evaluations and comparisons with uncertainty estimation techniques. We bridge this gap by conducting a comprehensive analysis of 35 adaptive retrieval methods, including 8 recent approaches and 27 uncertainty estimation techniques, across 6 datasets using 10 metrics for QA performance, self-knowledge, and efficiency. Our findings show that uncertainty estimation techniques often outperform complex pipelines in terms of efficiency and self-knowledge, while maintaining comparable QA performance.",
      "arxiv_url": "https://arxiv.org/abs/2501.12835",
      "pdf_url": "https://arxiv.org/pdf/2501.12835",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-01-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24264",
      "title": "Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations",
      "authors": [
        "Xin Quan",
        "Marco Valentino",
        "Louise A. Dennis",
        "André Freitas"
      ],
      "abstract": "Natural language explanations play a fundamental role in Natural Language Inference (NLI) by revealing how premises logically entail hypotheses. Recent work has shown that the interaction of large language models (LLMs) with theorem provers (TPs) can help verify and improve the validity of NLI explanations. However, TPs require translating natural language into machine-verifiable formal representations, a process that introduces the risk of semantic information loss and unfaithful interpretation, an issue compounded by LLMs' challenges in capturing critical logical structures with sufficient precision. Moreover, LLMs are still limited in their capacity for rigorous and robust proof construction within formal verification frameworks. To mitigate issues related to faithfulness and robustness, this paper investigates strategies to (1) alleviate semantic loss during autoformalisation, (2) efficiently identify and correct syntactic errors in logical representations, (3) explicitly use logical expressions to guide LLMs in generating structured proof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback for iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree using different LLMs demonstrate that the proposed strategies yield significant improvements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation refinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover, we show that specific interventions on the hybrid LLM-TP architecture can substantially improve efficiency, drastically reducing the number of iterations required for successful verification.",
      "arxiv_url": "https://arxiv.org/abs/2505.24264",
      "pdf_url": "https://arxiv.org/pdf/2505.24264",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13259",
      "title": "HumT DumT: Measuring and controlling human-like language in LLMs",
      "authors": [
        "Myra Cheng",
        "Sunny Yu",
        "Dan Jurafsky"
      ],
      "abstract": "Should LLMs generate language that makes them seem human? Human-like language might improve user experience, but might also lead to deception, overreliance, and stereotyping. Assessing these potential impacts requires a systematic way to measure human-like tone in LLM outputs. We introduce HumT and SocioT, metrics for human-like tone and other dimensions of social perceptions in text data based on relative probabilities from an LLM. By measuring HumT across preference and usage datasets, we find that users prefer less human-like outputs from LLMs in many contexts. HumT also offers insights into the perceptions and impacts of anthropomorphism: human-like LLM outputs are highly correlated with warmth, social closeness, femininity, and low status, which are closely linked to the aforementioned harms. We introduce DumT, a method using HumT to systematically control and reduce the degree of human-like tone while preserving model performance. DumT offers a practical approach for mitigating risks associated with anthropomorphic language generation.",
      "arxiv_url": "https://arxiv.org/abs/2502.13259",
      "pdf_url": "https://arxiv.org/pdf/2502.13259",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14627",
      "title": "ATRI: Mitigating Multilingual Audio Text Retrieval Inconsistencies by Reducing Data Distribution Errors",
      "authors": [
        "Yuguo Yin",
        "Yuxin Xie",
        "Wenyuan Yang",
        "Dongchao Yang",
        "Jinghan Ru",
        "Xianwei Zhuang",
        "Liming Liang",
        "Yuexian Zou"
      ],
      "abstract": "Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims to retrieve audio clips or multilingual texts from databases. However, existing ML-ATR schemes suffer from inconsistencies for instance similarity matching across languages. We theoretically analyze the inconsistency in terms of both multilingual modal alignment direction error and weight error, and propose the theoretical weight error upper bound for quantifying the inconsistency. Based on the analysis of the weight error upper bound, we find that the inconsistency problem stems from the data distribution error caused by random sampling of languages. We propose a consistent ML-ATR scheme using 1-to-k contrastive learning and audio-English co-anchor contrastive learning, aiming to mitigate the negative impact of data distribution error on recall and consistency in ML-ATR. Experimental results on the translated AudioCaps and Clotho datasets show that our scheme achieves state-of-the-art performance on recall and consistency metrics for eight mainstream languages, including English. Our code will be available at https://github.com/ATRI-ACL/ATRI-ACL.",
      "arxiv_url": "https://arxiv.org/abs/2502.14627",
      "pdf_url": "https://arxiv.org/pdf/2502.14627",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2784b8bb159a72df67c1343c4c5c3495f682999b",
      "title": "Generating OpenAPI Specifications from Online API Documentation with Large Language Models",
      "authors": [
        "Koren Lazar",
        "Matan Vetzler",
        "K. Kate",
        "Jason Tsay",
        "David Boaz",
        "Himanshu Gupta",
        "Avraham Shinnar",
        "R. Vallam",
        "David Amid",
        "Esther Goldbraich",
        "Jim Laredo",
        "Ateret Anaby-Tavor"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/2784b8bb159a72df67c1343c4c5c3495f682999b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.04254",
      "title": "CompileAgent: Automated Real-World Repo-Level Compilation with Tool-Integrated LLM-based Agent System",
      "authors": [
        "Li Hu",
        "Guoqiang Chen",
        "Xiuwei Shang",
        "Shaoyin Cheng",
        "Benlong Wu",
        "Gangyang Li",
        "Xu Zhu",
        "Weiming Zhang",
        "Neng H. Yu"
      ],
      "abstract": "With open-source projects growing in size and complexity, manual compilation becomes tedious and error-prone, highlighting the need for automation to improve efficiency and accuracy. However, the complexity of compilation instruction search and error resolution makes automatic compilation challenging. Inspired by the success of LLM-based agents in various fields, we propose CompileAgent, the first LLM-based agent framework dedicated to repo-level compilation. CompileAgent integrates five tools and a flow-based agent strategy, enabling interaction with software artifacts for compilation instruction search and error resolution. To measure the effectiveness of our method, we design a public repo-level benchmark CompileAgentBench, and we also design two baselines for comparison by combining two compilation-friendly schemes. The performance on this benchmark shows that our method significantly improves the compilation success rate, ranging from 10% to 71%. Meanwhile, we evaluate the performance of CompileAgent under different agent strategies and verify the effectiveness of the flow-based strategy. Additionally, we emphasize the scalability of CompileAgent, further expanding its application prospects.",
      "arxiv_url": "https://arxiv.org/abs/2505.04254",
      "pdf_url": "https://arxiv.org/pdf/2505.04254",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-05-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "27a62e5f4597fb898f81d1e919650dcf8ec3d1e3",
      "title": "Translation and Fusion Improves Cross-lingual Information Extraction",
      "authors": [
        "Yang Chen",
        "Vedaant Shah",
        "Alan Ritter"
      ],
      "abstract": "Large language models (LLMs) combined with instruction tuning have shown significant progress in information extraction (IE) tasks, exhibiting strong generalization capabilities to unseen datasets by following annotation guidelines. However, their applicability to low-resource languages remains limited due to lack of both labeled data for fine-tuning, and unlabeled text for pre-training. In this paper, we pro-pose TransFusion, a framework in which models are fine-tuned to use English translations of low-resource language data, enabling more precise predictions through annotation fusion. Based on TransFusion, we introduce GoLLIE-TF, a cross-lingual instruction-tuned LLM for IE tasks, designed to close the performance gap between high and low-resource languages. Our experiments across twelve multilingual IE datasets spanning 50 languages demonstrate that GoLLIE-TF achieves better cross-lingual transfer over the base model. In addition, we show that TransFusion significantly improves low-resource language named entity recognition when applied to proprietary models such as GPT-4 (+5 F1) with a prompting approach, or fine-tuning different language models including decoder-only (+14 F1) and encoder-only (+13 F1) architectures.",
      "arxiv_url": "https://www.semanticscholar.org/paper/27a62e5f4597fb898f81d1e919650dcf8ec3d1e3",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10507",
      "title": "The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks",
      "authors": [
        "Benedikt Ebing",
        "Goran Glavas"
      ],
      "abstract": "Translation-based strategies for cross-lingual transfer XLT such as translate-train -- training on noisy target language data translated from the source language -- and translate-test -- evaluating on noisy source language data translated from the target language -- are competitive XLT baselines. In XLT for token classification tasks, however, these strategies include label projection, the challenging step of mapping the labels from each token in the original sentence to its counterpart(s) in the translation. Although word aligners (WAs) are commonly used for label projection, the low-level design decisions for applying them to translation-based XLT have not been systematically investigated. Moreover, recent marker-based methods, which project labeled spans by inserting tags around them before (or after) translation, claim to outperform WAs in label projection for XLT. In this work, we revisit WAs for label projection, systematically investigating the effects of low-level design decisions on token-level XLT: (i) the algorithm for projecting labels between (multi-)token spans, (ii) filtering strategies to reduce the number of noisily mapped labels, and (iii) the pre-tokenization of the translated sentences. We find that all of these substantially impact translation-based XLT performance and show that, with optimized choices, XLT with WA offers performance at least comparable to that of marker-based methods. We then introduce a new projection strategy that ensembles translate-train and translate-test predictions and demonstrate that it substantially outperforms the marker-based projection. Crucially, we show that our proposed ensembling also reduces sensitivity to low-level WA design choices, resulting in more robust XLT for token classification tasks.",
      "arxiv_url": "https://arxiv.org/abs/2505.10507",
      "pdf_url": "https://arxiv.org/pdf/2505.10507",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.12325",
      "title": "LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media",
      "authors": [
        "Haiqi Zhang",
        "Zhengyuan Zhu",
        "Zeyu Zhang",
        "Chengkai Li"
      ],
      "abstract": "With the rapid expansion of content on social media platforms, analyzing and comprehending online discourse has become increasingly complex. This paper introduces LLMTaxo, a novel framework leveraging large language models for the automated construction of taxonomies of factual claims from social media by generating topics at multiple levels of granularity. The resulting hierarchical structure significantly reduces redundancy and improves information accessibility. We also propose dedicated taxonomy evaluation metrics to enable comprehensive assessment. Evaluations conducted on three diverse datasets demonstrate LLMTaxo's effectiveness in producing clear, coherent, and comprehensive taxonomies. Among the evaluated models, GPT-4o mini consistently outperforms others across most metrics. The framework's flexibility and low reliance on manual intervention underscore its potential for broad applicability.",
      "arxiv_url": "https://arxiv.org/abs/2504.12325",
      "pdf_url": "https://arxiv.org/pdf/2504.12325",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18511",
      "title": "ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models",
      "authors": [
        "Xuxu Liu",
        "Siyuan Liang",
        "Mengya Han",
        "Yong Luo",
        "Aishan Liu",
        "Xiantao Cai",
        "Zheng He",
        "Dacheng Tao"
      ],
      "abstract": "Generative large language models are crucial in natural language processing, but they are vulnerable to backdoor attacks, where subtle triggers compromise their behavior. Although backdoor attacks against LLMs are constantly emerging, existing benchmarks remain limited in terms of sufficient coverage of attack, metric system integrity, backdoor attack alignment. And existing pre-trained backdoor attacks are idealized in practice due to resource access constraints. Therefore we establish $\\textit{ELBA-Bench}$, a comprehensive and unified framework that allows attackers to inject backdoor through parameter efficient fine-tuning ($\\textit{e.g.,}$ LoRA) or without fine-tuning techniques ($\\textit{e.g.,}$ In-context-learning). $\\textit{ELBA-Bench}$ provides over 1300 experiments encompassing the implementations of 12 attack methods, 18 datasets, and 12 LLMs. Extensive experiments provide new invaluable findings into the strengths and limitations of various attack strategies. For instance, PEFT attack consistently outperform without fine-tuning approaches in classification tasks while showing strong cross-dataset generalization with optimized triggers boosting robustness; Task-relevant backdoor optimization techniques or attack prompts along with clean and adversarial demonstrations can enhance backdoor attack success while preserving model performance on clean samples. Additionally, we introduce a universal toolbox designed for standardized backdoor attack research, with the goal of propelling further progress in this vital area.",
      "arxiv_url": "https://arxiv.org/abs/2502.18511",
      "pdf_url": "https://arxiv.org/pdf/2502.18511",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.08248",
      "title": "Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models",
      "authors": [
        "Yifu Qiu",
        "Varun Embar",
        "Yizhe Zhang",
        "N. Jaitly",
        "Shay B. Cohen",
        "Benjamin Han"
      ],
      "abstract": "Recent advancements in long-context language models (LCLMs) promise to transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With their expanded context windows, LCLMs can process entire knowledge bases and perform retrieval and reasoning directly -- a capability we define as In-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like LOFT often overestimate LCLM performance by providing overly simplified contexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs in more realistic scenarios by including confounding passages retrieved with strong retrievers. We then propose three methods to enhance LCLM performance: (1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which uses attention heads to filter and de-noise long contexts during decoding, and (3) joint retrieval head training alongside the generation head. Our evaluation of five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with our best approach applied to Mistral-7B: +17 and +15 points by Exact Match on LOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised fine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks despite being a much smaller model.",
      "arxiv_url": "https://arxiv.org/abs/2501.08248",
      "pdf_url": "https://arxiv.org/pdf/2501.08248",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-01-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11767",
      "title": "From Selection to Generation: A Survey of LLM-based Active Learning",
      "authors": [
        "Yu Xia",
        "Subhojyoti Mukherjee",
        "Zhouhang Xie",
        "Junda Wu",
        "Xintong Li",
        "Ryan Aponte",
        "Hanjia Lyu",
        "Joe Barrow",
        "Hongjie Chen",
        "Franck Dernoncourt",
        "B. Kveton",
        "Tong Yu",
        "Ruiyi Zhang",
        "Jiuxiang Gu",
        "Nesreen K. Ahmed",
        "Yu Wang",
        "Xiang Chen",
        "Hanieh Deilamsalehy",
        "Sungchul Kim",
        "Zhengmian Hu",
        "Yue Zhao",
        "Nedim Lipka",
        "Seunghyun Yoon",
        "Tinghui Huang",
        "Zichao Wang",
        "Puneet Mathur",
        "Soumyabrata Pal",
        "Koyel Mukherjee",
        "Zhehao Zhang",
        "Namyong Park",
        "T. Nguyen",
        "Jiebo Luo",
        "Ryan A. Rossi",
        "Julian J. McAuley"
      ],
      "abstract": "Active Learning (AL) has been a powerful paradigm for improving model efficiency and performance by selecting the most informative data points for labeling and training. In recent active learning frameworks, Large Language Models (LLMs) have been employed not only for selection but also for generating entirely new data instances and providing more cost-effective annotations. Motivated by the increasing importance of high-quality data and efficient model training in the era of LLMs, we present a comprehensive survey on LLM-based Active Learning. We introduce an intuitive taxonomy that categorizes these techniques and discuss the transformative roles LLMs can play in the active learning loop. We further examine the impact of AL on LLM learning paradigms and its applications across various domains. Finally, we identify open challenges and propose future research directions. This survey aims to serve as an up-to-date resource for researchers and practitioners seeking to gain an intuitive understanding of LLM-based AL techniques and deploy them to new applications.",
      "arxiv_url": "https://arxiv.org/abs/2502.11767",
      "pdf_url": "https://arxiv.org/pdf/2502.11767",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "287e6e8a5593ba2169f3cf66e66e5d344a36064d",
      "title": "PIGuard: Prompt Injection Guardrail via Mitigating Overdefense for Free",
      "authors": [
        "Hao Li",
        "Xiaogeng Liu",
        "Ning Zhang",
        "Chaowei Xiao"
      ],
      "abstract": "Prompt injection attacks pose a critical threat to large language models (LLMs), enabling goal hijacking and data leakage. Prompt guard models, though effective in defense, suffer from over-defense—falsely flagging benign inputs as malicious due to trigger word bias. To address this issue, we introduce NotInject, an evaluation dataset that systematically measures over-defense across various prompt guard models. NotInject contains 339 benign samples enriched with trigger words common in prompt injection attacks, enabling fine-grained evaluation. Our results show that state-of-the-art models suffer from over-defense issues, with accuracy dropping close to random guessing levels (60%). To mitigate this, we propose PIGuard, a novel prompt guard model that incorporates a new training strategy, Mitigating Over-defense for Free (MOF), which significantly reduces the bias on trigger words. PIGuard demonstrates state-of-the-art performance on diverse benchmarks including Not-Inject, surpassing the existing best model by 30.4%, offering a robust and open-source so-lution for detecting prompt injection attacks. The code and datasets are released at https: //github.com/leolee99/PIGuard .",
      "arxiv_url": "https://www.semanticscholar.org/paper/287e6e8a5593ba2169f3cf66e66e5d344a36064d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.02107",
      "title": "TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining",
      "authors": [
        "Jeffrey Li",
        "Mohammadreza Armandpour",
        "Iman Mirzadeh",
        "Sachin Mehta",
        "Vaishaal Shankar",
        "Raviteja Vemulapalli",
        "Samy Bengio",
        "Oncel Tuzel",
        "Mehrdad Farajtabar",
        "Hadi Pouransari",
        "Fartash Faghri"
      ],
      "abstract": "Large Language Models (LLMs) trained on historical web data inevitably become outdated. We investigate evaluation strategies and update methods for LLMs as new data becomes available. We introduce a web-scale dataset for time-continual pretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of magnitude larger than previous continual language modeling benchmarks. We also design time-stratified evaluations across both general CC data and specific domains (Wikipedia, StackExchange, and code documentation) to assess how well various continual learning methods adapt to new data while retaining past knowledge. Our findings demonstrate that, on general CC data, autoregressive meta-schedules combined with a fixed-ratio replay of older data can achieve comparable held-out loss to re-training from scratch, while requiring significantly less computation (2.6x). However, the optimal balance between incorporating new data and replaying old data differs as replay is crucial to avoid forgetting on generic web data but less so on specific domains.",
      "arxiv_url": "https://arxiv.org/abs/2504.02107",
      "pdf_url": "https://arxiv.org/pdf/2504.02107",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19484",
      "title": "CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis",
      "authors": [
        "Ruixiang Feng",
        "Shen Gao",
        "Xiuying Chen",
        "Lisi Chen",
        "Shuo Shang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they often exhibit a specific cultural biases, neglecting the values and linguistic diversity of low-resource regions. This cultural bias not only undermines universal equality, but also risks reinforcing stereotypes and perpetuating discrimination. To address this, we propose CulFiT, a novel culturally-aware training paradigm that leverages multilingual data and fine-grained reward modeling to enhance cultural sensitivity and inclusivity. Our approach synthesizes diverse cultural-related questions, constructs critique data in culturally relevant languages, and employs fine-grained rewards to decompose cultural texts into verifiable knowledge units for interpretable evaluation. We also introduce GlobalCultureQA, a multilingual open-ended question-answering dataset designed to evaluate culturally-aware responses in a global context. Extensive experiments on three existing benchmarks and our GlobalCultureQA demonstrate that CulFiT achieves state-of-the-art open-source model performance in cultural alignment and general reasoning.",
      "arxiv_url": "https://arxiv.org/abs/2505.19484",
      "pdf_url": "https://arxiv.org/pdf/2505.19484",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.14927",
      "title": "MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance",
      "authors": [
        "Joseph Peper",
        "Wenzhao Qiu",
        "Ali Payani",
        "Lu Wang"
      ],
      "abstract": "Natural language processing evaluation has made significant progress, largely driven by the proliferation of powerful large language mod-els (LLMs). New evaluation benchmarks are of increasing priority as the reasoning capabilities of LLMs are expanding at a rapid pace. In particular, while multi-document (MD) reasoning is an area of extreme relevance given LLM capabilities in handling longer-context inputs, few benchmarks exist to rigorously examine model behavior in this setting. Moreover, the multi-document setting is historically challenging for benchmark creation due to the expensive cost of annotating long inputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs on the task of multi-document reasoning. Notably, MDBench is created through a novel synthetic generation process, allowing us to controllably and efficiently generate challenging document sets and the corresponding question-answer (QA) examples. Our novel technique operates on condensed structured seed knowledge, modifying it through LLM-assisted edits to induce MD-specific reasoning challenges. We then convert this structured knowledge into a natural text surface form, generating a document set and corresponding QA example. We analyze the behavior of popular LLMs and prompting techniques, finding that MDBENCH poses significant challenges for all methods, even with relatively short document sets. We also see our knowledge-guided generation technique (1) allows us to readily perform targeted analysis of MD-specific reasoning capabilities and (2) can be adapted quickly to account for new challenges and future modeling improvements.",
      "arxiv_url": "https://arxiv.org/abs/2506.14927",
      "pdf_url": "https://arxiv.org/pdf/2506.14927",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15343",
      "title": "Tokenization is Sensitive to Language Variation",
      "authors": [
        "Anna Wegmann",
        "Dong Nguyen",
        "David Jurgens"
      ],
      "abstract": "Variation in language is ubiquitous and often systematically linked to regional, social, and contextual factors. Tokenizers split texts into smaller units and might behave differently for less common linguistic forms. This might affect downstream LLM performance differently on two types of tasks: Tasks where the model should be robust to language variation (e.g., for semantic tasks like NLI, labels do not depend on whether a text uses British or American spelling) and tasks where the model should be sensitive to language variation (e.g., for form-based tasks like authorship verification, labels depend on whether a text uses British or American spelling). We pre-train BERT base models with the popular Byte-Pair Encoding algorithm to investigate how key tokenization design choices impact the performance of downstream models: the corpus used to train the tokenizer, the pre-tokenizer and the vocabulary size. We find that the best tokenizer varies on the two task types and that the pre-tokenizer has the biggest overall impact on performance. Further, we introduce a new approach to estimate tokenizer impact on downstream LLM performance, showing substantial improvement over metrics like R\\'enyi efficiency. We encourage more work on language variation and its relation to tokenizers and thus LLM performance.",
      "arxiv_url": "https://arxiv.org/abs/2502.15343",
      "pdf_url": "https://arxiv.org/pdf/2502.15343",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10093",
      "title": "Efficient Safety Alignment of Large Language Models via Preference Re-ranking and Representation-based Reward Modeling",
      "authors": [
        "Qiyuan Deng",
        "Xuefeng Bai",
        "Kehai Chen",
        "Yaowei Wang",
        "Liqiang Nie",
        "Min Zhang"
      ],
      "abstract": "Reinforcement Learning (RL) algorithms for safety alignment of Large Language Models (LLMs), such as Direct Preference Optimization (DPO), encounter the challenge of distribution shift. Current approaches typically address this issue through online sampling from the target policy, which requires significant computational resources. In this paper, we hypothesize that during off-policy training, while the ranking order of output generated by policy changes, their overall distribution remains relatively stable. This stability allows the conversion of the sampling process from the target policy into a computationally efficient re-ranking of preference data. Building on this hypothesis, we propose a new framework that leverages the model's intrinsic safety judgment capability to extract reward signals, which are then used to calculate label confidence for preference reordering. Extensive experiments and theoretical analysis demonstrate that the proposed method effectively addresses the distribution shift issue, remarkably enhancing the safety performance while avoiding about 300x computational overheads.",
      "arxiv_url": "https://arxiv.org/abs/2503.10093",
      "pdf_url": "https://arxiv.org/pdf/2503.10093",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "295f1955f44f13a8c417b3b4e1f807820e5eb960",
      "title": "Alleviating Hallucinations from Knowledge Misalignment in Large Language Models via Selective Abstention Learning",
      "authors": [
        "Lei Huang",
        "Xiaocheng Feng",
        "Weitao Ma",
        "Yuchun Fan",
        "Xiachong Feng",
        "Yuxuan Gu",
        "Yangfan Ye",
        "Liang Zhao",
        "Weihong Zhong",
        "Baoxin Wang",
        "Dayong Wu",
        "Guoping Hu",
        "Lingpeng Kong",
        "Tong Xiao",
        "Ting Liu",
        "Bing Qin"
      ],
      "abstract": "Large language models (LLMs) are known to suffer from severe hallucination issues. One of the main causes lies in the knowledge misalignment between the pre-training stage and the supervised fine-tuning stage. The unfamiliar knowledge encountered during fine-tuning may encourage LLMs to generate facts that are not grounded in parametric knowledge. To address this, we propose S EAL 1 , a novel training objec-tive with an abstention mechanism, in which the model learns to selectively reject tokens that misalign with the desired knowledge distribution via a special [REJ] token. This allows the model to have the alternative of acknowledging the insufficiency of knowledge rather than blindly assigning high probability to all ground-truth answers. We further propose a regularized decoding objective that penalizes uncertain predictions during inference by using the [REJ] probability learned during training. Extensive experiments on six short-form and long-form QA datasets with three LLMs of different sizes demonstrate that our method effectively alleviates hallucinations caused by knowledge mis-alignment. Further analysis highlights the adaptations of our method in answer",
      "arxiv_url": "https://www.semanticscholar.org/paper/295f1955f44f13a8c417b3b4e1f807820e5eb960",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.15215",
      "title": "MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs",
      "authors": [
        "Yongqi Fan",
        "Yating Wang",
        "Guandong Wang",
        "Jie Zhai",
        "Jingping Liu",
        "Qi Ye",
        "Tong Ruan"
      ],
      "abstract": "Open-ended question answering (QA) is a key task for evaluating the capabilities of large language models (LLMs). Compared to closed-ended QA, it demands longer answer statements, more nuanced reasoning processes, and diverse expressions, making refined and interpretable automatic evaluation both crucial and challenging. Traditional metrics like ROUGE and BERTScore struggle to capture semantic similarities due to different patterns between model responses and reference answers. Current LLM-based evaluation approaches, such as pairwise or listwise comparisons of candidate answers, lack intuitive interpretability. While pointwise scoring of each response provides some descriptions, it fails to adapt across different question contents. Most notably, existing methods overlook the distinction between factoid and non-factoid questions. To address these challenges, we propose \\textbf{MinosEval}, a novel evaluation method that first distinguishes open-ended questions and then ranks candidate answers using different evaluation strategies. For factoid questions, it applies an adaptive key-point scoring strategy, while for non-factoid questions, it uses an instance-aware listwise ranking strategy. Experiments on multiple open-ended QA datasets, including self-built ones with more candidate responses to complement community resources, show that MinosEval better aligns with human annotations and offers more interpretable results.",
      "arxiv_url": "https://arxiv.org/abs/2506.15215",
      "pdf_url": "https://arxiv.org/pdf/2506.15215",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.16007",
      "title": "Help Me Write a Story: Evaluating LLMs' Ability to Generate Writing Feedback",
      "authors": [
        "Hannah Rashkin",
        "Elizabeth Clark",
        "Fantine Huot",
        "Mirella Lapata"
      ],
      "abstract": "Can LLMs provide support to creative writers by giving meaningful writing feedback? In this paper, we explore the challenges and limitations of model-generated writing feedback by defining a new task, dataset, and evaluation frameworks. To study model performance in a controlled manner, we present a novel test set of 1,300 stories that we corrupted to intentionally introduce writing issues. We study the performance of commonly used LLMs in this task with both automatic and human evaluation metrics. Our analysis shows that current models have strong out-of-the-box behavior in many respects -- providing specific and mostly accurate writing feedback. However, models often fail to identify the biggest writing issue in the story and to correctly decide when to offer critical vs. positive feedback.",
      "arxiv_url": "https://arxiv.org/abs/2507.16007",
      "pdf_url": "https://arxiv.org/pdf/2507.16007",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07667",
      "title": "Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch",
      "authors": [
        "Prarabdh Shukla",
        "Wei Yin Chong",
        "Yash Patel",
        "Brennan Schaffner",
        "Danish Pruthi",
        "A. Bhagoji"
      ],
      "abstract": "To meet the demands of content moderation, online platforms have resorted to automated systems. Newer forms of real-time engagement($\\textit{e.g.}$, users commenting on live streams) on platforms like Twitch exert additional pressures on the latency expected of such moderation systems. Despite their prevalence, relatively little is known about the effectiveness of these systems. In this paper, we conduct an audit of Twitch's automated moderation tool ($\\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful content. For our audit, we create streaming accounts to act as siloed test beds, and interface with the live chat using Twitch's APIs to send over $107,000$ comments collated from $4$ datasets. We measure $\\texttt{AutoMod}$'s accuracy in flagging blatantly hateful content containing misogyny, racism, ableism and homophobia. Our experiments reveal that a large fraction of hateful messages, up to $94\\%$ on some datasets, $\\textit{bypass moderation}$. Contextual addition of slurs to these messages results in $100\\%$ removal, revealing $\\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We also find that contrary to Twitch's community guidelines, $\\texttt{AutoMod}$ blocks up to $89.5\\%$ of benign examples that use sensitive words in pedagogical or empowering contexts. Overall, our audit points to large gaps in $\\texttt{AutoMod}$'s capabilities and underscores the importance for such systems to understand context effectively.",
      "arxiv_url": "https://arxiv.org/abs/2506.07667",
      "pdf_url": "https://arxiv.org/pdf/2506.07667",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13260",
      "title": "Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models",
      "authors": [
        "Yingqian Cui",
        "Pengfei He",
        "Jingying Zeng",
        "Hui Liu",
        "Xianfeng Tang",
        "Zhenwei Dai",
        "Yan Han",
        "Chen Luo",
        "Jing Huang",
        "Zhen Li",
        "Suhang Wang",
        "Yue Xing",
        "Jiliang Tang",
        "Qi He"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning, which breaks down complex tasks into intermediate reasoning steps, has significantly enhanced the performance of large language models (LLMs) on challenging tasks. However, the detailed reasoning process in CoT often incurs long generation times and high computational costs, partly due to the inclusion of unnecessary steps. To address this, we propose a method to identify critical reasoning steps using perplexity as a measure of their importance: a step is deemed critical if its removal causes a significant increase in perplexity. Our method enables models to focus solely on generating these critical steps. This can be achieved through two approaches: refining demonstration examples in few-shot CoT or fine-tuning the model using selected examples that include only critical steps. Comprehensive experiments validate the effectiveness of our method, which achieves a better balance between the reasoning accuracy and efficiency of CoT.",
      "arxiv_url": "https://arxiv.org/abs/2502.13260",
      "pdf_url": "https://arxiv.org/pdf/2502.13260",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12731",
      "title": "Accelerating Adaptive Retrieval Augmented Generation via Instruction-Driven Representation Reduction of Retrieval Overlaps",
      "authors": [
        "Jie Ou",
        "Jinyu Guo",
        "Shuaihong Jiang",
        "Zhaokun Wang",
        "Libo Qin",
        "Shunyu Yao",
        "Wenhong Tian"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for expanding the knowledge of large language models. To handle complex queries more effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the generated quality through multiple interactions with external knowledge bases. Despite its effectiveness, A-RAG exacerbates the pre-existing efficiency challenges inherent in RAG, which are attributable to its reliance on multiple iterations of generation. Existing A-RAG approaches process all retrieved contents from scratch. However, they ignore the situation where there is a significant overlap in the content of the retrieval results across rounds. The overlapping content is redundantly represented, which leads to a large proportion of repeated computations, thus affecting the overall efficiency. To address this issue, this paper introduces a model-agnostic approach that can be generally applied to A-RAG methods, which is dedicated to reducing the redundant representation process caused by the overlapping of retrieval results. Specifically, we use cache access and parallel generation to speed up the prefilling and decoding stages respectively. Additionally, we also propose an instruction-driven module to further guide the model to more effectively attend to each part of the content in a more suitable way for LLMs. Experiments show that our approach achieves 2.79 and 2.33 times significant acceleration on average for prefilling and decoding respectively while maintaining equal generation quality.",
      "arxiv_url": "https://arxiv.org/abs/2505.12731",
      "pdf_url": "https://arxiv.org/pdf/2505.12731",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03458",
      "title": "Culture Matters in Toxic Language Detection in Persian",
      "authors": [
        "Zahra Bokaei",
        "Walid Magdy",
        "Bonnie Webber"
      ],
      "abstract": "Toxic language detection is crucial for creating safer online environments and limiting the spread of harmful content. While toxic language detection has been under-explored in Persian, the current work compares different methods for this task, including fine-tuning, data enrichment, zero-shot and few-shot learning, and cross-lingual transfer learning. What is especially compelling is the impact of cultural context on transfer learning for this task: We show that the language of a country with cultural similarities to Persian yields better results in transfer learning. Conversely, the improvement is lower when the language comes from a culturally distinct country. Warning: This paper contains examples of toxic language that may disturb some readers. These examples are included for the purpose of research on toxic detection.",
      "arxiv_url": "https://arxiv.org/abs/2506.03458",
      "pdf_url": "https://arxiv.org/pdf/2506.03458",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.14200",
      "title": "ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations",
      "authors": [
        "Brihi Joshi",
        "Keyu He",
        "Sahana Ramnath",
        "Sadra Sabouri",
        "Kaitlyn Zhou",
        "Souti Chattopadhyay",
        "Swabha Swayamdipta",
        "Xiang Ren"
      ],
      "abstract": "Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K\"Why\"questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an\"educator\"to assess model explanations' fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness.",
      "arxiv_url": "https://arxiv.org/abs/2506.14200",
      "pdf_url": "https://arxiv.org/pdf/2506.14200",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18218",
      "title": "CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games",
      "authors": [
        "Shuhang Xu",
        "Fangwei Zhong"
      ],
      "abstract": "Metaphors are a crucial way for humans to express complex or subtle ideas by comparing one concept to another, often from a different domain. However, many large language models (LLMs) struggle to interpret and apply metaphors in multi-agent language games, hindering their ability to engage in covert communication and semantic evasion, which are crucial for strategic communication. To address this challenge, we introduce CoMet, a framework that enables LLM-based agents to engage in metaphor processing. CoMet combines a hypothesis-based metaphor reasoner with a metaphor generator that improves through self-reflection and knowledge integration. This enhances the agents' ability to interpret and apply metaphors, improving the strategic and nuanced quality of their interactions. We evaluate CoMet on two multi-agent language games - Undercover and Adversarial Taboo - which emphasize Covert Communication and Semantic Evasion. Experimental results demonstrate that CoMet significantly enhances the agents' ability to communicate strategically using metaphors.",
      "arxiv_url": "https://arxiv.org/abs/2505.18218",
      "pdf_url": "https://arxiv.org/pdf/2505.18218",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.13360",
      "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning",
      "authors": [
        "Hai-Long Sun",
        "Zhun Sun",
        "Houwen Peng",
        "Han-Jia Ye"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the model's textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4 points vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems.",
      "arxiv_url": "https://arxiv.org/abs/2503.13360",
      "pdf_url": "https://arxiv.org/pdf/2503.13360",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12520",
      "title": "SafeEraser: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning",
      "authors": [
        "Junkai Chen",
        "Zhijie Deng",
        "Kening Zheng",
        "Yibo Yan",
        "Shuliang Liu",
        "PeiJun Wu",
        "Peijie Jiang",
        "Jia Liu",
        "Xuming Hu"
      ],
      "abstract": "As Multimodal Large Language Models (MLLMs) develop, their potential security issues have become increasingly prominent. Machine Unlearning (MU), as an effective strategy for forgetting specific knowledge in training data, has been widely used in privacy protection. However, MU for safety in MLLM has yet to be fully explored. To address this issue, we propose SAFEERASER, a safety unlearning benchmark for MLLMs, consisting of 3,000 images and 28.8K VQA pairs. We comprehensively evaluate unlearning methods from two perspectives: forget quality and model utility. Our findings show that existing MU methods struggle to maintain model performance while implementing the forget operation and often suffer from over-forgetting. Hence, we introduce Prompt Decouple (PD) Loss to alleviate over-forgetting through decouple prompt during unlearning process. To quantitatively measure over-forgetting mitigated by PD Loss, we propose a new metric called Safe Answer Refusal Rate (SARR). Experimental results demonstrate that combining PD Loss with existing unlearning methods can effectively prevent over-forgetting and achieve a decrease of 79.5% in the SARR metric of LLaVA-7B and LLaVA-13B, while maintaining forget quality and model utility. Our code and dataset will be released upon acceptance. Warning: This paper contains examples of harmful language and images, and reader discretion is recommended.",
      "arxiv_url": "https://arxiv.org/abs/2502.12520",
      "pdf_url": "https://arxiv.org/pdf/2502.12520",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10995",
      "title": "TigerLLM - A Family of Bangla Large Language Models",
      "authors": [
        "Nishat Raihan",
        "Marcos Zampieri"
      ],
      "abstract": "The development of Large Language Models (LLMs) remains heavily skewed towards English and a few other high-resource languages. This linguistic disparity is particularly evident for Bangla - the 5th most spoken language. A few initiatives attempted to create open-source Bangla LLMs with performance still behind high-resource languages and limited reproducibility. To address this gap, we introduce TigerLLM - a family of Bangla LLMs. Our results demonstrate that these models surpass all open-source alternatives and also outperform larger proprietary models like GPT3.5 across standard benchmarks, establishing TigerLLM as the new baseline for future Bangla language modeling.",
      "arxiv_url": "https://arxiv.org/abs/2503.10995",
      "pdf_url": "https://arxiv.org/pdf/2503.10995",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21068",
      "title": "Predicting Implicit Arguments in Procedural Video Instructions",
      "authors": [
        "Anil Batra",
        "Laura Sevilla-Lara",
        "Marcus Rohrbach",
        "Frank Keller"
      ],
      "abstract": "Procedural texts help AI enhance reasoning about context and action sequences. Transforming these into Semantic Role Labeling (SRL) improves understanding of individual steps by identifying predicate-argument structure like {verb,what,where/with}. Procedural instructions are highly elliptic, for instance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second step's where argument is inferred from the context, referring to where the cucumber was placed. Prior SRL benchmarks often miss implicit arguments, leading to incomplete understanding. To address this, we introduce Implicit-VidSRL, a dataset that necessitates inferring implicit and explicit arguments from contextual information in multimodal cooking procedures. Our proposed dataset benchmarks multimodal models' contextual reasoning, requiring entity tracking through visual changes in recipes. We study recent multimodal LLMs and reveal that they struggle to predict implicit arguments of what and where/with from multi-modal procedural data given the verb. Lastly, we propose iSRL-Qwen2-VL, which achieves a 17% relative improvement in F1-score for what-implicit and a 14.7% for where/with-implicit semantic roles over GPT-4o.",
      "arxiv_url": "https://arxiv.org/abs/2505.21068",
      "pdf_url": "https://arxiv.org/pdf/2505.21068",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06292",
      "title": "Mutual-Taught for Co-adapting Policy and Reward Models",
      "authors": [
        "Tianyuan Shi",
        "Canbin Huang",
        "Fanqi Wan",
        "Longguang Zhong",
        "Ziyi Yang",
        "Weizhou Shen",
        "Xiaojun Quan",
        "Ming Yan"
      ],
      "abstract": "During the preference optimization of large language models (LLMs), distribution shifts may arise between newly generated model samples and the data used to train the reward model (RM). This shift reduces the efficacy of the RM, which in turn negatively impacts the performance of the policy model (PM). To address this challenge, we propose Mutual-Taught, a self-training method that iteratively improves both the PM and RM without requiring additional human annotation. Our approach mirrors the expectation-maximization (EM) algorithm. In the E-step, the PM is updated using feedback from the current RM, guiding the PM toward a better approximation of the latent optimal preference distribution. In the M-step, we update the RM by constructing training data from the outputs of the PM before and after the E-step update. This process ensures that the RM adapts to the evolving policy distribution. Experimental results demonstrate that this iterative approach leads to consistent improvements in both models. Specifically, our 8B policy model, LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\\% on AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par with GPT-4o-2024-08-06 on RewardBench.",
      "arxiv_url": "https://arxiv.org/abs/2506.06292",
      "pdf_url": "https://arxiv.org/pdf/2506.06292",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.13725",
      "title": "SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs",
      "authors": [
        "Yu Guo",
        "Dong Jin",
        "Shenghao Ye",
        "Shuangwu Chen",
        "Jian Yang",
        "Xiaobin Tan"
      ],
      "abstract": "Large Language models (LLMs) have demonstrated significant potential in text-to-SQL reasoning tasks, yet a substantial performance gap persists between existing open-source models and their closed-source counterparts. In this paper, we introduce SQLForge, a novel approach for synthesizing reliable and diverse data to enhance text-to-SQL reasoning in LLMs. We improve data reliability through SQL syntax constraints and SQL-to-question reverse translation, ensuring data logic at both structural and semantic levels. We also propose an SQL template enrichment and iterative data domain exploration mechanism to boost data diversity. Building on the augmented data, we fine-tune a variety of open-source models with different architectures and parameter sizes, resulting in a family of models termed SQLForge-LM. SQLForge-LM achieves the state-of-the-art performance on the widely recognized Spider and BIRD benchmarks among the open-source models. Specifically, SQLForge-LM achieves EX accuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, significantly narrowing the performance gap with closed-source methods.",
      "arxiv_url": "https://arxiv.org/abs/2505.13725",
      "pdf_url": "https://arxiv.org/pdf/2505.13725",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2ac4e52ea6e37a2c200bdd800f1a790c7aa0415b",
      "title": "RTADev: Intention Aligned Multi-Agent Framework for Software Development",
      "authors": [
        "Jie Liu",
        "Guohua Wang",
        "Ronghui Yang",
        "Jiajie Zeng",
        "Mengchen Zhao",
        "Yi Cai"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/2ac4e52ea6e37a2c200bdd800f1a790c7aa0415b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19279",
      "title": "CritiQ: Mining Data Quality Criteria from Human Preferences",
      "authors": [
        "Honglin Guo",
        "Kai Lv",
        "Qipeng Guo",
        "Tianyi Liang",
        "Zhiheng Xi",
        "Demin Song",
        "Qiuyinzhe Zhang",
        "Yu Sun",
        "Kai Chen",
        "Xipeng Qiu",
        "Tao Gui"
      ],
      "abstract": "Language model heavily depends on high-quality data for optimal performance. Existing approaches rely on manually designed heuristics, the perplexity of existing models, training classifiers, or careful prompt engineering, which require significant expert experience and human annotation effort while introduce biases. We introduce CritiQ, a novel data selection method that automatically mines criteria from human preferences for data quality with only ~30 human-annotated pairs and performs efficient data selection. The main component, CritiQ Flow, employs a manager agent to evolve quality criteria and worker agents to make pairwise judgments. We build a knowledge base that extracts quality criteria from previous work to boost CritiQ Flow. Compared to perplexity- and classifier- based methods, verbal criteria are more interpretable and possess reusable value. After deriving the criteria, we train the CritiQ Scorer to give quality scores and perform efficient data selection. We demonstrate the effectiveness of our method in the code, math, and logic domains, achieving high accuracy on human-annotated test sets. To validate the quality of the selected data, we continually train Llama 3.1 models and observe improved performance on downstream tasks compared to uniform sampling. Ablation studies validate the benefits of the knowledge base and the reflection process. We analyze how criteria evolve and the effectiveness of majority voting.",
      "arxiv_url": "https://arxiv.org/abs/2502.19279",
      "pdf_url": "https://arxiv.org/pdf/2502.19279",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00980",
      "title": "LEMONADE: A Large Multilingual Expert-Annotated Abstractive Event Dataset for the Real World",
      "authors": [
        "Sina J. Semnani",
        "Pingyue Zhang",
        "Wanyue Zhai",
        "Haozhuo Li",
        "Ryan Beauchamp",
        "Trey Billing",
        "Katayoun Kishi",
        "Manling Li",
        "Monica S. Lam"
      ],
      "abstract": "This paper presents LEMONADE, a large-scale conflict event dataset comprising 39,786 events across 20 languages and 171 countries, with extensive coverage of region-specific entities. LEMONADE is based on a partially reannotated subset of the Armed Conflict Location&Event Data (ACLED), which has documented global conflict events for over a decade. To address the challenge of aggregating multilingual sources for global event analysis, we introduce abstractive event extraction (AEE) and its subtask, abstractive entity linking (AEL). Unlike conventional span-based event extraction, our approach detects event arguments and entities through holistic document understanding and normalizes them across the multilingual dataset. We evaluate various large language models (LLMs) on these tasks, adapt existing zero-shot event extraction systems, and benchmark supervised models. Additionally, we introduce ZEST, a novel zero-shot retrieval-based system for AEL. Our best zero-shot system achieves an end-to-end F1 score of 58.3%, with LLMs outperforming specialized event extraction models such as GoLLIE. For entity linking, ZEST achieves an F1 score of 45.7%, significantly surpassing OneNet, a state-of-the-art zero-shot baseline that achieves only 23.7%. However, these zero-shot results lag behind the best supervised systems by 20.1% and 37.0% in the end-to-end and AEL tasks, respectively, highlighting the need for further research.",
      "arxiv_url": "https://arxiv.org/abs/2506.00980",
      "pdf_url": "https://arxiv.org/pdf/2506.00980",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.08646",
      "title": "TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning",
      "authors": [
        "Mingyu Zheng",
        "Zhifan Feng",
        "Jia Wang",
        "Lanrui Wang",
        "Zheng Lin",
        "Yang Hao",
        "Weiping Wang"
      ],
      "abstract": "Despite the commendable progress of recent LLM-based data synthesis methods, they face two limitations in generating table instruction tuning data. First, they can not thoroughly explore the vast input space of table understanding tasks, leading to limited data diversity. Second, they ignore the weaknesses in table understanding ability of the target LLM and blindly pursue the increase of data quantity, resulting in suboptimal data efficiency. In this paper, we introduce a progressive and weakness-guided data synthesis framework tailored for table instruction tuning, named TableDreamer, to mitigate the above issues. Specifically, we first synthesize diverse tables and related instructions as seed data, and then perform an iterative exploration of the input space under the guidance of the newly identified weakness data, which eventually serve as the final training data for fine-tuning the target LLM. Extensive experiments on 10 tabular benchmarks demonstrate the effectiveness of the proposed framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62% (49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms state-of-the-art data synthesis baselines which use more training data. The code and data is available at https://github.com/SpursGoZmy/TableDreamer",
      "arxiv_url": "https://arxiv.org/abs/2506.08646",
      "pdf_url": "https://arxiv.org/pdf/2506.08646",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.10950",
      "title": "SpeechT-RAG: Reliable Depression Detection in LLMs with Retrieval-Augmented Generation Using Speech Timing Information",
      "authors": [
        "Xiangyu Zhang",
        "Hexin Liu",
        "Qiquan Zhang",
        "Beena Ahmed",
        "Julien Epps"
      ],
      "abstract": "Large Language Models (LLMs) have been increasingly adopted for health-related tasks, yet their performance in depression detection remains limited when relying solely on text input. While Retrieval-Augmented Generation (RAG) typically enhances LLM capabilities, our experiments indicate that traditional text-based RAG systems struggle to significantly improve depression detection accuracy. This challenge stems partly from the rich depression-relevant information encoded in acoustic speech patterns information that current text-only approaches fail to capture effectively. To address this limitation, we conduct a systematic analysis of temporal speech patterns, comparing healthy individuals with those experiencing depression. Based on our findings, we introduce Speech Timing-based Retrieval-Augmented Generation, SpeechT-RAG, a novel system that leverages speech timing features for both accurate depression detection and reliable confidence estimation. This integrated approach not only outperforms traditional text-based RAG systems in detection accuracy but also enhances uncertainty quantification through a confidence scoring mechanism that naturally extends from the same temporal features. Our unified framework achieves comparable results to fine-tuned LLMs without additional training while simultaneously addressing the fundamental requirements for both accuracy and trustworthiness in mental health assessment.",
      "arxiv_url": "https://arxiv.org/abs/2502.10950",
      "pdf_url": "https://arxiv.org/pdf/2502.10950",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.03191",
      "title": "CLIX: Cross-Lingual Explanations of Idiomatic Expressions",
      "authors": [
        "Aaron Gluck",
        "K. Wense",
        "Maria Pacheco"
      ],
      "abstract": "Automated definition generation systems have been proposed to support vocabulary expansion for language learners. The main barrier to the success of these systems is that learners often struggle to understand definitions due to the presence of potentially unfamiliar words and grammar, particularly when non-standard language is involved. To address these challenges, we propose CLIX, the task of Cross-Lingual explanations of Idiomatic eXpressions. We explore the capabilities of current NLP models for this task, and observe that while it remains challenging, large language models show promise. Finally, we perform a detailed error analysis to highlight the key challenges that need to be addressed before we can reliably incorporate these systems into educational tools.",
      "arxiv_url": "https://arxiv.org/abs/2501.03191",
      "pdf_url": "https://arxiv.org/pdf/2501.03191",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.12495",
      "title": "Beyond Text: Characterizing Domain Expert Needs in Document Research",
      "authors": [
        "Sireesh Gururaja",
        "Nupoor Gandhi",
        "Jeremiah Milbauer",
        "Emma Strubell"
      ],
      "abstract": "Working with documents is a key part of almost any knowledge work, from contextualizing research in a literature review to reviewing legal precedent. Recently, as their capabilities have expanded, primarily text-based NLP systems have often been billed as able to assist or even automate this kind of work. But to what extent are these systems able to model these tasks as experts conceptualize and perform them now? In this study, we interview sixteen domain experts across two domains to understand their processes of document research, and compare it to the current state of NLP systems. We find that our participants processes are idiosyncratic, iterative, and rely extensively on the social context of a document in addition its content; existing approaches in NLP and adjacent fields that explicitly center the document as an object, rather than as merely a container for text, tend to better reflect our participants' priorities, though they are often less accessible outside their research communities. We call on the NLP community to more carefully consider the role of the document in building useful tools that are accessible, personalizable, iterative, and socially aware.",
      "arxiv_url": "https://arxiv.org/abs/2504.12495",
      "pdf_url": "https://arxiv.org/pdf/2504.12495",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02899",
      "title": "IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator",
      "authors": [
        "Yusuke Sakai",
        "Takumi Goto",
        "Taro Watanabe"
      ],
      "abstract": "We propose IMPARA-GED, a novel reference-free automatic grammatical error correction (GEC) evaluation method with grammatical error detection (GED) capabilities. We focus on the quality estimator of IMPARA, an existing automatic GEC evaluation method, and construct that of IMPARA-GED using a pre-trained language model with enhanced GED capabilities. Experimental results on SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods, demonstrate that IMPARA-GED achieves the highest correlation with human sentence-level evaluations.",
      "arxiv_url": "https://arxiv.org/abs/2506.02899",
      "pdf_url": "https://arxiv.org/pdf/2506.02899",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2bd580a0ebe4566eede4f1c1015506e110eee865",
      "title": "Change Entity-guided Heterogeneous Representation Disentangling for Change Captioning",
      "authors": [
        "Yi Li",
        "Yunbin Tu",
        "Liang Li",
        "Li Su",
        "Qin Huang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/2bd580a0ebe4566eede4f1c1015506e110eee865",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01322",
      "title": "Zero-Shot Text-to-Speech for Vietnamese",
      "authors": [
        "Thi Vu",
        "L. T. Nguyen",
        "Dat Quoc Nguyen"
      ],
      "abstract": "This paper introduces PhoAudiobook, a newly curated dataset comprising 941 hours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook, we conduct experiments on three leading zero-shot TTS models: VALL-E, VoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook consistently enhances model performance across various metrics. Moreover, VALL-E and VoiceCraft exhibit superior performance in synthesizing short sentences, highlighting their robustness in handling diverse linguistic contexts. We publicly release PhoAudiobook to facilitate further research and development in Vietnamese text-to-speech.",
      "arxiv_url": "https://arxiv.org/abs/2506.01322",
      "pdf_url": "https://arxiv.org/pdf/2506.01322",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2c09156d2118dfdc2480a65dde6611397c4682d2",
      "title": "Analyzing and Mitigating Inconsistency in Discrete Speech Tokens for Neural Codec Language Models",
      "authors": [
        "Wenrui Liu",
        "Zhifang Guo",
        "Jin Xu",
        "Yuanjun Lv",
        "Yunfei Chu",
        "Zemin Liu",
        "Junyang Lin"
      ],
      "abstract": "Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training speech generation tasks with discrete speech token sequences. However, directly discretizing speech by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete speech to-kens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as Discrete Representation In-consistency (DRI) . This inconsistency can lead to a single speech segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in poor generated speech. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as En-Codec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS dataset (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online 1 .",
      "arxiv_url": "https://www.semanticscholar.org/paper/2c09156d2118dfdc2480a65dde6611397c4682d2",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13092",
      "title": "Text2World: Benchmarking Large Language Models for Symbolic World Model Generation",
      "authors": [
        "Mengkang Hu",
        "Tianxing Chen",
        "Yude Zou",
        "Yuheng Lei",
        "Qiguang Chen",
        "Ming Li",
        "Yao Mu",
        "Hongyuan Zhang",
        "Wenqi Shao",
        "Ping Luo"
      ],
      "abstract": "Recently, there has been growing interest in leveraging large language models (LLMs) to generate symbolic world models from textual descriptions. Although LLMs have been extensively explored in the context of world modeling, prior studies encountered several challenges, including evaluation randomness, dependence on indirect metrics, and a limited domain scope. To address these limitations, we introduce a novel benchmark, Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. We benchmark current LLMs using Text2World and find that reasoning models trained with large-scale reinforcement learning outperform others. However, even the best-performing model still demonstrates limited capabilities in world modeling. Building on these insights, we examine several promising strategies to enhance the world modeling capabilities of LLMs, including test-time scaling, agent training, and more. We hope that Text2World can serve as a crucial resource, laying the groundwork for future research in leveraging LLMs as world models. The project page is available at https://text-to-world.github.io/.",
      "arxiv_url": "https://arxiv.org/abs/2502.13092",
      "pdf_url": "https://arxiv.org/pdf/2502.13092",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2c4fb26896ef9ba41c285fc81cf1ca2b8271eb3f",
      "title": "Vision-aided Unsupervised Constituency Parsing with Multi-MLLM Debating",
      "authors": [
        "Dong Zhang",
        "Haiyan Tian",
        "Qingying Sun",
        "Shoushan Li"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/2c4fb26896ef9ba41c285fc81cf1ca2b8271eb3f",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12900",
      "title": "Soundwave: Less is More for Speech-Text Alignment in LLMs",
      "authors": [
        "Yuhao Zhang",
        "Zhiheng Liu",
        "Fan Bu",
        "Ruiyu Zhang",
        "Benyou Wang",
        "Haizhou Li"
      ],
      "abstract": "Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth. We focus on two fundamental problems between speech and text: the representation space gap and sequence length inconsistency. We propose Soundwave, which utilizes an efficient training strategy and a novel architecture to address these issues. Results show that Soundwave outperforms the advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks, using only one-fiftieth of the training data. Further analysis shows that Soundwave still retains its intelligence during conversation. The project is available at https://github.com/FreedomIntelligence/Soundwave.",
      "arxiv_url": "https://arxiv.org/abs/2502.12900",
      "pdf_url": "https://arxiv.org/pdf/2502.12900",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.05522",
      "title": "User Feedback Alignment for LLM-powered Exploration in Large-scale Recommendation Systems",
      "authors": [
        "Jianling Wang",
        "Yifan Liu",
        "Yinghao Sun",
        "Xuejian Ma",
        "Yueqi Wang",
        "He Ma",
        "Zhengyang Su",
        "Minmin Chen",
        "Mingyan Gao",
        "Onkar Dalal",
        "E. Chi",
        "Lichan Hong",
        "Ningren Han",
        "Haokai Lu"
      ],
      "abstract": "Exploration, the act of broadening user experiences beyond their established preferences, is challenging in large-scale recommendation systems due to feedback loops and limited signals on user exploration patterns. Large Language Models (LLMs) offer potential solutions by leveraging their world knowledge to recommend novel content outside these loops. A key challenge is aligning LLMs with user preferences while preserving their knowledge and reasoning. To enhance planning for new user interests using LLMs, this paper introduces a novel approach that combines hierarchical planning with LLM inference-time scaling. This method aims to improve recommendation relevancy without compromising novelty. We decouple novelty and user-alignment, training separate LLMs for each objective. We then scale up the novelty-focused LLM's inference and select the best-of-n predictions using the user-aligned LLM. Live experiments demonstrate efficacy, showing significant gains in both user satisfaction (measured by watch activity and active user counts) and exploration diversity.",
      "arxiv_url": "https://arxiv.org/abs/2504.05522",
      "pdf_url": "https://arxiv.org/pdf/2504.05522",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24427",
      "title": "Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts",
      "authors": [
        "Christopher Bagdon",
        "Aidan Combs",
        "Carina Silberer",
        "Roman Klinger"
      ],
      "abstract": "Accurate modeling of subjective phenomena such as emotion expression requires data annotated with authors' intentions. Commonly such data is collected by asking study participants to donate and label genuine content produced in the real world, or create content fitting particular labels during the study. Asking participants to create content is often simpler to implement and presents fewer risks to participant privacy than data donation. However, it is unclear if and how study-created content may differ from genuine content, and how differences may impact models. We collect study-created and genuine multimodal social media posts labeled for emotion and compare them on several dimensions, including model performance. We find that compared to genuine posts, study-created posts are longer, rely more on their text and less on their images for emotion expression, and focus more on emotion-prototypical events. The samples of participants willing to donate versus create posts are demographically different. Study-created data is valuable to train models that generalize well to genuine data, but realistic effectiveness estimates require genuine data.",
      "arxiv_url": "https://arxiv.org/abs/2505.24427",
      "pdf_url": "https://arxiv.org/pdf/2505.24427",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.01902",
      "title": "Graphically Speaking: Unmasking Abuse in Social Media with Conversation Insights",
      "authors": [
        "C'elia Nouri",
        "Jean-Philippe Cointet",
        "Chloé Clavel"
      ],
      "abstract": "Detecting abusive language in social media conversations poses significant challenges, as identifying abusiveness often depends on the conversational context, characterized by the content and topology of preceding comments. Traditional Abusive Language Detection (ALD) models often overlook this context, which can lead to unreliable performance metrics. Recent Natural Language Processing (NLP) methods that integrate conversational context often depend on limited and simplified representations, and report inconsistent results. In this paper, we propose a novel approach that utilize graph neural networks (GNNs) to model social media conversations as graphs, where nodes represent comments, and edges capture reply structures. We systematically investigate various graph representations and context windows to identify the optimal configuration for ALD. Our GNN model outperform both context-agnostic baselines and linear context-aware methods, achieving significant improvements in F1 scores. These findings demonstrate the critical role of structured conversational context and establish GNNs as a robust framework for advancing context-aware abusive language detection.",
      "arxiv_url": "https://arxiv.org/abs/2504.01902",
      "pdf_url": "https://arxiv.org/pdf/2504.01902",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2cfc45071698802e952d830b6b82f9b2f7623034",
      "title": "Can Language Models Capture Human Writing Preferences for Domain-Specific Text Summarization?",
      "authors": [
        "Jingbao Luo",
        "Ming Liu",
        "Ran Liu",
        "Yongpan Sheng",
        "Xin Hu",
        "Gang Li",
        "Peng Wu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/2cfc45071698802e952d830b6b82f9b2f7623034",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.16334",
      "title": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates",
      "authors": [
        "Ying Shen",
        "Lifu Huang"
      ],
      "abstract": "Recent findings reveal that much of the knowledge in a Transformer-based Large Language Model (LLM) is encoded in its feed-forward (FFN) layers, where each FNN layer can be interpreted as the summation of sub-updates, each corresponding to a weighted column vector from the FFN's value parameter matrix that often encodes human-interpretable concepts. In light of this, we hypothesize that model performance and behaviors can be further enhanced and controlled by modulating the contributions of these sub-updates based on their relevance to the input or target output style, and propose LLMBRACES, a novel and efficient method that computes relevance scores associated with value vectors in FFN layers and leverages these scores to dynamically adjust the contribution of sub-updates. By optimizing sub-update contributions, LLMBRACES refines the prediction process, leading to more accurate and reliable outputs, much like a'brace'providing support and stability. Moreover, LLMBRACES can be extended to support conditional control over generation characteristics, such as sentiment, thereby offering fine-grained steering of LLM outputs. Extensive experiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and Llama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both fine-tuning and zero-shot settings while requiring significantly fewer tunable parameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in sentiment-controlled generation and toxicity reduction, highlighting its potential for flexible, controlled text generation across applications.",
      "arxiv_url": "https://arxiv.org/abs/2503.16334",
      "pdf_url": "https://arxiv.org/pdf/2503.16334",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02073",
      "title": "Flow2Code: Evaluating Large Language Models for Flowchart-based Code Generation Capability",
      "authors": [
        "Mengliang He",
        "Jiayi Zeng",
        "Yankai Jiang",
        "Wei Zhang",
        "Zeming Liu",
        "Xiaoming Shi",
        "Aimin Zhou"
      ],
      "abstract": "While large language models (LLMs) show promise in code generation, existing benchmarks neglect the flowchart-based code generation. To promote further research on flowchart-based code generation, this work presents Flow2Code, a novel benchmark for flowchart-based code generation evaluation. The evaluation dataset spans 15 programming languages and includes 5,622 code segments paired with 16,866 flowcharts of three types: code, UML, and pseudocode. Extensive experiments with 13 multimodal LLMs reveal that current LLMs can not generate code based on flowcharts perfectly. Besides, experiment results show that the supervised fine-tuning technique contributes greatly to the models' performance. We publicly release our code and datasets at https://github.com/hml-github/Flow2Code.",
      "arxiv_url": "https://arxiv.org/abs/2506.02073",
      "pdf_url": "https://arxiv.org/pdf/2506.02073",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17793",
      "title": "Synthia: Novel Concept Design with Affordance Composition",
      "authors": [
        "Xiaomeng Jin",
        "Hyeonjeong Ha",
        "Jeonghwan Kim",
        "Jiateng Liu",
        "Zhenhailong Wang",
        "Khanh Duy Nguyen",
        "Ansel Blume",
        "Nanyun Peng",
        "Kai-Wei Chang",
        "Heng Ji"
      ],
      "abstract": "Text-to-image (T2I) models enable rapid concept design, making them widely used in AI-driven design. While recent studies focus on generating semantic and stylistic variations of given design concepts, functional coherence--the integration of multiple affordances into a single coherent concept--remains largely overlooked. In this paper, we introduce SYNTHIA, a framework for generating novel, functionally coherent designs based on desired affordances. Our approach leverages a hierarchical concept ontology that decomposes concepts into parts and affordances, serving as a crucial building block for functionally coherent design. We also develop a curriculum learning scheme based on our ontology that contrastively fine-tunes T2I models to progressively learn affordance composition while maintaining visual novelty. To elaborate, we (i) gradually increase affordance distance, guiding models from basic concept-affordance association to complex affordance compositions that integrate parts of distinct affordances into a single, coherent form, and (ii) enforce visual novelty by employing contrastive objectives to push learned representations away from existing concepts. Experimental results show that SYNTHIA outperforms state-of-the-art T2I models, demonstrating absolute gains of 25.1% and 14.7% for novelty and functional coherence in human evaluation, respectively.",
      "arxiv_url": "https://arxiv.org/abs/2502.17793",
      "pdf_url": "https://arxiv.org/pdf/2502.17793",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11594",
      "title": "iMOVE: Instance-Motion-Aware Video Understanding",
      "authors": [
        "Jiaze Li",
        "Yaya Shi",
        "Zongyang Ma",
        "Haoran Xu",
        "Feng Cheng",
        "Huihui Xiao",
        "Ruiwen Kang",
        "Fan Yang",
        "Tingting Gao",
        "Di Zhang"
      ],
      "abstract": "Enhancing the fine-grained instance spatiotemporal motion perception capabilities of Video Large Language Models is crucial for improving their temporal and general video understanding. However, current models struggle to perceive detailed and complex instance motions. To address these challenges, we have made improvements from both data and model perspectives. In terms of data, we have meticulously curated iMOVE-IT, the first large-scale instance-motion-aware video instruction-tuning dataset. This dataset is enriched with comprehensive instance motion annotations and spatiotemporal mutual-supervision tasks, providing extensive training for the model's instance-motion-awareness. Building on this foundation, we introduce iMOVE, an instance-motion-aware video foundation model that utilizes Event-aware Spatiotemporal Efficient Modeling to retain informative instance spatiotemporal motion details while maintaining computational efficiency. It also incorporates Relative Spatiotemporal Position Tokens to ensure awareness of instance spatiotemporal positions. Evaluations indicate that iMOVE excels not only in video temporal understanding and general video understanding but also demonstrates significant advantages in long-term video understanding.",
      "arxiv_url": "https://arxiv.org/abs/2502.11594",
      "pdf_url": "https://arxiv.org/pdf/2502.11594",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11084",
      "title": "Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction",
      "authors": [
        "Yuting Huang",
        "Chengyuan Liu",
        "Yifeng Feng",
        "Yiquan Wu",
        "Chao Wu",
        "Fei Wu",
        "Kun Kuang"
      ],
      "abstract": "As Large Language Models (LLMs) are widely applied in various domains, the safety of LLMs is increasingly attracting attention to avoid their powerful capabilities being misused. Existing jailbreak methods create a forced instruction-following scenario, or search adversarial prompts with prefix or suffix tokens to achieve a specific representation manually or automatically. However, they suffer from low efficiency and explicit jailbreak patterns, far from the real deployment of mass attacks to LLMs. In this paper, we point out that simply rewriting the original instruction can achieve a jailbreak, and we find that this rewriting approach is learnable and transferable. We propose the Rewrite to Jailbreak (R2J) approach, a transferable black-box jailbreak method to attack LLMs by iteratively exploring the weakness of the LLMs and automatically improving the attacking strategy. The jailbreak is more efficient and hard to identify since no additional features are introduced. Extensive experiments and analysis demonstrate the effectiveness of R2J, and we find that the jailbreak is also transferable to multiple datasets and various types of models with only a few queries. We hope our work motivates further investigation of LLM safety. The code can be found at https://github.com/ythuang02/R2J/.",
      "arxiv_url": "https://arxiv.org/abs/2502.11084",
      "pdf_url": "https://arxiv.org/pdf/2502.11084",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.01926",
      "title": "Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs",
      "authors": [
        "Angelina Wang",
        "Michelle Phan",
        "Daniel E. Ho",
        "Oluwasanmi Koyejo"
      ],
      "abstract": "Algorithmic fairness has conventionally adopted the mathematically convenient perspective of racial color-blindness (i.e., difference unaware treatment). However, we contend that in a range of important settings, group difference awareness matters. For example, differentiating between groups may be necessary in legal contexts (e.g., the U.S. compulsory draft applies to men but not women) and harm assessments (e.g., referring to girls as ``terrorists''may be less harmful than referring to Muslim people as such). Thus, in contrast to most fairness work, we study fairness through the perspective of treating people differently -- when it is contextually appropriate to. We first introduce an important distinction between descriptive (fact-based), normative (value-based), and correlation (association-based) benchmarks. This distinction is significant because each category requires separate interpretation and mitigation tailored to its specific characteristics. Then, we present a benchmark suite composed of eight different scenarios for a total of 16k questions that enables us to assess difference awareness. Finally, we show results across ten models that demonstrate difference awareness is a distinct dimension to fairness where existing bias mitigation strategies may backfire.",
      "arxiv_url": "https://arxiv.org/abs/2502.01926",
      "pdf_url": "https://arxiv.org/pdf/2502.01926",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12084",
      "title": "VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues",
      "authors": [
        "Jianshu Zhang",
        "Dongyu Yao",
        "Renjie Pi",
        "Paul Pu Liang",
        "Yi R. Fung"
      ],
      "abstract": "Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce \\textbf{VLM2-Bench}, a benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across twelve VLMs, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models'ability to link visual cues, highlighting a significant performance gap. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models'ability to independently structure and infer relationships among visual cues.",
      "arxiv_url": "https://arxiv.org/abs/2502.12084",
      "pdf_url": "https://arxiv.org/pdf/2502.12084",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17407",
      "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning",
      "authors": [
        "Guijin Son",
        "Jiwoo Hong",
        "Hyunwoo Ko",
        "James Thorne"
      ],
      "abstract": "Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing (BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although\"thinking LLMs\"have recently garnered significant attention, we find that their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. Moreover, while BF yields a 20-point improvement on English AIME, it provides only a 1.94-point average gain across other languages-a pattern consistent across the other test-time scaling methods we studied-higlighting that test-time scaling may not generalize as effectively to multilingual tasks. To foster further research, we release MCLM, MR1-1.5B, and evaluation results.",
      "arxiv_url": "https://arxiv.org/abs/2502.17407",
      "pdf_url": "https://arxiv.org/pdf/2502.17407",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.15354",
      "title": "Optimizing Decomposition for Optimal Claim Verification",
      "authors": [
        "Yining Lu",
        "Noah Ziems",
        "Hy Dang",
        "Meng Jiang"
      ],
      "abstract": "Current research on the \\textit{Decompose-Then-Verify} paradigm for evaluating the factuality of long-form text typically treats decomposition and verification in isolation, overlooking their interactions and potential misalignment. We find that existing decomposition policies, typically hand-crafted demonstrations, do not align well with downstream verifiers in terms of atomicity -- a novel metric quantifying information density -- leading to suboptimal verification results. We formulate finding the optimal decomposition policy for optimal verification as a bilevel optimization problem. To approximate a solution for this strongly NP-hard problem, we propose dynamic decomposition, a reinforcement learning framework that leverages verifier feedback to learn a policy for dynamically decomposing claims to verifier-preferred atomicity. Experimental results show that dynamic decomposition outperforms existing decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on average across varying verifiers, datasets, and atomcities of input claims.",
      "arxiv_url": "https://arxiv.org/abs/2503.15354",
      "pdf_url": "https://arxiv.org/pdf/2503.15354",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.11942",
      "title": "DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression",
      "authors": [
        "Yi Zhao",
        "Z. Li",
        "Hai Zhao",
        "Baoyuan Qi",
        "Guoming Liu"
      ],
      "abstract": "Task-agnostic prompt compression leverages the redundancy in natural language to reduce computational overhead and enhance information density within prompts, especially in long-context scenarios. Existing methods predominantly rely on information entropy as the metric to compress lexical units, aiming to achieve minimal information loss. However, these approaches overlook two critical aspects: (i) the importance of attention-critical tokens at the algorithmic level, and (ii) shifts in information entropy during the compression process. Motivated by these challenges, we propose a dynamic attention-aware approach for task-agnostic prompt compression (DAC). This approach effectively integrates entropy and attention information, dynamically sensing entropy shifts during compression to achieve fine-grained prompt compression. Extensive experiments across various domains, including LongBench, GSM8K, and BBH, show that DAC consistently yields robust and substantial improvements across a diverse range of tasks and LLMs, offering compelling evidence of its efficacy.",
      "arxiv_url": "https://arxiv.org/abs/2507.11942",
      "pdf_url": "https://arxiv.org/pdf/2507.11942",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-07-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.19188",
      "title": "Hierarchical Attention Generates Better Proofs",
      "authors": [
        "Jianlong Chen",
        "Chao Li",
        "Yang Yuan",
        "A. C. Yao"
      ],
      "abstract": "Large language models (LLMs) have shown promise in formal theorem proving, but their token-level processing often fails to capture the inherent hierarchical nature of mathematical proofs. We introduce \\textbf{Hierarchical Attention}, a regularization method that aligns LLMs' attention mechanisms with mathematical reasoning structures. Our approach establishes a five-level hierarchy from foundational elements to high-level concepts, ensuring structured information flow in proof generation. Experiments demonstrate that our method improves proof success rates by 2.05\\% on miniF2F and 1.69\\% on ProofNet while reducing proof complexity by 23.81\\% and 16.50\\% respectively. The code is available at https://github.com/Car-pe/HAGBP.",
      "arxiv_url": "https://arxiv.org/abs/2504.19188",
      "pdf_url": "https://arxiv.org/pdf/2504.19188",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.15196",
      "title": "EcomScriptBench: A Multi-task Benchmark for E-commerce Script Planning via Step-wise Intention-Driven Product Association",
      "authors": [
        "Weiqi Wang",
        "Limeng Cui",
        "Xin Liu",
        "Sreyashi Nag",
        "Wenju Xu",
        "Chen Luo",
        "S. Sarwar",
        "Yang Li",
        "Hansu Gu",
        "Hui Liu",
        "Changlong Yu",
        "Jiaxin Bai",
        "Yifan Gao",
        "Haiyang Zhang",
        "Qi He",
        "Shuiwang Ji",
        "Yangqiu Song"
      ],
      "abstract": "Goal-oriented script planning, or the ability to devise coherent sequences of actions toward specific goals, is commonly employed by humans to plan for typical activities. In e-commerce, customers increasingly seek LLM-based assistants to generate scripts and recommend products at each step, thereby facilitating convenient and efficient shopping experiences. However, this capability remains underexplored due to several challenges, including the inability of LLMs to simultaneously conduct script planning and product retrieval, difficulties in matching products caused by semantic discrepancies between planned actions and search queries, and a lack of methods and benchmark data for evaluation. In this paper, we step forward by formally defining the task of E-commerce Script Planning (EcomScript) as three sequential subtasks. We propose a novel framework that enables the scalable generation of product-enriched scripts by associating products with each step based on the semantic similarity between the actions and their purchase intentions. By applying our framework to real-world e-commerce data, we construct the very first large-scale EcomScript dataset, EcomScriptBench, which includes 605,229 scripts sourced from 2.4 million products. Human annotations are then conducted to provide gold labels for a sampled subset, forming an evaluation benchmark. Extensive experiments reveal that current (L)LMs face significant challenges with EcomScript tasks, even after fine-tuning, while injecting product purchase intentions improves their performance.",
      "arxiv_url": "https://arxiv.org/abs/2505.15196",
      "pdf_url": "https://arxiv.org/pdf/2505.15196",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2ef1770eae07c9684e215dc9e77bddd8e08b5d72",
      "title": "HTML: Hierarchical Topology Multi-task Learning for Semantic Parsing in Knowledge Base Question Answering",
      "authors": [
        "A. Wulamu",
        "Lyu Zhengyu",
        "Kaiyuan Gong",
        "Yu Han",
        "Zewen Wang",
        "Zhihong Zhu",
        "Bowen Xing"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/2ef1770eae07c9684e215dc9e77bddd8e08b5d72",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2ef7bbcba4e0e94151724a0462ad8e8000085f44",
      "title": "Browsing Like Human: A Multimodal Web Agent with Experiential Fast-and-Slow Thinking",
      "authors": [
        "Haohao Luo",
        "Jiayi Kuang",
        "Wei Liu",
        "Ying Shen",
        "Jian Luan",
        "Yang Deng"
      ],
      "abstract": "Automating web navigation which aims to build a web agent that follows user instructions to complete tasks like booking flights by inter-acting with websites, has received increasing attention due to its practical value. Although existing web agents are mostly equipped with visual perception, planning, and memory abilities, their reasoning process are still deviate from human cognition. In this work, we study the human thought pattern to empower agent with more human-like abilities in web navigation. To tackle this problem, we propose a novel multimodal web agent framework called WebExperT, which is designed to emulate the human planning process of \"thinking fast and slow\" to effectively decompose complex user instructions. Furthermore, WebExperT leverages experiential learning by reflecting from failure for continuously refining planning and decision-making outcomes. Experimental re-sults on the M IND 2W EB benchmark demonstrate the superiority of WebExperT in both supervised and unsupervised settings.",
      "arxiv_url": "https://www.semanticscholar.org/paper/2ef7bbcba4e0e94151724a0462ad8e8000085f44",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2f28080cdc8c76616e25c4640875389783e2a320",
      "title": "Taming LLMs with Gradient Grouping",
      "authors": [
        "Siyuan Li",
        "Juanxi Tian",
        "Zedong Wang",
        "Xin Jin",
        "Zicheng Liu",
        "Wentao Zhang",
        "Dan Xu"
      ],
      "abstract": "Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces S caling with G radient G rouping ( SGG ), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization.",
      "arxiv_url": "https://www.semanticscholar.org/paper/2f28080cdc8c76616e25c4640875389783e2a320",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2f58a1c9b387ebdaeeb2e6b36f044c06e34af540",
      "title": "Scaling LLMs' Social Reasoning: Sprinkle Cognitive \"Aha Moment\" into Fundamental Long-thought Logical Capabilities",
      "authors": [
        "Guiyang Hou",
        "Wenqi Zhang",
        "Zhe Zheng",
        "Yongliang Shen",
        "Weiming Lu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/2f58a1c9b387ebdaeeb2e6b36f044c06e34af540",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.13176",
      "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models",
      "authors": [
        "Zihao Cheng",
        "Hongru Wang",
        "Zeming Liu",
        "Yuhang Guo",
        "Yuanfang Guo",
        "Yunhong Wang",
        "Haifeng Wang"
      ],
      "abstract": "While integrating external tools into large language models (LLMs) enhances their ability to access real-time information and domain-specific services, existing approaches focus narrowly on functional tool selection following user instructions, overlooking the context-aware personalization in tool selection. This oversight leads to suboptimal user satisfaction and inefficient tool utilization, particularly when overlapping toolsets require nuanced selection based on contextual factors. To bridge this gap, we introduce ToolSpectrum, a benchmark designed to evaluate LLMs' capabilities in personalized tool utilization. Specifically, we formalize two key dimensions of personalization, user profile and environmental factors, and analyze their individual and synergistic impacts on tool utilization. Through extensive experiments on ToolSpectrum, we demonstrate that personalized tool utilization significantly improves user experience across diverse scenarios. However, even state-of-the-art LLMs exhibit the limited ability to reason jointly about user profiles and environmental factors, often prioritizing one dimension at the expense of the other. Our findings underscore the necessity of context-aware personalization in tool-augmented LLMs and reveal critical limitations for current models. Our data and code are available at https://github.com/Chengziha0/ToolSpectrum.",
      "arxiv_url": "https://arxiv.org/abs/2505.13176",
      "pdf_url": "https://arxiv.org/pdf/2505.13176",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.22196",
      "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge Devices",
      "authors": [
        "Jiyu Chen",
        "Shuang Peng",
        "Daxiong Luo",
        "Fan Yang",
        "Renshou Wu",
        "Fangyuan Li",
        "Xiaoxin Chen"
      ],
      "abstract": "Transformer-based large language models (LLMs) encounter challenges in processing long sequences on edge devices due to the quadratic complexity of attention mechanisms and growing memory demands from Key-Value (KV) cache. Existing KV cache optimizations struggle with irreversible token eviction in long-output tasks, while alternative sequence modeling architectures prove costly to adopt within established Transformer infrastructure. We present EdgeInfinite, a memory-efficient solution for infinite contexts that integrates compressed memory into Transformer-based LLMs through a trainable memory-gating module. This approach maintains full compatibility with standard Transformer architectures, requiring fine-tuning only a small part of parameters, and enables selective activation of the memory-gating module for long and short context task routing. The experimental result shows that EdgeInfinite achieves comparable performance to baseline Transformer-based LLM on long context benchmarks while optimizing memory consumption and time to first token.",
      "arxiv_url": "https://arxiv.org/abs/2503.22196",
      "pdf_url": "https://arxiv.org/pdf/2503.22196",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-03-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11726",
      "title": "Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures",
      "authors": [
        "Shun Inadumi",
        "Nobuhiro Ueda",
        "Koichiro Yoshino"
      ],
      "abstract": "Multimodal reference resolution, including phrase grounding, aims to understand the semantic relations between mentions and real-world objects. Phrase grounding between images and their captions is a well-established task. In contrast, for real-world applications, it is essential to integrate textual and multimodal reference resolution to unravel the reference relations within dialogue, especially in handling ambiguities caused by pronouns and ellipses. This paper presents a framework that unifies textual and multimodal reference resolution by mapping mention embeddings to object embeddings and selecting mentions or objects based on their similarity. Our experiments show that learning textual reference resolution, such as coreference resolution and predicate-argument structure analysis, positively affects performance in multimodal reference resolution. In particular, our model with coreference resolution performs better in pronoun phrase grounding than representative models for this task, MDETR and GLIP. Our qualitative analysis demonstrates that incorporating textual reference relations strengthens the confidence scores between mentions, including pronouns and predicates, and objects, which can reduce the ambiguities that arise in visually grounded dialogues.",
      "arxiv_url": "https://arxiv.org/abs/2505.11726",
      "pdf_url": "https://arxiv.org/pdf/2505.11726",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.15717",
      "title": "daDPO: Distribution-Aware DPO for Distilling Conversational Abilities",
      "authors": [
        "Zhengze Zhang",
        "Shiqi Wang",
        "Yiqun Shen",
        "Simin Guo",
        "Dahua Lin",
        "Xiaoliang Wang",
        "C. Nguyen",
        "Fei Tan"
      ],
      "abstract": "Large language models (LLMs) have demonstrated exceptional performance across various applications, but their conversational abilities decline sharply as model size decreases, presenting a barrier to their deployment in resource-constrained environments. Knowledge distillation with Direct Preference Optimization (dDPO) has emerged as a promising approach to enhancing the conversational abilities of smaller models using a larger teacher model. However, current methods primarily focus on'black-box'KD, which only uses the teacher's responses, overlooking the output distribution offered by the teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware DPO), a unified method for preference optimization and distribution-based distillation. We provide rigorous theoretical analysis and empirical validation, showing that daDPO outperforms existing methods in restoring performance for pruned models and enhancing smaller LLM models. Notably, in in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve near-teacher performance (-7.3% preference rate compared to that of dDPO's -31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model (14.0% win rate).",
      "arxiv_url": "https://arxiv.org/abs/2506.15717",
      "pdf_url": "https://arxiv.org/pdf/2506.15717",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01047",
      "title": "CHEER-Ekman: Fine-grained Embodied Emotion Classification",
      "authors": [
        "Phan Anh Duong",
        "Cat Luong",
        "Divyesh Bommana",
        "Tianyu Jiang"
      ],
      "abstract": "Emotions manifest through physical experiences and bodily reactions, yet identifying such embodied emotions in text remains understudied. We present an embodied emotion classification dataset, CHEER-Ekman, extending the existing binary embodied emotion dataset with Ekman's six basic emotion categories. Using automatic best-worst scaling with large language models, we achieve performance superior to supervised approaches on our new dataset. Our investigation reveals that simplified prompting instructions and chain-of-thought reasoning significantly improve emotion recognition accuracy, enabling smaller models to achieve competitive performance with larger ones. Our dataset is publicly available at: https://github.com/menamerai/cheer-ekman.",
      "arxiv_url": "https://arxiv.org/abs/2506.01047",
      "pdf_url": "https://arxiv.org/pdf/2506.01047",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.19325",
      "title": "FEAT: A Preference Feedback Dataset through a Cost-Effective Auto-Generation and Labeling Framework for English AI Tutoring",
      "authors": [
        "H. Seo",
        "Taewook Hwang",
        "Yohan Lee",
        "S. Jung"
      ],
      "abstract": "In English education tutoring, teacher feedback is essential for guiding students. Recently, AI-based tutoring systems have emerged to assist teachers; however, these systems require high-quality and large-scale teacher feedback data, which is both time-consuming and costly to generate manually. In this study, we propose FEAT, a cost-effective framework for generating teacher feedback, and have constructed three complementary datasets: (1) DIRECT-Manual (DM), where both humans and large language models (LLMs) collaboratively generate high-quality teacher feedback, albeit at a higher cost; (2) DIRECT-Generated (DG), an LLM-only generated, cost-effective dataset with lower quality;, and (3) DIRECT-Augmented (DA), primarily based on DG with a small portion of DM added to enhance quality while maintaining cost-efficiency. Experimental results showed that incorporating a small portion of DM (5-10%) into DG leads to superior performance compared to using 100% DM alone.",
      "arxiv_url": "https://arxiv.org/abs/2506.19325",
      "pdf_url": "https://arxiv.org/pdf/2506.19325",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.08820",
      "title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use? CoALM: A Unified Conversational Agentic Language Model",
      "authors": [
        "Emre Can Acikgoz",
        "Jeremy Greer",
        "Akul Datta",
        "Ze Yang",
        "William Zeng",
        "Oussama Elachqar",
        "Emmanouil Koukoumidis",
        "Dilek Hakkani-Tur",
        "Gokhan Tur"
      ],
      "abstract": "Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA), and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce CoALM (Conversational Agentic Language Model), a unified approach that integrates both conversational and agentic capabilities. We created CoALM-IT, a carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B, and CoALM 405B, which outperform top domain-specific models, including GPT-4o, across all three benchmarks. This demonstrates the feasibility of a single model approach for both TOD and LA, setting a new standard for conversational agents.",
      "arxiv_url": "https://arxiv.org/abs/2502.08820",
      "pdf_url": "https://arxiv.org/pdf/2502.08820",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12446",
      "title": "From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment",
      "authors": [
        "Bin Xie",
        "Bingbing Xu",
        "Yige Yuan",
        "Shengmao Zhu",
        "Huawei Shen"
      ],
      "abstract": "Inference-time alignment methods have gained significant attention for their efficiency and effectiveness in aligning large language models (LLMs) with human preferences. However, existing dominant approaches using reward-guided search (RGS) primarily rely on outcome reward models (ORMs), which suffer from a critical granularity mismatch: ORMs are designed to provide outcome rewards for complete responses, while RGS methods rely on process rewards to guide the policy, leading to inconsistent scoring and suboptimal alignment. To address this challenge, we introduce process reward models (PRMs) into RGS and argue that an ideal PRM should satisfy two objectives: Score Consistency, ensuring coherent evaluation across partial and complete responses, and Preference Consistency, aligning partial sequence assessments with human preferences. Based on these, we propose SP-PRM, a novel dual-consistency framework integrating score consistency-based and preference consistency-based partial evaluation modules without relying on human annotation. Extensive experiments on dialogue, summarization, and reasoning tasks demonstrate that SP-PRM substantially enhances existing RGS methods, achieving a 3.6%-10.3% improvement in GPT-4 evaluation scores across all tasks.",
      "arxiv_url": "https://arxiv.org/abs/2506.12446",
      "pdf_url": "https://arxiv.org/pdf/2506.12446",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.13089",
      "title": "Systematic Generalization in Language Models Scales with Information Entropy",
      "authors": [
        "Sondre Wold",
        "Lucas Georges Gabriel Charpentier",
        "Étienne Simon"
      ],
      "abstract": "Systematic generalization remains challenging for current language models, which are known to be both sensitive to semantically similar permutations of the input and to struggle with known concepts presented in novel contexts. Although benchmarks exist for assessing compositional behavior, it is unclear how to measure the difficulty of a systematic generalization problem. In this work, we show how one aspect of systematic generalization can be described by the entropy of the distribution of component parts in the training data. We formalize a framework for measuring entropy in a sequence-to-sequence task and find that the performance of popular model architectures scales with the entropy. Our work connects systematic generalization to information efficiency, and our results indicate that success at high entropy can be achieved even without built-in priors, and that success at low entropy can serve as a target for assessing progress towards robust systematic generalization.",
      "arxiv_url": "https://arxiv.org/abs/2505.13089",
      "pdf_url": "https://arxiv.org/pdf/2505.13089",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16235",
      "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
      "authors": [
        "Yifu Ding",
        "Wentao Jiang",
        "Shunyu Liu",
        "Yongcheng Jing",
        "Jinyang Guo",
        "Yingjie Wang",
        "Jing Zhang",
        "Zengmao Wang",
        "Ziwei Liu",
        "Bo Du",
        "Xianglong Liu",
        "Dacheng Tao"
      ],
      "abstract": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by structuring problem-solving as a spanning tree. However, recent methods focus on search accuracy while overlooking computational efficiency. The challenges of accelerating the ToT lie in the frequent switching of reasoning focus, and the redundant exploration of suboptimal solutions. To alleviate this dilemma, we propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework that aims to dynamically optimize the reasoning path in inference. It includes the Parallelism Streamline in the generation phase to build up a flexible and adaptive parallelism with arbitrary paths by fine-grained cache management and alignment. Meanwhile, the Search and Transition Mechanism filters potential candidates to dynamically maintain the reasoning focus on more possible solutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with Math500 and GSM8K datasets show that DPTS significantly improves efficiency by 2-4x on average while maintaining or even surpassing existing reasoning algorithms in accuracy, making ToT-based reasoning more scalable and computationally efficient.",
      "arxiv_url": "https://arxiv.org/abs/2502.16235",
      "pdf_url": "https://arxiv.org/pdf/2502.16235",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.13089",
      "title": "ClusComp: A Simple Paradigm for Model Compression and Efficient Finetuning",
      "authors": [
        "Baohao Liao",
        "Christian Herold",
        "Seyyed Hadi Hashemi",
        "S. Vasilev",
        "Shahram Khadivi",
        "C. Monz"
      ],
      "abstract": "As large language models (LLMs) scale, model compression is crucial for edge deployment and accessibility. Weight-only quantization reduces model size but suffers from performance degradation at lower bit widths. Moreover, standard finetuning is incompatible with quantized models, and alternative methods often fall short of full finetuning. In this paper, we propose ClusComp, a simple yet effective compression paradigm that clusters weight matrices into codebooks and finetunes them block-by-block. ClusComp (1) achieves superior performance in 2-4 bit quantization, (2) pushes compression to 1-bit while outperforming ultra-low-bit methods with minimal finetuning, and (3) enables efficient finetuning, even surpassing existing quantization-based approaches and rivaling full FP16 finetuning. Notably, ClusComp supports compression and finetuning of 70B LLMs on a single A6000-48GB GPU.",
      "arxiv_url": "https://arxiv.org/abs/2503.13089",
      "pdf_url": "https://arxiv.org/pdf/2503.13089",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18720",
      "title": "Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization",
      "authors": [
        "Meng Li",
        "Guangda Huzhang",
        "Haibo Zhang",
        "Xiting Wang",
        "Anxiang Zeng"
      ],
      "abstract": "Direct Preference Optimization (DPO) has emerged as a promising framework for aligning Large Language Models (LLMs) with human preferences by directly optimizing the log-likelihood difference between chosen and rejected responses. However, existing methods assign equal importance to all tokens in the response, while humans focus on more meaningful parts. This leads to suboptimal preference optimization, as irrelevant or noisy tokens disproportionately influence DPO loss. To address this limitation, we propose \\textbf{O}ptimal \\textbf{T}ransport-based token weighting scheme for enhancing direct \\textbf{P}reference \\textbf{O}ptimization (OTPO). By emphasizing semantically meaningful token pairs and de-emphasizing less relevant ones, our method introduces a context-aware token weighting scheme that yields a more contrastive reward difference estimate. This adaptive weighting enhances reward stability, improves interpretability, and ensures that preference optimization focuses on meaningful differences between responses. Extensive experiments have validated OTPO's effectiveness in improving instruction-following ability across various settings\\footnote{Code is available at https://github.com/Mimasss2/OTPO.}.",
      "arxiv_url": "https://arxiv.org/abs/2505.18720",
      "pdf_url": "https://arxiv.org/pdf/2505.18720",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3150a8bfc400c92e783815626ee953a0b8634042",
      "title": "Is a cute puyfred cute? Context-dependent form-meaning systematicity in LLMs",
      "authors": [
        "Jaïr A Waal",
        "Giovanni Cassani"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/3150a8bfc400c92e783815626ee953a0b8634042",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.09416",
      "title": "Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use a Different Evaluation Process than Human?",
      "authors": [
        "Takumi Goto",
        "Yusuke Sakai",
        "Taro Watanabe"
      ],
      "abstract": "One of the goals of automatic evaluation metrics in grammatical error correction (GEC) is to rank GEC systems such that it matches human preferences. However, current automatic evaluations are based on procedures that diverge from human evaluation. Specifically, human evaluation derives rankings by aggregating sentence-level relative evaluation results, e.g., pairwise comparisons, using a rating algorithm, whereas automatic evaluation averages sentence-level absolute scores to obtain corpus-level scores, which are then sorted to determine rankings. In this study, we propose an aggregation method for existing automatic evaluation metrics which aligns with human evaluation methods to bridge this gap. We conducted experiments using various metrics, including edit-based metrics, n-gram based metrics, and sentence-level metrics, and show that resolving the gap improves results for the most of metrics on the SEEDA benchmark. We also found that even BERT-based metrics sometimes outperform the metrics of GPT-4. The proposed ranking method is integrated gec-metrics.",
      "arxiv_url": "https://arxiv.org/abs/2502.09416",
      "pdf_url": "https://arxiv.org/pdf/2502.09416",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.16365",
      "title": "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse",
      "authors": [
        "Muyao Li",
        "Zihao Wang",
        "Kaichen He",
        "Xiaojian Ma",
        "Yitao Liang"
      ],
      "abstract": "Recently, action-based decision-making in open-world environments has gained significant attention. Visual Language Action (VLA) models, pretrained on large-scale web datasets, have shown promise in decision-making tasks. However, previous work has primarily focused on action post-training, often neglecting enhancements to the foundational model itself. In response, we introduce a novel approach, Act from Visual Language Post-Training, which refines Visual Language Models (VLMs) through visual and linguistic guidance in a self-supervised manner. This enhancement improves the models'capabilities in world knowledge, visual recognition, and spatial grounding in open-world environments. Following the above post-training paradigms, we obtain the first VLA models in Minecraft that can follow human instructions on over 1k different atomic tasks, including crafting, smelting, cooking, mining, and killing. Our experiments demonstrate that post-training on non-trajectory tasks leads to a significant 40% improvement over the best agent baseline on a diverse set of atomic tasks. Furthermore, we demonstrate that our approach surpasses traditional imitation learning-based policies in Minecraft, achieving state-of-the-art performance. We have open-sourced the code, models, and datasets to foster further research. The project page can be found in https://craftjarvis.github.io/JarvisVLA.",
      "arxiv_url": "https://arxiv.org/abs/2503.16365",
      "pdf_url": "https://arxiv.org/pdf/2503.16365",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01262",
      "title": "Exploring the Potential of LLMs as Personalized Assistants: Dataset, Evaluation, and Analysis",
      "authors": [
        "J. Mok",
        "Ik-hwan Kim",
        "Sangkwon Park",
        "Sungroh Yoon"
      ],
      "abstract": "Personalized AI assistants, a hallmark of the human-like capabilities of Large Language Models (LLMs), are a challenging application that intertwines multiple problems in LLM research. Despite the growing interest in the development of personalized assistants, the lack of an open-source conversational dataset tailored for personalization remains a significant obstacle for researchers in the field. To address this research gap, we introduce HiCUPID, a new benchmark to probe and unleash the potential of LLMs to deliver personalized responses. Alongside a conversational dataset, HiCUPID provides a Llama-3.2-based automated evaluation model whose assessment closely mirrors human preferences. We release our dataset, evaluation model, and code at https://github.com/12kimih/HiCUPID.",
      "arxiv_url": "https://arxiv.org/abs/2506.01262",
      "pdf_url": "https://arxiv.org/pdf/2506.01262",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "31d3c43f24eea9d5d35b573ee5e24d47a5d6c483",
      "title": "ADEPT-SQL: A High-performance Text-to-SQL Application for Real-World Enterprise-Level Databases",
      "authors": [
        "Yongnan Chen",
        "Zhuo Chang",
        "Shijia Gu",
        "Yuanhang Zong",
        "Mei Zhang",
        "Shiyu Wang",
        "Zixiang He",
        "Hongzhi Chen",
        "W. Jin",
        "Bin Cui"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/31d3c43f24eea9d5d35b573ee5e24d47a5d6c483",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07434",
      "title": "Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding",
      "authors": [
        "Feifan Song",
        "Shaohang Wei",
        "Wen Luo",
        "Yuxuan Fan",
        "Tianyu Liu",
        "Guoyin Wang",
        "Houfeng Wang"
      ],
      "abstract": "Large Language Models (LLMs) require alignment with human preferences to avoid generating offensive, false, or meaningless content. Recently, low-resource methods for LLM alignment have been popular, while still facing challenges in obtaining both high-quality and aligned content. Motivated by the observation that the difficulty of generating aligned responses is concentrated at the beginning of decoding, we propose a novel framework, Weak-to-Strong Decoding (WSD), to enhance the alignment ability of base models by the guidance of a small aligned model. The small model first drafts well-aligned beginnings, followed by the large base model to continue the rest, controlled by a well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign, to fine-tune a small-sized Pilot-3B as the draft model, which effectively enhances different base models under the WSD framework to outperform all baseline methods, while avoiding degradation on downstream tasks, termed as the alignment tax. Extensive experiments are further conducted to examine the impact of different settings and time efficiency, as well as analyses on the intrinsic mechanisms of WSD in depth.",
      "arxiv_url": "https://arxiv.org/abs/2506.07434",
      "pdf_url": "https://arxiv.org/pdf/2506.07434",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14356",
      "title": "Full-Step-DPO: Self-Supervised Preference Optimization with Step-wise Rewards for Mathematical Reasoning",
      "authors": [
        "Huimin Xu",
        "Xin Mao",
        "Fengming Li",
        "Xiaobao Wu",
        "Wang Chen",
        "Wei Zhang",
        "A. Luu"
      ],
      "abstract": "Direct Preference Optimization (DPO) often struggles with long-chain mathematical reasoning. Existing approaches, such as Step-DPO, typically improve this by focusing on the first erroneous step in the reasoning chain. However, they overlook all other steps and rely heavily on humans or GPT-4 to identify erroneous steps. To address these issues, we propose Full-Step-DPO, a novel DPO framework tailored for mathematical reasoning. Instead of optimizing only the first erroneous step, it leverages step-wise rewards from the entire reasoning chain. This is achieved by training a self-supervised process reward model, which automatically scores each step, providing rewards while avoiding reliance on external signals. Furthermore, we introduce a novel step-wise DPO loss, which dynamically updates gradients based on these step-wise rewards. This endows stronger reasoning capabilities to language models. Extensive evaluations on both in-domain and out-of-domain mathematical reasoning benchmarks across various base language models, demonstrate that Full-Step-DPO achieves superior performance compared to state-of-the-art baselines.",
      "arxiv_url": "https://arxiv.org/abs/2502.14356",
      "pdf_url": "https://arxiv.org/pdf/2502.14356",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.05855",
      "title": "ConSim: Measuring Concept-Based Explanations' Effectiveness with Automated Simulatability",
      "authors": [
        "Antonin Poch'e",
        "Alon Jacovi",
        "A. Picard",
        "Victor Boutin",
        "Fanny Jourdan"
      ],
      "abstract": "Concept-based explanations work by mapping complex model computations to human-understandable concepts. Evaluating such explanations is very difficult, as it includes not only the quality of the induced space of possible concepts but also how effectively the chosen concepts are communicated to users. Existing evaluation metrics often focus solely on the former, neglecting the latter. We introduce an evaluation framework for measuring concept explanations via automated simulatability: a simulator's ability to predict the explained model's outputs based on the provided explanations. This approach accounts for both the concept space and its interpretation in an end-to-end evaluation. Human studies for simulatability are notoriously difficult to enact, particularly at the scale of a wide, comprehensive empirical evaluation (which is the subject of this work). We propose using large language models (LLMs) as simulators to approximate the evaluation and report various analyses to make such approximations reliable. Our method allows for scalable and consistent evaluation across various models and datasets. We report a comprehensive empirical evaluation using this framework and show that LLMs provide consistent rankings of explanation methods. Code available at https://github.com/AnonymousConSim/ConSim.",
      "arxiv_url": "https://arxiv.org/abs/2501.05855",
      "pdf_url": "https://arxiv.org/pdf/2501.05855",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20959",
      "title": "Research Community Perspectives on \"Intelligence\" and Large Language Models",
      "authors": [
        "Bertram Højer",
        "Terne Sasha Thorn Jakobsen",
        "Anna Rogers",
        "S. Heinrich"
      ],
      "abstract": "Despite the widespread use of''artificial intelligence''(AI) framing in Natural Language Processing (NLP) research, it is not clear what researchers mean by''intelligence''. To that end, we present the results of a survey on the notion of''intelligence''among researchers and its role in the research agenda. The survey elicited complete responses from 303 researchers from a variety of fields including NLP, Machine Learning (ML), Cognitive Science, Linguistics, and Neuroscience. We identify 3 criteria of intelligence that the community agrees on the most: generalization, adaptability,&reasoning. Our results suggests that the perception of the current NLP systems as''intelligent''is a minority position (29%). Furthermore, only 16.2% of the respondents see developing intelligent systems as a research goal, and these respondents are more likely to consider the current systems intelligent.",
      "arxiv_url": "https://arxiv.org/abs/2505.20959",
      "pdf_url": "https://arxiv.org/pdf/2505.20959",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00773",
      "title": "Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models",
      "authors": [
        "Boheng Sheng",
        "Jiacheng Yao",
        "Meicong Zhang",
        "Guoxiu He"
      ],
      "abstract": "Large language models (LLMs) often struggle to accurately read and comprehend extremely long texts. Current methods for improvement typically rely on splitting long contexts into fixed-length chunks. However, fixed truncation risks separating semantically relevant content, leading to ambiguity and compromising accurate understanding. To overcome this limitation, we propose a straightforward approach for dynamically separating and selecting chunks of long context, facilitating a more streamlined input for LLMs. In particular, we compute semantic similarities between adjacent sentences, using lower similarities to adaptively divide long contexts into variable-length chunks. We further train a question-aware classifier to select sensitive chunks that are critical for answering specific questions. Experimental results on both single-hop and multi-hop question-answering benchmarks show that the proposed approach consistently outperforms strong baselines. Notably, it maintains robustness across a wide range of input lengths, handling sequences of up to 256k tokens. Our datasets and code are available at the following link: https://github.com/ECNU-Text-Computing/DCS",
      "arxiv_url": "https://arxiv.org/abs/2506.00773",
      "pdf_url": "https://arxiv.org/pdf/2506.00773",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "32e558075e6da0ecc45b74421296915349b4136e",
      "title": "LEAP & LEAN: Look-ahead Planning and Agile Navigation for LLM Agents",
      "authors": [
        "Nikhil Verma",
        "Manasa Bharadwaj"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/32e558075e6da0ecc45b74421296915349b4136e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "334f6c67a62af75847c41ea15552dde23679cf3b",
      "title": "RecordTwin: Towards Creating Safe Synthetic Clinical Corpora",
      "authors": [
        "Seiji Shimizu",
        "Ibrahim Baroud",
        "Lisa Raithel",
        "Shuntaro Yada",
        "Shoko Wakamiya",
        "E. Aramaki"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/334f6c67a62af75847c41ea15552dde23679cf3b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3378e381acdf8d8536bc9687e1b4183de1891487",
      "title": "To Chat or Task: a Multi-turn Dialogue Generation Framework for Task-Oriented Dialogue Systems",
      "authors": [
        "Daniel Rim",
        "Minsoo Cho",
        "Changwoo Chun",
        "Jaegul Choo"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/3378e381acdf8d8536bc9687e1b4183de1891487",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20767",
      "title": "CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models",
      "authors": [
        "Xiaqiang Tang",
        "Jian Li",
        "Ke-Bang Hu",
        "Du Nan",
        "Xiaolong Li",
        "Xi Zhang",
        "Weigao Sun",
        "Sihong Xie"
      ],
      "abstract": "Faithfulness hallucinations are claims generated by a Large Language Model (LLM) not supported by contexts provided to the LLM. Lacking assessment standards, existing benchmarks focus on\"factual statements\"that rephrase source materials while overlooking\"cognitive statements\"that involve making inferences from the given context. Consequently, evaluating and detecting the hallucination of cognitive statements remains challenging. Inspired by how evidence is assessed in the legal domain, we design a rigorous framework to assess different levels of faithfulness of cognitive statements and introduce the CogniBench dataset where we reveal insightful statistics. To keep pace with rapidly evolving LLMs, we further develop an automatic annotation pipeline that scales easily across different models. This results in a large-scale CogniBench-L dataset, which facilitates training accurate detectors for both factual and cognitive hallucinations. We release our model and datasets at: https://github.com/FUTUREEEEEE/CogniBench",
      "arxiv_url": "https://arxiv.org/abs/2505.20767",
      "pdf_url": "https://arxiv.org/pdf/2505.20767",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.00928",
      "title": "Taxonomizing Representational Harms using Speech Act Theory",
      "authors": [
        "Emily Corvi",
        "Hannah Washington",
        "Stefanie Reed",
        "Chad Atalla",
        "Alexandra Chouldechova",
        "P. A. Dow",
        "J. Garcia-Gathright",
        "Nicholas Pangakis",
        "Emily Sheng",
        "Dan Vann",
        "Matthew Vogel",
        "Hanna M. Wallach"
      ],
      "abstract": "Representational harms are widely recognized among fairness-related harms caused by generative language systems. However, their definitions are commonly under-specified. We make a theoretical contribution to the specification of representational harms by introducing a framework, grounded in speech act theory (Austin, 1962), that conceptualizes representational harms caused by generative language systems as the perlocutionary effects (i.e., real-world impacts) of particular types of illocutionary acts (i.e., system behaviors). Building on this argument and drawing on relevant literature from linguistic anthropology and sociolinguistics, we provide new definitions of stereotyping, demeaning, and erasure. We then use our framework to develop a granular taxonomy of illocutionary acts that cause representational harms, going beyond the high-level taxonomies presented in previous work. We also discuss the ways that our framework and taxonomy can support the development of valid measurement instruments. Finally, we demonstrate the utility of our framework and taxonomy via a case study that engages with recent conceptual debates about what constitutes a representational harm and how such harms should be measured.",
      "arxiv_url": "https://arxiv.org/abs/2504.00928",
      "pdf_url": "https://arxiv.org/pdf/2504.00928",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14116",
      "title": "Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst",
      "authors": [
        "Hongru Wang",
        "Deng Cai",
        "Wanjun Zhong",
        "Shijue Huang",
        "Jeff Z. Pan",
        "Zeming Liu",
        "Kam-Fai Wong"
      ],
      "abstract": "Inference-time scaling has attracted much attention which significantly enhance the performance of Large Language Models (LLMs) in complex reasoning tasks by increasing the length of Chain-of-Thought. These longer intermediate reasoning rationales embody various meta-reasoning skills in human cognition, such as reflection and decomposition, being difficult to create and acquire. In this work, we introduce \\textit{Self-Reasoning Language Model} (SRLM), where the model itself can synthesize longer CoT data and iteratively improve performance through self-training. By incorporating a few demonstration examples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from existing responses, which act as a reasoning catalyst, we demonstrate that SRLM not only enhances the model's initial performance but also ensures more stable and consistent improvements in subsequent iterations. Our proposed SRLM achieves an average absolute improvement of more than $+2.5$ points across five reasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models. Moreover, it brings more improvements with more times of sampling during inference, such as absolute $+7.89$ average improvement with $64$ sampling times, revealing the in-depth, diverse and creative reasoning paths in SRLM against the strong baseline.",
      "arxiv_url": "https://arxiv.org/abs/2505.14116",
      "pdf_url": "https://arxiv.org/pdf/2505.14116",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "33f897d90f6a13d38661671e8998b33c2e7d077f",
      "title": "On the Role of Semantic Proto-roles in Semantic Analysis: What do LLMs know about agency?",
      "authors": [
        "Elizabeth Spaulding",
        "Shafiuddin Rehan Ahmed",
        "James H. Martin"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/33f897d90f6a13d38661671e8998b33c2e7d077f",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10939",
      "title": "GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction",
      "authors": [
        "Mohammadtaha Bagherifard",
        "Sahar Rajabi",
        "Ali Edalat",
        "Yadollah Yaghoobzadeh"
      ],
      "abstract": "Large language models often struggle with zero-shot generalization, and several modular approaches have been proposed to address this challenge. Yet, we hypothesize that a key limitation remains: the entanglement of general knowledge and task-specific adaptations. To overcome this, we propose a modular framework that disentangles these components by constructing a library of task-specific LoRA modules alongside a general-domain LoRA. By subtracting this general knowledge component from each task-specific module, we obtain residual modules that focus more exclusively on task-relevant information, a method we call general knowledge subtraction (GenKnowSub). Leveraging the refined task-specific modules and the Arrow routing algorithm \\citep{ostapenko2024towards}, we dynamically select and combine modules for new inputs without additional training. Our studies on the Phi-3 model and standard Arrow as baselines reveal that using general knowledge LoRAs derived from diverse languages, including English, French, and German, yields consistent performance gains in both monolingual and cross-lingual settings across a wide set of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub generalizes to weaker LLMs. The complete code and data are available at https://github.com/saharsamr/Modular-LLM.",
      "arxiv_url": "https://arxiv.org/abs/2505.10939",
      "pdf_url": "https://arxiv.org/pdf/2505.10939",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "343237d6c825d4a6c36d77c4d0ef56dc440dc7c7",
      "title": "A rebuttal of two common deflationary stances against LLM cognition",
      "authors": [
        "Zak Hussain",
        "Rui Mata",
        "D. U. Wulff"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/343237d6c825d4a6c36d77c4d0ef56dc440dc7c7",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20124",
      "title": "TUNA: Comprehensive Fine-grained Temporal Understanding Evaluation on Dense Dynamic Videos",
      "authors": [
        "Fanheng Kong",
        "Jingyuan Zhang",
        "Hongzhi Zhang",
        "Shi Feng",
        "Daling Wang",
        "Linhao Yu",
        "Xingguang Ji",
        "Yu Tian",
        "Qi Wang",
        "Fuzheng Zhang",
        "Elmira Amirloo",
        "J. Fauconnier",
        "Christoph Roesmann",
        "Christian Kerl",
        "Rinu Boney",
        "Yu-Mei Qian",
        "Zirui Wang",
        "Afshin Dehghan",
        "Zhe Yinfei Yang",
        "Hidehisa Arai",
        "Keita Miwa",
        "Kento Sasaki",
        "Yu Ya-maguchi",
        "Kohei Watanabe",
        "Shunsuke Aoki",
        "Issei Yamamoto",
        "Covla",
        "Davide Caffagni",
        "Federico Cocchi",
        "Luca Barsellotti",
        "Nicholas Moratelli",
        "Sara Sarto",
        "L. Baraldi",
        "Marcella Cornia",
        "Rita Cuc-chiara",
        "Zhe Chen",
        "Weiyun Wang",
        "Haowen Tian",
        "Sheng-Tao Ye",
        "Zhangwei Gao",
        "Erfei Cui",
        "Wenwen Tong",
        "Kongzhi Hu",
        "Jiapeng Luo",
        "Zheng Ma",
        "Bo Li",
        "Yuan-hang Zhang",
        "Dong Guo",
        "Renrui Zhang",
        "Feng Li",
        "Hao Zhang",
        "Kaichen Zhang",
        "Yanwei Li",
        "Ziwei Liu",
        "Wei Li",
        "Zejun Ma",
        "Chunyuan Li",
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Kunchang Li",
        "Yali Wang",
        "Yi-Wei He",
        "Yizhuo Li",
        "Yi Wang",
        "Yi Liu",
        "Zun Wang",
        "Jilan Xu",
        "Guo Chen",
        "Ji Lin",
        "Hongxu Yin",
        "Wei Ping",
        "Pavlo Molchanov",
        "Ye Liu",
        "Zongyang Ma",
        "Zhongang Qi",
        "Yang Wu",
        "Changzheng Chen"
      ],
      "abstract": "Videos are unique in their integration of temporal elements, including camera, scene, action, and attribute, along with their dynamic relationships over time. However, existing benchmarks for video understanding often treat these properties separately or narrowly focus on specific aspects, overlooking the holistic nature of video content. To address this, we introduce TUNA, a temporal-oriented benchmark for fine-grained understanding on dense dynamic videos, with two complementary tasks: captioning and QA. Our TUNA features diverse video scenarios and dynamics, assisted by interpretable and robust evaluation criteria. We evaluate several leading models on our benchmark, providing fine-grained performance assessments across various dimensions. This evaluation reveals key challenges in video temporal understanding, such as limited action description, inadequate multi-subject understanding, and insensitivity to camera motion, offering valuable insights for improving video understanding models. The data and code are available at https://friedrichor.github.io/projects/TUNA.",
      "arxiv_url": "https://arxiv.org/abs/2505.20124",
      "pdf_url": "https://arxiv.org/pdf/2505.20124",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "346bf2d5a963938de9cb9ee636773c59dffe4c89",
      "title": "Mixture of Ordered Scoring Experts for Cross-prompt Essay Trait Scoring",
      "authors": [
        "Po-Kai Chen",
        "Bo-Wei Tsai",
        "Shao-Kuan Wei",
        "Chien-Yao Wang",
        "Jia-Ching Wang",
        "Yi-Ting Huang"
      ],
      "abstract": "Automated Essay Scoring (AES) plays a crucial role in language assessment. In particular, cross-prompt essay trait scoring provides learners with valuable feedback to improve their writing skills. However, due to the scarcity of prompts, most existing methods overlook critical information, such as content from prompts or essays, resulting in incomplete assessment perspectives. In this paper, we propose a robust AES framework, the Mixture of Ordered Scoring Experts (MOOSE), which integrates information from both prompts and essays. MOOSE employs three specialized experts to evaluate (1) the overall quality of an essay, (2) the relative quality across multiple essays, and (3) the relevance between an essay and its prompt. MOOSE introduces the ordered aggregation of assessment results from these experts along with effective feature learning techniques. Experimental results demonstrate that MOOSE achieves exceptionally stable and state-of-the-art performance in both cross-prompt scoring and multi-trait scoring on the ASAP++ dataset. The source code is released at https: /",
      "arxiv_url": "https://www.semanticscholar.org/paper/346bf2d5a963938de9cb9ee636773c59dffe4c89",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12597",
      "title": "Chain-Talker: Chain Understanding and Rendering for Empathetic Conversational Speech Synthesis",
      "authors": [
        "Yifan Hu",
        "Rui Liu",
        "Yi Ren",
        "Xiang Yin",
        "Haizhou Li"
      ],
      "abstract": "Conversational Speech Synthesis (CSS) aims to align synthesized speech with the emotional and stylistic context of user-agent interactions to achieve empathy. Current generative CSS models face interpretability limitations due to insufficient emotional perception and redundant discrete speech coding. To address the above issues, we present Chain-Talker, a three-stage framework mimicking human cognition: Emotion Understanding derives context-aware emotion descriptors from dialogue history; Semantic Understanding generates compact semantic codes via serialized prediction; and Empathetic Rendering synthesizes expressive speech by integrating both components. To support emotion modeling, we develop CSS-EmCap, an LLM-driven automated pipeline for generating precise conversational speech emotion captions. Experiments on three benchmark datasets demonstrate that Chain-Talker produces more expressive and empathetic speech than existing methods, with CSS-EmCap contributing to reliable emotion modeling. The code and demos are available at: https://github.com/AI-S2-Lab/Chain-Talker.",
      "arxiv_url": "https://arxiv.org/abs/2505.12597",
      "pdf_url": "https://arxiv.org/pdf/2505.12597",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.10330",
      "title": "Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach",
      "authors": [
        "Mohammed Bouri",
        "Adnane Saoud"
      ],
      "abstract": "Despite advancements in Natural Language Processing (NLP), models remain vulnerable to adversarial attacks, such as synonym substitutions. While prior work has focused on improving robustness for feed-forward and convolutional architectures, the robustness of recurrent networks and modern state space models (SSMs), such as S4, remains understudied. These architectures pose unique challenges due to their sequential processing and complex parameter dynamics. In this paper, we introduce a novel regularization technique based on Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the impact of input perturbations on model outputs. We focus on computing the GBM for three architectures: Long Short-Term Memory (LSTM), State Space models (S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance resilience against word substitution attacks, (2) improve generalization on clean text, and (3) providing the first systematic analysis of SSM (S4) robustness. Extensive experiments across multiple architectures and benchmark datasets demonstrate that our method improves adversarial robustness by up to 8.8% over existing baselines. These results highlight the effectiveness of our approach, outperforming several state-of-the-art methods in adversarial defense. Codes are available at https://github.com/BouriMohammed/GBM",
      "arxiv_url": "https://arxiv.org/abs/2507.10330",
      "pdf_url": "https://arxiv.org/pdf/2507.10330",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.07604",
      "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
      "authors": [
        "Tianhe Lin",
        "Jian Xie",
        "Siyu Yuan",
        "Deqing Yang"
      ],
      "abstract": "Test-time compute is emerging as a new paradigm for enhancing language models' complex multi-step reasoning capabilities, as demonstrated by the success of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on unfixed-pattern data tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization.",
      "arxiv_url": "https://arxiv.org/abs/2503.07604",
      "pdf_url": "https://arxiv.org/pdf/2503.07604",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2508.12801",
      "title": "Maximum Score Routing For Mixture-of-Experts",
      "authors": [
        "Bo Dong",
        "Yilong Fan",
        "Yutao Sun",
        "Zhenyu Li",
        "Tengyu Pan",
        "Zhou Xun",
        "Jianyong Wang"
      ],
      "abstract": "Routing networks in sparsely activated mixture-of-experts (MoE) dynamically allocate input tokens to top-k experts through differentiable sparse transformations, enabling scalable model capacity while preserving computational efficiency. Traditional MoE networks impose an expert capacity constraint to ensure GPU-friendly computation. However, this leads to token dropping when capacity is saturated and results in low hardware efficiency due to padding in underutilized experts. Removing the capacity constraint, in turn, compromises load balancing and computational efficiency. To address these issues, we propose Maximum Score Routing ($\\mathbf{MaxScore}$), a novel MoE routing paradigm that models routing as a minimum-cost maximum-flow problem and integrates a SoftTopk operator. MaxScore resolves the fundamental limitations of iterative rerouting and optimal transport formulations, achieving lower training losses and higher evaluation scores at equivalent FLOPs compared to both constrained and unconstrained baselines. Implementation details and experimental configurations can be obtained from $\\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.",
      "arxiv_url": "https://arxiv.org/abs/2508.12801",
      "pdf_url": "https://arxiv.org/pdf/2508.12801",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-08-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.10408",
      "title": "Knowledge Tracing in Programming Education Integrating Students' Questions",
      "authors": [
        "Doyoun Kim",
        "Suin Kim",
        "Yojan Jo"
      ],
      "abstract": "Knowledge tracing (KT) in programming education presents unique challenges due to the complexity of coding tasks and the diverse methods students use to solve problems. Although students' questions often contain valuable signals about their understanding and misconceptions, traditional KT models often neglect to incorporate these questions as inputs to address these challenges. This paper introduces SQKT (Students' Question-based Knowledge Tracing), a knowledge tracing model that leverages students' questions and automatically extracted skill information to enhance the accuracy of predicting students' performance on subsequent problems in programming education. Our method creates semantically rich embeddings that capture not only the surface-level content of the questions but also the student's mastery level and conceptual understanding. Experimental results demonstrate SQKT's superior performance in predicting student completion across various Python programming courses of differing difficulty levels. In in-domain experiments, SQKT achieved a 33.1\\% absolute improvement in AUC compared to baseline models. The model also exhibited robust generalization capabilities in cross-domain settings, effectively addressing data scarcity issues in advanced programming courses. SQKT can be used to tailor educational content to individual learning needs and design adaptive learning systems in computer science education.",
      "arxiv_url": "https://arxiv.org/abs/2502.10408",
      "pdf_url": "https://arxiv.org/pdf/2502.10408",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3513c75b3ff93b181c9770b58856fa1e83243395",
      "title": "The Two Paradigms of LLM Detection: Authorship Attribution vs Authorship Verification",
      "authors": [
        "Janek Bevendorff",
        "Matti Wiegmann",
        "Emmelie Richter",
        "Martin Potthast",
        "Benno Stein"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/3513c75b3ff93b181c9770b58856fa1e83243395",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04226",
      "title": "Efficient Knowledge Editing via Minimal Precomputation",
      "authors": [
        "Akshat Gupta",
        "Maochuan Lu",
        "Thomas Hartvigsen",
        "G. Anumanchipalli"
      ],
      "abstract": "Knowledge editing methods like MEMIT are able to make data and compute efficient updates of factual knowledge by using a single sentence to update facts and their consequences. However, what is often overlooked is a\"precomputation step\", which requires a one-time but significant computational cost. The authors of MEMIT originally precompute approximately 44 million hidden vectors per edited layer, which requires a forward pass over 44 million tokens. For GPT-J (6B), this precomputation step takes 36 hours on a single GPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this precomputation time grows with model size. In this paper, we show that this excessive computational cost is unnecessary. Knowledge editing using MEMIT and related methods, such as ROME and EMMET, can be performed by pre-computing a very small portion of the 44 million hidden vectors. We first present the theoretical minimum number of hidden vector precomputation required for solutions of these editing methods to exist. We then empirically show that knowledge editing using these methods can be done by pre-computing significantly fewer hidden vectors. Specifically, we show that the precomputation step can be done with less than 0.3% of the originally stipulated number of hidden vectors. This saves a significant amount of precomputation time and allows users to begin editing new models within a few minutes.",
      "arxiv_url": "https://arxiv.org/abs/2506.04226",
      "pdf_url": "https://arxiv.org/pdf/2506.04226",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3530911550610db785fc1a20f357805672be1e29",
      "title": "Small Changes, Big Impact: How Manipulating a Few Neurons Can Drastically Alter LLM Aggression",
      "authors": [
        "Jaewook Lee",
        "Junseo Jang",
        "Oh-Woog Kwon",
        "Harksoo Kim"
      ],
      "abstract": "Recent remarkable advances in Large Language Models (LLMs) have led to innovations in various domains such as education, health-care, and finance, while also raising serious concerns that they can be easily misused for malicious purposes. Most previous research has focused primarily on observing how jailbreak attack techniques bypass safety mechanisms like Reinforcement Learning through Human Feedback (RLHF). However, whether there are neurons within LLMs that directly govern aggression has not been sufficiently investigated. To fill this gap, this study identifies specific neurons (“aggression neurons”) closely related to the expression of aggression and systematically analyzes how manipulating them affects the model’s overall aggression. Specifically, using a large-scale synthetic text corpus (aggressive and non-aggressive), we measure the activation frequency of each neuron, then apply masking and activation techniques to quantitatively evaluate changes in aggression by layer and by manipulation ratio. Experimental results show that, in all models, manipulating only a small number of neurons can increase aggression by up to 33%, and the effect is even more extreme when aggression neurons are concentrated in certain layers. Moreover, even models of the same scale exhibit nonlinear changes in aggression patterns, suggesting that simple external safety measures alone may not be sufficient for complete defense.",
      "arxiv_url": "https://www.semanticscholar.org/paper/3530911550610db785fc1a20f357805672be1e29",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18460",
      "title": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers",
      "authors": [
        "Xueguang Ma",
        "Xi Victoria Lin",
        "Barlas Oğuz",
        "Jimmy Lin",
        "Wen-tau Yih",
        "Xilun Chen"
      ],
      "abstract": "Large language models (LLMs) have demonstrated strong effectiveness and robustness while fine-tuned as dense retrievers. However, their large parameter size brings significant inference time computational challenges, including high encoding costs for large-scale corpora and increased query latency, limiting their practical deployment. While smaller retrievers offer better efficiency, they often fail to generalize effectively with limited supervised fine-tuning data. In this work, we introduce DRAMA, a training framework that leverages LLMs to train smaller generalizable dense retrievers. In particular, we adopt pruned LLMs as the backbone and train on diverse LLM-augmented data in a single-stage contrastive learning setup. Experiments show that DRAMA offers better multilingual and long-context capabilities than traditional encoder-based retrievers, and achieves strong performance across multiple tasks and languages. These highlight the potential of connecting the training of smaller retrievers with the growing advancements in LLMs, bridging the gap between efficiency and generalization.",
      "arxiv_url": "https://arxiv.org/abs/2502.18460",
      "pdf_url": "https://arxiv.org/pdf/2502.18460",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2510.25817",
      "title": "A Survey on Efficient Large Language Model Training: From Data-centric Perspectives",
      "authors": [
        "Junyu Luo",
        "Bohan Wu",
        "Xiao Luo",
        "Zhiping Xiao",
        "Yiqiao Jin",
        "Rong-Cheng Tu",
        "Nan Yin",
        "Yifan Wang",
        "Jingyang Yuan",
        "Wei Ju",
        "Ming Zhang"
      ],
      "abstract": "Post-training of Large Language Models (LLMs) is crucial for unlocking their task generalization potential and domain-specific capabilities. However, the current LLM post-training paradigm faces significant data challenges, including the high costs of manual annotation and diminishing marginal returns on data scales. Therefore, achieving data-efficient post-training has become a key research question. In this paper, we present the first systematic survey of data-efficient LLM post-training from a data-centric perspective. We propose a taxonomy of data-efficient LLM post-training methods, covering data selection, data quality enhancement, synthetic data generation, data distillation and compression, and self-evolving data ecosystems. We summarize representative approaches in each category and outline future research directions. By examining the challenges in data-efficient LLM post-training, we highlight open problems and propose potential research avenues. We hope our work inspires further exploration into maximizing the potential of data utilization in large-scale model training. Paper List: https://github.com/luo-junyu/Awesome-Data-Efficient-LLM",
      "arxiv_url": "https://arxiv.org/abs/2510.25817",
      "pdf_url": "https://arxiv.org/pdf/2510.25817",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-10-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23820",
      "title": "Arbiters of Ambivalence: Challenges of Using LLMs in No-Consensus Tasks",
      "authors": [
        "Bhaktipriya Radharapu",
        "Manon Revel",
        "Megan Ung",
        "Sebastian Ruder",
        "Adina Williams"
      ],
      "abstract": "The increasing use of LLMs as substitutes for humans in ``aligning'' LLMs has raised questions about their ability to replicate human judgments and preferences, especially in ambivalent scenarios where humans disagree. This study examines the biases and limitations of LLMs in three roles: answer generator, judge, and debater. These roles loosely correspond to previously described alignment frameworks: preference alignment (judge) and scalable oversight (debater), with the answer generator reflecting the typical setting with user interactions. We develop a ``no-consensus'' benchmark by curating examples that encompass a variety of a priori ambivalent scenarios, each presenting two possible stances. Our results show that while LLMs can provide nuanced assessments when generating open-ended answers, they tend to take a stance on no-consensus topics when employed as judges or debaters. These findings underscore the necessity for more sophisticated methods for aligning LLMs without human oversight, highlighting that LLMs cannot fully capture human disagreement even on topics where humans themselves are divided.",
      "arxiv_url": "https://arxiv.org/abs/2505.23820",
      "pdf_url": "https://arxiv.org/pdf/2505.23820",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16529",
      "title": "Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual Program Generation",
      "authors": [
        "Deokhyung Kang",
        "Jeonghun Cho",
        "Yejin Jeon",
        "Sunbin Jang",
        "Minsub Lee",
        "Jawoon Cho",
        "G. Lee"
      ],
      "abstract": "Visual programming languages (VPLs) allow users to create programs through graphical interfaces, which results in easier accessibility and their widespread usage in various domains. To further enhance this accessibility, recent research has focused on generating VPL code from user instructions using large language models (LLMs). Specifically, by employing prompting-based methods, these studies have shown promising results. Nevertheless, such approaches can be less effective for industrial VPLs such as Ladder Diagram (LD). LD is a pivotal language used in industrial automation processes and involves extensive domain-specific configurations, which are difficult to capture in a single prompt. In this work, we demonstrate that training-based methods outperform prompting-based methods for LD generation accuracy, even with smaller backbone models. Building on these findings, we propose a two-stage training strategy to further enhance VPL generation. First, we employ retrieval-augmented fine-tuning to leverage the repetitive use of subroutines commonly seen in industrial VPLs. Second, we apply direct preference optimization (DPO) to further guide the model toward accurate outputs, using systematically generated preference pairs through graph editing operations. Extensive experiments on real-world LD data demonstrate that our approach improves program-level accuracy by over 10% compared to supervised fine-tuning, which highlights its potential to advance industrial automation.",
      "arxiv_url": "https://arxiv.org/abs/2502.16529",
      "pdf_url": "https://arxiv.org/pdf/2502.16529",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14759",
      "title": "LEANCODE: Understanding Models Better for Code Simplification of Pre-trained Large Language Models",
      "authors": [
        "Yan Wang",
        "Ling Ding",
        "Tien Nguyen",
        "Shaohuai Wang",
        "Yanan Zheng"
      ],
      "abstract": "Large Language Models for code often entail significant computational complexity, which grows significantly with the length of the input code sequence. We propose LeanCode for code simplification to reduce training and prediction time, leveraging code contexts in utilizing attention scores to represent the tokens'importance. We advocate for the selective removal of tokens based on the average context-aware attention scores rather than average scores across all inputs. LeanCode uses the attention scores of `CLS'tokens within the encoder for classification tasks, such as code search. It also employs the encoder-decoder attention scores to determine token significance for sequence-to-sequence tasks like code summarization. Our evaluation shows LeanCode's superiority over the SOTAs DietCode and Slimcode, with improvements of 60% and 16% for code search, and 29% and 27% for code summarization, respectively.",
      "arxiv_url": "https://arxiv.org/abs/2505.14759",
      "pdf_url": "https://arxiv.org/pdf/2505.14759",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.09338",
      "title": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs",
      "authors": [
        "Jingcheng Niu",
        "Xingdi Yuan",
        "Tong Wang",
        "H. Saghir",
        "Amir H. Abdi"
      ],
      "abstract": "We observe a novel phenomenon, contextual entrainment, across a wide range of language models (LMs) and prompt settings, providing a new mechanistic perspective on how LMs become distracted by ``irrelevant''contextual information in the input prompt. Specifically, LMs assign significantly higher logits (or probabilities) to any tokens that have previously appeared in the context prompt, even for random tokens. This suggests that contextual entrainment is a mechanistic phenomenon, occurring independently of the relevance or semantic relation of the tokens to the question or the rest of the sentence. We find statistically significant evidence that the magnitude of contextual entrainment is influenced by semantic factors. Counterfactual prompts have a greater effect compared to factual ones, suggesting that while contextual entrainment is a mechanistic phenomenon, it is modulated by semantic factors. We hypothesise that there is a circuit of attention heads -- the entrainment heads -- that corresponds to the contextual entrainment phenomenon. Using a novel entrainment head discovery method based on differentiable masking, we identify these heads across various settings. When we ``turn off''these heads, i.e., set their outputs to zero, the effect of contextual entrainment is significantly attenuated, causing the model to generate output that capitulates to what it would produce if no distracting context were provided. Our discovery of contextual entrainment, along with our investigation into LM distraction via the entrainment heads, marks a key step towards the mechanistic analysis and mitigation of the distraction problem.",
      "arxiv_url": "https://arxiv.org/abs/2505.09338",
      "pdf_url": "https://arxiv.org/pdf/2505.09338",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.03916",
      "title": "Dolphin: Moving Towards Closed-loop Auto-research through Thinking, Practice, and Feedback",
      "authors": [
        "Jiakang Yuan",
        "Xiangchao Yan",
        "Botian Shi",
        "Tao Chen",
        "Wanli Ouyang",
        "Bo Zhang",
        "Lei Bai",
        "Yu Qiao",
        "Bowen Zhou"
      ],
      "abstract": "The scientific research paradigm is undergoing a profound transformation owing to the development of Artificial Intelligence (AI). Recent works demonstrate that various AI-assisted research methods can largely improve research efficiency by improving data analysis, accelerating computation, and fostering novel idea generation. To further move towards the ultimate goal (i.e., automatic scientific research), in this paper, we introduce Dolphin, a closed-loop LLM-driven framework to enhance the automation level of scientific research. Dolphin first generates novel ideas based on feedback from previous experiments and relevant papers ranked by the topic and task attributes. Then, the generated ideas can be implemented using a code template refined and debugged with the designed exception-traceback-guided local code structure. Finally, Dolphin automatically analyzes the results of each idea and feeds the results back to the next round of idea generation. Experiments are conducted on the benchmark datasets of different topics and a subset of MLE-bench. Results show that Dolphin can continuously improve the performance of the input topic in a loop. We highlight that Dolphin can automatically propose methods that are comparable to the state-of-the-art in some tasks such as 3D point classification.",
      "arxiv_url": "https://arxiv.org/abs/2501.03916",
      "pdf_url": "https://arxiv.org/pdf/2501.03916",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21608",
      "title": "How does Misinformation Affect Large Language Model Behaviors and Preferences?",
      "authors": [
        "Miao Peng",
        "Nuo Chen",
        "Jianheng Tang",
        "Jia Li"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in knowledge-intensive tasks, while they remain vulnerable when encountering misinformation. Existing studies have explored the role of LLMs in combating misinformation, but there is still a lack of fine-grained analysis on the specific aspects and extent to which LLMs are influenced by misinformation. To bridge this gap, we present MisBench, the current largest and most comprehensive benchmark for evaluating LLMs' behavior and knowledge preference toward misinformation. MisBench consists of 10,346,712 pieces of misinformation, which uniquely considers both knowledge-based conflicts and stylistic variations in misinformation. Empirical results reveal that while LLMs demonstrate comparable abilities in discerning misinformation, they still remain susceptible to knowledge conflicts and stylistic variations. Based on these findings, we further propose a novel approach called Reconstruct to Discriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our study provides valuable insights into LLMs' interactions with misinformation, and we believe MisBench can serve as an effective benchmark for evaluating LLM-based detectors and enhancing their reliability in real-world applications. Codes and data are available at https://github.com/GKNL/MisBench.",
      "arxiv_url": "https://arxiv.org/abs/2505.21608",
      "pdf_url": "https://arxiv.org/pdf/2505.21608",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15920",
      "title": "Self-Taught Agentic Long Context Understanding",
      "authors": [
        "Yufan Zhuang",
        "Xiaodong Yu",
        "Jialian Wu",
        "Ximeng Sun",
        "Ze Wang",
        "Jiang Liu",
        "Yusheng Su",
        "Jingbo Shang",
        "Zicheng Liu",
        "E. Barsoum"
      ],
      "abstract": "Answering complex, long-context questions remains a major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a framework designed to enhance an LLM's understanding of such queries by integrating targeted self-clarification with contextual grounding within an agentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC), where models refine their understanding through self-generated clarification questions and corresponding contextual groundings. By scaling inference as a tree search where each node represents a CoC step, we achieve 97.8% answer recall on NarrativeQA with a search depth of up to three and a branching factor of eight. To amortize the high cost of this search process to training, we leverage the preference pairs for each step obtained by the CoC workflow and perform two-stage model finetuning: (1) supervised finetuning to learn effective decomposition strategies, and (2) direct preference optimization to enhance reasoning quality. This enables AgenticLU models to generate clarifications and retrieve relevant context effectively and efficiently in a single inference pass. Extensive experiments across seven long-context tasks demonstrate that AgenticLU significantly outperforms state-of-the-art prompting methods and specialized long-context LLMs, achieving robust multi-hop reasoning while sustaining consistent performance as context length grows.",
      "arxiv_url": "https://arxiv.org/abs/2502.15920",
      "pdf_url": "https://arxiv.org/pdf/2502.15920",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.17755",
      "title": "Improving Preference Extraction In LLMs By Identifying Latent Knowledge Through Classifying Probes",
      "authors": [
        "Sharan Maiya",
        "Yinhong Liu",
        "Ramit Debnath",
        "Anna Korhonen"
      ],
      "abstract": "Large Language Models (LLMs) are often used as automated judges to evaluate text, but their effectiveness can be hindered by various unintentional biases. We propose using linear classifying probes, trained by leveraging differences between contrasting pairs of prompts, to directly access LLMs' latent knowledge and extract more accurate preferences. Through extensive experiments using models of varying size from four different families and six diverse datasets assessing text quality evaluation and common sense reasoning, we demonstrate that both supervised and unsupervised probing approaches consistently outperform traditional generation-based judgement while maintaining similar computational costs. These probes generalise under domain shifts and can even outperform finetuned evaluators with the same training data size. Our results suggest linear probing offers an accurate, robust and computationally efficient approach for LLM-as-judge tasks while providing interpretable insights into how models encode judgement-relevant knowledge. Our data and code will be openly released in the future.",
      "arxiv_url": "https://arxiv.org/abs/2503.17755",
      "pdf_url": "https://arxiv.org/pdf/2503.17755",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13172",
      "title": "Unveiling Privacy Risks in LLM Agent Memory",
      "authors": [
        "Bo Wang",
        "Weiyi He",
        "Pengfei He",
        "Shenglai Zeng",
        "Zhen Xiang",
        "Yue Xing",
        "Jiliang Tang"
      ],
      "abstract": "Large Language Model (LLM) agents have become increasingly prevalent across various real-world applications. They enhance decision-making by storing private user-agent interactions in the memory module for demonstrations, introducing new privacy risks for LLM agents. In this work, we systematically investigate the vulnerability of LLM agents to our proposed Memory EXTRaction Attack (MEXTRA) under a black-box setting. To extract private information from memory, we propose an effective attacking prompt design and an automated prompt generation method based on different levels of knowledge about the LLM agent. Experiments on two representative agents demonstrate the effectiveness of MEXTRA. Moreover, we explore key factors influencing memory leakage from both the agent designer's and the attacker's perspectives. Our findings highlight the urgent need for effective memory safeguards in LLM agent design and deployment.",
      "arxiv_url": "https://arxiv.org/abs/2502.13172",
      "pdf_url": "https://arxiv.org/pdf/2502.13172",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.21096",
      "title": "DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning",
      "authors": [
        "Kang He",
        "Yuzhe Ding",
        "Haining Wang",
        "Fei Li",
        "Chong Teng",
        "Dong-Hong Ji"
      ],
      "abstract": "Previous multimodal sentence representation learning methods have achieved impressive performance. However, most approaches focus on aligning images and text at a coarse level, facing two critical challenges:cross-modal misalignment bias and intra-modal semantic divergence, which significantly degrade sentence representation quality. To address these challenges, we propose DALR (Dual-level Alignment Learning for Multimodal Sentence Representation). For cross-modal alignment, we propose a consistency learning module that softens negative samples and utilizes semantic similarity from an auxiliary task to achieve fine-grained cross-modal alignment. Additionally, we contend that sentence relationships go beyond binary positive-negative labels, exhibiting a more intricate ranking structure. To better capture these relationships and enhance representation quality, we integrate ranking distillation with global intra-modal alignment learning. Comprehensive experiments on semantic textual similarity (STS) and transfer (TR) tasks validate the effectiveness of our approach, consistently demonstrating its superiority over state-of-the-art baselines.",
      "arxiv_url": "https://arxiv.org/abs/2506.21096",
      "pdf_url": "https://arxiv.org/pdf/2506.21096",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.02302",
      "title": "DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning",
      "authors": [
        "Dohoon Kim",
        "Donghun Kang",
        "Taesup Moon"
      ],
      "abstract": "Domain-Adaptive Pre-training (DAP) has recently gained attention for its effectiveness in fine-tuning pre-trained models. Building on this, continual DAP has been explored to develop pre-trained models capable of incrementally incorporating different domain datasets. However, existing continual DAP methods face several limitations: (1) high computational cost and GPU memory usage during training; (2) sensitivity to incremental data order; and (3) providing a single, generalized model for all end tasks, which contradicts the essence of DAP. In this paper, we propose DoMIX, a novel approach that addresses these challenges by leveraging LoRA modules, a representative parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient and parallel domain-adaptive pre-training that is robust to domain order and effectively utilizes accumulated knowledge to provide tailored pre-trained models for specific tasks. We also demonstrate that our method can be extended beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available at https://github.com/dohoonkim-ai/DoMIX.",
      "arxiv_url": "https://arxiv.org/abs/2507.02302",
      "pdf_url": "https://arxiv.org/pdf/2507.02302",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.06233",
      "title": "Confidence Improves Self-Consistency in LLMs",
      "authors": [
        "Amir Taubenfeld",
        "Tom Sheffer",
        "E. Ofek",
        "Amir Feder",
        "Ariel Goldstein",
        "Zorik Gekhman",
        "G. Yona"
      ],
      "abstract": "Self-consistency decoding enhances LLMs'performance on reasoning tasks by sampling diverse reasoning paths and selecting the most frequent answer. However, it is computationally expensive, as sampling many of these (lengthy) paths is required to increase the chances that the correct answer emerges as the most frequent one. To address this, we introduce Confidence-Informed Self-Consistency (CISC). CISC performs a weighted majority vote based on confidence scores obtained directly from the model. By prioritizing high-confidence paths, it can identify the correct answer with a significantly smaller sample size. When tested on nine models and four datasets, CISC outperforms self-consistency in nearly all configurations, reducing the required number of reasoning paths by over 40% on average. In addition, we introduce the notion of within-question confidence evaluation, after showing that standard evaluation methods are poor predictors of success in distinguishing correct and incorrect answers to the same question. In fact, the most calibrated confidence method proved to be the least effective for CISC. Lastly, beyond these practical implications, our results and analyses show that LLMs can effectively judge the correctness of their own outputs, contributing to the ongoing debate on this topic.",
      "arxiv_url": "https://arxiv.org/abs/2502.06233",
      "pdf_url": "https://arxiv.org/pdf/2502.06233",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16869",
      "title": "MPO: Multilingual Safety Alignment via Reward Gap Optimization",
      "authors": [
        "Weixiang Zhao",
        "Yulin Hu",
        "Yang Deng",
        "Tong Wu",
        "Wenxuan Zhang",
        "Jiahe Guo",
        "An Zhang",
        "Yanyan Zhao",
        "Bing Qin",
        "Tat-Seng Chua",
        "Ting Liu"
      ],
      "abstract": "Large language models (LLMs) have become increasingly central to AI applications worldwide, necessitating robust multilingual safety alignment to ensure secure deployment across diverse linguistic contexts. Existing preference learning methods for safety alignment, such as RLHF and DPO, are primarily monolingual and struggle with noisy multilingual data. To address these limitations, we introduce Multilingual reward gaP Optimization (MPO), a novel approach that leverages the well-aligned safety capabilities of the dominant language (English) to improve safety alignment across multiple languages. MPO directly minimizes the reward gap difference between the dominant language and target languages, effectively transferring safety capabilities while preserving the original strengths of the dominant language. Extensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate MPO's efficacy in multilingual safety alignment without degrading general multilingual utility.",
      "arxiv_url": "https://arxiv.org/abs/2505.16869",
      "pdf_url": "https://arxiv.org/pdf/2505.16869",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.02769",
      "title": "InSerter: Speech Instruction Following with Unsupervised Interleaved Pre-training",
      "authors": [
        "Dingdong Wang",
        "Jin Xu",
        "Ruihang Chu",
        "Zhifang Guo",
        "Xiong Wang",
        "Jincenzi Wu",
        "Dongchao Yang",
        "Shengpeng Ji",
        "Junyang Lin"
      ],
      "abstract": "Recent advancements in speech large language models (SpeechLLMs) have attracted considerable attention. Nonetheless, current methods exhibit suboptimal performance in adhering to speech instructions. Notably, the intelligence of models significantly diminishes when processing speech-form input as compared to direct text-form input. Prior work has attempted to mitigate this semantic inconsistency between speech and text representations through techniques such as representation and behavior alignment, which involve the meticulous design of data pairs during the post-training phase. In this paper, we introduce a simple and scalable training method called InSerter, which stands for Interleaved Speech-Text Representation Pre-training. InSerter is designed to pre-train large-scale unsupervised speech-text sequences, where the speech is synthesized from randomly selected segments of an extensive text corpus using text-to-speech conversion. Consequently, the model acquires the ability to generate textual continuations corresponding to the provided speech segments, obviating the need for intensive data design endeavors. To systematically evaluate speech instruction-following capabilities, we introduce SpeechInstructBench, the first comprehensive benchmark specifically designed for speech-oriented instruction-following tasks. Our proposed InSerter achieves SOTA performance in SpeechInstructBench and demonstrates superior or competitive results across diverse speech processing tasks.",
      "arxiv_url": "https://arxiv.org/abs/2503.02769",
      "pdf_url": "https://arxiv.org/pdf/2503.02769",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.03928",
      "title": "CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate",
      "authors": [
        "Yiliu Sun",
        "Zicheng Zhao",
        "Sheng Wan",
        "Chen Gong"
      ],
      "abstract": "Nowadays, single Large Language Model (LLM) struggles with critical issues such as hallucination and inadequate reasoning abilities. To mitigate these issues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where LLM agents engage in in-depth debates with others on tasks. However, existing MAD methods face two major issues: (a) too lengthy input contexts, which causes LLM agents to get lost in plenty of input information and experiences performance drop; and (b) the overconfidence dilemma, where self-assured LLM agents dominate the debate, leading to low debating effectiveness. To address these limitations, we propose a novel MAD method called\"CortexDebate\". Inspired by the human brain's tendency to establish a sparse and dynamically optimized network among cortical areas governed by white matter, CortexDebate constructs a sparse debating graph among LLM agents, where each LLM agent only debates with the ones that are helpful to it. To optimize the graph, we propose a module named McKinsey-based Debate Matter (MDM), which acts as an artificial analog to white matter. By integrating the McKinsey Trust Formula, a well-established measure of trustworthiness from sociology, MDM enables credible evaluations that guide graph optimization. The effectiveness of our CortexDebate has been well demonstrated by extensive experimental results across eight datasets from four task types.",
      "arxiv_url": "https://arxiv.org/abs/2507.03928",
      "pdf_url": "https://arxiv.org/pdf/2507.03928",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.10973",
      "title": "Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for Emotion Recognition in Movie Dialogues",
      "authors": [
        "David Sasu",
        "Zehui Wu",
        "Ziwei Gong",
        "Run Chen",
        "Pengyuan Shi",
        "Lin Ai",
        "Julia Hirschberg",
        "Natalie Schluter"
      ],
      "abstract": "In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the first multimodal emotion dialogue dataset for an African language, addressing the significant lack of resources for low-resource languages in emotion recognition research. ACE, developed for the Akan language, contains 385 emotion-labeled dialogues and 6,162 utterances across audio, visual, and textual modalities, along with word-level prosodic prominence annotations. The presence of prosodic labels in this dataset also makes it the first prosodically annotated African language dataset. We demonstrate the quality and utility of ACE through experiments using state-of-the-art emotion recognition methods, establishing solid baselines for future research. We hope ACE inspires further work on inclusive, linguistically and culturally diverse NLP resources.",
      "arxiv_url": "https://arxiv.org/abs/2502.10973",
      "pdf_url": "https://arxiv.org/pdf/2502.10973",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3748edd97f2bc54750c83782bb74d11c1c4ea556",
      "title": "A Multi-persona Framework for Argument Quality Assessment",
      "authors": [
        "Bojun Jin",
        "Jianzhu Bao",
        "Yufang Hou",
        "Yang Sun",
        "Yice Zhang",
        "Huajie Wang",
        "Bin Liang",
        "Ruifeng Xu"
      ],
      "abstract": "Argument quality assessment faces inherent challenges due to its subjective nature, where different evaluators may assign varying quality scores for an argument based on personal perspectives. Although existing datasets collect opinions from multiple annotators to model subjectivity, most existing computational meth-ods fail to consider multi-perspective evaluation. To address this issue, we propose MPAQ, a multi-persona framework for argument quality assessment that simulates diverse evaluator perspectives through large language models. It first dynamically generates targeted personas tailored to an input argument, then simulates each persona’s reasoning process to evaluate the argument quality from multiple perspectives. To effectively generate fine-grained quality scores, we develop a coarse-to-fine scoring strategy that first generates a coarse-grained integer score and then refines it into a fine-grained decimal score. Experiments on IBM-Rank-30k and IBM-ArgQ-5.3kArgs datasets demonstrate that MPAQ consistently outperforms strong baselines while providing comprehensive multi-perspective rationales.",
      "arxiv_url": "https://www.semanticscholar.org/paper/3748edd97f2bc54750c83782bb74d11c1c4ea556",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05610",
      "title": "Mitigating Confounding in Speech-Based Dementia Detection through Weight Masking",
      "authors": [
        "Zhecheng Sheng",
        "Xiruo Ding",
        "Brian Hur",
        "Changye Li",
        "Trevor Cohen",
        "Serguei V. S. Pakhomov"
      ],
      "abstract": "Deep transformer models have been used to detect linguistic anomalies in patient transcripts for early Alzheimer's disease (AD) screening. While pre-trained neural language models (LMs) fine-tuned on AD transcripts perform well, little research has explored the effects of the gender of the speakers represented by these transcripts. This work addresses gender confounding in dementia detection and proposes two methods: the $\\textit{Extended Confounding Filter}$ and the $\\textit{Dual Filter}$, which isolate and ablate weights associated with gender. We evaluate these methods on dementia datasets with first-person narratives from patients with cognitive impairment and healthy controls. Our results show transformer models tend to overfit to training data distributions. Disrupting gender-related weights results in a deconfounded dementia classifier, with the trade-off of slightly reduced dementia detection performance.",
      "arxiv_url": "https://arxiv.org/abs/2506.05610",
      "pdf_url": "https://arxiv.org/pdf/2506.05610",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.08826",
      "title": "Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation",
      "authors": [
        "Mohammad Mahdi Abootorabi",
        "Amirhosein Zobeiri",
        "Mahdi Dehghani",
        "Mohammadali Mohammadkhani",
        "Bardia Mohammadi",
        "Omid Ghahroodi",
        "M. Baghshah",
        "Ehsaneddin Asgari"
      ],
      "abstract": "Large Language Models (LLMs) suffer from hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information for improved factual grounding. With advances in multimodal learning, Multimodal RAG extends this approach by incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges beyond those in unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, benchmarks, metrics, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We review training strategies, robustness enhancements, loss functions, and agent-based approaches, while also exploring the diverse Multimodal RAG scenarios. In addition, we outline open challenges and future directions to guide research in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. All resources are publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.",
      "arxiv_url": "https://arxiv.org/abs/2502.08826",
      "pdf_url": "https://arxiv.org/pdf/2502.08826",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Multi-Modal RAG",
        "RAG"
      ],
      "published_date": "2025-02-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "37cae2a66a8c62c7bfb9c5ed2f87a0092c786b09",
      "title": "Inverse Reinforcement Learning Meets Large Language Model Alignment",
      "authors": [
        "M. Schaar",
        "Hao Sun"
      ],
      "abstract": "In the era of Large Language Models (LLMs), alignment has emerged as a fundamental yet challenging problem in the pursuit of more reliable, controllable, and capable machine intelligence. The recent success of reasoning models and conversational AI systems has underscored the critical role of reinforcement learning (RL) in enhancing these systems, driving increased research interest at the intersection of RL and LLM alignment. This tutorial will provide a comprehensive review of recent advances in LLM alignment through the lens of inverse reinforcement learning (IRL), emphasizing the distinctions between RL techniques employed in LLM alignment and those in conventional RL tasks. In particular, we highlight the necessity of constructing neural reward models from human data and discuss the formal and practical implications of this paradigm shift. The tutorial will begin with fundamental concepts in RL to provide a foundation for the audience unfamiliar with the ﬁeld. We then examine recent advances in this research agenda, discussing key challenges and opportunities in conducting IRL for LLM alignment. Beyond methodological considerations, we explore practical aspects, including datasets, benchmarks, evaluation metrics, infrastructure, and computationally eﬃcient training and inference techniques. Finally, we draw insights from the literature on sparse-reward RL to identify open questions and potential research directions. By synthesizing ﬁndings from diverse studies, we aim to provide a structured and critical overview of the ﬁeld, highlight unresolved challenges, and outline promising future directions for improving LLM alignment through RL and IRL techniques.",
      "arxiv_url": "https://www.semanticscholar.org/paper/37cae2a66a8c62c7bfb9c5ed2f87a0092c786b09",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13497",
      "title": "Towards Geo-Culturally Grounded LLM Generations",
      "authors": [
        "Piyawat Lertvittayakumjorn",
        "David Kinney",
        "Vinodkumar Prabhakaran",
        "Donald Martin",
        "Sunipa Dev"
      ],
      "abstract": "Generative large language models (LLMs) have demonstrated gaps in diverse cultural awareness across the globe. We investigate the effect of retrieval augmented generation and search-grounding techniques on LLMs'ability to display familiarity with various national cultures. Specifically, we compare the performance of standard LLMs, LLMs augmented with retrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a web search (i.e., search grounding) on multiple cultural awareness benchmarks. We find that search grounding significantly improves the LLM performance on multiple-choice benchmarks that test propositional knowledge (e.g., cultural norms, artifacts, and institutions), while KB grounding's effectiveness is limited by inadequate knowledge base coverage and a suboptimal retriever. However, search grounding also increases the risk of stereotypical judgments by language models and fails to improve evaluators'judgments of cultural familiarity in a human evaluation with adequate statistical power. These results highlight the distinction between propositional cultural knowledge and open-ended cultural fluency when it comes to evaluating LLMs'cultural awareness.",
      "arxiv_url": "https://arxiv.org/abs/2502.13497",
      "pdf_url": "https://arxiv.org/pdf/2502.13497",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03434",
      "title": "Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models",
      "authors": [
        "Ahmad Dawar Hakimi",
        "Ali Modarressi",
        "Philipp Wicke",
        "Hinrich Schutze"
      ],
      "abstract": "Understanding how large language models (LLMs) acquire and store factual knowledge is crucial for enhancing their interpretability and reliability. In this work, we analyze the evolution of factual knowledge representation in the OLMo-7B model by tracking the roles of its attention heads and feed forward networks (FFNs) over the course of pre-training. We classify these components into four roles: general, entity, relation-answer, and fact-answer specific, and examine their stability and transitions. Our results show that LLMs initially depend on broad, general-purpose components, which later specialize as training progresses. Once the model reliably predicts answers, some components are repurposed, suggesting an adaptive learning process. Notably, attention heads display the highest turnover. We also present evidence that FFNs remain more stable throughout training. Furthermore, our probing experiments reveal that location-based relations converge to high accuracy earlier in training than name-based relations, highlighting how task complexity shapes acquisition dynamics. These insights offer a mechanistic view of knowledge formation in LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.03434",
      "pdf_url": "https://arxiv.org/pdf/2506.03434",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2509.11698",
      "title": "CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model",
      "authors": [
        "Wei-Hsin Yeh",
        "Yu-An Su",
        "Chih-Ning Chen",
        "Yi-Hsueh Lin",
        "Calvin Ku",
        "Wenhsin Chiu",
        "Min-Chun Hu",
        "Lun-Wei Ku"
      ],
      "abstract": "Motion instruction is a crucial task that helps athletes refine their technique by analyzing movements and providing corrective guidance. Although recent advances in multimodal models have improved motion understanding, generating precise and sport-specific instruction remains challenging due to the highly domain-specific nature of sports and the need for informative guidance. We propose CoachMe, a reference-based model that analyzes the differences between a learner's motion and a reference under temporal and physical aspects. This approach enables both domain-knowledge learning and the acquisition of a coach-like thinking process that identifies movement errors effectively and provides feedback to explain how to improve. In this paper, we illustrate how CoachMe adapts well to specific sports such as skating and boxing by learning from general movements and then leveraging limited data. Experiments show that CoachMe provides high-quality instructions instead of directions merely in the tone of a coach but without critical information. CoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on boxing. Analysis further confirms that it elaborates on errors and their corresponding improvement methods in the generated instructions. You can find CoachMe here: https://motionxperts.github.io/",
      "arxiv_url": "https://arxiv.org/abs/2509.11698",
      "pdf_url": "https://arxiv.org/pdf/2509.11698",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-09-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.20949",
      "title": "Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation",
      "authors": [
        "Chenkai Sun",
        "Denghui Zhang",
        "ChengXiang Zhai",
        "Heng Ji"
      ],
      "abstract": "Given the growing influence of language model-based agents on high-stakes societal decisions, from public policy to healthcare, ensuring their beneficial impact requires understanding the far-reaching implications of their suggestions. We propose a proof-of-concept framework that projects how model-generated advice could propagate through societal systems on a macroscopic scale over time, enabling more robust alignment. To assess the long-term safety awareness of language models, we also introduce a dataset of 100 indirect harm scenarios, testing models'ability to foresee adverse, non-obvious outcomes from seemingly harmless user prompts. Our approach achieves not only over 20% improvement on the new dataset but also an average win rate exceeding 70% against strong baselines on existing safety benchmarks (AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer agents.",
      "arxiv_url": "https://arxiv.org/abs/2506.20949",
      "pdf_url": "https://arxiv.org/pdf/2506.20949",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00277",
      "title": "Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings",
      "authors": [
        "Hans W. A. Hanley",
        "Zakir Durumeric"
      ],
      "abstract": "Contextual large language model embeddings are increasingly utilized for topic modeling and clustering. However, current methods often scale poorly, rely on opaque similarity metrics, and struggle in multilingual settings. In this work, we present a novel, scalable, interpretable, hierarchical, and multilingual approach to clustering news articles and social media data. To do this, we first train multilingual Matryoshka embeddings that can determine story similarity at varying levels of granularity based on which subset of the dimensions of the embeddings is examined. This embedding model achieves state-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson $\\rho$ = 0.816). Once trained, we develop an efficient hierarchical clustering algorithm that leverages the hierarchical nature of Matryoshka embeddings to identify unique news stories, narratives, and themes. We conclude by illustrating how our approach can identify and cluster stories, narratives, and overarching themes within real-world news datasets.",
      "arxiv_url": "https://arxiv.org/abs/2506.00277",
      "pdf_url": "https://arxiv.org/pdf/2506.00277",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.03648",
      "title": "Disentangling the Roles of Representation and Selection in Data Pruning",
      "authors": [
        "Yupei Du",
        "Yingjin Song",
        "Hugh Mee Wong",
        "Daniil Ignatev",
        "Albert Gatt",
        "Dong Nguyen"
      ],
      "abstract": "Data pruning, selecting small but impactful subsets, offers a promising way to efficiently scale NLP model training. However, existing methods often involve many different design choices, which have not been systematically studied. This limits future developments. In this work, we decompose data pruning into two key components: the data representation and the selection algorithm, and we systematically analyze their influence on the selection of instances. Our theoretical and empirical results highlight the crucial role of representations: better representations, e.g., training gradients, generally lead to a better selection of instances, regardless of the chosen selection algorithm. Furthermore, different selection algorithms excel in different settings, and none consistently outperforms the others. Moreover, the selection algorithms do not always align with their intended objectives: for example, algorithms designed for the same objective can select drastically different instances, highlighting the need for careful evaluation.",
      "arxiv_url": "https://arxiv.org/abs/2507.03648",
      "pdf_url": "https://arxiv.org/pdf/2507.03648",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.08961",
      "title": "A Fully Automated Pipeline for Conversational Discourse Annotation: Tree Scheme Generation and Labeling with Large Language Models",
      "authors": [
        "Kseniia Petukhova",
        "Ekaterina Kochmar"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have shown promise in automating discourse annotation for conversations. While manually designing tree annotation schemes significantly improves annotation quality for humans and models, their creation remains time-consuming and requires expert knowledge. We propose a fully automated pipeline that uses LLMs to construct such schemes and perform annotation. We evaluate our approach on speech functions (SFs) and the Switchboard-DAMSL (SWBD-DAMSL) taxonomies. Our experiments compare various design choices, and we show that frequency-guided decision trees, paired with an advanced LLM for annotation, can outperform previously manually designed trees and even match or surpass human annotators while significantly reducing the time required for annotation. We release all code and resultant schemes and annotations to facilitate future research on discourse annotation.",
      "arxiv_url": "https://arxiv.org/abs/2504.08961",
      "pdf_url": "https://arxiv.org/pdf/2504.08961",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04619",
      "title": "SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming User Sentiment Modeling",
      "authors": [
        "Xin Zhang",
        "Qiyu Wei",
        "Yingjie Zhu",
        "Linhai Zhang",
        "Deyu Zhou",
        "Sophia Ananiadou"
      ],
      "abstract": "User reviews on e-commerce platforms exhibit dynamic sentiment patterns driven by temporal and contextual factors. Traditional sentiment analysis methods focus on static reviews, failing to capture the evolving temporal relationship between user sentiment rating and textual content. Sentiment analysis on streaming reviews addresses this limitation by modeling and predicting the temporal evolution of user sentiments. However, it suffers from data sparsity, manifesting in temporal, spatial, and combined forms. In this paper, we introduce SynGraph, a novel framework designed to address data sparsity in sentiment analysis on streaming reviews. SynGraph alleviates data sparsity by categorizing users into mid-tail, long-tail, and extreme scenarios and incorporating LLM-augmented enhancements within a dynamic graph-based structure. Experiments on real-world datasets demonstrate its effectiveness in addressing sparsity and improving sentiment modeling in streaming reviews.",
      "arxiv_url": "https://arxiv.org/abs/2503.04619",
      "pdf_url": "https://arxiv.org/pdf/2503.04619",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03476",
      "title": "Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection",
      "authors": [
        "Chuyuan Li",
        "Raymond Li",
        "T. Field",
        "Giuseppe Carenini"
      ],
      "abstract": "Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that leads to dementia, and early intervention can greatly benefit from analyzing linguistic abnormalities. In this work, we explore the potential of Large Language Models (LLMs) as health assistants for AD diagnosis from patient-generated text using in-context learning (ICL), where tasks are defined through a few input-output examples. Empirical results reveal that conventional ICL methods, such as similarity-based selection, perform poorly for AD diagnosis, likely due to the inherent complexity of this task. To address this, we introduce Delta-KNN, a novel demonstration selection strategy that enhances ICL performance. Our method leverages a delta score to assess the relative gains of each training example, coupled with a KNN-based retriever that dynamically selects optimal\"representatives\"for a given input. Experiments on two AD detection datasets across three open-source LLMs demonstrate that Delta-KNN consistently outperforms existing ICL baselines. Notably, when using the Llama-3.1 model, our approach achieves new state-of-the-art results, surpassing even supervised classifiers.",
      "arxiv_url": "https://arxiv.org/abs/2506.03476",
      "pdf_url": "https://arxiv.org/pdf/2506.03476",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06775",
      "title": "They want to pretend not to understand: The Limits of Current LLMs in Interpreting Implicit Content of Political Discourse",
      "authors": [
        "Walter Paci",
        "Alessandro Panunzi",
        "Sandro Pezzelle"
      ],
      "abstract": "Implicit content plays a crucial role in political discourse, where speakers systematically employ pragmatic strategies such as implicatures and presuppositions to influence their audiences. Large Language Models (LLMs) have demonstrated strong performance in tasks requiring complex semantic and pragmatic understanding, highlighting their potential for detecting and explaining the meaning of implicit content. However, their ability to do this within political discourse remains largely underexplored. Leveraging, for the first time, the large IMPAQTS corpus, which comprises Italian political speeches with the annotation of manipulative implicit content, we propose methods to test the effectiveness of LLMs in this challenging problem. Through a multiple-choice task and an open-ended generation task, we demonstrate that all tested models struggle to interpret presuppositions and implicatures. We conclude that current LLMs lack the key pragmatic capabilities necessary for accurately interpreting highly implicit language, such as that found in political discourse. At the same time, we highlight promising trends and future directions for enhancing model performance. We release our data and code at https://github.com/WalterPaci/IMPAQTS-PID",
      "arxiv_url": "https://arxiv.org/abs/2506.06775",
      "pdf_url": "https://arxiv.org/pdf/2506.06775",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04167",
      "title": "The Role of Visual Modality in Multimodal Mathematical Reasoning: Challenges and Insights",
      "authors": [
        "Yufang Liu",
        "Yao Du",
        "Tao Ji",
        "Jianing Wang",
        "Yang Liu",
        "Yuanbin Wu",
        "Aimin Zhou",
        "Mengdi Zhang",
        "Xunliang Cai"
      ],
      "abstract": "Recent research has increasingly focused on multimodal mathematical reasoning, particularly emphasizing the creation of relevant datasets and benchmarks. Despite this, the role of visual information in reasoning has been underexplored. Our findings show that existing multimodal mathematical models minimally leverage visual information, and model performance remains largely unaffected by changes to or removal of images in the dataset. We attribute this to the dominance of textual information and answer options that inadvertently guide the model to correct answers. To improve evaluation methods, we introduce the HC-M3D dataset, specifically designed to require image reliance for problem-solving and to challenge models with similar, yet distinct, images that change the correct answer. In testing leading models, their failure to detect these subtle visual differences suggests limitations in current visual perception capabilities. Additionally, we observe that the common approach of improving general VQA capabilities by combining various types of image encoders does not contribute to math reasoning performance. This finding also presents a challenge to enhancing visual reliance during math reasoning. Our benchmark and code would be available at \\href{https://github.com/Yufang-Liu/visual_modality_role}{https://github.com/Yufang-Liu/visual\\_modality\\_role}.",
      "arxiv_url": "https://arxiv.org/abs/2503.04167",
      "pdf_url": "https://arxiv.org/pdf/2503.04167",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11184",
      "title": "Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs",
      "authors": [
        "Wenxuan Wang",
        "Xiaoyuan Liu",
        "Kuiyi Gao",
        "Jen-Tse Huang",
        "Youliang Yuan",
        "Pinjia He",
        "Shuai Wang",
        "Zhaopeng Tu"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have expanded the capabilities of traditional language models by enabling interaction through both text and images. However, ensuring the safety of these models remains a significant challenge, particularly in accurately identifying whether multimodal content is safe or unsafe-a capability we term safety awareness. In this paper, we introduce MMSafeAware, the first comprehensive multimodal safety awareness benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500 carefully curated image-prompt pairs. MMSafeAware includes both unsafe and over-safety subsets to assess models abilities to correctly identify unsafe content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine widely used MLLMs using MMSafeAware reveals that current models are not sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further explore three methods to improve safety awareness-prompting-based approaches, visual contrastive decoding, and vision-centric reasoning fine-tuning-but find that none achieve satisfactory performance. Our findings highlight the profound challenges in developing MLLMs with robust safety awareness, underscoring the need for further research in this area. All the code and data will be publicly available to facilitate future research.",
      "arxiv_url": "https://arxiv.org/abs/2502.11184",
      "pdf_url": "https://arxiv.org/pdf/2502.11184",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.01541",
      "title": "Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing",
      "authors": [
        "Álvaro Zaera",
        "Diana Nicoleta Popa",
        "Ivan Sekulic",
        "Paolo Rosso"
      ],
      "abstract": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented dialogue systems (TODS), as it ensures robustness to unseen and ambiguous queries. In this work, we propose a novel but simple modular framework that combines uncertainty modeling with fine-tuned large language models (LLMs) for efficient and accurate OOS detection. The first step applies uncertainty estimation to the output of an in-scope intent detection classifier, which is currently deployed in a real-world TODS handling tens of thousands of user interactions daily. The second step then leverages an emerging LLM-based approach, where a fine-tuned LLM is triggered to make a final decision on instances with high uncertainty. Unlike prior approaches, our method effectively balances computational efficiency and performance, combining traditional approaches with LLMs and yielding state-of-the-art results on key OOS detection benchmarks, including real-world OOS data acquired from a deployed TODS.",
      "arxiv_url": "https://arxiv.org/abs/2507.01541",
      "pdf_url": "https://arxiv.org/pdf/2507.01541",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.09825",
      "title": "KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning",
      "authors": [
        "Peiqi Sui",
        "Juan Diego Rodriguez",
        "Philippe Laban",
        "Dean Murphy",
        "Joseph P. Dexter",
        "R. So",
        "Samuel Baker",
        "Pramit Chaudhuri"
      ],
      "abstract": "Each year, tens of millions of essays are written and graded in college-level English courses. Students are asked to analyze literary and cultural texts through a process known as close reading, in which they gather textual details to formulate evidence-based arguments. Despite being viewed as a basis for critical thinking and widely adopted as a required element of university coursework, close reading has never been evaluated on large language models (LLMs), and multi-discipline benchmarks like MMLU do not include literature as a subject. To fill this gap, we present KRISTEVA, the first close reading benchmark for evaluating interpretive reasoning, consisting of 1331 multiple-choice questions adapted from classroom data. With KRISTEVA, we propose three progressively more difficult sets of tasks to approximate different elements of the close reading process, which we use to test how well LLMs may seem to understand and reason about literary works: 1) extracting stylistic features, 2) retrieving relevant contextual information from parametric knowledge, and 3) multi-hop reasoning between style and external contexts. Our baseline results find that, while state-of-the-art LLMs possess some college-level close reading competency (accuracy 49.7% - 69.7%), their performances still trail those of experienced human evaluators on 10 out of our 11 tasks.",
      "arxiv_url": "https://arxiv.org/abs/2505.09825",
      "pdf_url": "https://arxiv.org/pdf/2505.09825",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.14822",
      "title": "Completing A Systematic Review in Hours instead of Months with Interactive AI Agents",
      "authors": [
        "Rui Qiu",
        "Shijie Chen",
        "Yu Su",
        "Po-Yin Yen",
        "Han-Wei Shen"
      ],
      "abstract": "Systematic reviews (SRs) are vital for evidence-based practice in high stakes disciplines, such as healthcare, but are often impeded by intensive labors and lengthy processes that can take months to complete. Due to the high demand for domain expertise, existing automatic summarization methods fail to accurately identify relevant studies and generate high-quality summaries. To that end, we introduce InsightAgent, a human-centered interactive AI agent powered by large language models that revolutionize this workflow. InsightAgent partitions a large literature corpus based on semantics and employs a multi-agent design for more focused processing of literature, leading to significant improvement in the quality of generated SRs. InsightAgent also provides intuitive visualizations of the corpus and agent trajectories, allowing users to effortlessly monitor the actions of the agent and provide real-time feedback based on their expertise. Our user studies with 9 medical professionals demonstrate that the visualization and interaction mechanisms can effectively improve the quality of synthesized SRs by 27.2%, reaching 79.7% of human-written quality. At the same time, user satisfaction is improved by 34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather than months, to complete a high-quality systematic review.",
      "arxiv_url": "https://arxiv.org/abs/2504.14822",
      "pdf_url": "https://arxiv.org/pdf/2504.14822",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2025-04-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.03586",
      "title": "Benchmarking LLMs and LLM-based Agents in Practical Vulnerability Detection for Code Repositories",
      "authors": [
        "Alperen Yildiz",
        "Sin G. Teo",
        "Yiling Lou",
        "Yebo Feng",
        "Chong Wang",
        "Dinil Mon Divakaran"
      ],
      "abstract": "Large Language Models (LLMs) have shown promise in software vulnerability detection, particularly on function-level benchmarks like Devign and BigVul. However, real-world detection requires interprocedural analysis, as vulnerabilities often emerge through multi-hop function calls rather than isolated functions. While repository-level benchmarks like ReposVul and VulEval introduce interprocedural context, they remain computationally expensive, lack pairwise evaluation of vulnerability fixes, and explore limited context retrieval, limiting their practicality. We introduce JitVul, a JIT vulnerability detection benchmark linking each function to its vulnerability-introducing and fixing commits. Built from 879 CVEs spanning 91 vulnerability types, JitVul enables comprehensive evaluation of detection capabilities. Our results show that ReAct Agents, leveraging thought-action-observation and interprocedural context, perform better than LLMs in distinguishing vulnerable from benign code. While prompting strategies like Chain-of-Thought help LLMs, ReAct Agents require further refinement. Both methods show inconsistencies, either misidentifying vulnerabilities or over-analyzing security guards, indicating significant room for improvement.",
      "arxiv_url": "https://arxiv.org/abs/2503.03586",
      "pdf_url": "https://arxiv.org/pdf/2503.03586",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3a952585087293fffc66328793241efe3f45f056",
      "title": "Understanding Large Language Model Vulnerabilities to Social Bias Attacks",
      "authors": [
        "Jiaxu Zhao",
        "Meng Fang",
        "Fanghua Ye",
        "Ke Xu",
        "Qin Zhang",
        "Joey Tianyi Zhou",
        "Mykola Pechenizkiy"
      ],
      "abstract": "Warning: This paper contains content that may be offensive or upsetting. Large Language Models (LLMs) have become foundational in human-computer interaction, demonstrating remarkable linguistic capabilities across various tasks. However, there is a growing concern about their potential to per-petuate social biases present in their training data. In this paper, we comprehensively investigate the vulnerabilities of contemporary LLMs to various social bias attacks, including pre-fix injection, refusal suppression, and learned attack prompts. We evaluate popular models such as LLaMA-2, GPT-3.5, and GPT-4 across gender, racial, and religious bias types. Our findings reveal that models are generally more susceptible to gender bias attacks compared to racial or religious biases. We also explore novel aspects such as cross-bias and multiple-bias attacks, finding varying degrees of trans-ferability across bias types. Additionally, our results show that larger models and pretrained base models often exhibit higher susceptibility to bias attacks. These insights contribute to the development of more inclusive and ethically responsible LLMs, emphasizing the importance of understanding and mitigating potential bias vulnerabilities. We offer recommendations for model developers and users to enhance the robustness of LLMs against social bias attacks.",
      "arxiv_url": "https://www.semanticscholar.org/paper/3a952585087293fffc66328793241efe3f45f056",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.01366",
      "title": "ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding",
      "authors": [
        "Austin T. Wang",
        "ZeMing Gong",
        "Angel X. Chang"
      ],
      "abstract": "3D visual grounding (3DVG) involves localizing entities in a 3D scene referred to by natural language text. Such models are useful for embodied AI and scene retrieval applications, which involve searching for objects or patterns using natural language descriptions. While recent works have focused on LLM-based scaling of 3DVG datasets, these datasets do not capture the full range of potential prompts which could be specified in the English language. To ensure that we are scaling up and testing against a useful and representative set of prompts, we propose a framework for linguistically analyzing 3DVG prompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a diagnostic dataset for evaluating visual grounding methods against a diverse set of language patterns. We evaluate existing open-vocabulary 3DVG methods to demonstrate that these methods are not yet proficient in understanding and identifying the targets of more challenging, out-of-distribution prompts, toward real-world applications.",
      "arxiv_url": "https://arxiv.org/abs/2501.01366",
      "pdf_url": "https://arxiv.org/pdf/2501.01366",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.03991",
      "title": "Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles",
      "authors": [
        "Yuxi Xia",
        "Pedro Henrique Luz de Araujo",
        "Klim Zaporojets",
        "Benjamin Roth"
      ],
      "abstract": "Calibration, the alignment between model confidence and prediction accuracy, is critical for the reliable deployment of large language models (LLMs). Existing works neglect to measure the generalization of their methods to other prompt styles and different sizes of LLMs. To address this, we define a controlled experimental setting covering 12 LLMs and four prompt styles. We additionally investigate if incorporating the response agreement of multiple LLMs and an appropriate loss function can improve calibration performance. Concretely, we build Calib-n, a novel framework that trains an auxiliary model for confidence estimation that aggregates responses from multiple LLMs to capture inter-model agreement. To optimize calibration, we integrate focal and AUC surrogate losses alongside binary cross-entropy. Experiments across four datasets demonstrate that both response agreement and focal loss improve calibration from baselines. We find that few-shot prompts are the most effective for auxiliary model-based methods, and auxiliary models demonstrate robust calibration performance across accuracy variations, outperforming LLMs' internal probabilities and verbalized confidences. These insights deepen the understanding of influence factors in LLM calibration, supporting their reliable deployment in diverse applications.",
      "arxiv_url": "https://arxiv.org/abs/2501.03991",
      "pdf_url": "https://arxiv.org/pdf/2501.03991",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3b242a6a0c9a5e7d1c202ef32ab8c024aedd1715",
      "title": "FIHA: Automated Fine-grained Hallucinations Evaluations in Large Vision Language Models with Davidson Scene Graphs",
      "authors": [
        "Bowen Yan",
        "Zhengsong Zhang",
        "Liqiang Jing",
        "Eftekhar Hossain",
        "Xinya Du"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/3b242a6a0c9a5e7d1c202ef32ab8c024aedd1715",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.17629",
      "title": "TransBench: Breaking Barriers for Transferable Graphical User Interface Agents in Dynamic Digital Environments",
      "authors": [
        "Yuheng Lu",
        "Qian Yu",
        "Hongru Wang",
        "Zeming Liu",
        "Wei Su",
        "Yanping Liu",
        "Yuhang Guo",
        "Maocheng Liang",
        "Yunhong Wang",
        "Haifeng Wang"
      ],
      "abstract": "Graphical User Interface (GUI) agents, which autonomously operate on digital interfaces through natural language instructions, hold transformative potential for accessibility, automation, and user experience. A critical aspect of their functionality is grounding - the ability to map linguistic intents to visual and structural interface elements. However, existing GUI agents often struggle to adapt to the dynamic and interconnected nature of real-world digital environments, where tasks frequently span multiple platforms and applications while also being impacted by version updates. To address this, we introduce TransBench, the first benchmark designed to systematically evaluate and enhance the transferability of GUI agents across three key dimensions: cross-version transferability (adapting to version updates), cross-platform transferability (generalizing across platforms like iOS, Android, and Web), and cross-application transferability (handling tasks spanning functionally distinct apps). TransBench includes 15 app categories with diverse functionalities, capturing essential pages across versions and platforms to enable robust evaluation. Our experiments demonstrate significant improvements in grounding accuracy, showcasing the practical utility of GUI agents in dynamic, real-world environments. Our code and data will be publicly available at GitHub.",
      "arxiv_url": "https://arxiv.org/abs/2505.17629",
      "pdf_url": "https://arxiv.org/pdf/2505.17629",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10515",
      "title": "Probing LLMs for Multilingual Discourse Generalization Through a Unified Label Set",
      "authors": [
        "Florian Eichin",
        "Yang Janet Liu",
        "Barbara Plank",
        "Michael A. Hedderich"
      ],
      "abstract": "Discourse understanding is essential for many NLP tasks, yet most existing work remains constrained by framework-dependent discourse representations. This work investigates whether large language models (LLMs) capture discourse knowledge that generalizes across languages and frameworks. We address this question along two dimensions: (1) developing a unified discourse relation label set to facilitate cross-lingual and cross-framework discourse analysis, and (2) probing LLMs to assess whether they encode generalizable discourse abstractions. Using multilingual discourse relation classification as a testbed, we examine a comprehensive set of 23 LLMs of varying sizes and multilingual capabilities. Our results show that LLMs, especially those with multilingual training corpora, can generalize discourse information across languages and frameworks. Further layer-wise analyses reveal that language generalization at the discourse level is most salient in the intermediate layers. Lastly, our error analysis provides an account of challenging relation classes.",
      "arxiv_url": "https://arxiv.org/abs/2503.10515",
      "pdf_url": "https://arxiv.org/pdf/2503.10515",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3b85ceab74cfeecca6341b7ec3f256045ad88120",
      "title": "Disentangling Biased Knowledge from Reasoning in Large Language Models via Machine Unlearning",
      "authors": [
        "Zheyuan Liu",
        "Suraj Maharjan",
        "Wu Fanyou",
        "Rahil Parikh",
        "Belhassen Bayar",
        "Srinivasan H. Sengamedu",
        "Meng Jiang"
      ],
      "abstract": "The rapid development of Large Language Models (LLMs) has led to their widespread adoption across various domains, leveraging vast pre-training knowledge and impressive generalization capabilities. However, these models often inherit biased knowledge, resulting in unfair decisions in sensitive applications. It is challenging to remove this biased knowledge without compromising reasoning abilities due to the entangled nature of the learned knowledge within LLMs. To solve this problem, existing approaches have attempted to mitigate the bias using techniques such as fine-tuning with unbiased datasets, model merging, and gradient ascent. While these methods have experimentally proven effective, they can still be sub-optimum in fully disentangling biases from reasoning. To address this gap, we propose S elective D isentanglement U nlearning (SDU), a novel unlearning framework that selectively removes biased knowledge while preserving reasoning capabilities. SDU operates in three stages: identifying biased parameters us-ing a shadow LLM, fine-tuning with unbiased data, and performing selective parameter updates based on weight saliency. Experimental results across multiple LLMs show that SDU improves fairness accuracy by 14.7% and enhances reasoning performance by 62.6% compared to existing baselines. 2",
      "arxiv_url": "https://www.semanticscholar.org/paper/3b85ceab74cfeecca6341b7ec3f256045ad88120",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3b87ef307dfbed5a3881937cba6f8afaeba192c8",
      "title": "A Query-Response Framework for Whole-Page Complex-Layout Document Image Translation with Relevant Regional Concentration",
      "authors": [
        "Zhiyang Zhang",
        "Yaping Zhang",
        "Yupu Liang",
        "Zhiyuan Chen",
        "Lu Xiang",
        "Yang Zhao",
        "Yu Zhou",
        "Chengqing Zong"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/3b87ef307dfbed5a3881937cba6f8afaeba192c8",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.06659",
      "title": "Who Taught You That? Tracing Teachers in Model Distillation",
      "authors": [
        "Somin Wadhwa",
        "Chantal Shaib",
        "Silvio Amir",
        "Byron C. Wallace"
      ],
      "abstract": "Model distillation -- using outputs from a large teacher model to teach a small student model -- is a practical means of creating efficient models for a particular task. We ask: Can we identify a students' teacher based on its outputs? Such\"footprints\"left by teacher LLMs would be interesting artifacts. Beyond this, reliable teacher inference may have practical implications as actors seek to distill specific capabilities of massive proprietary LLMs into deployed smaller LMs, potentially violating terms of service. We consider practical task distillation targets including summarization, question answering, and instruction-following. We assume a finite set of candidate teacher models, which we treat as blackboxes. We design discriminative models that operate over lexical features. We find that $n$-gram similarity alone is unreliable for identifying teachers, but part-of-speech (PoS) templates preferred by student models mimic those of their teachers.",
      "arxiv_url": "https://arxiv.org/abs/2502.06659",
      "pdf_url": "https://arxiv.org/pdf/2502.06659",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.12485",
      "title": "R2D2: Remembering, Replaying and Dynamic Decision Making with a Reflective Agentic Memory",
      "authors": [
        "Tenghao Huang",
        "Kinjal Basu",
        "Ibrahim Abdelaziz",
        "P. Kapanipathi",
        "Jonathan May",
        "Muhao Chen"
      ],
      "abstract": "The proliferation of web agents necessitates advanced navigation and interaction strategies within complex web environments. Current models often struggle with efficient navigation and action execution due to limited visibility and understanding of web structures. Our proposed R2D2 framework addresses these challenges by integrating two paradigms: Remember and Reflect. The Remember paradigm uses a replay buffer that aids agents in reconstructing the web environment dynamically, thus enabling the formulation of a detailed\"map\"of previously visited pages. This helps in reducing navigational errors and optimizing the decision-making process during web interactions. Conversely, the Reflect paradigm allows agents to learn from past mistakes by providing a mechanism for error analysis and strategy refinement, enhancing overall task performance. We evaluate R2D2 using the WebArena benchmark, demonstrating substantial improvements over existing methods, including a 50% reduction in navigation errors and a threefold increase in task completion rates. Our findings suggest that a combination of memory-enhanced navigation and reflective learning promisingly advances the capabilities of web agents, potentially benefiting various applications such as automated customer service and personal digital assistants.",
      "arxiv_url": "https://arxiv.org/abs/2501.12485",
      "pdf_url": "https://arxiv.org/pdf/2501.12485",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2025-01-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.14235",
      "title": "A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs",
      "authors": [
        "Yimin Deng",
        "Yuxia Wu",
        "Yejing Wang",
        "Guoshuai Zhao",
        "Li Zhu",
        "Qidong Liu",
        "Derong Xu",
        "Zichuan Fu",
        "Xian Wu",
        "Yefeng Zheng",
        "Xiangyu Zhao",
        "Xueming Qian"
      ],
      "abstract": "Temporal knowledge graph reasoning aims to predict future events with knowledge of existing facts and plays a key role in various downstream tasks. Previous methods focused on either graph structure learning or semantic reasoning, failing to integrate dual reasoning perspectives to handle different prediction scenarios. Moreover, they lack the capability to capture the inherent differences between historical and non-historical events, which limits their generalization across different temporal contexts. To this end, we propose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs three kinds of expert modules to integrate both structural and semantic information, guiding the reasoning process for different events. Extensive experiments on three datasets demonstrate the effectiveness of our approach.",
      "arxiv_url": "https://arxiv.org/abs/2506.14235",
      "pdf_url": "https://arxiv.org/pdf/2506.14235",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00829",
      "title": "COMPKE: Complex Question Answering under Knowledge Editing",
      "authors": [
        "Keyuan Cheng",
        "Zijian Kan",
        "Zhixian He",
        "Zhuoran Zhang",
        "Muhammad Asif Ali",
        "Ke Xu",
        "Lijie Hu",
        "Di Wang"
      ],
      "abstract": "Knowledge Editing, which efficiently modifies the knowledge in large language models, has gathered great attention. Current benchmarks primarily use multi-hop question answering to assess and analyze newly injected or updated knowledge. However, we argue that these benchmarks fail to effectively evaluate how well the updated models apply this knowledge in real-life scenarios, particularly when questions require complex reasoning, involving one-to-many relationships or multi-step logical intersections. To fill in this gap, we introduce a new benchmark, COMPKE: Complex Question Answering under Knowledge Editing, which includes 11,924 complex questions that reflect real-life situations. We conduct an extensive evaluation of four knowledge editing methods on COMPKE, revealing that their effectiveness varies notably across different models. For instance, MeLLo attains an accuracy of 39.47 on GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further investigate the underlying causes of these disparities from both methodological and model-specific perspectives. The datasets are available at https://github.com/kzjkzj666/CompKE.",
      "arxiv_url": "https://arxiv.org/abs/2506.00829",
      "pdf_url": "https://arxiv.org/pdf/2506.00829",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.15509",
      "title": "SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation",
      "authors": [
        "Keqi Deng",
        "Wenxi Chen",
        "Xie Chen",
        "Phil Woodland"
      ],
      "abstract": "Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.",
      "arxiv_url": "https://arxiv.org/abs/2504.15509",
      "pdf_url": "https://arxiv.org/pdf/2504.15509",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16757",
      "title": "Entailment-Preserving First-order Logic Representations in Natural Language Entailment",
      "authors": [
        "Jinu Lee",
        "Qi Liu",
        "Runzhi Ma",
        "Vincent Han",
        "Ziqi Wang",
        "Heng Ji",
        "J. Hockenmaier"
      ],
      "abstract": "First-order logic (FOL) can represent the logical entailment semantics of natural language (NL) sentences, but determining natural language entailment using FOL remains a challenge. To address this, we propose the Entailment-Preserving FOL representations (EPF) task and introduce reference-free evaluation metrics for EPF, the Entailment-Preserving Rate (EPR) family. In EPF, one should generate FOL representations from multi-premise natural language entailment data (e.g. EntailmentBank) so that the automatic prover's result preserves the entailment labels. Experiments show that existing methods for NL-to-FOL translation struggle in EPF. To this extent, we propose a training method specialized for the task, iterative learning-to-rank, which directly optimizes the model's EPR score through a novel scoring function and a learning-to-rank objective. Our method achieves a 1.8-2.7% improvement in EPR and a 17.4-20.6% increase in EPR@16 compared to diverse baselines in three datasets. Further analyses reveal that iterative learning-to-rank effectively suppresses the arbitrariness of FOL representation by reducing the diversity of predicate signatures, and maintains strong performance across diverse inference types and out-of-domain data.",
      "arxiv_url": "https://arxiv.org/abs/2502.16757",
      "pdf_url": "https://arxiv.org/pdf/2502.16757",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06982",
      "title": "Chain of Methodologies: Scaling Test Time Computation without Training",
      "authors": [
        "Cong Liu",
        "Jie Wu",
        "Weigang Wu",
        "Xu Chen",
        "Liang Li",
        "Wei Zheng"
      ],
      "abstract": "Large Language Models (LLMs) often struggle with complex reasoning tasks due to insufficient in-depth insights in their training data, which are typically absent in publicly available documents. This paper introduces the Chain of Methodologies (CoM), an innovative and intuitive prompting framework that enhances structured thinking by integrating human methodological insights, enabling LLMs to tackle complex tasks with extended reasoning. CoM leverages the metacognitive abilities of advanced LLMs, activating systematic reasoning throught user-defined methodologies without explicit fine-tuning. Experiments show that CoM surpasses competitive baselines, demonstrating the potential of training-free prompting methods as robust solutions for complex reasoning tasks and bridging the gap toward human-level reasoning through human-like methodological insights.",
      "arxiv_url": "https://arxiv.org/abs/2506.06982",
      "pdf_url": "https://arxiv.org/pdf/2506.06982",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3c3a6e35324fe436b051b2c30d0d64e8c1a7c7db",
      "title": "Structure-adaptive Adversarial Contrastive Learning for Multi-Domain Fake News Detection",
      "authors": [
        "Lingwei Wei",
        "Dou Hu",
        "Wei Zhou",
        "Philip S. Yu",
        "Songlin Hu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/3c3a6e35324fe436b051b2c30d0d64e8c1a7c7db",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16971",
      "title": "LongSafety: Evaluating Long-Context Safety of Large Language Models",
      "authors": [
        "Yida Lu",
        "Jiale Cheng",
        "Zhexin Zhang",
        "Shiyao Cui",
        "Cunxiang Wang",
        "Xiaotao Gu",
        "Yuxiao Dong",
        "Jie Tang",
        "Hongning Wang",
        "Minlie Huang"
      ],
      "abstract": "As Large Language Models (LLMs) continue to advance in understanding and generating long sequences, new safety concerns have been introduced through the long context. However, the safety of LLMs in long-context tasks remains under-explored, leaving a significant gap in both evaluation and improvement of their safety. To address this, we introduce LongSafety, the first comprehensive benchmark specifically designed to evaluate LLM safety in open-ended long-context tasks. LongSafety encompasses 7 categories of safety issues and 6 user-oriented long-context tasks, with a total of 1,543 test cases, averaging 5,424 words per context. Our evaluation towards 16 representative LLMs reveals significant safety vulnerabilities, with most models achieving safety rates below 55%. Our findings also indicate that strong safety performance in short-context scenarios does not necessarily correlate with safety in long-context tasks, emphasizing the unique challenges and urgency of improving long-context safety. Moreover, through extensive analysis, we identify challenging safety issues and task types for long-context models. Furthermore, we find that relevant context and extended input sequences can exacerbate safety risks in long-context scenarios, highlighting the critical need for ongoing attention to long-context safety challenges. Our code and data are available at https://github.com/thu-coai/LongSafety.",
      "arxiv_url": "https://arxiv.org/abs/2502.16971",
      "pdf_url": "https://arxiv.org/pdf/2502.16971",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3c5e4b9ef078766411afc62b097f85a033bb079d",
      "title": "Question Answering in Climate Adaptation for Agriculture: Model Development and Evaluation with Expert Feedback",
      "authors": [
        "Vincent Nguyen",
        "Sarvnaz Karimi",
        "Willow Hallgren",
        "Mahesh Prakash"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/3c5e4b9ef078766411afc62b097f85a033bb079d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05400",
      "title": "Auto Review: Second Stage Error Detection for Highly Accurate Information Extraction from Phone Conversations",
      "authors": [
        "A. Qamar",
        "Arushi Raghuvanshi",
        "Conal Sathi",
        "Youngseo Son"
      ],
      "abstract": "Automating benefit verification phone calls saves time in healthcare and helps patients receive treatment faster. It is critical to obtain highly accurate information in these phone calls, as it can affect a patient's healthcare journey. Given the noise in phone call transcripts, we have a two-stage system that involves a post-call review phase for potentially noisy fields, where human reviewers manually verify the extracted data$\\unicode{x2013}$a labor-intensive task. To automate this stage, we introduce Auto Review, which significantly reduces manual effort while maintaining a high bar for accuracy. This system, being highly reliant on call transcripts, suffers a performance bottleneck due to automatic speech recognition (ASR) issues. This problem is further exacerbated by the use of domain-specific jargon in the calls. In this work, we propose a second-stage postprocessing pipeline for accurate information extraction. We improve accuracy by using multiple ASR alternatives and a pseudo-labeling approach that does not require manually corrected transcripts. Experiments with general-purpose large language models and feature-based model pipelines demonstrate substantial improvements in the quality of corrected call transcripts, thereby enhancing the efficiency of Auto Review.",
      "arxiv_url": "https://arxiv.org/abs/2506.05400",
      "pdf_url": "https://arxiv.org/pdf/2506.05400",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.18260",
      "title": "MAGI: Multi-Agent Guided Interview for Psychiatric Assessment",
      "authors": [
        "Guanqun Bi",
        "Zhuang Chen",
        "Zhoufu Liu",
        "Hongkai Wang",
        "Xiyao Xiao",
        "Yuqiang Xie",
        "Wen Zhang",
        "Yongkang Huang",
        "Yuxuan Chen",
        "Libiao Peng",
        "Yi Feng",
        "Minlie Huang"
      ],
      "abstract": "Automating structured clinical interviews could revolutionize mental healthcare accessibility, yet existing large language models (LLMs) approaches fail to align with psychiatric diagnostic protocols. We present MAGI, the first framework that transforms the gold-standard Mini International Neuropsychiatric Interview (MINI) into automatic computational workflows through coordinated multi-agent collaboration. MAGI dynamically navigates clinical logic via four specialized agents: 1) an interview tree guided navigation agent adhering to the MINI's branching structure, 2) an adaptive question agent blending diagnostic probing, explaining, and empathy, 3) a judgment agent validating whether the response from participants meet the node, and 4) a diagnosis Agent generating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map symptoms to clinical criteria. Experimental results on 1,002 real-world participants covering depression, generalized anxiety, social anxiety and suicide shows that MAGI advances LLM- assisted mental health assessment by combining clinical rigor, conversational adaptability, and explainable reasoning.",
      "arxiv_url": "https://arxiv.org/abs/2504.18260",
      "pdf_url": "https://arxiv.org/pdf/2504.18260",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.17525",
      "title": "Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning",
      "authors": [
        "Mingfei Lau",
        "Qian Chen",
        "Yeming Fang",
        "Tingting Xu",
        "Tongzhou Chen",
        "Pavel Golik"
      ],
      "abstract": "Our quality audit for three widely used public multilingual speech datasets - Mozilla Common Voice 17.0, FLEURS, and Vox Populi - shows that in some languages, these datasets suffer from significant quality issues, which may obfuscate downstream evaluation results while creating an illusion of success. We divide these quality issues into two categories: micro-level and macro-level. We find that macro-level issues are more prevalent in less institutionalized, often under-resourced languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that highlights the need for proactive language planning (e.g. orthography prescriptions, dialect boundary definition) and enhanced data quality control in the dataset creation process. We conclude by proposing guidelines and recommendations to mitigate these issues in future dataset development, emphasizing the importance of sociolinguistic awareness and language planning principles. Furthermore, we encourage research into how this creation process itself can be leveraged as a tool for community-led language planning and revitalization.",
      "arxiv_url": "https://arxiv.org/abs/2506.17525",
      "pdf_url": "https://arxiv.org/pdf/2506.17525",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16514",
      "title": "GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking",
      "authors": [
        "Yingjian Chen",
        "Haoran Liu",
        "Yinhong Liu",
        "Rui Yang",
        "Han Yuan",
        "Yanran Fu",
        "Pengyuan Zhou",
        "Qingyu Chen",
        "James Caverlee",
        "Irene Li"
      ],
      "abstract": "Large language models (LLMs) are widely used, but they often generate subtle factual errors, especially in long-form text. These errors are fatal in some specialized domains such as medicine. Existing fact-checking with grounding documents methods face two main challenges: (1) they struggle to understand complex multihop relations in long documents, often overlooking subtle factual errors; (2) most specialized methods rely on pairwise comparisons, requiring multiple model calls, leading to high resource and computational costs. To address these challenges, we propose GraphCheck , a fact-checking framework that uses extracted knowledge graphs to enhance text representation. Graph Neural Networks further process these graphs as a soft prompt, enabling LLMs to incorporate structured knowledge more effectively. Enhanced with graph-based reasoning, GraphCheck captures multihop reasoning chains that are often overlooked by existing methods, enabling precise and efficient fact-checking in a single inference call. Experimental results on seven benchmarks spanning both general and medical domains demonstrate up to a 7.1% overall improvement over baseline models. Notably, GraphCheck outperforms existing specialized fact-checkers and achieves comparable performance with state-of-the-art LLMs, such as DeepSeek-V3 and OpenAI-o1, with significantly fewer parameters.",
      "arxiv_url": "https://arxiv.org/abs/2502.16514",
      "pdf_url": "https://arxiv.org/pdf/2502.16514",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3cc5fb5003f8c242cd2243abe934be74e44f83ed",
      "title": "Natural Logic at the Core: Dynamic Rewards for Entailment Tree Generation",
      "authors": [
        "Jihao Shi",
        "Xiao Ding",
        "Kai Xiong",
        "Hengwei Zhao",
        "Bing Qin",
        "Ting Liu"
      ],
      "abstract": "Entailment trees are essential for enhancing interpretability and transparency in tasks like question answering and natural language understanding. However, existing approaches often lack logical consistency, as they rely on static reward structures or ignore the intricate dependencies within multi-step reasoning. To address these limitations, we propose a method that integrates natural logic principles into reinforcement learning, enabling dynamic reward computation to guide entailment tree generation. Our approach ensures logical consistency across reasoning steps while improving inter-pretability and generalization. Experiments on EntailmentBank demonstrate significant improvements over state-of-the-art methods, high-lighting the effectiveness of natural logic in structured reasoning.",
      "arxiv_url": "https://www.semanticscholar.org/paper/3cc5fb5003f8c242cd2243abe934be74e44f83ed",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24757",
      "title": "LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews",
      "authors": [
        "Christian Jaumann",
        "Andreas Wiedholz",
        "Annemarie Friedrich"
      ],
      "abstract": "The scientific literature is growing rapidly, making it hard to keep track of the state-of-the-art. Systematic literature reviews (SLRs) aim to identify and evaluate all relevant papers on a topic. After retrieving a set of candidate papers, the abstract screening phase determines initial relevance. To date, abstract screening methods using large language models (LLMs) focus on binary classification settings; existing question answering (QA) based ranking approaches suffer from error propagation. LLMs offer a unique opportunity to evaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks do not provide them exhaustively. We manually extract these criteria as well as research questions for 57 SLRs, mostly in the medical domain, enabling principled comparisons between approaches. Moreover, we propose LGAR, a zero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance scorer and a dense re-ranker. Our extensive experiments show that LGAR outperforms existing QA-based methods by 5-10 pp. in mean average precision. Our code and data is publicly available.",
      "arxiv_url": "https://arxiv.org/abs/2505.24757",
      "pdf_url": "https://arxiv.org/pdf/2505.24757",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06506",
      "title": "Biases Propagate in Encoder-based Vision-Language Models: A Systematic Analysis From Intrinsic Measures to Zero-shot Retrieval Outcomes",
      "authors": [
        "Kshitish Ghate",
        "Tessa E. S. Charlesworth",
        "Mona Diab",
        "Aylin Caliskan"
      ],
      "abstract": "To build fair AI systems we need to understand how social-group biases intrinsic to foundational encoder-based vision-language models (VLMs) manifest in biases in downstream tasks. In this study, we demonstrate that intrinsic biases in VLM representations systematically ``carry over'' or propagate into zero-shot retrieval tasks, revealing how deeply rooted biases shape a model's outputs. We introduce a controlled framework to measure this propagation by correlating (a) intrinsic measures of bias in the representational space with (b) extrinsic measures of bias in zero-shot text-to-image (TTI) and image-to-text (ITT) retrieval. Results show substantial correlations between intrinsic and extrinsic bias, with an average $\\rho$ = 0.83 $\\pm$ 0.10. This pattern is consistent across 114 analyses, both retrieval directions, six social groups, and three distinct VLMs. Notably, we find that larger/better-performing models exhibit greater bias propagation, a finding that raises concerns given the trend towards increasingly complex AI models. Our framework introduces baseline evaluation tasks to measure the propagation of group and valence signals. Investigations reveal that underrepresented groups experience less robust propagation, further skewing their model-related outcomes.",
      "arxiv_url": "https://arxiv.org/abs/2506.06506",
      "pdf_url": "https://arxiv.org/pdf/2506.06506",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24219",
      "title": "ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation",
      "authors": [
        "Lam Thanh Do",
        "Aaditya Bodke",
        "Pritom Saha Akash",
        "Kevin Chen-Chuan Chang"
      ],
      "abstract": "Unsupervised keyphrase prediction has gained growing interest in recent years. However, existing methods typically rely on heuristically defined importance scores, which may lead to inaccurate informativeness estimation. In addition, they lack consideration for time efficiency. To solve these problems, we propose ERU-KG, an unsupervised keyphrase generation (UKG) model that consists of an informativeness and a phraseness module. The former estimates the relevance of keyphrase candidates, while the latter generate those candidates. The informativeness module innovates by learning to model informativeness through references (e.g., queries, citation contexts, and titles) and at the term-level, thereby 1) capturing how the key concepts of documents are perceived in different contexts and 2) estimating informativeness of phrases more efficiently by aggregating term informativeness, removing the need for explicit modeling of the candidates. ERU-KG demonstrates its effectiveness on keyphrase generation benchmarks by outperforming unsupervised baselines and achieving on average 89\\% of the performance of a supervised model for top 10 predictions. Additionally, to highlight its practical utility, we evaluate the model on text retrieval tasks and show that keyphrases generated by ERU-KG are effective when employed as query and document expansions. Furthermore, inference speed tests reveal that ERU-KG is the fastest among baselines of similar model sizes. Finally, our proposed model can switch between keyphrase generation and extraction by adjusting hyperparameters, catering to diverse application requirements.",
      "arxiv_url": "https://arxiv.org/abs/2505.24219",
      "pdf_url": "https://arxiv.org/pdf/2505.24219",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19166",
      "title": "CodeIF: Benchmarking the Instruction-Following Capabilities of Large Language Models for Code Generation",
      "authors": [
        "Kaiwen Yan",
        "Hongcheng Guo",
        "Xuanqing Shi",
        "Jingyi Xu",
        "Yaonan Gu",
        "Zhoujun Li"
      ],
      "abstract": "With the rapid advancement of Large Language Models (LLMs), the demand for robust instruction-following capabilities in code generation tasks has grown significantly. Code generation not only facilitates faster prototyping and automated testing, but also augments developer efficiency through improved maintainability and reusability of code. In this paper, we introduce CodeIF, the first benchmark specifically designed to assess the abilities of LLMs to adhere to task-oriented instructions within diverse code generation scenarios. CodeIF encompasses a broad range of tasks, including function synthesis, error debugging, algorithmic refactoring, and code explanation, thereby providing a comprehensive suite to evaluate model performance across varying complexity levels and programming domains. We conduct extensive experiments with LLMs, analyzing their strengths and limitations in meeting the demands of these tasks. The experimental results offer valuable insights into how well current models align with human instructions, as well as the extent to which they can generate consistent, maintainable, and contextually relevant code. Our findings not only underscore the critical role that instruction-following LLMs can play in modern software development, but also illuminate pathways for future research aimed at enhancing their adaptability, reliability, and overall effectiveness in automated code generation. CodeIF data and code are publicly available: https://github.com/lin-rany/codeIF",
      "arxiv_url": "https://arxiv.org/abs/2502.19166",
      "pdf_url": "https://arxiv.org/pdf/2502.19166",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.11098",
      "title": "Debiasing Online Preference Learning via Preference Feature Preservation",
      "authors": [
        "Dongyoung Kim",
        "Jinsung Yoon",
        "Jinwoo Shin",
        "Jaehyung Kim"
      ],
      "abstract": "Recent preference learning frameworks for large language models (LLMs) simplify human preferences with binary pairwise comparisons and scalar rewards. This simplification could make LLMs' responses biased to mostly preferred features, and would be exacerbated during the iterations of online preference learning steps. To address these challenges, we propose a novel framework coined PFP (Preference Feature Preservation). The key idea of PFP is maintaining the distribution of human preference features and utilizing such rich signals throughout the online preference learning process. Specifically, PFP first extract preference features from offline pairwise human preference data and trains a feature classifier. Then, using trained classifier and the distribution preserving optimization, PFP maps appropriate preference features for a new input instruction during online learning. Lastly, PFP trains LLM using the existing preference learning method, by incorporating the preference feature into system prompts and enabling LLM to explicitly handle various human preferences. Our experiments demonstrate that PFP successfully mitigates the bias in preference features during online learning, and hence achieves superior performance compared to previous preference learning methods on standard benchmarks to evaluate LLM alignment.",
      "arxiv_url": "https://arxiv.org/abs/2506.11098",
      "pdf_url": "https://arxiv.org/pdf/2506.11098",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3d24a91de6ce9852359e0bf4ee0e975b91f2f1d7",
      "title": "Combining Domain and Alignment Vectors Provides Better Knowledge-Safety Trade-offs in LLMs",
      "authors": [
        "Megh Thakkar",
        "Quentin Fournier",
        "Matthew Riemer",
        "Pin-Yu Chen",
        "Amal Zouaq",
        "Payel Das",
        "Sarath Chandar"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/3d24a91de6ce9852359e0bf4ee0e975b91f2f1d7",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.09606",
      "title": "Human-LLM Coevolution: Evidence from Academic Writing",
      "authors": [
        "Mingmeng Geng",
        "Roberto Trotta"
      ],
      "abstract": "With a statistical analysis of arXiv paper abstracts, we report a marked drop in the frequency of several words previously identified as overused by ChatGPT, such as\"delve\", starting soon after they were pointed out in early 2024. The frequency of certain other words favored by ChatGPT, such as\"significant\", has instead kept increasing. These phenomena suggest that some authors of academic papers have adapted their use of large language models (LLMs), for example, by selecting outputs or applying modifications to the LLM-generated content. Such coevolution and cooperation of humans and LLMs thus introduce additional challenges to the detection of machine-generated text in real-world scenarios. Estimating the impact of LLMs on academic writing by examining word frequency remains feasible, and more attention should be paid to words that were already frequently employed, including those that have decreased in frequency due to LLMs' disfavor.",
      "arxiv_url": "https://arxiv.org/abs/2502.09606",
      "pdf_url": "https://arxiv.org/pdf/2502.09606",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19041",
      "title": "Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs",
      "authors": [
        "Shiyu Xiang",
        "Ansen Zhang",
        "Yanfei Cao",
        "Yang Fan",
        "Ronghao Chen"
      ],
      "abstract": "Although Aligned Large Language Models (LLMs) are trained to refuse harmful requests, they remain vulnerable to jailbreak attacks. Unfortunately, existing methods often focus on surface-level patterns, overlooking the deeper attack essences. As a result, defenses fail when attack prompts change, even though the underlying\"attack essence\"remains the same. To address this issue, we introduce EDDF, an \\textbf{E}ssence-\\textbf{D}riven \\textbf{D}efense \\textbf{F}ramework Against Jailbreak Attacks in LLMs. EDDF is a plug-and-play input-filtering method and operates in two stages: 1) offline essence database construction, and 2) online adversarial query detection. The key idea behind EDDF is to extract the\"attack essence\"from a diverse set of known attack instances and store it in an offline vector database. Experimental results demonstrate that EDDF significantly outperforms existing methods by reducing the Attack Success Rate by at least 20\\%, underscoring its superior robustness against jailbreak attacks.",
      "arxiv_url": "https://arxiv.org/abs/2502.19041",
      "pdf_url": "https://arxiv.org/pdf/2502.19041",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3d54f708909b25a5136eb4d6bfcbbed53649c933",
      "title": "DEEP: an automatic bidirectional translator leveraging an ASR for translation of Italian sign language",
      "authors": [
        "Nicolas Tagliabue",
        "Elisa Colletti",
        "Francesco Roberto Dani",
        "Roberto Tedesco",
        "Sonia Cenceschi",
        "Alessandro Trivilini"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/3d54f708909b25a5136eb4d6bfcbbed53649c933",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14285",
      "title": "Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach",
      "authors": [
        "Yurong Wu",
        "Fangwen Mu",
        "Qiuhong Zhang",
        "Jinjing Zhao",
        "Xinrun Xu",
        "Lingrui Mei",
        "Yang Wu",
        "Lin Shi",
        "Junjie Wang",
        "Zhiming Ding",
        "Yiwei Wang"
      ],
      "abstract": "Prompt trading has emerged as a significant intellectual property concern in recent years, where vendors entice users by showcasing sample images before selling prompt templates that can generate similar images. This work investigates a critical security vulnerability: attackers can steal prompt templates using only a limited number of sample images. To investigate this threat, we introduce Prism, a prompt-stealing benchmark consisting of 50 templates and 450 images, organized into Easy and Hard difficulty levels. To identify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a novel template stealing method that operates without model fine-tuning by leveraging differential evolution algorithms. The system first initializes population sets using multimodal large language models (MLLMs) based on predefined patterns, then iteratively generates enhanced offspring through MLLMs. During evolution, EvoStealer identifies common features across offspring to derive generalized templates. Our comprehensive evaluation conducted across open-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini) demonstrates that EvoStealer's stolen templates can reproduce images highly similar to originals and effectively generalize to other subjects, significantly outperforming baseline methods with an average improvement of over 10%. Moreover, our cost analysis reveals that EvoStealer achieves template stealing with negligible computational expenses. Our code and dataset are available at https://github.com/whitepagewu/evostealer.",
      "arxiv_url": "https://arxiv.org/abs/2502.14285",
      "pdf_url": "https://arxiv.org/pdf/2502.14285",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.13562",
      "title": "DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification",
      "authors": [
        "Yu Li",
        "Han Jiang",
        "Zhihua Wei"
      ],
      "abstract": "With the widespread adoption of Large Language Models (LLMs), jailbreak attacks have become an increasingly pressing safety concern. While safety-aligned LLMs can effectively defend against normal harmful queries, they remain vulnerable to such attacks. Existing defense methods primarily rely on fine-tuning or input modification, which often suffer from limited generalization and reduced utility. To address this, we introduce DETAM, a finetuning-free defense approach that improves the defensive capabilities against jailbreak attacks of LLMs via targeted attention modification. Specifically, we analyze the differences in attention scores between successful and unsuccessful defenses to identify the attention heads sensitive to jailbreak attacks. During inference, we reallocate attention to emphasize the user's core intention, minimizing interference from attack tokens. Our experimental results demonstrate that DETAM outperforms various baselines in jailbreak defense and exhibits robust generalization across different attacks and models, maintaining its effectiveness even on in-the-wild jailbreak data. Furthermore, in evaluating the model's utility, we incorporated over-defense datasets, which further validate the superior performance of our approach. The code will be released immediately upon acceptance.",
      "arxiv_url": "https://arxiv.org/abs/2504.13562",
      "pdf_url": "https://arxiv.org/pdf/2504.13562",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22299",
      "title": "Logical Consistency is Vital: Neural-Symbolic Information Retrieval for Negative-Constraint Queries",
      "authors": [
        "Ganlin Xu",
        "Zhoujia Zhang",
        "Wangyi Mei",
        "Jiaqing Liang",
        "Weijia Lu",
        "Xiaodong Zhang",
        "Zhifei Yang",
        "Xiaofeng Ma",
        "Yanghua Xiao",
        "Deqing Yang"
      ],
      "abstract": "Information retrieval plays a crucial role in resource localization. Current dense retrievers retrieve the relevant documents within a corpus via embedding similarities, which compute similarities between dense vectors mainly depending on word co-occurrence between queries and documents, but overlook the real query intents. Thus, they often retrieve numerous irrelevant documents. Particularly in the scenarios of complex queries such as \\emph{negative-constraint queries}, their retrieval performance could be catastrophic. To address the issue, we propose a neuro-symbolic information retrieval method, namely \\textbf{NS-IR}, that leverages first-order logic (FOL) to optimize the embeddings of naive natural language by considering the \\emph{logical consistency} between queries and documents. Specifically, we introduce two novel techniques, \\emph{logic alignment} and \\emph{connective constraint}, to rerank candidate documents, thereby enhancing retrieval relevance. Furthermore, we construct a new dataset \\textbf{NegConstraint} including negative-constraint queries to evaluate our NS-IR's performance on such complex IR scenarios. Our extensive experiments demonstrate that NS-IR not only achieves superior zero-shot retrieval performance on web search and low-resource retrieval tasks, but also performs better on negative-constraint queries. Our scource code and dataset are available at https://github.com/xgl-git/NS-IR-main.",
      "arxiv_url": "https://arxiv.org/abs/2505.22299",
      "pdf_url": "https://arxiv.org/pdf/2505.22299",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.10792",
      "title": "GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience Prediction",
      "authors": [
        "Jessica Lin",
        "Amir Zeldes"
      ],
      "abstract": "Determining and ranking the most salient entities in a text is critical for user-facing systems, especially as users increasingly rely on models to interpret long documents they only partially read. Graded entity salience addresses this need by assigning entities scores that reflect their relative importance in a text. Existing approaches fall into two main categories: subjective judgments of salience, which allow for gradient scoring but lack consistency, and summarization-based methods, which define salience as mention-worthiness in a summary, promoting explainability but limiting outputs to binary labels (entities are either summary-worthy or not). In this paper, we introduce a novel approach for graded entity salience that combines the strengths of both approaches. Using an English dataset spanning 12 spoken and written genres, we collect 5 summaries per document and calculate each entity's salience score based on its presence across these summaries. Our approach shows stronger correlation with scores based on human summaries and alignments, and outperforms existing techniques, including LLMs. We release our data and code at https://github.com/jl908069/gum_sum_salience to support further research on graded salient entity extraction.",
      "arxiv_url": "https://arxiv.org/abs/2504.10792",
      "pdf_url": "https://arxiv.org/pdf/2504.10792",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.13305",
      "title": "Computation Mechanism Behind LLM Position Generalization",
      "authors": [
        "Chi Han",
        "Heng Ji"
      ],
      "abstract": "Most written natural languages are composed of sequences of words and sentences. Similar to humans, large language models (LLMs) exhibit flexibility in handling textual positions - a phenomenon we term position generalization. They can understand texts with position perturbations and generalize to longer texts than those encountered during training with the latest techniques. These phenomena suggest that LLMs handle positions tolerantly, but how LLMs computationally process positional relevance remains largely unexplored. This work connects the linguistic phenomenon with LLMs'computational mechanisms. We show how LLMs enforce certain computational mechanisms for the aforementioned tolerance in position perturbations. Despite the complex design of the self-attention mechanism, this work reveals that LLMs learn a counterintuitive disentanglement of attention logits. Their values show a 0.959 linear correlation with an approximation of the arithmetic sum of positional relevance and semantic importance. Furthermore, we identify a prevalent pattern in intermediate features, which we prove theoretically enables this effect. The pattern, which is different from how randomly initialized parameters would behave, suggests that it is a learned behavior rather than a natural result of the model architecture. Based on these findings, we provide computational explanations and criteria for LLMs'position flexibilities. This work takes a pioneering step in linking position generalization with modern LLMs'internal mechanisms.",
      "arxiv_url": "https://arxiv.org/abs/2503.13305",
      "pdf_url": "https://arxiv.org/pdf/2503.13305",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11895",
      "title": "Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models?",
      "authors": [
        "Jacob Nielsen",
        "Peter Schneider-Kamp",
        "Lukas Galke"
      ],
      "abstract": "Large language models (LLMs) require immense resources for training and inference. Quantization, a technique that reduces the precision of model parameters, offers a promising solution for improving LLM efficiency and sustainability. While post-training quantization methods typically achieve 4-8 bits per parameter, recent research suggests that training LLMs with 1.58 bits per weight parameter from scratch can maintain model accuracy while greatly reducing memory requirements and energy consumption at inference time. Here, we investigate a training strategy for quantization-aware pre-training, where the models are first trained with 16-bit precision and then transition into 1.58-bit quantization-aware training. Our results on 11 downstream tasks show that this 16-to-1.58-bit training strategy is preferable over full 1.58-bit training and leaves models closer to those which have undergone 16-bit training. We further investigate the effects of retaining the optimizer state at the transition point and gradually phasing in quantization strength -- finding that both techniques alleviate the magnitude of loss spikes, but also that these effects can be compensated through further training.",
      "arxiv_url": "https://arxiv.org/abs/2502.11895",
      "pdf_url": "https://arxiv.org/pdf/2502.11895",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.02226",
      "title": "Knowledge Graph Retrieval-Augmented Generation for LLM-based Recommendation",
      "authors": [
        "Shijie Wang",
        "Wenqi Fan",
        "Yue Feng",
        "Xinyu Ma",
        "Shuaiqiang Wang",
        "Dawei Yin"
      ],
      "abstract": "Recommender systems have become increasingly vital in our daily lives, helping to alleviate the problem of information overload across various user-oriented online services. The emergence of Large Language Models (LLMs) has yielded remarkable achievements, demonstrating their potential for the development of next-generation recommender systems. Despite these advancements, LLM-based recommender systems face inherent limitations stemming from their LLM backbones, particularly issues of hallucinations and the lack of up-to-date and domain-specific knowledge. Recently, Retrieval-Augmented Generation (RAG) has garnered significant attention for addressing these limitations by leveraging external knowledge sources to enhance the understanding and generation of LLMs. However, vanilla RAG methods often introduce noise and neglect structural relationships in knowledge, limiting their effectiveness in LLM-based recommendations. To address these limitations, we propose to retrieve high-quality and up-to-date structure information from the knowledge graph (KG) to augment recommendations. Specifically, our approach develops a retrieval-augmented framework, termed K-RagRec, that facilitates the recommendation generation process by incorporating structure information from the external KG. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed method.",
      "arxiv_url": "https://arxiv.org/abs/2501.02226",
      "pdf_url": "https://arxiv.org/pdf/2501.02226",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-01-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20968",
      "title": "Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs",
      "authors": [
        "Weixiang Zhao",
        "Yulin Hu",
        "Yang Deng",
        "Jiahe Guo",
        "Xingyu Sui",
        "Xinyang Han",
        "An Zhang",
        "Yanyan Zhao",
        "Bing Qin",
        "Tat-Seng Chua",
        "Ting Liu"
      ],
      "abstract": "Role-playing enables large language models (LLMs) to engage users in immersive and personalized interactions, but it also introduces significant safety risks. Existing role-play fine-tuning techniques improve role adaptability but may degrade safety performance, particularly for villainous characters. In this work, we conduct the first comprehensive assessment of role-play fine-tuning risks by training 95 role-specific LLMs using RoleBench. Our experiments reveal that role-play fine-tuning leads to a noticeable decline in safety performance, with safety risks varying based on character traits. To tackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a novel method designed to balance role-playing capabilities and safety. Extensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and Qwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms state-of-the-art baselines under both LoRA and full-parameter fine-tuning settings. Our findings highlight the necessity of role-adaptive safety measures and provide insights into mitigating role-specific safety risks in role-playing LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2502.20968",
      "pdf_url": "https://arxiv.org/pdf/2502.20968",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-02-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12028",
      "title": "Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method",
      "authors": [
        "Yupei Ren",
        "Xinyi Zhou",
        "Ning Zhang",
        "Shangqing Zhao",
        "Man Lan",
        "Xiaopeng Bai"
      ],
      "abstract": "Argument mining has garnered increasing attention over the years, with the recent advancement of Large Language Models (LLMs) further propelling this trend. However, current argument relations remain relatively simplistic and foundational, struggling to capture the full scope of argument information, particularly when it comes to representing complex argument structures in real-world scenarios. To address this limitation, we propose 14 fine-grained relation types from both vertical and horizontal dimensions, thereby capturing the intricate interplay between argument components for a thorough understanding of argument structure. On this basis, we conducted extensive experiments on three tasks: argument component detection, relation prediction, and automated essay grading. Additionally, we explored the impact of writing quality on argument component detection and relation prediction, as well as the connections between discourse relations and argumentative features. The findings highlight the importance of fine-grained argumentative annotations for argumentative writing quality assessment and encourage multi-dimensional argument analysis.",
      "arxiv_url": "https://arxiv.org/abs/2505.12028",
      "pdf_url": "https://arxiv.org/pdf/2505.12028",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3f80b716d438b361bd9d318e2c8b3305350f08f5",
      "title": "ISR: Self-Refining Referring Expressions for Entity Grounding",
      "authors": [
        "Zhuocheng Yu",
        "Bingchan Zhao",
        "Yifan Song",
        "Sujian Li",
        "Zhonghui He"
      ],
      "abstract": "Entity grounding, a crucial task in constructing multimodal knowledge graphs, aims to align entities from knowledge graphs with their corresponding images. Unlike conventional visual grounding tasks that use referring expressions (REs) as inputs, entity grounding relies solely on entity names and types, presenting a significant challenge. To address this, we introduce a novel I terative S elf-R efinement ( ISR ) scheme to enhance the multimodal large language model’s capability to generate high quality REs for the given entities as explicit contextual clues. This training scheme, inspired by human learning dynamics and human annotation processes, enables the MLLM to iteratively generate and refine REs by learning from successes and failures, guided by outcome re-wards from a visual grounding model. This iterative cycle of self-refinement avoids overfitting to fixed annotations and fosters continued improvement in referring expression generation. Extensive experiments demonstrate that our methods surpasses other methods in entity grounding, highlighting its effectiveness, robustness and potential for broader applications 1 .",
      "arxiv_url": "https://www.semanticscholar.org/paper/3f80b716d438b361bd9d318e2c8b3305350f08f5",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.18901",
      "title": "REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?",
      "authors": [
        "Chuxuan Hu",
        "Liyun Zhang",
        "Yeji Lim",
        "Aum Wadhwani",
        "Austin Peters",
        "Daniel Kang"
      ],
      "abstract": "Assessing the reproducibility of social science papers is essential for promoting rigor in research processes, but manual assessment is costly. With recent advances in agentic AI systems (i.e., AI agents), we seek to evaluate their capability to automate this process. However, existing benchmarks for reproducing research papers (1) focus solely on reproducing results using provided code and data without assessing their consistency with the paper, (2) oversimplify real-world scenarios, and (3) lack necessary diversity in data formats and programming languages. To address these issues, we introduce REPRO-Bench, a collection of 112 task instances, each representing a social science paper with a publicly available reproduction report. The agents are tasked with assessing the reproducibility of the paper based on the original paper PDF and the corresponding reproduction package. REPRO-Bench features end-to-end evaluation tasks on the reproducibility of social science papers with complexity comparable to real-world assessments. We evaluate three representative AI agents on REPRO-Bench, with the best-performing agent achieving an accuracy of only 21.4%. Building on our empirical analysis, we develop REPRO-Agent, which improves the highest accuracy achieved by existing agents by 71%. We conclude that more advanced AI agents should be developed to automate real-world reproducibility assessment. REPRO-Bench is publicly available at https://github.com/uiuc-kang-lab/REPRO-Bench.",
      "arxiv_url": "https://arxiv.org/abs/2507.18901",
      "pdf_url": "https://arxiv.org/pdf/2507.18901",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2025-07-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3fb5ac41e3d8195dff2afc31cf88f02676da848e",
      "title": "Enhancing Machine Translation with Self-Supervised Preference Data",
      "authors": [
        "Haoxiang Sun",
        "Ruize Gao",
        "Pei Zhang",
        "Baosong Yang",
        "Rui Wang"
      ],
      "abstract": "Model alignment methods like Direct Preference Optimization (Rafailov et al., 2024) and Contrastive Preference Optimization (Xu et al., 2024b) have enhanced machine translation performance by leveraging preference data to enable models to reject suboptimal outputs. During preference data construction, previous approaches primarily rely on humans, strong models like GPT4 (OpenAI, 2023) or model self-sampling. In this study, we ﬁrst explain the shortcomings of this practice. Then, we propose Self-Supervised Preference Optimization (SSPO) , a novel framework which efﬁciently constructs translation preference data for iterative DPO training. Applying SSPO to 14B parameters large language models (LLMs) achieves comparable or better performance than GPT-4o on FLO-RES and multi-domain test datasets. We release an augmented MQM dataset in https: //github.com/sunny-sjtu/MQM-aug .",
      "arxiv_url": "https://www.semanticscholar.org/paper/3fb5ac41e3d8195dff2afc31cf88f02676da848e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "3fb75bed639c9f04bb7219dff2c45762ff81da43",
      "title": "RAG-Critic: Leveraging Automated Critic-Guided Agentic Workflow for Retrieval Augmented Generation",
      "authors": [
        "Guanting Dong",
        "Jiajie Jin",
        "Xiaoxi Li",
        "Yutao Zhu",
        "Zhicheng Dou",
        "Ji-Rong Wen"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has emerged as a pivotal technology in natural language processing, owing to its efficacy in generating factual content. However, its informative inputs and complex paradigms often lead to a greater variety of errors. Consequently, achieving automated on-policy assessment and error-oriented correction remains an unresolved issue. In this paper, we propose RAG-Critic, a novel framework that leverages a critic-guided agentic workflow to improve RAG capabilities autonomously. Specifically, we initially design a data-driven error mining pipeline to establish a hierarchical RAG error system. Based on this system, we progressively align an error-critic model using a coarse-to-fine training objective, which automatically provides fine-grained error feedback. Finally, we design a critic-guided agentic RAG workflow that cus-tomizes executor-based solution flows based on the error-critic model’s feedback, facilitating an error-driven self-correction process. Experimental results across seven RAG-related datasets confirm the effectiveness of RAG-Critic, while qualitative analysis offers practical insights for achieving reliable RAG systems. Our dataset and code are available at https: //github.com/RUC-NLPIR/RAG-Critic .",
      "arxiv_url": "https://www.semanticscholar.org/paper/3fb75bed639c9f04bb7219dff2c45762ff81da43",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24354",
      "title": "Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research",
      "authors": [
        "Qianqian Zhang",
        "Jiajia Liao",
        "Heting Ying",
        "Yibo Ma",
        "Haozhan Shen",
        "Jingcheng Li",
        "Peng Liu",
        "Lu Zhang",
        "Chunxin Fang",
        "Kyusong Lee",
        "Ruochen Xu",
        "Tiancheng Zhao"
      ],
      "abstract": "Language agents powered by large language models (LLMs) have demonstrated remarkable capabilities in understanding, reasoning, and executing complex tasks. However, developing robust agents presents significant challenges: substantial engineering overhead, lack of standardized components, and insufficient evaluation frameworks for fair comparison. We introduce Agent Graph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and extensible framework that addresses these challenges through three key contributions: (1) a modular architecture with a graph-based workflow engine, efficient memory management, and clean component abstraction; (2) a comprehensive suite of reusable agent algorithms implementing state-of-the-art reasoning approaches; and (3) a rigorous evaluation framework enabling systematic comparison across multiple dimensions. Through extensive experiments on mathematical reasoning and multimodal tasks, we evaluate various agent algorithms across different LLMs, revealing important insights about their relative strengths and applicability. Our results demonstrate that while sophisticated reasoning approaches can enhance agent capabilities, simpler methods like Chain-of-Thought often exhibit robust performance with significantly lower computational overhead. AGORA not only simplifies language agent development but also establishes a foundation for reproducible agent research through standardized evaluation protocols.",
      "arxiv_url": "https://arxiv.org/abs/2505.24354",
      "pdf_url": "https://arxiv.org/pdf/2505.24354",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2511.18889",
      "title": "CoreEval: Automatically Building Contamination-Resilient Datasets with Real-World Knowledge toward Reliable LLM Evaluation",
      "authors": [
        "Jingqian Zhao",
        "Bingbing Wang",
        "Geng Tu",
        "Yice Zhang",
        "Qianlong Wang",
        "Bin Liang",
        "Jing Li",
        "Ruifeng Xu"
      ],
      "abstract": "Data contamination poses a significant challenge to the fairness of LLM evaluations in natural language processing tasks by inadvertently exposing models to test data during training. Current studies attempt to mitigate this issue by modifying existing datasets or generating new ones from freshly collected information. However, these methods fall short of ensuring contamination-resilient evaluation, as they fail to fully eliminate pre-existing knowledge from models or preserve the semantic complexity of the original datasets. To address these limitations, we propose \\textbf{CoreEval}, a \\textbf{Co}ntamination-\\textbf{re}silient \\textbf{Eval}uation strategy for automatically updating data with real-world knowledge. This approach begins by extracting entity relationships from the original data and leveraging the GDELT database to retrieve relevant, up-to-date knowledge. The retrieved knowledge is then recontextualized and integrated with the original data, which is refined and restructured to ensure semantic coherence and enhanced task relevance. Ultimately, a robust data reflection mechanism is employed to iteratively verify and refine labels, ensuring consistency between the updated and original datasets. Extensive experiments on updated datasets validate the robustness of CoreEval, demonstrating its effectiveness in mitigating performance overestimation caused by data contamination.",
      "arxiv_url": "https://arxiv.org/abs/2511.18889",
      "pdf_url": "https://arxiv.org/pdf/2511.18889",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-11-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12361",
      "title": "ConFit v2: Improving Resume-Job Matching using Hypothetical Resume Embedding and Runner-Up Hard-Negative Mining",
      "authors": [
        "Xiao Yu",
        "Ruize Xu",
        "Chengyuan Xue",
        "Jinzhong Zhang",
        "Xu Ma",
        "Zhou Yu"
      ],
      "abstract": "A reliable resume-job matching system helps a company recommend suitable candidates from a pool of resumes and helps a job seeker find relevant jobs from a list of job posts. However, since job seekers apply only to a few jobs, interaction labels in resume-job datasets are sparse. We introduce ConFit v2, an improvement over ConFit to tackle this sparsity problem. We propose two techniques to enhance the encoder's contrastive training process: augmenting job data with hypothetical reference resume generated by a large language model; and creating high-quality hard negatives from unlabeled resume/job pairs using a novel hard-negative mining strategy. We evaluate ConFit v2 on two real-world datasets and demonstrate that it outperforms ConFit and prior methods (including BM25 and OpenAI text-embedding-003), achieving an average absolute improvement of 13.8% in recall and 17.5% in nDCG across job-ranking and resume-ranking tasks.",
      "arxiv_url": "https://arxiv.org/abs/2502.12361",
      "pdf_url": "https://arxiv.org/pdf/2502.12361",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.02951",
      "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
      "authors": [
        "Zhangchen Xu",
        "Yang Liu",
        "Yueqin Yin",
        "Mingyuan Zhou",
        "Radha Poovendran"
      ],
      "abstract": "We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of coverage (e.g., spanning simple coding tasks to advanced algorithmic problems) or verifiable correctness (e.g., unit tests). In contrast, KodCode comprises question-solution-test triplets that are systematically validated via a self-verification procedure. Our pipeline begins by synthesizing a broad range of coding questions, then generates solutions and test cases with additional attempts allocated to challenging problems. Finally, post-training data synthesis is done by rewriting questions into diverse formats and generating responses under a test-based reject sampling procedure from a reasoning model (DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding dataset. KodCode is suitable for supervised fine-tuning and the paired unit tests also provide great potential for RL tuning. Fine-tuning experiments on coding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench) demonstrate that KodCode-tuned models achieve state-of-the-art performance, surpassing models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B.",
      "arxiv_url": "https://arxiv.org/abs/2503.02951",
      "pdf_url": "https://arxiv.org/pdf/2503.02951",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12435",
      "title": "SGDPO: Self-Guided Direct Preference Optimization for Language Model Alignment",
      "authors": [
        "Wenqiao Zhu",
        "Ji Liu",
        "Lulu Wang",
        "Jun Wu",
        "Yulun Zhang"
      ],
      "abstract": "Direct Preference Optimization (DPO) is broadly utilized for aligning Large Language Models (LLMs) with human values because of its flexibility. Despite its effectiveness, it has been observed that the capability of DPO to generate human-preferred response is limited and the results of DPO are far from resilient. To address these limitations, in this paper we propose a novel Self-Guided Direct Preference Optimization algorithm, i.e., SGDPO, which incorporates a pilot term to steer the gradient flow during the optimization process, allowing for fine-grained control over the updates of chosen and rejected rewards. We provide a detailed theoretical analysis of our proposed method and elucidate its operational mechanism. Furthermore, we conduct comprehensive experiments on various models and benchmarks. The extensive experimental results demonstrate the consistency between the empirical results and our theoretical analysis and confirm the effectiveness of our proposed approach (up to 9.19% higher score).",
      "arxiv_url": "https://arxiv.org/abs/2505.12435",
      "pdf_url": "https://arxiv.org/pdf/2505.12435",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.06868",
      "title": "Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games",
      "authors": [
        "Seungwon Lim",
        "Seungbeen Lee",
        "Dongjun Min",
        "Youngjae Yu"
      ],
      "abstract": "Artificial agents are increasingly central to complex interactions and decision-making tasks, yet aligning their behaviors with desired human values remains an open challenge. In this work, we investigate how human-like personality traits influence agent behavior and performance within text-based interactive environments. We introduce PANDA: Personality Adapted Neural Decision Agents, a novel method for projecting human personality traits onto agents to guide their behavior. To induce personality in a text-based game agent, (i) we train a personality classifier to identify what personality type the agent's actions exhibit, and (ii) we integrate the personality profiles directly into the agent's policy-learning pipeline. By deploying agents embodying 16 distinct personality types across 25 text-based games and analyzing their trajectories, we demonstrate that an agent's action decisions can be guided toward specific personality profiles. Moreover, certain personality types, such as those characterized by higher levels of Openness, display marked advantages in performance. These findings underscore the promise of personality-adapted agents for fostering more aligned, effective, and human-centric decision-making in interactive environments.",
      "arxiv_url": "https://arxiv.org/abs/2504.06868",
      "pdf_url": "https://arxiv.org/pdf/2504.06868",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.08651",
      "title": "Scaling Context, Not Parameters: Training a Compact 7B Language Model for Efficient Long-Context Processing",
      "authors": [
        "Chen Wu",
        "Yin Song"
      ],
      "abstract": "We present MegaBeam-Mistral-7B, a language model that supports 512K-token context length. Our work addresses practical limitations in long-context training, supporting real-world tasks such as compliance monitoring and verification. Evaluated on three long-context benchmarks, our 7B-parameter model demonstrates superior in-context learning performance on HELMET and robust retrieval and tracing capability on RULER. It is currently the only open model to achieve competitive long-range reasoning on BABILong at 512K context length without RAG or targeted fine-tuning. Released as fully open source under the Apache 2.0 license, the model has been downloaded over 100,000 times on Hugging Face. Model available at: https://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k",
      "arxiv_url": "https://arxiv.org/abs/2505.08651",
      "pdf_url": "https://arxiv.org/pdf/2505.08651",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-05-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19293",
      "title": "100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?",
      "authors": [
        "Wang Yang",
        "Hongye Jin",
        "Shaochen Zhong",
        "Song Jiang",
        "Qifan Wang",
        "Vipin Chaudhary",
        "Xiaotian Han"
      ],
      "abstract": "Long-context capability is considered one of the most important abilities of LLMs, as a truly long context-capable LLM enables users to effortlessly process many originally exhausting tasks -- e.g., digesting a long-form document to find answers vs. directly asking an LLM about it. However, existing real-task-based long-context evaluation benchmarks have two major shortcomings. First, benchmarks like LongBench often do not provide proper metrics to separate long-context performance from the model's baseline ability, making cross-model comparison unclear. Second, such benchmarks are usually constructed with fixed input lengths, which limits their applicability across different models and fails to reveal when a model begins to break down. To address these issues, we introduce a length-controllable long-context benchmark and a novel metric that disentangles baseline knowledge from true long-context capabilities. Experiments demonstrate the superiority of our approach in effectively evaluating LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2505.19293",
      "pdf_url": "https://arxiv.org/pdf/2505.19293",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-05-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.19328",
      "title": "Substance over Style: Evaluating Proactive Conversational Coaching Agents",
      "authors": [
        "Vidya Srinivas",
        "Xuhai Xu",
        "Xin Liu",
        "Kumar Ayush",
        "Isaac R. Galatzer-Levy",
        "Shwetak N. Patel",
        "D. McDuff",
        "Tim Althoff"
      ],
      "abstract": "While NLP research has made strides in conversational tasks, many approaches focus on single-turn responses with well-defined objectives or evaluation criteria. In contrast, coaching presents unique challenges with initially undefined goals that evolve through multi-turn interactions, subjective evaluation criteria, mixed-initiative dialogue. In this work, we describe and implement five multi-turn coaching agents that exhibit distinct conversational styles, and evaluate them through a user study, collecting first-person feedback on 155 conversations. We find that users highly value core functionality, and that stylistic components in absence of core components are viewed negatively. By comparing user feedback with third-person evaluations from health experts and an LM, we reveal significant misalignment across evaluation approaches. Our findings provide insights into design and evaluation of conversational coaching agents and contribute toward improving human-centered NLP applications.",
      "arxiv_url": "https://arxiv.org/abs/2503.19328",
      "pdf_url": "https://arxiv.org/pdf/2503.19328",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.13844",
      "title": "Improve Language Model and Brain Alignment via Associative Memory",
      "authors": [
        "Congchi Yin",
        "Yongpeng Zhang",
        "Xuyun Wen",
        "Piji Li"
      ],
      "abstract": "Associative memory engages in the integration of relevant information for comprehension in the human cognition system. In this work, we seek to improve alignment between language models and human brain while processing speech information by integrating associative memory. After verifying the alignment between language model and brain by mapping language model activations to brain activity, the original text stimuli expanded with simulated associative memory are regarded as input to computational language models. We find the alignment between language model and brain is improved in brain regions closely related to associative memory processing. We also demonstrate large language models after specific supervised fine-tuning better align with brain response, by building the \\textit{Association} dataset containing 1000 samples of stories, with instructions encouraging associative memory as input and associated content as output.",
      "arxiv_url": "https://arxiv.org/abs/2505.13844",
      "pdf_url": "https://arxiv.org/pdf/2505.13844",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.01491",
      "title": "Memorization Inheritance in Sequence-Level Knowledge Distillation for Neural Machine Translation",
      "authors": [
        "Verna Dankers",
        "Vikas Raunak"
      ],
      "abstract": "In this work, we explore how instance-level memorization in the teacher Neural Machine Translation (NMT) model gets inherited by the student model in sequence-level knowledge distillation (SeqKD). We find that despite not directly seeing the original training data, students memorize more than baseline models (models of the same size, trained on the original data) -- 3.4% for exact matches and 57% for extractive memorization -- and show increased hallucination rates. Further, under this SeqKD setting, we also characterize how students behave on specific training data subgroups, such as subgroups with low quality and specific counterfactual memorization (CM) scores, and find that students exhibit amplified denoising on low-quality subgroups. Finally, we propose a modification to SeqKD named Adaptive-SeqKD, which intervenes in SeqKD to reduce memorization and hallucinations. Overall, we recommend caution when applying SeqKD: students inherit both their teachers'superior performance and their fault modes, thereby requiring active monitoring.",
      "arxiv_url": "https://arxiv.org/abs/2502.01491",
      "pdf_url": "https://arxiv.org/pdf/2502.01491",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19187",
      "title": "BIG-Bench Extra Hard",
      "authors": [
        "M. kazemi",
        "Bahare Fatemi",
        "Hritik Bansal",
        "John Palowitch",
        "Chrysovalantis Anastasiou",
        "Sanket Vaibhav Mehta",
        "Lalit K. Jain",
        "Virginia Aglietti",
        "Disha Jindal",
        "Peter Chen",
        "Nishanth Dikkala",
        "Gladys Tyen",
        "Xin Liu",
        "Uri Shalit",
        "S. Chiappa",
        "Kate Olszewska",
        "Yi Tay",
        "Vinh Q. Tran",
        "Quoc V. Le",
        "Orhan Firat"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in everyday applications, demanding robust general reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning benchmarks predominantly focus on mathematical and coding abilities, leaving a gap in evaluating broader reasoning proficiencies. One particular exception is the BIG-Bench dataset, which has served as a crucial benchmark for evaluating the general reasoning capabilities of LLMs, thanks to its diverse set of challenging tasks that allowed for a comprehensive assessment of general reasoning across various skills within a unified framework. However, recent advances in LLMs have led to saturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH). State-of-the-art models achieve near-perfect scores on many tasks in BBH, thus diminishing its utility. To address this limitation, we introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly increased difficulty. We evaluate various models on BBEH and observe a (harmonic) average accuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best reasoning-specialized model, indicating substantial room for improvement and highlighting the ongoing challenge of achieving robust general reasoning in LLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.",
      "arxiv_url": "https://arxiv.org/abs/2502.19187",
      "pdf_url": "https://arxiv.org/pdf/2502.19187",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "407ee2739eb834145ec2eb4369d67c4f0b6cf452",
      "title": "Can Large Language Models Understand Argument Schemes?",
      "authors": [
        "Elfia Bezou-Vrakatseli",
        "O. Cocarascu",
        "Sanjay Modgil"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/407ee2739eb834145ec2eb4369d67c4f0b6cf452",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06605",
      "title": "MedCite: Can Language Models Generate Verifiable Text for Medicine?",
      "authors": [
        "Xiao Wang",
        "Mengjue Tan",
        "Qiao Jin",
        "Guangzhi Xiong",
        "Yu Hu",
        "Aidong Zhang",
        "Zhiyong Lu",
        "Minjia Zhang"
      ],
      "abstract": "Existing LLM-based medical question-answering systems lack citation generation and evaluation capabilities, raising concerns about their adoption in practice. In this work, we introduce \\name, the first end-to-end framework that facilitates the design and evaluation of citation generation with LLMs for medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation method that generates high-quality citations. Our evaluation highlights the challenges and opportunities of citation generation for medical tasks, while identifying important design choices that have a significant impact on the final citation quality. Our proposed method achieves superior citation precision and recall improvements compared to strong baseline methods, and we show that evaluation results correlate well with annotation results from professional experts.",
      "arxiv_url": "https://arxiv.org/abs/2506.06605",
      "pdf_url": "https://arxiv.org/pdf/2506.06605",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12616",
      "title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
      "authors": [
        "Leonardo Ranaldi",
        "Marco Valentino",
        "Alexander Polonsky",
        "André Freitas"
      ],
      "abstract": "Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).",
      "arxiv_url": "https://arxiv.org/abs/2502.12616",
      "pdf_url": "https://arxiv.org/pdf/2502.12616",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12904",
      "title": "Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM Against Augmented Fraud and Phishing Inducements",
      "authors": [
        "Shu Yang",
        "Shenzhe Zhu",
        "Zeyu Wu",
        "Keyu Wang",
        "Junchi Yao",
        "Junchao Wu",
        "Lijie Hu",
        "Mengdi Li",
        "Derek Wong",
        "Di Wang"
      ],
      "abstract": "We introduce Fraud-R1, a benchmark designed to evaluate LLMs' ability to defend against internet fraud and phishing in dynamic, real-world scenarios. Fraud-R1 comprises 8,564 fraud cases sourced from phishing scams, fake job postings, social media, and news, categorized into 5 major fraud types. Unlike previous benchmarks, Fraud-R1 introduces a multi-round evaluation pipeline to assess LLMs' resistance to fraud at different stages, including credibility building, urgency creation, and emotional manipulation. Furthermore, we evaluate 15 LLMs under two settings: 1. Helpful-Assistant, where the LLM provides general decision-making assistance, and 2. Role-play, where the model assumes a specific persona, widely used in real-world agent-based interactions. Our evaluation reveals the significant challenges in defending against fraud and phishing inducement, especially in role-play settings and fake job postings. Additionally, we observe a substantial performance gap between Chinese and English, underscoring the need for improved multilingual fraud detection capabilities.",
      "arxiv_url": "https://arxiv.org/abs/2502.12904",
      "pdf_url": "https://arxiv.org/pdf/2502.12904",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.14883",
      "title": "Verify with Caution: The Pitfalls of Relying on Imperfect Factuality Metrics",
      "authors": [
        "Ameya Godbole",
        "Robin Jia"
      ],
      "abstract": "Improvements in large language models have led to increasing optimism that they can serve as reliable evaluators of natural language generation outputs. In this paper, we challenge this optimism by thoroughly re-evaluating five state-of-the-art factuality metrics on a collection of 11 datasets for summarization, retrieval-augmented generation, and question answering. We find that these evaluators are inconsistent with each other and often misestimate system-level performance, both of which can lead to a variety of pitfalls. We further show that these metrics exhibit biases against highly paraphrased outputs and outputs that draw upon faraway parts of the source documents. We urge users of these factuality metrics to proceed with caution and manually validate the reliability of these metrics in their domain of interest before proceeding.",
      "arxiv_url": "https://arxiv.org/abs/2501.14883",
      "pdf_url": "https://arxiv.org/pdf/2501.14883",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-01-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05000",
      "title": "SCOP: Evaluating the Comprehension Process of Large Language Models from a Cognitive View",
      "authors": [
        "Yongjie Xiao",
        "Hongru Liang",
        "Peixin Qin",
        "Yao Zhang",
        "Wenqiang Lei"
      ],
      "abstract": "Despite the great potential of large language models(LLMs) in machine comprehension, it is still disturbing to fully count on them in real-world scenarios. This is probably because there is no rational explanation for whether the comprehension process of LLMs is aligned with that of experts. In this paper, we propose SCOP to carefully examine how LLMs perform during the comprehension process from a cognitive view. Specifically, it is equipped with a systematical definition of five requisite skills during the comprehension process, a strict framework to construct testing data for these skills, and a detailed analysis of advanced open-sourced and closed-sourced LLMs using the testing data. With SCOP, we find that it is still challenging for LLMs to perform an expert-level comprehension process. Even so, we notice that LLMs share some similarities with experts, e.g., performing better at comprehending local information than global information. Further analysis reveals that LLMs can be somewhat unreliable -- they might reach correct answers through flawed comprehension processes. Based on SCOP, we suggest that one direction for improving LLMs is to focus more on the comprehension process, ensuring all comprehension skills are thoroughly developed during training.",
      "arxiv_url": "https://arxiv.org/abs/2506.05000",
      "pdf_url": "https://arxiv.org/pdf/2506.05000",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00536",
      "title": "Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing",
      "authors": [
        "Changyue Wang",
        "Weihang Su",
        "Qingyao Ai",
        "Yujia Zhou",
        "Yiqun Liu"
      ],
      "abstract": "Knowledge editing aims to efficiently update Large Language Models (LLMs) by modifying specific knowledge without retraining the entire model. Among knowledge editing approaches, in-context editing (ICE) offers a lightweight solution by injecting new knowledge directly into the input context, leaving model parameters unchanged. However, existing ICE approaches do not explicitly separate the newly injected knowledge from the model's original reasoning process. This entanglement often results in conflicts between external updates and internal parametric knowledge, undermining the consistency and accuracy of the reasoning path.In this work, we conduct preliminary experiments to examine how parametric knowledge influences reasoning path planning. We find that the model's reasoning is tightly coupled with its internal knowledge, and that naively injecting new information without adapting the reasoning path often leads to performance degradation, particularly in multi-hop tasks. To this end, we propose DecKER, a novel ICE framework that decouples reasoning from knowledge editing by generating a masked reasoning path and then resolving knowledge edits via hybrid retrieval and model-based validation. Experiments on multi-hop QA benchmarks show that DecKER significantly outperforms existing ICE methods by mitigating knowledge conflicts and preserving reasoning consistency. Our code is available at: https://github.com/bebr2/DecKER .",
      "arxiv_url": "https://arxiv.org/abs/2506.00536",
      "pdf_url": "https://arxiv.org/pdf/2506.00536",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00649",
      "title": "GuideX: Guided Synthetic Data Generation for Zero-Shot Information Extraction",
      "authors": [
        "Neil De La Fuente",
        "Oscar Sainz",
        "Iker Garc'ia-Ferrero",
        "Eneko Agirre"
      ],
      "abstract": "Information Extraction (IE) systems are traditionally domain-specific, requiring costly adaptation that involves expert schema design, data annotation, and model training. While Large Language Models have shown promise in zero-shot IE, performance degrades significantly in unseen domains where label definitions differ. This paper introduces GUIDEX, a novel method that automatically defines domain-specific schemas, infers guidelines, and generates synthetically labeled instances, allowing for better out-of-domain generalization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art across seven zeroshot Named Entity Recognition benchmarks. Models trained with GUIDEX gain up to 7 F1 points over previous methods without humanlabeled data, and nearly 2 F1 points higher when combined with it. Models trained on GUIDEX demonstrate enhanced comprehension of complex, domain-specific annotation schemas. Code, models, and synthetic datasets are available at neilus03.github.io/guidex.com",
      "arxiv_url": "https://arxiv.org/abs/2506.00649",
      "pdf_url": "https://arxiv.org/pdf/2506.00649",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00151",
      "title": "Palm: A Culturally Inclusive and Linguistically Diverse Dataset for Arabic LLMs",
      "authors": [
        "Fakhraddin Alwajih",
        "Abdellah El Mekki",
        "S. Magdy",
        "AbdelRahim Elmadany",
        "Omer Nacar",
        "El Moatez Billah Nagoudi",
        "Reem Abdel-Salam",
        "Hanin Atwany",
        "Youssef Nafea",
        "Abdulfattah Mohammed Yahya",
        "Rahaf Alhamouri",
        "Hamzah A. Alsayadi",
        "Hiba Zayed",
        "Sara Shatnawi",
        "Serry Sibaee",
        "Yasir Ech-Chammakhy",
        "Walid Al-Dhabyani",
        "Marwa Mohamed Ali",
        "Imen Jarraya",
        "Ahmed Oumar El-Shangiti",
        "A. Alraeesi",
        "Mohammed Anwar Al-Ghrawi",
        "Abdulrahman S. Al-Batati",
        "Elgizouli Mohamed",
        "Noha Elgindi",
        "Muhammed Saeed",
        "Houdaifa Atou",
        "Issam Ait Yahia",
        "Abdelhak Bouayad",
        "Mohammed Machrouh",
        "Amal Makouar",
        "Dania Alkawi",
        "Mukhtar Mohamed",
        "Safaa Abdelfadil",
        "Amine Ziad Ounnoughene",
        "Rouabhia Anfel",
        "Rwaa Assi",
        "Ahmed Sorkatti",
        "Mohamedou Cheikh Tourad",
        "Anis Koubaa",
        "Ismail Berrada",
        "Mustafa Jarrar",
        "Shady Shehata",
        "M. Abdul-Mageed"
      ],
      "abstract": "As large language models (LLMs) become increasingly integrated into daily life, ensuring their cultural sensitivity and inclusivity is paramount. We introduce our dataset, a year-long community-driven project covering all 22 Arab countries. The dataset includes instructions (input, response pairs) in both Modern Standard Arabic (MSA) and dialectal Arabic (DA), spanning 20 diverse topics. Built by a team of 44 researchers across the Arab world, all of whom are authors of this paper, our dataset offers a broad, inclusive perspective. We use our dataset to evaluate the cultural and dialectal capabilities of several frontier LLMs, revealing notable limitations. For instance, while closed-source LLMs generally exhibit strong performance, they are not without flaws, and smaller open-source models face greater challenges. Moreover, certain countries (e.g., Egypt, the UAE) appear better represented than others (e.g., Iraq, Mauritania, Yemen). Our annotation guidelines, code, and data for reproducibility are publicly available.",
      "arxiv_url": "https://arxiv.org/abs/2503.00151",
      "pdf_url": "https://arxiv.org/pdf/2503.00151",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01074",
      "title": "How Programming Concepts and Neurons Are Shared in Code Language Models",
      "authors": [
        "Amir Hossein Kargaran",
        "Yihong Liu",
        "Franccois Yvon",
        "Hinrich Schutze"
      ],
      "abstract": "Several studies have explored the mechanisms of large language models (LLMs) in coding tasks, but most have focused on programming languages (PLs) in a monolingual setting. In this paper, we investigate the relationship between multiple PLs and English in the concept space of LLMs. We perform a few-shot translation task on 21 PL pairs using two Llama-based models. By decoding the embeddings of intermediate layers during this task, we observe that the concept space is closer to English (including PL keywords) and assigns high probabilities to English tokens in the second half of the intermediate layers. We analyze neuron activations for 11 PLs and English, finding that while language-specific neurons are primarily concentrated in the bottom layers, those exclusive to each PL tend to appear in the top layers. For PLs that are highly aligned with multiple other PLs, identifying language-specific neurons is not feasible. These PLs also tend to have a larger keyword set than other PLs and are closer to the model's concept space regardless of the input/output PL in the translation task. Our findings provide insights into how LLMs internally represent PLs, revealing structural patterns in the model's concept space. Code is available at https://github.com/cisnlp/code-specific-neurons.",
      "arxiv_url": "https://arxiv.org/abs/2506.01074",
      "pdf_url": "https://arxiv.org/pdf/2506.01074",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04461",
      "title": "Behavioural vs. Representational Systematicity in End-to-End Models: An Opinionated Survey",
      "authors": [
        "Ivan Vegner",
        "Sydelle de Souza",
        "Valentin Forch",
        "Martha Lewis",
        "Leonidas A. A. Doumas"
      ],
      "abstract": "A core aspect of compositionality, systematicity is a desirable property in ML models as it enables strong generalization to novel contexts. This has led to numerous studies proposing benchmarks to assess systematic generalization, as well as models and training regimes designed to enhance it. Many of these efforts are framed as addressing the challenge posed by Fodor and Pylyshyn. However, while they argue for systematicity of representations, existing benchmarks and models primarily focus on the systematicity of behaviour. We emphasize the crucial nature of this distinction. Furthermore, building on Hadley's (1994) taxonomy of systematic generalization, we analyze the extent to which behavioural systematicity is tested by key benchmarks in the literature across language and vision. Finally, we highlight ways of assessing systematicity of representations in ML models as practiced in the field of mechanistic interpretability.",
      "arxiv_url": "https://arxiv.org/abs/2506.04461",
      "pdf_url": "https://arxiv.org/pdf/2506.04461",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14727",
      "title": "WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models",
      "authors": [
        "Yifu Chen",
        "Shengpeng Ji",
        "Haoxiao Wang",
        "Ziqing Wang",
        "Siyu Chen",
        "Jinzheng He",
        "Jin Xu",
        "Zhou Zhao"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) has gained widespread adoption owing to its capacity to empower large language models (LLMs) to integrate external knowledge. However, existing RAG frameworks are primarily designed for text-based LLMs and rely on Automatic Speech Recognition to process speech input, which discards crucial audio information, risks transcription errors, and increases computational overhead. Therefore, we introduce WavRAG, the first retrieval augmented generation framework with native, end-to-end audio support. WavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw audio for both embedding and retrieval. 2) WavRAG integrates audio and text into a unified knowledge representation. Specifically, we propose the WavRetriever to facilitate the retrieval from a text-audio hybrid knowledge base, and further enhance the in-context capabilities of spoken dialogue models through the integration of chain-of-thought reasoning. In comparison to state-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval performance while delivering a 10x acceleration. Furthermore, WavRAG's unique text-audio hybrid retrieval capability extends the boundaries of RAG to the audio modality.",
      "arxiv_url": "https://arxiv.org/abs/2502.14727",
      "pdf_url": "https://arxiv.org/pdf/2502.14727",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11423",
      "title": "Exploring Persona Sentiment Sensitivity in Personalized Dialogue Generation",
      "authors": [
        "Yonghyun Jun",
        "Hwanhee Lee"
      ],
      "abstract": "Personalized dialogue systems have advanced considerably with the integration of user-specific personas into large language models (LLMs). However, while LLMs can effectively generate personalized responses, the influence of persona sentiment on dialogue quality remains underexplored. In this work, we conduct a large-scale analysis of dialogues generated using a range of polarized user profiles. Our experiments reveal that dialogues involving negatively polarized users tend to overemphasize persona attributes. In contrast, positively polarized profiles yield dialogues that selectively incorporate persona information, resulting in smoother interactions. Furthermore, we find that personas with weak or neutral sentiment generally produce lower-quality dialogues. Motivated by these findings, we propose a dialogue generation approach that explicitly accounts for persona polarity by combining a turn-based generation strategy with a profile ordering mechanism and sentiment-aware prompting. Our study provides new insights into the sensitivity of LLMs to persona sentiment and offers guidance for developing more robust and nuanced personalized dialogue systems.",
      "arxiv_url": "https://arxiv.org/abs/2502.11423",
      "pdf_url": "https://arxiv.org/pdf/2502.11423",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01457",
      "title": "Structural Deep Encoding for Table Question Answering",
      "authors": [
        "Raphael Mouravieff",
        "Benjamin Piwowarski",
        "S. Lamprier"
      ],
      "abstract": "Although Transformers-based architectures excel at processing textual information, their naive adaptation for tabular data often involves flattening the table structure. This simplification can lead to the loss of essential inter-dependencies between rows, columns, and cells, while also posing scalability challenges for large tables. To address these issues, prior works have explored special tokens, structured embeddings, and sparse attention patterns. In this paper, we conduct a comprehensive analysis of tabular encoding techniques, which highlights the crucial role of attention sparsity in preserving structural information of tables. We also introduce a set of novel sparse attention mask designs for tabular data, that not only enhance computational efficiency but also preserve structural integrity, leading to better overall performance.",
      "arxiv_url": "https://arxiv.org/abs/2503.01457",
      "pdf_url": "https://arxiv.org/pdf/2503.01457",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10938",
      "title": "Accurate KV Cache Quantization with Outlier Tokens Tracing",
      "authors": [
        "Yi Su",
        "Yuechi Zhou",
        "Quantong Qiu",
        "Juntao Li",
        "Qingrong Xia",
        "Ping Li",
        "Xinyu Duan",
        "Zhefeng Wang",
        "Min Zhang"
      ],
      "abstract": "The impressive capabilities of Large Language Models (LLMs) come at the cost of substantial computational resources during deployment. While KV Cache can significantly reduce recomputation during inference, it also introduces additional memory overhead. KV Cache quantization presents a promising solution, striking a good balance between memory usage and accuracy. Previous research has shown that the Keys are distributed by channel, while the Values are distributed by token. Consequently, the common practice is to apply channel-wise quantization to the Keys and token-wise quantization to the Values. However, our further investigation reveals that a small subset of unusual tokens exhibit unique characteristics that deviate from this pattern, which can substantially impact quantization accuracy. To address this, we develop a simple yet effective method to identify these tokens accurately during the decoding process and exclude them from quantization as outlier tokens, significantly improving overall accuracy. Extensive experiments show that our method achieves significant accuracy improvements under 2-bit quantization and can deliver a 6.4 times reduction in memory usage and a 2.3 times increase in throughput.",
      "arxiv_url": "https://arxiv.org/abs/2505.10938",
      "pdf_url": "https://arxiv.org/pdf/2505.10938",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10267",
      "title": "An Expanded Massive Multilingual Dataset for High-Performance Language Technologies",
      "authors": [
        "Laurie Burchell",
        "Ona de Gibert",
        "Nikolay Arefyev",
        "Mikko Aulamo",
        "Marta Bañón",
        "Pinzhen Chen",
        "Mariia Fedorova",
        "Liane Guillou",
        "Barry Haddow",
        "Jan Hajivc",
        "and Jindvrich Helcl",
        "Erik Henriksson",
        "Mateusz Klimaszewski",
        "Ville Komulainen",
        "Andrey Kutuzov",
        "Joona Kytoniemi",
        "Veronika Laippala",
        "Petter Maehlum",
        "and Bhavitvya Malik",
        "Farrokh Mehryary",
        "Vladislav Mikhailov",
        "Nikita Moghe",
        "A. Myntti",
        "Dayyán O'Brien",
        "S. Oepen",
        "Proyag Pal",
        "Jousia Piha",
        "A. Pyysalo",
        "Gema Ram'irez-S'anchez",
        "David Samuel",
        "Pavel Stepachev",
        "and Jorg Tiedemann",
        "Duvsan Varivs",
        "Tereza Vojtvechov'a",
        "Jaume Zaragoza-Bernabeu"
      ],
      "abstract": "Training state-of-the-art large language models requires vast amounts of clean and diverse textual data. However, building suitable multilingual datasets remains a challenge. In this work, we present HPLT v2, a collection of high-quality multilingual monolingual and parallel corpora, extending prior work of the HPLT project. The monolingual portion of the data contains 8T tokens covering 193 languages, while the parallel data contains 380M sentence pairs covering 51 languages. We document the entire data pipeline and release the code to reproduce it. We provide extensive analysis of the quality and characteristics of our data. Finally, we evaluate the performance of language models and machine translation systems trained on HPLT v2, demonstrating its value.",
      "arxiv_url": "https://arxiv.org/abs/2503.10267",
      "pdf_url": "https://arxiv.org/pdf/2503.10267",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23461",
      "title": "UAQFact: Evaluating Factual Knowledge Utilization of LLMs on Unanswerable Questions",
      "authors": [
        "Chuanyuan Tan",
        "Wenbiao Shao",
        "Hao Xiong",
        "Tong Zhu",
        "Zhenhua Liu",
        "Kai Shi",
        "Wenliang Chen"
      ],
      "abstract": "Handling unanswerable questions (UAQ) is crucial for LLMs, as it helps prevent misleading responses in complex situations. While previous studies have built several datasets to assess LLMs' performance on UAQ, these datasets lack factual knowledge support, which limits the evaluation of LLMs' ability to utilize their factual knowledge when handling UAQ. To address the limitation, we introduce a new unanswerable question dataset UAQFact, a bilingual dataset with auxiliary factual knowledge created from a Knowledge Graph. Based on UAQFact, we further define two new tasks to measure LLMs' ability to utilize internal and external factual knowledge, respectively. Our experimental results across multiple LLM series show that UAQFact presents significant challenges, as LLMs do not consistently perform well even when they have factual knowledge stored. Additionally, we find that incorporating external knowledge may enhance performance, but LLMs still cannot make full use of the knowledge which may result in incorrect responses.",
      "arxiv_url": "https://arxiv.org/abs/2505.23461",
      "pdf_url": "https://arxiv.org/pdf/2505.23461",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12216",
      "title": "One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models",
      "authors": [
        "Rongguang Ye",
        "Ming Tang"
      ],
      "abstract": "Existing pruning methods for large language models (LLMs) focus on achieving high compression rates while maintaining model performance. Although these methods have demonstrated satisfactory performance in handling a single user's compression request, their processing time increases linearly with the number of requests, making them inefficient for real-world scenarios with multiple simultaneous requests. To address this limitation, we propose a Univeral Model for Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that learns to map arbitrary requests to their optimal pruning strategy. The challenge in training StratNet lies in the high computational cost of evaluating pruning strategies and the non-differentiable nature of the pruning process, which hinders gradient backpropagation for StratNet updates. To overcome these challenges, we leverage a Gaussian process to approximate the evaluation process. Since the gradient of the Gaussian process is computable, we can use it to approximate the gradient of the non-differentiable pruning process, thereby enabling StratNet updates. Experimental results show that UniCuCo is 28 times faster than baselines in processing 64 requests, while maintaining comparable accuracy to baselines.",
      "arxiv_url": "https://arxiv.org/abs/2505.12216",
      "pdf_url": "https://arxiv.org/pdf/2505.12216",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22137",
      "title": "Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments",
      "authors": [
        "Marc Feger",
        "Katarina Boland",
        "Stefan Dietze"
      ],
      "abstract": "Identifying arguments is a necessary prerequisite for various tasks in automated discourse analysis, particularly within contexts such as political debates, online discussions, and scientific reasoning. In addition to theoretical advances in understanding the constitution of arguments, a significant body of research has emerged around practical argument mining, supported by a growing number of publicly available datasets. On these benchmarks, BERT-like transformers have consistently performed best, reinforcing the belief that such models are broadly applicable across diverse contexts of debate. This study offers the first large-scale re-evaluation of such state-of-the-art models, with a specific focus on their ability to generalize in identifying arguments. We evaluate four transformers, three standard and one enhanced with contrastive pre-training for better generalization, on 17 English sentence-level datasets as most relevant to the task. Our findings show that, to varying degrees, these models tend to rely on lexical shortcuts tied to content words, suggesting that apparent progress may often be driven by dataset-specific cues rather than true task alignment. While the models achieve strong results on familiar benchmarks, their performance drops markedly when applied to unseen datasets. Nonetheless, incorporating both task-specific pre-training and joint benchmark training proves effective in enhancing both robustness and generalization.",
      "arxiv_url": "https://arxiv.org/abs/2505.22137",
      "pdf_url": "https://arxiv.org/pdf/2505.22137",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "420dd3670e65766d56e583ef04ce5f5c2c097dde",
      "title": "SpatialWebAgent: Leveraging Large Language Models for Automated Spatial Information Extraction and Map Grounding",
      "authors": [
        "Shunfeng Zheng",
        "Meng Fang",
        "Ling Chen"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/420dd3670e65766d56e583ef04ce5f5c2c097dde",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "4230d9ea298c430b0974f936f8747cbee578084e",
      "title": "HammerBench: Fine-Grained Function-Calling Evaluation in Real Mobile Assistant Scenarios",
      "authors": [
        "Jun Wang",
        "Jiamu Zhou",
        "Xihuai Wang",
        "Xiaoyun Mo",
        "Haoyu Zhang",
        "Qiqiang Lin",
        "Jincheng Jincheng",
        "Muning Wen",
        "Weinan Zhang",
        "Qiuying Peng"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/4230d9ea298c430b0974f936f8747cbee578084e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.16212",
      "title": "MathFusion: Enhancing Mathematic Problem-solving of LLM through Instruction Fusion",
      "authors": [
        "Qizhi Pei",
        "Lijun Wu",
        "Zhuoshi Pan",
        "Yu Li",
        "Honglin Lin",
        "Chenlin Ming",
        "Xin Gao",
        "Conghui He",
        "Rui Yan"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive progress in mathematical reasoning. While data augmentation is promising to enhance mathematical problem-solving ability, current approaches are predominantly limited to instance-level modifications-such as rephrasing or generating syntactic variations-which fail to capture and leverage the intrinsic relational structures inherent in mathematical knowledge. Inspired by human learning processes, where mathematical proficiency develops through systematic exposure to interconnected concepts, we introduce MathFusion, a novel framework that enhances mathematical reasoning through cross-problem instruction synthesis. MathFusion implements this through three fusion strategies: (1) sequential fusion, which chains related problems to model solution dependencies; (2) parallel fusion, which combines analogous problems to reinforce conceptual understanding; and (3) conditional fusion, which creates context-aware selective problems to enhance reasoning flexibility. By applying these strategies, we generate a new dataset, \\textbf{MathFusionQA}, followed by fine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental results demonstrate that MathFusion achieves substantial improvements in mathematical reasoning while maintaining high data efficiency, boosting performance by 18.0 points in accuracy across diverse benchmarks while requiring only 45K additional synthetic instructions, representing a substantial improvement over traditional single-instruction approaches. Our datasets, models, and code are publicly available at https://github.com/QizhiPei/mathfusion.",
      "arxiv_url": "https://arxiv.org/abs/2503.16212",
      "pdf_url": "https://arxiv.org/pdf/2503.16212",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16825",
      "title": "Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization",
      "authors": [
        "Yao Xiao",
        "Hai Ye",
        "Linyao Chen",
        "Hwee Tou Ng",
        "Li Bing",
        "Xiaoli Li",
        "Roy Ka-wei Lee"
      ],
      "abstract": "Iterative data generation and model retraining are widely used to align large language models (LLMs). It typically involves a policy model to generate on-policy responses and a reward model to guide training data selection. Direct Preference Optimization (DPO) further enhances this process by constructing preference pairs of chosen and rejected responses. In this work, we aim to \\emph{scale up} the number of on-policy samples via repeated random sampling to improve alignment performance. Conventional practice selects the sample with the highest reward as chosen and the lowest as rejected for DPO. However, our experiments reveal that this strategy leads to a \\emph{decline} in performance as the sample size increases. To address this, we investigate preference data construction through the lens of underlying normal distribution of sample rewards. We categorize the reward space into seven representative points and systematically explore all 21 ($C_7^2$) pairwise combinations. Through evaluations on four models using AlpacaEval 2, we find that selecting the rejected response at reward position $\\mu - 2\\sigma$ rather than the minimum reward, is crucial for optimal performance. We finally introduce a scalable preference data construction strategy that consistently enhances model performance as the sample scale increases.",
      "arxiv_url": "https://arxiv.org/abs/2502.16825",
      "pdf_url": "https://arxiv.org/pdf/2502.16825",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14315",
      "title": "Unveiling Cultural Blind Spots: Analyzing the Limitations of mLLMs in Procedural Text Comprehension",
      "authors": [
        "Amir Hossein Yari",
        "Fajri Koto"
      ],
      "abstract": "Despite the impressive performance of multilingual large language models (mLLMs) in various natural language processing tasks, their ability to understand procedural texts, particularly those with culture-specific content, remains largely unexplored. Texts describing cultural procedures, including rituals, traditional craftsmanship, and social etiquette, require an inherent understanding of cultural context, presenting a significant challenge for mLLMs. In this work, we introduce CAPTex, a benchmark designed to evaluate mLLMs' ability to process and reason about culturally diverse procedural texts across multiple languages using various methodologies to assess their performance. Our findings indicate that (1) mLLMs face difficulties with culturally contextualized procedural texts, showing notable performance declines in low-resource languages, (2) model performance fluctuates across cultural domains, with some areas presenting greater difficulties, and (3) language models exhibit better performance on multiple-choice tasks within conversational frameworks compared to direct questioning. These results underscore the current limitations of mLLMs in handling culturally nuanced procedural texts and highlight the need for culturally aware benchmarks like CAPTex to enhance their adaptability and comprehension across diverse linguistic and cultural landscapes.",
      "arxiv_url": "https://arxiv.org/abs/2502.14315",
      "pdf_url": "https://arxiv.org/pdf/2502.14315",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "42bfffbc4140e882bd49f2a953a64bb3f3d5a165",
      "title": "LexKeyPlan: Planning with Keyphrases and Retrieval Augmentation for Legal Text Generation: A Case Study on European Court of Human Rights Cases",
      "authors": [
        "Santosh T.y.s.s",
        "Elvin Quero Hernandez"
      ],
      "abstract": "Large language models excel at legal text generation but often produce hallucinations due to their sole reliance on parametric knowledge. Retrieval-augmented models mitigate this by providing relevant external documents to the model but struggle when retrieval is based only on past context, which may not align with the model’s intended future content. We introduce LexKeyPlan, a novel framework that integrates anticipatory planning into generation. Instead of relying solely on context for retrieval, LexKeyPlan generates keyphrases outlining future content serving as forward-looking plan, guiding retrieval for more accurate text generation. This work incorporates planning into legal text generation, demonstrating how keyphrases—representing legal con-cepts—enhance factual accuracy. By structuring retrieval around legal concepts, LexKey-Plan better aligns with legal reasoning, making it particularly suited for legal applications. Using the ECHR corpus as case study, we show that LexKeyPlan improves factual accuracy and coherence by retrieving information aligned with the intended content.",
      "arxiv_url": "https://www.semanticscholar.org/paper/42bfffbc4140e882bd49f2a953a64bb3f3d5a165",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.04151",
      "title": "STEP: Staged Parameter-Efficient Pre-training for Large Language Models",
      "authors": [
        "Kazuki Yano",
        "Takumi Ito",
        "Jun Suzuki"
      ],
      "abstract": "Pre-training large language models (LLMs) faces significant memory challenges due to the large size of model parameters. We introduce STaged parameter-Efficient Pre-training (STEP), which integrates parameter-efficient tuning techniques with model growth. We conduct experiments on pre-training LLMs of various sizes and demonstrate that STEP achieves up to a 53.9% reduction in maximum memory requirements compared to vanilla pre-training while maintaining equivalent performance. Furthermore, we show that the model by STEP performs comparably to vanilla pre-trained models on downstream tasks after instruction tuning.",
      "arxiv_url": "https://arxiv.org/abs/2504.04151",
      "pdf_url": "https://arxiv.org/pdf/2504.04151",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02338",
      "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL",
      "authors": [
        "Hyungjoo Chae",
        "Dongjin Kang",
        "Jihyuk Kim",
        "Beong-woo Kwak",
        "Sunghyun Park",
        "Haeju Park",
        "Jinyoung Yeo",
        "Moontae Lee",
        "Kyungjae Lee"
      ],
      "abstract": "With the release of R1, a publicly available large reasoning model (LRM), researchers commonly train new LRMs by training language models on R1's long chain-of-thought (CoT) inferences. While prior works show that LRMs' capabilities can be reproduced through direct distillation, the continued reliance on the existing models (e.g., R1) remains a critical limitation in advancing the field. As a first step toward independent LRM development, this paper explores the possibility of constructing a long CoT dataset with LLMs that are not trained for inference-time scaling. To this end, we present the Long CoT Collection, a dataset of 100K CoT rationales annotated using existing short CoT LLMs. We develop a pipeline that induces o1's novel reasoning strategies into short CoT LLMs, enabling them to think longer and introducing controllability over the thought budget to better manage the overthinking problem. Our extensive analyses validate that our dataset achieves quality comparable to--or slightly below--R1. Furthermore, our experiments demonstrate that training on our dataset not only strengthens general reasoning skills, but also provides a strong foundation for reinforcement learning--models initialized on our data achieve 2-3x larger gains with RLVR.",
      "arxiv_url": "https://arxiv.org/abs/2506.02338",
      "pdf_url": "https://arxiv.org/pdf/2506.02338",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "4310080e368cc5af234b39315a82b6e9dad9f010",
      "title": "M-RangeDetector: Enhancing Generalization in Machine-Generated Text Detection through Multi-Range Attention Masks",
      "authors": [
        "Kaijie Jiao",
        "Quan Wang",
        "L. Zhang",
        "Zikang Guo",
        "Zhendong Mao"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/4310080e368cc5af234b39315a82b6e9dad9f010",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00963",
      "title": "From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation",
      "authors": [
        "Cheng Cheng",
        "Zhenya Huang",
        "Guanhao Zhao",
        "Yuxiang Guo",
        "Xin Lin",
        "Jinze Wu",
        "Xin Li",
        "Shijin Wang"
      ],
      "abstract": "Automatically generating high-quality mathematical problems that align with educational objectives is a crucial task in NLP-based educational technology. Traditional generation methods focus primarily on textual quality, but they often overlook educational objectives. Moreover, these methods address only single-dimensional, simple question generation, failing to meet complex, multifaceted educational requirements. To address these challenges, we constructed and annotated EduMath, a dataset of 16k mathematical questions with multi-dimensional educational objectives. Based on this dataset, we developed EQGEVAL, which incorporates three evaluation dimensions and is designed to assess the ability of models to generate educational questions. Drawing inspiration from teachers' problem design processes, we propose the Educational Question Planning with self-Reflection (EQPR) method for educational mathematical question generation, following a\"plan-evaluate-optimize\"approach. Specifically, by combining planning algorithm based on Monte Carlo Tree Search with the generative capabilities of Large Language Models, we continuously optimize questions through iterative feedback. This self-optimization mechanism ensures that the generated questions both fit the educational context and strategically achieve specific basic educational objectives. Through extensive experiments based on EQGEVAL, we have demonstrated that EQPR achieves significant improvements in generating questions that meet multi-dimensional educational objectives.",
      "arxiv_url": "https://arxiv.org/abs/2506.00963",
      "pdf_url": "https://arxiv.org/pdf/2506.00963",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "43bcbee70b6f143b4df44c3af0488f5772d8cdf7",
      "title": "IRIS: Interpretable Retrieval-Augmented Classification for Long Interspersed Document Sequences",
      "authors": [
        "Fengnan Li",
        "Elliot D. Hill",
        "Jiang Shu",
        "Jiaxin Gao",
        "Matthew M Engelhard"
      ],
      "abstract": "Transformer-based models have achieved state-of-the-art performance in document classification but struggle with long-text processing due to the quadratic computational complexity in the self-attention module. Existing solutions, such as sparse attention, hierarchical models, and key sentence extraction, partially address the issue but still fall short when the input sequence is exceptionally lengthy. To address this challenge, we propose IRIS (Interpretable Retrieval-Augmented Classification for long Interspersed Document Sequences), a novel, lightweight framework that utilizes retrieval to efficiently classify long documents while enhancing interpretability. IRIS segments documents into chunks, stores their embeddings in a vector database, and retrieves those most relevant to a given task using learnable query vectors. A linear attention mechanism then aggregates the retrieved embeddings for classification, allowing the model to process arbitrarily long documents without increasing computational cost and remaining trainable on a single GPU. Our experiments across six datasets show that IRIS achieves comparable performance to baseline models on standard benchmarks, and excels in three clinical note disease risk prediction tasks where documents are extremely long and key information is sparse. Furthermore, IRIS provides global interpretability by revealing a clear summary of key risk factors identified by the model. These findings highlight the potential of IRIS as an efficient and interpretable solution for long-document classification, particularly in healthcare applications where both performance and explainability are crucial.",
      "arxiv_url": "https://www.semanticscholar.org/paper/43bcbee70b6f143b4df44c3af0488f5772d8cdf7",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "43f212bae5d763bae2694fc6358b8551b4801951",
      "title": "CDS: Data Synthesis Method Guided by Cognitive Diagnosis Theory",
      "authors": [
        "Haokun Zhao",
        "Jinyi Han",
        "Jiaqing Liang",
        "Yanghua Xiao"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated outstanding capabilities across various domains, but the increasing complexity of new challenges demands enhanced performance and adaptability. Traditional benchmarks, although comprehensive, often lack the granularity needed for detailed capability analysis. This study introduces the Cognitive Diagnostic Synthesis (CDS) method, which employs Cognitive Diagnosis Theory (CDT) for precise evaluation and targeted enhancement of LLMs. By decomposing complex tasks into discrete knowledge points, CDS accurately identifies and synthesizes data targeting model weaknesses, thereby enhancing the model’s performance. This framework proposes a comprehensive pipeline driven by knowledge point evaluation, synthesis, data augmentation, and filtering, which significantly improves the model’s mathematical and coding capabilities, achieving up to an 11.12% improvement in optimal scenarios.",
      "arxiv_url": "https://www.semanticscholar.org/paper/43f212bae5d763bae2694fc6358b8551b4801951",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24635",
      "title": "Disentangling Language and Culture for Evaluating Multilingual Large Language Models",
      "authors": [
        "Jiahao Ying",
        "Wei Tang",
        "Yiran Zhao",
        "Yixin Cao",
        "Yu Rong",
        "Wenxuan Zhang"
      ],
      "abstract": "This paper introduces a Dual Evaluation Framework to comprehensively assess the multilingual capabilities of LLMs. By decomposing the evaluation along the dimensions of linguistic medium and cultural context, this framework enables a nuanced analysis of LLMs' ability to process questions within both native and cross-cultural contexts cross-lingually. Extensive evaluations are conducted on a wide range of models, revealing a notable\"CulturalLinguistic Synergy\"phenomenon, where models exhibit better performance when questions are culturally aligned with the language. This phenomenon is further explored through interpretability probing, which shows that a higher proportion of specific neurons are activated in a language's cultural context. This activation proportion could serve as a potential indicator for evaluating multilingual performance during model training. Our findings challenge the prevailing notion that LLMs, primarily trained on English data, perform uniformly across languages and highlight the necessity of culturally and linguistically model evaluations. Our code can be found at https://yingjiahao14. github.io/Dual-Evaluation/.",
      "arxiv_url": "https://arxiv.org/abs/2505.24635",
      "pdf_url": "https://arxiv.org/pdf/2505.24635",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12583",
      "title": "LongFaith: Enhancing Long-Context Reasoning in LLMs with Faithful Synthetic Data",
      "authors": [
        "Cehao Yang",
        "Xueyuan Lin",
        "Chengjin Xu",
        "Xuhui Jiang",
        "Shengjie Ma",
        "Aofan Liu",
        "Hui Xiong",
        "Jian Guo"
      ],
      "abstract": "Despite the growing development of long-context large language models (LLMs), data-centric approaches relying on synthetic data have been hindered by issues related to faithfulness, which limit their effectiveness in enhancing model performance on tasks such as long-context reasoning and question answering (QA). These challenges are often exacerbated by misinformation caused by lack of verification, reasoning without attribution, and potential knowledge conflicts. We propose LongFaith, a novel pipeline for synthesizing faithful long-context reasoning instruction datasets. By integrating ground truth and citation-based reasoning prompts, we eliminate distractions and improve the accuracy of reasoning chains, thus mitigating the need for costly verification processes. We open-source two synthesized datasets, LongFaith-SFT and LongFaith-PO, which systematically address multiple dimensions of faithfulness, including verified reasoning, attribution, and contextual grounding. Extensive experiments on multi-hop reasoning datasets and LongBench demonstrate that models fine-tuned on these datasets significantly improve performance. Our ablation studies highlight the scalability and adaptability of the LongFaith pipeline, showcasing its broad applicability in developing long-context LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2502.12583",
      "pdf_url": "https://arxiv.org/pdf/2502.12583",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.16581",
      "title": "DialUp! Modeling the Language Continuum by Adapting Models to Dialects and Dialects to Models",
      "authors": [
        "Niyati Bafna",
        "Emily Chang",
        "N. R. Robinson",
        "David R. Mortensen",
        "Kenton Murray",
        "David Yarowsky",
        "Hale Sirin"
      ],
      "abstract": "Most of the world's languages and dialects are low-resource, and lack support in mainstream machine translation (MT) models. However, many of them have a closely-related high-resource language (HRL) neighbor, and differ in linguistically regular ways from it. This underscores the importance of model robustness to dialectal variation and cross-lingual generalization to the HRL dialect continuum. We present DialUp, consisting of a training-time technique for adapting a pretrained model to dialectal data (M->D), and an inference-time intervention adapting dialectal data to the model expertise (D->M). M->D induces model robustness to potentially unseen and unknown dialects by exposure to synthetic data exemplifying linguistic mechanisms of dialectal variation, whereas D->M treats dialectal divergence for known target dialects. These methods show considerable performance gains for several dialects from four language families, and modest gains for two other language families. We also conduct feature and error analyses, which show that language varieties with low baseline MT performance are more likely to benefit from these approaches.",
      "arxiv_url": "https://arxiv.org/abs/2501.16581",
      "pdf_url": "https://arxiv.org/pdf/2501.16581",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "44399160c8779f708d6e18dd99b1ea3a47458105",
      "title": "Fine-grained Knowledge Enhancement for Retrieval-Augmented Generation",
      "authors": [
        "Jingxuan Han",
        "Zhendong Mao",
        "Yi Liu",
        "Yexuan Che",
        "Zheren Fu",
        "Quan Wang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/44399160c8779f708d6e18dd99b1ea3a47458105",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.02460",
      "title": "Towards Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large Language Models in Medical Applications",
      "authors": [
        "Zhe Chen",
        "Yusheng Liao",
        "Shuyang Jiang",
        "Pingjie Wang",
        "Yiqiu Guo",
        "Yanfeng Wang",
        "Yu Wang"
      ],
      "abstract": "Large language models hold promise for addressing medical challenges, such as medical diagnosis reasoning, research knowledge acquisition, clinical decision-making, and consumer health inquiry support. However, they often generate hallucinations due to limited medical knowledge. Incorporating external knowledge is therefore critical, which necessitates multi-source knowledge acquisition. We address this challenge by framing it as a source planning problem, which is to formulate context-appropriate queries tailored to the attributes of diverse sources. Existing approaches either overlook source planning or fail to achieve it effectively due to misalignment between the model's expectation of the sources and their actual content. To bridge this gap, we present MedOmniKB, a repository comprising multigenre and multi-structured medical knowledge sources. Leveraging these sources, we propose the Source Planning Optimisation method, which enhances multi-source utilisation. Our approach involves enabling an expert model to explore and evaluate potential plans while training a smaller model to learn source alignment. Experimental results demonstrate that our method substantially improves multi-source planning performance, enabling the optimised small model to achieve state-of-the-art results in leveraging diverse medical knowledge sources.",
      "arxiv_url": "https://arxiv.org/abs/2501.02460",
      "pdf_url": "https://arxiv.org/pdf/2501.02460",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-01-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05447",
      "title": "Training Dynamics Underlying Language Model Scaling Laws: Loss Deceleration and Zero-Sum Learning",
      "authors": [
        "Andrei Mircea",
        "Supriyo Chakraborty",
        "Nima Chitsazan",
        "Irina Rish",
        "Ekaterina Lobacheva"
      ],
      "abstract": "This work aims to understand how scaling improves language models, specifically in terms of training dynamics. We find that language models undergo loss deceleration early in training; an abrupt slowdown in the rate of loss improvement, resulting in piecewise linear behaviour of the loss curve in log-log space. Scaling up the model mitigates this transition by (1) decreasing the loss at which deceleration occurs, and (2) improving the log-log rate of loss improvement after deceleration. We attribute loss deceleration to a type of degenerate training dynamics we term zero-sum learning (ZSL). In ZSL, per-example gradients become systematically opposed, leading to destructive interference in per-example changes in loss. As a result, improving loss on one subset of examples degrades it on another, bottlenecking overall progress. Loss deceleration and ZSL provide new insights into the training dynamics underlying language model scaling laws, and could potentially be targeted directly to improve language models independent of scale. We make our code and artefacts available at: https://github.com/mirandrom/zsl",
      "arxiv_url": "https://arxiv.org/abs/2506.05447",
      "pdf_url": "https://arxiv.org/pdf/2506.05447",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14173",
      "title": "THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation",
      "authors": [
        "Yunlong Liang",
        "Fandong Meng",
        "Jie Zhou"
      ],
      "abstract": "The sparse Mixture-of-Experts (MoE) has achieved significant progress for neural machine translation (NMT). However, there exist two limitations in current MoE solutions which may lead to sub-optimal performance: 1) they directly use the task knowledge of NMT into MoE (\\emph{e.g.}, domain/linguistics-specific knowledge), which are generally unavailable at practical application and neglect the naturally grouped domain/linguistic properties; 2) the expert selection only depends on the localized token representation without considering the context, which fully grasps the state of each token in a global view. To address the above limitations, we propose THOR-MoE via arming the MoE with hierarchical task-guided and context-responsive routing policies. Specifically, it 1) firstly predicts the domain/language label and then extracts mixed domain/language representation to allocate task-level experts in a hierarchical manner; 2) injects the context information to enhance the token routing from the pre-selected task-level experts set, which can help each token to be accurately routed to more specialized and suitable experts. Extensive experiments on multi-domain translation and multilingual translation benchmarks with different architectures consistently demonstrate the superior performance of THOR-MoE. Additionally, the THOR-MoE operates as a plug-and-play module compatible with existing Top-$k$~\\cite{shazeer2017} and Top-$p$~\\cite{huang-etal-2024-harder} routing schemes, ensuring broad applicability across diverse MoE architectures. For instance, compared with vanilla Top-$p$~\\cite{huang-etal-2024-harder} routing, the context-aware manner can achieve an average improvement of 0.75 BLEU with less than 22\\% activated parameters on multi-domain translation tasks.",
      "arxiv_url": "https://arxiv.org/abs/2505.14173",
      "pdf_url": "https://arxiv.org/pdf/2505.14173",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.07878",
      "title": "Token Level Routing Inference System for Edge Devices",
      "authors": [
        "Jianshu She",
        "Wenhao Zheng",
        "Zhengzhong Liu",
        "Hongyi Wang",
        "Eric P. Xing",
        "Huaxiu Yao",
        "Qirong Ho"
      ],
      "abstract": "The computational complexity of large language model (LLM) inference significantly constrains their deployment efficiency on edge devices. In contrast, small language models offer faster decoding and lower resource consumption but often suffer from degraded response quality and heightened susceptibility to hallucinations. To address this trade-off, collaborative decoding, in which a large model assists in generating critical tokens, has emerged as a promising solution. This paradigm leverages the strengths of both model types by enabling high-quality inference through selective intervention of the large model, while maintaining the speed and efficiency of the smaller model. In this work, we present a novel collaborative decoding inference system that allows small models to perform on-device inference while selectively consulting a cloud-based large model for critical token generation. Remarkably, the system achieves a 60% performance gain on CommonsenseQA using only a 0.5B model on an M1 MacBook, with under 7% of tokens generation uploaded to the large model in the cloud.",
      "arxiv_url": "https://arxiv.org/abs/2504.07878",
      "pdf_url": "https://arxiv.org/pdf/2504.07878",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23449",
      "title": "CMIE: Combining MLLM Insights with External Evidence for Explainable Out-of-Context Misinformation Detection",
      "authors": [
        "Fanxiao Li",
        "Jiaying Wu",
        "Canyuan He",
        "Wei Zhou"
      ],
      "abstract": "Multimodal large language models (MLLMs) have demonstrated impressive capabilities in visual reasoning and text generation. While previous studies have explored the application of MLLM for detecting out-of-context (OOC) misinformation, our empirical analysis reveals two persisting challenges of this paradigm. Evaluating the representative GPT-4o model on direct reasoning and evidence augmented reasoning, results indicate that MLLM struggle to capture the deeper relationships-specifically, cases in which the image and text are not directly connected but are associated through underlying semantic links. Moreover, noise in the evidence further impairs detection accuracy. To address these challenges, we propose CMIE, a novel OOC misinformation detection framework that incorporates a Coexistence Relationship Generation (CRG) strategy and an Association Scoring (AS) mechanism. CMIE identifies the underlying coexistence relationships between images and text, and selectively utilizes relevant evidence to enhance misinformation detection. Experimental results demonstrate that our approach outperforms existing methods.",
      "arxiv_url": "https://arxiv.org/abs/2505.23449",
      "pdf_url": "https://arxiv.org/pdf/2505.23449",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.21500",
      "title": "OpenHuEval: Evaluating Large Language Model on Hungarian Specifics",
      "authors": [
        "Haote Yang",
        "Xingjian Wei",
        "Jiang Wu",
        "Noémi Ligeti-Nagy",
        "Jiaxing Sun",
        "Yinfan Wang",
        "Z. Yang",
        "Junyuan Gao",
        "Jingchao Wang",
        "Bowen Jiang",
        "Shasha Wang",
        "Nanjun Yu",
        "Zihao Zhang",
        "Shixin Hong",
        "Hong-wei Liu",
        "Wei Li",
        "Songyang Zhang",
        "Dahua Lin",
        "Lijun Wu",
        "G'abor Pr'osz'eky",
        "Conghui He"
      ],
      "abstract": "We introduce OpenHuEval, the first benchmark for LLMs focusing on the Hungarian language and specifics. OpenHuEval is constructed from a vast collection of Hungarian-specific materials sourced from multiple origins. In the construction, we incorporated the latest design principles for evaluating LLMs, such as using real user queries from the internet, emphasizing the assessment of LLMs'generative capabilities, and employing LLM-as-judge to enhance the multidimensionality and accuracy of evaluations. Ultimately, OpenHuEval encompasses eight Hungarian-specific dimensions, featuring five tasks and 3953 questions. Consequently, OpenHuEval provides the comprehensive, in-depth, and scientifically accurate assessment of LLM performance in the context of the Hungarian language and its specifics. We evaluated current mainstream LLMs, including both traditional LLMs and recently developed Large Reasoning Models. The results demonstrate the significant necessity for evaluation and model optimization tailored to the Hungarian language and specifics. We also established the framework for analyzing the thinking processes of LRMs with OpenHuEval, revealing intrinsic patterns and mechanisms of these models in non-English languages, with Hungarian serving as a representative example. We will release OpenHuEval at https://github.com/opendatalab/OpenHuEval .",
      "arxiv_url": "https://arxiv.org/abs/2503.21500",
      "pdf_url": "https://arxiv.org/pdf/2503.21500",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.05390",
      "title": "Learning Task Representations from In-Context Learning",
      "authors": [
        "Baturay Sağlam",
        "Zhuoran Yang",
        "Dionysis Kalogerias",
        "Amin Karbasi"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable proficiency in in-context learning (ICL), where models adapt to new tasks through example-based prompts without requiring parameter updates. However, understanding how tasks are internally encoded and generalized remains a challenge. To address some of the empirical and technical gaps in the literature, we introduce an automated formulation for encoding task information in ICL prompts as a function of attention heads within the transformer architecture. This approach computes a single task vector as a weighted sum of attention heads, with the weights optimized causally via gradient descent. Our findings show that existing methods fail to generalize effectively to modalities beyond text. In response, we also design a benchmark to evaluate whether a task vector can preserve task fidelity in functional regression tasks. The proposed method successfully extracts task-specific information from in-context demonstrations and excels in both text and regression tasks, demonstrating its generalizability across modalities.",
      "arxiv_url": "https://arxiv.org/abs/2502.05390",
      "pdf_url": "https://arxiv.org/pdf/2502.05390",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03483",
      "title": "APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training",
      "authors": [
        "Jun Rao",
        "Zepeng Lin",
        "Xuebo Liu",
        "Xiaopeng Ke",
        "Lian Lian",
        "Dong Jin",
        "Shengjun Cheng",
        "Jun Yu",
        "Min Zhang"
      ],
      "abstract": "Large Language Models (LLMs) often require domain-specific fine-tuning to address targeted tasks, which risks degrading their general capabilities. Maintaining a balance between domain-specific enhancements and general model utility is a key challenge. This paper proposes a novel approach named APT (Weakness Case Acquisition and Iterative Preference Training) to enhance domain-specific performance with self-generated dis-preferred weakness data (bad cases and similar cases). APT uniquely focuses on training the model using only those samples where errors occur, alongside a small, similar set of samples retrieved for this purpose. This targeted training minimizes interference with the model's existing knowledge base, effectively retaining generic capabilities. Experimental results on the LLama-2 and Mistral-V0.3 models across various benchmarks demonstrate that APT ensures no reduction in generic capacity and achieves superior performance on downstream tasks compared to various existing methods. This validates our method as an effective strategy for enhancing domain-specific capabilities without sacrificing the model's broader applicability.",
      "arxiv_url": "https://arxiv.org/abs/2506.03483",
      "pdf_url": "https://arxiv.org/pdf/2506.03483",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00582",
      "title": "Do Language Models Mirror Human Confidence? Exploring Psychological Insights to Address Overconfidence in LLMs",
      "authors": [
        "Chenjun Xu",
        "Bingbing Wen",
        "Bin Han",
        "Robert Wolfe",
        "Lucy Lu Wang",
        "Bill Howe"
      ],
      "abstract": "Psychology research has shown that humans are poor at estimating their performance on tasks, tending towards underconfidence on easy tasks and overconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct, Claude-3-Sonnet, and GPT-4o, on a range of QA tasks of varying difficulty, and show that models exhibit subtle differences from human patterns of overconfidence: less sensitive to task difficulty, and when prompted to answer based on different personas -- e.g., expert vs layman, or different race, gender, and ages -- the models will respond with stereotypically biased confidence estimations even though their underlying answer accuracy remains the same. Based on these observations, we propose Answer-Free Confidence Estimation (AFCE) to improve confidence calibration and LLM interpretability in these settings. AFCE is a self-assessment method that employs two stages of prompting, first eliciting only confidence scores on questions, then asking separately for the answer. Experiments on the MMLU and GPQA datasets spanning subjects and difficulty show that this separation of tasks significantly reduces overconfidence and delivers more human-like sensitivity to task difficulty.",
      "arxiv_url": "https://arxiv.org/abs/2506.00582",
      "pdf_url": "https://arxiv.org/pdf/2506.00582",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.17579",
      "title": "Leveraging Human Production-Interpretation Asymmetries to Test LLM Cognitive Plausibility",
      "authors": [
        "S. Lam",
        "Qingcheng Zeng",
        "Jingyi Wu",
        "Rob Voigt"
      ],
      "abstract": "Whether large language models (LLMs) process language similarly to humans has been the subject of much theoretical and practical debate. We examine this question through the lens of the production-interpretation distinction found in human sentence processing and evaluate the extent to which instruction-tuned LLMs replicate this distinction. Using an empirically documented asymmetry between pronoun production and interpretation in humans for implicit causality verbs as a testbed, we find that some LLMs do quantitatively and qualitatively reflect human-like asymmetries between production and interpretation. We demonstrate that whether this behavior holds depends upon both model size-with larger models more likely to reflect human-like patterns and the choice of meta-linguistic prompts used to elicit the behavior. Our codes and results are available at https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025.",
      "arxiv_url": "https://arxiv.org/abs/2503.17579",
      "pdf_url": "https://arxiv.org/pdf/2503.17579",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "4611eebedad863414f2d7931a69cc8f88300983d",
      "title": "Learning First-Order Logic Rules for Argumentation Mining",
      "authors": [
        "Yang Sun",
        "Guanrong Chen",
        "Hamid Alinejad-Rokny",
        "Jianzhu Bao",
        "Yuqi Huang",
        "Bin Liang",
        "Kam-Fai Wong",
        "Min Yang",
        "Ruifeng Xu"
      ],
      "abstract": "Argumentation Mining (AM) aims to extract argumentative structures from texts by identifying argumentation components (ACs) and their argumentative relations (ARs). While previous works focus on representation learning to encode ACs and AC pairs, they fail to explicitly model the underlying reasoning patterns of AM, resulting in limited interpretability. This paper proposes a novel First-Order Logic reasoning framework for AM (FOL-AM), designed to explicitly capture logical reasoning paths within argumentative texts. By interpreting multiple AM subtasks as a unified relation query task modeled using FOL rules, FOL-AM facilitates multi-hop relational reasoning and enhances interpretability. The framework supports two flexible implementations: a fine-tuned approach to leverage task-specific learning, and a prompt-based method utilizing large language models to harness their generalization capabilities. Extensive experiments on two AM benchmarks demonstrate that FOL-AM outperforms strong baselines while significantly improving explain-ability.",
      "arxiv_url": "https://www.semanticscholar.org/paper/4611eebedad863414f2d7931a69cc8f88300983d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.10314",
      "title": "Detecting Sockpuppetry on Wikipedia Using Meta-Learning",
      "authors": [
        "Luc Raszewski",
        "Christine de Kock"
      ],
      "abstract": "Malicious sockpuppet detection on Wikipedia is critical to preserving access to reliable information on the internet and preventing the spread of disinformation. Prior machine learning approaches rely on stylistic and meta-data features, but do not prioritise adaptability to author-specific behaviours. As a result, they struggle to effectively model the behaviour of specific sockpuppet-groups, especially when text data is limited. To address this, we propose the application of meta-learning, a machine learning technique designed to improve performance in data-scarce settings by training models across multiple tasks. Meta-learning optimises a model for rapid adaptation to the writing style of a new sockpuppet-group. Our results show that meta-learning significantly enhances the precision of predictions compared to pre-trained models, marking an advancement in combating sockpuppetry on open editing platforms. We release a new dataset of sockpuppet investigations to foster future research in both sockpuppetry and meta-learning fields.",
      "arxiv_url": "https://arxiv.org/abs/2506.10314",
      "pdf_url": "https://arxiv.org/pdf/2506.10314",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "466178c08bd0862d59ac77d2c62b8deae34d87d6",
      "title": "Dynamic Prefix as Instructor for Incremental Named Entity Recognition: A Unified Seq2Seq Generation Framework",
      "authors": [
        "Zihao Wu",
        "YongXiang Hua",
        "Yongxin Zhu",
        "Fang Zhang",
        "Linli Xu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/466178c08bd0862d59ac77d2c62b8deae34d87d6",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19108",
      "title": "CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models",
      "authors": [
        "Yongheng Zhang",
        "Xu Liu",
        "Ruoxi Zhou",
        "Qiguang Chen",
        "Hao Fei",
        "Wenpeng Lu",
        "Libo Qin"
      ],
      "abstract": "Investigating hallucination issues in large language models (LLMs) within cross-lingual and cross-modal scenarios can greatly advance the large-scale deployment in real-world applications. Nevertheless, the current studies are limited to a single scenario, either cross-lingual or cross-modal, leaving a gap in the exploration of hallucinations in the joint cross-lingual and cross-modal scenarios. Motivated by this, we introduce a novel joint Cross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this gap. Specifically, CCHall simultaneously incorporates both cross-lingual and cross-modal hallucination scenarios, which can be used to assess the cross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a comprehensive evaluation on CCHall, exploring both mainstream open-source and closed-source LLMs. The experimental results highlight that current LLMs still struggle with CCHall. We hope CCHall can serve as a valuable resource to assess LLMs in joint cross-lingual and cross-modal scenarios.",
      "arxiv_url": "https://arxiv.org/abs/2505.19108",
      "pdf_url": "https://arxiv.org/pdf/2505.19108",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14847",
      "title": "Red-Teaming LLM Multi-Agent Systems via Communication Attacks",
      "authors": [
        "Pengfei He",
        "Yuping Lin",
        "Shen Dong",
        "Han Xu",
        "Yue Xing",
        "Hui Liu"
      ],
      "abstract": "Large Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized complex problem-solving capability by enabling sophisticated agent collaboration through message-based communications. While the communication framework is crucial for agent coordination, it also introduces a critical yet unexplored security vulnerability. In this work, we introduce Agent-in-the-Middle (AiTM), a novel attack that exploits the fundamental communication mechanisms in LLM-MAS by intercepting and manipulating inter-agent messages. Unlike existing attacks that compromise individual agents, AiTM demonstrates how an adversary can compromise entire multi-agent systems by only manipulating the messages passing between agents. To enable the attack under the challenges of limited control and role-restricted communication format, we develop an LLM-powered adversarial agent with a reflection mechanism that generates contextually-aware malicious instructions. Our comprehensive evaluation across various frameworks, communication structures, and real-world applications demonstrates that LLM-MAS is vulnerable to communication-based attacks, highlighting the need for robust security measures in multi-agent systems.",
      "arxiv_url": "https://arxiv.org/abs/2502.14847",
      "pdf_url": "https://arxiv.org/pdf/2502.14847",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11926",
      "title": "BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages",
      "authors": [
        "Shamsuddeen Hassan Muhammad",
        "Nedjma Djouhra Ousidhoum",
        "Idris Abdulmumin",
        "Jan Philip Wahle",
        "Terry Ruas",
        "Meriem Beloucif",
        "Christine de Kock",
        "Nirmal Surange",
        "Daniela Teodorescu",
        "I. Ahmad",
        "David Ifeoluwa Adelani",
        "Alham Fikri Aji",
        "Felermino Ali",
        "I. Alimova",
        "Vladimir Araujo",
        "Nikolay Babakov",
        "Naomi Baes",
        "Ana-Maria Bucur",
        "Andiswa Bukula",
        "Guanqun Cao",
        "Rodrigo Tufino Cardenas",
        "Rendi Chevi",
        "C. Chukwuneke",
        "Alexandra Ciobotaru",
        "Daryna Dementieva",
        "Murja Sani Gadanya",
        "Robert Geislinger",
        "Bela Gipp",
        "Oumaima Hourrane",
        "Oana Ignat",
        "F. I. Lawan",
        "Rooweither Mabuya",
        "Rahmad Mahendra",
        "V. Marivate",
        "Andrew Piper",
        "Alexander Panchenko",
        "Charles Henrique Porto Ferreira",
        "Vitaly Protasov",
        "Samuel Rutunda",
        "Manish Shrivastava",
        "Aura Cristina Udrea",
        "Lilian D. A. Wanzare",
        "Sophie Wu",
        "Florian Valentin Wunderlich",
        "Hanif Muhammad Zhafran",
        "Tianhui Zhang",
        "Yi Zhou",
        "Saif Mohammad"
      ],
      "abstract": "People worldwide use language in subtle and complex ways to express emotions. Although emotion recognition--an umbrella term for several NLP tasks--impacts various applications within NLP and beyond, most work in this area has focused on high-resource languages. This has led to significant disparities in research efforts and proposed solutions, particularly for under-resourced languages, which often lack high-quality annotated datasets. In this paper, we present BRIGHTER--a collection of multi-labeled, emotion-annotated datasets in 28 different languages and across several domains. BRIGHTER primarily covers low-resource languages from Africa, Asia, Eastern Europe, and Latin America, with instances labeled by fluent speakers. We highlight the challenges related to the data collection and annotation processes, and then report experimental results for monolingual and crosslingual multi-label emotion identification, as well as emotion intensity recognition. We analyse the variability in performance across languages and text domains, both with and without the use of LLMs, and show that the BRIGHTER datasets represent a meaningful step towards addressing the gap in text-based emotion recognition.",
      "arxiv_url": "https://arxiv.org/abs/2502.11926",
      "pdf_url": "https://arxiv.org/pdf/2502.11926",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19388",
      "title": "gec-metrics: A Unified Library for Grammatical Error Correction Evaluation",
      "authors": [
        "Takumi Goto",
        "Yusuke Sakai",
        "Taro Watanabe"
      ],
      "abstract": "We introduce gec-metrics, a library for using and developing grammatical error correction (GEC) evaluation metrics through a unified interface. Our library enables fair system comparisons by ensuring that everyone conducts evaluations using a consistent implementation. Moreover, it is designed with a strong focus on API usage, making it highly extensible. It also includes meta-evaluation functionalities and provides analysis and visualization scripts, contributing to developing GEC evaluation metrics. Our code is released under the MIT license and is also distributed as an installable package. The video is available on YouTube.",
      "arxiv_url": "https://arxiv.org/abs/2505.19388",
      "pdf_url": "https://arxiv.org/pdf/2505.19388",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "46b1d7439df0a440a7f07fb5a904ecfa43bd1c61",
      "title": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation",
      "authors": [
        "Shivalika Singh",
        "Angelika Romanou",
        "Clémentine Fourrier",
        "D. I. Adelani",
        "Jian Gang Ngui",
        "Daniel Vila-Suero",
        "Peerat Limkonchotiwat",
        "Kelly Marchisio",
        "Wei Qi Leong",
        "Yosephine Susanto",
        "Raymond Ng",
        "Shayne Longpre",
        "Sebastian Ruder",
        "Wei-Yin Ko",
        "Antoine Bosselut",
        "Alice Oh",
        "André F. T. Martins",
        "Leshem Choshen",
        "Daphne Ippolito",
        "Enzo Ferrante",
        "Marzieh Fadaee",
        "B. Ermiş",
        "Sara Hooker"
      ],
      "abstract": "Reliable multilingual evaluation is difficult, and culturally appropriate evaluation is even harder to achieve. A common practice to fill this gap is to machine-translate English evaluation sets",
      "arxiv_url": "https://www.semanticscholar.org/paper/46b1d7439df0a440a7f07fb5a904ecfa43bd1c61",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "46b4fa08f440bfa8732b145259fdd31bfdd4624c",
      "title": "The Impact of Name Age Perception on Job Recommendations in LLMs",
      "authors": [
        "M. Kamruzzaman",
        "Gene Louis Kim"
      ],
      "abstract": "Names often carry generational connotations, with certain names stereotypically associated with younger or older age groups. This study examines implicit age-related name bias in LLMs used for job recommendations. Analyzing six LLMs and 117 American names categorized by perceived age across 30 occupations, we find systematic bias: older-sounding names are favored for senior roles, while younger-sounding names are linked to youth-dominant jobs, reinforcing generational stereotypes. We also find that this bias is based on perceived rather than real ages associated with the names. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/46b4fa08f440bfa8732b145259fdd31bfdd4624c",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01407",
      "title": "Comparing LLM-generated and human-authored news text using formal syntactic theory",
      "authors": [
        "Olga Zamaraeva",
        "Dan Flickinger",
        "Francis Bond",
        "Carlos G'omez-Rodr'iguez"
      ],
      "abstract": "This study provides the first comprehensive comparison of New York Times-style text generated by six large language models against real, human-authored NYT writing. The comparison is based on a formal syntactic theory. We use Head-driven Phrase Structure Grammar (HPSG) to analyze the grammatical structure of the texts. We then investigate and illustrate the differences in the distributions of HPSG grammar types, revealing systematic distinctions between human and LLM-generated writing. These findings contribute to a deeper understanding of the syntactic behavior of LLMs as well as humans, within the NYT genre.",
      "arxiv_url": "https://arxiv.org/abs/2506.01407",
      "pdf_url": "https://arxiv.org/pdf/2506.01407",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11475",
      "title": "Focused-DPO: Enhancing Code Generation Through Focused Preference Optimization on Error-Prone Points",
      "authors": [
        "Kechi Zhang",
        "Ge Li",
        "Jia Li",
        "Yihong Dong",
        "Zhi Jin"
      ],
      "abstract": "Code generation models have shown significant potential for automating programming tasks. However, the challenge of generating accurate and reliable code persists due to the highly complex and long-reasoning nature of the task. Even state-of-the-art models often fail in code generation due to small errors, which can drastically affect the overall functionality of code. Our study identifies that current models tend to produce errors concentrated at specific error-prone points, which significantly impacts the accuracy of the generated code. To address this issue, we introduce Focused-DPO, a framework that enhances code generation by directing preference optimization towards these critical error-prone areas. This approach builds on Direct Preference Optimization, emphasizing accuracy in parts prone to errors. Additionally, we develop a method called Error-Point Identification, which constructs a dataset that targets these problematic points without requiring costly human annotations. Our experiments on benchmarks such as HumanEval(+), MBPP(+), and LiveCodeBench demonstrate that Focused-DPO significantly improves the precision and reliability of code generation, reducing common errors and enhancing overall code quality. By focusing on error-prone points, Focused-DPO advances the accuracy and functionality of model-generated code.",
      "arxiv_url": "https://arxiv.org/abs/2502.11475",
      "pdf_url": "https://arxiv.org/pdf/2502.11475",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20811",
      "title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models",
      "authors": [
        "Xiao Wang",
        "Jingyun Hua",
        "Weihong Lin",
        "Yuanxing Zhang",
        "Fuzheng Zhang",
        "Jianlong Wu",
        "Di Zhang",
        "Liqiang Nie"
      ],
      "abstract": "Recent Multi-modal Large Language Models (MLLMs) have made great progress in video understanding. However, their performance on videos involving human actions is still limited by the lack of high-quality data. To address this, we introduce a two-stage data annotation pipeline. First, we design strategies to accumulate videos featuring clear human actions from the Internet. Second, videos are annotated in a standardized caption format that uses human attributes to distinguish individuals and chronologically details their actions and interactions. Through this pipeline, we curate two datasets, namely HAICTrain and HAICBench. \\textbf{HAICTrain} comprises 126K video-caption pairs generated by Gemini-Pro and verified for training purposes. Meanwhile, \\textbf{HAICBench} includes 412 manually annotated video-caption pairs and 2,000 QA pairs, for a comprehensive evaluation of human action understanding. Experimental results demonstrate that training with HAICTrain not only significantly enhances human understanding abilities across 4 benchmarks, but can also improve text-to-video generation results. Both the HAICTrain and HAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.",
      "arxiv_url": "https://arxiv.org/abs/2502.20811",
      "pdf_url": "https://arxiv.org/pdf/2502.20811",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "474e0ebf8f8fa6cbb5b7317dafff2ea7e24b8611",
      "title": "Uncertainty in Causality: A New Frontier",
      "authors": [
        "Shaobo Cui",
        "Luca Mouchel",
        "Boi Faltings"
      ],
      "abstract": "Understanding uncertainty in causality is vital in various domains, including core NLP tasks like event causality extraction, common-sense reasoning, and counterfactual text generation. However, existing literature lacks a comprehensive examination of this area. This survey aims to fill this gap by thoroughly re-viewing the uncertainty in causality. We first introduce a novel trichotomy, categorizing causal uncertainty into aleatoric (inherent randomness in causal data), epistemic (causal model limitations), and ontological (existence of causal links) uncertainty. We then survey methods for quantifying uncertainty in causal analysis and highlight the complementary relationship be-tween causal uncertainty and causal strength. Furthermore, we examine the challenges that large language models (LLMs) face in handling causal uncertainty, such as hallucinations and inconsistencies, and propose key traits for an optimal causal LLM. Our paper reviews current approaches and outlines future research directions, aiming to serve as a practical guide for researchers and practitioners in this emerging field.",
      "arxiv_url": "https://www.semanticscholar.org/paper/474e0ebf8f8fa6cbb5b7317dafff2ea7e24b8611",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03735",
      "title": "Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models",
      "authors": [
        "Junling Wang",
        "Anna Rutkiewicz",
        "April Yi Wang",
        "Mrinmaya Sachan"
      ],
      "abstract": "Visuals are valuable tools for teaching math word problems (MWPs), helping young learners interpret textual descriptions into mathematical expressions before solving them. However, creating such visuals is labor-intensive and there is a lack of automated methods to support this process. In this paper, we present Math2Visual, an automatic framework for generating pedagogically meaningful visuals from MWP text descriptions. Math2Visual leverages a pre-defined visual language and a design space grounded in interviews with math teachers, to illustrate the core mathematical relationships in MWPs. Using Math2Visual, we construct an annotated dataset of 1,903 visuals and evaluate Text-to-Image (TTI) models for their ability to generate visuals that align with our design. We further fine-tune several TTI models with our dataset, demonstrating improvements in educational visual generation. Our work establishes a new benchmark for automated generation of pedagogically meaningful visuals and offers insights into key challenges in producing multimodal educational content, such as the misrepresentation of mathematical relationships and the omission of essential visual elements.",
      "arxiv_url": "https://arxiv.org/abs/2506.03735",
      "pdf_url": "https://arxiv.org/pdf/2506.03735",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15208",
      "title": "Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing",
      "authors": [
        "Zhilin Wang",
        "Yafu Li",
        "Jianhao Yan",
        "Yu Cheng",
        "Yue Zhang"
      ],
      "abstract": "Dynamical systems theory provides a framework for analyzing iterative processes and evolution over time. Within such systems, repetitive transformations can lead to stable configurations, known as attractors, including fixed points and limit cycles. Applying this perspective to large language models (LLMs), which iteratively map input text to output text, provides a principled approach to characterizing long-term behaviors. Successive paraphrasing serves as a compelling testbed for exploring such dynamics, as paraphrases re-express the same underlying meaning with linguistic variation. Although LLMs are expected to explore a diverse set of paraphrases in the text space, our study reveals that successive paraphrasing converges to stable periodic states, such as 2-period attractor cycles, limiting linguistic diversity. This phenomenon is attributed to the self-reinforcing nature of LLMs, as they iteratively favour and amplify certain textual forms over others. This pattern persists with increasing generation randomness or alternating prompts and LLMs. These findings underscore inherent constraints in LLM generative capability, while offering a novel dynamical systems perspective for studying their expressive potential.",
      "arxiv_url": "https://arxiv.org/abs/2502.15208",
      "pdf_url": "https://arxiv.org/pdf/2502.15208",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.14175",
      "title": "Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion",
      "authors": [
        "Yejun Yoon",
        "Jaeyoon Jung",
        "Seunghyun Yoon",
        "Kunwoo Park"
      ],
      "abstract": "Query expansion methods powered by large language models (LLMs) have demonstrated effectiveness in zero-shot retrieval tasks. These methods assume that LLMs can generate hypothetical documents that, when incorporated into a query vector, enhance the retrieval of real evidence. However, we challenge this assumption by investigating whether knowledge leakage in benchmarks contributes to the observed performance gains. Using fact verification as a testbed, we analyze whether the generated documents contain information entailed by ground-truth evidence and assess their impact on performance. Our findings indicate that, on average, performance improvements consistently occurred for claims whose generated documents included sentences entailed by gold evidence. This suggests that knowledge leakage may be present in fact-verification benchmarks, potentially inflating the perceived performance of LLM-based query expansion methods.",
      "arxiv_url": "https://arxiv.org/abs/2504.14175",
      "pdf_url": "https://arxiv.org/pdf/2504.14175",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.12505",
      "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process Errors Identification",
      "authors": [
        "Zhaopan Xu",
        "Pengfei Zhou",
        "Jiaxin Ai",
        "Wangbo Zhao",
        "Kai Wang",
        "Xiaojiang Peng",
        "Wenqi Shao",
        "Hongxun Yao",
        "Kaipeng Zhang"
      ],
      "abstract": "Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning and data production during training and guide LLMs toward correct steps during inference, thereby improving reasoning accuracy. However, existing benchmarks of PRMs are text-based and focus on error detection, neglecting other scenarios like reasoning search. To address this gap, we introduce MPBench, a comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. MPBench employs three evaluation paradigms, each targeting a specific role of PRMs in the reasoning process: (1) Step Correctness, which assesses the correctness of each intermediate reasoning step; (2) Answer Aggregation, which aggregates multiple solutions and selects the best one; and (3) Reasoning Process Search, which guides the search for optimal reasoning steps during inference. Through these paradigms, MPBench makes comprehensive evaluations and provides insights into the development of multimodal PRMs.",
      "arxiv_url": "https://arxiv.org/abs/2503.12505",
      "pdf_url": "https://arxiv.org/pdf/2503.12505",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03993",
      "title": "Words of Warmth: Trust and Sociability Norms for over 26k English Words",
      "authors": [
        "Saif M. Mohammad"
      ],
      "abstract": "Social psychologists have shown that Warmth (W) and Competence (C) are the primary dimensions along which we assess other people and groups. These dimensions impact various aspects of our lives from social competence and emotion regulation to success in the work place and how we view the world. More recent work has started to explore how these dimensions develop, why they have developed, and what they constitute. Of particular note, is the finding that warmth has two distinct components: Trust (T) and Sociability (S). In this work, we introduce Words of Warmth, the first large-scale repository of manually derived word--warmth (as well as word--trust and word--sociability) associations for over 26k English words. We show that the associations are highly reliable. We use the lexicons to study the rate at which children acquire WCTS words with age. Finally, we show that the lexicon enables a wide variety of bias and stereotype research through case studies on various target entities. Words of Warmth is freely available at: http://saifmohammad.com/warmth.html",
      "arxiv_url": "https://arxiv.org/abs/2506.03993",
      "pdf_url": "https://arxiv.org/pdf/2506.03993",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "47fd297d9c45f10967c9fa06b319d29be5595fac",
      "title": "LexTempus: Enhancing Temporal Generalizability of Legal Language Models Through Dynamic Mixture of Experts",
      "authors": [
        "Santosh T.Y.S.S",
        "Tuan-Quang Vuong"
      ],
      "abstract": "The rapid evolution of legal concepts over time necessitates that legal language models adapt swiftly accounting for the temporal dynamics. However, prior works have largely neglected this crucial dimension, treating legal adaptation as a static problem rather than a continuous process. To address this gap, we pioneer Lex-Tempus, a dynamic mixture of experts model that explicitly models the temporal evolution of legal language in a parameter-efficient on-line learning framework. LexTempus starts with a single lightweight adapter expert and dynamically expands by adding new experts as significant deviations in the data distribution are detected. This self-expansion strategy allows LexTempus to adapt to new information without forgetting past knowledge, thereby improving temporal generalization. We use a a non-parametric similarity-based router to merge relevant experts into a unified expert for each test instance, ensuring efficient inference without additional overhead. We validate the effectiveness of LexTempus on ECHR and EU case law datasets, demonstrating its superiority in both perplexity and open-ended text generation quality metrics.",
      "arxiv_url": "https://www.semanticscholar.org/paper/47fd297d9c45f10967c9fa06b319d29be5595fac",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "48090ae4e304b43e3cb831619efaf6466638ecbb",
      "title": "Out-of-Distribution Detection via LLM-Guided Outlier Generation for Text-attributed Graph",
      "authors": [
        "Xiangwei Lv",
        "Mengze Li",
        "Jingyuan Chen",
        "Zhiang Dong",
        "Sirui Han",
        "Beishui Liao"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/48090ae4e304b43e3cb831619efaf6466638ecbb",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14245",
      "title": "Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering",
      "authors": [
        "Rongzhi Zhu",
        "Xiangyu Liu",
        "Zequn Sun",
        "Yiwei Wang",
        "Wei Hu"
      ],
      "abstract": "In this paper, we identify a critical problem,\"lost-in-retrieval\", in retrieval-augmented multi-hop question answering (QA): the key entities are missed in LLMs' sub-question decomposition.\"Lost-in-retrieval\"significantly degrades the retrieval performance, which disrupts the reasoning chain and leads to the incorrect answers. To resolve this problem, we propose a progressive retrieval and rewriting method, namely ChainRAG, which sequentially handles each sub-question by completing missing key entities and retrieving relevant sentences from a sentence graph for answer generation. Each step in our retrieval and rewriting process builds upon the previous one, creating a seamless chain that leads to accurate retrieval and answers. Finally, all retrieved sentences and sub-question answers are integrated to generate a comprehensive answer to the original question. We evaluate ChainRAG on three multi-hop QA datasets - MuSiQue, 2Wiki, and HotpotQA - using three large language models: GPT4o-mini, Qwen2.5-72B, and GLM-4-Plus. Empirical results demonstrate that ChainRAG consistently outperforms baselines in both effectiveness and efficiency.",
      "arxiv_url": "https://arxiv.org/abs/2502.14245",
      "pdf_url": "https://arxiv.org/pdf/2502.14245",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12378",
      "title": "Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges",
      "authors": [
        "Bolei Ma",
        "Yuting Li",
        "Wei Zhou",
        "Ziwei Gong",
        "Yang Janet Liu",
        "Katja Jasinskaja",
        "Annemarie Friedrich",
        "Julia Hirschberg",
        "Frauke Kreuter",
        "Barbara Plank"
      ],
      "abstract": "Understanding pragmatics-the use of language in context-is crucial for developing NLP systems capable of interpreting nuanced language use. Despite recent advances in language technologies, including large language models, evaluating their ability to handle pragmatic phenomena such as implicatures and references remains challenging. To advance pragmatic abilities in models, it is essential to understand current evaluation trends and identify existing limitations. In this survey, we provide a comprehensive review of resources designed for evaluating pragmatic capabilities in NLP, categorizing datasets by the pragmatic phenomena they address. We analyze task designs, data collection methods, evaluation approaches, and their relevance to real-world applications. By examining these resources in the context of modern language models, we highlight emerging trends, challenges, and gaps in existing benchmarks. Our survey aims to clarify the landscape of pragmatic evaluation and guide the development of more comprehensive and targeted benchmarks, ultimately contributing to more nuanced and context-aware NLP models.",
      "arxiv_url": "https://arxiv.org/abs/2502.12378",
      "pdf_url": "https://arxiv.org/pdf/2502.12378",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.23520",
      "title": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data",
      "authors": [
        "Yu Zhang",
        "Ruijie Yu",
        "Jidong Tian",
        "Feng Zhu",
        "Jiapeng Liu",
        "Xiaokang Yang",
        "Yaohui Jin",
        "Yanyan Xu"
      ],
      "abstract": "With the increasing interest in robotic synthesis in the context of organic chemistry, the automated extraction of chemical procedures from literature is critical. However, this task remains challenging due to the inherent ambiguity of chemical language and the high cost of human annotation required for developing reliable computer-aided extraction protocols. Here, we present ChemActor, a fully fine-tuned large language model (LLM), as a chemical executor to convert between unstructured experimental procedures and structured action sequences. We propose a sequential LLM-generated data framework to address the challenges of insufficient and low-quality annotated data. This framework integrates a data selection module that selects data based on distribution divergence, with a general-purpose LLM, to generate machine-executable actions from a single molecule input. Additionally, we introduce a novel multi-round LLMs circle review metric, which reflects the model's advanced understanding of chemical experimental procedures. Extensive experiments on reaction-to-description (R2D) and description-to-action (D2A) tasks demonstrate that ChemActor, augmented by LLM-generated data, achieves state-of-the-art performance, outperforming the baseline model by 10%. The code is available at: https://github.com/Zhanghahah/ChemActor.",
      "arxiv_url": "https://arxiv.org/abs/2506.23520",
      "pdf_url": "https://arxiv.org/pdf/2506.23520",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16877",
      "title": "Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings",
      "authors": [
        "Yuqicheng Zhu",
        "Daniel Hernández",
        "Yuan He",
        "Zifeng Ding",
        "Bo Xiong",
        "Evgeny Kharlamov",
        "Steffen Staab"
      ],
      "abstract": "Uncertainty quantification in Knowledge Graph Embedding (KGE) methods is crucial for ensuring the reliability of downstream applications. A recent work applies conformal prediction to KGE methods, providing uncertainty estimates by generating a set of answers that is guaranteed to include the true answer with a predefined confidence level. However, existing methods provide probabilistic guarantees averaged over a reference set of queries and answers (marginal coverage guarantee). In high-stakes applications such as medical diagnosis, a stronger guarantee is often required: the predicted sets must provide consistent coverage per query (conditional coverage guarantee). We propose CondKGCP, a novel method that approximates predicate-conditional coverage guarantees while maintaining compact prediction sets. CondKGCP merges predicates with similar vector representations and augments calibration with rank information. We prove the theoretical guarantees and demonstrate empirical effectiveness of CondKGCP by comprehensive evaluations.",
      "arxiv_url": "https://arxiv.org/abs/2505.16877",
      "pdf_url": "https://arxiv.org/pdf/2505.16877",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03619",
      "title": "Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales",
      "authors": [
        "Ayuto Tsutsumi",
        "Yuu Jinnai"
      ],
      "abstract": "Although Large Language Models (LLMs) have demonstrated strong language understanding and generation abilities across various languages, their cultural knowledge is often limited to English-speaking communities, which can marginalize the cultures of non-English communities. To address the problem, evaluation of the cultural awareness of the LLMs and the methods to develop culturally aware LLMs have been investigated. In this study, we focus on evaluating knowledge of folktales, a key medium for conveying and circulating culture. In particular, we focus on Japanese folktales, specifically on knowledge of Yokai. Yokai are supernatural creatures originating from Japanese folktales that continue to be popular motifs in art and entertainment today. Yokai have long served as a medium for cultural expression, making them an ideal subject for assessing the cultural awareness of LLMs. We introduce YokaiEval, a benchmark dataset consisting of 809 multiple-choice questions (each with four options) designed to probe knowledge about yokai. We evaluate the performance of 31 Japanese and multilingual LLMs on this dataset. The results show that models trained with Japanese language resources achieve higher accuracy than English-centric models, with those that underwent continued pretraining in Japanese, particularly those based on Llama-3, performing especially well. The code and dataset are available at https://github.com/CyberAgentA ILab/YokaiEval.",
      "arxiv_url": "https://arxiv.org/abs/2506.03619",
      "pdf_url": "https://arxiv.org/pdf/2506.03619",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "48a1277a6cf3856dddd23dfb8215ce9a6b5d8f15",
      "title": "Knowledge Image Matters: Improving Knowledge-Based Visual Reasoning with Multi-Image Large Language Models",
      "authors": [
        "Guanghui Ye",
        "Huan Zhao",
        "Zhixue Zhao",
        "Xupeng Zha",
        "Yang Liu",
        "Zhihua Jiang"
      ],
      "abstract": "We revisit knowledge-based visual reasoning (KB-VR) in light of modern advances in multimodal large language models (MLLMs), and make the following contributions: (i) We pro-pose V isual K nowledge C ard ( VKC ) – a novel image that incorporates not only internal visual knowledge (e.g., scene-aware information) detected from the raw image, but also external world knowledge (e.g., attribute or object knowledge) produced by a knowledge generator; (ii) We present VKC -enhanced M ulti-I mage R easoning ( VKC-MIR ) – a four-stage pipeline which harnesses a state-of-the-art scene perception engine to construct an initial VKC (Stage-1), a powerful LLM to generate relevant domain knowledge (Stage-2), an excellent image editing toolkit to introduce generated knowledge into an iteratively-edited VKC (Stage-3), and finally, an emerging multi-image MLLM to solve the VKC-enhanced task (Stage-4). By performing experiments on three popular KB-VR benchmarks, our approach achieves new state-of-the-art results compared to previous top-performing models. Our code is available at: https://github.",
      "arxiv_url": "https://www.semanticscholar.org/paper/48a1277a6cf3856dddd23dfb8215ce9a6b5d8f15",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00419",
      "title": "Teaching an Old LLM Secure Coding: Localized Preference Optimization on Distilled Preferences",
      "authors": [
        "Mohammad Saqib Hasan",
        "Saikat Chakraborty",
        "Santu Karmaker",
        "Niranjan Balasubramanian"
      ],
      "abstract": "LLM generated code often contains security issues. We address two key challenges in improving secure code generation. First, obtaining high quality training data covering a broad set of security issues is critical. To address this, we introduce a method for distilling a preference dataset of insecure and secure code pairs from frontier LLMs, along with a security reasoning that explains the issues and the fix. The key idea here is to make use of security knowledge sources to devise a systematic prompting strategy that ensures broad coverage. Second, aligning models to secure code requires focusing on localized regions of code. Direct preference optimization methods, like SimPO, are not designed to handle these localized differences and turn out to be ineffective. We address this with a new localized preference optimization algorithm that masks the security related tokens in both the winning (secure) and losing (insecure) responses. To prevent loss in code quality, we also add a regularizer. Evaluations show that both training on our dataset, DiSCo, and the new preference optimization algorithm, LPO, yield substantial reductions in code insecurity while also improving overall code quality. Code and dataset are available at https://github.com/StonyBrookNLP/disco-lpo.",
      "arxiv_url": "https://arxiv.org/abs/2506.00419",
      "pdf_url": "https://arxiv.org/pdf/2506.00419",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "48ad74b11dfb8cad7f5319114df5af15f843d15d",
      "title": "MeMoTune: A Measure and Moment-Driven Fine-Tuning Framework for Quantized Large Language Models",
      "authors": [
        "Yun Zhang",
        "Xue Geng",
        "Lizi Liao",
        "Jintong Sun",
        "Minghe Yu",
        "Ge Yu"
      ],
      "abstract": "Quantizing large language models (LLMs) is essential for reducing memory and computational costs in natural language processing. Existing methods combine quantization with parameter-efficient fine-tuning but often fail to meet practical performance requirements. This paper introduces MeMoTune, a novel fine-tuning framework for quantized LLMs. By employing a measure and moment approach within a low-rank approximation framework in probability measure space, MeMoTune optimizes the objective function for superior fine-tuning results. The update process is further refined through scaled gradient, enhancing convergence efficiency and noise robustness. Experiments on tasks like text generation, summarization, and understanding show MeMoTune significantly outperforms state-of-the-art methods, e.g. fine-tuning Llama2-13B on GSM8K improves accuracy by 5.5%, while fine-tuning DeBERTaV3-base on CoLA of GLUE increases Matthews correlation by 1.7%. The code is publicly available at github.com/hddyyyb/MeMoTune .",
      "arxiv_url": "https://www.semanticscholar.org/paper/48ad74b11dfb8cad7f5319114df5af15f843d15d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.01707",
      "title": "InfiniteICL: Breaking the Limit of Context Window Size via Long Short-term Memory Transformation",
      "authors": [
        "Bowen Cao",
        "Deng Cai",
        "Wai Lam"
      ],
      "abstract": "In-context learning (ICL) is critical for large language models (LLMs), but its effectiveness is constrained by finite context windows, particularly in ultra-long contexts. To overcome this, we introduce InfiniteICL, a framework that parallels context and parameters in LLMs with short- and long-term memory in human cognitive systems, focusing on transforming temporary context knowledge into permanent parameter updates. This approach significantly reduces memory usage, maintains robust performance across varying input lengths, and theoretically enables infinite context integration through the principles of context knowledge elicitation, selection, and consolidation. Evaluations demonstrate that our method reduces context length by 90% while achieving 103% average performance of full-context prompting across fact recall, grounded reasoning, and skill acquisition tasks. When conducting sequential multi-turn transformations on complex, real-world contexts (with length up to 2M tokens), our approach surpasses full-context prompting while using only 0.4% of the original contexts. These findings highlight InfiniteICL's potential to enhance the scalability and efficiency of LLMs by breaking the limitations of conventional context window sizes.",
      "arxiv_url": "https://arxiv.org/abs/2504.01707",
      "pdf_url": "https://arxiv.org/pdf/2504.01707",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-04-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "48d136152e8627f532c78802414892998a80825c",
      "title": "Bridge-Coder: Transferring Model Capabilities from High-Resource to Low-Resource Programming Language",
      "authors": [
        "Jacob Austin",
        "Augustus Odena",
        "Maxwell I. Nye",
        "Maarten",
        "Henryk Bosma",
        "David Michalewski",
        "Ellen Dohan",
        "Carrie Jiang",
        "Michael Cai",
        "Quoc Terry",
        "Le",
        "Maarten Bosma",
        "H. Michalewski",
        "David Dohan",
        "Ellen Jiang",
        "Carrie J. Cai",
        "Michael Terry",
        "Quoc V. Le",
        "Tom B. Brown",
        "Benjamin Mann",
        "N. Ryder",
        "Jared D Subbiah",
        "Prafulla Kaplan",
        "A. Dhariwal",
        "P. Neelakantan",
        "Girish Shyam",
        "Amanda Sastry",
        "Sandhini Askell",
        "Ariel Agarwal",
        "Herbert-Voss",
        "Gretchen Krueger",
        "T. Henighan",
        "R. Child",
        "Aditya Ramesh",
        "Daniel M. Ziegler",
        "Jeffrey Wu",
        "Clemens Winter",
        "Chris Hesse",
        "Mark Chen",
        "Alec Radford",
        "I. Sutskever",
        "Federico Cassano",
        "John Gouwar",
        "F. Lucchetti",
        "Claire Schlesinger",
        "Anders Freeman",
        "Carolyn Jane",
        "Molly Q Anderson",
        "Michael Feldman",
        "Greenberg",
        "Daniel Nguyen",
        "Syd-674 ney Nguyen",
        "Luna Phipps-Costin",
        "Donald Pinckney",
        "Ming-Ho Yee",
        "Yangtian Zi",
        "C. Anderson",
        "Jerry Tworek",
        "Hee-woo Jun",
        "Qim-ing Yuan",
        "Henrique Pondé",
        "Oliveira Pinto",
        "Jared Kaplan",
        "Greg Brockman",
        "Alex Ray",
        "Raul Puri",
        "Michael Petrov",
        "Heidy Khlaaf",
        "Girish Sas-667 try",
        "Pamela Mishkin",
        "Brooke Chan",
        "Scott Gray",
        "Mikhail Pavlov",
        "Alethea Power",
        "Lukasz Kaiser",
        "Mo Bavarian",
        "William Saunders",
        "Andrew N Carr",
        "Jan Leike",
        "Josh Achiam",
        "Vedant Misra",
        "Evan Morikawa",
        "Matthew Knight",
        "M. Brundage",
        "Mira Murati",
        "Katie Mayer",
        "Peter Welinder",
        "Bob McGrew",
        "Dario Amodei",
        "Sam McCandlish",
        "Wojciech Zaremba",
        "Evaluat-698",
        "Henrique Ponde De",
        "Alexis Conneau",
        "Kartikay Khandelwal",
        "Naman Goyal",
        "Vishrav Chaudhary",
        "Guillaume Wenzek",
        "Francisco Guzmán",
        "Edouard Grave",
        "Myle Ott",
        "Luke Zettle-709 moyer",
        "Veselin Stoyanov. 2019",
        "Unsupervised",
        "Tri Dao",
        "Dan Fu",
        "Stefano Ermon",
        "Atri Rudra",
        "Christopher Ré. 2022",
        "Yan-Qiu Ding",
        "Zijian Wang",
        "Wasi Ahmad",
        "Hantian Ding",
        "Ming Tan",
        "Murali Nihal Jain",
        "Krishna Ra-720 manathan",
        "Ramesh Nallapati",
        "Parminder Bhatia",
        "Abhimanyu Dubey",
        "Abhinav Jauhri",
        "Abhinav Pandey",
        "Abhishek Kadian",
        "Ahmad Al-Dahle",
        "Aiesha Letman",
        "Akhil Mathur",
        "A. Schelten",
        "Amy Yang",
        "Daniel Fried",
        "Armen Aghajanyan",
        "Jessy Lin",
        "Sida Wang",
        "Eric Wallace",
        "Freda Shi",
        "Ruiqi Zhong",
        "Wen-tau Yih",
        "Daya Guo",
        "Qihao Zhu",
        "Dejian Yang",
        "Zhenda Xie",
        "Wentao Dong",
        "Guanting Zhang",
        "Xiao Chen",
        "Bi",
        "Haoyang Huang",
        "Yaobo Liang",
        "Nan Duan",
        "Ming Gong",
        "Loubna Raymond Li",
        "B. Allal",
        "Niklas",
        "Denis Muennighoff",
        "Chenghao Kocetkov",
        "Marc Mou",
        "Percy Liang",
        "Rishi Bommasani",
        "Dimitris Tony Lee",
        "Dilara Tsipras",
        "Michihiro Soylu",
        "Yasunaga",
        "Yian",
        "Deepak Zhang",
        "Yuhuai Narayanan",
        "Ananya Wu",
        "Ku-770",
        "Shuai Lu",
        "Shuo Ren",
        "Alexey Junjie Huang",
        "A. Svyatkovskiy",
        "Colin B Blanco",
        "Clement",
        "Ziyang Luo",
        "Can Xu",
        "Pu Zhao",
        "Qingfeng Sun",
        "Xiubo Geng",
        "Wenxiang Hu",
        "Chongyang Tao",
        "Jing Ma",
        "Niklas Muennighoff",
        "Qian Liu",
        "A. Zebaze",
        "Qinkai",
        "Binyuan Zheng",
        "Terry Yue Hui",
        "Zhuo",
        "Swayam"
      ],
      "abstract": "Most LLMs universally excel at generating 001 code for high-resource programming languages 002 (HRPLs) like Python , a capability that has 003 become standard due to the abundance of 004 training data. However, they struggle signif-005 icantly with low-resource programming lan-006 guages (LRPLs) such as D , exacerbating the 007 digital divide. This gap limits developers us-008 ing LRPLs from equally benefiting and hinders 009 innovation within underrepresented program-010 ming communities. To make matters worse, 011 manually generating data for LRPLs is highly 012 labor intensive and requires expensive expert 013 effort. In this work, we begin by analyzing the 014 NL-PL Gap, where LLMs’ direct-generated 015 LRPL data often suffers from subpar quality 016 due to the misalignment between natural lan-017 guage (NL) instructions and programming lan-018 guage (PL) outputs. To address this issue, we 019 introduce Bridge-Assist Generation , a method 020 to generate LRPL data utilizing LLM’s general 021 knowledge, HRPL proficiency, and in-context 022 learning capabilities. To further maximize 023 the utility of the generated data, we propose 024 Bridged Alignment to obtain Bridge-Coder . 025 To thoroughly evaluate our approach, we se-026 lect four relatively LRPLs: R , D , Racket , and 027 Bash . Experimental results reveal that Bridge-028 Coder achieves significant improvements over 029 the original model, with average gains of 18.71 030 and 10.81 on two comprehensive benchmarks, 031 M-HumanEval and M-MBPP. 032",
      "arxiv_url": "https://www.semanticscholar.org/paper/48d136152e8627f532c78802414892998a80825c",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.02701",
      "title": "MindBridge: Scalable and Cross-Model Knowledge Editing via Memory-Augmented Modality",
      "authors": [
        "Shuaike Li",
        "Kai Zhang",
        "Qi Liu",
        "Enhong Chen"
      ],
      "abstract": "Knowledge editing is a technique for efficiently and accurately updating the knowledge of large language models (LLMs) to alleviate obsolescence and correct errors. However, most existing methods overfit to specific models, causing edited knowledge to be discarded during each LLM update and requiring frequent re-editing, which is particularly burdensome in today's rapidly evolving open-source community. To address this issue, we propose the problem of cross-model knowledge editing and introduce MindBridge, a scalable solution inspired by the low coupling between modality processing and LLMs in multi-modal models. MindBridge introduces the novel concept of memory modality, which encodes edited knowledge as an independent modality. It first performs LLM-agnostic pre-training of the memory modality and then integrates it with various LLMs. Extensive experiments on multiple LLMs and popular knowledge editing datasets demonstrate that MindBridge achieves superior performance even in editing tens of thousands of knowledge entries and can flexibly adapt to different LLMs. Our code is available at https://github.com/CrashBugger/MindBridge.",
      "arxiv_url": "https://arxiv.org/abs/2503.02701",
      "pdf_url": "https://arxiv.org/pdf/2503.02701",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.21463",
      "title": "SpeechFake: A Large-Scale Multilingual Speech Deepfake Dataset Incorporating Cutting-Edge Generation Methods",
      "authors": [
        "Wen Huang",
        "Yanmei Gu",
        "Zhiming Wang",
        "Huijia Zhu",
        "Yanmin Qian"
      ],
      "abstract": "As speech generation technology advances, the risk of misuse through deepfake audio has become a pressing concern, which underscores the critical need for robust detection systems. However, many existing speech deepfake datasets are limited in scale and diversity, making it challenging to train models that can generalize well to unseen deepfakes. To address these gaps, we introduce SpeechFake, a large-scale dataset designed specifically for speech deepfake detection. SpeechFake includes over 3 million deepfake samples, totaling more than 3,000 hours of audio, generated using 40 different speech synthesis tools. The dataset encompasses a wide range of generation techniques, including text-to-speech, voice conversion, and neural vocoder, incorporating the latest cutting-edge methods. It also provides multilingual support, spanning 46 languages. In this paper, we offer a detailed overview of the dataset's creation, composition, and statistics. We also present baseline results by training detection models on SpeechFake, demonstrating strong performance on both its own test sets and various unseen test sets. Additionally, we conduct experiments to rigorously explore how generation methods, language diversity, and speaker variation affect detection performance. We believe SpeechFake will be a valuable resource for advancing speech deepfake detection and developing more robust models for evolving generation techniques.",
      "arxiv_url": "https://arxiv.org/abs/2507.21463",
      "pdf_url": "https://arxiv.org/pdf/2507.21463",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "4954ac117a2d2a1d83cd87c583607f4684fb9249",
      "title": "Multimodal Invariant Sentiment Representation Learning",
      "authors": [
        "Aoqiang Zhu",
        "Min Hu",
        "Xiaohua Wang",
        "Jiaoyun Yang",
        "Yiming Tang",
        "Ning An"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/4954ac117a2d2a1d83cd87c583607f4684fb9249",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03857",
      "title": "Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation",
      "authors": [
        "Mingxuan Xia",
        "Haobo Wang",
        "Yixuan Li",
        "Zewei Yu",
        "Jindong Wang",
        "Junbo Zhao",
        "Runze Wu"
      ],
      "abstract": "Recently, Large Language Models (LLMs) have demonstrated significant potential for data annotation, markedly reducing the labor costs associated with downstream applications. However, existing methods mostly adopt an aggressive strategy by prompting LLM to determine a single gold label for each unlabeled sample. Due to the inherent uncertainty within LLMs, they often produce incorrect labels for difficult samples, severely compromising the data quality for downstream applications. Motivated by ambiguity aversion in human behaviors, we propose a novel candidate annotation paradigm wherein large language models are encouraged to output all possible labels when incurring uncertainty. To ensure unique labels are provided for downstream tasks, we develop a teacher-student framework CanDist that distills candidate annotations with a Small Language Model (SLM). We further provide a rigorous justification demonstrating that distilling candidate annotations from the teacher LLM offers superior theoretical guarantees compared to directly using single annotations. Extensive experiments across six text classification tasks validate the effectiveness of our proposed method. The source code is available at https://github.com/MingxuanXia/CanDist.",
      "arxiv_url": "https://arxiv.org/abs/2506.03857",
      "pdf_url": "https://arxiv.org/pdf/2506.03857",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00249",
      "title": "MIR: Methodology Inspiration Retrieval for Scientific Research Problems",
      "authors": [
        "Aniketh Garikaparthi",
        "Manasi S. Patwardhan",
        "Aditya Kanade",
        "Aman Hassan",
        "L. Vig",
        "Arman Cohan"
      ],
      "abstract": "There has been a surge of interest in harnessing the reasoning capabilities of Large Language Models (LLMs) to accelerate scientific discovery. While existing approaches rely on grounding the discovery process within the relevant literature, effectiveness varies significantly with the quality and nature of the retrieved literature. We address the challenge of retrieving prior work whose concepts can inspire solutions for a given research problem, a task we define as Methodology Inspiration Retrieval (MIR). We construct a novel dataset tailored for training and evaluating retrievers on MIR, and establish baselines. To address MIR, we build the Methodology Adjacency Graph (MAG); capturing methodological lineage through citation relationships. We leverage MAG to embed an\"intuitive prior\"into dense retrievers for identifying patterns of methodological inspiration beyond superficial semantic similarity. This achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average Precision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking strategies to MIR, yielding additional improvements of +4.5 in Recall@3 and +4.8 in mAP. Through extensive ablation studies and qualitative analyses, we exhibit the promise of MIR in enhancing automated scientific discovery and outline avenues for advancing inspiration-driven retrieval.",
      "arxiv_url": "https://arxiv.org/abs/2506.00249",
      "pdf_url": "https://arxiv.org/pdf/2506.00249",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23170",
      "title": "ZIPA: A family of efficient models for multilingual phone recognition",
      "authors": [
        "Jian Zhu",
        "Farhan Samir",
        "Eleanor Chodroff",
        "David R. Mortensen"
      ],
      "abstract": "We present ZIPA, a family of efficient speech models that advances the state-of-the-art performance of crosslinguistic phone recognition. We first curated IPAPack++, a large-scale multilingual speech corpus with 17,132 hours of normalized phone transcriptions and a novel evaluation set capturing unseen languages and sociophonetic variation. With the large-scale training data, ZIPA, including transducer (ZIPA-T) and CTC-based (ZIPA-CR) variants, leverage the efficient Zipformer backbones and outperform existing phone recognition systems with much fewer parameters. Further scaling via noisy student training on 11,000 hours of pseudo-labeled multilingual data yields further improvement. While ZIPA achieves strong performance on benchmarks, error analysis reveals persistent limitations in modeling sociophonetic diversity, underscoring challenges for future research.",
      "arxiv_url": "https://arxiv.org/abs/2505.23170",
      "pdf_url": "https://arxiv.org/pdf/2505.23170",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.08120",
      "title": "Conservative Bias in Large Language Models: Measuring Relation Predictions",
      "authors": [
        "Toyin Aguda",
        "Erik Wilson",
        "Allan Anzagira",
        "Simerjot Kaur",
        "Charese H. Smiley"
      ],
      "abstract": "Large language models (LLMs) exhibit pronounced conservative bias in relation extraction tasks, frequently defaulting to No_Relation label when an appropriate option is unavailable. While this behavior helps prevent incorrect relation assignments, our analysis reveals that it also leads to significant information loss when reasoning is not explicitly included in the output. We systematically evaluate this trade-off across multiple prompts, datasets, and relation types, introducing the concept of Hobson's choice to capture scenarios where models opt for safe but uninformative labels over hallucinated ones. Our findings suggest that conservative bias occurs twice as often as hallucination. To quantify this effect, we use SBERT and LLM prompts to capture the semantic similarity between conservative bias behaviors in constrained prompts and labels generated from semi-constrained and open-ended prompts.",
      "arxiv_url": "https://arxiv.org/abs/2506.08120",
      "pdf_url": "https://arxiv.org/pdf/2506.08120",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12134",
      "title": "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs",
      "authors": [
        "Yige Xu",
        "Xu Guo",
        "Zhiwei Zeng",
        "Chunyan Miao"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to solve complex reasoning tasks by generating intermediate reasoning steps. However, most existing approaches focus on hard token decoding, which constrains reasoning within the discrete vocabulary space and may not always be optimal. While recent efforts explore continuous-space reasoning, they often require full-model fine-tuning and suffer from catastrophic forgetting, limiting their applicability to state-of-the-art LLMs that already perform well in zero-shot settings with a proper instruction. To address this challenge, we propose a novel approach for continuous-space reasoning that does not require modifying the LLM. Specifically, we employ a lightweight fixed assistant model to speculatively generate instance-specific soft thought tokens as the initial chain of thoughts, which are then mapped into the LLM's representation space via a trainable projection module. Experimental results on five reasoning benchmarks demonstrate that our method enhances LLM reasoning performance through supervised, parameter-efficient fine-tuning. Source code is available at https://github.com/xuyige/SoftCoT.",
      "arxiv_url": "https://arxiv.org/abs/2502.12134",
      "pdf_url": "https://arxiv.org/pdf/2502.12134",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.08279",
      "title": "What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations",
      "authors": [
        "Dongqi Liu",
        "Chenxi Whitehouse",
        "Xi Yu",
        "Louis Mahon",
        "Rohit Saxena",
        "Zheng Zhao",
        "Yifu Qiu",
        "Mirella Lapata",
        "Vera Demberg"
      ],
      "abstract": "Transforming recorded videos into concise and accurate textual summaries is a growing challenge in multimodal learning. This paper introduces VISTA, a dataset specifically designed for video-to-text summarization in scientific domains. VISTA contains 18,599 recorded AI conference presentations paired with their corresponding paper abstracts. We benchmark the performance of state-of-the-art large models and apply a plan-based framework to better capture the structured nature of abstracts. Both human and automated evaluations confirm that explicit planning enhances summary quality and factual consistency. However, a considerable gap remains between models and human performance, highlighting the challenges of our dataset. This study aims to pave the way for future research on scientific video-to-text summarization.",
      "arxiv_url": "https://arxiv.org/abs/2502.08279",
      "pdf_url": "https://arxiv.org/pdf/2502.08279",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "4a396823e8a78d496b263464c9d1631317b7244f",
      "title": "PIPER: Benchmarking and Prompting Event Reasoning Boundary of LLMs via Debiasing-Distillation Enhanced Tuning",
      "authors": [
        "Zhicong Lu",
        "Changyuan Tian",
        "PeiguangLi PeiguangLi",
        "Li Jin",
        "Sirui Wang",
        "Wei Jia",
        "Ying Shen",
        "Guangluan Xu"
      ],
      "abstract": "While Large Language Models (LLMs) excel in diverse domains, their validity in event reasoning remains underexplored. Most existing works merely stagnate at assessing LLMs’ event reasoning with a single event relational type or reasoning format, failing to conduct a complete evaluation and provide a practical so-lution for capability enhancement. In this paper, we propose PIPER , the first comprehensive benchmark for P robing I nto the P erformance boundary of LLMs in E vent R easoning. Motivated by our evaluation observations and error patterns analysis, we meticulously craft 10K diverse instruction-tuning demonstrations to alleviate event reasoning-oriented data scarcity. Additionally, a novel D ebiasing and D istillation-E nhanced S upervised F ine-T uning ( D 2 E-SFT ) strategy is presented, which facilitates adhering to context and fixating significant contextual event information to elevate the event reasoning capability. Specifically, D 2 E-SFT removes the given sample’s context to construct an imagined sample, subtracting its logits to mitigate the bias of neglecting context and improve contextual faithfulness. To guide",
      "arxiv_url": "https://www.semanticscholar.org/paper/4a396823e8a78d496b263464c9d1631317b7244f",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.08640",
      "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse Attention",
      "authors": [
        "Emily Xiao",
        "Chin-Jou Li",
        "Yilin Zhang",
        "Graham Neubig",
        "Amanda Bertsch"
      ],
      "abstract": "Many-shot in-context learning has recently shown promise as an alternative to finetuning, with the major advantage that the same model can be served for multiple tasks. However, this shifts the computational burden from training-time to inference-time, making deployment of many-shot ICL challenging to justify in-practice. This cost is further increased if a custom demonstration set is retrieved for each inference example. We present Dynamic Block-Sparse Attention, a training-free framework for retrieval-based many-shot in-context learning. By combining carefully designed block-sparse attention and retrieval of cached groups of demonstrations, we achieve comparable per-example latency to finetuning while maintaining on average>95% of the best method's accuracy across strong ICL and finetuning baselines. We hope that this will further enable the deployment of many-shot ICL at scale.",
      "arxiv_url": "https://arxiv.org/abs/2503.08640",
      "pdf_url": "https://arxiv.org/pdf/2503.08640",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "4a5922be1c0abaf2e21b869376c0fcf95a5f3320",
      "title": "Lemmatisation & Morphological Analysis of Unedited Greek: Do Simple Tasks Need Complex Solutions?",
      "authors": [
        "C. Swaelens",
        "Ilse De Vos",
        "Els Lefever"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/4a5922be1c0abaf2e21b869376c0fcf95a5f3320",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.16148",
      "title": "Only a Little to the Left: A Theory-grounded Measure of Political Bias in Large Language Models",
      "authors": [
        "Mats Faulborn",
        "Indira Sen",
        "Max Pellert",
        "Andreas Spitz",
        "David Garcia"
      ],
      "abstract": "Prompt-based language models like GPT4 and LLaMa have been used for a wide variety of use cases such as simulating agents, searching for information, or for content analysis. For all of these applications and others, political biases in these models can affect their performance. Several researchers have attempted to study political bias in language models using evaluation suites based on surveys, such as the Political Compass Test (PCT), often finding a particular leaning favored by these models. However, there is some variation in the exact prompting techniques, leading to diverging findings, and most research relies on constrained-answer settings to extract model responses. Moreover, the Political Compass Test is not a scientifically valid survey instrument. In this work, we contribute a political bias measured informed by political science theory, building on survey design principles to test a wide variety of input prompts, while taking into account prompt sensitivity. We then prompt 11 different open and commercial models, differentiating between instruction-tuned and non-instruction-tuned models, and automatically classify their political stances from 88,110 responses. Leveraging this dataset, we compute political bias profiles across different prompt variations and find that while PCT exaggerates bias in certain models like GPT3.5, measures of political bias are often unstable, but generally more left-leaning for instruction-tuned models. Code and data are available on: https://github.com/MaFa211/theory_grounded_pol_bias",
      "arxiv_url": "https://arxiv.org/abs/2503.16148",
      "pdf_url": "https://arxiv.org/pdf/2503.16148",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17214",
      "title": "CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought",
      "authors": [
        "Boxuan Zhang",
        "Ruqi Zhang"
      ],
      "abstract": "Large language models (LLMs) excel in many tasks but struggle to accurately quantify uncertainty in their generated responses. This limitation makes it challenging to detect misinformation and ensure reliable decision-making. Existing uncertainty quantification (UQ) methods for LLMs are primarily prompt-wise rather than response-wise, often requiring multiple response samples, which incurs high computational costs. Moreover, LLMs have been shown to be overconfident, particularly when using reasoning steps to derive their answers. In this work, we propose CoT-UQ, a response-wise UQ framework that integrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT) into the UQ process. CoT-UQ captures critical information during inference by extracting keywords from each reasoning step and assessing their importance to the final answer. This key reasoning information is then aggregated to produce a final uncertainty estimate. We conduct extensive experiments based on Llama Family with model sizes varying from 8B to 13B across logical and mathematical reasoning tasks. Experimental results demonstrate that CoT-UQ significantly outperforms existing UQ methods, achieving an average improvement of 5.9% AUROC compared to current UQ methods. The code is available at: https://github.com/ZBox1005/CoT-UQ.",
      "arxiv_url": "https://arxiv.org/abs/2502.17214",
      "pdf_url": "https://arxiv.org/pdf/2502.17214",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.07301",
      "title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning",
      "authors": [
        "Zhenru Zhang",
        "Chujie Zheng",
        "Yangzhen Wu",
        "Beichen Zhang",
        "Runji Lin",
        "Bowen Yu",
        "Dayiheng Liu",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "abstract": "Process Reward Models (PRMs) emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), which aim to identify and mitigate intermediate errors in the reasoning processes. However, the development of effective PRMs faces significant challenges, particularly in data annotation and evaluation methodologies. In this paper, through extensive experiments, we demonstrate that commonly used Monte Carlo (MC) estimation-based data synthesis for PRMs typically yields inferior performance and generalization compared to LLM-as-a-judge and human annotation methods. MC estimation relies on completion models to evaluate current-step correctness, leading to inaccurate step verification. Furthermore, we identify potential biases in conventional Best-of-N (BoN) evaluation strategies for PRMs: (1) The unreliable policy models generate responses with correct answers but flawed processes, leading to a misalignment between the evaluation criteria of BoN and the PRM objectives of process verification. (2) The tolerance of PRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a significant proportion of minimum scores concentrated on the final answer steps, revealing the shift from process to outcome-based assessment in BoN Optimized PRMs. To address these challenges, we develop a consensus filtering mechanism that effectively integrates MC estimation with LLM-as-a-judge and advocates a more comprehensive evaluation framework that combines response-level and step-level metrics. Based on the mechanisms, we significantly improve both model performance and data efficiency in the BoN evaluation and the step-wise error identification task. Finally, we release a new state-of-the-art PRM that outperforms existing open-source alternatives and provides practical guidelines for future research in building process supervision models.",
      "arxiv_url": "https://arxiv.org/abs/2501.07301",
      "pdf_url": "https://arxiv.org/pdf/2501.07301",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.05040",
      "title": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution",
      "authors": [
        "Chengxing Xie",
        "Bowen Li",
        "Chang Gao",
        "He Du",
        "Wai Lam",
        "Difan Zou",
        "Kai Chen"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency across a variety of complex tasks. One significant application of LLMs is in tackling software engineering challenges, particularly in resolving real-world tasks on GitHub by fixing code based on the issues reported by the users. However, many current approaches rely on proprietary LLMs, which limits reproducibility, accessibility, and transparency. The critical components of LLMs for addressing software engineering issues and how their capabilities can be effectively enhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a novel open-source framework designed to effectively and efficiently resolve GitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval module and a code editing module. The retrieval module employs BM25 along with a lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the code editing module utilizes the other model to generate patches for the identified files. To mitigate the lack of publicly available datasets, we compile an extensive dataset that includes 110K GitHub issues along with their corresponding patches and train the two models of SWE-Fixer separately. We assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving competitive performance among open-source models with scores of 22.0% and 30.2%. Furthermore, SWE-Fixer reaches state-of-the-art performance (24.7% on Lite and 32.8% on Verified) with PASS_TO_PASS (P2P) filtering. Additionally, our approach requires only two model calls per instance, making it significantly more efficient than existing methods. These results highlight the effectiveness of SWE-Fixer in real-world code-fixing scenarios. We will make our model, dataset, and code publicly available at https://github.com/InternLM/SWE-Fixer.",
      "arxiv_url": "https://arxiv.org/abs/2501.05040",
      "pdf_url": "https://arxiv.org/pdf/2501.05040",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "4a9d49d3da9410757891775529c5b613d50bcbef",
      "title": "Colloquial Singaporean English Style Transfer with Fine-Grained Explainable Control",
      "authors": [
        "Jinggui Liang",
        "Dung Vo",
        "Yap Hong Xian",
        "Hai Leong Chieu",
        "K. M. Chai",
        "Jing Jiang",
        "Lizi Liao"
      ],
      "abstract": "Colloquial Singaporean English (Singlish) is an informal English marked by a unique blend of languages reflecting Singapore’s multicultural identity. Style transfer between Singlish and Standard (formal) English is vital for various applications, yet existing methods often lack explainability and fine-grained control. To fill this gap, we contribute in two key ways. First, we construct a large, high-quality dataset of formal and informal sentences, annotated across six linguistic aspects—Syntax, Lexical Borrow-ing, Pragmatics, Prosody/Phonology, Emoti-cons/Punctuation, and Code-Switching—with detailed explanations. Starting with manually annotated cases, we scaled the dataset to 140K with ensured quality. Second, inspired by the \"Society of Mind\" theory, we propose a novel multi-agent framework where large language models (LLMs) act as expert agents for each linguistic aspect. These agents collaborate by iteratively generating, critiquing, and refining responses to achieve controlled, explainable style transfer. Both automatic metrics and human evaluations confirm that our method enables precise, interpretable transformations, advancing explainability in NLP for Singlish. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/4a9d49d3da9410757891775529c5b613d50bcbef",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12467",
      "title": "Beyond Frameworks: Unpacking Collaboration Strategies in Multi-Agent Systems",
      "authors": [
        "Hao Wang",
        "Sendong Zhao",
        "Jingbo Wang",
        "Zewen Qiang",
        "Bing Qin",
        "Ting Liu"
      ],
      "abstract": "Multi-agent collaboration has emerged as a pivotal paradigm for addressing complex, distributed tasks in large language model (LLM)-driven applications. While prior research has focused on high-level architectural frameworks, the granular mechanisms governing agents, critical to performance and scalability, remain underexplored. This study systematically investigates four dimensions of collaboration strategies: (1) agent governance, (2) participation control, (3) interaction dynamics, and (4) dialogue history management. Through rigorous experimentation under two context-dependent scenarios: Distributed Evidence Integration (DEI) and Structured Evidence Synthesis (SES), we quantify the impact of these strategies on both task accuracy and computational efficiency. Our findings reveal that centralized governance, instructor-led participation, ordered interaction patterns, and instructor-curated context summarization collectively optimize the trade-off between decision quality and resource utilization with the support of the proposed Token-Accuracy Ratio (TAR). This work establishes a foundation for designing adaptive, scalable multi-agent systems, shifting the focus from structural novelty to strategic interaction mechanics.",
      "arxiv_url": "https://arxiv.org/abs/2505.12467",
      "pdf_url": "https://arxiv.org/pdf/2505.12467",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01776",
      "title": "MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation",
      "authors": [
        "Yile Liu",
        "Ziwei Ma",
        "Xiu Jiang",
        "Jinglu Hu",
        "Jing Chang",
        "Liang Li"
      ],
      "abstract": "With the rapid adoption of large language models (LLMs) in natural language processing, the ability to follow instructions has emerged as a key metric for evaluating their practical utility. However, existing evaluation methods often focus on single-language scenarios, overlooking the challenges and differences present in multilingual and cross-lingual contexts. To address this gap, we introduce MaXIFE: a comprehensive evaluation benchmark designed to assess instruction-following capabilities across 23 different languages with 1667 verifiable instruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based Evaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to evaluate several leading commercial LLMs, establishing baseline results for future comparisons. By providing a standardized tool for multilingual instruction-following evaluation, MaXIFE aims to advance research and development in natural language processing.",
      "arxiv_url": "https://arxiv.org/abs/2506.01776",
      "pdf_url": "https://arxiv.org/pdf/2506.01776",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15401",
      "title": "Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning",
      "authors": [
        "Xuetao Ma",
        "Wenbin Jiang",
        "Hua Huang"
      ],
      "abstract": "In-context learning (ICL) can significantly enhance the complex reasoning capabilities of large language models (LLMs), with the key lying in the selection and ordering of demonstration examples. Previous methods typically relied on simple features to measure the relevance between examples. We argue that these features are not sufficient to reflect the intrinsic connections between examples. In this study, we propose a curriculum ICL strategy guided by problem-solving logic. We select demonstration examples by analyzing the problem-solving logic and order them based on curriculum learning. Specifically, we constructed a problem-solving logic instruction set based on the BREAK dataset and fine-tuned a language model to analyze the problem-solving logic of examples. Subsequently, we selected appropriate demonstration examples based on problem-solving logic and assessed their difficulty according to the number of problem-solving steps. In accordance with the principles of curriculum learning, we ordered the examples from easy to hard to serve as contextual prompts. Experimental results on multiple benchmarks indicate that our method outperforms previous ICL approaches in terms of performance and efficiency, effectively enhancing the complex reasoning capabilities of LLMs. Our project will be released at https://github.com/maxuetao/CurriculumICL",
      "arxiv_url": "https://arxiv.org/abs/2502.15401",
      "pdf_url": "https://arxiv.org/pdf/2502.15401",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.12882",
      "title": "DAPI: Domain Adaptive Toxicity Probe Vector Intervention for Fine-Grained Detoxification",
      "authors": [
        "Cho Hyeonsu",
        "Dooyoung Kim",
        "Youngjoong Ko"
      ],
      "abstract": "There have been attempts to utilize linear probe for detoxification, with existing studies relying on a single toxicity probe vector to reduce toxicity. However, toxicity can be fine-grained into various subcategories, making it difficult to remove certain types of toxicity by using a single toxicity probe vector. To address this limitation, we propose a category-specific toxicity probe vector approach. First, we train multiple toxicity probe vectors for different toxicity categories. During generation, we dynamically select the most relevant toxicity probe vector based on the current context. Finally, the selected vector is dynamically scaled and subtracted from model. Our method successfully mitigated toxicity from categories that the single probe vector approach failed to detoxify. Experiments demonstrate that our approach achieves up to a 78.52% reduction in toxicity on the evaluation dataset, while fluency remains nearly unchanged, with only a 0.052% drop compared to the unsteered model.",
      "arxiv_url": "https://arxiv.org/abs/2503.12882",
      "pdf_url": "https://arxiv.org/pdf/2503.12882",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14119",
      "title": "Meaning Beyond Truth Conditions: Evaluating Discourse Level Understanding via Anaphora Accessibility",
      "authors": [
        "Xiaomeng Zhu",
        "Zhenghao Zhou",
        "Simon Charlow",
        "Robert Frank"
      ],
      "abstract": "We present a hierarchy of natural language understanding abilities and argue for the importance of moving beyond assessments of understanding at the lexical and sentence levels to the discourse level. We propose the task of anaphora accessibility as a diagnostic for assessing discourse understanding, and to this end, present an evaluation dataset inspired by theoretical research in dynamic semantics. We evaluate human and LLM performance on our dataset and find that LLMs and humans align on some tasks and diverge on others. Such divergence can be explained by LLMs' reliance on specific lexical items during language comprehension, in contrast to human sensitivity to structural abstractions.",
      "arxiv_url": "https://arxiv.org/abs/2502.14119",
      "pdf_url": "https://arxiv.org/pdf/2502.14119",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2508.01432",
      "title": "TripTailor: A Real-World Benchmark for Personalized Travel Planning",
      "authors": [
        "Kaimin Wang",
        "Yuanzhe Shen",
        "Changze Lv",
        "Xiaoqing Zheng",
        "Xuanjing Huang"
      ],
      "abstract": "The continuous evolution and enhanced reasoning capabilities of large language models (LLMs) have elevated their role in complex tasks, notably in travel planning, where demand for personalized, high-quality itineraries is rising. However, current benchmarks often rely on unrealistic simulated data, failing to reflect the differences between LLM-generated and real-world itineraries. Existing evaluation metrics, which primarily emphasize constraints, fall short of providing a comprehensive assessment of the overall quality of travel plans. To address these limitations, we introduce TripTailor, a benchmark designed specifically for personalized travel planning in real-world scenarios. This dataset features an extensive collection of over 500,000 real-world points of interest (POIs) and nearly 4,000 diverse travel itineraries, complete with detailed information, providing a more authentic evaluation framework. Experiments show that fewer than 10\\% of the itineraries generated by the latest state-of-the-art LLMs achieve human-level performance. Moreover, we identify several critical challenges in travel planning, including the feasibility, rationality, and personalized customization of the proposed solutions. We hope that TripTailor will drive the development of travel planning agents capable of understanding and meeting user needs while generating practical itineraries. Our code and dataset are available at https://github.com/swxkfm/TripTailor",
      "arxiv_url": "https://arxiv.org/abs/2508.01432",
      "pdf_url": "https://arxiv.org/pdf/2508.01432",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-08-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.17182",
      "title": "Dialogue Systems for Emotional Support via Value Reinforcement",
      "authors": [
        "Juhee Kim",
        "Chunghu Mok",
        "Jisun Lee",
        "Hyang Sook Kim",
        "Yohan Jo"
      ],
      "abstract": "Emotional support dialogue systems aim to reduce help-seekers' distress and help them overcome challenges. While human values$\\unicode{x2013}$core beliefs that shape an individual's priorities$\\unicode{x2013}$are increasingly emphasized in contemporary psychological therapy for their role in fostering internal transformation and long-term emotional well-being, their integration into emotional support systems remains underexplored. To bridge this gap, we present a value-driven method for training emotional support dialogue systems designed to reinforce positive values in seekers. Notably, our model identifies which values to reinforce at each turn and how to do so, by leveraging online support conversations from Reddit. We evaluate the method across support skills, seekers' emotional intensity, and value reinforcement. Our method consistently outperforms various baselines, effectively exploring and eliciting values from seekers. Additionally, leveraging crowd knowledge from Reddit significantly enhances its effectiveness. Therapists highlighted its ability to validate seekers' challenges and emphasize positive aspects of their situations$\\unicode{x2013}$both crucial elements of value reinforcement. Our work, being the first to integrate value reinforcement into emotional support systems, demonstrates its promise and establishes a foundation for future research.",
      "arxiv_url": "https://arxiv.org/abs/2501.17182",
      "pdf_url": "https://arxiv.org/pdf/2501.17182",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "4b5624a720c0dbde7bbaed41bc00ec7ee798aed8",
      "title": "DYNTEXT: Semantic-Aware Dynamic Text Sanitization for Privacy-Preserving LLM Inference",
      "authors": [
        "Juhua Zhang",
        "Zhiliang Tian",
        "Minghang Zhu",
        "Yiping Song",
        "Taishu Sheng",
        "Siyi Yang",
        "Qiunan Du",
        "Xinwang Liu",
        "Minlie Huang",
        "Dongsheng Li"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/4b5624a720c0dbde7bbaed41bc00ec7ee798aed8",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16377",
      "title": "Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines",
      "authors": [
        "Saurabh Srivastava",
        "Sweta Pati",
        "Ziyu Yao"
      ],
      "abstract": "In this work, we study the effect of annotation guidelines -- textual descriptions of event types and arguments, when instruction-tuning large language models for event extraction. We conducted a series of experiments with both human-provided and machine-generated guidelines in both full- and low-data settings. Our results demonstrate the promise of annotation guidelines when there is a decent amount of training data and highlight its effectiveness in improving cross-schema generalization and low-frequency event-type performance.",
      "arxiv_url": "https://arxiv.org/abs/2502.16377",
      "pdf_url": "https://arxiv.org/pdf/2502.16377",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.20648",
      "title": "SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data",
      "authors": [
        "Michael Ogezi",
        "Freda Shi"
      ],
      "abstract": "Vision-language models (VLMs) work well in tasks ranging from image captioning to visual question answering (VQA), yet they struggle with spatial reasoning, a key skill for understanding our physical world that humans excel at. We find that spatial relations are generally rare in widely used VL datasets, with only a few being well represented, while most form a long tail of underrepresented relations. This gap leaves VLMs ill-equipped to handle diverse spatial relationships. To bridge it, we construct a synthetic VQA dataset focused on spatial reasoning generated from hyper-detailed image descriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset consists of 455k samples containing 3.4 million QA pairs. Trained on this dataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements on spatial reasoning benchmarks, achieving up to a 49% performance gain on the What's Up benchmark, while maintaining strong results on general tasks. Our work narrows the gap between human and VLM spatial reasoning and makes VLMs more capable in real-world tasks such as robotics and navigation.",
      "arxiv_url": "https://arxiv.org/abs/2504.20648",
      "pdf_url": "https://arxiv.org/pdf/2504.20648",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12788",
      "title": "Commonsense Reasoning in Arab Culture",
      "authors": [
        "A. Sadallah",
        "Junior Cedric Tonga",
        "Khalid Almubarak",
        "Saeed Almheiri",
        "Farah Atif",
        "Chatrine Qwaider",
        "Karima Kadaoui",
        "Sara Shatnawi",
        "Yaser Alesh",
        "Fajri Koto"
      ],
      "abstract": "Despite progress in Arabic large language models, such as Jais and AceGPT, their evaluation on commonsense reasoning has largely relied on machine-translated datasets, which lack cultural depth and may introduce Anglocentric biases. Commonsense reasoning is shaped by geographical and cultural contexts, and existing English datasets fail to capture the diversity of the Arab world. To address this, we introduce ArabCulture, a commonsense reasoning dataset in Modern Standard Arabic (MSA), covering cultures of 13 countries across the Gulf, Levant, North Africa, and the Nile Valley. The dataset was built from scratch by engaging native speakers to write and validate culturally relevant questions for their respective countries. ArabCulture spans 12 daily life domains with 54 fine-grained subtopics, reflecting various aspects of social norms, traditions, and everyday experiences. Zero-shot evaluations show that open-weight language models with up to 32B parameters struggle to comprehend diverse Arab cultures, with performance varying across regions. These findings highlight the need for more culturally aware models and datasets tailored to the Arabic-speaking world.",
      "arxiv_url": "https://arxiv.org/abs/2502.12788",
      "pdf_url": "https://arxiv.org/pdf/2502.12788",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.18132",
      "title": "MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection",
      "authors": [
        "Yibo Yan",
        "Shen Wang",
        "Jiahao Huo",
        "Philip S. Yu",
        "Xuming Hu",
        "Qingsong Wen"
      ],
      "abstract": "Mathematical error detection in educational settings presents a significant challenge for Multimodal Large Language Models (MLLMs), requiring a sophisticated understanding of both visual and textual mathematical content along with complex reasoning capabilities. Though effective in mathematical problem-solving, MLLMs often struggle with the nuanced task of identifying and categorizing student errors in multimodal mathematical contexts. Therefore, we introduce MathAgent, a novel Mixture-of-Math-Agent framework designed specifically to address these challenges. Our approach decomposes error detection into three phases, each handled by a specialized agent: an image-text consistency validator, a visual semantic interpreter, and an integrative error analyzer. This architecture enables more accurate processing of mathematical content by explicitly modeling relationships between multimodal problems and student solution steps. We evaluate MathAgent on real-world educational data, demonstrating approximately 5% higher accuracy in error step identification and 3% improvement in error categorization compared to baseline models. Besides, MathAgent has been successfully deployed in an educational platform that has served over one million K-12 students, achieving nearly 90% student satisfaction while generating significant cost savings by reducing manual error detection.",
      "arxiv_url": "https://arxiv.org/abs/2503.18132",
      "pdf_url": "https://arxiv.org/pdf/2503.18132",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2025-03-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.02694",
      "title": "Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers",
      "authors": [
        "Zhijian Xu",
        "Yilun Zhao",
        "Manasi S. Patwardhan",
        "L. Vig",
        "Arman Cohan"
      ],
      "abstract": "Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. We first present a comprehensive taxonomy of limitation types in scientific research, with a focus on AI. Guided by this taxonomy, for studying limitations, we present LimitGen, the first comprehensive benchmark for evaluating LLMs'capability to support early-stage feedback and complement human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of high-quality papers, and LimitGen-Human, a collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, we augment them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. Our approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback.",
      "arxiv_url": "https://arxiv.org/abs/2507.02694",
      "pdf_url": "https://arxiv.org/pdf/2507.02694",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11688",
      "title": "From Isolates to Families: Using Neural Networks for Automated Language Affiliation",
      "authors": [
        "Frederic Blum",
        "Steffen Herbold",
        "Johann-Mattis List"
      ],
      "abstract": "In historical linguistics, the affiliation of languages to a common language family is traditionally carried out using a complex workflow that relies on manually comparing individual languages. Large-scale standardized collections of multilingual wordlists and grammatical language structures might help to improve this and open new avenues for developing automated language affiliation workflows. Here, we present neural network models that use lexical and grammatical data from a worldwide sample of more than 1,000 languages with known affiliations to classify individual languages into families. In line with the traditional assumption of most linguists, our results show that models trained on lexical data alone outperform models solely based on grammatical data, whereas combining both types of data yields even better performance. In additional experiments, we show how our models can identify long-ranging relations between entire subgroups, how they can be employed to investigate potential relatives of linguistic isolates, and how they can help us to obtain first hints on the affiliation of so far unaffiliated languages. We conclude that models for automated language affiliation trained on lexical and grammatical data provide comparative linguists with a valuable tool for evaluating hypotheses about deep and unknown language relations.",
      "arxiv_url": "https://arxiv.org/abs/2502.11688",
      "pdf_url": "https://arxiv.org/pdf/2502.11688",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.02324",
      "title": "PromptCoT: Synthesizing Olympiad-level Problems for Mathematical Reasoning in Large Language Models",
      "authors": [
        "Xueliang Zhao",
        "Wei Wu",
        "Jian Guan",
        "Lingpeng Kong"
      ],
      "abstract": "The ability of large language models to solve complex mathematical problems has progressed significantly, particularly for tasks requiring advanced reasoning. However, the scarcity of sufficiently challenging problems, particularly at the Olympiad level, hinders further advancements. In this work, we introduce PromptCoT, a novel approach for automatically generating high-quality Olympiad-level math problems. The proposed method synthesizes complex problems based on mathematical concepts and the rationale behind problem construction, emulating the thought processes of experienced problem designers. We provide a theoretical analysis demonstrating that an optimal rationale should maximize both the likelihood of rationale generation given the associated concepts and the likelihood of problem generation conditioned on both the rationale and the concepts. Our method is evaluated on standard benchmarks including GSM8K, MATH-500, and AIME2024, where it consistently outperforms existing problem generation methods. Furthermore, we demonstrate that PromptCoT exhibits superior data scalability, consistently maintaining high performance as the dataset size increases, outperforming the baselines. The implementation is available at https://github.com/zhaoxlpku/PromptCoT.",
      "arxiv_url": "https://arxiv.org/abs/2503.02324",
      "pdf_url": "https://arxiv.org/pdf/2503.02324",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.04110",
      "title": "PEIRCE: Unifying Material and Formal Reasoning via LLM-Driven Neuro-Symbolic Refinement",
      "authors": [
        "Xin Quan",
        "Marco Valentino",
        "Danilo S. Carvalho",
        "Dhairya Dalal",
        "André Freitas"
      ],
      "abstract": "A persistent challenge in AI is the effective integration of material and formal inference - the former concerning the plausibility and contextual relevance of arguments, while the latter focusing on their logical and structural validity. Large Language Models (LLMs), by virtue of their extensive pre-training on large textual corpora, exhibit strong capabilities in material inference. However, their reasoning often lacks formal rigour and verifiability. At the same time, LLMs' linguistic competence positions them as a promising bridge between natural and formal languages, opening up new opportunities for combining these two modes of reasoning. In this paper, we introduce PEIRCE, a neuro-symbolic framework designed to unify material and formal inference through an iterative conjecture-criticism process. Within this framework, LLMs play the central role of generating candidate solutions in natural and formal languages, which are then evaluated and refined via interaction with external critique models. These critiques include symbolic provers, which assess formal validity, as well as soft evaluators that measure the quality of the generated arguments along linguistic and epistemic dimensions such as plausibility, coherence, and parsimony. While PEIRCE is a general-purpose framework, we demonstrate its capabilities in the domain of natural language explanation generation - a setting that inherently demands both material adequacy and formal correctness.",
      "arxiv_url": "https://arxiv.org/abs/2504.04110",
      "pdf_url": "https://arxiv.org/pdf/2504.04110",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01891",
      "title": "MMSciBench: Benchmarking Language Models on Chinese Multimodal Scientific Problems",
      "authors": [
        "Xinwu Ye",
        "Chengfan Li",
        "Siming Chen",
        "Wei Wei",
        "Robert Tang"
      ],
      "abstract": "Recent advances in large language models (LLMs) and vision-language models (LVLMs) have shown promise across many tasks, yet their scientific reasoning capabilities remain untested, particularly in multimodal settings. We present MMSciBench, a benchmark for evaluating mathematical and physical reasoning through text-only and text-image formats, with human-annotated difficulty levels, solutions with detailed explanations, and taxonomic mappings. Evaluation of state-of-the-art models reveals significant limitations, with even the best model achieving only \\textbf{63.77\\%} accuracy and particularly struggling with visual reasoning tasks. Our analysis exposes critical gaps in complex reasoning and visual-textual integration, establishing MMSciBench as a rigorous standard for measuring progress in multimodal scientific understanding. The code for MMSciBench is open-sourced at GitHub, and the dataset is available at Hugging Face.",
      "arxiv_url": "https://arxiv.org/abs/2503.01891",
      "pdf_url": "https://arxiv.org/pdf/2503.01891",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12304",
      "title": "Warmup Generations: A Task-Agnostic Approach for Guiding Sequence-to-Sequence Learning with Unsupervised Initial State Generation",
      "authors": [
        "Senyu Li",
        "Zipeng Sun",
        "Jiayi Wang",
        "Xue Liu",
        "Pontus Stenetorp",
        "Siva Reddy",
        "David Ifeoluwa Adelani"
      ],
      "abstract": "Traditional supervised fine-tuning (SFT) strategies for sequence-to-sequence tasks often train models to directly generate the target output. Recent work has shown that guiding models with intermediate steps, such as keywords, outlines, or reasoning chains, can significantly improve performance, coherence, and interpretability. However, these methods often depend on predefined intermediate formats and annotated data, limiting their scalability and generalizability. In this work, we introduce a task-agnostic framework that enables models to generate intermediate\"warmup\"sequences. These warmup sequences, serving as an initial state for subsequent generation, are optimized to enhance the probability of generating the target sequence without relying on external supervision or human-designed structures. Drawing inspiration from reinforcement learning principles, our method iteratively refines these intermediate steps to maximize their contribution to the final output, similar to reward-driven optimization in reinforcement learning with human feedback. Experimental results across tasks such as translation, summarization, and multi-choice question answering for logical reasoning show that our approach outperforms traditional SFT methods, and offers a scalable and flexible solution for sequence-to-sequence tasks.",
      "arxiv_url": "https://arxiv.org/abs/2502.12304",
      "pdf_url": "https://arxiv.org/pdf/2502.12304",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14131",
      "title": "Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering",
      "authors": [
        "Wei Zhou",
        "Mohsen Mesgar",
        "Heike Adel",
        "Annemarie Friedrich"
      ],
      "abstract": "In table question answering (TQA), tables are encoded as either texts or images. Prior work suggests that passing images of tables to multi-modal large language models (MLLMs) performs comparably to or even better than using textual input with large language models (LLMs). However, the lack of controlled setups limits fine-grained distinctions between these approaches. In this paper, we conduct the first controlled study on the effectiveness of several combinations of table representations and models from two perspectives: question complexity and table size. We build a new benchmark based on existing TQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we find that the best combination of table representation and model varies across setups. We propose FRES, a method selecting table representations dynamically, and observe a 10% average performance improvement compared to using both representations indiscriminately.",
      "arxiv_url": "https://arxiv.org/abs/2505.14131",
      "pdf_url": "https://arxiv.org/pdf/2505.14131",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16794",
      "title": "AAD-LLM: Neural Attention-Driven Auditory Scene Understanding",
      "authors": [
        "Xilin Jiang",
        "Sukru Samet Dindar",
        "Vishal Choudhari",
        "Stephan Bickel",
        "A. Mehta",
        "G. Mckhann",
        "A. Flinker",
        "Daniel Friedman",
        "N. Mesgarani"
      ],
      "abstract": "Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existing models do not incorporate this selectivity, limiting their ability to generate perception-aligned responses. To address this, we introduce Intention-Informed Auditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM (AAD-LLM), a prototype system that integrates brain signals to infer listener attention. AAD-LLM extends an auditory LLM by incorporating intracranial electroencephalography (iEEG) recordings to decode which speaker a listener is attending to and refine responses accordingly. The model first predicts the attended speaker from neural activity, then conditions response generation on this inferred attentional state. We evaluate AAD-LLM on speaker description, speech transcription and extraction, and question answering in multitalker scenarios, with both objective and subjective ratings showing improved alignment with listener intention. By taking a first step toward intention-aware auditory AI, this work explores a new paradigm where listener perception informs machine listening, paving the way for future listener-centered auditory systems. Demo and code available: https://aad-llm.github.io.",
      "arxiv_url": "https://arxiv.org/abs/2502.16794",
      "pdf_url": "https://arxiv.org/pdf/2502.16794",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.14491",
      "title": "Analyzing the Effect of Linguistic Similarity on Cross-Lingual Transfer: Tasks and Experimental Setups Matter",
      "authors": [
        "Verena Blaschke",
        "Masha Fedzechkina",
        "Maartje ter Hoeve"
      ],
      "abstract": "Cross-lingual transfer is a popular approach to increase the amount of training data for NLP tasks in a low-resource context. However, the best strategy to decide which cross-lingual data to include is unclear. Prior research often focuses on a small set of languages from a few language families and/or a single task. It is still an open question how these findings extend to a wider variety of languages and tasks. In this work, we analyze cross-lingual transfer for 263 languages from a wide variety of language families. Moreover, we include three popular NLP tasks: POS tagging, dependency parsing, and topic classification. Our findings indicate that the effect of linguistic similarity on transfer performance depends on a range of factors: the NLP task, the (mono- or multilingual) input representations, and the definition of linguistic similarity.",
      "arxiv_url": "https://arxiv.org/abs/2501.14491",
      "pdf_url": "https://arxiv.org/pdf/2501.14491",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "4ca4029d84ae29ffbadf00879721644c705dc4f5",
      "title": "MECoT: Markov Emotional Chain-of-Thought for Personality-Consistent Role-Playing",
      "authors": [
        "Yangbo Wei",
        "Zhen Huang",
        "Fangzhou Zhao",
        "Qi Feng",
        "Wei W. Xing"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/4ca4029d84ae29ffbadf00879721644c705dc4f5",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12636",
      "title": "Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing",
      "authors": [
        "Jiakuan Xie",
        "Pengfei Cao",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "Knowledge editing, which aims to update the knowledge encoded in language models, can be deceptive. Despite the fact that many existing knowledge editing algorithms achieve near-perfect performance on conventional metrics, the models edited by them are still prone to generating original knowledge. This paper introduces the concept of\"superficial editing\"to describe this phenomenon. Our comprehensive evaluation reveals that this issue presents a significant challenge to existing algorithms. Through systematic investigation, we identify and validate two key factors contributing to this issue: (1) the residual stream at the last subject position in earlier layers and (2) specific attention modules in later layers. Notably, certain attention heads in later layers, along with specific left singular vectors in their output matrices, encapsulate the original knowledge and exhibit a causal relationship with superficial editing. Furthermore, we extend our analysis to the task of superficial unlearning, where we observe consistent patterns in the behavior of specific attention heads and their corresponding left singular vectors, thereby demonstrating the robustness and broader applicability of our methodology and conclusions. Our code is available here.",
      "arxiv_url": "https://arxiv.org/abs/2505.12636",
      "pdf_url": "https://arxiv.org/pdf/2505.12636",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14837",
      "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs",
      "authors": [
        "Tao Ji",
        "Bin Guo",
        "Yuanbin Wu",
        "Qipeng Guo",
        "Lixing Shen",
        "Zhan Chen",
        "Xipeng Qiu",
        "Qi Zhang",
        "Tao Gui"
      ],
      "abstract": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by DeepSeek, designed to ensure efficient and economical inference by significantly compressing the Key-Value (KV) cache into a latent vector. Compared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its variants such as Grouped-Query Attention (GQA) exhibit significant cost disadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA without pre-training from scratch is both meaningful and challenging. This paper proposes the first data-efficient fine-tuning method for transitioning from MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE, we remove RoPE from dimensions of queries and keys that contribute less to the attention scores, for low-rank approximation, we introduce joint SVD approximations based on the pre-trained parameters of keys and values. These carefully designed strategies enable MHA2MLA to recover performance using only a small fraction (0.3% to 0.6%) of the data, significantly reducing inference costs while seamlessly integrating with compression techniques such as KV cache quantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%, with only a 0.5% drop in LongBench performance.",
      "arxiv_url": "https://arxiv.org/abs/2502.14837",
      "pdf_url": "https://arxiv.org/pdf/2502.14837",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.02173",
      "title": "Mass-Editing Memory with Attention in Transformers: A cross-lingual exploration of knowledge",
      "authors": [
        "Daniel Mela",
        "Aitor Gonzalez-Agirre",
        "Javier Hernando",
        "Marta Villegas"
      ],
      "abstract": "Recent research has explored methods for updating and modifying factual knowledge in large language models, often focusing on specific multi-layer perceptron blocks. This study expands on this work by examining the effectiveness of existing knowledge editing methods across languages and delving into the role of attention mechanisms in this process. Drawing from the insights gained, we propose Mass-Editing Memory with Attention in Transformers (MEMAT), a method that achieves significant improvements in all metrics while requiring minimal parameter modifications. MEMAT delivers a remarkable 10% increase in magnitude metrics, benefits languages not included in the training data and also demonstrates a high degree of portability. Our code and data are at https://github.com/dtamayo-nlp/MEMAT.",
      "arxiv_url": "https://arxiv.org/abs/2502.02173",
      "pdf_url": "https://arxiv.org/pdf/2502.02173",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11598",
      "title": "Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?",
      "authors": [
        "Leyi Pan",
        "Aiwei Liu",
        "Shiyu Huang",
        "Yijian Lu",
        "Xuming Hu",
        "Lijie Wen",
        "Irwin King",
        "Philip S. Yu"
      ],
      "abstract": "The radioactive nature of Large Language Model (LLM) watermarking enables the detection of watermarks inherited by student models when trained on the outputs of watermarked teacher models, making it a promising tool for preventing unauthorized knowledge distillation. However, the robustness of watermark radioactivity against adversarial actors remains largely unexplored. In this paper, we investigate whether student models can acquire the capabilities of teacher models through knowledge distillation while avoiding watermark inheritance. We propose two categories of watermark removal approaches: pre-distillation removal through untargeted and targeted training data paraphrasing (UP and TP), and post-distillation removal through inference-time watermark neutralization (WN). Extensive experiments across multiple model pairs, watermarking schemes and hyper-parameter settings demonstrate that both TP and WN thoroughly eliminate inherited watermarks, with WN achieving this while maintaining knowledge transfer efficiency and low computational overhead. Given the ongoing deployment of watermarking techniques in production LLMs, these findings emphasize the urgent need for more robust defense strategies. Our code is available at https://github.com/THU-BPM/Watermark-Radioactivity-Attack.",
      "arxiv_url": "https://arxiv.org/abs/2502.11598",
      "pdf_url": "https://arxiv.org/pdf/2502.11598",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "4d0adf6961e61405f3a19ef3a24dcba2db1532ae",
      "title": "KAPA: A Deliberative Agent Framework with Tree-Structured Knowledge Base for Multi-Domain User Intent Understanding",
      "authors": [
        "Jiakai Tang",
        "Shiqi Shen",
        "Zhipeng Wang",
        "Gong Zhi",
        "Xueyang Feng",
        "Zexu Sun",
        "Haoran Tan",
        "Xu Chen"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/4d0adf6961e61405f3a19ef3a24dcba2db1532ae",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01265",
      "title": "Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines",
      "authors": [
        "Do Xuan Long",
        "Duong Ngoc Yen",
        "Do Xuan Trong",
        "Anh Tuan Luu",
        "Kenji Kawaguchi",
        "Shafiq Joty",
        "Min-Yen Kan",
        "Nancy F. Chen"
      ],
      "abstract": "In-context learning (ICL) is an important yet not fully understood ability of pre-trained large language models (LLMs). It can greatly enhance task performance using a few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-form generation tasks such as summarization. Under appropriately realistic assumptions, we empirically and theoretically show that ICL demonstrations alone are insufficient to teach LLMs the task language and format distributions for generation. We argue for explicit exposure to the task distributions and hypothesize that defining them by prompting enhances model performance. To this end, we present LongGuide, which efficiently generates two parallel streams of guidelines capturing task language and format properties: (i) Metric Guidelines (MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output Constraint Guidelines (OCGs) that constrain generation at both token and sentence levels. LongGuide automatically selects the best combination of guidelines, improving both strong open- and closed-source LLMs by over 5% in both zero- and few-shot settings. We show that LongGuide is generalizable, learnable by weak models to enhance strong ones, and integrates synergistically with automatic prompt optimizers.",
      "arxiv_url": "https://arxiv.org/abs/2506.01265",
      "pdf_url": "https://arxiv.org/pdf/2506.01265",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.03417",
      "title": "When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding Models Against Misinformation Edits",
      "authors": [
        "Jabez Magomere",
        "Emanuele La Malfa",
        "Manuel Tonneau",
        "Ashkan Kazemi",
        "Scott Hale"
      ],
      "abstract": "Online misinformation remains a critical challenge, and fact-checkers increasingly rely on claim matching systems that use sentence embedding models to retrieve relevant fact-checks. However, as users interact with claims online, they often introduce edits, and it remains unclear whether current embedding models used in retrieval are robust to such edits. To investigate this, we introduce a perturbation framework that generates valid and natural claim variations, enabling us to assess the robustness of a wide-range of sentence embedding models in a multi-stage retrieval pipeline and evaluate the effectiveness of various mitigation approaches. Our evaluation reveals that standard embedding models exhibit notable performance drops on edited claims, while LLM-distilled embedding models offer improved robustness at a higher computational cost. Although a strong reranker helps to reduce the performance drop, it cannot fully compensate for first-stage retrieval gaps. To address these retrieval gaps, we evaluate train- and inference-time mitigation approaches, demonstrating that they can improve in-domain robustness by up to 17 percentage points and boost out-of-domain generalization by 10 percentage points. Overall, our findings provide practical improvements to claim-matching systems, enabling more reliable fact-checking of evolving misinformation. Code and data are available at https://github.com/JabezNzomo99/claim-matching-robustness.",
      "arxiv_url": "https://arxiv.org/abs/2503.03417",
      "pdf_url": "https://arxiv.org/pdf/2503.03417",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11095",
      "title": "Towards Better Evaluation for Generated Patent Claims",
      "authors": [
        "Lekang Jiang",
        "Pascal A Scherz",
        "Stephan Goetz"
      ],
      "abstract": "Patent claims define the scope of protection and establish the legal boundaries of an invention. Drafting these claims is a complex and time-consuming process that usually requires the expertise of skilled patent attorneys, which can form a large access barrier for many small enterprises. To solve these challenges, researchers have investigated the use of large language models (LLMs) for automating patent claim generation. However, existing studies highlight inconsistencies between automated evaluation metrics and human expert assessments. To bridge this gap, we introduce Patent-CE, the first comprehensive benchmark for evaluating patent claims. Patent-CE includes comparative claim evaluations annotated by patent experts, focusing on five key criteria: feature completeness, conceptual clarity, terminology consistency, logical linkage, and overall quality. Additionally, we propose PatClaimEval, a novel multi-dimensional evaluation method specifically designed for patent claims. Our experiments demonstrate that PatClaimEval achieves the highest correlation with human expert evaluations across all assessment criteria among all tested metrics. This research provides the groundwork for more accurate evaluations of automated patent claim generation systems.",
      "arxiv_url": "https://arxiv.org/abs/2505.11095",
      "pdf_url": "https://arxiv.org/pdf/2505.11095",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "4d3e252af289e26ebb0baa0947d5e1f50741a514",
      "title": "Dynamic Steering With Episodic Memory For Large Language Models",
      "authors": [
        "Van Dai Do",
        "Q. Tran",
        "S. Venkatesh",
        "Hung Le"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/4d3e252af289e26ebb0baa0947d5e1f50741a514",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.07826",
      "title": "Magnet: Multi-turn Tool-use Data Synthesis and Distillation via Graph Translation",
      "authors": [
        "Fan Yin",
        "Zifeng Wang",
        "I-Hung Hsu",
        "Jun Yan",
        "Ke Jiang",
        "Yanfei Chen",
        "Jindong Gu",
        "Long T. Le",
        "Kai-Wei Chang",
        "Chen-Yu Lee",
        "Hamid Palangi",
        "Tomas Pfister"
      ],
      "abstract": "Large language models (LLMs) have exhibited the ability to effectively utilize external tools to address user queries. However, their performance may be limited in complex, multi-turn interactions involving users and multiple tools. To address this, we propose Magnet, a principled framework for synthesizing high-quality training trajectories to enhance the function calling capability of large language model agents in multi-turn conversations with humans. The framework is based on automatic and iterative translations from a function signature path to a sequence of queries and executable function calls. We model the complicated function interactions in multi-turn cases with graph and design novel node operations to build reliable signature paths. Motivated by context distillation, when guiding the generation of positive and negative trajectories using a teacher model, we provide reference function call sequences as positive hints in context and contrastive, incorrect function calls as negative hints. Experiments show that training with the positive trajectories with supervised fine-tuning and preference optimization against negative trajectories, our 14B model, Magnet-14B-mDPO, obtains 68.01 on BFCL-v3 and 73.30 on ToolQuery, surpassing the performance of the teacher model Gemini-1.5-pro-002 by a large margin in function calling.",
      "arxiv_url": "https://arxiv.org/abs/2503.07826",
      "pdf_url": "https://arxiv.org/pdf/2503.07826",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-03-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.00472",
      "title": "Memorizing is Not Enough: Deep Knowledge Injection Through Reasoning",
      "authors": [
        "Ruoxi Xu",
        "Yunjie Ji",
        "Boxi Cao",
        "Yaojie Lu",
        "Hongyu Lin",
        "Xianpei Han",
        "Ben He",
        "Yingfei Sun",
        "Xiangang Li",
        "Le Sun"
      ],
      "abstract": "Although large language models (LLMs) excel in knowledge recall and reasoning, their static nature leads to outdated information as the real world evolves or when adapting to domain-specific knowledge, highlighting the need for effective knowledge injection. However, current research on knowledge injection remains superficial, mainly focusing on knowledge memorization and retrieval. This paper proposes a four-tier knowledge injection framework that systematically defines the levels of knowledge injection: memorization, retrieval, reasoning, and association. Based on this framework, we introduce DeepKnowledge, a synthetic experimental testbed designed for fine-grained evaluation of the depth of knowledge injection across three knowledge types (novel, incremental, and updated). We then explore various knowledge injection scenarios and evaluate the depth of knowledge injection for each scenario on the benchmark. Experimental results reveal key factors to reach each level of knowledge injection for LLMs and establish a mapping between the levels of knowledge injection and the corresponding suitable injection methods, aiming to provide a comprehensive approach for efficient knowledge injection across various levels.",
      "arxiv_url": "https://arxiv.org/abs/2504.00472",
      "pdf_url": "https://arxiv.org/pdf/2504.00472",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "4ded0c20dcbc0c50b900ff35a7377820d63d1caf",
      "title": "Empathy Prediction from Diverse Perspectives",
      "authors": [
        "Francine Chen",
        "Scott A. Carter",
        "Tatiana Lau",
        "N. Bravo",
        "Sumanta Bhattacharyya",
        "Kate A. Sieck",
        "Charlene C. Wu"
      ],
      "abstract": "A person’s perspective on a topic can influence their empathy towards a story. To investigate the use of personal perspective in empathy prediction, we collected a dataset, Empa-thyFromPerspectives, where a user rates their empathy towards a story by a person with a different perspective on a prompted topic. We observed in the dataset that user perspective can be important for empathy prediction and developed a model, PPEP, that uses a rater’s perspective as context for predicting the rater’s empathy towards a story. Experiments comparing PPEP with baseline models show that use of personal perspective significantly improves performance. A user study indicated that human empathy ratings of stories generally agreed with PPEP’s relative empathy rankings.",
      "arxiv_url": "https://www.semanticscholar.org/paper/4ded0c20dcbc0c50b900ff35a7377820d63d1caf",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06910",
      "title": "Causal Graph based Event Reasoning using Semantic Relation Experts",
      "authors": [
        "Mahnaz Koupaee",
        "Xueying Bai",
        "Mudan Chen",
        "Greg Durrett",
        "Nathanael Chambers",
        "Niranjan Balasubramanian"
      ],
      "abstract": "Understanding how events in a scenario causally connect with each other is important for effectively modeling and reasoning about events. But event reasoning remains a difficult challenge, and despite recent advances, Large Language Models (LLMs) still struggle to accurately identify causal connections between events. This struggle leads to poor performance on deeper reasoning tasks like event forecasting and timeline understanding. To address this challenge, we investigate the generation of causal event graphs (e.g., A enables B) as a parallel mechanism to help LLMs explicitly represent causality during inference. This paper evaluates both how to generate correct graphs as well as how graphs can assist reasoning. We propose a collaborative approach to causal graph generation where we use LLMs to simulate experts that focus on specific semantic relations. The experts engage in multiple rounds of discussions which are then consolidated by a final expert. Then, to demonstrate the utility of causal graphs, we use them on multiple downstream applications, and also introduce a new explainable event prediction task that requires a causal chain of events in the explanation. These explanations are more informative and coherent than baseline generations. Finally, our overall approach not finetuned on any downstream task, achieves competitive results with state-of-the-art models on both forecasting and next event prediction tasks.",
      "arxiv_url": "https://arxiv.org/abs/2506.06910",
      "pdf_url": "https://arxiv.org/pdf/2506.06910",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13520",
      "title": "A Large and Balanced Corpus for Fine-grained Arabic Readability Assessment",
      "authors": [
        "Khalid N. Elmadani",
        "Nizar Habash",
        "Hanada Taha-Thomure"
      ],
      "abstract": "This paper introduces the Balanced Arabic Readability Evaluation Corpus (BAREC), a large-scale, fine-grained dataset for Arabic readability assessment. BAREC consists of 69,441 sentences spanning 1+ million words, carefully curated to cover 19 readability levels, from kindergarten to postgraduate comprehension. The corpus balances genre diversity, topical coverage, and target audiences, offering a comprehensive resource for evaluating Arabic text complexity. The corpus was fully manually annotated by a large team of annotators. The average pairwise inter-annotator agreement, measured by Quadratic Weighted Kappa, is 81.8%, reflecting a high level of substantial agreement. Beyond presenting the corpus, we benchmark automatic readability assessment across different granularity levels, comparing a range of techniques. Our results highlight the challenges and opportunities in Arabic readability modeling, demonstrating competitive performance across various methods. To support research and education, we make BAREC openly available, along with detailed annotation guidelines and benchmark results.",
      "arxiv_url": "https://arxiv.org/abs/2502.13520",
      "pdf_url": "https://arxiv.org/pdf/2502.13520",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11811",
      "title": "BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering",
      "authors": [
        "Taolin Zhang",
        "Dongyang Li",
        "Qizhou Chen",
        "Chengyu Wang",
        "Xiaofeng He"
      ],
      "abstract": "Multi-hop question answering (QA) involves finding multiple relevant passages and performing step-by-step reasoning to answer complex questions. Previous works on multi-hop QA employ specific methods from different modeling perspectives based on large language models (LLMs), regardless of the question types. In this paper, we first conduct an in-depth analysis of public multi-hop QA benchmarks, dividing the questions into four types and evaluating five types of cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step, Iterative-step, Sub-step, and Adaptive-step. We find that different types of multi-hop questions have varying degrees of sensitivity to different types of methods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to address multi-hop QA by specifically focusing on the correspondence between question types and methods, where each type of method is regarded as an ''operator'' by prompting LLMs differently. The first level of BELLE includes multiple agents that debate to obtain an executive plan of combined ''operators'' to address the multi-hop QA task comprehensively. During the debate, in addition to the basic roles of affirmative debater, negative debater, and judge, at the second level, we further leverage fast and slow debaters to monitor whether changes in viewpoints are reasonable. Extensive experiments demonstrate that BELLE significantly outperforms strong baselines in various datasets. Additionally, the model consumption of BELLE is higher cost-effectiveness than that of single models in more complex multi-hop QA scenarios.",
      "arxiv_url": "https://arxiv.org/abs/2505.11811",
      "pdf_url": "https://arxiv.org/pdf/2505.11811",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11127",
      "title": "G-Safeguard: A Topology-Guided Security Lens and Treatment on LLM-based Multi-agent Systems",
      "authors": [
        "Shilong Wang",
        "Gui-Min Zhang",
        "Miao Yu",
        "Guancheng Wan",
        "Fanci Meng",
        "Chongye Guo",
        "Kun Wang",
        "Yang Wang"
      ],
      "abstract": "Large Language Model (LLM)-based Multi-agent Systems (MAS) have demonstrated remarkable capabilities in various complex tasks, ranging from collaborative problem-solving to autonomous decision-making. However, as these systems become increasingly integrated into critical applications, their vulnerability to adversarial attacks, misinformation propagation, and unintended behaviors have raised significant concerns. To address this challenge, we introduce G-Safeguard, a topology-guided security lens and treatment for robust LLM-MAS, which leverages graph neural networks to detect anomalies on the multi-agent utterance graph and employ topological intervention for attack remediation. Extensive experiments demonstrate that G-Safeguard: (I) exhibits significant effectiveness under various attack strategies, recovering over 40% of the performance for prompt injection; (II) is highly adaptable to diverse LLM backbones and large-scale MAS; (III) can seamlessly combine with mainstream MAS with security guarantees. The code is available at https://github.com/wslong20/G-safeguard.",
      "arxiv_url": "https://arxiv.org/abs/2502.11127",
      "pdf_url": "https://arxiv.org/pdf/2502.11127",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06636",
      "title": "SafeLawBench: Towards Safe Alignment of Large Language Models",
      "authors": [
        "Chuxue Cao",
        "Han Zhu",
        "Jiaming Ji",
        "Qichao Sun",
        "Zhenghao Zhu",
        "Yinyu Wu",
        "Juntao Dai",
        "Yaodong Yang",
        "Sirui Han",
        "Yike Guo"
      ],
      "abstract": "With the growing prevalence of large language models (LLMs), the safety of LLMs has raised significant concerns. However, there is still a lack of definitive standards for evaluating their safety due to the subjective nature of current safety benchmarks. To address this gap, we conducted the first exploration of LLMs' safety evaluation from a legal perspective by proposing the SafeLawBench benchmark. SafeLawBench categorizes safety risks into three levels based on legal standards, providing a systematic and comprehensive framework for evaluation. It comprises 24,860 multi-choice questions and 1,106 open-domain question-answering (QA) tasks. Our evaluation included 2 closed-source LLMs and 18 open-source LLMs using zero-shot and few-shot prompting, highlighting the safety features of each model. We also evaluated the LLMs' safety-related reasoning stability and refusal behavior. Additionally, we found that a majority voting mechanism can enhance model performance. Notably, even leading SOTA models like Claude-3.5-Sonnet and GPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench, while the average accuracy of 20 LLMs remains at 68.8\\%. We urge the community to prioritize research on the safety of LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.06636",
      "pdf_url": "https://arxiv.org/pdf/2506.06636",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.13977",
      "title": "Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms",
      "authors": [
        "Rajvardhan Oak",
        "Muhammad Haroon",
        "C. Jo",
        "Magdalena Wojcieszak",
        "Anshuman Chhabra"
      ],
      "abstract": "Social media platforms utilize Machine Learning (ML) and Artificial Intelligence (AI) powered recommendation algorithms to maximize user engagement, which can result in inadvertent exposure to harmful content. Current moderation efforts, reliant on classifiers trained with extensive human-annotated data, struggle with scalability and adapting to new forms of harm. To address these challenges, we propose a novel re-ranking approach using Large Language Models (LLMs) in zero-shot and few-shot settings. Our method dynamically assesses and re-ranks content sequences, effectively mitigating harmful content exposure without requiring extensive labeled data. Alongside traditional ranking metrics, we also introduce two new metrics to evaluate the effectiveness of re-ranking in reducing exposure to harmful content. Through experiments on three datasets, three models and across three configurations, we demonstrate that our LLM-based approach significantly outperforms existing proprietary moderation approaches, offering a scalable and adaptable solution for harm mitigation.",
      "arxiv_url": "https://arxiv.org/abs/2501.13977",
      "pdf_url": "https://arxiv.org/pdf/2501.13977",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04249",
      "title": "How to Mitigate Overfitting in Weak-to-strong Generalization?",
      "authors": [
        "Junhao Shi",
        "Qinyuan Cheng",
        "Zhaoye Fei",
        "Y. Zheng",
        "Qipeng Guo",
        "Xipeng Qiu"
      ],
      "abstract": "Aligning powerful AI models on tasks that surpass human evaluation capabilities is the central problem of \\textbf{superalignment}. To address this problem, weak-to-strong generalization aims to elicit the capabilities of strong models through weak supervisors and ensure that the behavior of strong models aligns with the intentions of weak supervisors without unsafe behaviors such as deception. Although weak-to-strong generalization exhibiting certain generalization capabilities, strong models exhibit significant overfitting in weak-to-strong generalization: Due to the strong fit ability of strong models, erroneous labels from weak supervisors may lead to overfitting in strong models. In addition, simply filtering out incorrect labels may lead to a degeneration in question quality, resulting in a weak generalization ability of strong models on hard questions. To mitigate overfitting in weak-to-strong generalization, we propose a two-stage framework that simultaneously improves the quality of supervision signals and the quality of input questions. Experimental results in three series of large language models and two mathematical benchmarks demonstrate that our framework significantly improves PGR compared to naive weak-to-strong generalization, even achieving up to 100\\% PGR on some models.",
      "arxiv_url": "https://arxiv.org/abs/2503.04249",
      "pdf_url": "https://arxiv.org/pdf/2503.04249",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13179",
      "title": "PTQ1.61: Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models",
      "authors": [
        "Jiaqi Zhao",
        "Miao Zhang",
        "Ming Wang",
        "Yuzhang Shang",
        "Kaihao Zhang",
        "Weili Guan",
        "Yaowei Wang",
        "Min Zhang"
      ],
      "abstract": "Large Language Models (LLMs) suffer severe performance degradation when facing extremely low-bit (sub 2-bit) quantization. Several existing sub 2-bit post-training quantization (PTQ) methods utilize a mix-precision scheme by leveraging an unstructured fine-grained mask to explicitly distinguish salient weights, while which introduces an extra 1-bit or more per weight. To explore the real limit of PTQ, we propose an extremely low-bit PTQ method called PTQ1.61, which enables weight quantization to 1.61-bit for the first time. Specifically, we first introduce a one-dimensional structured mask with negligibly additional 0.0002-bit per weight based on input activations from the perspective of reducing the upper bound of quantization error to allocate corresponding salient weight channels to 4-bit. For non-salient channels binarization, an efficient block-wise scaling factors optimization framework is then presented to take implicit row-wise correlations and angular biases into account. Different from prior works that concentrate on adjusting quantization methodologies, we further propose a novel paradigm called quantization preprocessing, where we argue that transforming the weight distribution of the pretrained model before quantization can alleviate the difficulty in per-channel extremely low-bit PTQ. Extensive experiments indicate our PTQ1.61 achieves state-of-the-art performance in extremely low-bit quantization. Codes are available at https://github.com/zjq0455/PTQ1.61.",
      "arxiv_url": "https://arxiv.org/abs/2502.13179",
      "pdf_url": "https://arxiv.org/pdf/2502.13179",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14301",
      "title": "SEA-HELM: Southeast Asian Holistic Evaluation of Language Models",
      "authors": [
        "Yosephine Susanto",
        "Adithya Venkatadri Hulagadri",
        "J. Montalan",
        "Jian Gang Ngui",
        "Xian Bin Yong",
        "Weiqi Leong",
        "Hamsawardhini Rengarajan",
        "Peerat Limkonchotiwat",
        "Yifan Mai",
        "William-Chandra Tjhi"
      ],
      "abstract": "With the rapid emergence of novel capabilities in Large Language Models (LLMs), the need for rigorous multilingual and multicultural benchmarks that are integrated has become more pronounced. Though existing LLM benchmarks are capable of evaluating specific capabilities of LLMs in English as well as in various mid- to low-resource languages, including those in the Southeast Asian (SEA) region, a comprehensive and culturally representative evaluation suite for the SEA languages has not been developed thus far. Here, we present SEA-HELM, a holistic linguistic and cultural LLM evaluation suite that emphasises SEA languages, comprising five core pillars: (1) NLP Classics, (2) LLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM currently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also introduce the SEA-HELM leaderboard, which allows users to understand models' multilingual and multicultural performance in a systematic and user-friendly manner. We make the SEA-HELM evaluation code publicly available.",
      "arxiv_url": "https://arxiv.org/abs/2502.14301",
      "pdf_url": "https://arxiv.org/pdf/2502.14301",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00334",
      "title": "Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind Benchmark for Large Language Models",
      "authors": [
        "G. Yeo",
        "Kokil Jaidka"
      ],
      "abstract": "Datasets used for emotion recognition tasks typically contain overt cues that can be used in predicting the emotions expressed in a text. However, one challenge is that texts sometimes contain covert contextual cues that are rich in affective semantics, which warrant higher-order reasoning abilities to infer emotional states, not simply the emotions conveyed. This study advances beyond surface-level perceptual features to investigate how large language models (LLMs) reason about others' emotional states using contextual information, within a Theory-of-Mind (ToM) framework. Grounded in Cognitive Appraisal Theory, we curate a specialized ToM evaluation dataset1 to assess both forward reasoning - from context to emotion- and backward reasoning - from emotion to inferred context. We showed that LLMs can reason to a certain extent, although they are poor at associating situational outcomes and appraisals with specific emotions. Our work highlights the need for psychological theories in the training and evaluation of LLMs in the context of emotion reasoning.",
      "arxiv_url": "https://arxiv.org/abs/2506.00334",
      "pdf_url": "https://arxiv.org/pdf/2506.00334",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13646",
      "title": "D.Va: Validate Your Demonstration First Before You Use It",
      "authors": [
        "Qi Zhang",
        "Zhiqing Xiao",
        "Rui Xiao",
        "Lirong Gao",
        "Junbo Zhao"
      ],
      "abstract": "In-context learning (ICL) has demonstrated significant potential in enhancing the capabilities of large language models (LLMs) during inference. It's well-established that ICL heavily relies on selecting effective demonstrations to generate outputs that better align with the expected results. As for demonstration selection, previous approaches have typically relied on intuitive metrics to evaluate the effectiveness of demonstrations, which often results in limited robustness and poor cross-model generalization capabilities. To tackle these challenges, we propose a novel method, \\textbf{D}emonstration \\textbf{VA}lidation (\\textbf{D.Va}), which integrates a demonstration validation perspective into this field. By introducing the demonstration validation mechanism, our method effectively identifies demonstrations that are both effective and highly generalizable. \\textbf{D.Va} surpasses all existing demonstration selection techniques across both natural language understanding (NLU) and natural language generation (NLG) tasks. Additionally, we demonstrate the robustness and generalizability of our approach across various language models with different retrieval models.",
      "arxiv_url": "https://arxiv.org/abs/2502.13646",
      "pdf_url": "https://arxiv.org/pdf/2502.13646",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04929",
      "title": "ConECT Dataset: Overcoming Data Scarcity in Context-Aware E-Commerce MT",
      "authors": [
        "Miko Pokrywka",
        "Wojciech Kusa",
        "Mieszko Rutkowski",
        "Mikołaj Koszowski"
      ],
      "abstract": "Neural Machine Translation (NMT) has improved translation by using Transformer-based models, but it still struggles with word ambiguity and context. This problem is especially important in domain-specific applications, which often have problems with unclear sentences or poor data quality. Our research explores how adding information to models can improve translations in the context of e-commerce data. To this end we create ConECT -- a new Czech-to-Polish e-commerce product translation dataset coupled with images and product metadata consisting of 11,400 sentence pairs. We then investigate and compare different methods that are applicable to context-aware translation. We test a vision-language model (VLM), finding that visual context aids translation quality. Additionally, we explore the incorporation of contextual information into text-to-text models, such as the product's category path or image descriptions. The results of our study demonstrate that the incorporation of contextual information leads to an improvement in the quality of machine translation. We make the new dataset publicly available.",
      "arxiv_url": "https://arxiv.org/abs/2506.04929",
      "pdf_url": "https://arxiv.org/pdf/2506.04929",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07621",
      "title": "LoRMA: Low-Rank Multiplicative Adaptation for LLMs",
      "authors": [
        "Harsh Bihany",
        "Shubham Patel",
        "Ashutosh Modi"
      ],
      "abstract": "Large Language Models have shown remarkable capabilities in the NLP domain. Their effectiveness can mainly be attributed to their ability to adapt to an array of downstream tasks. However, generally, full fine-tuning is a computationally expensive job. To mitigate this, many techniques have been developed that prime efficiency, a prominent one being Low-Rank Adaptation (LoRA). However, LoRA and its variants employ re-parametrized additive updates. In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which shifts the paradigm of additive updates to a richer space of matrix multiplicative transformations. We tackle challenges such as computational complexity and rank bottleneck of matrix multiplication by effectively re-ordering operations and introducing rank inflation strategies. We conduct extensive experiments to demonstrate the effectiveness of our approach in terms of various evaluation metrics.",
      "arxiv_url": "https://arxiv.org/abs/2506.07621",
      "pdf_url": "https://arxiv.org/pdf/2506.07621",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19586",
      "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization",
      "authors": [
        "Dingyu Yao",
        "Bowen Shen",
        "Zheng Lin",
        "Wei Liu",
        "Jian Luan",
        "Bin Wang",
        "Weiping Wang"
      ],
      "abstract": "The Key-Value (KV) cache in generative large language models (LLMs) introduces substantial memory overhead. Existing works mitigate this burden by offloading or compressing the KV cache. However, loading the entire cache incurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU communication, while aggressive compression causes notable performance degradation. We identify that certain layers in the LLM need to maintain global information and are unsuitable for selective loading. In contrast, other layers primarily focus on a few tokens with dominant activations that potentially incur substantial quantization error. This observation leads to a key insight that loading dominant tokens and quantizing all tokens can complement each other. Building on this insight, we propose a hybrid compression method, TailorKV, which seamlessly integrates quantization and offloading. TailorKV develops an inference framework along with a hardware-friendly implementation that leverages these complementary characteristics. Extensive long-context evaluations exhibit that TailorKV achieves nearly lossless performance under aggressive compression settings, outperforming the state-of-the-art. Particularly, the Llama-3.1-8B with 128k context can be served within a single RTX 3090 GPU, reaching 82 ms per token during decoding.",
      "arxiv_url": "https://arxiv.org/abs/2505.19586",
      "pdf_url": "https://arxiv.org/pdf/2505.19586",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.05122",
      "title": "DoCIA: An Online Document-Level Context Incorporation Agent for Speech Translation",
      "authors": [
        "Xinglin Lyu",
        "Wei Tang",
        "Yuang Li",
        "Xiaofeng Zhao",
        "Ming Zhu",
        "Junhui Li",
        "Yunfei Lu",
        "Min Zhang",
        "Daimeng Wei",
        "Hao Yang"
      ],
      "abstract": "Document-level context is crucial for handling discourse challenges in text-to-text document-level machine translation (MT). Despite the increased discourse challenges introduced by noise from automatic speech recognition (ASR), the integration of document-level context in speech translation (ST) remains insufficiently explored. In this paper, we develop DoCIA, an online framework that enhances ST performance by incorporating document-level context. DoCIA decomposes the ST pipeline into four stages. Document-level context is integrated into the ASR refinement, MT, and MT refinement stages through auxiliary LLM (large language model)-based modules. Furthermore, DoCIA leverages document-level information in a multi-level manner while minimizing computational overhead. Additionally, a simple yet effective determination mechanism is introduced to prevent hallucinations from excessive refinement, ensuring the reliability of the final results. Experimental results show that DoCIA significantly outperforms traditional ST baselines in both sentence and discourse metrics across four LLMs, demonstrating its effectiveness in improving ST performance.",
      "arxiv_url": "https://arxiv.org/abs/2504.05122",
      "pdf_url": "https://arxiv.org/pdf/2504.05122",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14352",
      "title": "SR-LLM: Rethinking the Structured Representation in Large Language Model",
      "authors": [
        "Jiahuan Zhang",
        "Tianheng Wang",
        "Hanqing Wu",
        "Ziyi Huang",
        "Yulong Wu",
        "Dongbai Chen",
        "Linfeng Song",
        "Yue Zhang",
        "Guozheng Rao",
        "Kaicheng Yu"
      ],
      "abstract": "Structured representations, exemplified by Abstract Meaning Representation (AMR), have long been pivotal in computational linguistics. However, their role remains ambiguous in the Large Language Models (LLMs) era. Initial attempts to integrate structured representation into LLMs via a zero-shot setting yielded inferior performance. We hypothesize that such a decline stems from the structure information being passed into LLMs in a code format unfamiliar to LLMs' training corpora. Consequently, we propose SR-LLM, an innovative framework with two settings to explore a superior way of integrating structured representation with LLMs from training-free and training-dependent perspectives. The former integrates structural information through natural language descriptions in LLM prompts, whereas its counterpart augments the model's inference capability through fine-tuning on linguistically described structured representations. Performance improvements were observed in widely downstream datasets, with particularly notable gains of 3.17% and 12.38% in PAWS. To the best of our knowledge, this work represents the pioneering demonstration that leveraging structural representations can substantially enhance LLMs' inference capability. We hope that our work sheds light and encourages future research to enhance the reasoning and interoperability of LLMs by structure data.",
      "arxiv_url": "https://arxiv.org/abs/2502.14352",
      "pdf_url": "https://arxiv.org/pdf/2502.14352",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "50249db1f16bd5f8bbf7b1fe0b39981931b20287",
      "title": "UCS-SQL: Uniting Content and Structure for Enhanced Semantic Bridging In Text-to-SQL",
      "authors": [
        "Zhenhe Wu",
        "Zhongqiu Li",
        "Jie Zhang",
        "Zhongjiang He",
        "Jian Yang",
        "Yu Zhao",
        "Ruiyu Fang",
        "Bing Wang",
        "Hongyan Xie",
        "Shuangyong Song",
        "Zhoujun Li"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/50249db1f16bd5f8bbf7b1fe0b39981931b20287",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.01743",
      "title": "Automating Legal Interpretation with LLMs: Retrieval, Generation, and Evaluation",
      "authors": [
        "Kangcheng Luo",
        "Quzhe Huang",
        "Cong Jiang",
        "Yansong Feng"
      ],
      "abstract": "Interpreting the law is always essential for the law to adapt to the ever-changing society. It is a critical and challenging task even for legal practitioners, as it requires meticulous and professional annotations and summarizations by legal experts, which are admittedly time-consuming and expensive to collect at scale. To alleviate the burden on legal experts, we propose a method for automated legal interpretation. Specifically, by emulating doctrinal legal research, we introduce a novel framework, ATRIE, to address Legal Concept Interpretation, a typical task in legal interpretation. ATRIE utilizes large language models (LLMs) to AuTomatically Retrieve concept-related information, Interpret legal concepts, and Evaluate generated interpretations, eliminating dependence on legal experts. ATRIE comprises a legal concept interpreter and a legal concept interpretation evaluator. The interpreter uses LLMs to retrieve relevant information from previous cases and interpret legal concepts. The evaluator uses performance changes on Legal Concept Entailment, a downstream task we propose, as a proxy of interpretation quality. Automated and multifaceted human evaluations indicate that the quality of our interpretations is comparable to those written by legal experts, with superior comprehensiveness and readability. Although there remains a slight gap in accuracy, it can already assist legal practitioners in improving the efficiency of legal interpretation.",
      "arxiv_url": "https://arxiv.org/abs/2501.01743",
      "pdf_url": "https://arxiv.org/pdf/2501.01743",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "50765ec78fc94148d1ba924d3625f146e59e09f1",
      "title": "Assessing Reliability and Political Bias In LLMs' Judgements of Formal and Material Inferences With Partisan Conclusions",
      "authors": [
        "Reto Gubelmann",
        "Ghassen Karray"
      ],
      "abstract": "This article examines LLMs’ ability to correctly label simple inferences with partisan conclusions. For this, we develop a dataset with both formal and material inferences, containing logically equivalent pairs of inferences with conclusions that favor either the political left or the political right. This allows us to focus on political bias as a source of decrease in performance. Our samples are synthetically generated and thus highly controlled, covering both English and German. We assess the performance of 16 configurations of both open and proprietary state-of-the-art LLMs on that dataset, finding generally unreliable performance as well as widespread political bias which, in the case of the English samples, persists throughout our experimental settings.",
      "arxiv_url": "https://www.semanticscholar.org/paper/50765ec78fc94148d1ba924d3625f146e59e09f1",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.16167",
      "title": "CodeReviewQA: The Code Review Comprehension Assessment for Large Language Models",
      "authors": [
        "Hong Yi Lin",
        "Chunhua Liu",
        "Haoyu Gao",
        "Patanamon Thongtanunam",
        "Christoph Treude"
      ],
      "abstract": "State-of-the-art large language models (LLMs) have demonstrated impressive code generation capabilities but struggle with real-world software engineering tasks, such as revising source code to address code reviews, hindering their practical use. Code review comments are often implicit, ambiguous, and colloquial, requiring models to grasp both code and human intent. This challenge calls for evaluating large language models' ability to bridge both technical and conversational contexts. While existing work has employed the automated code refinement (ACR) task to resolve these comments, current evaluation methods fall short, relying on text matching metrics that provide limited insight into model failures and remain susceptible to training data contamination. To address these limitations, we introduce a novel evaluation benchmark, $\\textbf{CodeReviewQA}$ that enables us to conduct fine-grained assessment of model capabilities and mitigate data contamination risks. In CodeReviewQA, we decompose the generation task of code refinement into $\\textbf{three essential reasoning steps}$: $\\textit{change type recognition}$ (CTR), $\\textit{change localisation}$ (CL), and $\\textit{solution identification}$ (SI). Each step is reformulated as multiple-choice questions with varied difficulty levels, enabling precise assessment of model capabilities, while mitigating data contamination risks. Our comprehensive evaluation spans 72 recently released large language models on $\\textbf{900 manually curated, high-quality examples}$ across nine programming languages. Our results show that CodeReviewQA is able to expose specific model weaknesses in code review comprehension, disentangled from their generative automated code refinement results.",
      "arxiv_url": "https://arxiv.org/abs/2503.16167",
      "pdf_url": "https://arxiv.org/pdf/2503.16167",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21003",
      "title": "Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?",
      "authors": [
        "Yifei Wang",
        "Yu Sheng",
        "Linjing Li",
        "D. Zeng"
      ],
      "abstract": "Recent advances in handling long sequences have facilitated the exploration of long-context in-context learning (ICL). While much of the existing research emphasizes performance improvements driven by additional in-context examples, the influence on the trustworthiness of generated responses remains underexplored. This paper addresses this gap by investigating how increased examples influence predictive uncertainty, an essential aspect in trustworthiness. We begin by systematically quantifying the uncertainty of ICL with varying shot counts, analyzing the impact of example quantity. Through uncertainty decomposition, we introduce a novel perspective on performance enhancement, with a focus on epistemic uncertainty (EU). Our results reveal that additional examples reduce total uncertainty in both simple and complex tasks by injecting task-specific knowledge, thereby diminishing EU and enhancing performance. For complex tasks, these advantages emerge only after addressing the increased noise and uncertainty associated with longer inputs. Finally, we explore the evolution of internal confidence across layers, unveiling the mechanisms driving the reduction in uncertainty.",
      "arxiv_url": "https://arxiv.org/abs/2505.21003",
      "pdf_url": "https://arxiv.org/pdf/2505.21003",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.01068",
      "title": "Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs",
      "authors": [
        "Yijie Jin",
        "Junjie Peng",
        "Xuanchao Lin",
        "Haochen Yuan",
        "Lan Wang",
        "Cangzhi Zheng"
      ],
      "abstract": "Multimodal Sentiment Analysis (MSA) is a rapidly developing field that integrates multimodal information to recognize sentiments, and existing models have made significant progress in this area. The central challenge in MSA is multimodal fusion, which is predominantly addressed by Multimodal Transformers (MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns. In this work, from the perspective of efficiency optimization, we propose and prove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and we introduce the graph-structured representation pattern of MulTs. Based on this pattern, we propose an Interlaced Mask (IM) mechanism to design the Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is formally equivalent to MulTs which achieves an efficient weight-sharing mechanism without information disorder through IM, enabling All-Modal-In-One fusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called Decomposition is implemented to ensure avoiding additional computational overhead. Moreover, it achieves significantly higher performance than traditional MulTs. To further validate the effectiveness of GsiT itself and the HMHG concept, we integrate them into multiple state-of-the-art models and demonstrate notable performance improvements and parameter reduction on widely used MSA datasets.",
      "arxiv_url": "https://arxiv.org/abs/2505.01068",
      "pdf_url": "https://arxiv.org/pdf/2505.01068",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2512.23300",
      "title": "AI4Reading: Chinese Audiobook Interpretation System Based on Multi-Agent Collaboration",
      "authors": [
        "Minjiang Huang",
        "Jipeng Qiang",
        "Yi Zhu",
        "Chaowei Zhang",
        "Zhaoxiang Zhang",
        "Kui Yu"
      ],
      "abstract": "Audiobook interpretations are attracting increasing attention, as they provide accessible and in-depth analyses of books that offer readers practical insights and intellectual inspiration. However, their manual creation process remains time-consuming and resource-intensive. To address this challenge, we propose AI4Reading, a multi-agent collaboration system leveraging large language models (LLMs) and speech synthesis technology to generate podcast, like audiobook interpretations. The system is designed to meet three key objectives: accurate content preservation, enhanced comprehensibility, and a logical narrative structure. To achieve these goals, we develop a framework composed of 11 specialized agents,including topic analysts, case analysts, editors, a narrator, and proofreaders that work in concert to explore themes, extract real world cases, refine content organization, and synthesize natural spoken language. By comparing expert interpretations with our system's output, the results show that although AI4Reading still has a gap in speech generation quality, the generated interpretative scripts are simpler and more accurate.",
      "arxiv_url": "https://arxiv.org/abs/2512.23300",
      "pdf_url": "https://arxiv.org/pdf/2512.23300",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-12-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04772",
      "title": "Identifying Reliable Evaluation Metrics for Scientific Text Revision",
      "authors": [
        "Léane Jourdan",
        "Florian Boudin",
        "Richard Dufour",
        "Nicolas Hernandez"
      ],
      "abstract": "Evaluating text revision in scientific writing remains a challenge, as traditional metrics such as ROUGE and BERTScore primarily focus on similarity rather than capturing meaningful improvements. In this work, we analyse and identify the limitations of these metrics and explore alternative evaluation methods that better align with human judgments. We first conduct a manual annotation study to assess the quality of different revisions. Then, we investigate reference-free evaluation metrics from related NLP domains. Additionally, we examine LLM-as-a-judge approaches, analysing their ability to assess revisions with and without a gold reference. Our results show that LLMs effectively assess instruction-following but struggle with correctness, while domain-specific metrics provide complementary insights. We find that a hybrid approach combining LLM-as-a-judge evaluation and task-specific metrics offers the most reliable assessment of revision quality.",
      "arxiv_url": "https://arxiv.org/abs/2506.04772",
      "pdf_url": "https://arxiv.org/pdf/2506.04772",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "51cfafe85e6c6693fcdf58432b9cae05d53cb7ea",
      "title": "Unveiling the Potential of BERT-family: A New Recipe for Building Scalable, General and Competitive Large Language Models",
      "authors": [
        "Yisheng Xiao",
        "Juntao Li",
        "Wenpeng Hu",
        "Zhunchen Luo",
        "Min Zhang"
      ],
      "abstract": "BERT-family have been increasingly explored for adaptation to scenarios beyond language understanding tasks, with more recent efforts focused on enabling them to become good instruction followers. These explorations have endowed BERT-family with new roles and human expectations, showcasing their potential on par with current state-of-the-art (SOTA) large language models (LLMs). However, several certain shortcomings in previous BERT-family, such as the relatively sub-optimal training corpora, learning procedure, and model architecture, all impede the further advancement of these models for serving as general and competitive LLMs. Therefore, we aim to address these deficiencies in this paper. Our study not only introduces a more suitable pre-training task that helps BERT-family excel in wider applications to realize generality but also explores the integration of cutting-edge technologies into our model to further enhance their capabilities. Our final models, termed Bi directional G eneral L anguage M odels ( BiGLM ), exhibit performance levels comparable to current SOTA LLMs across a spectrum of tasks. More-over, we conduct detailed analyses to study the effects of scaling and training corpora for BiGLM. To the best of our knowledge, our work represents the early attempt to offer a recipe for building novel types of scalable, general, and competitive LLMs that diverge from current autoregressive modeling methodology. Our codes and models are available on Github 1 .",
      "arxiv_url": "https://www.semanticscholar.org/paper/51cfafe85e6c6693fcdf58432b9cae05d53cb7ea",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00841",
      "title": "A Law Reasoning Benchmark for LLM with Tree-Organized Structures including Factum Probandum, Evidence and Experiences",
      "authors": [
        "Jiaxin Shen",
        "Jinan Xu",
        "Huiqi Hu",
        "Luyi Lin",
        "Fei Zheng",
        "Guoyang Ma",
        "Fandong Meng",
        "Jie Zhou",
        "Wenjuan Han"
      ],
      "abstract": "While progress has been made in legal applications, law reasoning, crucial for fair adjudication, remains unexplored. We propose a transparent law reasoning schema enriched with hierarchical factum probandum, evidence, and implicit experience, enabling public scrutiny and preventing bias. Inspired by this schema, we introduce the challenging task, which takes a textual case description and outputs a hierarchical structure justifying the final decision. We also create the first crowd-sourced dataset for this task, enabling comprehensive evaluation. Simultaneously, we propose an agent framework that employs a comprehensive suite of legal analysis tools to address the challenge task. This benchmark paves the way for transparent and accountable AI-assisted law reasoning in the ``Intelligent Court''.",
      "arxiv_url": "https://arxiv.org/abs/2503.00841",
      "pdf_url": "https://arxiv.org/pdf/2503.00841",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-03-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.03434",
      "title": "RASD: Retrieval-Augmented Speculative Decoding",
      "authors": [
        "Guofeng Quan",
        "Wenfeng Feng",
        "Chuzhan Hao",
        "Guochao Jiang",
        "Yuewei Zhang",
        "Hao Wang"
      ],
      "abstract": "Speculative decoding accelerates inference in large language models (LLMs) by generating draft tokens for target model verification. Current approaches for obtaining draft tokens rely on lightweight draft models or additional model structures to generate draft tokens and retrieve context from databases. Due to the draft model's small size and limited training data, model-based speculative decoding frequently becomes less effective in out-of-domain scenarios. Additionally, the time cost of the drafting phase results in a low upper limit on acceptance length during the verification step, limiting overall efficiency. This paper proposes RASD (Retrieval-Augmented Speculative Decoding), which adopts retrieval methods to enhance model-based speculative decoding. We introduce tree pruning and tree fusion to achieve this. Specifically, we develop a pruning method based on the draft model's probability distribution to construct the optimal retrieval tree. Second, we employ the longest prefix matching algorithm to merge the tree generated by the draft model with the retrieval tree, resulting in a unified tree for verification. Experimental results demonstrate that RASD achieves state-of-the-art inference acceleration across tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD exhibits strong scalability, seamlessly integrating with various speculative decoding approaches, including both generation-based and retrieval-based methods.",
      "arxiv_url": "https://arxiv.org/abs/2503.03434",
      "pdf_url": "https://arxiv.org/pdf/2503.03434",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.17550",
      "title": "HalluLens: LLM Hallucination Benchmark",
      "authors": [
        "Yejin Bang",
        "Ziwei Ji",
        "A. Schelten",
        "A. Hartshorn",
        "T. Fowler",
        "Cheng Zhang",
        "Nicola Cancedda",
        "Pascale Fung"
      ],
      "abstract": "Large language models (LLMs) often generate responses that deviate from user input or training data, a phenomenon known as\"hallucination.\"These hallucinations undermine user trust and hinder the adoption of generative AI systems. Addressing hallucinations is essential for the advancement of LLMs. This paper introduces a comprehensive hallucination benchmark, incorporating both new extrinsic and existing intrinsic evaluation tasks, built upon clear taxonomy of hallucination. A major challenge in benchmarking hallucinations is the lack of a unified framework due to inconsistent definitions and categorizations. We disentangle LLM hallucination from\"factuality,\"proposing a clear taxonomy that distinguishes between extrinsic and intrinsic hallucinations, to promote consistency and facilitate research. Extrinsic hallucinations, where the generated content is not consistent with the training data, are increasingly important as LLMs evolve. Our benchmark includes dynamic test set generation to mitigate data leakage and ensure robustness against such leakage. We also analyze existing benchmarks, highlighting their limitations and saturation. The work aims to: (1) establish a clear taxonomy of hallucinations, (2) introduce new extrinsic hallucination tasks, with data that can be dynamically regenerated to prevent saturation by leakage, (3) provide a comprehensive analysis of existing benchmarks, distinguishing them from factuality evaluations.",
      "arxiv_url": "https://arxiv.org/abs/2504.17550",
      "pdf_url": "https://arxiv.org/pdf/2504.17550",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "527d3040946d2c30648579a6db47e0de23fb1c2c",
      "title": "SEK: Self-Explained Keywords Empower Large Language Models for Code Generation",
      "authors": [
        "Lishui Fan",
        "Mouxiang Chen",
        "Zhongxin Liu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/527d3040946d2c30648579a6db47e0de23fb1c2c",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07443",
      "title": "LegalReasoner: Step-wised Verification-Correction for Legal Judgment Reasoning",
      "authors": [
        "Weijie Shi",
        "Han Zhu",
        "Jiaming Ji",
        "Mengze Li",
        "Jipeng Zhang",
        "Ruiyuan Zhang",
        "Jia Zhu",
        "Jiajie Xu",
        "Sirui Han",
        "Yike Guo"
      ],
      "abstract": "Legal judgment prediction (LJP) aims to function as a judge by making final rulings based on case claims and facts, which plays a vital role in the judicial domain for supporting court decision-making and improving judicial efficiency. However, existing methods often struggle with logical errors when conducting complex legal reasoning. We propose LegalReasoner, which enhances LJP reliability through step-wise verification and correction of the reasoning process. Specifically, it first identifies dispute points to decompose complex cases, and then conducts step-wise reasoning while employing a process verifier to validate each step's logic from correctness, progressiveness, and potential perspectives. When errors are detected, expert-designed attribution and resolution strategies are applied for correction. To fine-tune LegalReasoner, we release the LegalHK dataset, containing 58,130 Hong Kong court cases with detailed annotations of dispute points, step-by-step reasoning chains, and process verification labels. Experiments demonstrate that LegalReasoner significantly improves concordance with court decisions from 72.37 to 80.27 on LLAMA-3.1-70B. The data is available at https://huggingface.co/datasets/weijiezz/LegalHK.",
      "arxiv_url": "https://arxiv.org/abs/2506.07443",
      "pdf_url": "https://arxiv.org/pdf/2506.07443",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14354",
      "title": "WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications",
      "authors": [
        "Xin Li",
        "Mengbing Liu",
        "Li Wei",
        "Jiancheng An",
        "Mérouane Debbah",
        "Chau Yuen"
      ],
      "abstract": "Large Language Models (LLMs) have achieved impressive results across a broad array of tasks, yet their capacity for complex, domain-specific mathematical reasoning-particularly in wireless communications-remains underexplored. In this work, we introduce WirelessMathBench, a novel benchmark specifically designed to evaluate LLMs on mathematical modeling challenges to wireless communications engineering. Our benchmark consists of 587 meticulously curated questions sourced from 40 state-of-the-art research papers, encompassing a diverse spectrum of tasks ranging from basic multiple-choice questions to complex equation completion tasks, including both partial and full completions, all of which rigorously adhere to physical and dimensional constraints. Through extensive experimentation with leading LLMs, we observe that while many models excel in basic recall tasks, their performance degrades significantly when reconstructing partially or fully obscured equations, exposing fundamental limitations in current LLMs. Even DeepSeek-R1, the best performer on our benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83% success rate in full equation completion. By publicly releasing WirelessMathBench along with the evaluation toolkit, we aim to advance the development of more robust, domain-aware LLMs for wireless system analysis and broader engineering applications.",
      "arxiv_url": "https://arxiv.org/abs/2505.14354",
      "pdf_url": "https://arxiv.org/pdf/2505.14354",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03941",
      "title": "Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations",
      "authors": [
        "Vivian Nguyen",
        "Lillian Lee",
        "Cristian Danescu-Niculescu-Mizil"
      ],
      "abstract": "During a conversation, there can come certain moments where its outcome hangs in the balance. In these pivotal moments, how one responds can put the conversation on substantially different trajectories leading to significantly different outcomes. Systems that can detect when such moments arise could assist conversationalists in domains with highly consequential outcomes, such as mental health crisis counseling. In this work, we introduce an unsupervised computational method for detecting such pivotal moments as they happen, in an online fashion. Our approach relies on the intuition that a moment is pivotal if our expectation of the outcome varies widely depending on what might be said next. By applying our method to crisis counseling conversations, we first validate it by showing that it aligns with human perception -- counselors take significantly longer to respond during moments detected by our method -- and with the eventual conversational trajectory -- which is more likely to change course at these times. We then use our framework to explore the relation of the counselor's response during pivotal moments with the eventual outcome of the session.",
      "arxiv_url": "https://arxiv.org/abs/2506.03941",
      "pdf_url": "https://arxiv.org/pdf/2506.03941",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.21369",
      "title": "Turbocharging Web Automation: The Impact of Compressed History States",
      "authors": [
        "Xiyue Zhu",
        "Peng Tang",
        "Haofu Liao",
        "Srikar Appalaraju"
      ],
      "abstract": "Language models have led to a leap forward in web automation. The current web automation approaches take the current web state, history actions, and language instruction as inputs to predict the next action, overlooking the importance of history states. However, the highly verbose nature of web page states can result in long input sequences and sparse information, hampering the effective utilization of history states. In this paper, we propose a novel web history compressor approach to turbocharge web automation using history states. Our approach employs a history compressor module that distills the most task-relevant information from each history state into a fixed-length short representation, mitigating the challenges posed by the highly verbose history states. Experiments are conducted on the Mind2Web and WebLINX datasets to evaluate the effectiveness of our approach. Results show that our approach obtains 1.2-5.4% absolute accuracy improvements compared to the baseline approach without history inputs.",
      "arxiv_url": "https://arxiv.org/abs/2507.21369",
      "pdf_url": "https://arxiv.org/pdf/2507.21369",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "534c1d42e6d5e63129f7a575b98ea4dc2edac3b6",
      "title": "Privacy Preserving Data Selection for Bias Mitigation in Speech Models",
      "authors": [
        "Alkis Koudounas",
        "Eliana Pastor",
        "Vittorio Mazzia",
        "Manuel Giollo",
        "Thomas Gueudré",
        "Elisa Reale",
        "Luca Cagliero",
        "Sandro Cumani",
        "Luca de Alfaro",
        "Elena Baralis",
        "Daniele Amberti"
      ],
      "abstract": "Effectively selecting data from population sub-groups where a model performs poorly is crucial for improving its performance. Traditional methods for identifying these subgroups often rely on sensitive information, raising privacy issues. Additionally, gathering such information at runtime might be impractical. This paper introduces a cost-effective strategy that addresses these concerns. We identify underper-forming subgroups and train a model to predict if an utterance belongs to these subgroups without needing sensitive information. This model helps mitigate bias by selecting and adding new data, which is labeled as challenging, for re-training the speech model. Experimental results on intent classification and automatic speech recognition tasks show the effectiveness of our approach in reducing biases and enhancing performance, with improvements in reducing error rates of up to 39% for FSC, 16% for ITALIC, and 22% for LibriSpeech.",
      "arxiv_url": "https://www.semanticscholar.org/paper/534c1d42e6d5e63129f7a575b98ea4dc2edac3b6",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.19093",
      "title": "CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through Cryptography Challenges",
      "authors": [
        "Yu Li",
        "Qizhi Pei",
        "Mengyuan Sun",
        "Honglin Lin",
        "Chenlin Ming",
        "Xin Gao",
        "Jiang Wu",
        "Conghui He",
        "Lijun Wu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, especially the recent advancements in reasoning, such as o1 and o3, pushing the boundaries of AI. Despite these impressive achievements in mathematics and coding, the reasoning abilities of LLMs in domains requiring cryptographic expertise remain underexplored. In this paper, we introduce CipherBank, a comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs in cryptographic decryption tasks. CipherBank comprises 2,358 meticulously crafted problems, covering 262 unique plaintexts across 5 domains and 14 subdomains, with a focus on privacy-sensitive and real-world scenarios that necessitate encryption. From a cryptographic perspective, CipherBank incorporates 3 major categories of encryption methods, spanning 9 distinct algorithms, ranging from classical ciphers to custom cryptographic techniques. We evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and cutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results reveal significant gaps in reasoning abilities not only between general-purpose chat LLMs and reasoning-focused LLMs but also in the performance of current reasoning-focused models when applied to classical cryptographic decryption tasks, highlighting the challenges these models face in understanding and manipulating encrypted data. Through detailed analysis and error investigations, we provide several key observations that shed light on the limitations and potential improvement areas for LLMs in cryptographic reasoning. These findings underscore the need for continuous advancements in LLM reasoning capabilities.",
      "arxiv_url": "https://arxiv.org/abs/2504.19093",
      "pdf_url": "https://arxiv.org/pdf/2504.19093",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12473",
      "title": "TagRouter: Learning Route to LLMs through Tags for Open-Domain Text Generation Tasks",
      "authors": [
        "Zhou Chen",
        "Zhiqiang Wei",
        "Yuqi Bai",
        "Xue Xiong",
        "Jianmin Wu"
      ],
      "abstract": "Model routing allocates queries to the suitable model, improving system performance while reducing costs. However, existing routing methods face practical limitations that hinder scalability in large-scale applications and struggle to keep up with the rapid growth of the large language model (LLM) ecosystem. To tackle these challenges, we propose TagRouter, a training-free model routing method designed to optimize the synergy among multiple LLMs for open-domain text generation tasks. Experimental results demonstrate that TagRouter outperforms 13 baseline methods, increasing the accept rate of system by 6.15% and reducing costs by 17.20%, achieving optimal cost-efficiency. Our findings provides the LLM community with an efficient and scalable solution for model ensembling, offering users an evolvable\"super model.\"",
      "arxiv_url": "https://arxiv.org/abs/2506.12473",
      "pdf_url": "https://arxiv.org/pdf/2506.12473",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11075",
      "title": "Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models",
      "authors": [
        "Haoyang Li",
        "Xuejia Chen",
        "Zhanchao Xu",
        "Darian Li",
        "Nicole Hu",
        "Fei Teng",
        "Yiming Li",
        "Luyu Qiu",
        "Chen Jason Zhang",
        "Qing Li",
        "Lei Chen"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in natural language processing tasks, such as text generation and semantic understanding. However, their performance on numerical reasoning tasks, such as basic arithmetic, numerical retrieval, and magnitude comparison, remains surprisingly poor. This gap arises from their reliance on surface-level statistical patterns rather than understanding numbers as continuous magnitudes. Existing benchmarks primarily focus on either linguistic competence or structured mathematical problem-solving, neglecting fundamental numerical reasoning required in real-world scenarios. To bridge this gap, we propose NumericBench, a comprehensive benchmark to evaluate six fundamental numerical capabilities: number recognition, arithmetic operations, contextual retrieval, comparison, summary, and logical reasoning. NumericBench includes datasets ranging from synthetic number lists to the crawled real-world data, addressing challenges like long contexts, noise, and multi-step reasoning. Extensive experiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal persistent weaknesses in numerical reasoning, highlighting the urgent need to improve numerically-aware language modeling. The benchmark is released in: https://github.com/TreeAI-Lab/NumericBench.",
      "arxiv_url": "https://arxiv.org/abs/2502.11075",
      "pdf_url": "https://arxiv.org/pdf/2502.11075",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12025",
      "title": "SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities",
      "authors": [
        "Fengqing Jiang",
        "Zhangchen Xu",
        "Yuetai Li",
        "Luyao Niu",
        "Zhen Xiang",
        "Bo Li",
        "Bill Yuchen Lin",
        "Radha Poovendran"
      ],
      "abstract": "Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 12 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMs with SafeChain, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks.",
      "arxiv_url": "https://arxiv.org/abs/2502.12025",
      "pdf_url": "https://arxiv.org/pdf/2502.12025",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.04264",
      "title": "Lost in Multilinguality: Dissecting Cross-lingual Factual Inconsistency in Transformer Language Models",
      "authors": [
        "Mingyang Wang",
        "Heike Adel",
        "Lukas Lange",
        "Yihong Liu",
        "Ercong Nie",
        "Jannik Strotgen",
        "Hinrich Schutze"
      ],
      "abstract": "Multilingual language models (MLMs) store factual knowledge across languages but often struggle to provide consistent responses to semantically equivalent prompts in different languages. While previous studies point out this cross-lingual inconsistency issue, the underlying causes remain unexplored. In this work, we use mechanistic interpretability methods to investigate cross-lingual inconsistencies in MLMs. We find that MLMs encode knowledge in a language-independent concept space through most layers, and only transition to language-specific spaces in the final layers. Failures during the language transition often result in incorrect predictions in the target language, even when the answers are correct in other languages. To mitigate this inconsistency issue, we propose a linear shortcut method that bypasses computations in the final layers, enhancing both prediction accuracy and cross-lingual consistency. Our findings shed light on the internal mechanisms of MLMs and provide a lightweight, effective strategy for producing more consistent factual outputs.",
      "arxiv_url": "https://arxiv.org/abs/2504.04264",
      "pdf_url": "https://arxiv.org/pdf/2504.04264",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.10737",
      "title": "TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to Evolving Research Corpora",
      "authors": [
        "Priyanka Kargupta",
        "Nan Zhang",
        "Yunyi Zhang",
        "Rui Zhang",
        "Prasenjit Mitra",
        "Jiawei Han"
      ],
      "abstract": "The rapid evolution of scientific fields introduces challenges in organizing and retrieving scientific literature. While expert-curated taxonomies have traditionally addressed this need, the process is time-consuming and expensive. Furthermore, recent automatic taxonomy construction methods either (1) over-rely on a specific corpus, sacrificing generalizability, or (2) depend heavily on the general knowledge of large language models (LLMs) contained within their pre-training datasets, often overlooking the dynamic nature of evolving scientific domains. Additionally, these approaches fail to account for the multi-faceted nature of scientific literature, where a single research paper may contribute to multiple dimensions (e.g., methodology, new tasks, evaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a framework that dynamically adapts an LLM-generated taxonomy to a given corpus across multiple dimensions. TaxoAdapt performs iterative hierarchical classification, expanding both the taxonomy width and depth based on corpus' topical distribution. We demonstrate its state-of-the-art performance across a diverse set of computer science conferences over the years to showcase its ability to structure and capture the evolution of scientific fields. As a multidimensional method, TaxoAdapt generates taxonomies that are 26.51% more granularity-preserving and 50.41% more coherent than the most competitive baselines judged by LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.10737",
      "pdf_url": "https://arxiv.org/pdf/2506.10737",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.11463",
      "title": "Curiosity-Driven Reinforcement Learning from Human Feedback",
      "authors": [
        "Haoran Sun",
        "Yekun Chai",
        "Shuohuan Wang",
        "Yu Sun",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but often at the cost of reduced output diversity. This trade-off between diversity and alignment quality remains a significant challenge. Drawing inspiration from curiosity-driven exploration in reinforcement learning, we introduce curiosity-driven RLHF (CD-RLHF), a framework that incorporates intrinsic rewards for novel states, alongside traditional sparse extrinsic rewards, to optimize both output diversity and alignment quality. We demonstrate the effectiveness of CD-RLHF through extensive experiments on a range of tasks, including text summarization and instruction following. Our approach achieves significant gains in diversity on multiple diversity-oriented metrics while maintaining alignment with human preferences comparable to standard RLHF. We make our code publicly available at https://github.com/ernie-research/CD-RLHF.",
      "arxiv_url": "https://arxiv.org/abs/2501.11463",
      "pdf_url": "https://arxiv.org/pdf/2501.11463",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00481",
      "title": "PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion Strategies, Viewer Characteristics, and Persuasiveness Ratings",
      "authors": [
        "Junseo Kim",
        "Jongwook Han",
        "Dongmin Choi",
        "Jongwook Yoon",
        "Eun-Ju Lee",
        "Yohan Jo"
      ],
      "abstract": "Visual persuasion, which uses visual elements to influence cognition and behaviors, is crucial in fields such as advertising and political communication. With recent advancements in artificial intelligence, there is growing potential to develop persuasive systems that automatically generate persuasive images tailored to individuals. However, a significant bottleneck in this area is the lack of comprehensive datasets that connect the persuasiveness of images with the personal information about those who evaluated the images. To address this gap and facilitate technological advancements in personalized visual persuasion, we release the Personalized Visual Persuasion (PVP) dataset, comprising 28,454 persuasive images across 596 messages and 9 persuasion strategies. Importantly, the PVP dataset provides persuasiveness scores of images evaluated by 2,521 human annotators, along with their demographic and psychological characteristics (personality traits and values). We demonstrate the utility of our dataset by developing a persuasive image generator and an automated evaluator, and establish benchmark baselines. Our experiments reveal that incorporating psychological characteristics enhances the generation and evaluation of persuasive images, providing valuable insights for personalized visual persuasion.",
      "arxiv_url": "https://arxiv.org/abs/2506.00481",
      "pdf_url": "https://arxiv.org/pdf/2506.00481",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Personalization"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14742",
      "title": "Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis",
      "authors": [
        "Hong Huang",
        "Dapeng Wu"
      ],
      "abstract": "Large language models (LLMs) have made exciting achievements across various domains, yet their deployment on resource-constrained personal devices remains hindered by the prohibitive computational and memory demands of task-specific fine-tuning. While quantization offers a pathway to efficiency, existing methods struggle to balance performance and overhead, either incurring high computational/memory costs or failing to address activation outliers, a critical bottleneck in quantized fine-tuning. To address these challenges, we propose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning, certain activation outlier channels retain stable spatial positions across training iterations. Building on OSSH, we propose Quaff, a Quantized parameter-efficient fine-tuning framework for LLMs, optimizing low-precision activation representations through targeted momentum scaling. Quaff dynamically suppresses outliers exclusively in invariant channels using lightweight operations, eliminating full-precision weight storage and global rescaling while reducing quantization errors. Extensive experiments across ten benchmarks validate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA reasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory savings over full-precision fine-tuning while improving accuracy by 0.6% on the Phi-3 model, reconciling the triple trade-off between efficiency, performance, and deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080 Super) without sacrificing model utility, Quaff democratizes personalized LLM deployment. The code is available at https://github.com/Little0o0/Quaff.git.",
      "arxiv_url": "https://arxiv.org/abs/2505.14742",
      "pdf_url": "https://arxiv.org/pdf/2505.14742",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.08900",
      "title": "Can Uniform Meaning Representation Help GPT-4 Translate from Indigenous Languages?",
      "authors": [
        "Shira Wein"
      ],
      "abstract": "While ChatGPT and GPT-based models are able to effectively perform many tasks without additional fine-tuning, they struggle with tasks related to extremely low-resource languages and indigenous languages. Uniform Meaning Representation (UMR), a semantic representation designed to capture the meaning of texts in many languages, is well-positioned to be leveraged in the development of low-resource language technologies. In this work, we explore the downstream utility of UMR for low-resource languages by incorporating it into GPT-4 prompts. Specifically, we examine the ability of GPT-4 to perform translation from three indigenous languages (Navajo, Ar\\'apaho, and Kukama), with and without demonstrations, as well as with and without UMR annotations. Ultimately, we find that in the majority of our test cases, integrating UMR into the prompt results in a statistically significant increase in performance, which is a promising indication of future applications of the UMR formalism.",
      "arxiv_url": "https://arxiv.org/abs/2502.08900",
      "pdf_url": "https://arxiv.org/pdf/2502.08900",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.10970",
      "title": "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs",
      "authors": [
        "Nitay Calderon",
        "Roi Reichart",
        "Rotem Dror"
      ],
      "abstract": "The\"LLM-as-an-annotator\"and\"LLM-as-a-judge\"paradigms employ Large Language Models (LLMs) as annotators, judges, and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure, the Alternative Annotator Test (alt-test), that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM annotators and judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming the open-source LLMs we examine, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.",
      "arxiv_url": "https://arxiv.org/abs/2501.10970",
      "pdf_url": "https://arxiv.org/pdf/2501.10970",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.08385",
      "title": "Scholar Inbox: Personalized Paper Recommendations for Scientists",
      "authors": [
        "Markus Flicke",
        "Glenn Angrabeit",
        "Madhav Iyengar",
        "Vitalii Protsenko",
        "Illia Shakun",
        "Jovan Cicvaric",
        "Bora Kargi",
        "Haoyu He",
        "Lukas Schuler",
        "Lewin Scholz",
        "Kavyanjali Agnihotri",
        "Yong Cao",
        "Andreas Geiger"
      ],
      "abstract": "Scholar Inbox is a new open-access platform designed to address the challenges researchers face in staying current with the rapidly expanding volume of scientific literature. We provide personalized recommendations, continuous updates from open-access archives (arXiv, bioRxiv, etc.), visual paper summaries, semantic search, and a range of tools to streamline research workflows and promote open research access. The platform's personalized recommendation system is trained on user ratings, ensuring that recommendations are tailored to individual researchers' interests. To further enhance the user experience, Scholar Inbox also offers a map of science that provides an overview of research across domains, enabling users to easily explore specific topics. We use this map to address the cold start problem common in recommender systems, as well as an active learning strategy that iteratively prompts users to rate a selection of papers, allowing the system to learn user preferences quickly. We evaluate the quality of our recommendation system on a novel dataset of 800k user ratings, which we make publicly available, as well as via an extensive user study. https://www.scholar-inbox.com/",
      "arxiv_url": "https://arxiv.org/abs/2504.08385",
      "pdf_url": "https://arxiv.org/pdf/2504.08385",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Personalization"
      ],
      "published_date": "2025-04-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24251",
      "title": "Proactive Guidance of Multi-Turn Conversation in Industrial Search",
      "authors": [
        "Xiaoyu Li",
        "Xiao Li",
        "Li Gao",
        "Yiding Liu",
        "Xiaoyang Wang",
        "Shuaiqiang Wang",
        "Junfeng Wang",
        "Dawei Yin"
      ],
      "abstract": "The evolution of Large Language Models (LLMs) has significantly advanced multi-turn conversation systems, emphasizing the need for proactive guidance to enhance users' interactions. However, these systems face challenges in dynamically adapting to shifts in users' goals and maintaining low latency for real-time interactions. In the Baidu Search AI assistant, an industrial-scale multi-turn search system, we propose a novel two-phase framework to provide proactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning (G-SFT), employs a goal adaptation agent that dynamically adapts to user goal shifts and provides goal-relevant contextual information. G-SFT also incorporates scalable knowledge transfer to distill insights from LLMs into a lightweight model for real-time interaction. The second phase, Click-oriented Reinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically constructs preference pairs from user click signals, and proactively improves click-through rates through more engaging guidance. This dual-phase architecture achieves complementary objectives: G-SFT ensures accurate goal tracking, while C-RL optimizes interaction quality through click signal-driven reinforcement learning. Extensive experiments demonstrate that our framework achieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and 25.28% CTR in online deployment (149.06% relative improvement), while reducing inference latency by 69.55% through scalable knowledge distillation.",
      "arxiv_url": "https://arxiv.org/abs/2505.24251",
      "pdf_url": "https://arxiv.org/pdf/2505.24251",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.12368",
      "title": "InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model",
      "authors": [
        "Yuhang Zang",
        "Xiao-wen Dong",
        "Pan Zhang",
        "Yuhang Cao",
        "Ziyu Liu",
        "Shengyuan Ding",
        "Shenxi Wu",
        "Yubo Ma",
        "Haodong Duan",
        "Wenwei Zhang",
        "Kai Chen",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "abstract": "Despite the promising performance of Large Vision Language Models (LVLMs) in visual understanding, they occasionally generate incorrect outputs. While reward models (RMs) with reinforcement learning or test-time scaling offer the potential for improving generation quality, a critical gap remains: publicly available multi-modal RMs for LVLMs are scarce, and the implementation details of proprietary models are often unclear. We bridge this gap with InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective multi-modal reward model that aligns LVLMs with human preferences. To ensure the robustness and versatility of IXC-2.5-Reward, we set up a high-quality multi-modal preference corpus spanning text, image, and video inputs across diverse domains, such as instruction following, general understanding, text-rich documents, mathematical reasoning, and video understanding. IXC-2.5-Reward achieves excellent results on the latest multi-modal reward model benchmark and shows competitive performance on text-only reward model benchmarks. We further demonstrate three key applications of IXC-2.5-Reward: (1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward with Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows consistent improvements in instruction following and multi-modal open-ended dialogue; (2) Selecting the best response from candidate responses for test-time scaling; and (3) Filtering outlier or noisy samples from existing image and video instruction tuning training data. To ensure reproducibility and facilitate further research, we have open-sourced all model weights and training recipes at https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward",
      "arxiv_url": "https://arxiv.org/abs/2501.12368",
      "pdf_url": "https://arxiv.org/pdf/2501.12368",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00777",
      "title": "Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge",
      "authors": [
        "Md Tahmid Rahman Laskar",
        "Israt Jahan",
        "Elham Dolatabadi",
        "Chun Peng",
        "Enamul Hoque",
        "Jimmy X. Huang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in biomedical relation extraction, even in zero-shot scenarios. However, evaluating LLMs in this task remains challenging due to their ability to generate human-like text, often producing synonyms or abbreviations of gold-standard answers, making traditional automatic evaluation metrics unreliable. On the other hand, while human evaluation is more reliable, it is costly and time-consuming, making it impractical for real-world applications. This paper investigates the use of LLMs-as-the-Judge as an alternative evaluation method for biomedical relation extraction. We benchmark 8 LLMs as judges to evaluate the responses generated by 5 other LLMs across 3 biomedical relation extraction datasets. Unlike other text-generation tasks, we observe that LLM-based judges perform quite poorly (usually below 50% accuracy) in the biomedical relation extraction task. Our findings reveal that it happens mainly because relations extracted by LLMs do not adhere to any standard format. To address this, we propose structured output formatting for LLM-generated responses that helps LLM-Judges to improve their performance by about 15% (on average). We also introduce a domain adaptation technique to further enhance LLM-Judge performance by effectively transferring knowledge between datasets. We release both our human-annotated and LLM-annotated judgment data (36k samples in total) for public use here: https://github.com/tahmedge/llm_judge_biomedical_re.",
      "arxiv_url": "https://arxiv.org/abs/2506.00777",
      "pdf_url": "https://arxiv.org/pdf/2506.00777",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.07365",
      "title": "LongReD: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation",
      "authors": [
        "Zican Dong",
        "Junyi Li",
        "Jinhao Jiang",
        "Mingyu Xu",
        "Wayne Xin Zhao",
        "Bingning Wang",
        "Weipeng Chen"
      ],
      "abstract": "Large language models (LLMs) have gained extended context windows through scaling positional encodings and lightweight continual pre-training. However, this often leads to degraded performance on short-text tasks, while the reasons for this degradation remain insufficiently explored. In this work, we identify two primary factors contributing to this issue: distribution drift in hidden states and attention scores, and catastrophic forgetting during continual pre-training. To address these challenges, we propose Long Context Pre-training with Restoration Distillation (LongReD), a novel approach designed to mitigate short-text performance degradation through minimizing the distribution discrepancy between the extended and original models. Besides training on long texts, LongReD distills the hidden state of selected layers from the original model on short texts. Additionally, LongReD also introduces a short-to-long distillation, aligning the output distribution on short texts with that on long texts by leveraging skipped positional indices. Experiments on common text benchmarks demonstrate that LongReD effectively preserves the model's short-text performance while maintaining comparable or even better capacity to handle long texts than baselines. Our code is available at https://github.com/RUCAIBox/LongReD.",
      "arxiv_url": "https://arxiv.org/abs/2502.07365",
      "pdf_url": "https://arxiv.org/pdf/2502.07365",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-02-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.04962",
      "title": "VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models",
      "authors": [
        "Wenqian Cui",
        "Xiaoqi Jiao",
        "Ziqiao Meng",
        "Irwin King"
      ],
      "abstract": "With the rising need for speech-based interaction models, end-to-end Spoken Language Models (SLMs) have emerged as a promising solution. While these models require comprehensive world knowledge for meaningful and reliable human interactions, existing question-answering (QA) benchmarks fall short in evaluating SLMs' knowledge understanding due to their inability to support end-to-end speech evaluation and account for varied input audio conditions. To address these limitations, we present VoxEval, a novel SpeechQA benchmark that assesses SLMs' knowledge understanding through pure speech interactions. Our benchmark 1) uniquely maintains speech format for both inputs and outputs, 2) evaluates model robustness across diverse input audio conditions, and 3) pioneers the assessment of complex tasks like mathematical reasoning in spoken format. Systematic evaluation demonstrates that VoxEval presents significant challenges to current SLMs, revealing their sensitivity to varying audio conditions and highlighting the need to enhance reasoning capabilities in future development. We hope this benchmark could guide the advancement of more sophisticated and reliable SLMs. VoxEval dataset is available at: https://github.com/dreamtheater123/VoxEval",
      "arxiv_url": "https://arxiv.org/abs/2501.04962",
      "pdf_url": "https://arxiv.org/pdf/2501.04962",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.03962",
      "title": "On the Acquisition of Shared Grammatical Representations in Bilingual Language Models",
      "authors": [
        "Catherine Arnett",
        "Tyler A. Chang",
        "James A. Michaelov",
        "Benjamin Bergen"
      ],
      "abstract": "Crosslingual transfer is crucial to contemporary language models' multilingual capabilities, but how it occurs is not well understood. We ask what happens to a monolingual language model when it begins to be trained on a second language. Specifically, we train small bilingual models for which we control the amount of data for each language and the order of language exposure. To find evidence of shared multilingual representations, we turn to structural priming, a method used to study grammatical representations in humans. We first replicate previous crosslingual structural priming results and find that after controlling for training data quantity and language exposure, there are asymmetrical effects across language pairs and directions. We argue that this asymmetry may shape hypotheses about human structural priming effects. We also find that structural priming effects are less robust for less similar language pairs, highlighting potential limitations of crosslingual transfer learning and shared representations for typologically diverse languages.",
      "arxiv_url": "https://arxiv.org/abs/2503.03962",
      "pdf_url": "https://arxiv.org/pdf/2503.03962",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12355",
      "title": "QiMeng-Attention: SOTA Attention Operator is generated by SOTA Attention Algorithm",
      "authors": [
        "Qirui Zhou",
        "Shaohui Peng",
        "Weiqiang Xiong",
        "Haixin Chen",
        "Yuanbo Wen",
        "Haochen Li",
        "Ling Li",
        "Qi Guo",
        "Yongwei Zhao",
        "Ke Gao",
        "Rui Chen",
        "Yanjun Wu",
        "Chen Zhao",
        "Yunji Chen"
      ],
      "abstract": "The attention operator remains a critical performance bottleneck in large language models (LLMs), particularly for long-context scenarios. While FlashAttention is the most widely used and effective GPU-aware acceleration algorithm, it must require time-consuming and hardware-specific manual implementation, limiting adaptability across GPU architectures. Existing LLMs have shown a lot of promise in code generation tasks, but struggle to generate high-performance attention code. The key challenge is it cannot comprehend the complex data flow and computation process of the attention operator and utilize low-level primitive to exploit GPU performance. To address the above challenge, we propose an LLM-friendly Thinking Language (LLM-TL) to help LLMs decouple the generation of high-level optimization logic and low-level implementation on GPU, and enhance LLMs' understanding of attention operator. Along with a 2-stage reasoning workflow, TL-Code generation and translation, the LLMs can automatically generate FlashAttention implementation on diverse GPUs, establishing a self-optimizing paradigm for generating high-performance attention operators in attention-centric algorithms. Verified on A100, RTX8000, and T4 GPUs, the performance of our methods significantly outshines that of vanilla LLMs, achieving a speed-up of up to 35.16x. Besides, our method not only surpasses human-optimized libraries (cuDNN and official library) in most scenarios but also extends support to unsupported hardware and data types, reducing development time from months to minutes compared with human experts.",
      "arxiv_url": "https://arxiv.org/abs/2506.12355",
      "pdf_url": "https://arxiv.org/pdf/2506.12355",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02995",
      "title": "It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems",
      "authors": [
        "Iuliia Zaitova",
        "Badr M. Abdullah",
        "Wei Xue",
        "Dietrich Klakow",
        "Bernd Mobius",
        "T. Avgustinova"
      ],
      "abstract": "Idioms are defined as a group of words with a figurative meaning not deducible from their individual components. Although modern machine translation systems have made remarkable progress, translating idioms remains a major challenge, especially for speech-to-text systems, where research on this topic is notably sparse. In this paper, we systematically evaluate idiom translation as compared to conventional news translation in both text-to-text machine translation (MT) and speech-to-text translation (SLT) systems across two language pairs (German to English, Russian to English). We compare state-of-the-art end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large v3) with MT systems (SeamlessM4T SLT-to-text, No Language Left Behind), Large Language Models (DeepSeek, LLaMA) and cascaded alternatives. Our results reveal that SLT systems experience a pronounced performance drop on idiomatic data, often reverting to literal translations even in higher layers, whereas MT systems and Large Language Models demonstrate better handling of idioms. These findings underscore the need for idiom-specific strategies and improved internal representations in SLT architectures.",
      "arxiv_url": "https://arxiv.org/abs/2506.02995",
      "pdf_url": "https://arxiv.org/pdf/2506.02995",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "55b8e794f70750d155c6beccf8c5ca1cb48224df",
      "title": "LexCLiPR: Cross-Lingual Paragraph Retrieval from Legal Judgments",
      "authors": [
        "Rohit Upadhya",
        "Santosh T.Y.S.S"
      ],
      "abstract": "Efficient retrieval of pinpointed information from case law is crucial for legal professionals but challenging due to the length and complexity of legal judgments. Existing works mostly often focus on retrieving entire cases rather than precise, paragraph-level information. Moreover, multilingual legal practice necessitates cross-lingual retrieval, most works have been limited to monolingual settings. To address these gaps, we introduce LexCLiPR, a cross-lingual dataset for paragraph-level retrieval from European Court of Human Rights (ECtHR) judgments, leveraging multilingual case law guides and distant supervision to cu-rate our dataset. We evaluate retrieval models in a zero-shot setting, revealing the limitations of pre-trained multilingual models for cross-lingual tasks in low-resource languages and the importance of retrieval based post-training strategies. In fine-tuning settings, we observe that two-tower models excel in cross-lingual retrieval, while siamese architectures are better suited for monolingual tasks. Fine-tuning multilingual models on native language queries improves performance but struggles to generalize to unseen legal concepts, highlighting the need for robust strategies to address topical distribution shifts in the legal queries. 1 .",
      "arxiv_url": "https://www.semanticscholar.org/paper/55b8e794f70750d155c6beccf8c5ca1cb48224df",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "560c28169854c4a0eeeac5842ceb0430ec44a870",
      "title": "AdaV: Adaptive Text-visual Redirection for Vision-Language Models",
      "authors": [
        "Jiayi Han",
        "Liang Du",
        "Yiwen Wu",
        "Guanming Liang",
        "Xiangguo Zhou",
        "Weibo Zheng",
        "Donghong Han",
        "Zixun Sun"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/560c28169854c4a0eeeac5842ceb0430ec44a870",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.01015",
      "title": "Value Portrait: Assessing Language Models' Values through Psychometrically and Ecologically Valid Items",
      "authors": [
        "Jongwook Han",
        "Dongmin Choi",
        "Woojung Song",
        "Eun-Ju Lee",
        "Yohan Jo"
      ],
      "abstract": "The importance of benchmarks for assessing the values of language models has been pronounced due to the growing need of more authentic, human-aligned responses. However, existing benchmarks rely on human or machine annotations that are vulnerable to value-related biases. Furthermore, the tested scenarios often diverge from real-world contexts in which models are commonly used to generate text and express values. To address these issues, we propose the Value Portrait benchmark, a reliable framework for evaluating LLMs' value orientations with two key characteristics. First, the benchmark consists of items that capture real-life user-LLM interactions, enhancing the relevance of assessment results to real-world LLM usage. Second, each item is rated by human subjects based on its similarity to their own thoughts, and correlations between these ratings and the subjects' actual value scores are derived. This psychometrically validated approach ensures that items strongly correlated with specific values serve as reliable items for assessing those values. Through evaluating 44 LLMs with our benchmark, we find that these models prioritize Benevolence, Security, and Self-Direction values while placing less emphasis on Tradition, Power, and Achievement values. Also, our analysis reveals biases in how LLMs perceive various demographic groups, deviating from real human data.",
      "arxiv_url": "https://arxiv.org/abs/2505.01015",
      "pdf_url": "https://arxiv.org/pdf/2505.01015",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19631",
      "title": "Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models",
      "authors": [
        "Zihong Zhang",
        "Liqi He",
        "Z. Li",
        "Lefei Zhang",
        "Hai Zhao",
        "Bo Du"
      ],
      "abstract": "Word segmentation stands as a cornerstone of Natural Language Processing (NLP). Based on the concept of\"comprehend first, segment later\", we propose a new framework to explore the limit of unsupervised word segmentation with Large Language Models (LLMs) and evaluate the semantic understanding capabilities of LLMs based on word segmentation. We employ current mainstream LLMs to perform word segmentation across multiple languages to assess LLMs'\"comprehension\". Our findings reveal that LLMs are capable of following simple prompts to segment raw text into words. There is a trend suggesting that models with more parameters tend to perform better on multiple languages. Additionally, we introduce a novel unsupervised method, termed LLACA ($\\textbf{L}$arge $\\textbf{L}$anguage Model-Inspired $\\textbf{A}$ho-$\\textbf{C}$orasick $\\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities of Aho-Corasick automata, LLACA innovatively combines these with the deep insights of well-pretrained LLMs. This approach not only enables the construction of a dynamic $n$-gram model that adjusts based on contextual information but also integrates the nuanced understanding of LLMs, offering significant improvements over traditional methods. Our source code is available at https://github.com/hkr04/LLACA",
      "arxiv_url": "https://arxiv.org/abs/2505.19631",
      "pdf_url": "https://arxiv.org/pdf/2505.19631",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "568e8b182dca58599332ab526af9aa7e1bd7a8ec",
      "title": "MutantPrompt: Prompt Optimization via Mutation Under a Budget on Modest-sized LMs",
      "authors": [
        "Arijit Nag",
        "Animesh Mukherjee",
        "Niloy Ganguly",
        "Soumen Chakrabarti"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/568e8b182dca58599332ab526af9aa7e1bd7a8ec",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12654",
      "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals",
      "authors": [
        "Yuxin Lin",
        "Yinglin Zheng",
        "Ming Zeng",
        "Wangzheng Shi"
      ],
      "abstract": "This paper addresses the gap in predicting turn-taking and backchannel actions in human-machine conversations using multi-modal signals (linguistic, acoustic, and visual). To overcome the limitation of existing datasets, we propose an automatic data collection pipeline that allows us to collect and annotate over 210 hours of human conversation videos. From this, we construct a Multi-Modal Face-to-Face (MM-F2F) human conversation dataset, including over 1.5M words and corresponding turn-taking and backchannel annotations from approximately 20M frames. Additionally, we present an end-to-end framework that predicts the probability of turn-taking and backchannel actions from multi-modal signals. The proposed model emphasizes the interrelation between modalities and supports any combination of text, audio, and video inputs, making it adaptable to a variety of realistic scenarios. Our experiments show that our approach achieves state-of-the-art performance on turn-taking and backchannel prediction tasks, achieving a 10% increase in F1-score on turn-taking and a 33% increase on backchannel prediction. Our dataset and code are publicly available online to ease of subsequent research.",
      "arxiv_url": "https://arxiv.org/abs/2505.12654",
      "pdf_url": "https://arxiv.org/pdf/2505.12654",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07479",
      "title": "Improving Fairness of Large Language Models in Multi-document Summarization",
      "authors": [
        "Haoyuan Li",
        "Rui Zhang",
        "Snigdha Chaturvedi"
      ],
      "abstract": "Fairness in multi-document summarization (MDS) is crucial for providing comprehensive views across documents with diverse social attribute values, which can significantly impact decision-making. For example, a summarization system that tends to overrepresent negative reviews of products can mislead customers into disregarding good products. Previous works measure fairness in MDS at two levels: summary-level and corpus-level. While summary-level fairness focuses on individual summaries, corpus-level fairness focuses on a corpus of summaries. Recent methods primarily focus on summary-level fairness. We propose FairPO, a preference tuning method that focuses on both summary-level and corpus-level fairness in MDS. To improve summary-level fairness, we propose to generate preference pairs by perturbing document sets. To improve corpus-level fairness, we propose fairness-aware preference tuning by dynamically adjusting the weights of preference pairs. Our experiments show that FairPO outperforms strong baselines while maintaining the critical qualities of summaries. The code is available at https://github.com/leehaoyuan/coverage_fairnes.",
      "arxiv_url": "https://arxiv.org/abs/2506.07479",
      "pdf_url": "https://arxiv.org/pdf/2506.07479",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.08725",
      "title": "DocAgent: A Multi-Agent System for Automated Code Documentation Generation",
      "authors": [
        "Dayu Yang",
        "Antoine Simoulin",
        "Xin Qian",
        "Xiaoyi Liu",
        "Yuwei Cao",
        "Zhaopu Teng",
        "Grey Yang"
      ],
      "abstract": "High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories.",
      "arxiv_url": "https://arxiv.org/abs/2504.08725",
      "pdf_url": "https://arxiv.org/pdf/2504.08725",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.13824",
      "title": "MLDebugging: Towards Benchmarking Code Debugging Across Multi-Library Scenarios",
      "authors": [
        "Jinyang Huang",
        "Xiachong Feng",
        "Qiguang Chen",
        "Hanjie Zhao",
        "Zihui Cheng",
        "Jiesong Bai",
        "Jingxuan Zhou",
        "Min Li",
        "Libo Qin"
      ],
      "abstract": "Code debugging is a crucial task in software engineering, which attracts increasing attention. While remarkable success has been made in the era of large language models (LLMs), current research still focuses on the simple no-library or single-library setting, ignoring the complex multi-library scenario in real-world applications. To address this limitation, we make the first attempt to introduce MLDebugging (Multi-Library Debugging), a comprehensive benchmark designed to assess debugging challenges within multi-library Python code. Specifically, MLDebugging encompasses 126 distinct Python libraries, covering a wide range of multi-library code issues, categorized into seven distinct types. Furthermore, we conduct a thorough evaluation of MLDebugging using both mainstream open-source and closed-source LLMs and highlight that current LLMs still struggle to correctly perform code debugging across multi-library scenarios. We hope this work can uncover the potential of LLMs in multi-library debugging scenario and offer insights for future research.",
      "arxiv_url": "https://arxiv.org/abs/2506.13824",
      "pdf_url": "https://arxiv.org/pdf/2506.13824",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18710",
      "title": "GainRAG: Preference Alignment in Retrieval-Augmented Generation through Gain Signal Synthesis",
      "authors": [
        "Yi Jiang",
        "Sendong Zhao",
        "Jianbo Li",
        "Hao Wang",
        "Bing Qin"
      ],
      "abstract": "The Retrieval-Augmented Generation (RAG) framework introduces a retrieval module to dynamically inject retrieved information into the input context of large language models (LLMs), and has demonstrated significant success in various NLP tasks. However, the current study points out that there is a preference gap between retrievers and LLMs in the RAG framework, which limit the further improvement of system performance. Some highly relevant passages may interfere with LLM reasoning because they contain complex or contradictory information; while some indirectly related or even inaccurate content may help LLM generate more accurate answers by providing suggestive information or logical clues. To solve this, we propose GainRAG, a novel approach that aligns the retriever's and LLM's preferences by defining a new metric,\"gain\", which measure how well an input passage contributes to correct outputs. Specifically, we propose a method to estimate these gain signals and train a middleware that aligns the preferences of the retriever and the LLM using only limited data. In addition, we introduce a pseudo-passage strategy to mitigate degradation. The experimental results on 6 datasets verify the effectiveness of GainRAG.",
      "arxiv_url": "https://arxiv.org/abs/2505.18710",
      "pdf_url": "https://arxiv.org/pdf/2505.18710",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-05-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.05213",
      "title": "Personalized Text Generation with Contrastive Activation Steering",
      "authors": [
        "Jinghao Zhang",
        "Yuting Liu",
        "Wenjie Wang",
        "Q. Liu",
        "Shu Wu",
        "Liang Wang",
        "Tat-Seng Chua"
      ],
      "abstract": "Personalized text generation aims to infer users' writing style preferences from their historical texts and generate outputs that faithfully reflect these stylistic characteristics. Existing solutions primarily adopt two paradigms: retrieval-augmented generation (RAG) and parameter-efficient fine-tuning (PEFT). While these approaches have advanced the field, they suffer from two critical limitations: (1) the entanglement of content semantics and stylistic patterns in historical texts impedes accurate modeling of user-specific writing preferences; and (2) scalability challenges arising from both RAG's inference latency by retrieval operations and PEFT's parameter storage requirements for per user model. To overcome these limitations, we propose StyleVector, a training-free framework that disentangles and represents personalized writing style as a vector in LLM's activation space, enabling style-steered generation during inference without requiring costly retrieval or parameter storage. Comprehensive experiments demonstrate that our framework achieves a significant 8% relative improvement in personalized generation while reducing storage requirements by 1700 times over PEFT method.",
      "arxiv_url": "https://arxiv.org/abs/2503.05213",
      "pdf_url": "https://arxiv.org/pdf/2503.05213",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization",
        "RAG"
      ],
      "published_date": "2025-03-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5760a16be85faa9b4403670865a34418e9fe8f8d",
      "title": "Variable Layerwise Quantization: A Simple and Effective Approach to Quantize LLMs",
      "authors": [
        "Razvan-Gabriel Dumitru",
        "Vikas Yadav",
        "Rishabh Maheshwary",
        "Paul-Ioan Clotan",
        "Sathwik Tejaswi Madhusudhan",
        "Mihai Surdeanu"
      ],
      "abstract": "We present a simple variable quantization approach that quantizes different layers of a large language model (LLM) at different bit levels. Specifically, we quantize the most important layers to higher bit precision and less important layers to lower bits to achieve floating point quantization levels. We propose two effective strategies to measure the importance of layers within LLMs: the first measures the importance of a layer based on how different its output embeddings are from the input embeddings (the higher the better); the second estimates the importance of a layer using the number of layer weights that are much larger than average (the smaller the better). We show that quantizing different layers at varying bits according to our importance scores results in minimal performance drop with a far more compressed model size. Finally, we present several practical key takeaways from our variable layer-wise quantization experiments: (a) LLM performance under variable quantization remains close to the original model until 25-50% of layers are moved in lower quantization using our proposed ordering but only until 5-10% if moved using no specific ordering; (b) Quantizing LLMs to lower bits performs substantially better than pruning unless extreme quantization (2-bit) is used; and (c) Layer-wise quantization to lower bits works better in the case of larger LLMs with more layers compared to smaller LLMs with fewer layers. The code used to run the experiments is available at: https://github.com/RazvanDu/LayerwiseQuant.",
      "arxiv_url": "https://www.semanticscholar.org/paper/5760a16be85faa9b4403670865a34418e9fe8f8d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.01630",
      "title": "TReMu: Towards Neuro-Symbolic Temporal Reasoning for LLM-Agents with Memory in Multi-Session Dialogues",
      "authors": [
        "Yubin Ge",
        "Salvatore Romeo",
        "Jason Cai",
        "Raphael Shu",
        "Monica Sunkara",
        "Yassine Benajiba",
        "Yi Zhang"
      ],
      "abstract": "Temporal reasoning in multi-session dialogues presents a significant challenge which has been under-studied in previous temporal reasoning benchmarks. To bridge this gap, we propose a new evaluation task for temporal reasoning in multi-session dialogues and introduce an approach to construct a new benchmark by augmenting dialogues from LoCoMo and creating multi-choice QAs. Furthermore, we present TReMu, a new framework aimed at enhancing the temporal reasoning capabilities of LLM-agents in this context. Specifically, the framework employs time-aware memorization through timeline summarization, generating retrievable memory by summarizing events in each dialogue session with their inferred dates. Additionally, we integrate neuro-symbolic temporal reasoning, where LLMs generate Python code to perform temporal calculations and select answers. Experimental evaluations on popular LLMs demonstrate that our benchmark is challenging, and the proposed framework significantly improves temporal reasoning performance compared to baseline methods, raising from 29.83 on GPT-4o via standard prompting to 77.67 via our approach and highlighting its effectiveness in addressing temporal reasoning in multi-session dialogues.",
      "arxiv_url": "https://arxiv.org/abs/2502.01630",
      "pdf_url": "https://arxiv.org/pdf/2502.01630",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11753",
      "title": "HintsOfTruth: A Multimodal Checkworthiness Detection Dataset with Real and Synthetic Claims",
      "authors": [
        "Michiel van der Meer",
        "Pavel Korshunov",
        "Sébastien Marcel",
        "Lonneke van der Plas"
      ],
      "abstract": "Misinformation can be countered with fact-checking, but the process is costly and slow. Identifying checkworthy claims is the first step, where automation can help scale fact-checkers' efforts. However, detection methods struggle with content that is (1) multimodal, (2) from diverse domains, and (3) synthetic. We introduce HintsOfTruth, a public dataset for multimodal checkworthiness detection with 27K real-world and synthetic image/claim pairs. The mix of real and synthetic data makes this dataset unique and ideal for benchmarking detection methods. We compare fine-tuned and prompted Large Language Models (LLMs). We find that well-configured lightweight text-based encoders perform comparably to multimodal models but the former only focus on identifying non-claim-like content. Multimodal LLMs can be more accurate but come at a significant computational cost, making them impractical for large-scale applications. When faced with synthetic data, multimodal models perform more robustly.",
      "arxiv_url": "https://arxiv.org/abs/2502.11753",
      "pdf_url": "https://arxiv.org/pdf/2502.11753",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5787cbfa2dfc2260d2a23db9aa17511d58fe155d",
      "title": "DualGuard: A Parameter Space Transformation Approach for Bidirectional Defense in Split-Based LLM Fine-Tuning",
      "authors": [
        "Zihan Liu",
        "Yizheng Wang",
        "Rui Wang",
        "Sai Wu"
      ],
      "abstract": "Integrating split learning with large language model fine-tuning (LLM-FT) enables secure collaboration between a trusted local client and a well-equipped remote server, but it is vulnerable to data reconstruction attacks (DRAs) that exploit transmitted activations and gradients. Current defense methods, like adding noise to activations or gradients, often sacrifice task-specific model performance under strict privacy constraints. This paper introduces Du-alGuard, a bidirectional defense mechanism against DRAs for split-based LLM-FT. Du-alGuard proposes a local warm-up parameter space transformation to alter client-side model parameters before training, using multi-task learning to strike a balance between privacy protection and model performance. Additionally, a global fine-tuning parameter space retention strategy prevents the model from reverting to vulnerable states during formal fine-tuning. Experiments show that DualGuard outperforms current defense methods against various DRAs, while maintaining task performance. Our code will be made publicly available.",
      "arxiv_url": "https://www.semanticscholar.org/paper/5787cbfa2dfc2260d2a23db9aa17511d58fe155d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11358",
      "title": "Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System",
      "authors": [
        "Ziyou Jiang",
        "Mingyang Li",
        "Guowei Yang",
        "Junjie Wang",
        "Yuekai Huang",
        "Zhiyuan Chang",
        "Qing Wang"
      ],
      "abstract": "Information theft attacks pose a significant risk to Large Language Model (LLM) tool-learning systems. Adversaries can inject malicious commands through compromised tools, manipulating LLMs to send sensitive information to these tools, which leads to potential privacy breaches. However, existing attack approaches are black-box oriented and rely on static commands that cannot adapt flexibly to the changes in user queries and the invocation chain of tools. It makes malicious commands more likely to be detected by LLM and leads to attack failure. In this paper, we propose AutoCMD, a dynamic attack comment generation approach for information theft attacks in LLM tool-learning systems. Inspired by the concept of mimicking the familiar, AutoCMD is capable of inferring the information utilized by upstream tools in the toolchain through learning on open-source systems and reinforcement with target system examples, thereby generating more targeted commands for information theft. The evaluation results show that AutoCMD outperforms the baselines with +13.2% $ASR_{Theft}$, and can be generalized to new tool-learning systems to expose their information leakage risks. We also design four defense methods to effectively protect tool-learning systems from the attack.",
      "arxiv_url": "https://arxiv.org/abs/2502.11358",
      "pdf_url": "https://arxiv.org/pdf/2502.11358",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17651",
      "title": "METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling",
      "authors": [
        "Bingxuan Li",
        "Yiwei Wang",
        "Jiuxiang Gu",
        "Kai-Wei Chang",
        "Nanyun Peng"
      ],
      "abstract": "Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL, a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves 5.2% improvement over the current best result in the chart generation task. The METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithmic computational budget grows from 512 to 8192 tokens. In addition, we find that separating different modalities during the critique process of METAL boosts the self-correction capability of VLMs in the multimodal context.",
      "arxiv_url": "https://arxiv.org/abs/2502.17651",
      "pdf_url": "https://arxiv.org/pdf/2502.17651",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5817606be8485c7989483be7ffce88f85500d80a",
      "title": "Enhancing Retrieval-Augmented Generation via Evidence Tree Search",
      "authors": [
        "Hao Sun",
        "Hengyi Cai",
        "Yuchen Li",
        "Xuanbo Fan",
        "Xiaochi Wei",
        "Shuaiqiang Wang",
        "Yan Zhang",
        "Dawei Yin"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) is widely used to enhance Large Language Models (LLMs) by grounding responses in external knowledge. However, in real-world applications, retrievers often return lengthy documents with redundant or irrelevant content, confusing downstream readers. While evidence retrieval aims to address this by extracting key information, it faces critical challenges: (1) inability to model synergistic inter-dependencies among evidence sentences, (2) lack of supervision for evaluating multi-sentence evidence quality, and (3) computational inefficiency in navigating exponentially growing search spaces of candidate evidence sets. To tackle these challenges, we propose ETS (Evidence Tree Search), a novel framework that reformulates evidence retrieval as a dynamic tree expansion process. Our approach first constructs an evidence tree where each path represents a candidate evidence set, explicitly modeling inter-sentence dependencies through context-aware node selection. We then leverage Monte Carlo Tree Search (MCTS) to efficiently assess evidence quality and introduce an Early-Terminating Beam Search strategy to efficiently accelerate the model inference. Extensive experiments on five datasets demonstrate that ETS significantly outperforms existing methods across different readers. Our code and datasets will be released to facilitate future research.",
      "arxiv_url": "https://www.semanticscholar.org/paper/5817606be8485c7989483be7ffce88f85500d80a",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16143",
      "title": "The Law of Knowledge Overshadowing: Towards Understanding, Predicting, and Preventing LLM Hallucination",
      "authors": [
        "Yuji Zhang",
        "Sha Li",
        "Cheng Qian",
        "Jiateng Liu",
        "Pengfei Yu",
        "Chi Han",
        "Y. Fung",
        "Kathleen McKeown",
        "ChengXiang Zhai",
        "Manling Li",
        "Heng Ji"
      ],
      "abstract": "Hallucination is a persistent challenge in large language models (LLMs), where even with rigorous quality control, models often generate distorted facts. This paradox, in which error generation continues despite high-quality training data, calls for a deeper understanding of the underlying LLM mechanisms. To address it, we propose a novel concept: knowledge overshadowing, where model's dominant knowledge can obscure less prominent knowledge during text generation, causing the model to fabricate inaccurate details. Building on this idea, we introduce a novel framework to quantify factual hallucinations by modeling knowledge overshadowing. Central to our approach is the log-linear law, which predicts that the rate of factual hallucination increases linearly with the logarithmic scale of (1) Knowledge Popularity, (2) Knowledge Length, and (3) Model Size. The law provides a means to preemptively quantify hallucinations, offering foresight into their occurrence even before model training or inference. Built on overshadowing effect, we propose a new decoding strategy CoDa, to mitigate hallucinations, which notably enhance model factuality on Overshadow (27.9%), MemoTrap (13.1%) and NQ-Swap (18.3%). Our findings not only deepen understandings of the underlying mechanisms behind hallucinations but also provide actionable insights for developing more predictable and controllable language models.",
      "arxiv_url": "https://arxiv.org/abs/2502.16143",
      "pdf_url": "https://arxiv.org/pdf/2502.16143",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5825d5bce93b907d5d735cbcb79e3faab44ce50a",
      "title": "UniT: One Document, Many Revisions, Too Many Edit Intention Taxonomies",
      "authors": [
        "Fangping Lan",
        "Abdullah Aljebreen",
        "Eduard Constantin Dragut"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/5825d5bce93b907d5d735cbcb79e3faab44ce50a",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13441",
      "title": "The Self-Improvement Paradox: Can Language Models Bootstrap Reasoning Capabilities without External Scaffolding?",
      "authors": [
        "Yutao Sun",
        "Mingshuai Chen",
        "Tiancheng Zhao",
        "Ruochen Xu",
        "Zilun Zhang",
        "Jianwei Yin"
      ],
      "abstract": "Self-improving large language models (LLMs) -- i.e., to improve the performance of an LLM by fine-tuning it with synthetic data generated by itself -- is a promising way to advance the capabilities of LLMs while avoiding extensive supervision. Existing approaches to self-improvement often rely on external supervision signals in the form of seed data and/or assistance from third-party models. This paper presents Crescent -- a simple yet effective framework for generating high-quality synthetic question-answer data in a fully autonomous manner. Crescent first elicits the LLM to generate raw questions via a bait prompt, then diversifies these questions leveraging a rejection sampling-based self-deduplication, and finally feeds the questions to the LLM and collects the corresponding answers by means of majority voting. We show that Crescent sheds light on the potential of true self-improvement with zero external supervision signals for math reasoning; in particular, Crescent-generated question-answer pairs suffice to (i) improve the reasoning capabilities of an LLM while preserving its general performance (especially in the 0-shot setting); and (ii) distil LLM knowledge to weaker models more effectively than existing methods based on seed-dataset augmentation.",
      "arxiv_url": "https://arxiv.org/abs/2502.13441",
      "pdf_url": "https://arxiv.org/pdf/2502.13441",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.00979",
      "title": "Enhancing LLM Agent Safety via Causal Influence Prompting",
      "authors": [
        "Dongyoon Hahm",
        "Woogyeol Jin",
        "June Suk Choi",
        "Sungsoo Ahn",
        "Kimin Lee"
      ],
      "abstract": "As autonomous agents powered by large language models (LLMs) continue to demonstrate potential across various assistive tasks, ensuring their safe and reliable behavior is crucial for preventing unintended consequences. In this work, we introduce CIP, a novel technique that leverages causal influence diagrams (CIDs) to identify and mitigate risks arising from agent decision-making. CIDs provide a structured representation of cause-and-effect relationships, enabling agents to anticipate harmful outcomes and make safer decisions. Our approach consists of three key steps: (1) initializing a CID based on task specifications to outline the decision-making process, (2) guiding agent interactions with the environment using the CID, and (3) iteratively refining the CID based on observed behaviors and outcomes. Experimental results demonstrate that our method effectively enhances safety in both code execution and mobile device control tasks.",
      "arxiv_url": "https://arxiv.org/abs/2507.00979",
      "pdf_url": "https://arxiv.org/pdf/2507.00979",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18411",
      "title": "OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference",
      "authors": [
        "Xiangyu Zhao",
        "Shengyuan Ding",
        "Zicheng Zhang",
        "Haian Huang",
        "Maosong Cao",
        "Weiyun Wang",
        "Jiaqi Wang",
        "Xinyu Fang",
        "Wenhai Wang",
        "Guangtao Zhai",
        "Haodong Duan",
        "Hua Yang",
        "Kai Chen"
      ],
      "abstract": "Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs' alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at https://github.com/PhoenixZ810/OmniAlign-V.",
      "arxiv_url": "https://arxiv.org/abs/2502.18411",
      "pdf_url": "https://arxiv.org/pdf/2502.18411",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.22853",
      "title": "DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues",
      "authors": [
        "Kyochul Jang",
        "Donghyeon Lee",
        "Kyusik Kim",
        "Dongseok Heo",
        "Taewhoo Lee",
        "Woojeong Kim",
        "Bongwon Suh"
      ],
      "abstract": "Existing function-calling benchmarks focus on single-turn interactions. However, they overlook the complexity of real-world scenarios. To quantify how existing benchmarks address practical applications, we introduce DICE-SCORE, a metric that evaluates the dispersion of tool-related information such as function name and parameter values throughout the dialogue. Analyzing existing benchmarks through DICE-SCORE reveals notably low scores, highlighting the need for more realistic scenarios. To address this gap, we present DICE-BENCH, a framework that constructs practical function-calling datasets by synthesizing conversations through a tool graph that maintains dependencies across rounds and a multi-agent system with distinct personas to enhance dialogue naturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our experiments on 19 LLMs with DICE-BENCH show that significant advances are still required before such models can be deployed effectively in real-world settings. Our code and data are all publicly available: https://snuhcc.github.io/DICE-Bench/.",
      "arxiv_url": "https://arxiv.org/abs/2506.22853",
      "pdf_url": "https://arxiv.org/pdf/2506.22853",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-06-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04997",
      "title": "Towards Storage-Efficient Visual Document Retrieval: An Empirical Study on Reducing Patch-Level Embeddings",
      "authors": [
        "Yubo Ma",
        "Jinsong Li",
        "Yuhang Zang",
        "Xiaobao Wu",
        "Xiao-wen Dong",
        "Pan Zhang",
        "Yuhang Cao",
        "Haodong Duan",
        "Jiaqi Wang",
        "Yixin Cao",
        "Aixin Sun"
      ],
      "abstract": "Despite the strong performance of ColPali/ColQwen2 in Visualized Document Retrieval (VDR), it encodes each page into multiple patch-level embeddings and leads to excessive memory usage. This empirical study investigates methods to reduce patch embeddings per page at minimum performance degradation. We evaluate two token-reduction strategies: token pruning and token merging. Regarding token pruning, we surprisingly observe that a simple random strategy outperforms other sophisticated pruning methods, though still far from satisfactory. Further analysis reveals that pruning is inherently unsuitable for VDR as it requires removing certain page embeddings without query-specific information. Turning to token merging (more suitable for VDR), we search for the optimal combinations of merging strategy across three dimensions and develop Light-ColPali/ColQwen2. It maintains 98.2% of retrieval performance with only 11.8% of original memory usage, and preserves 94.6% effectiveness at 2.8% memory footprint. We expect our empirical findings and resulting Light-ColPali/ColQwen2 offer valuable insights and establish a competitive baseline for future research towards efficient VDR.",
      "arxiv_url": "https://arxiv.org/abs/2506.04997",
      "pdf_url": "https://arxiv.org/pdf/2506.04997",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.04649",
      "title": "FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights",
      "authors": [
        "Chengzhang Yu",
        "Yiming Zhang",
        "Zhixin Liu",
        "Zenghui Ding",
        "Yining Sun",
        "Zhanpeng Jin"
      ],
      "abstract": "The automation of scientific research through large language models (LLMs) presents significant opportunities but faces critical challenges in knowledge synthesis and quality assurance. We introduce Feedback-Refined Agent Methodology (FRAME), a novel framework that enhances medical paper generation through iterative refinement and structured feedback. Our approach comprises three key innovations: (1) A structured dataset construction method that decomposes 4,287 medical papers into essential research components through iterative refinement; (2) A tripartite architecture integrating Generator, Evaluator, and Reflector agents that progressively improve content quality through metric-driven feedback; and (3) A comprehensive evaluation framework that combines statistical metrics with human-grounded benchmarks. Experimental results demonstrate FRAME's effectiveness, achieving significant improvements over conventional approaches across multiple models (9.91% average gain with DeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation dimensions. Human evaluation confirms that FRAME-generated papers achieve quality comparable to human-authored works, with particular strength in synthesizing future research directions. The results demonstrated our work could efficiently assist medical research by building a robust foundation for automated medical research paper generation while maintaining rigorous academic standards.",
      "arxiv_url": "https://arxiv.org/abs/2505.04649",
      "pdf_url": "https://arxiv.org/pdf/2505.04649",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "58f6ac8c023e18e7c8886770ec583e0fdd616e86",
      "title": "Qwen2.5-xCoder: Multi-Agent Collaboration for Multilingual Code Instruction Tuning",
      "authors": [
        "Jian Yang",
        "Wei Zhang",
        "Yibo Miao",
        "Shanghaoran Quan",
        "Zhenhe Wu",
        "Qiyao Peng",
        "Liqun Yang",
        "Tianyu Liu",
        "Zeyu Cui",
        "Binyuan Hui",
        "Junyang Lin"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/58f6ac8c023e18e7c8886770ec583e0fdd616e86",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "592f4abcbefd6295278ca12cc679691a51b4e110",
      "title": "Taxonomy-Driven Knowledge Graph Construction for Domain-Specific Scientific Applications",
      "authors": [
        "Huitong Pan",
        "Qi Zhang",
        "Mustapha Adamu",
        "E. Dragut",
        "Longin Jan Latecki"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/592f4abcbefd6295278ca12cc679691a51b4e110",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04385",
      "title": "MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP",
      "authors": [
        "Kurt Micallef",
        "Claudia Borg"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various Natural Language Processing (NLP) tasks, largely due to their generalisability and ability to perform tasks without additional training. However, their effectiveness for low-resource languages remains limited. In this study, we evaluate the performance of 55 publicly available LLMs on Maltese, a low-resource language, using a newly introduced benchmark covering 11 discriminative and generative tasks. Our experiments highlight that many models perform poorly, particularly on generative tasks, and that smaller fine-tuned models often perform better across all tasks. From our multidimensional analysis, we investigate various factors impacting performance. We conclude that prior exposure to Maltese during pre-training and instruction-tuning emerges as the most important factor. We also examine the trade-offs between fine-tuning and prompting, highlighting that while fine-tuning requires a higher initial cost, it yields better performance and lower inference costs. Through this work, we aim to highlight the need for more inclusive language technologies and recommend that researchers working with low-resource languages consider more\"traditional\"language modelling approaches.",
      "arxiv_url": "https://arxiv.org/abs/2506.04385",
      "pdf_url": "https://arxiv.org/pdf/2506.04385",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12301",
      "title": "Unveiling Confirmation Bias in Chain-of-Thought Reasoning",
      "authors": [
        "Yue Wan",
        "Xiaowei Jia",
        "Xiang Li"
      ],
      "abstract": "Chain-of-thought (CoT) prompting has been widely adopted to enhance the reasoning capabilities of large language models (LLMs). However, the effectiveness of CoT reasoning is inconsistent across tasks with different reasoning types. This work presents a novel perspective to understand CoT behavior through the lens of \\textit{confirmation bias} in cognitive psychology. Specifically, we examine how model internal beliefs, approximated by direct question-answering probabilities, affect both reasoning generation ($Q \\to R$) and reasoning-guided answer prediction ($QR \\to A$) in CoT. By decomposing CoT into a two-stage process, we conduct a thorough correlation analysis in model beliefs, rationale attributes, and stage-wise performance. Our results provide strong evidence of confirmation bias in LLMs, such that model beliefs not only skew the reasoning process but also influence how rationales are utilized for answer prediction. Furthermore, the interplay between task vulnerability to confirmation bias and the strength of beliefs also provides explanations for CoT effectiveness across reasoning tasks and models. Overall, this study provides a valuable insight for the needs of better prompting strategies that mitigate confirmation bias to enhance reasoning performance. Code is available at \\textit{https://github.com/yuewan2/biasedcot}.",
      "arxiv_url": "https://arxiv.org/abs/2506.12301",
      "pdf_url": "https://arxiv.org/pdf/2506.12301",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22222",
      "title": "Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation",
      "authors": [
        "Yunsoo Kim",
        "Jinge Wu",
        "Su-Hwan Kim",
        "Pardeep Vasudev",
        "Jiashu Shen",
        "Honghan Wu"
      ],
      "abstract": "Recent advancements in multimodal Large Language Models (LLMs) have significantly enhanced the automation of medical image analysis, particularly in generating radiology reports from chest X-rays (CXR). However, these models still suffer from hallucinations and clinically significant errors, limiting their reliability in real-world applications. In this study, we propose Look&Mark (L&M), a novel grounding fixation strategy that integrates radiologist eye fixations (Look) and bounding box annotations (Mark) into the LLM prompting framework. Unlike conventional fine-tuning, L&M leverages in-context learning to achieve substantial performance gains without retraining. When evaluated across multiple domain-specific and general-purpose models, L&M demonstrates significant gains, including a 1.2% improvement in overall metrics (A.AVG) for CXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for LLaVA-Med. General-purpose models also benefit from L&M combined with in-context learning, with LLaVA-OV achieving an 87.3% clinical average performance (C.AVG)-the highest among all models, even surpassing those explicitly trained for CXR report generation. Expert evaluations further confirm that L&M reduces clinically significant errors (by 0.43 average errors per report), such as false predictions and omissions, enhancing both accuracy and reliability. These findings highlight L&M's potential as a scalable and efficient solution for AI-assisted radiology, paving the way for improved diagnostic workflows in low-resource clinical settings.",
      "arxiv_url": "https://arxiv.org/abs/2505.22222",
      "pdf_url": "https://arxiv.org/pdf/2505.22222",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14258",
      "title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information",
      "authors": [
        "Yein Park",
        "Chanwoong Yoon",
        "Jungwoo Park",
        "Minbyul Jeong",
        "Jaewoo Kang"
      ],
      "abstract": "While the ability of language models to elicit facts has been widely investigated, how they handle temporally changing facts remains underexplored. We discover Temporal Heads, specific attention heads that primarily handle temporal knowledge, through circuit analysis. We confirm that these heads are present across multiple models, though their specific locations may vary, and their responses differ depending on the type of knowledge and its corresponding years. Disabling these heads degrades the model's ability to recall time-specific knowledge while maintaining its general capabilities without compromising time-invariant and question-answering performances. Moreover, the heads are activated not only numeric conditions (\"In 2004\") but also textual aliases (\"In the year ...\"), indicating that they encode a temporal dimension beyond simple numerical representation. Furthermore, we expand the potential of our findings by demonstrating how temporal knowledge can be edited by adjusting the values of these heads.",
      "arxiv_url": "https://arxiv.org/abs/2502.14258",
      "pdf_url": "https://arxiv.org/pdf/2502.14258",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12727",
      "title": "What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma",
      "authors": [
        "Han Meng",
        "Yancan Chen",
        "Yunan Li",
        "Yitian Yang",
        "Jungup Lee",
        "Renwen Zhang",
        "Yi-Chieh Lee"
      ],
      "abstract": "Mental-health stigma remains a pervasive social problem that hampers treatment-seeking and recovery. Existing resources for training neural models to finely classify such stigma are limited, relying primarily on social-media or synthetic data without theoretical underpinnings. To remedy this gap, we present an expert-annotated, theory-informed corpus of human-chatbot interviews, comprising 4,141 snippets from 684 participants with documented socio-cultural backgrounds. Our experiments benchmark state-of-the-art neural models and empirically unpack the challenges of stigma detection. This dataset can facilitate research on computationally detecting, neutralizing, and counteracting mental-health stigma. Our corpus is openly available at https://github.com/HanMeng2004/Mental-Health-Stigma-Interview-Corpus.",
      "arxiv_url": "https://arxiv.org/abs/2505.12727",
      "pdf_url": "https://arxiv.org/pdf/2505.12727",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.00969",
      "title": "Wizard of Shopping: Target-Oriented E-commerce Dialogue Generation with Decision Tree Branching",
      "authors": [
        "Xiangci Li",
        "Zhiyu Chen",
        "J. Choi",
        "Nikhita Vedula",
        "B. Fetahu",
        "Oleg Rokhlenko",
        "S. Malmasi"
      ],
      "abstract": "The goal of conversational product search (CPS) is to develop an intelligent, chat-based shopping assistant that can directly interact with customers to understand shopping intents, ask clarification questions, and find relevant products. However, training such assistants is hindered mainly due to the lack of reliable and large-scale datasets. Prior human-annotated CPS datasets are extremely small in size and lack integration with real-world product search systems. We propose a novel approach, TRACER, which leverages large language models (LLMs) to generate realistic and natural conversations for different shopping domains. TRACER's novelty lies in grounding the generation to dialogue plans, which are product search trajectories predicted from a decision tree model, that guarantees relevant product discovery in the shortest number of search conditions. We also release the first target-oriented CPS dataset Wizard of Shopping (WoS), containing highly natural and coherent conversations (3.6k) from three shopping domains. Finally, we demonstrate the quality and effectiveness of WoS via human evaluations and downstream tasks.",
      "arxiv_url": "https://arxiv.org/abs/2502.00969",
      "pdf_url": "https://arxiv.org/pdf/2502.00969",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.17821",
      "title": "VideoVista-CulturalLingo: 360° Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension",
      "authors": [
        "Xinyu Chen",
        "Yunxin Li",
        "Haoyuan Shi",
        "Baotian Hu",
        "Wenhan Luo",
        "Yaowei Wang",
        "Min Zhang"
      ],
      "abstract": "Assessing the video comprehension capabilities of multimodal AI systems can effectively measure their understanding and reasoning abilities. Most video evaluation benchmarks are limited to a single language, typically English, and predominantly feature videos rooted in Western cultural contexts. In this paper, we present VideoVista-CulturalLingo, the first video evaluation benchmark designed to bridge cultural, linguistic, and domain divide in video comprehension. Our work differs from existing benchmarks in the following ways: 1) Cultural diversity, incorporating cultures from China, North America, and Europe; 2) Multi-linguistics, with questions presented in Chinese and English-two of the most widely spoken languages; and 3) Broad domain, featuring videos sourced from hundreds of human-created domains. VideoVista-CulturalLingo contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent open-source or proprietary video large models. From the experiment results, we observe that: 1) Existing models perform worse on Chinese-centric questions than Western-centric ones, particularly those related to Chinese history; 2) Current open-source models still exhibit limitations in temporal understanding, especially in the Event Localization task, achieving a maximum score of only 45.2%; 3) Mainstream models demonstrate strong performance in general scientific questions, while open-source models demonstrate weak performance in mathematics.",
      "arxiv_url": "https://arxiv.org/abs/2504.17821",
      "pdf_url": "https://arxiv.org/pdf/2504.17821",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11284",
      "title": "Balancing the Budget: Understanding Trade-offs Between Supervised and Preference-Based Finetuning",
      "authors": [
        "Mohit Raghavendra",
        "Junmo Kang",
        "Alan Ritter"
      ],
      "abstract": "Post-training of Large Language Models often involves a pipeline of Supervised Finetuning (SFT) followed by Preference Finetuning (PFT) using methods like Direct Preference Optimization. Both stages require annotated data that are very different in structure and costs. We study how to optimally allocate a fixed training data budget between the two stages, through extensive experiments spanning four diverse tasks, multiple model sizes and various data annotation costs. Our findings reveal that just SFT on the base model dominates performance in low-data regimes ($<1,000$ annotated examples). With larger data-budgets, we observe that a combination of SFT and PFT, often with increasing portions allocated towards preference data yields optimal performance. However, completely eliminating SFT and running PFT directly on the base model yields suboptimal performance, described as the cold start problem on tasks like mathematics. We observe that this is due to the distribution shift arising from using DPO directly on the base model to elicit step-by-step reasoning. This limitation can be effectively addressed by allocating even a small portion ($<10$%) of the budget to SFT first, resulting in performance improvements of $15-20$% on analytical benchmarks like GSM8k. These results provide actionable insights for researchers and practitioners optimizing model development under budget constraints, where high-quality data curation often represents a significant portion of the total costs of model development.",
      "arxiv_url": "https://arxiv.org/abs/2502.11284",
      "pdf_url": "https://arxiv.org/pdf/2502.11284",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.09824",
      "title": "Abacus-SQL: A Text-to-SQL System Empowering Cross-Domain and Open-Domain Database Retrieval",
      "authors": [
        "Keyan Xu",
        "Dingzirui Wang",
        "Xuanliang Zhang",
        "Qingfu Zhu",
        "Wanxiang Che"
      ],
      "abstract": "The existing text-to-SQL systems have made significant progress in SQL query generation, but they still face numerous challenges. Existing systems often lack retrieval capabilities for open-domain databases, requiring users to manually filter relevant databases. Additionally, their cross-domain transferability is limited, making it challenging to accommodate diverse query requirements. To address these issues, we propose Abacus-SQL. Abacus-SQL utilizes database retrieval technology to accurately locate the required databases in an open-domain database environment. It also enhances the system cross-domain transfer ability through data augmentation methods. Moreover, Abacus-SQL employs Pre-SQL and Self-debug methods, thereby enhancing the accuracy of SQL queries. Experimental results demonstrate that Abacus-SQL performs excellently in multi-turn text-to-SQL tasks, effectively validating the approach's effectiveness. Abacus-SQL is publicly accessible at https://huozi.8wss.com/abacus-sql/.",
      "arxiv_url": "https://arxiv.org/abs/2504.09824",
      "pdf_url": "https://arxiv.org/pdf/2504.09824",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24672",
      "title": "TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis",
      "authors": [
        "Xiaorui Wu",
        "Xiaofeng Mao",
        "Fei Li",
        "Xin Zhang",
        "Xuanhong Li",
        "Chong Teng",
        "Donghong Ji",
        "Zhuang Li"
      ],
      "abstract": "Large Language Models (LLMs) excel in various natural language processing tasks but remain vulnerable to generating harmful content or being exploited for malicious purposes. Although safety alignment datasets have been introduced to mitigate such risks through supervised fine-tuning (SFT), these datasets often lack comprehensive risk coverage. Most existing datasets focus primarily on lexical diversity while neglecting other critical dimensions. To address this limitation, we propose a novel analysis framework to systematically measure the risk coverage of alignment datasets across three essential dimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We further introduce TRIDENT, an automated pipeline that leverages persona-based, zero-shot LLM generation to produce diverse and comprehensive instructions spanning these dimensions. Each harmful instruction is paired with an ethically aligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311 examples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on TRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29% reduction in Harm Score, and a 20% decrease in Attack Success Rate compared to the best-performing baseline model fine-tuned on the WildBreak dataset.",
      "arxiv_url": "https://arxiv.org/abs/2505.24672",
      "pdf_url": "https://arxiv.org/pdf/2505.24672",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.17149",
      "title": "Large Language Models for Predictive Analysis: How Far Are They?",
      "authors": [
        "Qin Chen",
        "Yuanyi Ren",
        "Xiaojun Ma",
        "Yuyang Shi"
      ],
      "abstract": "Predictive analysis is a cornerstone of modern decision-making, with applications in various domains. Large Language Models (LLMs) have emerged as powerful tools in enabling nuanced, knowledge-intensive conversations, thus aiding in complex decision-making tasks. With the burgeoning expectation to harness LLMs for predictive analysis, there is an urgent need to systematically assess their capability in this domain. However, there is a lack of relevant evaluations in existing studies. To bridge this gap, we introduce the \\textbf{PredictiQ} benchmark, which integrates 1130 sophisticated predictive analysis queries originating from 44 real-world datasets of 8 diverse fields. We design an evaluation protocol considering text analysis, code generation, and their alignment. Twelve renowned LLMs are evaluated, offering insights into their practical use in predictive analysis. Generally, we believe that existing LLMs still face considerable challenges in conducting predictive analysis. See \\href{https://github.com/Cqkkkkkk/PredictiQ}{Github}.",
      "arxiv_url": "https://arxiv.org/abs/2505.17149",
      "pdf_url": "https://arxiv.org/pdf/2505.17149",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.13953",
      "title": "Redundancy Principles for MLLMs Benchmarks",
      "authors": [
        "Zicheng Zhang",
        "Xiangyu Zhao",
        "Xinyu Fang",
        "Chunyi Li",
        "Xiaohong Liu",
        "Xiongkuo Min",
        "Haodong Duan",
        "Kai Chen",
        "Guangtao Zhai"
      ],
      "abstract": "With the rapid iteration of Multi-modality Large Language Models (MLLMs) and the evolving demands of the field, the number of benchmarks produced annually has surged into the hundreds. The rapid growth has inevitably led to significant redundancy among benchmarks. Therefore, it is crucial to take a step back and critically assess the current state of redundancy and propose targeted principles for constructing effective MLLM benchmarks. In this paper, we focus on redundancy from three key perspectives: 1) Redundancy of benchmark capability dimensions, 2) Redundancy in the number of test questions, and 3) Cross-benchmark redundancy within specific domains. Through the comprehensive analysis over hundreds of MLLMs' performance across more than 20 benchmarks, we aim to quantitatively measure the level of redundancy lies in existing MLLM evaluations, provide valuable insights to guide the future development of MLLM benchmarks, and offer strategies to refine and address redundancy issues effectively. The code is available at https://github.com/zzc-1998/Benchmark-Redundancy.",
      "arxiv_url": "https://arxiv.org/abs/2501.13953",
      "pdf_url": "https://arxiv.org/pdf/2501.13953",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5a76b98a09fd13bc17b7a13686409c63f286bffe",
      "title": "ENGinius: A Bilingual LLM Optimized for Plant Construction Engineering",
      "authors": [
        "Wooseong Lee",
        "Minseo Kim",
        "Taeil Hur",
        "Gyeong Hwan Jang",
        "Woncheol Lee",
        "Maro Na",
        "Taeuk Kim"
      ],
      "abstract": "Recent advances in large language models (LLMs) have drawn attention for their potential to automate and optimize processes across various sectors. However, the adoption of LLMs in the plant construction industry remains limited, mainly due to its highly specialized nature and the lack of resources for domain-specific training and evaluation. In this work, we propose ENGinius, the first LLM designed for plant construction engineering. We present procedures for data construction and model training, along with the first benchmarks tailored to this under-represented domain. We show that ENGinius delivers optimized responses to plant engineers by leveraging enriched domain knowledge. We also demonstrate its practical impact and use cases, such as technical document processing and multilingual communication.",
      "arxiv_url": "https://www.semanticscholar.org/paper/5a76b98a09fd13bc17b7a13686409c63f286bffe",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10780",
      "title": "SECRET: Semi-supervised Clinical Trial Document Similarity Search",
      "authors": [
        "Trisha Das",
        "Afrah Shafquat",
        "Mandis Beigi",
        "J. Aptekar",
        "Jimeng Sun"
      ],
      "abstract": "Clinical trials are vital for evaluation of safety and efficacy of new treatments. However, clinical trials are resource-intensive, time-consuming and expensive to conduct, where errors in trial design, reduced efficacy, and safety events can result in significant delays, financial losses, and damage to reputation. These risks underline the importance of informed and strategic decisions in trial design to mitigate these risks and improve the chances of a successful trial. Identifying similar historical trials is critical as these trials can provide an important reference for potential pitfalls and challenges including serious adverse events, dosage inaccuracies, recruitment difficulties, patient adherence issues, etc. Addressing these challenges in trial design can lead to development of more effective study protocols with optimized patient safety and trial efficiency. In this paper, we present a novel method to identify similar historical trials by summarizing clinical trial protocols and searching for similar trials based on a query trial's protocol. Our approach significantly outperforms all baselines, achieving up to a 78% improvement in recall@1 and a 53% improvement in precision@1 over the best baseline. We also show that our method outperforms all other baselines in partial trial similarity search and zero-shot patient-trial matching, highlighting its superior utility in these tasks.",
      "arxiv_url": "https://arxiv.org/abs/2505.10780",
      "pdf_url": "https://arxiv.org/pdf/2505.10780",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00445",
      "title": "G2S: A General-to-Specific Learning Framework for Temporal Knowledge Graph Forecasting with Large Language Models",
      "authors": [
        "Long Bai",
        "Zixuan Li",
        "Xiaolong Jin",
        "Jiafeng Guo",
        "Xueqi Cheng",
        "Tat-Seng Chua"
      ],
      "abstract": "Forecasting over Temporal Knowledge Graphs (TKGs) which predicts future facts based on historical ones has received much attention. Recent studies have introduced Large Language Models (LLMs) for this task to enhance the models' generalization abilities. However, these models perform forecasting via simultaneously learning two kinds of entangled knowledge in the TKG: (1) general patterns, i.e., invariant temporal structures shared across different scenarios; and (2) scenario information, i.e., factual knowledge engaged in specific scenario, such as entities and relations. As a result, the learning processes of these two kinds of knowledge may interfere with each other, which potentially impact the generalization abilities of the models. To enhance the generalization ability of LLMs on this task, in this paper, we propose a General-to-Specific learning framework (G2S) that disentangles the learning processes of the above two kinds of knowledge. In the general learning stage, we mask the scenario information in different TKGs and convert it into anonymous temporal structures. After training on these structures, the model is able to capture the general patterns across different TKGs. In the specific learning stage, we inject the scenario information into the structures via either in-context learning or fine-tuning modes. Experimental results show that G2S effectively improves the generalization abilities of LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.00445",
      "pdf_url": "https://arxiv.org/pdf/2506.00445",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12800",
      "title": "OZSpeech: One-step Zero-shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching",
      "authors": [
        "Hieu-Nghia Huynh-Nguyen",
        "N. Nguyen",
        "Huynh Nguyen Dang",
        "T. Vo",
        "T. Hy",
        "Van Nguyen"
      ],
      "abstract": "Text-to-speech (TTS) systems have seen significant advancements in recent years, driven by improvements in deep learning and neural network architectures. Viewing the output speech as a data distribution, previous approaches often employ traditional speech representations, such as waveforms or spectrograms, within the Flow Matching framework. However, these methods have limitations, including overlooking various speech attributes and incurring high computational costs due to additional constraints introduced during training. To address these challenges, we introduce OZSpeech, the first TTS method to explore optimal transport conditional flow matching with one-step sampling and a learned prior as the condition, effectively disregarding preceding states and reducing the number of sampling steps. Our approach operates on disentangled, factorized components of speech in token format, enabling accurate modeling of each speech attribute, which enhances the TTS system's ability to precisely clone the prompt speech. Experimental results show that our method achieves promising performance over existing methods in content accuracy, naturalness, prosody generation, and speaker style preservation. Audio samples are available at our demo page https://ozspeech.github.io/OZSpeech_Web/.",
      "arxiv_url": "https://arxiv.org/abs/2505.12800",
      "pdf_url": "https://arxiv.org/pdf/2505.12800",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.15188",
      "title": "Synergistic Weak-Strong Collaboration by Aligning Preferences",
      "authors": [
        "Yizhu Jiao",
        "Xuchao Zhang",
        "Zhaoyang Wang",
        "Yubo Ma",
        "Zhun Deng",
        "Rujia Wang",
        "Chetan Bansal",
        "S. Rajmohan",
        "Jiawei Han",
        "Huaxiu Yao"
      ],
      "abstract": "Current Large Language Models (LLMs) excel in general reasoning yet struggle with specialized tasks requiring proprietary or domain-specific knowledge. Fine-tuning large models for every niche application is often infeasible due to black-box constraints and high computational overhead. To address this, we propose a collaborative framework that pairs a specialized weak model with a general strong model. The weak model, tailored to specific domains, produces initial drafts and background information, while the strong model leverages its advanced reasoning to refine these drafts, extending LLMs' capabilities to critical yet specialized tasks. To optimize this collaboration, we introduce a collaborative feedback to fine-tunes the weak model, which quantifies the influence of the weak model's contributions in the collaboration procedure and establishes preference pairs to guide preference tuning of the weak model. We validate our framework through experiments on three domains. We find that the collaboration significantly outperforms each model alone by leveraging complementary strengths. Moreover, aligning the weak model with the collaborative preference further enhances overall performance.",
      "arxiv_url": "https://arxiv.org/abs/2504.15188",
      "pdf_url": "https://arxiv.org/pdf/2504.15188",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.15071",
      "title": "Can Large Language Models Understand Internet Buzzwords Through User-Generated Content",
      "authors": [
        "Chen Huang",
        "Junkai Luo",
        "Xinzuo Wang",
        "Wenqiang Lei",
        "Jiancheng Lv"
      ],
      "abstract": "The massive user-generated content (UGC) available in Chinese social media is giving rise to the possibility of studying internet buzzwords. In this paper, we study if large language models (LLMs) can generate accurate definitions for these buzzwords based on UGC as examples. Our work serves a threefold contribution. First, we introduce CHEER, the first dataset of Chinese internet buzzwords, each annotated with a definition and relevant UGC. Second, we propose a novel method, called RESS, to effectively steer the comprehending process of LLMs to produce more accurate buzzword definitions, mirroring the skills of human language learning. Third, with CHEER, we benchmark the strengths and weaknesses of various off-the-shelf definition generation methods and our RESS. Our benchmark demonstrates the effectiveness of RESS while revealing crucial shared challenges: over-reliance on prior exposure, underdeveloped inferential abilities, and difficulty identifying high-quality UGC to facilitate comprehension. We believe our work lays the groundwork for future advancements in LLM-based definition generation. Our dataset and code are available at https://github.com/SCUNLP/Buzzword.",
      "arxiv_url": "https://arxiv.org/abs/2505.15071",
      "pdf_url": "https://arxiv.org/pdf/2505.15071",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.05111",
      "title": "Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders",
      "authors": [
        "Boyi Deng",
        "Yunyang Wan",
        "Yidan Zhang",
        "Baosong Yang",
        "Fuli Feng"
      ],
      "abstract": "The mechanisms behind multilingual capabilities in Large Language Models (LLMs) have been examined using neuron-based or internal-activation-based methods. However, these methods often face challenges such as superposition and layer-wise activation variance, which limit their reliability. Sparse Autoencoders (SAEs) offer a more nuanced analysis by decomposing the activations of LLMs into a sparse linear combination of SAE features. We introduce a novel metric to assess the monolinguality of features obtained from SAEs, discovering that some features are strongly related to specific languages. Additionally, we show that ablating these SAE features only significantly reduces abilities in one language of LLMs, leaving others almost unaffected. Interestingly, we find some languages have multiple synergistic SAE features, and ablating them together yields greater improvement than ablating individually. Moreover, we leverage these SAE-derived language-specific features to enhance steering vectors, achieving control over the language generated by LLMs. The code is publicly available at https://github.com/Aatrox103/multilingual-llm-features.",
      "arxiv_url": "https://arxiv.org/abs/2505.05111",
      "pdf_url": "https://arxiv.org/pdf/2505.05111",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06401",
      "title": "Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs",
      "authors": [
        "Hongming Yang",
        "Shi Lin",
        "Jun Shao",
        "Changting Lin",
        "Donghai Zhu",
        "Meng Han",
        "Qinglei Kong"
      ],
      "abstract": "Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized models designed to run efficiently on consumer-grade hardware, offering significant advantages in resource efficiency, cost-effectiveness, and data privacy. However, these models often struggle with limited inference and reasoning capabilities, which restrict their performance on complex tasks and limit their practical applicability. Moreover, existing prompt optimization methods typically rely on extensive manual effort or the meta-cognitive abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To address these challenges, we introduce DeBoP, a new Direct Behavior Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting technique. Unlike CoT Prompting, DeBoP is an automatic optimization method, which focuses on the optimization directly on the behavior of LwLLMs. In particular, DeBoP transforms the optimization of complex prompts into the optimization of discrete, quantifiable execution sequences using a gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging tasks where state-of-the-art LLMs excel but LwLLMs generally underperform. Experimental results demonstrate that DeBoP significantly outperforms recent prompt optimization methods on most tasks. In particular, DeBoP-optimized LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by approximately 60% compared to other automatic prompt optimization methods.",
      "arxiv_url": "https://arxiv.org/abs/2506.06401",
      "pdf_url": "https://arxiv.org/pdf/2506.06401",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24355",
      "title": "Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model",
      "authors": [
        "Sihan Tan",
        "Taro Miyazaki",
        "Kazuhiro Nakadai"
      ],
      "abstract": "Sign Language Translation (SLT) aims to convert sign language (SL) videos into spoken language text, thereby bridging the communication gap between the sign and the spoken community. While most existing works focus on translating a single sign language into a single spoken language (one-to-one SLT), leveraging multilingual resources could mitigate low-resource issues and enhance accessibility. However, multilingual SLT (MLSLT) remains unexplored due to language conflicts and alignment difficulties across SLs and spoken languages. To address these challenges, we propose a multilingual gloss-free model with dual CTC objectives for token-level SL identification and spoken text generation. Our model supports 10 SLs and handles one-to-one, many-to-one, and many-to-many SLT tasks, achieving competitive performance compared to state-of-the-art methods on three widely adopted benchmarks: multilingual SP-10, PHOENIX14T, and CSL-Daily.",
      "arxiv_url": "https://arxiv.org/abs/2505.24355",
      "pdf_url": "https://arxiv.org/pdf/2505.24355",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04779",
      "title": "Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of LLMs on Formal Specification Inference",
      "authors": [
        "Thanh Le-Cong",
        "Bach Le",
        "Toby Murray"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being used to automate programming tasks. Yet, LLMs' capabilities in reasoning about program semantics are still inadequately studied, leaving significant potential for further exploration. This paper introduces FormalBench, a comprehensive benchmark designed to evaluate LLMs' reasoning abilities on program semantics, particularly via the task of synthesizing formal program specifications to assist verifying program correctness. This task requires both comprehensive reasoning over all possible program executions and the generation of precise, syntactically correct expressions that adhere to formal syntax and semantics. Using this benchmark, we evaluated the ability of LLMs in synthesizing consistent and complete specifications. Our findings show that LLMs perform well with simple control flows but struggle with more complex structures, especially loops, even with advanced prompting. Additionally, LLMs exhibit limited robustness against semantic-preserving transformations. We also highlight common failure patterns and design self-repair prompts, improving success rates by 25%.",
      "arxiv_url": "https://arxiv.org/abs/2503.04779",
      "pdf_url": "https://arxiv.org/pdf/2503.04779",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5b2234269a8c65eca9068ddd56f293ec1067c0fb",
      "title": "A Survey of Post-Training Scaling in Large Language Models",
      "authors": [
        "Hanyu Lai",
        "Xiao Liu",
        "Junjie Gao",
        "Jiale Cheng",
        "Zehan Qi",
        "Yifan Xu",
        "Shuntian Yao",
        "Dan Zhang",
        "Jinhua Du",
        "Zhenyu Hou",
        "Xin Lv",
        "Minlie Huang",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable proficiency in understanding and generating human natural languages, mainly owing to the \"scaling law\" that optimizes relationships among language modeling loss, model parameters, and pre-trained tokens. However, with the exhaustion of high-quality internet corpora and increasing computational demands, the sustainability of pre-training scaling needs to be addressed. This paper presents a comprehensive survey of post-training scaling, an emergent paradigm aiming to relieve the limitations of traditional pre-training by focusing on the alignment phase, which traditionally accounts for a minor fraction of the total training computation. Our survey categorizes post-training scaling into three key methodologies: Supervised Fine-tuning (SFT), Reinforcement Learning from Feedback (RLxF), and Test-time Compute (TTC). We provide an in-depth analysis of the motivation behind post-training scaling, the scalable variants of these methodologies, and a comparative discussion against traditional approaches. By examining the latest advancements, identifying promising application scenarios, and highlighting unresolved issues, we seek a coherent understanding and map future research trajectories in the landscape of post-training scaling for LLMs.",
      "arxiv_url": "https://www.semanticscholar.org/paper/5b2234269a8c65eca9068ddd56f293ec1067c0fb",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21250",
      "title": "ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision",
      "authors": [
        "Dosung Lee",
        "Wonjun Oh",
        "Boyoung Kim",
        "Minyoung Kim",
        "Joonsuk Park",
        "Paul Hongsuck Seo"
      ],
      "abstract": "Multi-hop question answering (MHQA) involves reasoning across multiple documents to answer complex questions. Dense retrievers typically outperform sparse methods like BM25 by leveraging semantic embeddings; however, they require labeled query-document pairs for fine-tuning. This poses a significant challenge in MHQA due to the high variability of queries (reformulated) questions throughout the reasoning steps. To overcome this limitation, we introduce Retriever Supervision with Consistency and Relevance (ReSCORE), a novel method for training dense retrievers for MHQA without labeled documents. ReSCORE leverages large language models to capture each documents relevance to the question and consistency with the correct answer and use them to train a retriever within an iterative question-answering framework. Experiments on three MHQA benchmarks demonstrate the effectiveness of ReSCORE, with significant improvements in retrieval, and in turn, the state-of-the-art MHQA performance. Our implementation is available at: https://leeds1219.github.io/ReSCORE.",
      "arxiv_url": "https://arxiv.org/abs/2505.21250",
      "pdf_url": "https://arxiv.org/pdf/2505.21250",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5b66e661791932719c90c6ee54917027d59606b5",
      "title": "Biased LLMs can Influence Political Decision-Making",
      "authors": [
        "Jillian R. Fisher",
        "Shangbin Feng",
        "Robert Aron",
        "Thomas Richardson",
        "Yejin Choi",
        "Daniel W. Fisher",
        "Jennifer Pan",
        "Yulia Tsvetkov",
        "Katharina Reinecke"
      ],
      "abstract": "As modern large language models (LLMs) become integral to everyday tasks, concerns about their inherent biases and their potential impact on human decision-making have emerged. While bias in models are well-documented, less is known about how these biases influence human decisions. This paper presents two interactive experiments investigating the effects of partisan bias in LLMs on political opinions and decision-making. Participants interacted freely with either a biased liberal, biased conservative, or unbiased control model while completing these tasks. We found that participants exposed to partisan biased models were significantly more likely to adopt opinions and make decisions which matched the LLM’s bias. Even more surprising, this influence was seen when the model bias and personal political partisan-ship of the participant were opposite. However, we also discovered that prior knowledge of AI was weakly correlated with a reduction of the impact of the bias, highlighting the possible importance of AI education for robust mitigation of bias effects. Our findings not only highlight the critical effects of interacting with biased LLMs and its ability to impact public discourse and political conduct, but also highlights potential techniques for mitigating these risks in the future.",
      "arxiv_url": "https://www.semanticscholar.org/paper/5b66e661791932719c90c6ee54917027d59606b5",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.06186",
      "title": "Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies",
      "authors": [
        "Massimiliano Pronesti",
        "Joao H. Bettencourt-Silva",
        "Paul Flanagan",
        "Alessandra Pascale",
        "Oisin Redmond",
        "Anya Belz",
        "Yufang Hou"
      ],
      "abstract": "Extracting scientific evidence from biomedical studies for clinical research questions (e.g., Does stem cell transplantation improve quality of life in patients with medically refractory Crohn's disease compared to placebo?) is a crucial step in synthesising biomedical evidence. In this paper, we focus on the task of document-level scientific evidence extraction for clinical questions with conflicting evidence. To support this task, we create a dataset called CochraneForest, leveraging forest plots from Cochrane systematic reviews. It comprises 202 annotated forest plots, associated clinical research questions, full texts of studies, and study-specific conclusions. Building on CochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a retrieval-augmented generation framework designed to tackle the unique challenges of evidence extraction. Our experiments show that URCA outperforms the best existing methods by up to 10.3% in F1 score on this task. However, the results also underscore the complexity of CochraneForest, establishing it as a challenging testbed for advancing automated evidence synthesis systems.",
      "arxiv_url": "https://arxiv.org/abs/2505.06186",
      "pdf_url": "https://arxiv.org/pdf/2505.06186",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-05-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01326",
      "title": "ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for Operations Research",
      "authors": [
        "Zhiyuan Wang",
        "Bokui Chen",
        "Yinya Huang",
        "Qingxing Cao",
        "Ming He",
        "Jianpin Fan",
        "Xiaodan Liang"
      ],
      "abstract": "Operations research (OR) is widely deployed to solve critical decision-making problems with complex objectives and constraints, impacting manufacturing, logistics, finance, and healthcare outcomes. While Large Language Models (LLMs) have shown promising results in various domains, their practical application in industry-relevant operations research (OR) problems presents significant challenges and opportunities. Preliminary industrial applications of LLMs for operations research face two critical deployment challenges: 1) Self-correction focuses on code syntax rather than mathematical accuracy, causing costly errors; 2) Complex expert selection creates unpredictable workflows that reduce transparency and increase maintenance costs, making them impractical for time-sensitive business applications. To address these business limitations, we introduce ORMind, a cognitive-inspired framework that enhances optimization through counterfactual reasoning. Our approach emulates human cognition, implementing an end-to-end workflow that systematically transforms requirements into mathematical models and executable solver code. It is currently being tested internally in Lenovo's AI Assistant, with plans to enhance optimization capabilities for both business and consumer customers. Experiments demonstrate that ORMind outperforms existing methods, achieving a 9.5\\% improvement on the NL4Opt dataset and a 14.6\\% improvement on the ComplexOR dataset.",
      "arxiv_url": "https://arxiv.org/abs/2506.01326",
      "pdf_url": "https://arxiv.org/pdf/2506.01326",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00425",
      "title": "Inter-Passage Verification for Multi-evidence Multi-answer QA",
      "authors": [
        "Bingsen Chen",
        "Shengjie Wang",
        "Xi Ye",
        "Chen Zhao"
      ],
      "abstract": "Multi-answer question answering (QA), where questions can have many valid answers, presents a significant challenge for existing retrieval-augmented generation-based QA systems, as these systems struggle to retrieve and then synthesize a large number of evidence passages. To tackle these challenges, we propose a new multi-answer QA framework -- Retrieval-augmented Independent Reading with Inter-passage Verification (RI$^2$VER). Our framework retrieves a large set of passages and processes each passage individually to generate an initial high-recall but noisy answer set. Then we propose a new inter-passage verification pipeline that validates every candidate answer through (1) Verification Question Generation, (2) Gathering Additional Evidence, and (3) Verification with inter-passage synthesis. Evaluations on the QAMPARI and RoMQA datasets demonstrate that our framework significantly outperforms existing baselines across various model sizes, achieving an average F1 score improvement of 11.17%. Further analysis validates that our inter-passage verification pipeline enables our framework to be particularly beneficial for questions requiring multi-evidence synthesis.",
      "arxiv_url": "https://arxiv.org/abs/2506.00425",
      "pdf_url": "https://arxiv.org/pdf/2506.00425",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14279",
      "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering",
      "authors": [
        "Jennifer D’Souza",
        "Hamed Babaei Giglou",
        "Quentin Münch"
      ],
      "abstract": "Large Language Models (LLMs) drive scientific question-answering on modern search engines, yet their evaluation robustness remains underexplored. We introduce YESciEval, an open-source framework that combines fine-grained rubric-based assessment with reinforcement learning to mitigate optimism bias in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including adversarial variants, with evaluation scores from multiple LLMs. Independent of proprietary models and human feedback, our approach enables scalable, cost-free evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI alignment and fosters robust, transparent evaluation essential for scientific inquiry.",
      "arxiv_url": "https://arxiv.org/abs/2505.14279",
      "pdf_url": "https://arxiv.org/pdf/2505.14279",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.22708",
      "title": "CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation",
      "authors": [
        "Peter Alexander Jansen",
        "Oyvind Tafjord",
        "Marissa Radensky",
        "Pao Siangliulue",
        "Tom Hope",
        "Bhavana Dalvi",
        "Bodhisattwa Prasad Majumder",
        "Daniel S. Weld",
        "Peter Clark"
      ],
      "abstract": "Despite the surge of interest in autonomous scientific discovery (ASD) of software artifacts (e.g., improved ML algorithms), current ASD systems face two key limitations: (1) they largely explore variants of existing codebases or similarly constrained design spaces, and (2) they produce large volumes of research artifacts (such as automatically generated papers and code) that are typically evaluated using conference-style paper review with limited evaluation of code. In this work we introduce CodeScientist, a novel ASD system that frames ideation and experiment construction as a form of genetic search jointly over combinations of research articles and codeblocks defining common actions in a domain (like prompting a language model). We use this paradigm to conduct hundreds of automated experiments on machine-generated ideas broadly in the domain of agents and virtual environments, with the system returning 19 discoveries, 6 of which were judged as being both at least minimally sound and incrementally novel after a multi-faceted evaluation beyond that typically conducted in prior work, including external (conference-style) review, code review, and replication attempts. Moreover, the discoveries span new tasks, agents, metrics, and data, suggesting a qualitative shift from benchmark optimization to broader discoveries.",
      "arxiv_url": "https://arxiv.org/abs/2503.22708",
      "pdf_url": "https://arxiv.org/pdf/2503.22708",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5c195fd3f72443b201b2fba90bc8ea8f69a62e14",
      "title": "StructFact: Reasoning Factual Knowledge from Structured Data with Large Language Models",
      "authors": [
        "Sirui Huang",
        "Yanggan Gu",
        "Zhonghao Li",
        "Xuming Hu",
        "Qing Li",
        "Guandong Xu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/5c195fd3f72443b201b2fba90bc8ea8f69a62e14",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5c2b1a00336a98aebb136bfef86dab9201d79340",
      "title": "Act2P: LLM-Driven Online Dialogue Act Classification for Power Analysis",
      "authors": [
        "Zhangwenbo Zhangwenbo",
        "Wang Yuhan"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/5c2b1a00336a98aebb136bfef86dab9201d79340",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16940",
      "title": "Reasoning Does Not Necessarily Improve Role-Playing Ability",
      "authors": [
        "Xiachong Feng",
        "Longxu Dou",
        "Lingpeng Kong"
      ],
      "abstract": "The application of role-playing large language models (LLMs) is rapidly expanding in both academic and commercial domains, driving an increasing demand for high-precision role-playing models. Simultaneously, the rapid advancement of reasoning techniques has continuously pushed the performance boundaries of LLMs. This intersection of practical role-playing demands and evolving reasoning capabilities raises an important research question:\"Can reasoning techniques enhance the role-playing capabilities of LLMs?\"To address this, we conduct a comprehensive study using 6 role-playing benchmarks, 24 LLMs, and 3 distinct role-playing strategies, comparing the effectiveness of direct zero-shot role-playing, role-playing with Chain-of-Thought (CoT), and role-playing using reasoning-optimized LLMs. Our findings reveal that CoT may reduce role-playing performance, reasoning-optimized LLMs are unsuitable for role-playing, reasoning ability disrupts the role-playing scaling law, large models still lack proficiency in advanced role-playing, and Chinese role-playing performance surpasses English role-playing performance. Furthermore, based on extensive experimental results, we propose two promising future research directions: Role-aware CoT for improving role-playing LLMs and Reinforcement Learning for role-playing LLMs, aiming to enhance the adaptability, consistency, and effectiveness of role-playing LLMs for both research and real-world applications.",
      "arxiv_url": "https://arxiv.org/abs/2502.16940",
      "pdf_url": "https://arxiv.org/pdf/2502.16940",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2509.17493",
      "title": "Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages",
      "authors": [
        "Wenhao Zhuang",
        "Yuan Sun",
        "Xiaobing Zhao"
      ],
      "abstract": "As large language models (LLMs) are trained on increasingly diverse and extensive multilingual corpora, they demonstrate cross-lingual transfer capabilities. However, these capabilities often fail to effectively extend to low-resource languages, particularly those utilizing non-Latin scripts. While transliterating low-resource languages into Latin script presents a natural solution, there currently lacks a comprehensive framework for integrating transliteration into LLMs training and deployment. Taking a pragmatic approach, this paper innovatively combines character transliteration with Huffman coding to design a complete transliteration framework. Our proposed framework offers the following advantages: 1) Compression: Reduces storage requirements for low-resource language content, achieving up to 50% reduction in file size and 50-80% reduction in token count. 2) Accuracy: Guarantees 100% lossless conversion from transliterated text back to the source language. 3) Efficiency: Eliminates the need for vocabulary expansion for low-resource languages, improving training and inference efficiency. 4) Scalability: The framework can be extended to other low-resource languages. We validate the effectiveness of our framework across multiple downstream tasks, including text classification, machine reading comprehension, and machine translation. Experimental results demonstrate that our method significantly enhances the model's capability to process low-resource languages while maintaining performance on high-resource languages. Our data and code are publicly available at https://github.com/CMLI-NLP/HuffmanTranslit.",
      "arxiv_url": "https://arxiv.org/abs/2509.17493",
      "pdf_url": "https://arxiv.org/pdf/2509.17493",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-09-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.01054",
      "title": "Dynamic Scaling of Unit Tests for Code Reward Modeling",
      "authors": [
        "Zeyao Ma",
        "Xiaokang Zhang",
        "Jing Zhang",
        "Jifan Yu",
        "Sijia Luo",
        "Jie Tang"
      ],
      "abstract": "Current large language models (LLMs) often struggle to produce accurate responses on the first attempt for complex reasoning tasks like code generation. Prior research tackles this challenge by generating multiple candidate solutions and validating them with LLM-generated unit tests. The execution results of unit tests serve as reward signals to identify correct solutions. As LLMs always confidently make mistakes, these unit tests are not reliable, thereby diminishing the quality of reward signals. Motivated by the observation that scaling the number of solutions improves LLM performance, we explore the impact of scaling unit tests to enhance reward signal quality. Our pioneer experiment reveals a positive correlation between the number of unit tests and reward signal quality, with greater benefits observed in more challenging problems. Based on these insights, we propose CodeRM-8B, a lightweight yet effective unit test generator that enables efficient and high-quality unit test scaling. Additionally, we implement a dynamic scaling mechanism that adapts the number of unit tests based on problem difficulty, further improving efficiency. Experimental results show that our approach significantly improves performance across various models on three benchmarks (e.g., with gains of 18.43% for Llama3-8B and 3.42% for GPT-4o-mini on HumanEval Plus).",
      "arxiv_url": "https://arxiv.org/abs/2501.01054",
      "pdf_url": "https://arxiv.org/pdf/2501.01054",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10084",
      "title": "Why Prompt Design Matters and Works: A Complexity Analysis of Prompt Search Space in LLMs",
      "authors": [
        "Xiang Zhang",
        "Juntai Cao",
        "Jiaqi Wei",
        "Chenyu You",
        "Dujian Ding"
      ],
      "abstract": "Despite the remarkable successes of large language models (LLMs), the underlying Transformer architecture has inherent limitations in handling complex reasoning tasks. Chain-of-thought (CoT) prompting has emerged as a practical workaround, but most CoT-based methods rely on a single, generic prompt such as\"think step by step\", with no task-specific adaptation. These approaches expect the model to discover an effective reasoning path on its own, forcing it to search through a vast prompt space. In contrast, several studies have explored task-specific prompt designs to boost performance. However, these designs are typically developed through trial and error, lacking theoretical grounding. As a result, prompt engineering remains largely ad hoc and unguided. In this paper, we provide a theoretical framework that explains why some prompts succeed while others fail. We show that prompts function as selectors, extracting task-relevant information from the model's full hidden state during CoT reasoning. Each prompt defines a unique trajectory through the answer space, and the choice of trajectory is crucial for task performance and future navigation within the space. We analyze the complexity of finding optimal prompts and characterize the size of the prompt space for a given task. Our theory reveals principles behind effective prompt design and shows that naive CoT-using self-guided prompts like\"think step by step\"-can severely hinder performance. Through experiments, we show that optimal prompt search can lead to more than a 50% improvement on reasoning tasks, providing a theoretical foundation for prompt engineering.",
      "arxiv_url": "https://arxiv.org/abs/2503.10084",
      "pdf_url": "https://arxiv.org/pdf/2503.10084",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11116",
      "title": "Gumbel Reranking: Differentiable End-to-End Reranker Optimization",
      "authors": [
        "Siyuan Huang",
        "Zhiyuan Ma",
        "Jintao Du",
        "Changhua Meng",
        "Weiqiang Wang",
        "Jingwen Leng",
        "Minyi Guo",
        "Zhouhan Lin"
      ],
      "abstract": "RAG systems rely on rerankers to identify relevant documents. However, fine-tuning these models remains challenging due to the scarcity of annotated query-document pairs. Existing distillation-based approaches suffer from training-inference misalignment and fail to capture interdependencies among candidate documents. To overcome these limitations, we reframe the reranking process as an attention-mask problem and propose Gumbel Reranking, an end-to-end training framework for rerankers aimed at minimizing the training-inference gap. In our approach, reranker optimization is reformulated as learning a stochastic, document-wise Top-$k$ attention mask using the Gumbel Trick and Relaxed Top-$k$ Sampling. This formulation enables end-to-end optimization by minimizing the overall language loss. Experiments across various settings consistently demonstrate performance gains, including a 10.4\\% improvement in recall on HotpotQA for distinguishing indirectly relevant documents.",
      "arxiv_url": "https://arxiv.org/abs/2502.11116",
      "pdf_url": "https://arxiv.org/pdf/2502.11116",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23026",
      "title": "Context-Robust Knowledge Editing for Language Models",
      "authors": [
        "Haewon Park",
        "Gyubin Choi",
        "Minjun Kim",
        "Yohan Jo"
      ],
      "abstract": "Knowledge editing (KE) methods offer an efficient way to modify knowledge in large language models. Current KE evaluations typically assess editing success by considering only the edited knowledge without any preceding contexts. In real-world applications, however, preceding contexts often trigger the retrieval of the original knowledge and undermine the intended edit. To address this issue, we develop CHED -- a benchmark designed to evaluate the context robustness of KE methods. Evaluations on CHED show that they often fail when preceding contexts are present. To mitigate this shortcoming, we introduce CoRE, a KE method designed to strengthen context robustness by minimizing context-sensitive variance in hidden states of the model for edited knowledge. This method not only improves the editing success rate in situations where a preceding context is present but also preserves the overall capabilities of the model. We provide an in-depth analysis of the differing impacts of preceding contexts when introduced as user utterances versus assistant responses, and we dissect attention-score patterns to assess how specific tokens influence editing success.",
      "arxiv_url": "https://arxiv.org/abs/2505.23026",
      "pdf_url": "https://arxiv.org/pdf/2505.23026",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13645",
      "title": "Measuring the Effect of Transcription Noise on Downstream Language Understanding Tasks",
      "authors": [
        "Ori Shapira",
        "Shlomo E. Chazan",
        "Amir DN Cohen"
      ],
      "abstract": "With the increasing prevalence of recorded human speech, spoken language understanding (SLU) is essential for its efficient processing. In order to process the speech, it is commonly transcribed using automatic speech recognition technology. This speech-to-text transition introduces errors into the transcripts, which subsequently propagate to downstream NLP tasks, such as dialogue summarization. While it is known that transcript noise affects downstream tasks, a systematic approach to analyzing its effects across different noise severities and types has not been addressed. We propose a configurable framework for assessing task models in diverse noisy settings, and for examining the impact of transcript-cleaning techniques. The framework facilitates the investigation of task model behavior, which can in turn support the development of effective SLU solutions. We exemplify the utility of our framework on three SLU tasks and four task models, offering insights regarding the effect of transcript noise on tasks in general and models in particular. For instance, we find that task models can tolerate a certain level of noise, and are affected differently by the types of errors in the transcript.",
      "arxiv_url": "https://arxiv.org/abs/2502.13645",
      "pdf_url": "https://arxiv.org/pdf/2502.13645",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15821",
      "title": "Towards Robust ESG Analysis Against Greenwashing Risks: Aspect-Action Analysis with Cross-Category Generalization",
      "authors": [
        "Keane Ong",
        "Rui Mao",
        "Deeksha Varshney",
        "Erik Cambria",
        "G. Mengaldo"
      ],
      "abstract": "Sustainability reports are key for evaluating companies' environmental, social and governance, ESG performance, but their content is increasingly obscured by greenwashing - sustainability claims that are misleading, exaggerated, and fabricated. Yet, existing NLP approaches for ESG analysis lack robustness against greenwashing risks, often extracting insights that reflect misleading or exaggerated sustainability claims rather than objective ESG performance. To bridge this gap, we introduce A3CG - Aspect-Action Analysis with Cross-Category Generalization, as a novel dataset to improve the robustness of ESG analysis amid the prevalence of greenwashing. By explicitly linking sustainability aspects with their associated actions, A3CG facilitates a more fine-grained and transparent evaluation of sustainability claims, ensuring that insights are grounded in verifiable actions rather than vague or misleading rhetoric. Additionally, A3CG emphasizes cross-category generalization. This ensures robust model performance in aspect-action analysis even when companies change their reports to selectively favor certain sustainability areas. Through experiments on A3CG, we analyze state-of-the-art supervised models and LLMs, uncovering their limitations and outlining key directions for future research.",
      "arxiv_url": "https://arxiv.org/abs/2502.15821",
      "pdf_url": "https://arxiv.org/pdf/2502.15821",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5d86729c5e2701c19b14220cf2f308d4ff85c92d",
      "title": "The Role of Abstract Representations and Observed Preferences in the Ordering of Binomials in Large Language Models",
      "authors": [
        "Z. Houghton",
        "Kenji Sagae",
        "Emily Morgan"
      ],
      "abstract": "To what extent do large language models learn abstract representations as opposed to more superficial aspects of their very large training corpora? We examine this question in the context of binomial ordering preferences involving two conjoined nouns in English. When choosing a binomial ordering ( radio and television vs tele-vision and radio ), humans rely on more than simply the observed frequency of each option. Humans also rely on abstract ordering preferences (e.g., preferences for short words before long words). We investigate whether large language models simply rely on the observed preference in their training data, or whether they are capable of learning the abstract ordering preferences (i.e., abstract representations) that humans rely on. Our results suggest that both smaller and larger models’ ordering preferences are driven exclusively by their experience with that item in the training data. Our study provides further insights into differences between how large language models represent and use language and how humans do it, particularly with respect to the use of abstract representations versus observed preferences.",
      "arxiv_url": "https://www.semanticscholar.org/paper/5d86729c5e2701c19b14220cf2f308d4ff85c92d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.01543",
      "title": "Is External Information Useful for Stance Detection with LLMs?",
      "authors": [
        "Quang Minh Nguyen",
        "Taegyoon Kim"
      ],
      "abstract": "In the stance detection task, a text is classified as either favorable, opposing, or neutral towards a target. Prior work suggests that the use of external information, e.g., excerpts from Wikipedia, improves stance detection performance. However, whether or not such information can benefit large language models (LLMs) remains an unanswered question, despite their wide adoption in many reasoning tasks. In this study, we conduct a systematic evaluation on how Wikipedia and web search external information can affect stance detection across eight LLMs and in three datasets with 12 targets. Surprisingly, we find that such information degrades performance in most cases, with macro F1 scores dropping by up to 27.9\\%. We explain this through experiments showing LLMs'tendency to align their predictions with the stance and sentiment of the provided information rather than the ground truth stance of the given text. We also find that performance degradation persists with chain-of-thought prompting, while fine-tuning mitigates but does not fully eliminate it. Our findings, in contrast to previous literature on BERT-based systems which suggests that external information enhances performance, highlight the risks of information biases in LLM-based stance classifiers. Code is available at https://github.com/ngqm/acl2025-stance-detection.",
      "arxiv_url": "https://arxiv.org/abs/2507.01543",
      "pdf_url": "https://arxiv.org/pdf/2507.01543",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5dc2680014d2ee25a193ac007c64c6d5655ebf86",
      "title": "EMRs2CSP : Mining Clinical Status Pathway from Electronic Medical Records",
      "authors": [
        "Yifei Chen",
        "Ruihui Hou",
        "Jingping Liu",
        "Tong Ruan"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/5dc2680014d2ee25a193ac007c64c6d5655ebf86",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11932",
      "title": "Neuro-Symbolic Query Compiler",
      "authors": [
        "Yuyao Zhang",
        "Zhicheng Dou",
        "Xiaoxi Li",
        "Jiajie Jin",
        "Yongkang Wu",
        "Zhonghua Li",
        "Qi Ye",
        "Ji-Rong Wen"
      ],
      "abstract": "Precise recognition of search intent in Retrieval-Augmented Generation (RAG) systems remains a challenging goal, especially under resource constraints and for complex queries with nested structures and dependencies. This paper presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar rules and compiler design, to bridge this gap. It theoretically designs a minimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize complex queries. Unlike previous methods, this grammar maintains completeness while minimizing redundancy. Based on this, QCompiler includes a Query Expression Translator, a Lexical Syntax Parser, and a Recursive Descent Processor to compile queries into Abstract Syntax Trees (ASTs) for execution. The atomicity of the sub-queries in the leaf nodes ensures more precise document retrieval and response generation, significantly improving the RAG system's ability to address complex queries.",
      "arxiv_url": "https://arxiv.org/abs/2505.11932",
      "pdf_url": "https://arxiv.org/pdf/2505.11932",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5dd017d63695b015242d8cb1c8675a7fa38f246b",
      "title": "AGRec: Adapting Autoregressive Decoders with Graph Reasoning for LLM-based Sequential Recommendation",
      "authors": [
        "Xinfeng Wang",
        "Jin Cui",
        "Fumiyo Fukumoto",
        "Yoshimi Suzuki"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/5dd017d63695b015242d8cb1c8675a7fa38f246b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2512.12839",
      "title": "What Matters in Evaluating Book-Length Stories? A Systematic Study of Long Story Evaluation",
      "authors": [
        "Dingyi Yang",
        "Qin Jin"
      ],
      "abstract": "In this work, we conduct systematic research in a challenging area: the automatic evaluation of book-length stories (>100K tokens). Our study focuses on two key questions: (1) understanding which evaluation aspects matter most to readers, and (2) exploring effective methods for evaluating lengthy stories. We introduce the first large-scale benchmark, LongStoryEval, comprising 600 newly published books with an average length of 121K tokens (maximum 397K). Each book includes its average rating and multiple reader reviews, presented as critiques organized by evaluation aspects. By analyzing all user-mentioned aspects, we propose an evaluation criteria structure and conduct experiments to identify the most significant aspects among the 8 top-level criteria. For evaluation methods, we compare the effectiveness of three types: aggregation-based, incremental-updated, and summary-based evaluations. Our findings reveal that aggregation- and summary-based evaluations perform better, with the former excelling in detail assessment and the latter offering greater efficiency. Building on these insights, we further propose NovelCritique, an 8B model that leverages the efficient summary-based framework to review and score stories across specified aspects. NovelCritique outperforms commercial models like GPT-4o in aligning with human evaluations. Our datasets and codes are available at https://github.com/DingyiYang/LongStoryEval.",
      "arxiv_url": "https://arxiv.org/abs/2512.12839",
      "pdf_url": "https://arxiv.org/pdf/2512.12839",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-12-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.05790",
      "title": "Understanding Impact of Human Feedback via Influence Functions",
      "authors": [
        "Taywon Min",
        "Haeone Lee",
        "Hanho Ryu",
        "Yongchan Kwon",
        "Kimin Lee"
      ],
      "abstract": "In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn suitable reward models from human feedback to align large language models (LLMs) with human intentions. However, human feedback can often be noisy, inconsistent, or biased, especially when evaluating complex responses. Such feedback can lead to misaligned reward signals, potentially causing unintended side effects during the RLHF process. To address these challenges, we explore the use of influence functions to measure the impact of human feedback on the performance of reward models. We propose a compute-efficient approximation method that enables the application of influence functions to LLM-based reward models and large-scale preference datasets. Our experiments showcase two key applications of influence functions: (1) detecting common labeler biases in human feedback datasets and (2) guiding labelers in refining their strategies to better align with expert feedback. By quantifying the impact of human feedback, we believe that influence functions can enhance feedback interpretability and contribute to scalable oversight in RLHF, helping labelers provide more accurate and consistent feedback. Source code is available at https://github.com/mintaywon/IF_RLHF",
      "arxiv_url": "https://arxiv.org/abs/2501.05790",
      "pdf_url": "https://arxiv.org/pdf/2501.05790",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2511.01864",
      "title": "Missing the Margins: A Systematic Literature Review on the Demographic Representativeness of LLMs",
      "authors": [
        "Indira Sen",
        "Marlene Lutz",
        "Elisa Rogers",
        "David García",
        "Markus Strohmaier"
      ],
      "abstract": "Many applications of Large Language Models (LLMs) require them to either simulate people or offer personalized functionality, making the demographic representativeness of LLMs crucial for equitable utility. At the same time, we know little about the extent to which these models actually reflect the demographic attributes and behaviors of certain groups or populations, with conflicting findings in empirical research. To shed light on this debate, we review 211 papers on the demographic representativeness of LLMs. We find that while 29% of the studies report positive conclusions on the representativeness of LLMs, 30% of these do not evaluate LLMs across multiple demographic categories or within demographic subcategories. Another 35% and 47% of the papers concluding positively fail to specify these subcategories altogether for gender and race, respectively. Of the articles that do report subcategories, fewer than half include marginalized groups in their study. Finally, more than a third of the papers do not define the target population to whom their findings apply; of those that do define it either implicitly or explicitly, a large majority study only the U.S. Taken together, our findings suggest an inflated perception of LLM representativeness in the broader community. We recommend more precise evaluation methods and comprehensive documentation of demographic attributes to ensure the responsible use of LLMs for social applications. Our annotated list of papers and analysis code is publicly available.",
      "arxiv_url": "https://arxiv.org/abs/2511.01864",
      "pdf_url": "https://arxiv.org/pdf/2511.01864",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-10-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5defbcca5f151edab6bca9d19c17131b9b66724b",
      "title": "EnerGIZAr: Leveraging GIZA++ for Effective Tokenizer Initialization",
      "authors": [
        "Pranaydeep Singh",
        "Eneko Agirre",
        "Gorka Azkune",
        "Orphée De Clercq",
        "Els Lefever"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/5defbcca5f151edab6bca9d19c17131b9b66724b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22943",
      "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates",
      "authors": [
        "Jaewoo Ahn",
        "Heeseung Yun",
        "Dayoon Ko",
        "Gunhee Kim"
      ],
      "abstract": "While pre-trained multimodal representations (e.g., CLIP) have shown impressive capabilities, they exhibit significant compositional vulnerabilities leading to counterintuitive judgments. We introduce Multimodal Adversarial Compositionality (MAC), a benchmark that leverages large language models (LLMs) to generate deceptive text samples to exploit these vulnerabilities across different modalities and evaluates them through both sample-wise attack success rate and group-wise entropy-based diversity. To improve zero-shot methods, we propose a self-training approach that leverages rejection-sampling fine-tuning with diversity-promoting filtering, which enhances both attack success rate and sample diversity. Using smaller language models like Llama-3.1-8B, our approach demonstrates superior performance in revealing compositional vulnerabilities across various multimodal representations, including images, videos, and audios.",
      "arxiv_url": "https://arxiv.org/abs/2505.22943",
      "pdf_url": "https://arxiv.org/pdf/2505.22943",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.10877",
      "title": "Enhancing Medical Dialogue Generation through Knowledge Refinement and Dynamic Prompt Adjustment",
      "authors": [
        "Hongda Sun",
        "Jiaren Peng",
        "Wenzhong Yang",
        "Liang He",
        "Bo Du",
        "Rui Yan"
      ],
      "abstract": "Medical dialogue systems (MDS) have emerged as crucial online platforms for enabling multi-turn, context-aware conversations with patients. However, existing MDS often struggle to (1) identify relevant medical knowledge and (2) generate personalized, medically accurate responses. To address these challenges, we propose MedRef, a novel MDS that incorporates knowledge refining and dynamic prompt adjustment. First, we employ a knowledge refining mechanism to filter out irrelevant medical data, improving predictions of critical medical entities in responses. Additionally, we design a comprehensive prompt structure that incorporates historical details and evident details. To enable real-time adaptability to diverse patient conditions, we implement two key modules, Triplet Filter and Demo Selector, providing appropriate knowledge and demonstrations equipped in the system prompt. Extensive experiments on MedDG and KaMed benchmarks show that MedRef outperforms state-of-the-art baselines in both generation quality and medical entity accuracy, underscoring its effectiveness and reliability for real-world healthcare applications.",
      "arxiv_url": "https://arxiv.org/abs/2506.10877",
      "pdf_url": "https://arxiv.org/pdf/2506.10877",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Personalization"
      ],
      "published_date": "2025-06-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.12563",
      "title": "MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation",
      "authors": [
        "Haris Riaz",
        "Sourav S. Bhabesh",
        "Vinayak Arannil",
        "Miguel Ballesteros",
        "Graham Horwood"
      ],
      "abstract": "Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data generated using larger Language models. Questions remain about leveraging synthetic data for other use cases, such as adapting LLMs to specific domains. A key limitation of synthetic data is low diversity, which negatively impacts its downstream applicability for improving other models. To address this, we propose MetaSynth, a method for generating synthetic data that enhances diversity through meta-prompting, where a language model orchestrates multiple\"expert\"LLM agents to collaboratively generate data. Using only 25 million tokens of synthetic data generated with MetaSynth, we successfully adapt a well-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and Biomedicine-without compromising the capabilities of the resulting model in general tasks. In addition, we evaluate the diversity of our synthetic data using seven automated metrics, and find that it approaches the diversity of LLM pre-training corpora. Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms the base LLM, showing improvements of up to 4.08% in Finance and 13.75% in Biomedicine. The same model shows degraded performance when trained on data generated using a template prompt, even when the template includes prior generations and varying In-Context exemplars of real data. Our findings suggest that a few million tokens of diverse synthetic data without mixing any real data, is sufficient for effective domain adaptation when using MetaSynth.",
      "arxiv_url": "https://arxiv.org/abs/2504.12563",
      "pdf_url": "https://arxiv.org/pdf/2504.12563",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-04-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11491",
      "title": "Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering",
      "authors": [
        "Runxuan Liu",
        "Bei Luo",
        "Jiaqi Li",
        "Baoxin Wang",
        "Ming Liu",
        "Dayong Wu",
        "Shijin Wang",
        "Bing Qin"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable capabilities in natural language processing. However, in knowledge graph question answering tasks (KGQA), there remains the issue of answering questions that require multi-hop reasoning. Existing methods rely on entity vector matching, but the purpose of the question is abstract and difficult to match with specific entities. As a result, it is difficult to establish reasoning paths to the purpose, which leads to information loss and redundancy. To address this issue, inspired by human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a novel framework that constructs reasoning paths from purposes back to conditions. ORT operates in three key phases: (1) using LLM to extract purpose labels and condition labels, (2) constructing label reasoning paths based on the KG ontology, and (3) using the label reasoning paths to guide knowledge retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves state-of-the-art performance and significantly enhances the capability of LLMs for KGQA.",
      "arxiv_url": "https://arxiv.org/abs/2502.11491",
      "pdf_url": "https://arxiv.org/pdf/2502.11491",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.05037",
      "title": "Collapse of Dense Retrievers: Short, Early, and Literal Biases Outranking Factual Evidence",
      "authors": [
        "Mohsen Fayyaz",
        "Ali Modarressi",
        "Hin-rich Schuetze",
        "Nanyun Peng"
      ],
      "abstract": "Dense retrieval models are commonly used in Information Retrieval (IR) applications, such as Retrieval-Augmented Generation (RAG). Since they often serve as the first step in these systems, their robustness is critical to avoid downstream failures. In this work, we repurpose a relation extraction dataset (e.g., Re-DocRED) to design controlled experiments that quantify the impact of heuristic biases, such as a preference for shorter documents, on retrievers like Dragon+ and Contriever. We uncover major vulnerabilities, showing retrievers favor shorter documents, early positions, repeated entities, and literal matches, all while ignoring the answer's presence! Notably, when multiple biases combine, models exhibit catastrophic performance degradation, selecting the answer-containing document in less than 10% of cases over a synthetic biased document without the answer. Furthermore, we show that these biases have direct consequences for downstream applications like RAG, where retrieval-preferred documents can mislead LLMs, resulting in a 34% performance drop than providing no documents at all. https://huggingface.co/datasets/mohsenfayyaz/ColDeR",
      "arxiv_url": "https://arxiv.org/abs/2503.05037",
      "pdf_url": "https://arxiv.org/pdf/2503.05037",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.10120",
      "title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
      "authors": [
        "Yichen He",
        "Guanhua Huang",
        "Peiyuan Feng",
        "Yuan Lin",
        "Yuchen Zhang",
        "Hang Li",
        "Weinan E"
      ],
      "abstract": "We introduce PaSa, an advanced Paper Search agent powered by large language models. PaSa can autonomously make a series of decisions, including invoking search tools, reading papers, and selecting relevant references, to ultimately obtain comprehensive and accurate results for complex scholar queries. We optimize PaSa using reinforcement learning with a synthetic dataset, AutoScholarQuery, which includes 35k fine-grained academic queries and corresponding papers sourced from top-tier AI conference publications. Additionally, we develop RealScholarQuery, a benchmark collecting real-world academic queries to assess PaSa performance in more realistic scenarios. Despite being trained on synthetic data, PaSa significantly outperforms existing baselines on RealScholarQuery, including Google, Google Scholar, Google with GPT-4o for paraphrased queries, ChatGPT (search-enabled GPT-4o), GPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably, PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78% in recall@20 and 39.90% in recall@50, and exceeds PaSa-GPT-4o by 30.36% in recall and 4.25% in precision. Model, datasets, and code are available at https://github.com/bytedance/pasa.",
      "arxiv_url": "https://arxiv.org/abs/2501.10120",
      "pdf_url": "https://arxiv.org/pdf/2501.10120",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Search Agent"
      ],
      "published_date": "2025-01-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.18056",
      "title": "MiniELM: A Lightweight and Adaptive Query Rewriting Framework for E-Commerce Search Optimization",
      "authors": [
        "Duy A. Nguyen",
        "R. Mohan",
        "Van Yang",
        "Pritom Saha Akash",
        "Kevin Chen-Chuan Chang"
      ],
      "abstract": "Query rewriting (QR) is a critical technique in e-commerce search, addressing the lexical gap between user queries and product descriptions to enhance search performance. Existing QR approaches typically fall into two categories: discriminative models and generative methods leveraging large language models (LLMs). Discriminative models often struggle with natural language understanding and offer limited flexibility in rewriting, while generative LLMs, despite producing high-quality rewrites, face high inference latency and cost in online settings. These limitations force offline deployment, making them vulnerable to issues like information staleness and semantic drift. To overcome these challenges, we propose a novel hybrid pipeline for QR that balances efficiency and effectiveness. Our approach combines offline knowledge distillation to create a lightweight but efficient student model with online reinforcement learning (RL) to refine query rewriting dynamically using real-time feedback. A key innovation is the use of LLMs as simulated human feedback, enabling scalable reward signals and cost-effective evaluation without manual annotations. Experimental results on Amazon ESCI dataset demonstrate significant improvements in query relevance, diversity, and adaptability, as well as positive feedback from the LLM simulation. This work contributes to advancing LLM capabilities for domain-specific applications, offering a robust solution for dynamic and complex e-commerce search environments.",
      "arxiv_url": "https://arxiv.org/abs/2501.18056",
      "pdf_url": "https://arxiv.org/pdf/2501.18056",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.16048",
      "title": "Meta-Learning Neural Mechanisms rather than Bayesian Priors",
      "authors": [
        "Michael Goodale",
        "Salvador Mascarenhas",
        "Yair Lakretz"
      ],
      "abstract": "Children acquire language despite being exposed to several orders of magnitude less data than large language models require. Meta-learning has been proposed as a way to integrate human-like learning biases into neural-network architectures, combining both the structured generalizations of symbolic models with the scalability of neural-network models. But what does meta-learning exactly imbue the model with? We investigate the meta-learning of formal languages and find that, contrary to previous claims, meta-trained models are not learning simplicity-based priors when meta-trained on datasets organised around simplicity. Rather, we find evidence that meta-training imprints neural mechanisms (such as counters) into the model, which function like cognitive primitives for the network on downstream tasks. Most surprisingly, we find that meta-training on a single formal language can provide as much improvement to a model as meta-training on 5000 different formal languages, provided that the formal language incentivizes the learning of useful neural mechanisms. Taken together, our findings provide practical implications for efficient meta-learning paradigms and new theoretical insights into linking symbolic theories and neural mechanisms.",
      "arxiv_url": "https://arxiv.org/abs/2503.16048",
      "pdf_url": "https://arxiv.org/pdf/2503.16048",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03149",
      "title": "Causal Estimation of Tokenisation Bias",
      "authors": [
        "Pietro Lesci",
        "Clara Meister",
        "Thomas Hofmann",
        "Andreas Vlachos",
        "Tiago Pimentel"
      ],
      "abstract": "Modern language models are typically trained over subword sequences, but ultimately define probabilities over character-strings. Ideally, the choice of the tokeniser -- which maps character-strings to subwords -- should not affect the probability assigned to the underlying character-string; in practice, it does. We define this mismatch as tokenisation bias. In this work, we quantify one particular type of tokenisation bias: the effect of including or not a subword (e.g., $\\langle hello \\rangle$) in a tokeniser's vocabulary on the probability a trained model assigns to the corresponding characters (i.e., \\textit{``hello''}). Estimating this effect is challenging because each model is trained with only one tokeniser. We address this by framing tokenisation bias as a causal effect and estimating it using the regression discontinuity design. Specifically, we exploit the fact that tokenisation algorithms rank subwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an arbitrary cutoff point. As such, we can estimate a causal effect by comparing similar subwords around this cutoff. Experimentally, we find that tokenisation consistently affects models' outputs across scales, vocabularies, and tokenisers. Notably, a subword's presence in a small model's vocabulary may increase its characters' probability by up to 17 times, highlighting tokenisation as a key design choice in language modelling.",
      "arxiv_url": "https://arxiv.org/abs/2506.03149",
      "pdf_url": "https://arxiv.org/pdf/2506.03149",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5eccd7f448c33865e256e3b1f4a2c521ea988325",
      "title": "One-Dimensional Object Detection for Streaming Text Segmentation of Meeting Dialogue",
      "authors": [
        "Rui He",
        "Zhongqing Wang",
        "Minjie Qiang",
        "Hongling Wang",
        "Yifan. zhang Yifan. zhang",
        "Hua Xu",
        "Shuai Fan",
        "Guodong Zhou"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/5eccd7f448c33865e256e3b1f4a2c521ea988325",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5ef7cc3f6df6d606ee589d4ec6cc138f326e1bdb",
      "title": "Sentimental Image Generation for Aspect-based Sentiment Analysis",
      "authors": [
        "Xiaoyi Bao",
        "Jinghang Gu",
        "Zhongqing Wang",
        "Chu-Ren Huang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/5ef7cc3f6df6d606ee589d4ec6cc138f326e1bdb",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00817",
      "title": "One for All: Update Parameterized Knowledge Across Multiple Models",
      "authors": [
        "Weitao Ma",
        "Xiyuan Du",
        "Xiaocheng Feng",
        "Lei Huang",
        "Yi-Chong Huang",
        "Huiyi Zhang",
        "Xiaoliang Yang",
        "Baohang Li",
        "Xiachong Feng",
        "Ting Liu",
        "Bing Qin"
      ],
      "abstract": "Large language models (LLMs) encode vast world knowledge but struggle to stay up-to-date, often leading to errors and hallucinations. Knowledge editing offers an efficient alternative to retraining, enabling targeted modifications by updating specific model parameters. However, existing methods primarily focus on individual models, posing challenges in efficiently updating multiple models and adapting to new models. To address this, we propose OnceEdit, a novel ensemble-based approach that employs a plug-in model as the editing module, enabling stable knowledge updates across multiple models. Building on the model ensemble, OnceEdit introduces two key mechanisms to enhance its effectiveness. First, we introduce a dynamic weight mechanism through a \\weight token for distinguishing between edit-related and non-edit-related instances, ensuring the appropriate utilization of knowledge from integrated models. Second, we incorporate an ensemble enhancement mechanism to mitigate the excessive reliance on the central model inherent in the model ensemble technique, making it more suitable for knowledge editing. Extensive experiments on diverse LLMs demonstrate that OnceEdit consistently outperforms existing methods while achieving superior editing efficiency. Further analysis confirms its adaptability and stability in multi-model editing scenarios. Our code will be available.",
      "arxiv_url": "https://arxiv.org/abs/2506.00817",
      "pdf_url": "https://arxiv.org/pdf/2506.00817",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.12559",
      "title": "AdaReTaKe: Adaptive Redundancy Reduction to Perceive Longer for Video-language Understanding",
      "authors": [
        "Xiao Wang",
        "Qingyi Si",
        "Jianlong Wu",
        "Shiyu Zhu",
        "Li Cao",
        "Liqiang Nie"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have revolutionized video understanding, yet are still limited by context length when processing long videos. Recent methods compress videos by leveraging visual redundancy uniformly, yielding promising results. Nevertheless, our quantitative analysis shows that redundancy varies significantly across time and model layers, necessitating a more flexible compression strategy. We propose AdaReTaKe, a training-free method that flexibly reduces visual redundancy by allocating compression ratios among time and layers with theoretical guarantees. Integrated into state-of-the-art MLLMs, AdaReTaKe improves processing capacity from 256 to 2048 frames while preserving critical information. Experiments on VideoMME, MLVU, LongVideoBench, and LVBench datasets demonstrate that AdaReTaKe outperforms existing methods by 2.3% and 2.8% for 7B and 72B models, respectively, with even greater improvements of 5.9% and 6.0% on the longest LVBench. Our code is available at https://github.com/SCZwangxiao/video-FlexReduc.git.",
      "arxiv_url": "https://arxiv.org/abs/2503.12559",
      "pdf_url": "https://arxiv.org/pdf/2503.12559",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12734",
      "title": "Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training",
      "authors": [
        "Yuanfan Li",
        "Zhaohan Zhang",
        "Chengzhengxu Li",
        "Chao Shen",
        "Xiaoming Liu"
      ],
      "abstract": "Machine-generated Text (MGT) detection is crucial for regulating and attributing online texts. While the existing MGT detectors achieve strong performance, they remain vulnerable to simple perturbations and adversarial attacks. To build an effective defense against malicious perturbations, we view MGT detection from a threat modeling perspective, that is, analyzing the model's vulnerability from an adversary's point of view and exploring effective mitigations. To this end, we introduce an adversarial framework for training a robust MGT detector, named GREedy Adversary PromoTed DefendER (GREATER). The GREATER consists of two key components: an adversary GREATER-A and a detector GREATER-D. The GREATER-D learns to defend against the adversarial attack from GREATER-A and generalizes the defense to other attacks. GREATER-A identifies and perturbs the critical tokens in embedding space, along with greedy search and pruning to generate stealthy and disruptive adversarial examples. Besides, we update the GREATER-A and GREATER-D synchronously, encouraging the GREATER-D to generalize its defense to different attacks and varying attack intensities. Our experimental results across 10 text perturbation strategies and 6 adversarial attacks show that our GREATER-D reduces the Attack Success Rate (ASR) by 0.67% compared with SOTA defense methods while our GREATER-A is demonstrated to be more effective and efficient than SOTA attack approaches. Codes and dataset are available in https://github.com/Liyuuuu111/GREATER.",
      "arxiv_url": "https://arxiv.org/abs/2502.12734",
      "pdf_url": "https://arxiv.org/pdf/2502.12734",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.21182",
      "title": "SDD: Self-Degraded Defense against Malicious Fine-tuning",
      "authors": [
        "ZiXuan Chen",
        "Weikai Lu",
        "Xin Lin",
        "Ziqian Zeng"
      ],
      "abstract": "Open-source Large Language Models (LLMs) often employ safety alignment methods to resist harmful instructions. However, recent research shows that maliciously fine-tuning these LLMs on harmful data can easily bypass these safeguards. To counter this, we theoretically uncover why malicious fine-tuning succeeds and identify potential defense strategies. Building on the theoretical analysis, we introduce the Self-Degraded Defense (SDD) framework. SDD encourages LLMs to produce high-quality but irrelevant responses to harmful prompts. When attackers attempt malicious fine-tuning, the general capability of the LLM aligned by SDD will significantly decrease, rendering it incapable of following harmful instructions. Our experimental results confirm SDD's effectiveness against such attacks.",
      "arxiv_url": "https://arxiv.org/abs/2507.21182",
      "pdf_url": "https://arxiv.org/pdf/2507.21182",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.05261",
      "title": "TokenShapley: Token Level Context Attribution with Shapley Value",
      "authors": [
        "Yingtai Xiao",
        "Yuqing Zhu",
        "Sirat Samyoun",
        "Wanrong Zhang",
        "Jiachen T. Wang",
        "Jian Du"
      ],
      "abstract": "Large language models (LLMs) demonstrate strong capabilities in in-context learning, but verifying the correctness of their generated responses remains a challenge. Prior work has explored attribution at the sentence level, but these methods fall short when users seek attribution for specific keywords within the response, such as numbers, years, or names. To address this limitation, we propose TokenShapley, a novel token-level attribution method that combines Shapley value-based data attribution with KNN-based retrieval techniques inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed datastore for contextual retrieval and computing Shapley values to quantify token importance, TokenShapley provides a fine-grained data attribution approach. Extensive evaluations on four benchmarks show that TokenShapley outperforms state-of-the-art baselines in token-level attribution, achieving an 11-23% improvement in accuracy.",
      "arxiv_url": "https://arxiv.org/abs/2507.05261",
      "pdf_url": "https://arxiv.org/pdf/2507.05261",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02959",
      "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring",
      "authors": [
        "Zhixiong Su",
        "Yichen Wang",
        "Herun Wan",
        "Zhaohan Zhang",
        "Minnan Luo"
      ],
      "abstract": "The misuse of large language models (LLMs) poses potential risks, motivating the development of machine-generated text (MGT) detection. Existing literature primarily concentrates on binary, document-level detection, thereby neglecting texts that are composed jointly by human and LLM contributions. Hence, this paper explores the possibility of fine-grained MGT detection under human-AI coauthoring. We suggest fine-grained detectors can pave pathways toward coauthored text detection with a numeric AI ratio. Specifically, we propose a dataset, HACo-Det, which produces human-AI coauthored texts via an automatic pipeline with word-level attribution labels. We retrofit seven prevailing document-level detectors to generalize them to word-level detection. Then we evaluate these detectors on HACo-Det on both word- and sentence-level detection tasks. Empirical results show that metric-based methods struggle to conduct fine-grained detection with a 0.462 average F1 score, while finetuned models show superior performance and better generalization across domains. However, we argue that fine-grained co-authored text detection is far from solved. We further analyze factors influencing performance, e.g., context window, and highlight the limitations of current methods, pointing to potential avenues for improvement.",
      "arxiv_url": "https://arxiv.org/abs/2506.02959",
      "pdf_url": "https://arxiv.org/pdf/2506.02959",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "5f62a9ec5a73496cab02dfde78451055205d9d65",
      "title": "Lexical Recall or Logical Reasoning: Probing the Limits of Reasoning Abilities in Large Language Models",
      "authors": [
        "Henrike Beyer",
        "Chris Reed"
      ],
      "abstract": "Despite the increasing interest in the reasoning abilities of Large Language Models (LLMs), existing work shows limitations in assessing logic abilities independently from lexical memory. We address this gap with Mystery-Zebra. This robust two-part benchmark (4,290 puzzles) challenges the logic abstraction abilities of LLMs in two setups: (1) a lexical obfus-cation setup tests the dependence of LLMs on lexical content based on two canonical grid puzzles widely spread on the Internet; (2) a set of new grid puzzles in 42 different sizes and 12 difficulty levels tests how the formal difficulty degree of a puzzle affects LLMs. We test open and closed-weight LLMs on both parts of the benchmark. The results on part two suggest that model sizes up to 70B parameters have only a minor influence when solving newly generated puzzles, while performance mainly relates to the number of items in the puzzle. The results on the first part of the benchmark suggest that the applied obfuscation strategies help to mitigate effects of logic puzzles being part of LLM training data, showing a drastic drop in performance for obfuscated versions of well-known puzzles. In addition we conduct a case-study on the first part of the benchmark predicting the position of single items, unveiling that the reasoning abilities of LLMs are mainly limited to a few consecutive steps of reasoning. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/5f62a9ec5a73496cab02dfde78451055205d9d65",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01048",
      "title": "IRT-Router: Effective and Interpretable Multi-LLM Routing via Item Response Theory",
      "authors": [
        "Wei Song",
        "Zhenya Huang",
        "Cheng Cheng",
        "Weibo Gao",
        "Bihan Xu",
        "Guanhao Zhao",
        "Fei Wang",
        "Runze Wu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated exceptional performance across a wide range of natural language tasks. However, selecting the optimal LLM to respond to a user query often necessitates a delicate balance between performance and cost. While powerful models deliver better results, they come at a high cost, whereas smaller models are more cost-effective but less capable. To address this trade-off, we propose IRT-Router, a multi-LLM routing framework that efficiently routes user queries to the most suitable LLM. Inspired by Item Response Theory (IRT), a psychological measurement methodology, IRT-Router explicitly models the relationship between LLM capabilities and user query attributes. This not only enables accurate prediction of response performance but also provides interpretable insights, such as LLM abilities and query difficulty. Additionally, we design an online query warm-up technique based on semantic similarity, further enhancing the online generalization capability of IRT-Router. Extensive experiments on 20 LLMs and 12 datasets demonstrate that IRT-Router outperforms most baseline methods in terms of effectiveness and interpretability. Its superior performance in cold-start scenarios further confirms the reliability and practicality of IRT-Router in real-world applications. Code is available at https://github.com/Mercidaiha/IRT-Router.",
      "arxiv_url": "https://arxiv.org/abs/2506.01048",
      "pdf_url": "https://arxiv.org/pdf/2506.01048",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.16344",
      "title": "WhiSPA: Semantically and Psychologically Aligned Whisper with Self-Supervised Contrastive and Student-Teacher Learning",
      "authors": [
        "Rajath Rao",
        "Adithya V Ganesan",
        "O. Kjell",
        "Jonah Luby",
        "Akshay Raghavan",
        "Scott Feltman",
        "Whitney R. Ringwald",
        "Ryan L. Boyd",
        "Benjamin J. Luft",
        "C. Ruggero",
        "Neville Ryant",
        "Roman Kotov",
        "H. A. Schwartz"
      ],
      "abstract": "Current speech encoding pipelines often rely on an additional text-based LM to get robust representations of human communication, even though SotA speech-to-text models often have a LM within. This work proposes an approach to improve the LM within an audio model such that the subsequent text-LM is unnecessary. We introduce WhiSPA (Whisper with Semantic and Psychological Alignment), which leverages a novel audio training objective: contrastive loss with a language model embedding as a teacher. Using over 500k speech segments from mental health audio interviews, we evaluate the utility of aligning Whisper's latent space with semantic representations from a text autoencoder (SBERT) and lexically derived embeddings of basic psychological dimensions: emotion and personality. Over self-supervised affective tasks and downstream psychological tasks, WhiSPA surpasses current speech encoders, achieving an average error reduction of 73.4% and 83.8%, respectively. WhiSPA demonstrates that it is not always necessary to run a subsequent text LM on speech-to-text output in order to get a rich psychological representation of human communication.",
      "arxiv_url": "https://arxiv.org/abs/2501.16344",
      "pdf_url": "https://arxiv.org/pdf/2501.16344",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20129",
      "title": "Finite State Automata Inside Transformers with Chain-of-Thought: A Mechanistic Study on State Tracking",
      "authors": [
        "Yifan Zhang",
        "Wenyu Du",
        "Dongming Jin",
        "Jie Fu",
        "Zhi Jin"
      ],
      "abstract": "Chain-of-thought (CoT) significantly enhances the performance of large language models (LLMs) across a wide range of tasks, and prior research shows that CoT can theoretically increase expressiveness. However, there is limited mechanistic understanding of the algorithms that Transformer+CoT can learn. Our key contributions are: (1) We evaluate the state tracking capabilities of Transformer+CoT and its variants, confirming the effectiveness of CoT. (2) Next, we identify the circuit (a subset of model components, responsible for tracking the world state), indicating that late-layer MLP neurons play a key role. We propose two metrics, compression and distinction, and show that the neuron sets for each state achieve nearly 100% accuracy, providing evidence of an implicit finite state automaton (FSA) embedded within the model. (3) Additionally, we explore three challenging settings: skipping intermediate steps, introducing data noises, and testing length generalization. Our results demonstrate that Transformer+CoT learns robust algorithms (FSAs), highlighting its resilience in challenging scenarios. Our code is available at https://github.com/IvanChangPKU/FSA.",
      "arxiv_url": "https://arxiv.org/abs/2502.20129",
      "pdf_url": "https://arxiv.org/pdf/2502.20129",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22018",
      "title": "Improving Continual Pre-training Through Seamless Data Packing",
      "authors": [
        "Ruicheng Yin",
        "Xuan Gao",
        "Changze Lv",
        "Xiaohua Wang",
        "Xiaoqing Zheng",
        "Xuanjing Huang"
      ],
      "abstract": "Continual pre-training has demonstrated significant potential in enhancing model performance, particularly in domain-specific scenarios. The most common approach for packing data before continual pre-training involves concatenating input texts and splitting them into fixed-length sequences. While straightforward and efficient, this method often leads to excessive truncation and context discontinuity, which can hinder model performance. To address these issues, we explore the potential of data engineering to enhance continual pre-training, particularly its impact on model performance and efficiency. We propose Seamless Packing (SP), a novel data packing strategy aimed at preserving contextual information more effectively and enhancing model performance. Our approach employs a sliding window technique in the first stage that synchronizes overlapping tokens across consecutive sequences, ensuring better continuity and contextual coherence. In the second stage, we adopt a First-Fit-Decreasing algorithm to pack shorter texts into bins slightly larger than the target sequence length, thereby minimizing padding and truncation. Empirical evaluations across various model architectures and corpus domains demonstrate the effectiveness of our method, outperforming baseline method in 99% of all settings. Code is available at https://github.com/Infernus-WIND/Seamless-Packing.",
      "arxiv_url": "https://arxiv.org/abs/2505.22018",
      "pdf_url": "https://arxiv.org/pdf/2505.22018",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.02625",
      "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis",
      "authors": [
        "Qingkai Fang",
        "Yan Zhou",
        "Shoutao Guo",
        "Shaolei Zhang",
        "Yang Feng"
      ],
      "abstract": "Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data.",
      "arxiv_url": "https://arxiv.org/abs/2505.02625",
      "pdf_url": "https://arxiv.org/pdf/2505.02625",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06657",
      "title": "Quantile Regression with Large Language Models for Price Prediction",
      "authors": [
        "Nikhita Vedula",
        "Dushyanta Dhyani",
        "Laleh Jalali",
        "Boris N. Oreshkin",
        "Mohsen Bayati",
        "S. Malmasi"
      ],
      "abstract": "Large Language Models (LLMs) have shown promise in structured prediction tasks, including regression, but existing approaches primarily focus on point estimates and lack systematic comparison across different methods. We investigate probabilistic regression using LLMs for unstructured inputs, addressing challenging text-to-distribution prediction tasks such as price estimation where both nuanced text understanding and uncertainty quantification are critical. We propose a novel quantile regression approach that enables LLMs to produce full predictive distributions, improving upon traditional point estimates. Through extensive experiments across three diverse price prediction datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads significantly outperforms traditional approaches for both point and distributional estimations, as measured by three established metrics each for prediction accuracy and distributional calibration. Our systematic comparison of LLM approaches, model architectures, training approaches, and data scaling reveals that Mistral-7B consistently outperforms encoder architectures, embedding-based methods, and few-shot learning methods. Our experiments also reveal the effectiveness of LLM-assisted label correction in achieving human-level accuracy without systematic bias. Our curated datasets are made available at https://github.com/vnik18/llm-price-quantile-reg/ to support future research.",
      "arxiv_url": "https://arxiv.org/abs/2506.06657",
      "pdf_url": "https://arxiv.org/pdf/2506.06657",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07510",
      "title": "DeRAGEC: Denoising Named Entity Candidates with Synthetic Rationale for ASR Error Correction",
      "authors": [
        "Solee Im",
        "Wonjun Lee",
        "Jinmyeong An",
        "Yunsu Kim",
        "Jungseul Ok",
        "Gary Geunbae Lee"
      ],
      "abstract": "We present DeRAGEC, a method for improving Named Entity (NE) correction in Automatic Speech Recognition (ASR) systems. By extending the Retrieval-Augmented Generative Error Correction (RAGEC) framework, DeRAGEC employs synthetic denoising rationales to filter out noisy NE candidates before correction. By leveraging phonetic similarity and augmented definitions, it refines noisy retrieved NEs using in-context learning, requiring no additional training. Experimental results on CommonVoice and STOP datasets show significant improvements in Word Error Rate (WER) and NE hit ratio, outperforming baseline ASR and RAGEC methods. Specifically, we achieved a 28% relative reduction in WER compared to ASR without postprocessing. Our source code is publicly available at: https://github.com/solee0022/deragec",
      "arxiv_url": "https://arxiv.org/abs/2506.07510",
      "pdf_url": "https://arxiv.org/pdf/2506.07510",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.06706",
      "title": "PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on UML Flowcharts",
      "authors": [
        "Ming Zhang",
        "Yuhui Wang",
        "Yujiong Shen",
        "Tingyi Yang",
        "Changhao Jiang",
        "Yilong Wu",
        "Shihan Dou",
        "Qinhao Chen",
        "Zhiheng Xi",
        "Zhihao Zhang",
        "Yi Dong",
        "Zhen Wang",
        "Zhihui Fei",
        "Mingyang Wan",
        "Tao Liang",
        "Guojun Ma",
        "Qi Zhang",
        "Tao Gui",
        "Xuanjing Huang"
      ],
      "abstract": "Process-driven dialogue systems, which operate under strict predefined process constraints, are essential in customer service and equipment maintenance scenarios. Although Large Language Models (LLMs) have shown remarkable progress in dialogue and reasoning, they still struggle to solve these strictly constrained dialogue tasks. To address this challenge, we construct Process Flow Dialogue (PFDial) dataset, which contains 12,705 high-quality Chinese dialogue instructions derived from 440 flowcharts containing 5,055 process nodes. Based on PlantUML specification, each UML flowchart is converted into atomic dialogue units i.e., structured five-tuples. Experimental results demonstrate that a 7B model trained with merely 800 samples, and a 0.5B model trained on total data both can surpass 90% accuracy. Additionally, the 8B model can surpass GPT-4o up to 43.88% with an average of 11.00%. We further evaluate models' performance on challenging backward transitions in process flows and conduct an in-depth analysis of various dataset formats to reveal their impact on model performance in handling decision and sequential branches. The data is released in https://github.com/KongLongGeFDU/PFDial.",
      "arxiv_url": "https://arxiv.org/abs/2503.06706",
      "pdf_url": "https://arxiv.org/pdf/2503.06706",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "60b772a9fa659c8cb5ebc28df9243323501e73c8",
      "title": "Consistency-Aware Online Multi-Objective Alignment for Related Search Query Generation",
      "authors": [
        "Shuxian Bi",
        "Chongming Gao",
        "Wenjie Wang",
        "Yueqi Mou",
        "Chenxu Wang",
        "Tang Biao",
        "Peng Yan",
        "Fuli Feng"
      ],
      "abstract": "Modern digital platforms rely on related search query recommendations to enhance engagement, yet existing methods fail to reconcile click-through rate (CTR) optimization with topic expansion. We propose CMAQ , a C onsistent M ulti-Objective A ligned Q uery generation framework that harmonizes these goals through three components: (1) reward modeling to quantify objectives, (2) style alignment for format compliance, and (3) consistency-aware optimization to coordinate joint improvements. CMAQ employs adaptive β -scaled DPO with geometric mean rewards, balancing CTR and expansion while mitigating objective conflicts. Extensive offline and online evaluations in a large-scale industrial setting demonstrate CMAQ’s superiority, achieving significant CTR gains (+2.3%) and higher human-rated query quality compared to state-of-the-art methods. Our approach enables high-quality query generation while sustaining user engagement and platform ecosystem health.",
      "arxiv_url": "https://www.semanticscholar.org/paper/60b772a9fa659c8cb5ebc28df9243323501e73c8",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "60bd91b3b8b768cb158956135fad7a5869b93e08",
      "title": "DECAF: A Dynamically Extensible Corpus Analysis Framework",
      "authors": [
        "Max Müller-Eberstein",
        "Rob van der Goot",
        "Anna Rogers"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/60bd91b3b8b768cb158956135fad7a5869b93e08",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.08569",
      "title": "DeepReview: Improving LLM-based Paper Review with Human-like Deep Thinking Process",
      "authors": [
        "Minjun Zhu",
        "Yixuan Weng",
        "Linyi Yang",
        "Yue Zhang"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly utilized in scientific research assessment, particularly in automated paper review. However, existing LLM-based review systems face significant challenges, including limited domain expertise, hallucinated reasoning, and a lack of structured evaluation. To address these limitations, we introduce DeepReview, a multi-stage framework designed to emulate expert reviewers by incorporating structured analysis, literature retrieval, and evidence-based argumentation. Using DeepReview-13K, a curated dataset with structured annotations, we train DeepReviewer-14B, which outperforms CycleReviewer-70B with fewer tokens. In its best mode, DeepReviewer-14B achieves win rates of 88.21\\% and 80.20\\% against GPT-o1 and DeepSeek-R1 in evaluations. Our work sets a new benchmark for LLM-based paper review, with all resources publicly available. The code, model, dataset and demo have be released in http://ai-researcher.net.",
      "arxiv_url": "https://arxiv.org/abs/2503.08569",
      "pdf_url": "https://arxiv.org/pdf/2503.08569",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12388",
      "title": "Group then Scale: Dynamic Mixture-of-Experts Multilingual Language Model",
      "authors": [
        "Chong Li",
        "Yingzhuo Deng",
        "Jiajun Zhang",
        "Chengqing Zong"
      ],
      "abstract": "The curse of multilinguality phenomenon is a fundamental problem of multilingual Large Language Models (LLMs), where the competition between massive languages results in inferior performance. It mainly comes from limited capacity and negative transfer between dissimilar languages. To address this issue, we propose a method to dynamically group and scale up the parameters of multilingual LLM while boosting positive transfer among similar languages. Specifically, the model is first tuned on monolingual corpus to determine the parameter deviation in each layer and quantify the similarity between languages. Layers with more deviations are extended to mixture-of-experts layers to reduce competition between languages, where one expert module serves one group of similar languages. Experimental results on 18 to 128 languages show that our method reduces the negative transfer between languages and significantly boosts multilingual performance with fewer parameters. Such language group specialization on experts benefits the new language adaptation and reduces the inference on the previous multilingual knowledge learned.",
      "arxiv_url": "https://arxiv.org/abs/2506.12388",
      "pdf_url": "https://arxiv.org/pdf/2506.12388",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11436",
      "title": "GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art",
      "authors": [
        "Yiming Lei",
        "Chenkai Zhang",
        "Zeming Liu",
        "Haitao Leng",
        "Shaoguo Liu",
        "Tingting Gao",
        "Qingjie Liu",
        "Yunhong Wang"
      ],
      "abstract": "Video Comment Art enhances user engagement by providing creative content that conveys humor, satire, or emotional resonance, requiring a nuanced and comprehensive grasp of cultural and contextual subtleties. Although Multimodal Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they still struggle to generate creative expressions such as resonant jokes and insightful satire. Moreover, existing benchmarks are constrained by their limited modalities and insufficient categories, hindering the exploration of comprehensive creativity in video-based Comment Art creation. To address these limitations, we introduce GODBench, a novel benchmark that integrates video and text modalities to systematically evaluate MLLMs' abilities to compose Comment Art. Furthermore, inspired by the propagation patterns of waves in physics, we propose Ripple of Thought (RoT), a multi-step reasoning framework designed to enhance the creativity of MLLMs. Extensive experiments reveal that existing MLLMs and CoT methods still face significant challenges in understanding and generating creative video comments. In contrast, RoT provides an effective approach to improve creative composing, highlighting its potential to drive meaningful advancements in MLLM-based creativity. GODBench is publicly available at https://github.com/stan-lei/GODBench-ACL2025.",
      "arxiv_url": "https://arxiv.org/abs/2505.11436",
      "pdf_url": "https://arxiv.org/pdf/2505.11436",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.05258",
      "title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models",
      "authors": [
        "A. Bazaga",
        "Rexhina Blloshmi",
        "Bill Byrne",
        "A. D. Gispert"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as powerful tools for generating coherent text, understanding context, and performing reasoning tasks. However, they struggle with temporal reasoning, which requires processing time-related information such as event sequencing, durations, and inter-temporal relationships. These capabilities are critical for applications including question answering, scheduling, and historical analysis. In this paper, we introduce TISER, a novel framework that enhances the temporal reasoning abilities of LLMs through a multi-stage process that combines timeline construction with iterative self-reflection. Our approach leverages test-time scaling to extend the length of reasoning traces, enabling models to capture complex temporal dependencies more effectively. This strategy not only boosts reasoning accuracy but also improves the traceability of the inference process. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, including out-of-distribution test sets, and reveal that TISER enables smaller open-source models to surpass larger closed-weight models on challenging temporal reasoning tasks.",
      "arxiv_url": "https://arxiv.org/abs/2504.05258",
      "pdf_url": "https://arxiv.org/pdf/2504.05258",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.03040",
      "title": "ChronoSense: Exploring Temporal Understanding in Large Language Models with Time Intervals of Events",
      "authors": [
        "Duygu Sezen Islakoglu",
        "Jan-Christoph Kalo"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable success in various NLP tasks, yet they still face significant challenges in reasoning and arithmetic. Temporal reasoning, a critical component of natural language understanding, has raised increasing research attention. However, comprehensive testing of Allen's interval relations (e.g., before, after, during) -- a fundamental framework for temporal relationships -- remains underexplored. To fill this gap, we present ChronoSense, a new benchmark for evaluating LLMs'temporal understanding. It includes 16 tasks, focusing on identifying the Allen relation between two temporal events and temporal arithmetic, using both abstract events and real-world data from Wikidata. We assess the performance of seven recent LLMs using this benchmark and the results indicate that models handle Allen relations, even symmetrical ones, quite differently. Moreover, the findings suggest that the models may rely on memorization to answer time-related questions. Overall, the models'low performance highlights the need for improved temporal understanding in LLMs and ChronoSense offers a robust framework for future research in this area. Our dataset and the source code are available at https://github.com/duyguislakoglu/chronosense.",
      "arxiv_url": "https://arxiv.org/abs/2501.03040",
      "pdf_url": "https://arxiv.org/pdf/2501.03040",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05073",
      "title": "Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection through Intent Differentiation and Emoji Interpretation",
      "authors": [
        "Soumitra Ghosh",
        "G. Singh",
        "Shambhavi",
        "Sabarna Choudhury",
        "Asif Ekbal"
      ],
      "abstract": "Self-harm detection on social media is critical for early intervention and mental health support, yet remains challenging due to the subtle, context-dependent nature of such expressions. Identifying self-harm intent aids suicide prevention by enabling timely responses, but current large language models (LLMs) struggle to interpret implicit cues in casual language and emojis. This work enhances LLMs' comprehension of self-harm by distinguishing intent through nuanced language-emoji interplay. We present the Centennial Emoji Sensitivity Matrix (CESM-100), a curated set of 100 emojis with contextual self-harm interpretations and the Self-Harm Identification aNd intent Extraction with Supportive emoji sensitivity (SHINES) dataset, offering detailed annotations for self-harm labels, casual mentions (CMs), and serious intents (SIs). Our unified framework: a) enriches inputs using CESM-100; b) fine-tunes LLMs for multi-task learning: self-harm detection (primary) and CM/SI span detection (auxiliary); c) generates explainable rationales for self-harm predictions. We evaluate the framework on three state-of-the-art LLMs-Llama 3, Mental-Alpaca, and MentalLlama, across zero-shot, few-shot, and fine-tuned scenarios. By coupling intent differentiation with contextual cues, our approach commendably enhances LLM performance in both detection and explanation tasks, effectively addressing the inherent ambiguity in self-harm signals. The SHINES dataset, CESM-100 and codebase are publicly available at: https://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES .",
      "arxiv_url": "https://arxiv.org/abs/2506.05073",
      "pdf_url": "https://arxiv.org/pdf/2506.05073",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11104",
      "title": "Enhancing Cross-Tokenizer Knowledge Distillation with Contextual Dynamical Mapping",
      "authors": [
        "Yijie Chen",
        "Yijin Liu",
        "Fandong Meng",
        "Yufeng Chen",
        "Jinan Xu",
        "Jie Zhou"
      ],
      "abstract": "Knowledge Distillation (KD) has emerged as a prominent technique for model compression. However, conventional KD approaches primarily focus on homogeneous architectures with identical tokenizers, constraining their applicability in cross-architecture scenarios. As for the cross-tokenizer KD, the differences in the tokenizers give rise to two fundamental challenges: (1) sequence misalignment caused by divergent tokenization strategies, and (2) mismatched vocabulary size and composition. While existing probability-matching methods attempt to address these issues, their efficacy remains limited due to suboptimal alignment in both the sequence and vocabulary aspects. To overcome these limitations, we propose Contextual Dynamic Mapping (CDM), a novel cross-tokenizer distillation framework that employs contextual information to enhance sequence alignment precision and dynamically improves vocabulary mapping. We evaluated the effectiveness of our approach across five advanced and widely-used model families (i.e, LLama3, Phi3, Gemma2, OPT and Qwen2), which were configured into three distinct teacher-student pairs. Our method shows significant advantages over existing cross-tokenizer distillation baselines across diverse benchmarks, including instruction-following, code generation and math. Notably, our analysis reveals that combining conventional same-tokenizer distillation and cross-tokenizer distillation through CDM yields further performance improvements. The code is available at https://github.com/pppa2019/ContexualDynamicMapping",
      "arxiv_url": "https://arxiv.org/abs/2502.11104",
      "pdf_url": "https://arxiv.org/pdf/2502.11104",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12404",
      "title": "WMT24++: Expanding the Language Coverage of WMT24 to 55 Languages & Dialects",
      "authors": [
        "Daniel Deutsch",
        "Eleftheria Briakou",
        "Isaac Caswell",
        "Mara Finkelstein",
        "Rebecca Galor",
        "Juraj Juraska",
        "Geza Kovacs",
        "Alison Lui",
        "Ricardo Rei",
        "Jason Riesa",
        "Shruti Rijhwani",
        "Parker Riley",
        "Elizabeth Salesky",
        "Firas Trabelsi",
        "Stephanie Winkler",
        "Biao Zhang",
        "Markus Freitag"
      ],
      "abstract": "As large language models (LLM) become more and more capable in languages other than English, it is important to collect benchmark datasets in order to evaluate their multilingual performance, including on tasks like machine translation (MT). In this work, we extend the WMT24 dataset to cover 55 languages by collecting new human-written references and post-edits for 46 new languages and dialects in addition to post-edits of the references in 8 out of 9 languages in the original WMT24 dataset. The dataset covers four domains: literary, news, social, and speech. We benchmark a variety of MT providers and LLMs on the collected dataset using automatic metrics and find that LLMs are the best-performing MT systems in all 55 languages. These results should be confirmed using a human-based evaluation, which we leave for future work.",
      "arxiv_url": "https://arxiv.org/abs/2502.12404",
      "pdf_url": "https://arxiv.org/pdf/2502.12404",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.04945",
      "title": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models",
      "authors": [
        "Qingyu Ren",
        "Jie Zeng",
        "Qi He",
        "Jiaqing Liang",
        "Yanghua Xiao",
        "Weikang Zhou",
        "Zeye Sun",
        "Fei Yu"
      ],
      "abstract": "It is crucial for large language models (LLMs) to follow instructions that involve multiple constraints. However, it is an unexplored area to enhance LLMs' ability to follow soft constraints. To bridge the gap, we initially design a pipeline to construct datasets with high-quality outputs automatically. Additionally, to fully utilize the positive and negative samples generated during the data construction process, we choose Direct Preference Optimization (DPO) as the training method. Furthermore, taking into account the difficulty of soft constraints indicated by the number of constraints, we design a curriculum learning training paradigm based on the constraint quantity. We experimentally evaluate the effectiveness of our methods in improving LLMs' soft constraint following ability and analyze the factors driving the improvements.The datasets and code are publicly available at https://github.com/Rainier-rq/FollowSoftConstraint.",
      "arxiv_url": "https://arxiv.org/abs/2501.04945",
      "pdf_url": "https://arxiv.org/pdf/2501.04945",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15666",
      "title": "Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing",
      "authors": [
        "Shoumik Saha",
        "S. Feizi"
      ],
      "abstract": "The growing use of large language models (LLMs) for text generation has led to widespread concerns about AI-generated content detection. However, an overlooked challenge is AI-polished text, where human-written content undergoes subtle refinements using AI tools. This raises a critical question: should minimally polished text be classified as AI-generated? Such classification can lead to false plagiarism accusations and misleading claims about AI prevalence in online content. In this study, we systematically evaluate twelve state-of-the-art AI-text detectors using our AI-Polished-Text Evaluation (APT-Eval) dataset, which contains 14.7K samples refined at varying AI-involvement levels. Our findings reveal that detectors frequently flag even minimally polished text as AI-generated, struggle to differentiate between degrees of AI involvement, and exhibit biases against older and smaller models. These limitations highlight the urgent need for more nuanced detection methodologies.",
      "arxiv_url": "https://arxiv.org/abs/2502.15666",
      "pdf_url": "https://arxiv.org/pdf/2502.15666",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20903",
      "title": "Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?",
      "authors": [
        "Ziming Wang",
        "Zeyu Shi",
        "Haoyi Zhou",
        "Shiqi Gao",
        "Qingyun Sun",
        "Jianxin Li"
      ],
      "abstract": "Fine-tuned Large Language Models (LLMs) often demonstrate poor calibration, with their confidence scores misaligned with actual performance. While calibration has been extensively studied in models trained from scratch, the impact of LLMs' prior knowledge on calibration during fine-tuning remains understudied. Our research reveals that LLMs' prior knowledge causes potential poor calibration due to the ubiquitous presence of known data in real-world fine-tuning, which appears harmful for calibration. Specifically, data aligned with LLMs' prior knowledge would induce overconfidence, while new knowledge improves calibration. Our findings expose a tension: LLMs' encyclopedic knowledge, while enabling task versatility, undermines calibration through unavoidable knowledge overlaps. To address this, we propose CogCalib, a cognition-aware framework that applies targeted learning strategies according to the model's prior knowledge. Experiments across 7 tasks using 3 LLM families prove that CogCalib significantly improves calibration while maintaining performance, achieving an average 57\\% reduction in ECE compared to standard fine-tuning in Llama3-8B. These improvements generalize well to out-of-domain tasks, enhancing the objectivity and reliability of domain-specific LLMs, and making them more trustworthy for critical human-AI interaction applications.",
      "arxiv_url": "https://arxiv.org/abs/2505.20903",
      "pdf_url": "https://arxiv.org/pdf/2505.20903",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04848",
      "title": "MockConf: A Student Interpretation Dataset: Analysis, Word- and Span-level Alignment and Baselines",
      "authors": [
        "D'avid Javorsk'y",
        "O. Bojar",
        "Franccois Yvon"
      ],
      "abstract": "In simultaneous interpreting, an interpreter renders a source speech into another language with a very short lag, much sooner than sentences are finished. In order to understand and later reproduce this dynamic and complex task automatically, we need dedicated datasets and tools for analysis, monitoring, and evaluation, such as parallel speech corpora, and tools for their automatic annotation. Existing parallel corpora of translated texts and associated alignment algorithms hardly fill this gap, as they fail to model long-range interactions between speech segments or specific types of divergences (e.g., shortening, simplification, functional generalization) between the original and interpreted speeches. In this work, we introduce MockConf, a student interpreting dataset that was collected from Mock Conferences run as part of the students' curriculum. This dataset contains 7 hours of recordings in 5 European languages, transcribed and aligned at the level of spans and words. We further implement and release InterAlign, a modern web-based annotation tool for parallel word and span annotations on long inputs, suitable for aligning simultaneous interpreting. We propose metrics for the evaluation and a baseline for automatic alignment. Dataset and tools are released to the community.",
      "arxiv_url": "https://arxiv.org/abs/2506.04848",
      "pdf_url": "https://arxiv.org/pdf/2506.04848",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12763",
      "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization",
      "authors": [
        "Sunghwan Kim",
        "Dongjin Kang",
        "Taeyoon Kwon",
        "Hyungjoo Chae",
        "Dongha Lee",
        "Jinyoung Yeo"
      ],
      "abstract": "Reward models (RMs) play a crucial role in reinforcement learning from human feedback (RLHF), aligning model behavior with human preferences. However, existing benchmarks for reward models show a weak correlation with the performance of optimized policies, suggesting that they fail to accurately assess the true capabilities of RMs. To bridge this gap, we explore several evaluation designs through the lens of reward overoptimization\\textemdash a phenomenon that captures both how well the reward model aligns with human preferences and the dynamics of the learning signal it provides to the policy. The results highlight three key findings on how to construct a reliable benchmark: (i) it is important to minimize differences between chosen and rejected responses beyond correctness, (ii) evaluating reward models requires multiple comparisons across a wide range of chosen and rejected responses, and (iii) given that reward models encounter responses with diverse representations, responses should be sourced from a variety of models. However, we also observe that a extremely high correlation with degree of overoptimization leads to comparatively lower correlation with certain downstream performance. Thus, when designing a benchmark, it is desirable to use the degree of overoptimization as a useful tool, rather than the end goal.",
      "arxiv_url": "https://arxiv.org/abs/2505.12763",
      "pdf_url": "https://arxiv.org/pdf/2505.12763",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06844",
      "title": "Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning on Evolving Base Models",
      "authors": [
        "Naibin Gu",
        "Peng Fu",
        "Xiyu Liu",
        "Ke Ma",
        "Zheng Lin",
        "Weiping Wang"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) has become a common method for fine-tuning large language models, where a base model can serve multiple users through PEFT module switching. To enhance user experience, base models require periodic updates. However, once updated, PEFT modules fine-tuned on previous versions often suffer substantial performance degradation on newer versions. Re-tuning these numerous modules to restore performance would incur significant computational costs. Through a comprehensive analysis of the changes that occur during base model updates, we uncover an interesting phenomenon: continual training primarily affects task-specific knowledge stored in Feed-Forward Networks (FFN), while having less impact on the task-specific pattern in the Attention mechanism. Based on these findings, we introduce Trans-PEFT, a novel approach that enhances the PEFT module by focusing on the task-specific pattern while reducing its dependence on certain knowledge in the base model. Further theoretical analysis supports our approach. Extensive experiments across 7 base models and 12 datasets demonstrate that Trans-PEFT trained modules can maintain performance on updated base models without re-tuning, significantly reducing maintenance overhead in real-world applications.",
      "arxiv_url": "https://arxiv.org/abs/2506.06844",
      "pdf_url": "https://arxiv.org/pdf/2506.06844",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.03612",
      "title": "Multi-Hop Reasoning for Question Answering with Hyperbolic Representations",
      "authors": [
        "Simon Welz",
        "Lucie Flek",
        "Akbar Karimi"
      ],
      "abstract": "Hyperbolic representations are effective in modeling knowledge graph data which is prevalently used to facilitate multi-hop reasoning. However, a rigorous and detailed comparison of the two spaces for this task is lacking. In this paper, through a simple integration of hyperbolic representations with an encoder-decoder model, we perform a controlled and comprehensive set of experiments to compare the capacity of hyperbolic space versus Euclidean space in multi-hop reasoning. Our results show that the former consistently outperforms the latter across a diverse set of datasets. In addition, through an ablation study, we show that a learnable curvature initialized with the delta hyperbolicity of the utilized data yields superior results to random initializations. Furthermore, our findings suggest that hyperbolic representations can be significantly more advantageous when the datasets exhibit a more hierarchical structure.",
      "arxiv_url": "https://arxiv.org/abs/2507.03612",
      "pdf_url": "https://arxiv.org/pdf/2507.03612",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "626f7a458f318b1eb54d49c7c323bc357b36ae6e",
      "title": "Do Multimodal Large Language Models Truly See What We Point At? Investigating Indexical, Iconic, and Symbolic Gesture Comprehension",
      "authors": [
        "Noriki Nishida",
        "K. Inoue",
        "Hideki Nakayama",
        "Mayumi Bono",
        "K. Takanashi"
      ],
      "abstract": "Understanding hand gestures is essential for human communication, yet it remains unclear how well multimodal large language models (MLLMs) comprehend them. In this paper, we examine MLLMs’ ability to interpret indexi-cal gestures, which require external referential grounding, in comparison to iconic gestures, which depict imagery, and symbolic gestures, which are conventionally defined. We hypothesize that MLLMs, lacking real-world referential understanding, will struggle significantly with indexical gestures. To test this, we manually annotated five gesture type labels to 925 gesture instances from the Miraikan SC Corpus and analyzed gesture descriptions generated by state-of-the-art MLLMs, including GPT-4o. Our findings reveal a consistent weakness across models in interpreting indexical gestures, suggesting that MLLMs rely heavily on linguistic priors or commonsense knowledge rather than grounding their interpretations in visual or contextual cues.",
      "arxiv_url": "https://www.semanticscholar.org/paper/626f7a458f318b1eb54d49c7c323bc357b36ae6e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11859",
      "title": "Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics",
      "authors": [
        "Wenrui Xu",
        "Dalin Lyu",
        "Weihang Wang",
        "J. Feng",
        "Chen Gao",
        "Yong Li"
      ],
      "abstract": "The Theory of Multiple Intelligences underscores the hierarchical nature of cognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer a psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual Language Models (VLMs): Spatial Perception, Spatial Relation, Spatial Orientation, Mental Rotation, and Spatial Visualization. Benchmarking 13 mainstream VLMs through nine validated psychometric experiments reveals significant gaps versus humans (average score 24.95 vs. 68.38), with three key findings: 1) VLMs mirror human hierarchies (strongest in 2D orientation, weakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller models such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading (30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought (0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from architectural constraints. Identified barriers include weak geometry encoding and missing dynamic simulation. By linking psychometric BSAs to VLM capabilities, we provide a diagnostic toolkit for spatial intelligence evaluation, methodological foundations for embodied AI development, and a cognitive science-informed roadmap for achieving human-like spatial intelligence.",
      "arxiv_url": "https://arxiv.org/abs/2502.11859",
      "pdf_url": "https://arxiv.org/pdf/2502.11859",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.10937",
      "title": "SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention",
      "authors": [
        "Chengshuai Zhao",
        "Zhen Tan",
        "Chau-Wai Wong",
        "Xinyan Zhao",
        "Tianlong Chen",
        "Huan Liu"
      ],
      "abstract": "Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively $\\underline{\\textbf{S}}$imulates $\\underline{\\textbf{C}}$ontent $\\underline{\\textbf{A}}$nalysis via $\\underline{\\textbf{L}}$arge language model (LLM) ag$\\underline{\\textbf{E}}$nts. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.",
      "arxiv_url": "https://arxiv.org/abs/2502.10937",
      "pdf_url": "https://arxiv.org/pdf/2502.10937",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24575",
      "title": "NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization",
      "authors": [
        "Hyuntak Kim",
        "Byung-Hak Kim"
      ],
      "abstract": "Summarizing long-form narratives--such as books, movies, and TV scripts--requires capturing intricate plotlines, character interactions, and thematic coherence, a task that remains challenging for existing LLMs. We introduce NexusSum, a multi-agent LLM framework for narrative summarization that processes long-form text through a structured, sequential pipeline--without requiring fine-tuning. Our approach introduces two key innovations: (1) Dialogue-to-Description Transformation: A narrative-specific preprocessing method that standardizes character dialogue and descriptive text into a unified format, improving coherence. (2) Hierarchical Multi-LLM Summarization: A structured summarization pipeline that optimizes chunk processing and controls output length for accurate, high-quality summaries. Our method establishes a new state-of-the-art in narrative summarization, achieving up to a 30.0% improvement in BERTScore (F1) across books, movies, and TV scripts. These results demonstrate the effectiveness of multi-agent LLMs in handling long-form content, offering a scalable approach for structured summarization in diverse storytelling domains.",
      "arxiv_url": "https://arxiv.org/abs/2505.24575",
      "pdf_url": "https://arxiv.org/pdf/2505.24575",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20317",
      "title": "Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases",
      "authors": [
        "Yongjia Lei",
        "Haoyu Han",
        "Ryan A. Rossi",
        "Franck Dernoncourt",
        "Nedim Lipka",
        "M. Halappanavar",
        "Jiliang Tang",
        "Yu Wang"
      ],
      "abstract": "Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR.",
      "arxiv_url": "https://arxiv.org/abs/2502.20317",
      "pdf_url": "https://arxiv.org/pdf/2502.20317",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11417",
      "title": "DiSCo: Device-Server Collaborative LLM-Based Text Streaming Services",
      "authors": [
        "Ting Sun",
        "Penghan Wang",
        "Fan Lai"
      ],
      "abstract": "The rapid rise of large language models (LLMs) in text streaming services has introduced significant cost and Quality of Experience (QoE) challenges in serving millions of daily requests, especially in meeting Time-To-First-Token (TTFT) and Time-Between-Token (TBT) requirements for real-time interactions. Our real-world measurements show that both server-based and on-device deployments struggle to meet diverse QoE demands: server deployments face high costs and last-hop issues (e.g., Internet latency and dynamics), while on-device LLM inference is constrained by resources. We introduce DiSCo, a device-server cooperative scheduler designed to optimize users' QoE by adaptively routing requests and migrating response generation between endpoints while maintaining cost constraints. DiSCo employs cost-aware scheduling, leveraging the predictable speed of on-device LLM inference with the flexible capacity of server-based inference to dispatch requests on the fly, while introducing a token-level migration mechanism to ensure consistent token delivery during migration. Evaluations on real-world workloads -- including commercial services like OpenAI GPT and DeepSeek, and open-source deployments such as LLaMA3 -- show that DiSCo can improve users' QoE by reducing tail TTFT (11-52\\%) and mean TTFT (6-78\\%) across different model-device configurations, while dramatically reducing serving costs by up to 84\\% through its migration mechanism while maintaining comparable QoE levels.",
      "arxiv_url": "https://arxiv.org/abs/2502.11417",
      "pdf_url": "https://arxiv.org/pdf/2502.11417",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.17399",
      "title": "MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs",
      "authors": [
        "Ved Sirdeshmukh",
        "Kaustubh Deshpande",
        "Johannes Mols",
        "Lifeng Jin",
        "E. Cardona",
        "Dean Lee",
        "Jeremy Kritz",
        "Willow E. Primack",
        "Summer Yue",
        "Chen Xing"
      ],
      "abstract": "We present MultiChallenge, a pioneering benchmark evaluating large language models (LLMs) on conducting multi-turn conversations with human users, a crucial yet underexamined capability for their applications. MultiChallenge identifies four categories of challenges in multi-turn conversations that are not only common and realistic among current human-LLM interactions, but are also challenging to all current frontier LLMs. All 4 challenges require accurate instruction-following, context allocation, and in-context reasoning at the same time. We also develop LLM as judge with instance-level rubrics to facilitate an automatic evaluation method with fair agreement with experienced human raters. Despite achieving near-perfect scores on existing multi-turn evaluation benchmarks, all frontier models have less than 50% accuracy on MultiChallenge, with the top-performing Claude 3.5 Sonnet (June 2024) achieving just a 41.4% average accuracy.",
      "arxiv_url": "https://arxiv.org/abs/2501.17399",
      "pdf_url": "https://arxiv.org/pdf/2501.17399",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.11381",
      "title": "A Variational Approach for Mitigating Entity Bias in Relation Extraction",
      "authors": [
        "Samuel Mensah",
        "Elena Kochkina",
        "Jabez Magomere",
        "J. Sain",
        "Simerjot Kaur",
        "Charese H. Smiley"
      ],
      "abstract": "Mitigating entity bias is a critical challenge in Relation Extraction (RE), where models often rely excessively on entities, resulting in poor generalization. This paper presents a novel approach to address this issue by adapting a Variational Information Bottleneck (VIB) framework. Our method compresses entity-specific information while preserving task-relevant features. It achieves state-of-the-art performance on relation extraction datasets across general, financial, and biomedical domains, in both indomain (original test sets) and out-of-domain (modified test sets with type-constrained entity replacements) settings. Our approach offers a robust, interpretable, and theoretically grounded methodology.",
      "arxiv_url": "https://arxiv.org/abs/2506.11381",
      "pdf_url": "https://arxiv.org/pdf/2506.11381",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "633e35a19d83af05a3182dc72ed22f89c13a56f8",
      "title": "Walk in Others' Shoes with a Single Glance: Human-Centric Visual Grounding with Top-View Perspective Transformation",
      "authors": [
        "Yuqi Bu",
        "Xin Wu",
        "Zirui Zhao",
        "Yi Cai",
        "David Hsu",
        "Qiong Liu"
      ],
      "abstract": "Visual perspective-taking, an ability to envision others’ perspectives from a single self-perspective, is vital in human-robot interactions. Thus, we introduce a human-centric visual grounding task and a dataset to evaluate this ability. Recent advances in vision-language models (VLMs) have shown potential for inferring others’ perspectives, yet are insensitive to information differences induced by slight perspective changes. To address this problem, we propose a top-view enhanced perspective transformation (TEP) method, which decomposes the transition from robot to human perspectives through an abstract top-view representation. It unifies perspectives and facilitates the capture of information differences from diverse perspectives. Experimental results show that TEP improves performance by up to 18%, exhibits perspective-taking abilities across various perspectives, and generalizes effectively to robotic and dynamic scenarios. ‡",
      "arxiv_url": "https://www.semanticscholar.org/paper/633e35a19d83af05a3182dc72ed22f89c13a56f8",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19121",
      "title": "Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models",
      "authors": [
        "Seunguk Yu",
        "Juhwan Choi",
        "Youngbin Kim"
      ],
      "abstract": "Despite the recent strides in large language models, studies have underscored the existence of social biases within these systems. In this paper, we delve into the validation and comparison of the ethical biases of LLMs concerning globally discussed and potentially sensitive topics, hypothesizing that these biases may arise from language-specific distinctions. Introducing the Multilingual Sensitive Questions&Answers Dataset (MSQAD), we collected news articles from Human Rights Watch covering 17 topics, and generated socially sensitive questions along with corresponding responses in multiple languages. We scrutinized the biases of these responses across languages and topics, employing two statistical hypothesis tests. The results showed that the null hypotheses were rejected in most cases, indicating biases arising from cross-language differences. It demonstrates that ethical biases in responses are widespread across various languages, and notably, these biases were prevalent even among different LLMs. By making the proposed MSQAD openly available, we aim to facilitate future research endeavors focused on examining cross-language biases in LLMs and their variant models.",
      "arxiv_url": "https://arxiv.org/abs/2505.19121",
      "pdf_url": "https://arxiv.org/pdf/2505.19121",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17957",
      "title": "On Synthetic Data Strategies for Domain-Specific Generative Retrieval",
      "authors": [
        "Haoyang Wen",
        "Jiang Guo",
        "Yi Zhang",
        "Jiarong Jiang",
        "Zhiguo Wang"
      ],
      "abstract": "This paper investigates synthetic data generation strategies in developing generative retrieval models for domain-specific corpora, thereby addressing the scalability challenges inherent in manually annotating in-domain queries. We study the data strategies for a two-stage training framework: in the first stage, which focuses on learning to decode document identifiers from queries, we investigate LLM-generated queries across multiple granularity (e.g. chunks, sentences) and domain-relevant search constraints that can better capture nuanced relevancy signals. In the second stage, which aims to refine document ranking through preference learning, we explore the strategies for mining hard negatives based on the initial model's predictions. Experiments on public datasets over diverse domains demonstrate the effectiveness of our synthetic data generation and hard negative sampling approach.",
      "arxiv_url": "https://arxiv.org/abs/2502.17957",
      "pdf_url": "https://arxiv.org/pdf/2502.17957",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13791",
      "title": "From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions",
      "authors": [
        "Nathanaël Carraz Rakotonirina",
        "Mohammed Hamdy",
        "Jon Ander Campos",
        "Lucas Weber",
        "A. Testoni",
        "Marzieh Fadaee",
        "Sandro Pezzelle",
        "Marco Del Tredici"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used in working environments for a wide range of tasks, excelling at solving individual problems in isolation. However, are they also able to effectively collaborate over long-term interactions? To investigate this, we introduce MemoryCode, a synthetic multi-session dataset designed to test LLMs' ability to track and execute simple coding instructions amid irrelevant information, simulating a realistic setting. While all the models we tested handle isolated instructions well, even the performance of state-of-the-art models like GPT-4o deteriorates when instructions are spread across sessions. Our analysis suggests this is due to their failure to retrieve and integrate information over long instruction chains. Our results highlight a fundamental limitation of current LLMs, restricting their ability to collaborate effectively in long interactions.",
      "arxiv_url": "https://arxiv.org/abs/2502.13791",
      "pdf_url": "https://arxiv.org/pdf/2502.13791",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "63bbe3fd66e109b130a08d6d417bce5eb9f0c0e9",
      "title": "Enhanced Data Synthesis for LLM through Reasoning Structures Generated by Hierarchical GFlowNet",
      "authors": [
        "Tianpeng Bu",
        "Minying Zhang",
        "Hongtao Duan",
        "Shurui Li",
        "Lulu Hu",
        "Yu Li"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/63bbe3fd66e109b130a08d6d417bce5eb9f0c0e9",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20679",
      "title": "SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations",
      "authors": [
        "Danush Khanna",
        "Pratinav Seth",
        "Sidhaarth Murali",
        "Aditya Kumar Guru",
        "Siddharth Shukla",
        "Tanuj Tyagi",
        "Sandeep Chaurasia",
        "Kripabandhu Ghosh"
      ],
      "abstract": "Mental manipulation is a subtle yet pervasive form of abuse in interpersonal communication, making its detection critical for safeguarding potential victims. However, due to manipulation's nuanced and context-specific nature, identifying manipulative language in complex, multi-turn, and multi-person conversations remains a significant challenge for large language models (LLMs). To address this gap, we introduce the MultiManip dataset, comprising 220 multi-turn, multi-person dialogues balanced between manipulative and non-manipulative interactions, all drawn from reality shows that mimic real-world scenarios. For manipulative interactions, it includes 11 distinct manipulations depicting real-life scenarios. We conduct extensive evaluations of state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various prompting strategies. Despite their capabilities, these models often struggle to detect manipulation effectively. To overcome this limitation, we propose SELF-PERCEPT, a novel, two-stage prompting framework inspired by Self-Perception Theory, demonstrating strong performance in detecting multi-person, multi-turn mental manipulation. Our code and data are publicly available at https://github.com/danushkhanna/self-percept .",
      "arxiv_url": "https://arxiv.org/abs/2505.20679",
      "pdf_url": "https://arxiv.org/pdf/2505.20679",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22076",
      "title": "ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation",
      "authors": [
        "Maja Stahl",
        "Timon Ziegenbein",
        "Joonsuk Park",
        "Henning Wachsmuth"
      ],
      "abstract": "Training large language models (LLMs) to follow instructions has significantly enhanced their ability to tackle unseen tasks. However, despite their strong generalization capabilities, instruction-following LLMs encounter difficulties when dealing with tasks that require domain knowledge. This work introduces a specialized instruction fine-tuning for the domain of computational argumentation (CA). The goal is to enable an LLM to effectively tackle any unseen CA tasks while preserving its generalization capabilities. Reviewing existing CA research, we crafted natural language instructions for 105 CA tasks to this end. On this basis, we developed a CA-specific benchmark for LLMs that allows for a comprehensive evaluation of LLMs' capabilities in solving various CA tasks. We synthesized 52k CA-related instructions, adapting the self-instruct process to train a CA-specialized instruction-following LLM. Our experiments suggest that CA-specialized instruction fine-tuning significantly enhances the LLM on both seen and unseen CA tasks. At the same time, performance on the general NLP tasks of the SuperNI benchmark remains stable.",
      "arxiv_url": "https://arxiv.org/abs/2505.22076",
      "pdf_url": "https://arxiv.org/pdf/2505.22076",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10561",
      "title": "T2A-Feedback: Improving Basic Capabilities of Text-to-Audio Generation via Fine-grained AI Feedback",
      "authors": [
        "Zehan Wang",
        "Ke Lei",
        "Chen Zhu",
        "Jia-Bin Huang",
        "Sashuai Zhou",
        "Luping Liu",
        "Xize Cheng",
        "Shengpeng Ji",
        "Zhenhui Ye",
        "Tao Jin",
        "Zhou Zhao"
      ],
      "abstract": "Text-to-audio (T2A) generation has achieved remarkable progress in generating a variety of audio outputs from language prompts. However, current state-of-the-art T2A models still struggle to satisfy human preferences for prompt-following and acoustic quality when generating complex multi-event audio. To improve the performance of the model in these high-level applications, we propose to enhance the basic capabilities of the model with AI feedback learning. First, we introduce fine-grained AI audio scoring pipelines to: 1) verify whether each event in the text prompt is present in the audio (Event Occurrence Score), 2) detect deviations in event sequences from the language description (Event Sequence Score), and 3) assess the overall acoustic and harmonic quality of the generated audio (Acoustic&Harmonic Quality). We evaluate these three automatic scoring pipelines and find that they correlate significantly better with human preferences than other evaluation metrics. This highlights their value as both feedback signals and evaluation metrics. Utilizing our robust scoring pipelines, we construct a large audio preference dataset, T2A-FeedBack, which contains 41k prompts and 249k audios, each accompanied by detailed scores. Moreover, we introduce T2A-EpicBench, a benchmark that focuses on long captions, multi-events, and story-telling scenarios, aiming to evaluate the advanced capabilities of T2A models. Finally, we demonstrate how T2A-FeedBack can enhance current state-of-the-art audio model. With simple preference tuning, the audio generation model exhibits significant improvements in both simple (AudioCaps test set) and complex (T2A-EpicBench) scenarios.",
      "arxiv_url": "https://arxiv.org/abs/2505.10561",
      "pdf_url": "https://arxiv.org/pdf/2505.10561",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15487",
      "title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models",
      "authors": [
        "Martina Miliani",
        "Serenna Auriemma",
        "Alessandro Bondielli",
        "Emmanuele Chersoni",
        "Lucia C. Passaro",
        "Irene Sucameli",
        "Alessandro Lenci"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used in tasks requiring interpretive and inferential accuracy. In this paper, we introduce ExpliCa, a new dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely integrates both causal and temporal relations presented in different linguistic orders and explicitly expressed by linguistic connectives. The dataset is enriched with crowdsourced human acceptability ratings. We tested LLMs on ExpliCa through prompting and perplexity-based metrics. We assessed seven commercial and open-source LLMs, revealing that even top models struggle to reach 0.80 accuracy. Interestingly, models tend to confound temporal relations with causal ones, and their performance is also strongly influenced by the linguistic order of the events. Finally, perplexity-based scores and prompting performance are differently affected by model size.",
      "arxiv_url": "https://arxiv.org/abs/2502.15487",
      "pdf_url": "https://arxiv.org/pdf/2502.15487",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12509",
      "title": "LegalCore: A Dataset for Event Coreference Resolution in Legal Documents",
      "authors": [
        "Kangda Wei",
        "Xi Shi",
        "Jonathan Tong",
        "Sai Ramana Reddy",
        "Anandhavelu Natarajan",
        "R. Jain",
        "Aparna Garimella",
        "Ruihong Huang"
      ],
      "abstract": "Recognizing events and their coreferential mentions in a document is essential for understanding semantic meanings of text. The existing research on event coreference resolution is mostly limited to news articles. In this paper, we present the first dataset for the legal domain, LegalCore, which has been annotated with comprehensive event and event coreference information. The legal contract documents we annotated in this dataset are several times longer than news articles, with an average length of around 25k tokens per document. The annotations show that legal documents have dense event mentions and feature both short-distance and super long-distance coreference links between event mentions. We further benchmark mainstream Large Language Models (LLMs) on this dataset for both event detection and event coreference resolution tasks, and find that this dataset poses significant challenges for state-of-the-art open-source and proprietary LLMs, which perform significantly worse than a supervised baseline. We will publish the dataset as well as the code.",
      "arxiv_url": "https://arxiv.org/abs/2502.12509",
      "pdf_url": "https://arxiv.org/pdf/2502.12509",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12436",
      "title": "Should I Trust You? Detecting Deception in Negotiations using Counterfactual RL",
      "authors": [
        "Wichayaporn Wongkamjan",
        "Yanze Wang",
        "Feng Gu",
        "Denis Peskoff",
        "Jonathan K. Kummerfeld",
        "Jonathan May",
        "J. Boyd-Graber"
      ],
      "abstract": "An increasingly common socio-technical problem is people being taken in by offers that sound ``too good to be true'', where persuasion and trust shape decision-making. This paper investigates how \\abr{ai} can help detect these deceptive scenarios. We analyze how humans strategically deceive each other in \\textit{Diplomacy}, a board game that requires both natural language communication and strategic reasoning. This requires extracting logical forms of proposed agreements in player communications and computing the relative rewards of the proposal using agents' value functions. Combined with text-based features, this can improve our deception detection. Our method detects human deception with a high precision when compared to a Large Language Model approach that flags many true messages as deceptive. Future human-\\abr{ai} interaction tools can build on our methods for deception detection by triggering \\textit{friction} to give users a chance of interrogating suspicious proposals.",
      "arxiv_url": "https://arxiv.org/abs/2502.12436",
      "pdf_url": "https://arxiv.org/pdf/2502.12436",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "648775dc9ac5f40adf4e2cff81ec3304d16d1711",
      "title": "A Constrained Text Revision Agent via Iterative Planning and Searching",
      "authors": [
        "Hannan Cao",
        "Hwee Tou Ng"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/648775dc9ac5f40adf4e2cff81ec3304d16d1711",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "648cca12bbb069c1bbc818cb5115bc72287da68e",
      "title": "PARSQL: Enhancing Text-to-SQL through SQL Parsing and Reasoning",
      "authors": [
        "Yaxun Dai",
        "Haiqin Yang",
        "Hao Mou",
        "Pingfu Chao"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/648cca12bbb069c1bbc818cb5115bc72287da68e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.15027",
      "title": "DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models",
      "authors": [
        "Chengyu Wang",
        "Junbing Yan",
        "Yuanhao Yue",
        "Jun Huang"
      ],
      "abstract": "Enhancing computational efficiency and reducing deployment costs for large language models (LLMs) have become critical challenges in various resource-constrained scenarios. In this work, we present DistilQwen2.5, a family of distilled, lightweight LLMs derived from the public Qwen2.5 models. These distilled models exhibit enhanced instruction-following capabilities compared to the original models based on a series of distillation techniques that incorporate knowledge from much larger LLMs. In our industrial practice, we first leverage powerful proprietary LLMs with varying capacities as multi-agent teachers to select, rewrite, and refine instruction-response pairs that are more suitable for student LLMs to learn. After standard fine-tuning, we further leverage a computationally efficient model fusion approach that enables student models to progressively integrate fine-grained hidden knowledge from their teachers. Experimental evaluations demonstrate that the distilled models possess significantly stronger capabilities than their original checkpoints. Additionally, we present use cases to illustrate the applications of our framework in real-world scenarios. To facilitate practical use, we have released all the DistilQwen2.5 models to the open-source community.",
      "arxiv_url": "https://arxiv.org/abs/2504.15027",
      "pdf_url": "https://arxiv.org/pdf/2504.15027",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.05720",
      "title": "That is Unacceptable: the Moral Foundations of Canceling",
      "authors": [
        "Soda Marem Lo",
        "Oscar Araque",
        "Rajesh Sharma",
        "M. Stranisci"
      ],
      "abstract": "Canceling is a morally-driven phenomenon that hinders the development of safe social media platforms and contributes to ideological polarization. To address this issue we present the Canceling Attitudes Detection (CADE) dataset, an annotated corpus of canceling incidents aimed at exploring the factors of disagreements in evaluating people canceling attitudes on social media. Specifically, we study the impact of annotators'morality in their perception of canceling, showing that morality is an independent axis for the explanation of disagreement on this phenomenon. Annotator's judgments heavily depend on the type of controversial events and involved celebrities. This shows the need to develop more event-centric datasets to better understand how harms are perpetrated in social media and to develop more aware technologies for their detection.",
      "arxiv_url": "https://arxiv.org/abs/2503.05720",
      "pdf_url": "https://arxiv.org/pdf/2503.05720",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "64a422d88d007b05ad0b0a4d3ff4aac23804a018",
      "title": "Low-Entropy Watermark Detection via Bayes' Rule Derived Detector",
      "authors": [
        "Beining Huang",
        "Du Su",
        "Fei Sun",
        "Qi Cao",
        "Huawei Shen",
        "Xueqi Cheng"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/64a422d88d007b05ad0b0a4d3ff4aac23804a018",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.06528",
      "title": "InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes Under Herd Behavior",
      "authors": [
        "Huisheng Wang",
        "Zhuoshi Pan",
        "Hangjing Zhang",
        "Mingxiao Liu",
        "Hanqing Gao",
        "†. H.VickyZhao",
        "Samuel A Assefa",
        "Danial Dervovic",
        "Mahmoud Mahfouz",
        "Robert E Tillman",
        "Prashant Reddy",
        "Manuela Veloso. 2020",
        "Christopher Avery",
        "Peter Zemsky. 1998",
        "Multidimensional",
        "Yuntao Bai",
        "Andy Jones",
        "Kamal K. Ndousse",
        "Amanda Askell",
        "Anna Chen",
        "Nova Dassarma",
        "Dawn Drain",
        "Samuel R. Bowman",
        "Jeeyoon Hyun",
        "Ethan Perez",
        "Edwin Chen",
        "Craig Pettit",
        "Scott Heiner",
        "Kamil˙e Lukoši¯ut˙e",
        "Hong-You Chen",
        "Cheng-Hao Tu",
        "Ziwei Li",
        "Han-Wei Shen",
        "Wei-Lun Chao",
        "Team Glm",
        "Aohan Zeng",
        "Bin Xu",
        "Bowen Wang",
        "Chenhui Zhang",
        "Da Yin",
        "Diego Rojas",
        "Guanyu Feng",
        "Hanyu Lai",
        "Hao Yu",
        "Hongning Wang",
        "Jiadai Sun",
        "Jiajie Zhang",
        "Jiale Cheng",
        "Jiayi Gui",
        "Jie Tang",
        "Jing Zhang",
        "Juanzi Li",
        "Lei Zhao",
        "Lindong Wu",
        "Lucen Zhong",
        "Mingdao Liu",
        "Minlie Huang",
        "Peng Zhang",
        "Qinkai Zheng",
        "Rui Lu",
        "Shuaiqi Duan",
        "Shudan Zhang",
        "S. Cao",
        "Weng Shuxun Yang",
        "Lam Tam",
        "Wenyi Zhao",
        "Xiao Liu",
        "Xiao Xia",
        "Xiaohan Zhang",
        "Xiaotao Gu",
        "Xin Lv",
        "Xinghan Liu",
        "Xinyi Liu",
        "Xinyue Yang",
        "Xixuan Song",
        "Xunkai Zhang",
        "Yifan An",
        "Yifan Xu",
        "Yilin Niu",
        "Yuantao Yang",
        "Yueyan Li",
        "Yushi Bai",
        "Yuxiao Dong",
        "Zehan Qi",
        "Zhaoyu Wang",
        "Zhen Yang",
        "Zhengxiao Du",
        "Zhenyu Hou",
        "Zihan Wang",
        "Aaron Grattafiori",
        "Abhimanyu Dubey",
        "Abhinav Jauhri",
        "Abhinav Pandey",
        "Abhishek Kadian",
        "Ahmad Al-Dahle",
        "Aiesha Letman",
        "Akhil Mathur",
        "A. Schelten",
        "Edward J. Hu",
        "Yelong Shen",
        "Zeyuan Phillip Wallis",
        "An Yang",
        "Baosong Yang",
        "Binyuan Hui",
        "Bo Zheng",
        "Bowen Yu",
        "Chang Zhou",
        "Chengpeng Li",
        "Chengyuan Li",
        "Dayiheng Liu",
        "Fei Huang",
        "Guanting Dong",
        "Haoran Wei",
        "Huan Lin",
        "Jialong Tang",
        "Jialin Wang",
        "Jian Yang",
        "Jianhong Tu",
        "Jianwei Zhang",
        "Jianxin Ma",
        "Jin Xu",
        "Jingren Zhou",
        "Jinze Bai",
        "Jinzheng He",
        "Junyang Lin",
        "Kai Dang",
        "Keming Lu",
        "Ke-Yang Chen",
        "Kexin Yang",
        "Mei Li",
        "Mingfeng Xue",
        "Na Ni",
        "Pei Zhang",
        "Peng Wang",
        "Ru Peng",
        "Rui Men",
        "Ruize Gao",
        "Runji Lin",
        "Shijie Wang",
        "Shuai Bai",
        "Sinan Tan",
        "Tianhang Zhu",
        "Tianhao Li",
        "Tianyu Liu",
        "Wenbin Ge",
        "Xiaodong Deng",
        "Shujuan Zhao",
        "Lingfeng Qiao",
        "Kangyang Luo",
        "Qianwen Zhang",
        "Junru Lu",
        "Di Yin. 2024",
        "SNFinLLM",
        "Wayne Xin Zhao",
        "Kun Zhou",
        "Junyi Li",
        "Tianyi Tang",
        "Xiaolei Wang",
        "Yupeng Hou",
        "Yingqian Min",
        "Beichen Zhang",
        "Junjie Zhang",
        "Zican Dong",
        "Yifan Du",
        "Chen Yang",
        "Yushuo Chen",
        "Zhipeng Chen",
        "Jinhao Jiang"
      ],
      "abstract": "Aligning Large Language Models (LLMs) with investor decision-making processes under herd behavior is a critical challenge in behavioral finance, which grapples with a fundamental limitation: the scarcity of real-user data needed for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM outputs and human behavioral patterns, its reliance on massive authentic data imposes substantial collection costs and privacy risks. We propose InvestAlign, a novel framework that constructs high-quality SFT datasets by leveraging theoretical solutions to similar and simple optimal investment problems rather than complex scenarios. Our theoretical analysis demonstrates that training LLMs with InvestAlign-generated data achieves faster parameter convergence than using real-user data, suggesting superior learning efficiency. Furthermore, we develop InvestAgent, an LLM agent fine-tuned with InvestAlign, which demonstrates significantly closer alignment to real-user data than pre-SFT models in both simple and complex investment problems. This highlights our proposed InvestAlign as a promising approach with the potential to address complex optimal investment problems and align LLMs with investor decision-making processes under herd behavior. Our code is publicly available at https://github.com/thu-social-network-research-group/InvestAlign.",
      "arxiv_url": "https://arxiv.org/abs/2507.06528",
      "pdf_url": "https://arxiv.org/pdf/2507.06528",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19073",
      "title": "Towards Harmonized Uncertainty Estimation for Large Language Models",
      "authors": [
        "Rui Li",
        "Jing Long",
        "Muge Qi",
        "Heming Xia",
        "Lei Sha",
        "Peiyi Wang",
        "Zhifang Sui"
      ],
      "abstract": "To facilitate robust and trustworthy deployment of large language models (LLMs), it is essential to quantify the reliability of their generations through uncertainty estimation. While recent efforts have made significant advancements by leveraging the internal logic and linguistic features of LLMs to estimate uncertainty scores, our empirical analysis highlights the pitfalls of these methods to strike a harmonized estimation between indication, balance, and calibration, which hinders their broader capability for accurate uncertainty estimation. To address this challenge, we propose CUE (Corrector for Uncertainty Estimation): A straightforward yet effective method that employs a lightweight model trained on data aligned with the target LLM's performance to adjust uncertainty scores. Comprehensive experiments across diverse models and tasks demonstrate its effectiveness, which achieves consistent improvements of up to 60% over existing methods.",
      "arxiv_url": "https://arxiv.org/abs/2505.19073",
      "pdf_url": "https://arxiv.org/pdf/2505.19073",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.11073",
      "title": "CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention",
      "authors": [
        "Zekai Ye",
        "Qiming Li",
        "Xiaocheng Feng",
        "Libo Qin",
        "Yi-Chong Huang",
        "Baohang Li",
        "Kui Jiang",
        "Yang Xiang",
        "Zhirui Zhang",
        "Yunfei Lu",
        "Duyu Tang",
        "Dandan Tu",
        "Bing Qin"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal abilities but remain prone to multilingual object hallucination, with a higher likelihood of generating responses inconsistent with the visual input when utilizing queries in non-English languages compared to English. Most existing approaches to address these rely on pretraining or fine-tuning, which are resource-intensive. In this paper, inspired by observing the disparities in cross-modal attention patterns across languages, we propose Cross-Lingual Attention Intervention for Mitigating multilingual object hallucination (CLAIM) in LVLMs, a novel near training-free method by aligning attention patterns. CLAIM first identifies language-specific cross-modal attention heads, then estimates language shift vectors from English to the target language, and finally intervenes in the attention outputs during inference to facilitate cross-lingual visual perception capability alignment. Extensive experiments demonstrate that CLAIM achieves an average improvement of 13.56% (up to 30% in Spanish) on the POPE and 21.75% on the hallucination subsets of the MME benchmark across various languages. Further analysis reveals that multilingual attention divergence is most prominent in intermediate layers, highlighting their critical role in multilingual scenarios.",
      "arxiv_url": "https://arxiv.org/abs/2506.11073",
      "pdf_url": "https://arxiv.org/pdf/2506.11073",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18791",
      "title": "Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs",
      "authors": [
        "Jungsoo Park",
        "Junmo Kang",
        "Gabriel Stanovsky",
        "Alan Ritter"
      ],
      "abstract": "The surge of LLM studies makes synthesizing their findings challenging. Analysis of experimental results from literature can uncover important trends across studies, but the time-consuming nature of manual data extraction limits its use. Our study presents a semi-automated approach for literature analysis that accelerates data extraction using LLMs. It automatically identifies relevant arXiv papers, extracts experimental results and related attributes, and organizes them into a structured dataset, LLMEvalDB. We then conduct an automated literature analysis of frontier LLMs, reducing the effort of paper surveying and data extraction by more than 93% compared to manual approaches. We validate LLMEvalDB by showing that it reproduces key findings from a recent manual analysis of Chain-of-Thought (CoT) reasoning and also uncovers new insights that go beyond it, showing, for example, that in-context examples benefit coding&multimodal tasks but offer limited gains in math reasoning tasks compared to zero-shot CoT. Our automatically updatable dataset enables continuous tracking of target models by extracting evaluation studies as new data becomes available. Through LLMEvalDB and empirical analysis, we provide insights into LLMs while facilitating ongoing literature analyses of their behavior.",
      "arxiv_url": "https://arxiv.org/abs/2502.18791",
      "pdf_url": "https://arxiv.org/pdf/2502.18791",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10772",
      "title": "Ranked Voting based Self-Consistency of Large Language Models",
      "authors": [
        "Weiqin Wang",
        "Yile Wang",
        "Hui Huang"
      ],
      "abstract": "Majority voting is considered an effective method to enhance chain-of-thought reasoning, as it selects the answer with the highest\"self-consistency\"among different reasoning paths (Wang et al., 2023). However, previous chain-of-thought reasoning methods typically generate only a single answer in each trial, thereby ignoring the possibility of other potential answers. As a result, these alternative answers are often overlooked in subsequent voting processes. In this work, we propose to generate ranked answers in each reasoning process and conduct ranked voting among multiple ranked answers from different responses, thereby making the overall self-consistency more reliable. Specifically, we use three ranked voting methods: Instant-runoff voting, Borda count voting, and mean reciprocal rank voting. We validate our methods on six datasets, including three multiple-choice and three open-ended question-answering tasks, using both advanced open-source and closed-source large language models. Extensive experimental results indicate that our proposed method outperforms the baselines, showcasing the potential of leveraging the information of ranked answers and using ranked voting to improve reasoning performance. The code is available at https://github.com/szu-tera/RankedVotingSC.",
      "arxiv_url": "https://arxiv.org/abs/2505.10772",
      "pdf_url": "https://arxiv.org/pdf/2505.10772",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "6578806662072300f211220c21aecc09c911dc51",
      "title": "Uncertainty-Aware Iterative Preference Optimization for Enhanced LLM Reasoning",
      "authors": [
        "Lei Li",
        "Hehuan Liu",
        "Yaxin Zhou",
        "ZhaoYang Gui",
        "Xudong Weng",
        "Yi Yuan",
        "Zheng Wei",
        "Zang Li"
      ],
      "abstract": "Direct Preference Optimization (DPO) has recently emerged as an efficient and effective method for aligning large language models with human preferences. However, constructing high-quality preference datasets remains challenging, often necessitating expensive manual or powerful LM annotations. Additionally, standard DPO exhibits suboptimal performance in complex reasoning tasks, such as mathematical and code reasoning. In this paper, we introduce an approach to collect preference pairs through iterative sampling and execution feedback, tailored to the current learning state ( e.g. well-learned, mis-learned, and un-learned) of the policy model. To alleviate the failures of DPO and improve its applicability in reasoning tasks, we propose IUPO, an iterative uncertainty-aware preference optimization method that achieves fine-grained preference control by assessing model confidence. We validate our approach across three reasoning tasks, incorporating five established reasoning datasets and one self-curated dataset. Our experimental results demonstrate an overall improvement of 3.6% over the standard DPO method and show the model exhibits promising generalizability.",
      "arxiv_url": "https://www.semanticscholar.org/paper/6578806662072300f211220c21aecc09c911dc51",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.23634",
      "title": "gMBA: Expression Semantic Guided Mixed Boolean-Arithmetic Deobfuscation Using Transformer Architectures",
      "authors": [
        "Youjeong Noh",
        "Joon-Young Paik",
        "Jingun Kwon",
        "Eun-Sun Cho"
      ],
      "abstract": "Mixed Boolean-Arithmetic (MBA) obfuscation protects intellectual property by converting programs into forms that are more complex to analyze. However, MBA has been increasingly exploited by malware developers to evade detection and cause significant real-world problems. Traditional MBA deobfuscation methods often consider these expressions as part of a black box and overlook their internal semantic information. To bridge this gap, we propose a truth table, which is an automatically constructed semantic representation of an expression's behavior that does not rely on external resources. The truth table is a mathematical form that represents the output of expression for all possible combinations of input. We also propose a general and extensible guided MBA deobfuscation framework (gMBA) that modifies a Transformer-based neural encoder-decoder Seq2Seq architecture to incorporate this semantic guidance. Experimental results and in-depth analysis show that integrating expression semantics significantly improves performance and highlights the importance of internal semantic expressions in recovering obfuscated code to its original form.",
      "arxiv_url": "https://arxiv.org/abs/2506.23634",
      "pdf_url": "https://arxiv.org/pdf/2506.23634",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.16870",
      "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
      "authors": [
        "Anshumann",
        "Mohd Abbas Zaidi",
        "Akhil Kedia",
        "Jinwoo Ahn",
        "T. Kwon",
        "Kangwook Lee",
        "Haejun Lee",
        "Joohyung Lee"
      ],
      "abstract": "Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse knowledge distillation such as caching Top-K probabilities, while intuitive, provide biased estimates of teacher probability distribution to the student, resulting in suboptimal performance and calibration. We propose an importance-sampling-based method `Random Sampling Knowledge Distillation', which provides unbiased estimates, preserves the gradient in expectation, and requires storing significantly sparser logits. Our method enables faster training of student models with marginal overhead (<10%) compared to cross-entropy based training, while maintaining competitive performance compared to full distillation, across a range of model sizes from 300M to 3B.",
      "arxiv_url": "https://arxiv.org/abs/2503.16870",
      "pdf_url": "https://arxiv.org/pdf/2503.16870",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11882",
      "title": "Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration",
      "authors": [
        "Shao Zhang",
        "Xihuai Wang",
        "Wenhao Zhang",
        "Chaoran Li",
        "Junru Song",
        "Tingyu Li",
        "Lin Qiu",
        "Xuezhi Cao",
        "Xunliang Cai",
        "Wen Yao",
        "Weinan Zhang",
        "Xinbing Wang",
        "Ying Wen"
      ],
      "abstract": "Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. DPT-Agent can effectively help LLMs convert correct slow thinking and reasoning into executable actions, thereby improving performance. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.",
      "arxiv_url": "https://arxiv.org/abs/2502.11882",
      "pdf_url": "https://arxiv.org/pdf/2502.11882",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10354",
      "title": "LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations",
      "authors": [
        "Yile Wang",
        "Zhanyu Shen",
        "Hui Huang"
      ],
      "abstract": "Semantic text representation is a fundamental task in the field of natural language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have demonstrated excellent performance, but the values of each dimension are difficult to trace and interpret. Bag-of-words, as classic sparse interpretable embeddings, suffers from poor performance. Recently, Benara et al. (2024) propose interpretable text embeddings using large language models, which forms\"0/1\"embeddings based on responses to a series of questions. These interpretable text embeddings are typically high-dimensional (larger than 10,000). In this work, we propose Low-dimensional (lower than 500) Dense and Interpretable text embeddings with Relative representations (LDIR). The numerical values of its dimensions indicate semantic relatedness to different anchor texts through farthest point sampling, offering both semantic representation as well as a certain level of traceability and interpretability. We validate LDIR on multiple semantic textual similarity, retrieval, and clustering tasks. Extensive experimental results show that LDIR performs close to the black-box baseline models and outperforms the interpretable embeddings baselines with much fewer dimensions. Code is available at https://github.com/szu-tera/LDIR.",
      "arxiv_url": "https://arxiv.org/abs/2505.10354",
      "pdf_url": "https://arxiv.org/pdf/2505.10354",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "661b0b4627b5d9aa0525bfc27d050093d90775b4",
      "title": "Exploring the Role of Mental Health Conversational Agents in Training Medical Students and Professionals: A Systematic Literature Review",
      "authors": [
        "Thushari Atapattu",
        "M. Thilakaratne",
        "Duc Nhan Do",
        "Mahen Herath",
        "Katrina E. Falkner"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/661b0b4627b5d9aa0525bfc27d050093d90775b4",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.07096",
      "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens",
      "authors": [
        "Jiacheng Liu",
        "Taylor Blanton",
        "Yanai Elazar",
        "Sewon Min",
        "YenSung Chen",
        "Arnavi Chheda-Kothary",
        "Huy Tran",
        "Byron Bischoff",
        "Eric Stuart Marsh",
        "Michael Schmitz",
        "Cassidy Trier",
        "Aaron Sarnat",
        "Jenna James",
        "Jon Borchardt",
        "Bailey Kuehl",
        "Evie Cheng",
        "Karen Farley",
        "Sruthi Sreeram",
        "Taira Anderson",
        "David Albright",
        "Carissa Schoenick",
        "Luca Soldaini",
        "Dirk Groeneveld",
        "Rock Yuren Pang",
        "Pang Wei Koh",
        "Noah A. Smith",
        "Sophie Lebrecht",
        "Yejin Choi",
        "Hanna Hajishirzi",
        "Ali Farhadi",
        "Jesse Dodge"
      ],
      "abstract": "We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.",
      "arxiv_url": "https://arxiv.org/abs/2504.07096",
      "pdf_url": "https://arxiv.org/pdf/2504.07096",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14318",
      "title": "RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection",
      "authors": [
        "Wenjun Hou",
        "Yi Cheng",
        "Kaishuai Xu",
        "Heng Li",
        "Yan Hu",
        "Wenjie Li",
        "Jiangming Liu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in various domains, including radiology report generation. Previous approaches have attempted to utilize multimodal LLMs for this task, enhancing their performance through the integration of domain-specific knowledge retrieval. However, these approaches often overlook the knowledge already embedded within the LLMs, leading to redundant information integration. To address this limitation, we propose Radar, a framework for enhancing radiology report generation with supplementary knowledge injection. Radar improves report generation by systematically leveraging both the internal knowledge of an LLM and externally retrieved information. Specifically, it first extracts the model's acquired knowledge that aligns with expert image-based classification outputs. It then retrieves relevant supplementary knowledge to further enrich this information. Finally, by aggregating both sources, Radar generates more accurate and informative radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU X-ray demonstrate that our model outperforms state-of-the-art LLMs in both language quality and clinical accuracy.",
      "arxiv_url": "https://arxiv.org/abs/2505.14318",
      "pdf_url": "https://arxiv.org/pdf/2505.14318",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2510.16455",
      "title": "RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning",
      "authors": [
        "Deyi Ji",
        "Yuekui Yang",
        "Haiyang Wu",
        "Shaoping Ma",
        "Tianrun Chen",
        "Lanyun Zhu"
      ],
      "abstract": "Advertisement (Ad) video violation detection is critical for ensuring platform compliance, but existing methods struggle with precise temporal grounding, noisy annotations, and limited generalization. We propose RAVEN, a novel framework that integrates curriculum reinforcement learning with multimodal large language models (MLLMs) to enhance reasoning and cognitive capabilities for violation detection. RAVEN employs a progressive training strategy, combining precisely and coarsely annotated data, and leverages Group Relative Policy Optimization (GRPO) to develop emergent reasoning abilities without explicit reasoning annotations. Multiple hierarchical sophisticated reward mechanism ensures precise temporal grounding and consistent category prediction. Experiments on industrial datasets and public benchmarks show that RAVEN achieves superior performances in violation category accuracy and temporal interval localization. We also design a pipeline to deploy the RAVEN on the online Ad services, and online A/B testing further validates its practical applicability, with significant improvements in precision and recall. RAVEN also demonstrates strong generalization, mitigating the catastrophic forgetting issue associated with supervised fine-tuning.",
      "arxiv_url": "https://arxiv.org/abs/2510.16455",
      "pdf_url": "https://arxiv.org/pdf/2510.16455",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-10-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "667b7793918be6078a9539d24a3ce4140d28c09c",
      "title": "Correcting on Graph: Faithful Semantic Parsing over Knowledge Graphs with Large Language Models",
      "authors": [
        "Rui Zhao",
        "Feng Zhao",
        "Hong Zhang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/667b7793918be6078a9539d24a3ce4140d28c09c",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11935",
      "title": "ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing",
      "authors": [
        "Xuanle Zhao",
        "Xuexin Liu",
        "Haoyue Yang",
        "Xianzhen Luo",
        "Fanhu Zeng",
        "Jianling Li",
        "Qi Shi",
        "Chi Chen"
      ],
      "abstract": "Although multimodal large language models (MLLMs) show promise in generating chart rendering code, editing charts via code presents a greater challenge. This task demands MLLMs to integrate chart understanding and reasoning capacities, which are labor-intensive. While many MLLMs claim such editing capabilities, current evaluations rely on limited case studies, highlighting the urgent need for a comprehensive evaluation framework. In this work, we propose \\textsc{ChartEdit}, a novel benchmark designed for chart editing tasks, featuring $1405$ diverse editing instructions applied to $233$ real-world charts, each manually annotated and validated for accuracy. Utilizing \\textsc{ChartEdit}, we evaluate the performance of 10 mainstream MLLMs across two types of experiments at both the code and chart levels. The results suggest that large-scale models can generate code to produce images that partially match the reference images. However, their ability to generate accurate edits according to the instructions remains limited. The state-of-the-art (SOTA) model achieves a score of only $59.96$, highlighting significant challenges in precise modification. In contrast, small-scale models, including chart-domain models, struggle both with following editing instructions and generating overall chart images, underscoring the need for further development in this area. Code is available at https://github.com/xxlllz/ChartEdit.",
      "arxiv_url": "https://arxiv.org/abs/2505.11935",
      "pdf_url": "https://arxiv.org/pdf/2505.11935",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.09506",
      "title": "Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models",
      "authors": [
        "J. Wu",
        "Gefei Gu",
        "Yanan Zheng",
        "Dit-Yan Yeung",
        "Arman Cohan"
      ],
      "abstract": "Long-context language models (LCLMs) have exhibited impressive capabilities in long-context understanding tasks. Among these, long-context referencing -- a crucial task that requires LCLMs to attribute items of interest to specific parts of long-context data -- remains underexplored. To bridge this gap, this paper proposes Referencing Evaluation for Long-context Language Models (Ref-Long), a novel benchmark designed to assess the long-context referencing capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the indexes of documents that reference a specific key, emphasizing contextual relationships between the key and the documents over simple retrieval. Based on the task design, we construct three subsets ranging from synthetic to realistic scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs reveal significant shortcomings in long-context referencing, even among advanced models like GPT-4o. To further investigate these challenges, we conduct comprehensive analyses, including human evaluations, task format adjustments, fine-tuning experiments, and error analyses, leading to several key insights. Our data and code can be found in https://github. com/wujunjie1998/Ref-Long.",
      "arxiv_url": "https://arxiv.org/abs/2507.09506",
      "pdf_url": "https://arxiv.org/pdf/2507.09506",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.15629",
      "title": "Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability",
      "authors": [
        "Yusuke Sakai",
        "Hidetaka Kamigaito",
        "Taro Watanabe"
      ],
      "abstract": "In generative commonsense reasoning tasks such as CommonGen, generative large language models (LLMs) compose sentences that include all given concepts. However, when focusing on instruction-following capabilities, if a prompt specifies a concept order, LLMs must generate sentences that adhere to the specified order. To address this, we propose Ordered CommonGen, a benchmark designed to evaluate the compositional generalization and instruction-following abilities of LLMs. This benchmark measures ordered coverage to assess whether concepts are generated in the specified order, enabling a simultaneous evaluation of both abilities. We conducted a comprehensive analysis using 36 LLMs and found that, while LLMs generally understand the intent of instructions, biases toward specific concept order patterns often lead to low-diversity outputs or identical results even when the concept order is altered. Moreover, even the most instruction-compliant LLM achieved only about 75% ordered coverage, highlighting the need for improvements in both instruction-following and compositional generalization capabilities.",
      "arxiv_url": "https://arxiv.org/abs/2506.15629",
      "pdf_url": "https://arxiv.org/pdf/2506.15629",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "66edcd3f1a22e282b0b19554b6c7339462040a95",
      "title": "Socratic Style Chain-of-Thoughts Help LLMs to be a Better Reasoner",
      "authors": [
        "Jiangbo Pei",
        "Peiyu Liu",
        "Xin Zhao",
        "Aidong Men",
        "Yang Liu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/66edcd3f1a22e282b0b19554b6c7339462040a95",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.11614",
      "title": "Neutralizing Bias in LLM Reasoning using Entailment Graphs",
      "authors": [
        "Liang Cheng",
        "Tianyi Li",
        "Zhaowei Wang",
        "Tianyang Liu",
        "Mark Steedman"
      ],
      "abstract": "LLMs are often claimed to be capable of Natural Language Inference (NLI), which is widely regarded as a cornerstone of more complex forms of reasoning. However, recent works show that LLMs still suffer from hallucinations in NLI due to attestation bias, where LLMs overly rely on propositional memory to build shortcuts. To solve the issue, we design an unsupervised framework to construct counterfactual reasoning data and fine-tune LLMs to reduce attestation bias. To measure bias reduction, we build bias-adversarial variants of NLI datasets with randomly replaced predicates in premises while keeping hypotheses unchanged. Extensive evaluations show that our framework can significantly reduce hallucinations from attestation bias. Then, we further evaluate LLMs fine-tuned with our framework on original NLI datasets and their bias-neutralized versions, where original entities are replaced with randomly sampled ones. Extensive results show that our framework consistently improves inferential performance on both original and bias-neutralized NLI datasets.",
      "arxiv_url": "https://arxiv.org/abs/2503.11614",
      "pdf_url": "https://arxiv.org/pdf/2503.11614",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.10316",
      "title": "Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling",
      "authors": [
        "Suvodip Dey",
        "Yi-Jyun Sun",
        "Gokhan Tur",
        "Dilek Hakkani-Tur"
      ],
      "abstract": "Recent LLMs have enabled significant advancements for conversational agents. However, they are also well known to hallucinate, producing responses that seem plausible but are factually incorrect. On the other hand, users tend to over-rely on LLM-based AI agents, accepting AI's suggestion even when it is wrong. Adding positive friction, such as explanations or getting user confirmations, has been proposed as a mitigation in AI-supported decision-making systems. In this paper, we propose an accountability model for LLM-based task-oriented dialogue agents to address user overreliance via friction turns in cases of model uncertainty and errors associated with dialogue state tracking (DST). The accountability model is an augmented LLM with an additional accountability head that functions as a binary classifier to predict the relevant slots of the dialogue state mentioned in the conversation. We perform our experiments with multiple backbone LLMs on two established benchmarks (MultiWOZ and Snips). Our empirical findings demonstrate that the proposed approach not only enables reliable estimation of AI agent errors but also guides the decoder in generating more accurate actions. We observe around 3% absolute improvement in joint goal accuracy (JGA) of DST output by incorporating accountability heads into modern LLMs. Self-correcting the detected errors further increases the JGA from 67.13 to 70.51, achieving state-of-the-art DST performance. Finally, we show that error correction through user confirmations (friction turn) achieves a similar performance gain, highlighting its potential to reduce user overreliance.",
      "arxiv_url": "https://arxiv.org/abs/2501.10316",
      "pdf_url": "https://arxiv.org/pdf/2501.10316",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-01-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2508.05053",
      "title": "Finding Needles in Images: Can Multi-modal LLMs Locate Fine Details?",
      "authors": [
        "Parth Thakkar",
        "Ankush Agarwal",
        "Prasad Kasu",
        "Pulkit Bansal",
        "Chaitanya Devaguptapu"
      ],
      "abstract": "While Multi-modal Large Language Models (MLLMs) have shown impressive capabilities in document understanding tasks, their ability to locate and reason about fine-grained details within complex documents remains understudied. Consider searching a restaurant menu for a specific nutritional detail or identifying a disclaimer in a lengthy newspaper article tasks that demand careful attention to small but significant details within a broader narrative, akin to Finding Needles in Images (NiM). To address this gap, we introduce NiM, a carefully curated benchmark spanning diverse real-world documents including newspapers, menus, and lecture images, specifically designed to evaluate MLLMs'capability in these intricate tasks. Building on this, we further propose Spot-IT, a simple yet effective approach that enhances MLLMs capability through intelligent patch selection and Gaussian attention, motivated from how humans zoom and focus when searching documents. Our extensive experiments reveal both the capabilities and limitations of current MLLMs in handling fine-grained document understanding tasks, while demonstrating the effectiveness of our approach. Spot-IT achieves significant improvements over baseline methods, particularly in scenarios requiring precise detail extraction from complex layouts.",
      "arxiv_url": "https://arxiv.org/abs/2508.05053",
      "pdf_url": "https://arxiv.org/pdf/2508.05053",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-08-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.09897",
      "title": "TAMP: Token-Adaptive Layerwise Pruning in Multimodal Large Language Models",
      "authors": [
        "Jaewoo Lee",
        "Keyang Xuan",
        "C. Ekbote",
        "Sandeep Polisetty",
        "Yi R. Fung",
        "Paul Pu Liang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have shown remarkable versatility in understanding diverse multimodal data and tasks. However, these capabilities come with an increased model scale. While post-training pruning reduces model size in unimodal models, its application to MLLMs often yields limited success. Our analysis discovers that conventional methods fail to account for the unique token attributes across layers and modalities inherent to MLLMs. Inspired by this observation, we propose TAMP, a simple yet effective pruning framework tailored for MLLMs, featuring two key components: (1) Diversity-Aware Sparsity, which adjusts sparsity ratio per layer based on diversities among multimodal output tokens, preserving more parameters in high-diversity layers; and (2) Adaptive Multimodal Input Activation, which identifies representative multimodal input tokens using attention scores to guide unstructured weight pruning. We validate our method on two state-of-the-art MLLMs: LLaVA-NeXT, designed for vision-language tasks, and VideoLLaMA2, capable of processing audio, visual, and language modalities. Empirical experiments across various multimodal evaluation benchmarks demonstrate that each component of our approach substantially outperforms existing pruning techniques.",
      "arxiv_url": "https://arxiv.org/abs/2504.09897",
      "pdf_url": "https://arxiv.org/pdf/2504.09897",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05725",
      "title": "Large Language Models are Good Relational Learners",
      "authors": [
        "Fan Wu",
        "Vijay Prakash Dwivedi",
        "J. Leskovec"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across various domains, yet their application to relational deep learning (RDL) remains underexplored. Existing approaches adapt LLMs by traversing relational links between entities in a database and converting the structured data into flat text documents. Still, this text-based serialization disregards critical relational structures, introduces redundancy, and often exceeds standard LLM context lengths. We introduce Rel-LLM, a novel architecture that utilizes a graph neural network (GNN)- based encoder to generate structured relational prompts for LLMs within a retrieval-augmented generation (RAG) framework. Unlike traditional text-based serialization approaches, our method preserves the inherent relational structure of databases while enabling LLMs to effectively process and reason over complex entity relationships. Specifically, the GNN encoder extracts a local subgraph around an entity to build feature representations that contain relevant entity relationships and temporal dependencies. These representations are transformed into structured prompts using a denormalization process, effectively allowing the LLM to reason over relational structures. Through extensive experiments, we demonstrate that Rel-LLM outperforms existing methods on key RDL tasks, offering a scalable and efficient approach to integrating LLMs with structured data sources. Code is available at https://github.com/smiles724/Rel-LLM.",
      "arxiv_url": "https://arxiv.org/abs/2506.05725",
      "pdf_url": "https://arxiv.org/pdf/2506.05725",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-06-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10402",
      "title": "Rethinking Repetition Problems of LLMs in Code Generation",
      "authors": [
        "Yihong Dong",
        "Yuchen Liu",
        "Xue Jiang",
        "Zhi Jin",
        "Ge Li"
      ],
      "abstract": "With the advent of neural language models, the performance of code generation has been significantly boosted. However, the problem of repetitions during the generation process continues to linger. Previous work has primarily focused on content repetition, which is merely a fraction of the broader repetition problem in code generation. A more prevalent and challenging problem is structural repetition. In structural repetition, the repeated code appears in various patterns but possesses a fixed structure, which can be inherently reflected in grammar. In this paper, we formally define structural repetition and propose an efficient decoding approach called RPG, which stands for Repetition Penalization based on Grammar, to alleviate the repetition problems in code generation for LLMs. Specifically, RPG first leverages grammar rules to identify repetition problems during code generation, and then strategically decays the likelihood of critical tokens that contribute to repetitions, thereby mitigating them in code generation. To facilitate this study, we construct a new dataset CodeRepetEval to comprehensively evaluate approaches for mitigating the repetition problems in code generation. Extensive experimental results demonstrate that RPG substantially outperforms the best-performing baselines on CodeRepetEval dataset as well as HumanEval and MBPP benchmarks, effectively reducing repetitions and enhancing the quality of generated code.",
      "arxiv_url": "https://arxiv.org/abs/2505.10402",
      "pdf_url": "https://arxiv.org/pdf/2505.10402",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.16727",
      "title": "Unveiling the Lack of LVLM Robustness to Fundamental Visual Variations: Why and Path Forward",
      "authors": [
        "Zhiyuan Fan",
        "Yumeng Wang",
        "Sandeep Polisetty",
        "Yi R. Fung"
      ],
      "abstract": "Large Vision Language Models (LVLMs) excel in various vision-language tasks. Yet, their robustness to visual variations in position, scale, orientation, and context that objects in natural scenes inevitably exhibit due to changes in viewpoint and environment remains largely underexplored. To bridge this gap, we introduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating Visual Variation Robustness of LVLMs, which encompasses automated evaluation dataset generation and principled metrics for thorough robustness assessment. Through extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability to visual variations, in which even advanced models that excel at complex vision-language tasks significantly underperform on simple tasks such as object recognition. Interestingly, these models exhibit a distinct visual position bias that contradicts theories of effective receptive fields, and demonstrate a human-like visual acuity threshold. To identify the source of these vulnerabilities, we present a systematic framework for component-level analysis, featuring a novel visualization approach for aligned visual features. Results show that these vulnerabilities stem from error accumulation in the pipeline architecture and inadequate multimodal alignment. Complementary experiments with synthetic data further demonstrate that these limitations are fundamentally architectural deficiencies, scoring the need for architectural innovations in future LVLM designs.",
      "arxiv_url": "https://arxiv.org/abs/2504.16727",
      "pdf_url": "https://arxiv.org/pdf/2504.16727",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19959",
      "title": "MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models",
      "authors": [
        "Zhongzhan Huang",
        "Guoming Ling",
        "Shan Zhong",
        "Hefeng Wu",
        "Liang Lin"
      ],
      "abstract": "Long Context Understanding (LCU) is a critical area for exploration in current large language models (LLMs). However, due to the inherently lengthy nature of long-text data, existing LCU benchmarks for LLMs often result in prohibitively high evaluation costs, like testing time and inference expenses. Through extensive experimentation, we discover that existing LCU benchmarks exhibit significant redundancy, which means the inefficiency in evaluation. In this paper, we propose a concise data compression method tailored for long-text data with sparse information characteristics. By pruning the well-known LCU benchmark LongBench, we create MiniLongBench. This benchmark includes only 237 test samples across six major task categories and 21 distinct tasks. Through empirical analysis of over 60 LLMs, MiniLongBench achieves an average evaluation cost reduced to only 4.5% of the original while maintaining an average rank correlation coefficient of 0.97 with LongBench results. Therefore, our MiniLongBench, as a low-cost benchmark, holds great potential to substantially drive future research into the LCU capabilities of LLMs. See https://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.",
      "arxiv_url": "https://arxiv.org/abs/2505.19959",
      "pdf_url": "https://arxiv.org/pdf/2505.19959",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16763",
      "title": "Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation",
      "authors": [
        "Hongji Yang",
        "Yucheng Zhou",
        "Wencheng Han",
        "Jianbing Shen"
      ],
      "abstract": "Text-to-image models are powerful for producing high-quality images based on given text prompts, but crafting these prompts often requires specialized vocabulary. To address this, existing methods train rewriting models with supervision from large amounts of manually annotated data and trained aesthetic assessment models. To alleviate the dependence on data scale for model training and the biases introduced by trained models, we propose a novel prompt optimization framework, designed to rephrase a simple user prompt into a sophisticated prompt to a text-to-image model. Specifically, we employ the large vision language models (LVLMs) as the solver to rewrite the user prompt, and concurrently, employ LVLMs as a reward model to score the aesthetics and alignment of the images generated by the optimized prompt. Instead of laborious human feedback, we exploit the prior knowledge of the LVLM to provide rewards, i.e., AI feedback. Simultaneously, the solver and the reward model are unified into one model and iterated in reinforcement learning to achieve self-improvement by giving a solution and judging itself. Results on two popular datasets demonstrate that our method outperforms other strong competitors.",
      "arxiv_url": "https://arxiv.org/abs/2505.16763",
      "pdf_url": "https://arxiv.org/pdf/2505.16763",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "67ff71a431895befb89dce40f50f458dadb3b2f8",
      "title": "Multi-word Measures: Modeling Semantic Change in Compound Nouns",
      "authors": [
        "Christopher Jenkins",
        "Filip Miletic",
        "Sabine Schulte im Walde"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/67ff71a431895befb89dce40f50f458dadb3b2f8",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "685d5c1d94d1908d9bf45058bfc5088250e1d30e",
      "title": "Search-in-Context: Efficient Multi-Hop QA over Long Contexts via Monte Carlo Tree Search with Dynamic KV Retrieval",
      "authors": [
        "Jiabei Chen",
        "Guang Liu",
        "Shizhu He",
        "Kun Luo",
        "Yao Xu",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/685d5c1d94d1908d9bf45058bfc5088250e1d30e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.14669",
      "title": "Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data",
      "authors": [
        "Wei Zou",
        "Sen Yang",
        "Yu Bao",
        "Shujian Huang",
        "Jiajun Chen",
        "Shanbo Cheng"
      ],
      "abstract": "The rise of Large Language Models (LLMs) has reshaped machine translation (MT), but multilingual MT still relies heavily on parallel data for supervised fine-tuning (SFT), facing challenges like data scarcity for low-resource languages and catastrophic forgetting. To address these issues, we propose TRANS-ZERO, a self-play framework that leverages only monolingual data and the intrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic Monte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong translation performance that rivals supervised methods. Experiments demonstrate that this approach not only matches the performance of models trained on large-scale parallel data but also excels in non-English translation directions. Further analysis reveals that G-MCTS itself significantly enhances translation quality by exploring semantically consistent candidates through iterative translations, providing a robust foundation for the framework's succuss.",
      "arxiv_url": "https://arxiv.org/abs/2504.14669",
      "pdf_url": "https://arxiv.org/pdf/2504.14669",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24646",
      "title": "PRISM: A Framework for Producing Interpretable Political Bias Embeddings with Political-Aware Cross-Encoder",
      "authors": [
        "Yiqun Sun",
        "Qiang Huang",
        "Anthony K. H. Tung",
        "Jun Yu"
      ],
      "abstract": "Semantic Text Embedding is a fundamental NLP task that encodes textual content into vector representations, where proximity in the embedding space reflects semantic similarity. While existing embedding models excel at capturing general meaning, they often overlook ideological nuances, limiting their effectiveness in tasks that require an understanding of political bias. To address this gap, we introduce PRISM, the first framework designed to Produce inteRpretable polItical biaS eMbeddings. PRISM operates in two key stages: (1) Controversial Topic Bias Indicator Mining, which systematically extracts fine-grained political topics and their corresponding bias indicators from weakly labeled news data, and (2) Cross-Encoder Political Bias Embedding, which assigns structured bias scores to news articles based on their alignment with these indicators. This approach ensures that embeddings are explicitly tied to bias-revealing dimensions, enhancing both interpretability and predictive power. Through extensive experiments on two large-scale datasets, we demonstrate that PRISM outperforms state-of-the-art text embedding models in political bias classification while offering highly interpretable representations that facilitate diversified retrieval and ideological analysis. The source code is available at https://github.com/dukesun99/ACL-PRISM.",
      "arxiv_url": "https://arxiv.org/abs/2505.24646",
      "pdf_url": "https://arxiv.org/pdf/2505.24646",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14436",
      "title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models",
      "authors": [
        "Yuqiao Tan",
        "Shizhu He",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "Large Language Models (LLMs) offer a transparent brain with accessible parameters that encode extensive knowledge, which can be analyzed, located and transferred. Consequently, a key research challenge is to transcend traditional knowledge transfer paradigms rooted in symbolic language and achieve genuine Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods for transferring knowledge across LLMs of different scales through parameters presents an intriguing and valuable research direction. In this paper, we first demonstrate $\\textbf{Alignment}$ in parametric space is the fundamental prerequisite to achieve successful cross-scale PKT. We redefine the previously explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes extracted parameters for LoRA initialization and requires subsequent fine-tune for alignment. Hence, to reduce cost for further fine-tuning, we introduce a novel Pre-Align PKT (PrePKT) paradigm and propose a solution called $\\textbf{LaTen}$ ($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that aligns the parametric spaces of LLMs across scales only using several training steps without following training. Comprehensive experiments on four benchmarks demonstrate that both PostPKT and PrePKT face challenges in achieving consistently stable transfer. Through in-depth analysis, we identify $\\textbf{Neural Incompatibility}$ as the ethological and parametric structural differences between LLMs of varying scales, presenting fundamental challenges to achieving effective PKT. These findings provide fresh insights into the parametric architectures of LLMs and highlight promising directions for future research on efficient PKT. Our code is available at https://github.com/Trae1ounG/Neural_Incompatibility.",
      "arxiv_url": "https://arxiv.org/abs/2505.14436",
      "pdf_url": "https://arxiv.org/pdf/2505.14436",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "691c4774eb43de817560f211ceba889210777864",
      "title": "Reflection on Knowledge Graph for Large Language Models Reasoning",
      "authors": [
        "Yigeng Zhou",
        "Wu Li",
        "Yifan Lu",
        "Jing Li",
        "Fangming Liu",
        "Meishan Zhang",
        "Yequan Wang",
        "Daojing He",
        "Honghai Liu",
        "Min Zhang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/691c4774eb43de817560f211ceba889210777864",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05136",
      "title": "Information Locality as an Inductive Bias for Neural Language Models",
      "authors": [
        "Taiga Someya",
        "Anej Svete",
        "Brian DuSell",
        "Timothy J. O'Donnell",
        "Mario Giulianelli",
        "Ryan Cotterell"
      ],
      "abstract": "Inductive biases are inherent in every machine learning system, shaping how models generalize from finite data. In the case of neural language models (LMs), debates persist as to whether these biases align with or diverge from human processing constraints. To address this issue, we propose a quantitative framework that allows for controlled investigations into the nature of these biases. Within our framework, we introduce $m$-local entropy$\\unicode{x2013}$an information-theoretic measure derived from average lossy-context surprisal$\\unicode{x2013}$that captures the local uncertainty of a language by quantifying how effectively the $m-1$ preceding symbols disambiguate the next symbol. In experiments on both perturbed natural language corpora and languages defined by probabilistic finite-state automata (PFSAs), we show that languages with higher $m$-local entropy are more difficult for Transformer and LSTM LMs to learn. These results suggest that neural LMs, much like humans, are highly sensitive to the local statistical structure of a language.",
      "arxiv_url": "https://arxiv.org/abs/2506.05136",
      "pdf_url": "https://arxiv.org/pdf/2506.05136",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.19697",
      "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models",
      "authors": [
        "Jungwoo Park",
        "Taewhoo Lee",
        "Chanwoong Yoon",
        "Hyeon Hwang",
        "Jaewoo Kang"
      ],
      "abstract": "Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. We introduce Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency; (2) Single-Scale RMSNorm, preventing channel-wise amplification; and (3) a learnable embedding projection, redistributing activation magnitudes originating from embedding matrices. We validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, our OSP model achieves a 35.7 average score across 10 benchmarks (compared to 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. Our work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
      "arxiv_url": "https://arxiv.org/abs/2506.19697",
      "pdf_url": "https://arxiv.org/pdf/2506.19697",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.15573",
      "title": "Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction",
      "authors": [
        "Yuxin Jiang",
        "Yufei Wang",
        "Chuhan Wu",
        "Xinyi Dai",
        "Yan Xu",
        "Weinan Gan",
        "Yasheng Wang",
        "Xin Jiang",
        "Lifeng Shang",
        "Ruiming Tang",
        "Wei Wang"
      ],
      "abstract": "The improvement of LLMs' instruction-following capabilities depends critically on the availability of high-quality instruction-response pairs. While existing automatic data synthetic methods alleviate the burden of manual curation, they often rely heavily on either the quality of seed data or strong assumptions about the structure and content of web documents. To tackle these challenges, we propose Web Reconstruction (WebR), a fully automated framework for synthesizing high-quality instruction-tuning (IT) data directly from raw web documents with minimal assumptions. Leveraging the inherent diversity of raw web content, we conceptualize web reconstruction as an instruction-tuning data synthesis task via a novel dual-perspective paradigm--Web as Instruction and Web as Response--where each web document is designated as either an instruction or a response to trigger the reconstruction process. Comprehensive experiments show that datasets generated by WebR outperform state-of-the-art baselines by up to 16.65% across four instruction-following benchmarks. Notably, WebR demonstrates superior compatibility, data efficiency, and scalability, enabling enhanced domain adaptation with minimal effort. The data and code are publicly available at https://github.com/YJiangcm/WebR.",
      "arxiv_url": "https://arxiv.org/abs/2504.15573",
      "pdf_url": "https://arxiv.org/pdf/2504.15573",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13509",
      "title": "ProMedTS: A Self-Supervised, Prompt-Guided Multimodal Approach for Integrating Medical Text and Time Series",
      "authors": [
        "Shuai Niu",
        "Jing Ma",
        "Hongzhan Lin",
        "Liang Bai",
        "Zhihua Wang",
        "Wei Bi",
        "Yida Xu",
        "Guo Li",
        "Xian Yang"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable performance in vision-language tasks, but their application in the medical field remains underexplored, particularly for integrating structured time series data with unstructured clinical notes. In clinical practice, dynamic time series data, such as lab test results, capture critical temporal patterns, while clinical notes provide rich semantic context. Merging these modalities is challenging due to the inherent differences between continuous signals and discrete text. To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal framework that employs prompt-guided learning to unify these heterogeneous data types. Our approach leverages lightweight anomaly detection to generate anomaly captions that serve as prompts, guiding the encoding of raw time series data into informative prompt embeddings. These prompt embeddings are aligned with textual representations in a shared latent space, preserving fine-grained temporal nuances alongside semantic insights. Furthermore, our framework incorporates tailored self-supervised objectives to enhance both intra- and inter-modal alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world datasets, and the results demonstrate that our method consistently outperforms state-of-the-art approaches.",
      "arxiv_url": "https://arxiv.org/abs/2502.13509",
      "pdf_url": "https://arxiv.org/pdf/2502.13509",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21033",
      "title": "Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation",
      "authors": [
        "Seungmin Lee",
        "Yongsang Yoo",
        "Minhwa Jung",
        "Min Song"
      ],
      "abstract": "Dialogue Topic Segmentation (DTS) aims to divide dialogues into coherent segments. DTS plays a crucial role in various NLP downstream tasks, but suffers from chronic problems: data shortage, labeling ambiguity, and incremental complexity of recently proposed solutions. On the other hand, Despite advances in Large Language Models (LLMs) and reasoning strategies, these have rarely been applied to DTS. This paper introduces Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation, which utilizes LLM-based multi-step deductive reasoning to enhance DTS performance and enable case study using intermediate result. Our method employs a structured prompting approach for bidirectional context summarization, utterance intent classification, and deductive topic shift detection. In the intent classification process, we propose the generalizable intent list for domain-agnostic dialogue intent classification. Experiments in various dialogue settings demonstrate that Def-DTS consistently outperforms traditional and state-of-the-art approaches, with each subtask contributing to improved performance, particularly in reducing type 2 error. We also explore the potential for autolabeling, emphasizing the importance of LLM reasoning techniques in DTS.",
      "arxiv_url": "https://arxiv.org/abs/2505.21033",
      "pdf_url": "https://arxiv.org/pdf/2505.21033",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16173",
      "title": "Mapping 1,000+ Language Models via the Log-Likelihood Vector",
      "authors": [
        "Momose Oyama",
        "Hiroaki Yamagiwa",
        "Yusuke Takase",
        "H. Shimodaira"
      ],
      "abstract": "To compare autoregressive language models at scale, we propose using log-likelihood vectors computed on a predefined text set as model features. This approach has a solid theoretical basis: when treated as model coordinates, their squared Euclidean distance approximates the Kullback-Leibler divergence of text-generation probabilities. Our method is highly scalable, with computational cost growing linearly in both the number of models and text samples, and is easy to implement as the required features are derived from cross-entropy loss. Applying this method to over 1,000 language models, we constructed a\"model map,\"providing a new perspective on large-scale model analysis.",
      "arxiv_url": "https://arxiv.org/abs/2502.16173",
      "pdf_url": "https://arxiv.org/pdf/2502.16173",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01629",
      "title": "Cross-Lingual Generalization and Compression: From Language-Specific to Shared Neurons",
      "authors": [
        "Frederick Riemenschneider",
        "Anette Frank"
      ],
      "abstract": "Multilingual language models (MLLMs) have demonstrated remarkable abilities to transfer knowledge across languages, despite being trained without explicit cross-lingual supervision. We analyze the parameter spaces of three MLLMs to study how their representations evolve during pre-training, observing patterns consistent with compression: models initially form language-specific representations, which gradually converge into cross-lingual abstractions as training progresses. Through probing experiments, we observe a clear transition from uniform language identification capabilities across layers to more specialized layer functions. For deeper analysis, we focus on neurons that encode distinct semantic concepts. By tracing their development during pre-training, we show how they gradually align across languages. Notably, we identify specific neurons that emerge as increasingly reliable predictors for the same concepts across languages.",
      "arxiv_url": "https://arxiv.org/abs/2506.01629",
      "pdf_url": "https://arxiv.org/pdf/2506.01629",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16503",
      "title": "FanChuan: A Multilingual and Graph-Structured Benchmark For Parody Detection and Analysis",
      "authors": [
        "Yilun Zheng",
        "Sha Li",
        "Fangkun Wu",
        "Yang Ziyi",
        "Hongchao Lin",
        "Zhichao Hu",
        "Xinjun Cai",
        "Ziming Wang",
        "Jinxuan Chen",
        "Sitao Luan",
        "Jiahao Xu",
        "Lihui Chen"
      ],
      "abstract": "Parody is an emerging phenomenon on social media, where individuals imitate a role or position opposite to their own, often for humor, provocation, or controversy. Detecting and analyzing parody can be challenging and is often reliant on context, yet it plays a crucial role in understanding cultural values, promoting subcultures, and enhancing self-expression. However, the study of parody is hindered by limited available data and deficient diversity in current datasets. To bridge this gap, we built seven parody datasets from both English and Chinese corpora, with 14,755 annotated users and 21,210 annotated comments in total. To provide sufficient context information, we also collect replies and construct user-interaction graphs to provide richer contextual information, which is lacking in existing datasets. With these datasets, we test traditional methods and Large Language Models (LLMs) on three key tasks: (1) parody detection, (2) comment sentiment analysis with parody, and (3) user sentiment analysis with parody. Our extensive experiments reveal that parody-related tasks still remain challenging for all models, and contextual information plays a critical role. Interestingly, we find that, in certain scenarios, traditional sentence embedding methods combined with simple classifiers can outperform advanced LLMs, i.e. DeepSeek-R1 and GPT-o3, highlighting parody as a significant challenge for LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2502.16503",
      "pdf_url": "https://arxiv.org/pdf/2502.16503",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19684",
      "title": "GRACE: A Granular Benchmark for Evaluating Model Calibration against Human Calibration",
      "authors": [
        "Yoo Yeon Sung",
        "Eve Fleisig",
        "Yu Hou",
        "Ishan Upadhyay",
        "J. Boyd-Graber"
      ],
      "abstract": "Language models are often miscalibrated, leading to confidently incorrect answers. We introduce GRACE, a benchmark for language model calibration that incorporates comparison with human calibration. GRACE consists of question-answer pairs, in which each question contains a series of clues that gradually become easier, all leading to the same answer; models must answer correctly as early as possible as the clues are revealed. This setting permits granular measurement of model calibration based on how early, accurately, and confidently a model answers. After collecting these questions, we host live human vs. model competitions to gather 1,749 data points on human and model teams' timing, accuracy, and confidence. We propose a metric, CalScore, that uses GRACE to analyze model calibration errors and identify types of model miscalibration that differ from human behavior. We find that although humans are less accurate than models, humans are generally better calibrated. Since state-of-the-art models struggle on GRACE, it effectively evaluates progress on improving model calibration.",
      "arxiv_url": "https://arxiv.org/abs/2502.19684",
      "pdf_url": "https://arxiv.org/pdf/2502.19684",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "69c75b961346099b983f45f84554a667e0ef77ea",
      "title": "How do LLMs' Preferences Affect Event Argument Extraction? CAT: Addressing Preference Traps in Unsupervised EAE",
      "authors": [
        "Yunhao Wei",
        "Kai Shuang",
        "Zhiyi Li",
        "Chenrui Mao"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/69c75b961346099b983f45f84554a667e0ef77ea",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "6a13b89a95b3cd794c79dbe8f78e9ded0d5413e1",
      "title": "Debate4MATH: Multi-Agent Debate for Fine-Grained Reasoning in Math",
      "authors": [
        "Shaowei Zhang",
        "Deyi Xiong"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/6a13b89a95b3cd794c79dbe8f78e9ded0d5413e1",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.13513",
      "title": "K/DA: Automated Data Generation Pipeline for Detoxifying Implicitly Offensive Language in Korean",
      "authors": [
        "Minkyeong Jeon",
        "Hyemin Jeong",
        "Yerang Kim",
        "Jiyoung Kim",
        "Jae Hyeon Cho",
        "Byung-Jun Lee"
      ],
      "abstract": "Language detoxification involves removing toxicity from offensive language. While a neutral-toxic paired dataset provides a straightforward approach for training detoxification models, creating such datasets presents several challenges: i) the need for human annotation to build paired data, and ii) the rapid evolution of offensive terms, rendering static datasets quickly outdated. To tackle these challenges, we introduce an automated paired data generation pipeline, called K/DA. This pipeline is designed to generate offensive language with implicit offensiveness and trend-aligned slang, making the resulting dataset suitable for detoxification model training. We demonstrate that the dataset generated by K/DA exhibits high pair consistency and greater implicit offensiveness compared to existing Korean datasets, and also demonstrates applicability to other languages. Furthermore, it enables effective training of a high-performing detoxification model with simple instruction fine-tuning.",
      "arxiv_url": "https://arxiv.org/abs/2506.13513",
      "pdf_url": "https://arxiv.org/pdf/2506.13513",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "6a3e290561cd1a342e2031d31e85814292aadd17",
      "title": "NetSafe: Exploring the Topological Safety of Multi-agent System",
      "authors": [
        "Miao Yu",
        "Shilong Wang",
        "Guibin Zhang",
        "Junyuan Mao",
        "Chenlong Yin",
        "Qijiong Liu",
        "Kun Wang",
        "Qingsong Wen",
        "Yang Wang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/6a3e290561cd1a342e2031d31e85814292aadd17",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01512",
      "title": "Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes",
      "authors": [
        "Meng Li",
        "Michael Vrazitulis",
        "David Schlangen"
      ],
      "abstract": "Rational speakers are supposed to know what they know and what they do not know, and to generate expressions matching the strength of evidence. In contrast, it is still a challenge for current large language models to generate corresponding utterances based on the assessment of facts and confidence in an uncertain real-world environment. While it has recently become popular to estimate and calibrate confidence of LLMs with verbalized uncertainty, what is lacking is a careful examination of the linguistic knowledge of uncertainty encoded in the latent space of LLMs. In this paper, we draw on typological frameworks of epistemic expressions to evaluate LLMs' knowledge of epistemic modality, using controlled stories. Our experiments show that the performance of LLMs in generating epistemic expressions is limited and not robust, and hence the expressions of uncertainty generated by LLMs are not always reliable. To build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge of epistemic modality in LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.01512",
      "pdf_url": "https://arxiv.org/pdf/2506.01512",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18380",
      "title": "RedactOR: An LLM-Powered Framework for Automatic Clinical Data De-Identification",
      "authors": [
        "Praphul Singh",
        "Charlotte Dzialo",
        "Jangwon Kim",
        "Sumana Srivatsa",
        "Irfan Ullah",
        "Sri Gadde",
        "K. Kenthapadi"
      ],
      "abstract": "Ensuring clinical data privacy while preserving utility is critical for AI-driven healthcare and data analytics. Existing de-identification (De-ID) methods, including rule-based techniques, deep learning models, and large language models (LLMs), often suffer from recall errors, limited generalization, and inefficiencies, limiting their real-world applicability. We propose a fully automated, multi-modal framework, RedactOR for de-identifying structured and unstructured electronic health records, including clinical audio records. Our framework employs cost-efficient De-ID strategies, including intelligent routing, hybrid rule and LLM based approaches, and a two-step audio redaction approach. We present a retrieval-based entity relexicalization approach to ensure consistent substitutions of protected entities, thereby enhancing data coherence for downstream applications. We discuss key design desiderata, de-identification and relexicalization methodology, and modular architecture of RedactOR and its integration with the Oracle Health Clinical AI system. Evaluated on the i2b2 2014 De-ID dataset using standard metrics with strict recall, our approach achieves competitive performance while optimizing token usage to reduce LLM costs. Finally, we discuss key lessons and insights from deployment in real-world AI- driven healthcare data pipelines.",
      "arxiv_url": "https://arxiv.org/abs/2505.18380",
      "pdf_url": "https://arxiv.org/pdf/2505.18380",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16827",
      "title": "GUI-explorer: Autonomous Exploration and Mining of Transition-aware Knowledge for GUI Agent",
      "authors": [
        "Bin Xie",
        "Rui Shao",
        "Gongwei Chen",
        "Kaiwen Zhou",
        "Yinchuan Li",
        "Jie Liu",
        "Min Zhang",
        "Liqiang Nie"
      ],
      "abstract": "GUI automation faces critical challenges in dynamic environments. MLLMs suffer from two key issues: misinterpreting UI components and outdated knowledge. Traditional fine-tuning methods are costly for app-specific knowledge updates. We propose GUI-explorer, a training-free GUI agent that incorporates two fundamental mechanisms: (1) Autonomous Exploration of Function-aware Trajectory. To comprehensively cover all application functionalities, we design a Function-aware Task Goal Generator that automatically constructs exploration goals by analyzing GUI structural information (e.g., screenshots and activity hierarchies). This enables systematic exploration to collect diverse trajectories. (2) Unsupervised Mining of Transition-aware Knowledge. To establish precise screen-operation logic, we develop a Transition-aware Knowledge Extractor that extracts effective screen-operation logic through unsupervised analysis the state transition of structured interaction triples (observation, action, outcome). This eliminates the need for human involvement in knowledge extraction. With a task success rate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows significant improvements over SOTA agents. It requires no parameter updates for new apps. GUI-explorer is open-sourced and publicly available at https://github.com/JiuTian-VL/GUI-explorer.",
      "arxiv_url": "https://arxiv.org/abs/2505.16827",
      "pdf_url": "https://arxiv.org/pdf/2505.16827",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.17363",
      "title": "A Large-Scale Real-World Evaluation of LLM-Based Virtual Teaching Assistant",
      "authors": [
        "Sunjun Kweon",
        "Sooyohn Nam",
        "Hyunseung Lim",
        "Hwajung Hong",
        "Edward Choi"
      ],
      "abstract": "Virtual Teaching Assistants (VTAs) powered by Large Language Models (LLMs) have the potential to enhance student learning by providing instant feedback and facilitating multi-turn interactions. However, empirical studies on their effectiveness and acceptance in real-world classrooms are limited, leaving their practical impact uncertain. In this study, we develop an LLM-based VTA and deploy it in an introductory AI programming course with 477 graduate students. To assess how student perceptions of the VTA's performance evolve over time, we conduct three rounds of comprehensive surveys at different stages of the course. Additionally, we analyze 3,869 student--VTA interaction pairs to identify common question types and engagement patterns. We then compare these interactions with traditional student--human instructor interactions to evaluate the VTA's role in the learning process. Through a large-scale empirical study and interaction analysis, we assess the feasibility of deploying VTAs in real-world classrooms and identify key challenges for broader adoption. Finally, we release the source code of our VTA system, fostering future advancements in AI-driven education: \\texttt{https://github.com/sean0042/VTA}.",
      "arxiv_url": "https://arxiv.org/abs/2506.17363",
      "pdf_url": "https://arxiv.org/pdf/2506.17363",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.06645",
      "title": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings",
      "authors": [
        "Tong Liu",
        "Xiao Yu",
        "Wenxuan Zhou",
        "Jindong Gu",
        "Volker Tresp"
      ],
      "abstract": "Efficient preference optimization algorithms such as Direct Preference Optimization (DPO) have become a popular approach in aligning large language models (LLMs) with human preferences. These algorithms implicitly treat the LLM as a reward model, and focus on training it to correct misranked preference pairs. However, recent work~\\citep{chen2024preference} empirically finds that DPO training \\textit{rarely improves these misranked preference pairs}, despite its gradient emphasizing on these cases. We introduce FocalPO, a DPO variant that instead \\textit{down-weighs} misranked preference pairs and prioritizes enhancing the model's understanding of pairs that it can already rank correctly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this by adding a modulating factor to dynamically scale DPO loss. Our experiment demonstrates that FocalPO surpasses DPO and its variants on popular benchmarks like Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the introduced hyperparameter fixed. Additionally, we empirically reveals how FocalPO affects training on correct and incorrect sample groups, further underscoring its effectiveness.",
      "arxiv_url": "https://arxiv.org/abs/2501.06645",
      "pdf_url": "https://arxiv.org/pdf/2501.06645",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "6b415eee617dc2507ed4ca1b0da0d4028e012fa6",
      "title": "White Men Lead, Black Women Help? Benchmarking and Mitigating Language Agency Social Biases in LLMs",
      "authors": [
        "Yixin Wan",
        "Kai-Wei Chang"
      ],
      "abstract": "Social biases can manifest in language agency. While several studies approached agency-related bias in human-written language, very limited research has investigated such biases in Large Language Model (LLM)-generated content. In addition, previous works often rely on string-matching techniques to identify agentic and communal words within texts, which fall short of accurately classifying language agency. We introduce the novel Language Agency Bias Evaluation (LABE) benchmark, which comprehensively evaluates biases in LLMs by analyzing agency levels attributed to different demographic groups in model generations. LABE leverages 5,400 template-based prompts, an accurate agency classifier, and corresponding bias metrics to test for gender, racial, and in-tersectional language agency biases in LLMs on 3 text generation tasks: biographies, professor reviews, and reference letters. We also contribute the Language Agency Classification (LAC) dataset , consisting of 3,724 agen-tic and communal sentences. Using LABE, we unveil language agency social biases in 3 recent LLMs: ChatGPT, Llama3, and Mis-tral. We observe that: (1) LLM generations tend to demonstrate greater gender bias than human-written texts; (2) Models demonstrate remarkably higher levels of intersectional bias than the other bias aspects. Those who are at the intersection of gender and racial minority groups—such as Black females—are consistently described by texts with lower levels of agency, aligning with real-world social inequalities; (3) Among the 3 LLMs investigated, Llama3 demonstrates the greatest overall bias; (4) Not only does prompt-based mitigation fail to resolve language agency bias in LLMs, but it frequently leads to the exacerbation of biases in generated texts.",
      "arxiv_url": "https://www.semanticscholar.org/paper/6b415eee617dc2507ed4ca1b0da0d4028e012fa6",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01163",
      "title": "Bandit-Based Prompt Design Strategy Selection Improves Prompt Optimizers",
      "authors": [
        "Rin Ashizawa",
        "Yoichi Hirose",
        "Nozomu Yoshinari",
        "Kento Uchida",
        "Shinichi Shirakawa"
      ],
      "abstract": "Prompt optimization aims to search for effective prompts that enhance the performance of large language models (LLMs). Although existing prompt optimization methods have discovered effective prompts, they often differ from sophisticated prompts carefully designed by human experts. Prompt design strategies, representing best practices for improving prompt performance, can be key to improving prompt optimization. Recently, a method termed the Autonomous Prompt Engineering Toolbox (APET) has incorporated various prompt design strategies into the prompt optimization process. In APET, the LLM is needed to implicitly select and apply the appropriate strategies because prompt design strategies can have negative effects. This implicit selection may be suboptimal due to the limited optimization capabilities of LLMs. This paper introduces Optimizing Prompts with sTrategy Selection (OPTS), which implements explicit selection mechanisms for prompt design. We propose three mechanisms, including a Thompson sampling-based approach, and integrate them into EvoPrompt, a well-known prompt optimizer. Experiments optimizing prompts for two LLMs, Llama-3-8B-Instruct and GPT-4o mini, were conducted using BIG-Bench Hard. Our results show that the selection of prompt design strategies improves the performance of EvoPrompt, and the Thompson sampling-based mechanism achieves the best overall results. Our experimental code is provided at https://github.com/shiralab/OPTS .",
      "arxiv_url": "https://arxiv.org/abs/2503.01163",
      "pdf_url": "https://arxiv.org/pdf/2503.01163",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.10362",
      "title": "CLaMP 3: Universal Music Information Retrieval Across Unaligned Modalities and Unseen Languages",
      "authors": [
        "Shangda Wu",
        "Zhancheng Guo",
        "Ruibin Yuan",
        "Junyan Jiang",
        "Seungheon Doh",
        "Gus G. Xia",
        "Juhan Nam",
        "Xiaobing Li",
        "Feng Yu",
        "Maosong Sun"
      ],
      "abstract": "CLaMP 3 is a unified framework developed to address challenges of cross-modal and cross-lingual generalization in music information retrieval. Using contrastive learning, it aligns all major music modalities--including sheet music, performance signals, and audio recordings--with multilingual text in a shared representation space, enabling retrieval across unaligned modalities with text as a bridge. It features a multilingual text encoder adaptable to unseen languages, exhibiting strong cross-lingual generalization. Leveraging retrieval-augmented generation, we curated M4-RAG, a web-scale dataset consisting of 2.31 million music-text pairs. This dataset is enriched with detailed metadata that represents a wide array of global musical traditions. To advance future research, we release WikiMT-X, a benchmark comprising 1,000 triplets of sheet music, audio, and richly varied text descriptions. Experiments show that CLaMP 3 achieves state-of-the-art performance on multiple MIR tasks, significantly surpassing previous strong baselines and demonstrating excellent generalization in multimodal and multilingual music contexts.",
      "arxiv_url": "https://arxiv.org/abs/2502.10362",
      "pdf_url": "https://arxiv.org/pdf/2502.10362",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-02-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13775",
      "title": "VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare",
      "authors": [
        "Anudeex Shetty",
        "Amin Beheshti",
        "M. Dras",
        "Usman Naseem"
      ],
      "abstract": "Alignment techniques have become central to ensuring that Large Language Models (LLMs) generate outputs consistent with human values. However, existing alignment paradigms often model an averaged or monolithic preference, failing to account for the diversity of perspectives across cultures, demographics, and communities. This limitation is particularly critical in health-related scenarios, where plurality is essential due to the influence of culture, religion, personal values, and conflicting opinions. Despite progress in pluralistic alignment, no prior work has focused on health, likely due to the unavailability of publicly available datasets. To address this gap, we introduce VITAL, a new benchmark dataset comprising 13.1K value-laden situations and 5.4K multiple-choice questions focused on health, designed to assess and benchmark pluralistic alignment methodologies. Through extensive evaluation of eight LLMs of varying sizes, we demonstrate that existing pluralistic alignment techniques fall short in effectively accommodating diverse healthcare beliefs, underscoring the need for tailored AI alignment in specific domains. This work highlights the limitations of current approaches and lays the groundwork for developing health-specific alignment solutions.",
      "arxiv_url": "https://arxiv.org/abs/2502.13775",
      "pdf_url": "https://arxiv.org/pdf/2502.13775",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.15355",
      "title": "SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models' Knowledge of Indian Culture",
      "authors": [
        "Arijit Maji",
        "Raghvendra Kumar",
        "Akash Ghosh",
        "Anushka Anushka",
        "Sriparna Saha"
      ],
      "abstract": "Language Models (LMs) are indispensable tools shaping modern workflows, but their global effectiveness depends on understanding local socio-cultural contexts. To address this, we introduce SANSKRITI, a benchmark designed to evaluate language models'comprehension of India's rich cultural diversity. Comprising 21,853 meticulously curated question-answer pairs spanning 28 states and 8 union territories, SANSKRITI is the largest dataset for testing Indian cultural knowledge. It covers sixteen key attributes of Indian culture: rituals and ceremonies, history, tourism, cuisine, dance and music, costume, language, art, festivals, religion, medicine, transport, sports, nightlife, and personalities, providing a comprehensive representation of India's cultural tapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic Language Models (ILMs), and Small Language Models (SLMs), revealing significant disparities in their ability to handle culturally nuanced queries, with many models struggling in region-specific contexts. By offering an extensive, culturally rich, and diverse dataset, SANSKRITI sets a new standard for assessing and improving the cultural understanding of LMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.15355",
      "pdf_url": "https://arxiv.org/pdf/2506.15355",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14718",
      "title": "Entity Framing and Role Portrayal in the News",
      "authors": [
        "Tarek Mahmoud",
        "Zhuohan Xie",
        "D. Dimitrov",
        "Nikolaos Nikolaidis",
        "Purificaccao Silvano",
        "Roman Yangarber",
        "Shivam Sharma",
        "Elisa Sartori",
        "Nicolas Stefanovitch",
        "Giovanni Da San Martino",
        "Jakub Piskorski",
        "Preslav Nakov"
      ],
      "abstract": "We introduce a novel multilingual hierarchical corpus annotated for entity framing and role portrayal in news articles. The dataset uses a unique taxonomy inspired by storytelling elements, comprising 22 fine-grained roles, or archetypes, nested within three main categories: protagonist, antagonist, and innocent. Each archetype is carefully defined, capturing nuanced portrayals of entities such as guardian, martyr, and underdog for protagonists; tyrant, deceiver, and bigot for antagonists; and victim, scapegoat, and exploited for innocents. The dataset includes 1,378 recent news articles in five languages (Bulgarian, English, Hindi, European Portuguese, and Russian) focusing on two critical domains of global significance: the Ukraine-Russia War and Climate Change. Over 5,800 entity mentions have been annotated with role labels. This dataset serves as a valuable resource for research into role portrayal and has broader implications for news analysis. We describe the characteristics of the dataset and the annotation process, and we report evaluation results on fine-tuned state-of-the-art multilingual transformers and hierarchical zero-shot learning using LLMs at the level of a document, a paragraph, and a sentence.",
      "arxiv_url": "https://arxiv.org/abs/2502.14718",
      "pdf_url": "https://arxiv.org/pdf/2502.14718",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02041",
      "title": "Enhancing Multimodal Continual Instruction Tuning with BranchLoRA",
      "authors": [
        "Duzhen Zhang",
        "Yong Ren",
        "Zhongzhi Li",
        "Yahan Yu",
        "Jiahua Dong",
        "Chenxing Li",
        "Zhilong Ji",
        "Jinfeng Bai"
      ],
      "abstract": "Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal Large Language Models (MLLMs) to continually align with human intent across sequential tasks. Existing approaches often rely on the Mixture-of-Experts (MoE) LoRA framework to preserve previous instruction alignments. However, these methods are prone to Catastrophic Forgetting (CF), as they aggregate all LoRA blocks via simple summation, which compromises performance over time. In this paper, we identify a critical parameter inefficiency in the MoELoRA framework within the MCIT context. Based on this insight, we propose BranchLoRA, an asymmetric framework to enhance both efficiency and performance. To mitigate CF, we introduce a flexible tuning-freezing mechanism within BranchLoRA, enabling branches to specialize in intra-task knowledge while fostering inter-task collaboration. Moreover, we incrementally incorporate task-specific routers to ensure an optimal branch distribution over time, rather than favoring the most recent task. To streamline inference, we introduce a task selector that automatically routes test inputs to the appropriate router without requiring task identity. Extensive experiments on the latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms MoELoRA and maintains its superiority across various MLLM sizes.",
      "arxiv_url": "https://arxiv.org/abs/2506.02041",
      "pdf_url": "https://arxiv.org/pdf/2506.02041",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "6c3a94694aec80a3abb5350172a61b7066d00b1a",
      "title": "Employing Discourse Coherence Enhancement to Improve Cross-Document Event and Entity Coreference Resolution",
      "authors": [
        "Xinyu Chen",
        "Peifeng Li",
        "Qiaoming Zhu"
      ],
      "abstract": "Cross-Document Coreference Resolution (CDCR) aims to identify and group together mentions of a specific event or entity that occur across multiple documents. In contrast to the within-document tasks, in which event and entity mentions are linked by rich and coherent contexts, cross-document mentions lack such critical contexts, which presents a significant challenge in establishing connections among them. To address this issue, we introduce a novel task Cross-Document Discourse Coherence Enhancement (CD-DCE) to enhance the discourse coherence between two cross-document event or entity mentions. Specifically, CD-DCE first selects coherent texts and then adds them between two cross-document mentions to form a new coherent document. Subsequently, the coherent text is employed to represent the event or entity mentions and to resolve any coreferent mentions. Experimental results on the three popular datasets demonstrate that our proposed method 1 outperforms several state-of-the-art baselines.",
      "arxiv_url": "https://www.semanticscholar.org/paper/6c3a94694aec80a3abb5350172a61b7066d00b1a",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11300",
      "title": "CORDIAL: Can Multimodal Large Language Models Effectively Understand Coherence Relationships?",
      "authors": [
        "Aashish Anantha Ramakrishnan",
        "Aadarsh Anantha Ramakrishnan",
        "Dongwon Lee"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) are renowned for their superior instruction-following and reasoning capabilities across diverse problem domains. However, existing benchmarks primarily focus on assessing factual and logical correctness in downstream tasks, with limited emphasis on evaluating MLLMs' ability to interpret pragmatic cues and intermodal relationships. To address this gap, we assess the competency of MLLMs in performing Multimodal Discourse Analysis (MDA) using Coherence Relations. Our benchmark, CORDIAL, encompasses a broad spectrum of Coherence Relations across 3 different discourse domains at varying levels of granularity. Through our experiments on 10+ MLLMs employing different prompting strategies, we show that even top models like Gemini 1.5 Pro and GPT-4o fail to match the performance of simple classifier-based baselines. This study emphasizes the need to move beyond similarity-based metrics and adopt a discourse-driven framework for evaluating MLLMs, providing a more nuanced assessment of their capabilities. The benchmark and code are available at: https://aashish2000.github.io/CORDIAL/",
      "arxiv_url": "https://arxiv.org/abs/2502.11300",
      "pdf_url": "https://arxiv.org/pdf/2502.11300",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.14928",
      "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework",
      "authors": [
        "Yao Shi",
        "Rongkeng Liang",
        "Yong Xu"
      ],
      "abstract": "Large language models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78% agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.",
      "arxiv_url": "https://arxiv.org/abs/2504.14928",
      "pdf_url": "https://arxiv.org/pdf/2504.14928",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.00999",
      "title": "La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America",
      "authors": [
        "María Grandury",
        "Javier Aula-Blasco",
        "Júlia Falcão",
        "Clémentine Fourrier",
        "Miguel González Saiz",
        "Gonzalo Martínez",
        "Gonzalo Santamaría Gómez",
        "Rodrigo Agerri",
        "Nuria Aldama-García",
        "Luis Chiruzzo",
        "Javier Conde",
        "Helena Gómez-Adorno",
        "Marta Guerrero Nieto",
        "Guido Ivetta",
        "N. Fuertes",
        "Flor Miriam Plaza del Arco",
        "M. Martín-Valdivia",
        "Helena Montoro Zamorano",
        "Carmen Muñoz Sanz",
        "Pedro Reviriego",
        "Leire Rosado Plaza",
        "Alejandro Vaca Serrano",
        "María Estrella Vallecillo Rodríguez",
        "Jorge Vallego",
        "Irune Zubiaga"
      ],
      "abstract": "Leaderboards showcase the current capabilities and limitations of Large Language Models (LLMs). To motivate the development of LLMs that represent the linguistic and cultural diversity of the Spanish-speaking community, we present La Leaderboard, the first open-source leaderboard to evaluate generative LLMs in languages and language varieties of Spain and Latin America. La Leaderboard is a community-driven project that aims to establish an evaluation standard for everyone interested in developing LLMs for the Spanish-speaking community. This initial version combines 66 datasets in Basque, Catalan, Galician, and different Spanish varieties, showcasing the evaluation results of 50 models. To encourage community-driven development of leaderboards in other languages, we explain our methodology, including guidance on selecting the most suitable evaluation setup for each downstream task. In particular, we provide a rationale for using fewer few-shot examples than typically found in the literature, aiming to reduce environmental impact and facilitate access to reproducible results for a broader research community.",
      "arxiv_url": "https://arxiv.org/abs/2507.00999",
      "pdf_url": "https://arxiv.org/pdf/2507.00999",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "6cd22334cd9425aaa3f6b9de0f5da0aca1c2fc92",
      "title": "Targeted Syntactic Evaluation for Grammatical Error Correction",
      "authors": [
        "Aomi Koyama",
        "Masato Mita",
        "Su-Youn Yoon",
        "Y. Takama",
        "Mamoru Komachi"
      ],
      "abstract": "Language learners encounter a wide range of grammar items across the beginner, intermediate, and advanced levels. To develop grammatical error correction (GEC) models effectively, it is crucial to identify which grammar items are easier or more challenging for models to correct. However, conventional benchmarks based on learner-produced texts are insufficient for conducting detailed evaluations of GEC model performance across a wide range of grammar items due to biases in their distribution. To address this issue, we propose a new evaluation paradigm that assesses GEC models using minimal pairs of ungrammatical and grammatical sentences for each grammar item. As the first benchmark within this paradigm, we introduce the C EFR-based T argeted S yntactic E valuation Dataset for G rammatical Error Correction ( CTSEG ), which complements existing English benchmarks by enabling fine-grained analyses previously unattainable with conventional datasets. Using CTSEG , we evaluate three mainstream types of English GEC models: sequence-to-sequence models, sequence tagging models, and prompt-based models. The results indicate that while current models perform well on beginner-level grammar items, their performance deteriorates substantially for intermediate and",
      "arxiv_url": "https://www.semanticscholar.org/paper/6cd22334cd9425aaa3f6b9de0f5da0aca1c2fc92",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.06598",
      "title": "ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code Generation",
      "authors": [
        "Xuanle Zhao",
        "Xianzhen Luo",
        "Qi Shi",
        "Chi Chen",
        "Shuo Wang",
        "Wanxiang Che",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in chart understanding tasks. However, interpreting charts with textual descriptions often leads to information loss, as it fails to fully capture the dense information embedded in charts. In contrast, parsing charts into code provides lossless representations that can effectively contain all critical details. Although existing open-source MLLMs have achieved success in chart understanding tasks, they still face two major challenges when applied to chart-to-code tasks: (1) Low executability and poor restoration of chart details in the generated code and (2) Lack of large-scale and diverse training data. To address these challenges, we propose \\textbf{ChartCoder}, the first dedicated chart-to-code MLLM, which leverages Code LLMs as the language backbone to enhance the executability of the generated code. Furthermore, we introduce \\textbf{Chart2Code-160k}, the first large-scale and diverse dataset for chart-to-code generation, and propose the \\textbf{Snippet-of-Thought (SoT)} method, which transforms direct chart-to-code generation data into step-by-step generation. Experiments demonstrate that ChartCoder, with only 7B parameters, surpasses existing open-source MLLMs on chart-to-code benchmarks, achieving superior chart restoration and code excitability. Our code is available at https://github.com/thunlp/ChartCoder.",
      "arxiv_url": "https://arxiv.org/abs/2501.06598",
      "pdf_url": "https://arxiv.org/pdf/2501.06598",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.13733",
      "title": "CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual, Multi-Generator and Multi-Domain Settings",
      "authors": [
        "Daniil Orel",
        "Dilshod Azizov",
        "Preslav Nakov"
      ],
      "abstract": "Large language models (LLMs) have revolutionized code generation, automating programming with remarkable efficiency. However, these advancements challenge programming skills, ethics, and assessment integrity, making the detection of LLM-generated code essential for maintaining accountability and standards. While, there has been some research on this problem, it generally lacks domain coverage and robustness, and only covers a small number of programming languages. To this end, we propose a framework capable of distinguishing between human- and LLM-written code across multiple programming languages, code generators, and domains. We use a large-scale dataset from renowned platforms and LLM-based code generators, alongside applying rigorous data quality checks, feature engineering, and comparative analysis using evaluation of traditional machine learning models, pre-trained language models (PLMs), and LLMs for code detection. We perform an evaluation on out-of-domain scenarios, such as detecting the authorship and hybrid authorship of generated code and generalizing to unseen models, domains, and programming languages. Moreover, our extensive experiments show that our framework effectively distinguishes human- from LLM-written code and sets a new benchmark for this task.",
      "arxiv_url": "https://arxiv.org/abs/2503.13733",
      "pdf_url": "https://arxiv.org/pdf/2503.13733",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19830",
      "title": "Revisiting Self-Consistency from Dynamic Distributional Alignment Perspective on Answer Aggregation",
      "authors": [
        "Yiwei Li",
        "Ji Zhang",
        "Shaoxiong Feng",
        "Peiwen Yuan",
        "Xinglin Wang",
        "Jiayi Shi",
        "Yueqi Zhang",
        "Chuyi Tan",
        "Boyuan Pan",
        "Yao Hu",
        "Kan Li"
      ],
      "abstract": "Self-consistency improves reasoning by aggregating diverse stochastic samples, yet the dynamics behind its efficacy remain underexplored. We reframe self-consistency as a dynamic distributional alignment problem, revealing that decoding temperature not only governs sampling randomness but also actively shapes the latent answer distribution. Given that high temperatures require prohibitively large sample sizes to stabilize, while low temperatures risk amplifying biases, we propose a confidence-driven mechanism that dynamically calibrates temperature: sharpening the sampling distribution under uncertainty to align with high-probability modes, and promoting exploration when confidence is high. Experiments on mathematical reasoning tasks show this approach outperforms fixed-diversity baselines under limited samples, improving both average and best-case performance across varying initial temperatures without additional data or modules. This establishes self-consistency as a synchronization challenge between sampling dynamics and evolving answer distributions.",
      "arxiv_url": "https://arxiv.org/abs/2502.19830",
      "pdf_url": "https://arxiv.org/pdf/2502.19830",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11010",
      "title": "Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models",
      "authors": [
        "Jiangxu Wu",
        "Cong Wang",
        "Tianhuang Su",
        "Jun Yang",
        "Haozhi Lin",
        "Chao Zhang",
        "Ming Peng",
        "Kai Shi",
        "SongPan Yang",
        "BinQing Pan",
        "ZiXian Li",
        "Ni Yang",
        "Zhenyu Yang"
      ],
      "abstract": "The effectiveness of large language models (LLMs) in conversational AI is hindered by their reliance on single-turn supervised fine-tuning (SFT) data, which limits contextual coherence in multi-turn dialogues. Existing methods for generating multi-turn dialogue data struggle to ensure both diversity and quality in instructions. To address this, we propose Review-Instruct, a novel framework that synthesizes multi-turn conversations through an iterative\"Ask-Respond-Review\"process involving three agent roles: a Candidate, multiple Reviewers, and a Chairman. The framework iteratively refines instructions by incorporating Reviewer feedback, enhancing dialogue diversity and difficulty. We construct a multi-turn dataset using the Alpaca dataset and fine-tune the LLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate significant improvements, achieving absolute gains of 2.9\\% on MMLU-Pro and 2\\% on MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B. Ablation studies confirm the critical role of the Review stage and the use of multiple Reviewers in boosting instruction diversity and difficulty. Our work highlights the potential of review-driven, multi-agent frameworks for generating high-quality conversational data at scale.",
      "arxiv_url": "https://arxiv.org/abs/2505.11010",
      "pdf_url": "https://arxiv.org/pdf/2505.11010",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.17073",
      "title": "A Study into Investigating Temporal Robustness of LLMs",
      "authors": [
        "Jonas Wallat",
        "Abdelrahman Abdallah",
        "Adam Jatowt",
        "Avishek Anand"
      ],
      "abstract": "Large Language Models (LLMs) encapsulate a surprising amount of factual world knowledge. However, their performance on temporal questions and historical knowledge is limited because they often cannot understand temporal scope and orientation or neglect the temporal aspect altogether. In this study, we aim to measure precisely how robust LLMs are for question answering based on their ability to process temporal information and perform tasks requiring temporal reasoning and temporal factual knowledge. Specifically, we design eight time-sensitive robustness tests for factual information to check the sensitivity of six popular LLMs in the zero-shot setting. Overall, we find LLMs lacking temporal robustness, especially to temporal reformulations and the use of different granularities of temporal references. We show how a selection of these eight tests can be used automatically to judge a model's temporal robustness for user questions on the fly. Finally, we apply the findings of this study to improve the temporal QA performance by up to 55 percent.",
      "arxiv_url": "https://arxiv.org/abs/2503.17073",
      "pdf_url": "https://arxiv.org/pdf/2503.17073",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2510.09217",
      "title": "IRIS: An Iterative and Integrated Framework for Verifiable Causal Discovery in the Absence of Tabular Data",
      "authors": [
        "Tao Feng",
        "Lizhen Qu",
        "Niket Tandon",
        "Gholamreza Haffari"
      ],
      "abstract": "Causal discovery is fundamental to scientific research, yet traditional statistical algorithms face significant challenges, including expensive data collection, redundant computation for known relations, and unrealistic assumptions. While recent LLM-based methods excel at identifying commonly known causal relations, they fail to uncover novel relations. We introduce IRIS (Iterative Retrieval and Integrated System for Real-Time Causal Discovery), a novel framework that addresses these limitations. Starting with a set of initial variables, IRIS automatically collects relevant documents, extracts variables, and uncovers causal relations. Our hybrid causal discovery method combines statistical algorithms and LLM-based methods to discover known and novel causal relations. In addition to causal discovery on initial variables, the missing variable proposal component of IRIS identifies and incorporates missing variables to expand the causal graphs. Our approach enables real-time causal discovery from only a set of initial variables without requiring pre-existing datasets.",
      "arxiv_url": "https://arxiv.org/abs/2510.09217",
      "pdf_url": "https://arxiv.org/pdf/2510.09217",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-10-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01671",
      "title": "AIMSCheck: Leveraging LLMs for AI-Assisted Review of Modern Slavery Statements Across Jurisdictions",
      "authors": [
        "Adriana Eufrosiana Bora",
        "Akshatha Arodi",
        "Duoyi Zhang",
        "Jordan Bannister",
        "Mirko Bronzi",
        "Arsène Fansi Tchango",
        "Md. Abul Bashar",
        "Richi Nayak",
        "Kerrie L. Mengersen"
      ],
      "abstract": "Modern Slavery Acts mandate that corporations disclose their efforts to combat modern slavery, aiming to enhance transparency and strengthen practices for its eradication. However, verifying these statements remains challenging due to their complex, diversified language and the sheer number of statements that must be reviewed. The development of NLP tools to assist in this task is also difficult due to a scarcity of annotated data. Furthermore, as modern slavery transparency legislation has been introduced in several countries, the generalizability of such tools across legal jurisdictions must be studied. To address these challenges, we work with domain experts to make two key contributions. First, we present AIMS.uk and AIMS.ca, newly annotated datasets from the UK and Canada to enable cross-jurisdictional evaluation. Second, we introduce AIMSCheck, an end-to-end framework for compliance validation. AIMSCheck decomposes the compliance assessment task into three levels, enhancing interpretability and practical applicability. Our experiments show that models trained on an Australian dataset generalize well across UK and Canadian jurisdictions, demonstrating the potential for broader application in compliance monitoring. We release the benchmark datasets and AIMSCheck to the public to advance AI-adoption in compliance assessment and drive further research in this field.",
      "arxiv_url": "https://arxiv.org/abs/2506.01671",
      "pdf_url": "https://arxiv.org/pdf/2506.01671",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19966",
      "title": "Learning to Select In-Context Demonstration Preferred by Large Language Model",
      "authors": [
        "Zheng Zhang",
        "Shaocheng Lan",
        "Lei Song",
        "Jiang Bian",
        "Yexin Li",
        "K. Ren"
      ],
      "abstract": "In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks during inference using only a few demonstrations. However, ICL performance is highly dependent on the selection of these demonstrations. Recent work explores retrieval-based methods for selecting query-specific demonstrations, but these approaches often rely on surrogate objectives such as metric learning, failing to directly optimize ICL performance. Consequently, they struggle to identify truly beneficial demonstrations. Moreover, their discriminative retrieval paradigm is ineffective when the candidate pool lacks sufficient high-quality demonstrations. To address these challenges, we propose GenICL, a novel generative preference learning framework that leverages LLM feedback to directly optimize demonstration selection for ICL. Experiments on 19 datasets across 11 task categories demonstrate that GenICL achieves superior performance than existing methods in selecting the most effective demonstrations, leading to better ICL performance.",
      "arxiv_url": "https://arxiv.org/abs/2505.19966",
      "pdf_url": "https://arxiv.org/pdf/2505.19966",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01954",
      "title": "DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation",
      "authors": [
        "Jennifer Chen",
        "Aidar Myrzakhan",
        "Yaxin Luo",
        "Hassaan Muhammad Khan",
        "S. Mahmoud Bsharat",
        "Zhiqiang Shen"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) methods have proven highly effective for tasks requiring factual consistency and robust knowledge retrieval. However, large-scale RAG systems consume significant computational resources and are prone to generating hallucinated content from Humans. In this work, we introduce $\\texttt{DRAG}$, a novel framework for distilling RAG knowledge from large-scale Language Models (LLMs) into small LMs (SLMs). Our approach leverages evidence- and knowledge graph-based distillation, ensuring that the distilled model retains critical factual knowledge while significantly reducing model size and computational cost. By aligning the smaller model's predictions with a structured knowledge graph and ranked evidence, $\\texttt{DRAG}$ effectively mitigates hallucinations and improves factual accuracy. We further present a case demonstrating how our framework mitigates user privacy risks and introduce a corresponding benchmark. Experimental evaluations on multiple benchmarks demonstrate that our method outperforms the prior competitive RAG methods like MiniRAG for SLMs by up to 27.7% using the same models, preserving high-level efficiency and reliability. With $\\texttt{DRAG}$, we provide a practical and resource-efficient roadmap to deploying enhanced retrieval and generation capabilities in small-sized LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.01954",
      "pdf_url": "https://arxiv.org/pdf/2506.01954",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "6df4fbd28bbb015069e77f36f3b50e5c34abe871",
      "title": "EC-RAFT: Automated Generation of Clinical Trial Eligibility Criteria through Retrieval-Augmented Fine-Tuning",
      "authors": [
        "Nopporn Lekuthai",
        "Nattawit Pewngam",
        "Supitcha Sokrai",
        "Titipat Achakulvisut"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/6df4fbd28bbb015069e77f36f3b50e5c34abe871",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "6e002d9279b62c6ed627cc5c2b169968e24efb25",
      "title": "Exploring the Choice Behavior of Large Language Models",
      "authors": [
        "Weidong Wu",
        "Qinlin Zhao",
        "Hao Chen",
        "Lexin Zhou",
        "Defu Lian",
        "Hong Xie"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/6e002d9279b62c6ed627cc5c2b169968e24efb25",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19674",
      "title": "Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations",
      "authors": [
        "Chaoyi Xiang",
        "Chunhua Liu",
        "S. Deyne",
        "Lea Frermann"
      ],
      "abstract": "As the impact of large language models increases, understanding the moral values they reflect becomes ever more important. Assessing the nature of moral values as understood by these models via direct prompting is challenging due to potential leakage of human norms into model training data, and their sensitivity to prompt formulation. Instead, we propose to use word associations, which have been shown to reflect moral reasoning in humans, as low-level underlying representations to obtain a more robust picture of LLMs' moral reasoning. We study moral differences in associations from western English-speaking communities and LLMs trained predominantly on English data. First, we create a large dataset of LLM-generated word associations, resembling an existing data set of human word associations. Next, we propose a novel method to propagate moral values based on seed words derived from Moral Foundation Theory through the human and LLM-generated association graphs. Finally, we compare the resulting moral conceptualizations, highlighting detailed but systematic differences between moral values emerging from English speakers and LLM associations.",
      "arxiv_url": "https://arxiv.org/abs/2505.19674",
      "pdf_url": "https://arxiv.org/pdf/2505.19674",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.02197",
      "title": "ATLaS: Agent Tuning via Learning Critical Steps",
      "authors": [
        "Zhixun Chen",
        "Ming Li",
        "Yuxuan Huang",
        "Yali Du",
        "Meng Fang",
        "Tianyi Zhou"
      ],
      "abstract": "Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training's focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLaS maintains and improves base LLM skills as generalist agents interacting with diverse environments.",
      "arxiv_url": "https://arxiv.org/abs/2503.02197",
      "pdf_url": "https://arxiv.org/pdf/2503.02197",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18503",
      "title": "Focus on What Matters: Enhancing Medical Vision-Language Models with Automatic Attention Alignment Tuning",
      "authors": [
        "Aofei Chang",
        "Le Huang",
        "Alex Boyd",
        "Parminder Bhatia",
        "Taha A. Kass-Hout",
        "Cao Xiao",
        "Fenglong Ma"
      ],
      "abstract": "Medical Large Vision-Language Models (Med-LVLMs) often exhibit suboptimal attention distribution on visual inputs, leading to hallucinated or inaccurate outputs. Existing mitigation methods primarily rely on inference-time interventions, which are limited in attention adaptation or require additional supervision. To address this, we propose A$^3$Tune, a novel fine-tuning framework for Automatic Attention Alignment Tuning. A$^3$Tune leverages zero-shot weak labels from SAM, refines them into prompt-aware labels using BioMedCLIP, and then selectively modifies visually-critical attention heads to improve alignment while minimizing interference. Additionally, we introduce a A$^3$MoE module, enabling adaptive parameter selection for attention tuning across diverse prompts and images. Extensive experiments on medical VQA and report generation benchmarks show that A$^3$Tune outperforms state-of-the-art baselines, achieving enhanced attention distributions and performance in Med-LVLMs.",
      "arxiv_url": "https://arxiv.org/abs/2505.18503",
      "pdf_url": "https://arxiv.org/pdf/2505.18503",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "6e5fce29cbd9db7f1053b310df253b6b4b3c2991",
      "title": "Mitigate Position Bias in LLMs via Scaling a Single Hidden States Channel",
      "authors": [
        "Yijiong Yu",
        "Huiqiang Jiang",
        "Xufang Luo",
        "Qianhui Wu",
        "Chin-Yew Lin",
        "Dongsheng Li",
        "Yuqing Yang",
        "Yongfeng Huang",
        "Lili Qiu"
      ],
      "abstract": "Long-context language models (LCLMs) can process long context, but still exhibit position bias, also known as “lost in the middle”, which indicates placing key information in the middle of the context will significantly affect performance. To mitigating this, we first explore the micro-level manifestations of position bias, concluding that attention weights are a micro-level expression of position bias. Then we identify that, in addition to position embeddings, positional information in hidden states also contributes to position bias, and it manifests itself in specific channels of hidden states, called positional hidden states. Based on these, we propose a method to mitigate position bias by scaling positional hidden states. Experiments on NaturalQuestions Multi-document QA, KV retrieval and LongBench, using various models including RoPE models, context window-extended models, and Alibi models, demonstrate the effectiveness and generalizability of our approach. Our method can improve performance by up to 15.2% in “lost in the mid-dle” benchmark by modifying just one channel of hidden states. Our code is available at https://aka.ms/PositionalHidden .",
      "arxiv_url": "https://www.semanticscholar.org/paper/6e5fce29cbd9db7f1053b310df253b6b4b3c2991",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18990",
      "title": "GenTool: Enhancing Tool Generalization in Language Models through Zero-to-One and Weak-to-Strong Simulation",
      "authors": [
        "Jie He",
        "Jennifer Neville",
        "Mengting Wan",
        "Longqi Yang",
        "Hui Liu",
        "Xiaofeng Xu",
        "Xia Song",
        "Jeff Z. Pan",
        "Pei Zhou"
      ],
      "abstract": "Large Language Models (LLMs) can enhance their capabilities as AI assistants by integrating external tools, allowing them to access a wider range of information. While recent LLMs are typically fine-tuned with tool usage examples during supervised fine-tuning (SFT), questions remain about their ability to develop robust tool-usage skills and can effectively generalize to unseen queries and tools. In this work, we present GenTool, a novel training framework that prepares LLMs for diverse generalization challenges in tool utilization. Our approach addresses two fundamental dimensions critical for real-world applications: Zero-to-One Generalization, enabling the model to address queries initially lacking a suitable tool by adopting and utilizing one when it becomes available, and Weak-to-Strong Generalization, allowing models to leverage enhanced versions of existing tools to solve queries. To achieve this, we develop synthetic training data simulating these two dimensions of tool usage and introduce a two-stage fine-tuning approach: optimizing tool ranking, then refining tool selection. Through extensive experiments across four generalization scenarios, we demonstrate that our method significantly enhances the tool-usage capabilities of LLMs ranging from 1B to 8B parameters, achieving performance that surpasses GPT-4o. Furthermore, our analysis also provides valuable insights into the challenges LLMs encounter in tool generalization.",
      "arxiv_url": "https://arxiv.org/abs/2502.18990",
      "pdf_url": "https://arxiv.org/pdf/2502.18990",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22630",
      "title": "Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs",
      "authors": [
        "Ziling Cheng",
        "Meng Cao",
        "Marc-Antoine Rondeau",
        "J. C. K. Cheung"
      ],
      "abstract": "The widespread success of large language models (LLMs) on NLP benchmarks has been accompanied by concerns that LLMs function primarily as stochastic parrots that reproduce texts similar to what they saw during pre-training, often erroneously. But what is the nature of their errors, and do these errors exhibit any regularities? In this work, we examine irrelevant context hallucinations, in which models integrate misleading contextual cues into their predictions. Through behavioral analysis, we show that these errors result from a structured yet flawed mechanism that we term class-based (mis)generalization, in which models combine abstract class cues with features extracted from the query or context to derive answers. Furthermore, mechanistic interpretability experiments on Llama-3, Mistral, and Pythia across 39 factual recall relation types reveal that this behavior is reflected in the model's internal computations: (i) abstract class representations are constructed in lower layers before being refined into specific answers in higher layers, (ii) feature selection is governed by two competing circuits -- one prioritizing direct query-based reasoning, the other incorporating contextual cues -- whose relative influences determine the final output. Our findings provide a more nuanced perspective on the stochastic parrot argument: through form-based training, LLMs can exhibit generalization leveraging abstractions, albeit in unreliable ways based on contextual cues -- what we term stochastic chameleons.",
      "arxiv_url": "https://arxiv.org/abs/2505.22630",
      "pdf_url": "https://arxiv.org/pdf/2505.22630",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00784",
      "title": "Research Borderlands: Analysing Writing Across Research Cultures",
      "authors": [
        "Shaily Bhatt",
        "Tal August",
        "Maria Antoniak"
      ],
      "abstract": "Improving cultural competence of language technologies is important. However most recent works rarely engage with the communities they study, and instead rely on synthetic setups and imperfect proxies of culture. In this work, we take a human-centered approach to discover and measure language-based cultural norms, and cultural competence of LLMs. We focus on a single kind of culture, research cultures, and a single task, adapting writing across research cultures. Through a set of interviews with interdisciplinary researchers, who are experts at moving between cultures, we create a framework of structural, stylistic, rhetorical, and citational norms that vary across research cultures. We operationalise these features with a suite of computational metrics and use them for (a) surfacing latent cultural norms in human-written research papers at scale; and (b) highlighting the lack of cultural competence of LLMs, and their tendency to homogenise writing. Overall, our work illustrates the efficacy of a human-centered approach to measuring cultural norms in human-written and LLM-generated texts.",
      "arxiv_url": "https://arxiv.org/abs/2506.00784",
      "pdf_url": "https://arxiv.org/pdf/2506.00784",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22298",
      "title": "Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing",
      "authors": [
        "Yifan Lu",
        "Jing Li",
        "Yigeng Zhou",
        "Yihui Zhang",
        "Wenya Wang",
        "Xiucheng Li",
        "Meishan Zhang",
        "Fangming Liu",
        "Jun Yu",
        "Min Zhang"
      ],
      "abstract": "Large language models (LLMs) exhibit impressive language capabilities but remain vulnerable to malicious prompts and jailbreaking attacks. Existing knowledge editing methods for LLM detoxification face two major challenges. First, they often rely on entity-specific localization, making them ineffective against adversarial inputs without explicit entities. Second, these methods suffer from over-editing, where detoxified models reject legitimate queries, compromising overall performance. In this paper, we propose ToxEdit, a toxicity-aware knowledge editing approach that dynamically detects toxic activation patterns during forward propagation. It then routes computations through adaptive inter-layer pathways to mitigate toxicity effectively. This design ensures precise toxicity mitigation while preserving LLMs' general capabilities. To more accurately assess over-editing, we also enhance the SafeEdit benchmark by incorporating instruction-following evaluation tasks. Experimental results on multiple LLMs demonstrate that our ToxEdit outperforms previous state-of-the-art methods in both detoxification performance and safeguarding general capabilities of LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2505.22298",
      "pdf_url": "https://arxiv.org/pdf/2505.22298",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22240",
      "title": "BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain",
      "authors": [
        "Yunsoo Kim",
        "Yusuf Abdulle",
        "Honghan Wu"
      ],
      "abstract": "Biomedical reasoning often requires traversing interconnected relationships across entities such as drugs, diseases, and proteins. Despite the increasing prominence of large language models (LLMs), existing benchmarks lack the ability to evaluate multi-hop reasoning in the biomedical domain, particularly for queries involving one-to-many and many-to-many relationships. This gap leaves the critical challenges of biomedical multi-hop reasoning underexplored. To address this, we introduce BioHopR, a novel benchmark designed to evaluate multi-hop, multi-answer reasoning in structured biomedical knowledge graphs. Built from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop reasoning tasks that reflect real-world biomedical complexities. Evaluations of state-of-the-art models reveal that O3-mini, a proprietary reasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on 2-hop tasks, outperforming proprietary models such as GPT4O and open-source biomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all models exhibit significant declines in multi-hop performance, underscoring the challenges of resolving implicit reasoning steps in the biomedical domain. By addressing the lack of benchmarks for multi-hop reasoning in biomedical domain, BioHopR sets a new standard for evaluating reasoning capabilities and highlights critical gaps between proprietary and open-source models while paving the way for future advancements in biomedical LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2505.22240",
      "pdf_url": "https://arxiv.org/pdf/2505.22240",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13451",
      "title": "MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation",
      "authors": [
        "Lingfeng Zhang",
        "Xiaoshuai Hao",
        "Qinwen Xu",
        "Qiang Zhang",
        "Xinyao Zhang",
        "Pengwei Wang",
        "Jing Zhang",
        "Zhongyuan Wang",
        "Shanghang Zhang",
        "Renjing Xu"
      ],
      "abstract": "Vision-and-language navigation (VLN) is a key task in Embodied AI, requiring agents to navigate diverse and unseen environments while following natural language instructions. Traditional approaches rely heavily on historical observations as spatio-temporal contexts for decision making, leading to significant storage and computational overhead. In this paper, we introduce MapNav, a novel end-to-end VLN model that leverages Annotated Semantic Map (ASM) to replace historical frames. Specifically, our approach constructs a top-down semantic map at the start of each episode and update it at each timestep, allowing for precise object mapping and structured navigation information. Then, we enhance this map with explicit textual labels for key regions, transforming abstract semantics into clear navigation cues and generate our ASM. MapNav agent using the constructed ASM as input, and use the powerful end-to-end capabilities of VLM to empower VLN. Extensive experiments demonstrate that MapNav achieves state-of-the-art (SOTA) performance in both simulated and real-world environments, validating the effectiveness of our method. Moreover, we will release our ASM generation source code and dataset to ensure reproducibility, contributing valuable resources to the field. We believe that our proposed MapNav can be used as a new memory representation method in VLN, paving the way for future research in this field.",
      "arxiv_url": "https://arxiv.org/abs/2502.13451",
      "pdf_url": "https://arxiv.org/pdf/2502.13451",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01235",
      "title": "Your Model is Overconfident, and Other Lies We Tell Ourselves",
      "authors": [
        "Timothee Mickus",
        "Aman Sinha",
        "Ra'ul V'azquez"
      ],
      "abstract": "The difficulty intrinsic to a given example, rooted in its inherent ambiguity, is a key yet often overlooked factor in evaluating neural NLP models. We investigate the interplay and divergence among various metrics for assessing intrinsic difficulty, including annotator dissensus, training dynamics, and model confidence. Through a comprehensive analysis using 29 models on three datasets, we reveal that while correlations exist among these metrics, their relationships are neither linear nor monotonic. By disentangling these dimensions of uncertainty, we aim to refine our understanding of data complexity and its implications for evaluating and improving NLP models.",
      "arxiv_url": "https://arxiv.org/abs/2503.01235",
      "pdf_url": "https://arxiv.org/pdf/2503.01235",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.02506",
      "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use",
      "authors": [
        "Junjie Ye",
        "Zhengyin Du",
        "Xuesong Yao",
        "Weijian Lin",
        "Yufei Xu",
        "Zehui Chen",
        "Zaiyuan Wang",
        "Sining Zhu",
        "Zhiheng Xi",
        "Siyu Yuan",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang",
        "Jiechao Chen"
      ],
      "abstract": "Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprising 995 user queries and 3,912 associated tools, specifically designed for rigorous evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach that includes tool creation, document refinement, and code generation. We evaluate 14 LLMs across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and GPT), uncovering significant challenges in handling multi-hop tool-use scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%, underscoring substantial room for improvement. Further analysis reveals variations in tool-use strategies for various families, offering actionable insights to guide the development of more effective approaches. Code and data can be found in https://huggingface.co/datasets/bytedance-research/ToolHop.",
      "arxiv_url": "https://arxiv.org/abs/2501.02506",
      "pdf_url": "https://arxiv.org/pdf/2501.02506",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-01-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14949",
      "title": "KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding",
      "authors": [
        "Ahmed Heakl",
        "Muhammad Abdullah Sohail",
        "Mukul Ranjan",
        "Rania Hossam",
        "Ghazi Ahmed",
        "Mohamed El-Geish",
        "Omar Maher",
        "Zhiqiang Shen",
        "F. Khan",
        "Salman H. Khan"
      ],
      "abstract": "With the growing adoption of Retrieval-Augmented Generation (RAG) in document processing, robust text recognition has become increasingly critical for knowledge extraction. While OCR (Optical Character Recognition) for English and other languages benefits from large datasets and well-established benchmarks, Arabic OCR faces unique challenges due to its cursive script, right-to-left text flow, and complex typographic and calligraphic features. We present KITAB-Bench, a comprehensive Arabic OCR benchmark that fills the gaps in current evaluation systems. Our benchmark comprises 8,809 samples across 9 major domains and 36 sub-domains, encompassing diverse document types including handwritten text, structured tables, and specialized coverage of 21 chart types for business intelligence. Our findings show that modern vision-language models (such as GPT-4o, Gemini, and Qwen) outperform traditional OCR approaches (like EasyOCR, PaddleOCR, and Surya) by an average of 60% in Character Error Rate (CER). Furthermore, we highlight significant limitations of current Arabic OCR models, particularly in PDF-to-Markdown conversion, where the best model Gemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges in accurately recognizing Arabic text, including issues with complex fonts, numeral recognition errors, word elongation, and table structure detection. This work establishes a rigorous evaluation framework that can drive improvements in Arabic document analysis methods and bridge the performance gap with English OCR technologies.",
      "arxiv_url": "https://arxiv.org/abs/2502.14949",
      "pdf_url": "https://arxiv.org/pdf/2502.14949",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "6f88d0ee59b84f079d7a3f9e57ba176689d52980",
      "title": "Semantic-Eval : A Semantic Comprehension Evaluation Framework for Large Language Models Generation without Training",
      "authors": [
        "Shusheng Li",
        "Jiale Li",
        "Yifei Qu",
        "Xinwei Shi",
        "Yanliang Guo",
        "Ziyi He",
        "Yubo Wang",
        "Wenjun Tan"
      ],
      "abstract": "With the increasing prominence of large language models (LLMs), evaluating their text-generation capabilities has become an essential research challenge. Although LLM-based evaluation methods exhibit robust performance, the inherent stochastic nature of the LLM generation process introduces a degree of uncertainty in alignment with human preferences. To address this limitation, we propose Semantic-Eval, the first training-free framework designed to assess LLM-generated text based on semantic understanding. This framework computes semantic similarity between pairwise texts to evaluate the interdependence of semantic units, integrating a graph-based weighting mechanism to account for the differential contributions of individual sentences. A pre-trained natural language inference (NLI) model is also incorporated to mitigate potential semantic relationship biases. We evaluate Semantic-Eval across eight datasets that encompass four common NLP tasks. The experimental results indicate that Semantic-Eval surpasses traditional N-gram and BERT-based evaluation metrics, aligning more closely with human judgments and demonstrating a higher correlation than smaller LLMs. However,",
      "arxiv_url": "https://www.semanticscholar.org/paper/6f88d0ee59b84f079d7a3f9e57ba176689d52980",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.14900",
      "title": "Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings",
      "authors": [
        "Imane Guellil",
        "Salomé Andres",
        "A. Anand",
        "Bruce Guthrie",
        "Huayu Zhang",
        "Abul Hasan",
        "Honghan Wu",
        "Beatrice Alex"
      ],
      "abstract": "In this work, we present a manually annotated corpus for Adverse Event (AE) extraction from discharge summaries of elderly patients, a population often underrepresented in clinical NLP resources. The dataset includes 14 clinically significant AEs-such as falls, delirium, and intracranial haemorrhage, along with contextual attributes like negation, diagnosis type, and in-hospital occurrence. Uniquely, the annotation schema supports both discontinuous and overlapping entities, addressing challenges rarely tackled in prior work. We evaluate multiple models using FlairNLP across three annotation granularities: fine-grained, coarse-grained, and coarse-grained with negation. While transformer-based models (e.g., BERT-cased) achieve strong performance on document-level coarse-grained extraction (F1 = 0.943), performance drops notably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly for rare events and complex attributes. These results demonstrate that despite high-level scores, significant challenges remain in detecting underrepresented AEs and capturing nuanced clinical language. Developed within a Trusted Research Environment (TRE), the dataset is available upon request via DataLoch and serves as a robust benchmark for evaluating AE extraction methods and supporting future cross-dataset generalisation.",
      "arxiv_url": "https://arxiv.org/abs/2506.14900",
      "pdf_url": "https://arxiv.org/pdf/2506.14900",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.17662",
      "title": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware Contrastive Learning",
      "authors": [
        "Ke Ji",
        "Yixin Lian",
        "Linxu Li",
        "Jingsheng Gao",
        "Weiyuan Li",
        "Bin Dai"
      ],
      "abstract": "In recent years, large language models (LLMs) have achieved breakthrough progress in many dialogue generation tasks. However, their lack of emotion and fine-grained role awareness limits the model's ability to provide personalized and diverse interactions further. Current methods face high costs in collecting high-quality annotated data for scenarios such as role-playing, and traditional human alignment methods are difficult to deploy due to the inherent diversity of model behavior in role-playing scenarios. Inspired by the alignment of models for safety behaviors through RLHF (Reinforcement Learning from Human Feedback), in this paper, we revisit model role-playing behavior from the perspective of persona alignment and propose a novel annotation-free framework named \\textbf{\\underline{P}}ersona-Aware \\textbf{\\underline{C}}ontrastive \\textbf{\\underline{L}}earning (PCL) to align LLMs' behavior during role-playing, enhancing the model's role consistency. Specifically, we first design a role chain method to encourage the model to self-question based on the role characteristics and dialogue context to adjust personality consistency. Then, we further enhance the model's role-playing strategy through iterative contrastive learning between the use of role characteristics and not. Experiments on both black-box and white-box LLMs show that LLMs equipped with PCL significantly outperform vanilla LLMs under automatic evaluation methods (CharEval \\&GPT-4) and human expert evaluation.",
      "arxiv_url": "https://arxiv.org/abs/2503.17662",
      "pdf_url": "https://arxiv.org/pdf/2503.17662",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-03-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24442",
      "title": "RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation",
      "authors": [
        "Zhentao Xie",
        "Chengcheng Han",
        "Jinxin Shi",
        "Wenjun Cui",
        "Xin Zhao",
        "Xingjiao Wu",
        "Jiabao Zhao"
      ],
      "abstract": "Although multi-agent systems based on large language models show strong capabilities on multiple tasks, they are still limited by high computational overhead, information loss, and robustness. Inspired by ResNet's residual learning, we propose Residual Mixture-of-Agents (RMoA), integrating residual connections to optimize efficiency and reliability. To maximize information utilization from model responses while minimizing computational costs, we innovatively design an embedding-based diversity selection mechanism that greedily selects responses via vector similarity. Furthermore, to mitigate iterative information degradation, we introduce a Residual Extraction Agent to preserve cross-layer incremental information by capturing inter-layer response differences, coupled with a Residual Aggregation Agent for hierarchical information integration. Additionally, we propose an adaptive termination mechanism that dynamically halts processing based on residual convergence, further improving inference efficiency. RMoA achieves state-of-the-art performance on the benchmarks of across alignment, mathematical reasoning, code generation, and multitasking understanding, while significantly reducing computational overhead. Code is available at https://github.com/mindhunter01/RMoA.",
      "arxiv_url": "https://arxiv.org/abs/2505.24442",
      "pdf_url": "https://arxiv.org/pdf/2505.24442",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00930",
      "title": "Aligning VLM Assistants with Personalized Situated Cognition",
      "authors": [
        "Yongqi Li",
        "Shen Zhou",
        "Xiaohu Li",
        "Xin Miao",
        "Jintao Wen",
        "Mayi Xu",
        "Jianhao Chen",
        "Birong Pan",
        "Hankun Kang",
        "Yuanyuan Zhu",
        "Ming Zhong",
        "Tieyun Qian"
      ],
      "abstract": "Vision-language models (VLMs) aligned with general human objectives, such as being harmless and hallucination-free, have become valuable assistants of humans in managing visual tasks. However, people with diversified backgrounds have different cognition even in the same situation. Consequently, they may have personalized expectations for VLM assistants. This highlights the urgent need to align VLM assistants with personalized situated cognition for real-world assistance. To study this problem, we first simplify it by characterizing individuals based on the sociological concept of Role-Set. Then, we propose to evaluate the individuals' actions to examine whether the personalized alignment is achieved. Further, we construct a benchmark named PCogAlignBench, which includes 18k instances and 20 individuals with different Role-Sets. Finally, we present a framework called PCogAlign, which constructs a cognition-aware and action-based reward model for personalized alignment. Experimental results and human evaluations demonstrate the reliability of the PCogAlignBench and the effectiveness of our proposed PCogAlign. We will open-source the constructed benchmark and code at https://github.com/NLPGM/PCogAlign.",
      "arxiv_url": "https://arxiv.org/abs/2506.00930",
      "pdf_url": "https://arxiv.org/pdf/2506.00930",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Personalization"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05167",
      "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
      "authors": [
        "Yeonseok Jeong",
        "Jinsu Kim",
        "Dohyeon Lee",
        "Seung-won Hwang"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain Question Answering (ODQA) by leveraging external documents through Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer context, context compression is necessary. However, prior compression methods do not focus on filtering out non-evidential information, which limit the performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or ECoRAG framework. ECoRAG improves LLM performance by compressing retrieved documents based on evidentiality, ensuring whether answer generation is supported by the correct evidence. As an additional step, ECoRAG reflects whether the compressed content provides sufficient evidence, and if not, retrieves more until sufficient. Experiments show that ECoRAG improves LLM performance on ODQA tasks, outperforming existing compression methods. Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency but also minimizes token usage by retaining only the necessary information to generate the correct answer. Code is available at https://github.com/ldilab/ECoRAG.",
      "arxiv_url": "https://arxiv.org/abs/2506.05167",
      "pdf_url": "https://arxiv.org/pdf/2506.05167",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM",
        "RAG"
      ],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19628",
      "title": "HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices",
      "authors": [
        "Silin Li",
        "Yuhang Guo",
        "Jiashu Yao",
        "Zeming Liu",
        "Haifeng Wang"
      ],
      "abstract": "Large language models (LLMs) have the potential to revolutionize smart home assistants by enhancing their ability to accurately understand user needs and respond appropriately, which is extremely beneficial for building a smarter home environment. While recent studies have explored integrating LLMs into smart home systems, they primarily focus on handling straightforward, valid single-device operation instructions. However, real-world scenarios are far more complex and often involve users issuing invalid instructions or controlling multiple devices simultaneously. These have two main challenges: LLMs must accurately identify and rectify errors in user instructions and execute multiple user instructions perfectly. To address these challenges and advance the development of LLM-based smart home assistants, we introduce HomeBench, the first smart home dataset with valid and invalid instructions across single and multiple devices in this paper. We have experimental results on 13 distinct LLMs; e.g., GPT-4o achieves only a 0.0% success rate in the scenario of invalid multi-device instructions, revealing that the existing state-of-the-art LLMs still cannot perform well in this situation even with the help of in-context learning, retrieval-augmented generation, and fine-tuning. Our code and dataset are publicly available at https://github.com/BITHLP/HomeBench.",
      "arxiv_url": "https://arxiv.org/abs/2505.19628",
      "pdf_url": "https://arxiv.org/pdf/2505.19628",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.12451",
      "title": "S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling",
      "authors": [
        "Suman Adhya",
        "Debarshi Kumar Sanyal"
      ],
      "abstract": "Modeling latent representations in a hyperspherical space has proven effective for capturing directional similarities in high-dimensional text data, benefiting topic modeling. Variational autoencoder-based neural topic models (VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical structure. However, VAE-NTMs often suffer from posterior collapse, where the KL divergence term in the objective function highly diminishes, leading to ineffective latent representations. To mitigate this issue while modeling hyperspherical structure in the latent space, we propose the Spherical Sliced Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior distribution supported on the unit hypersphere and leverages the Spherical Sliced-Wasserstein distance to align the aggregated posterior distribution with the prior. Experimental results demonstrate that S2WTM outperforms state-of-the-art topic models, generating more coherent and diverse topics while improving performance on downstream tasks.",
      "arxiv_url": "https://arxiv.org/abs/2507.12451",
      "pdf_url": "https://arxiv.org/pdf/2507.12451",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "706863b88c4a4614ce71a49f3d0719aaa5acc4a6",
      "title": "Generative Error Correction for Emotion-aware Speech-to-text Translation",
      "authors": [
        "Zhengdong Yang",
        "Sheng Li",
        "Chenhui Chu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/706863b88c4a4614ce71a49f3d0719aaa5acc4a6",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00622",
      "title": "Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples",
      "authors": [
        "Haesung Pyun",
        "Yoonah Park",
        "Yohan Jo"
      ],
      "abstract": "In dialogue state tracking (DST), in-context learning comprises a retriever that selects labeled dialogues as in-context examples and a DST model that uses these examples to infer the dialogue state of the query dialogue. Existing methods for constructing training data for retrievers suffer from three key limitations: (1) the synergistic effect of examples is not considered, (2) the linguistic characteristics of the query are not sufficiently factored in, and (3) scoring is not directly optimized for DST performance. Consequently, the retriever can fail to retrieve examples that would substantially improve DST performance. To address these issues, we present CombiSearch, a method that scores effective in-context examples based on their combinatorial impact on DST performance. Our evaluation on MultiWOZ shows that retrievers trained with CombiSearch surpass state-of-the-art models, achieving a 20x gain in data efficiency and generalizing well to the SGD dataset. Moreover, CombiSearch attains a 12% absolute improvement in the upper bound DST performance over traditional approaches when no retrieval errors are assumed. This significantly increases the headroom for practical DST performance while demonstrating that existing methods rely on suboptimal data for retriever training.",
      "arxiv_url": "https://arxiv.org/abs/2506.00622",
      "pdf_url": "https://arxiv.org/pdf/2506.00622",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.02009",
      "title": "Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts",
      "authors": [
        "Youcheng Huang",
        "Chen Huang",
        "Duanyu Feng",
        "Wenqiang Lei",
        "Jiancheng Lv"
      ],
      "abstract": "Understanding the inner workings of Large Language Models (LLMs) is a critical research frontier. Prior research has shown that a single LLM's concept representations can be captured as steering vectors (SVs), enabling the control of LLM behavior (e.g., towards generating harmful content). Our work takes a novel approach by exploring the intricate relationships between concept representations across different LLMs, drawing an intriguing parallel to Plato's Allegory of the Cave. In particular, we introduce a linear transformation method to bridge these representations and present three key findings: 1) Concept representations across different LLMs can be effectively aligned using simple linear transformations, enabling efficient cross-model transfer and behavioral control via SVs. 2) This linear transformation generalizes across concepts, facilitating alignment and control of SVs representing different concepts across LLMs. 3) A weak-to-strong transferability exists between LLM concept representations, whereby SVs extracted from smaller LLMs can effectively control the behavior of larger LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2501.02009",
      "pdf_url": "https://arxiv.org/pdf/2501.02009",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.04635",
      "title": "Steering off Course: Reliability Challenges in Steering Language Models",
      "authors": [
        "Patrick Queiroz Da Silva",
        "Hari Sethuraman",
        "Dheeraj Rajagopal",
        "Hanna Hajishirzi",
        "Sachin Kumar"
      ],
      "abstract": "Steering methods for language models (LMs) have gained traction as lightweight alternatives to fine-tuning, enabling targeted modifications to model activations. However, prior studies primarily report results on a few models, leaving critical gaps in understanding the robustness of these methods. In this work, we systematically examine three prominent steering methods -- DoLa, function vectors, and task vectors. In contrast to the original studies, which evaluated a handful of models, we test up to 36 models belonging to 14 families with sizes ranging from 1.5B to 70B parameters. Our experiments reveal substantial variability in the effectiveness of the steering approaches, with a large number of models showing no improvement and at times degradation in steering performance. Our analysis demonstrate fundamental flaws in the assumptions underlying these methods, challenging their reliability as scalable steering solutions.",
      "arxiv_url": "https://arxiv.org/abs/2504.04635",
      "pdf_url": "https://arxiv.org/pdf/2504.04635",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2509.06184",
      "title": "Understanding the Influence of Synthetic Data for Text Embedders",
      "authors": [
        "Jacob Mitchell Springer",
        "Vaibhav Adlakha",
        "Siva Reddy",
        "Aditi Raghunathan",
        "Marius Mosbach"
      ],
      "abstract": "Recent progress in developing general purpose text embedders has been driven by training on ever-growing corpora of synthetic LLM-generated data. Nonetheless, no publicly available synthetic dataset exists, posing a barrier to studying its role for generalization. To address this issue, we first reproduce and publicly release the synthetic data proposed by Wang et al. (Mistral-E5). Our synthetic data is high quality and leads to consistent improvements in performance. Next, we critically examine where exactly synthetic data improves model generalization. Our analysis reveals that benefits from synthetic data are sparse and highly localized to individual datasets. Moreover, we observe trade-offs between the performance on different categories and data that benefits one task, degrades performance on another. Our findings highlight the limitations of current synthetic data approaches for building general-purpose embedders and challenge the notion that training on synthetic data leads to more robust embedding models across tasks.",
      "arxiv_url": "https://arxiv.org/abs/2509.06184",
      "pdf_url": "https://arxiv.org/pdf/2509.06184",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-09-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01846",
      "title": "Code-Switching and Syntax: A Large-Scale Experiment",
      "authors": [
        "Igor Sterner",
        "Simone Teufel"
      ],
      "abstract": "The theoretical code-switching (CS) literature provides numerous pointwise investigations that aim to explain patterns in CS, i.e. why bilinguals switch language in certain positions in a sentence more often than in others. A resulting consensus is that CS can be explained by the syntax of the contributing languages. There is however no large-scale, multi-language, cross-phenomena experiment that tests this claim. When designing such an experiment, we need to make sure that the system that is predicting where bilinguals tend to switch has access only to syntactic information. We provide such an experiment here. Results show that syntax alone is sufficient for an automatic system to distinguish between sentences in minimal pairs of CS, to the same degree as bilingual humans. Furthermore, the learnt syntactic patterns generalise well to unseen language pairs.",
      "arxiv_url": "https://arxiv.org/abs/2506.01846",
      "pdf_url": "https://arxiv.org/pdf/2506.01846",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11541",
      "title": "MorphMark: Flexible Adaptive Watermarking for Large Language Models",
      "authors": [
        "Zongqi Wang",
        "Tianle Gu",
        "Baoyuan Wu",
        "Yujiu Yang"
      ],
      "abstract": "Watermarking by altering token sampling probabilities based on red-green list is a promising method for tracing the origin of text generated by large language models (LLMs). However, existing watermark methods often struggle with a fundamental dilemma: improving watermark effectiveness (the detectability of the watermark) often comes at the cost of reduced text quality. This trade-off limits their practical application. To address this challenge, we first formalize the problem within a multi-objective trade-off analysis framework. Within this framework, we identify a key factor that influences the dilemma. Unlike existing methods, where watermark strength is typically treated as a fixed hyperparameter, our theoretical insights lead to the development of MorphMarka method that adaptively adjusts the watermark strength in response to changes in the identified factor, thereby achieving an effective resolution of the dilemma. In addition, MorphMark also prioritizes flexibility since it is a model-agnostic and model-free watermark method, thereby offering a practical solution for real-world deployment, particularly in light of the rapid evolution of AI models. Extensive experiments demonstrate that MorphMark achieves a superior resolution of the effectiveness-quality dilemma, while also offering greater flexibility and time and space efficiency.",
      "arxiv_url": "https://arxiv.org/abs/2505.11541",
      "pdf_url": "https://arxiv.org/pdf/2505.11541",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02818",
      "title": "ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations",
      "authors": [
        "Ekaterina Grishina",
        "Mikhail Gorbunov",
        "Maxim Rakhuba"
      ],
      "abstract": "Large language models (LLMs) demonstrate impressive results in natural language processing tasks but require a significant amount of computational and memory resources. Structured matrix representations are a promising way for reducing the number of parameters of these models. However, it seems unrealistic to expect that weight matrices of pretrained models can be accurately represented by structured matrices without any fine-tuning. To overcome this issue, we utilize the fact that LLM output is invariant under certain orthogonal transformations of weight matrices. This insight can be leveraged to identify transformations that significantly improve the compressibility of weights within structured classes. The proposed approach is applicable to various types of structured matrices that support efficient projection operations. Code is available at https://github.com/GrishKate/ProcrustesGPT",
      "arxiv_url": "https://arxiv.org/abs/2506.02818",
      "pdf_url": "https://arxiv.org/pdf/2506.02818",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23078",
      "title": "Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport",
      "authors": [
        "Yuu Jinnai"
      ],
      "abstract": "Document-level text generation tasks are known to be more difficult than sentence-level text generation tasks as they require the understanding of longer context to generate high-quality texts. In this paper, we investigate the adaption of Minimum Bayes Risk (MBR) decoding for document-level text generation tasks. MBR decoding makes use of a utility function to estimate the output with the highest expected utility from a set of candidate outputs. Although MBR decoding is shown to be effective in a wide range of sentence-level text generation tasks, its performance on document-level text generation tasks is limited as many of the utility functions are designed for evaluating the utility of sentences. To this end, we propose MBR-OT, a variant of MBR decoding using Wasserstein distance to compute the utility of a document using a sentence-level utility function. The experimental result shows that the performance of MBR-OT outperforms that of the standard MBR in document-level machine translation, text simplification, and dense image captioning tasks. Our code is available at https://github.com/jinnaiyuu/mbr-optimal-transport",
      "arxiv_url": "https://arxiv.org/abs/2505.23078",
      "pdf_url": "https://arxiv.org/pdf/2505.23078",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.19878",
      "title": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation",
      "authors": [
        "Nengbo Wang",
        "Xiaotian Han",
        "Jagdip Singh",
        "Jing Ma",
        "Vipin Chaudhary"
      ],
      "abstract": "Large language models (LLMs) have revolutionized natural language processing (NLP), particularly through Retrieval-Augmented Generation (RAG), which enhances LLM capabilities by integrating external knowledge. However, traditional RAG systems face critical limitations, including disrupted contextual integrity due to text chunking, and over-reliance on semantic similarity for retrieval. To address these issues, we propose CausalRAG, a novel framework that incorporates causal graphs into the retrieval process. By constructing and tracing causal relationships, CausalRAG preserves contextual continuity and improves retrieval precision, leading to more accurate and interpretable responses. We evaluate CausalRAG against regular RAG and graph-based RAG approaches, demonstrating its superiority across several metrics. Our findings suggest that grounding retrieval in causal reasoning provides a promising approach to knowledge-intensive tasks.",
      "arxiv_url": "https://arxiv.org/abs/2503.19878",
      "pdf_url": "https://arxiv.org/pdf/2503.19878",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-03-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.19643",
      "title": "Can You Share Your Story? Modeling Clients' Metacognition and Openness for LLM Therapist Evaluation",
      "authors": [
        "Minju Kim",
        "Dongje Yoo",
        "Yeonjun Hwang",
        "Minseok Kang",
        "Namyoung Kim",
        "Minju Gwak",
        "Beong-woo Kwak",
        "Hyungjoo Chae",
        "Harim Kim",
        "Yunjoong Lee",
        "Min Hee Kim",
        "Dayi Jung",
        "Kyong-Mee Chung",
        "Jinyoung Yeo"
      ],
      "abstract": "Understanding clients'thoughts and beliefs is fundamental in counseling, yet current evaluations of LLM therapists often fail to assess this ability. Existing evaluation methods rely on client simulators that clearly disclose internal states to the therapist, making it difficult to determine whether an LLM therapist can uncover unexpressed perspectives. To address this limitation, we introduce MindVoyager, a novel evaluation framework featuring a controllable and realistic client simulator which dynamically adapts itself based on the ongoing counseling session, offering a more realistic and challenging evaluation environment. We further introduce evaluation metrics that assess the exploration ability of LLM therapists by measuring their thorough understanding of client's beliefs and thoughts.",
      "arxiv_url": "https://arxiv.org/abs/2507.19643",
      "pdf_url": "https://arxiv.org/pdf/2507.19643",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03090",
      "title": "Literary Evidence Retrieval via Long-Context Language Models",
      "authors": [
        "Katherine Thai",
        "Mohit Iyyer"
      ],
      "abstract": "How well do modern long-context language models understand literary fiction? We explore this question via the task of literary evidence retrieval, repurposing the RELiC dataset of That et al. (2022) to construct a benchmark where the entire text of a primary source (e.g., The Great Gatsby) is provided to an LLM alongside literary criticism with a missing quotation from that work. This setting, in which the model must generate the missing quotation, mirrors the human process of literary analysis by requiring models to perform both global narrative reasoning and close textual examination. We curate a high-quality subset of 292 examples through extensive filtering and human verification. Our experiments show that recent reasoning models, such as Gemini Pro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In contrast, the best open-weight model achieves only 29.1% accuracy, highlighting a wide gap in interpretive reasoning between open and closed-weight models. Despite their speed and apparent accuracy, even the strongest models struggle with nuanced literary signals and overgeneration, signaling open challenges for applying LLMs to literary analysis. We release our dataset and evaluation code to encourage future work in this direction.",
      "arxiv_url": "https://arxiv.org/abs/2506.03090",
      "pdf_url": "https://arxiv.org/pdf/2506.03090",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.06101",
      "title": "From Conversation to Automation: Leveraging LLMs for Problem-Solving Therapy Analysis",
      "authors": [
        "Elham Aghakhani",
        "Lu Wang",
        "Karla T. Washington",
        "G. Demiris",
        "Jina Huh-Yoo",
        "R. Rezapour"
      ],
      "abstract": "Problem-solving therapy (PST) is a structured psychological approach that helps individuals manage stress and resolve personal issues by guiding them through problem identification, solution brainstorming, decision-making, and outcome evaluation. As mental health care increasingly adopts technologies like chatbots and large language models (LLMs), it is important to thoroughly understand how each session of PST is conducted before attempting to automate it. We developed a comprehensive framework for PST annotation using established PST Core Strategies and a set of novel Facilitative Strategies to analyze a corpus of real-world therapy transcripts to determine which strategies are most prevalent. Using various LLMs and transformer-based models, we found that GPT-4o outperformed all models, achieving the highest accuracy (0.76) in identifying all strategies. To gain deeper insights, we examined how strategies are applied by analyzing Therapeutic Dynamics (autonomy, self-disclosure, and metaphor), and linguistic patterns within our labeled data. Our research highlights LLMs' potential to automate therapy dialogue analysis, offering a scalable tool for mental health interventions. Our framework enhances PST by improving accessibility, effectiveness, and personalized support for therapists.",
      "arxiv_url": "https://arxiv.org/abs/2501.06101",
      "pdf_url": "https://arxiv.org/pdf/2501.06101",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-01-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2509.00842",
      "title": "Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings",
      "authors": [
        "Tengyu Pan",
        "Zhichao Duan",
        "Zhenyu Li",
        "Bo Dong",
        "Ning Liu",
        "Xiuxing Li",
        "Jianyong Wang"
      ],
      "abstract": "Text embedding models are essential for various natural language processing tasks, enabling the effective encoding of semantic information into dense vector representations. These models are typically optimized using triplets of (query, positive, negative) data pairs for contrastive learning, where the negative samples play a critical role in enhancing the model's ability to discern subtle semantic distinctions. In this work, we introduce a Multi-Granularity Hard-negative (MGH) synthesis framework that leverages large language models (LLMs) to generate diverse negative samples with varying levels of similarity with the query. This approach facilitates a coarse-to-fine curriculum learning strategy during supervised training, allowing the embedding model to progressively learn more nuanced semantic representations. Meanwhile, we propose an Anchor Token Aware (ATA) pooling method that assigns higher weights to anchor tokens based on aggregation patterns observed in LLMs, improving text embedding accuracy without increasing model complexity. Comprehensive experiments on the MTEB benchmark demonstrate that our methods achieve state-of-the-art performance, surpassing existing synthesis strategies both with synthetic data and when combined with public retrieval datasets.",
      "arxiv_url": "https://arxiv.org/abs/2509.00842",
      "pdf_url": "https://arxiv.org/pdf/2509.00842",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-08-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.11478",
      "title": "Each Graph is a New Language: Graph Learning with LLMs",
      "authors": [
        "Huachi Zhou",
        "Jiahe Du",
        "Chuang Zhou",
        "Chang Yang",
        "Yilin Xiao",
        "Yuxuan Xie",
        "Xiao Huang"
      ],
      "abstract": "Recent efforts leverage Large Language Models (LLMs) for modeling text-attributed graph structures in node classification tasks. These approaches describe graph structures for LLMs to understand or aggregate LLM-generated textual attribute embeddings through graph structure. However, these approaches face two main limitations in modeling graph structures with LLMs. (i) Graph descriptions become verbose in describing high-order graph structure. (ii) Textual attributes alone do not contain adequate graph structure information. It is challenging to model graph structure concisely and adequately with LLMs. LLMs lack built-in mechanisms to model graph structures directly. They also struggle with complex long-range dependencies between high-order nodes and target nodes. Inspired by the observation that LLMs pre-trained on one language can achieve exceptional performance on another with minimal additional training, we propose \\textbf{G}raph-\\textbf{D}efined \\textbf{L}anguage for \\textbf{L}arge \\textbf{L}anguage \\textbf{M}odel (GDL4LLM). This novel framework enables LLMs to transfer their powerful language understanding capabilities to graph-structured data. GDL4LLM translates graphs into a graph language corpus instead of graph descriptions and pre-trains LLMs on this corpus to adequately understand graph structures. During fine-tuning, this corpus describes the structural information of target nodes concisely with only a few tokens. By treating graphs as a new language, GDL4LLM enables LLMs to model graph structures adequately and concisely for node classification tasks. Extensive experiments on three real-world datasets demonstrate that GDL4LLM outperforms description-based and textual attribute embeddings-based baselines by efficiently modeling different orders of graph structure with LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2501.11478",
      "pdf_url": "https://arxiv.org/pdf/2501.11478",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7239b1546724e86a57437d692b97d0d96e2b3187",
      "title": "Decoder-Only LLMs can be Masked Auto-Encoders",
      "authors": [
        "Dan Qiao",
        "Yuan Gao",
        "Zheming Yang",
        "Di Yang",
        "Ziheng Wu",
        "Pengcheng Lu",
        "Minghui Qiu",
        "Juntao Li",
        "Min Zhang"
      ],
      "abstract": "Modern NLP workflows (e.g., RAG systems) require different models for generation and embedding tasks, where bidirectional pre-trained encoders and decoder-only Large Language Models (LLMs) dominate respective tasks. Structural differences between models result in extra development costs and limit knowledge sharing between tasks. In this work, we present UniMAE, a novel unsupervised training method that transforms a Decoder-Only LLM into a Uni -Directional M asked A uto-E ncoder. UniMAE compresses high-quality semantic information into the [EOS] embedding while preserving the generation capabilities of LLMs. Comprehensive evaluations across 56 MTEB datasets demonstrate that UniMAE can achieve state-of-the-art results under unsupervised settings with merely 100 training steps, establishing the first effective approach to unifying generation and representation learning in decoder-only architectures.",
      "arxiv_url": "https://www.semanticscholar.org/paper/7239b1546724e86a57437d692b97d0d96e2b3187",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.09924",
      "title": "From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models",
      "authors": [
        "Yidan Wang",
        "Yubing Ren",
        "Yanan Cao",
        "Binxing Fang"
      ],
      "abstract": "The rise of Large Language Models (LLMs) has heightened concerns about the misuse of AI-generated text, making watermarking a promising solution. Mainstream watermarking schemes for LLMs fall into two categories: logits-based and sampling-based. However, current schemes entail trade-offs among robustness, text quality, and security. To mitigate this, we integrate logits-based and sampling-based schemes, harnessing their respective strengths to achieve synergy. In this paper, we propose a versatile symbiotic watermarking framework with three strategies: serial, parallel, and hybrid. The hybrid framework adaptively embeds watermarks using token entropy and semantic entropy, optimizing the balance between detectability, robustness, text quality, and security. Furthermore, we validate our approach through comprehensive experiments on various datasets and models. Experimental results indicate that our method outperforms existing baselines and achieves state-of-the-art (SOTA) performance. We believe this framework provides novel insights into diverse watermarking paradigms. Our code is available at https://github.com/redwyd/SymMark.",
      "arxiv_url": "https://arxiv.org/abs/2505.09924",
      "pdf_url": "https://arxiv.org/pdf/2505.09924",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13442",
      "title": "TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation",
      "authors": [
        "Jialin Ouyang"
      ],
      "abstract": "Large language models (LLMs) now achieve near-human performance on standard math word problem benchmarks (e.g., GSM8K), yet their true reasoning ability remains disputed. A key concern is that models often produce confident, yet unfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic dataset that systematically generates infinite unanswerable math word problems and their answerable counterparts, by representing each question as a tree and removing chosen necessary conditions. Experiments show TreeCut effectively induce hallucinations in large language models, including GPT-4o and o3-mini, with rates of 64% and 44% in their respective worst-case scenarios under zero-shot setting. Further analysis highlights that deeper or more complex trees, composite item names, and removing necessary condition near the middle of a path all increase the likelihood of hallucinations, underscoring the persistent challenges LLMs face in identifying unanswerable math problems. The dataset generation code and sample data are available at https://github.com/j-bagel/treecut-math.",
      "arxiv_url": "https://arxiv.org/abs/2502.13442",
      "pdf_url": "https://arxiv.org/pdf/2502.13442",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.03438",
      "title": "BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic Theorem Proving",
      "authors": [
        "Ran Xin",
        "Chenguang Xi",
        "Jie Yang",
        "Feng Chen",
        "Hang Wu",
        "Xia Xiao",
        "Yifan Sun",
        "Shen Zheng",
        "Kai Shen"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have spurred growing interest in automatic theorem proving using Lean4, where effective tree search methods are crucial for navigating the underlying large proof search spaces. While the existing approaches primarily rely on value functions and/or Monte Carlo Tree Search (MCTS), the potential of simpler methods like Best-First Tree Search (BFS) remains underexplored. In this paper, we investigate whether BFS can achieve competitive performance in large-scale theorem proving tasks. We present BFS-Prover, a scalable expert iteration framework, featuring three key innovations. First, we implement strategic data filtering at each expert iteration round, excluding problems solvable via beam search node expansion to focus on harder cases. Second, we improve the sample efficiency of BFS through Direct Preference Optimization (DPO) applied to state-tactic pairs automatically annotated with compiler error feedback, refining the LLM's policy to prioritize productive expansions. Third, we employ length normalization in BFS to encourage exploration of deeper proof paths. BFS-Prover achieves a state-of-the-art score of $72.95\\%$ on the MiniF2F test set and therefore challenges the perceived necessity of complex tree search methods, demonstrating that BFS can achieve competitive performance when properly scaled. To facilitate further research and development in this area, we have open-sourced our model at https://huggingface.co/ByteDance-Seed/BFS-Prover-V1-7B.",
      "arxiv_url": "https://arxiv.org/abs/2502.03438",
      "pdf_url": "https://arxiv.org/pdf/2502.03438",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.07749",
      "title": "NorEval: A Norwegian Language Understanding and Generation Evaluation Benchmark",
      "authors": [
        "Vladislav Mikhailov",
        "T. Enstad",
        "David Samuel",
        "Hans Christian Farsethaas",
        "Andrey Kutuzov",
        "Erik Velldal",
        "Lilja Ovrelid"
      ],
      "abstract": "This paper introduces NorEval, a new and comprehensive evaluation suite for large-scale standardized benchmarking of Norwegian generative language models (LMs). NorEval consists of 24 high-quality human-created datasets -- of which five are created from scratch. In contrast to existing benchmarks for Norwegian, NorEval covers a broad spectrum of task categories targeting Norwegian language understanding and generation, establishes human baselines, and focuses on both of the official written standards of the Norwegian language: Bokm{\\aa}l and Nynorsk. All our datasets and a collection of over 100 human-written prompts are integrated into LM Evaluation Harness, ensuring flexible and reproducible evaluation. We describe the NorEval design and present the results of benchmarking 19 open-source pre-trained and instruction-tuned LMs for Norwegian in various scenarios. Our benchmark, evaluation framework, and annotation materials are publicly available.",
      "arxiv_url": "https://arxiv.org/abs/2504.07749",
      "pdf_url": "https://arxiv.org/pdf/2504.07749",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.05888",
      "title": "QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation",
      "authors": [
        "Bang Nguyen",
        "Tingting Du",
        "Mengxia Yu",
        "Lawrence Angrave",
        "Meng Jiang"
      ],
      "abstract": "While the Question Generation (QG) task has been increasingly adopted in educational assessments, its evaluation remains limited by approaches that lack a clear connection to the educational values of test items. In this work, we introduce test item analysis, a method frequently used by educators to assess test question quality, into QG evaluation. Specifically, we construct pairs of candidate questions that differ in quality across dimensions such as topic coverage, item difficulty, item discrimination, and distractor efficiency. We then examine whether existing QG evaluation approaches can effectively distinguish these differences. Our findings reveal significant shortcomings in these approaches with respect to accurately assessing test item quality in relation to student performance. To address this gap, we propose a novel QG evaluation framework, QG-SMS, which leverages Large Language Model for Student Modeling and Simulation to perform test item analysis. As demonstrated in our extensive experiments and human evaluation study, the additional perspectives introduced by the simulated student profiles lead to a more effective and robust assessment of test items.",
      "arxiv_url": "https://arxiv.org/abs/2503.05888",
      "pdf_url": "https://arxiv.org/pdf/2503.05888",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.14996",
      "title": "Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering",
      "authors": [
        "F. Molfese",
        "Luca Moroni",
        "Luca Gioffré",
        "Alessandro Sciré",
        "Simone Conia",
        "Roberto Navigli"
      ],
      "abstract": "One of the most widely used tasks for evaluating Large Language Models (LLMs) is Multiple-Choice Question Answering (MCQA). While open-ended question answering tasks are more challenging to evaluate, MCQA tasks are, in principle, easier to assess, as the model's answer is thought to be simple to extract and is compared directly to a set of predefined choices. However, recent studies have started to question the reliability of MCQA evaluation, showing that multiple factors can significantly impact the reported performance of LLMs, especially when the model generates free-form text before selecting one of the answer choices. In this work, we shed light on the inconsistencies of MCQA evaluation strategies, which can lead to inaccurate and misleading model comparisons. We systematically analyze whether existing answer extraction methods are aligned with human judgment, and how they are influenced by answer constraints in the prompt across different domains. Our experiments demonstrate that traditional evaluation strategies often underestimate LLM capabilities, while LLM-based answer extractors are prone to systematic errors. Moreover, we reveal a fundamental trade-off between including format constraints in the prompt to simplify answer extraction and allowing models to generate free-form text to improve reasoning. Our findings call for standardized evaluation methodologies and highlight the need for more reliable and consistent MCQA evaluation practices.",
      "arxiv_url": "https://arxiv.org/abs/2503.14996",
      "pdf_url": "https://arxiv.org/pdf/2503.14996",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.18119",
      "title": "Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models",
      "authors": [
        "Qika Lin",
        "Tianzhe Zhao",
        "Kai He",
        "Zhen Peng",
        "Fangzhi Xu",
        "Ling Huang",
        "Jingying Ma",
        "Mengling Feng"
      ],
      "abstract": "Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (\\ie, tokens) that align the format of language sentences. We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration. The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes. Further, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods.",
      "arxiv_url": "https://arxiv.org/abs/2501.18119",
      "pdf_url": "https://arxiv.org/pdf/2501.18119",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01606",
      "title": "Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering",
      "authors": [
        "Zhanghao Hu",
        "Hanqi Yan",
        "Qinglin Zhu",
        "Zhenyi Shen",
        "Yulan He",
        "Lin Gui"
      ],
      "abstract": "Large language models have recently pushed open domain question answering (ODQA) to new frontiers. However, prevailing retriever-reader pipelines often depend on multiple rounds of prompt level instructions, leading to high computational overhead, instability, and suboptimal retrieval coverage. In this paper, we propose EmbQA, an embedding-level framework that alleviates these shortcomings by enhancing both the retriever and the reader. Specifically, we refine query representations via lightweight linear layers under an unsupervised contrastive learning objective, thereby reordering retrieved passages to highlight those most likely to contain correct answers. Additionally, we introduce an exploratory embedding that broadens the model's latent semantic space to diversify candidate generation and employs an entropy-based selection mechanism to choose the most confident answer automatically. Extensive experiments across three open-source LLMs, three retrieval methods, and four ODQA benchmarks demonstrate that EmbQA substantially outperforms recent baselines in both accuracy and efficiency.",
      "arxiv_url": "https://arxiv.org/abs/2503.01606",
      "pdf_url": "https://arxiv.org/pdf/2503.01606",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10452",
      "title": "DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language Models in Code Generation",
      "authors": [
        "Wenhao Hu",
        "Jinhao Duan",
        "Chunchen Wei",
        "Li Zhang",
        "Yue-feng Zhang",
        "Kaidi Xu"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) has significantly improved their performance in code generation tasks. However, existing code benchmarks remain static, consisting of fixed datasets with predefined problems. This makes them vulnerable to memorization during training, where LLMs recall specific test cases instead of generalizing to new problems, leading to data contamination and unreliable evaluation results. To address these issues, we introduce DynaCode, a dynamic, complexity-aware benchmark that overcomes the limitations of static datasets. DynaCode evaluates LLMs systematically using a complexity-aware metric, incorporating both code complexity and call-graph structures. DynaCode achieves large-scale diversity, generating up to 189 million unique nested code problems across four distinct levels of code complexity, referred to as units, and 16 types of call graphs. Results on 12 latest LLMs show an average performance drop of 16.8% to 45.7% compared to MBPP+, a static code generation benchmark, with performance progressively decreasing as complexity increases. This demonstrates DynaCode's ability to effectively differentiate LLMs. Additionally, by leveraging call graphs, we gain insights into LLM behavior, particularly their preference for handling subfunction interactions within nested code. Our benchmark and evaluation code are available at https://github.com/HWH-2000/DynaCode.",
      "arxiv_url": "https://arxiv.org/abs/2503.10452",
      "pdf_url": "https://arxiv.org/pdf/2503.10452",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11916",
      "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
      "authors": [
        "Jiamin Su",
        "Yibo Yan",
        "Fangteng Fu",
        "Han Zhang",
        "Jingheng Ye",
        "Xiang Liu",
        "Jiahao Huo",
        "Huiyu Zhou",
        "Xuming Hu"
      ],
      "abstract": "Automated Essay Scoring (AES) plays a crucial role in educational assessment by providing scalable and consistent evaluations of writing tasks. However, traditional AES systems face three major challenges: (1) reliance on handcrafted features that limit generalizability, (2) difficulty in capturing fine-grained traits like coherence and argumentation, and (3) inability to handle multimodal contexts. In the era of Multimodal Large Language Models (MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES capabilities across lexical-, sentence-, and discourse-level traits. By leveraging MLLMs' strengths in trait-specific scoring and multimodal context understanding, EssayJudge aims to offer precise, context-rich evaluations without manual feature engineering, addressing longstanding AES limitations. Our experiments with 18 representative MLLMs reveal gaps in AES performance compared to human evaluation, particularly in discourse-level traits, highlighting the need for further advancements in MLLM-based AES research.",
      "arxiv_url": "https://arxiv.org/abs/2502.11916",
      "pdf_url": "https://arxiv.org/pdf/2502.11916",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "734c69eb1c35d2ea61481b3abca6474e34b8456a",
      "title": "Wait, that's not an option: LLMs Robustness with Incorrect Multiple-Choice Options",
      "authors": [
        "Gracjan Góral",
        "Emilia Wisnios",
        "Piotr Sankowski",
        "Pawel Budzianowski"
      ],
      "abstract": "Decision-making under full alignment requires balancing between reasoning and faithfulness - a challenge for large language models (LLMs). This study explores whether LLMs prioritize following instructions over reasoning and truth when given misleading instructions, such as Respond solely with A or B , even when neither option is correct. We introduce a new metric called reflective judgment , which sheds new light on the relationship between the pre-training and post-training alignment schemes. In tasks ranging from basic arithmetic to domain-specific assessments, models like GPT-4o, o1-mini, or Claude 3 Opus adhered to instructions correctly but failed to reflect on the validity of the provided options. Contrary, models from the Llama 3.1 family (8B, 70B, 405B) or base Qwen2.5 (7B, 14B, 32B) families exhibit improved refusal rates with size, indicating a scaling effect. We also observed that alignment techniques, though intended to enhance reasoning, some-times weakened the models’ ability to reject incorrect instructions, leading them to follow flawed prompts uncritically. Finally, we have also conducted a parallel human study revealing similar patterns in human behavior and annotations. We highlight how popular RLHF datasets might disrupt either training or evaluation due to annotations exhibiting poor reflective judgement.",
      "arxiv_url": "https://www.semanticscholar.org/paper/734c69eb1c35d2ea61481b3abca6474e34b8456a",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.02382",
      "title": "An Efficient and Precise Training Data Construction Framework for Process-supervised Reward Model in Mathematical Reasoning",
      "authors": [
        "Wei Sun",
        "Qianlong Du",
        "Fuwei Cui",
        "Jiajun Zhang"
      ],
      "abstract": "Enhancing the mathematical reasoning capabilities of Large Language Models (LLMs) is of great scientific and practical significance. Researchers typically employ process-supervised reward models (PRMs) to guide the reasoning process, effectively improving the models'reasoning abilities. However, existing methods for constructing process supervision training data, such as manual annotation and per-step Monte Carlo estimation, are often costly or suffer from poor quality. To address these challenges, this paper introduces a framework called EpicPRM, which annotates each intermediate reasoning step based on its quantified contribution and uses an adaptive binary search algorithm to enhance both annotation precision and efficiency. Using this approach, we efficiently construct a high-quality process supervision training dataset named Epic50k, consisting of 50k annotated intermediate steps. Compared to other publicly available datasets, the PRM trained on Epic50k demonstrates significantly superior performance. Getting Epic50k at https://github.com/xiaolizh1/EpicPRM.",
      "arxiv_url": "https://arxiv.org/abs/2503.02382",
      "pdf_url": "https://arxiv.org/pdf/2503.02382",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2509.14270",
      "title": "SpeechWeave: Diverse Multilingual Synthetic Text & Audio Data Generation Pipeline for Training Text to Speech Models",
      "authors": [
        "Karan Dua",
        "Puneet Mittal",
        "Ranjeet Gupta",
        "Hitesh Laxmichand Patel"
      ],
      "abstract": "High-quality Text-to-Speech (TTS) model training requires extensive and diverse text and speech data. It is challenging to procure such data from real sources due to issues of domain specificity, licensing, and scalability. Large language models (LLMs) can certainly generate textual data, but they create repetitive text with insufficient variation in the prompt during the generation process. Another important aspect in TTS training data is text normalization. Tools for normalization might occasionally introduce anomalies or overlook valuable patterns, and thus impact data quality. Furthermore, it is also impractical to rely on voice artists for large scale speech recording in commercial TTS systems with standardized voices. To address these challenges, we propose SpeechWeave, a synthetic speech data generation pipeline that is capable of automating the generation of multilingual, domain-specific datasets for training TTS models. Our experiments reveal that our pipeline generates data that is 10-48% more diverse than the baseline across various linguistic and phonetic metrics, along with speaker-standardized speech audio while generating approximately 97% correctly normalized text. Our approach enables scalable, high-quality data generation for TTS training, improving diversity, normalization, and voice consistency in the generated datasets.",
      "arxiv_url": "https://arxiv.org/abs/2509.14270",
      "pdf_url": "https://arxiv.org/pdf/2509.14270",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-09-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00958",
      "title": "Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues",
      "authors": [
        "Youngmin Kim",
        "Jiwan Chung",
        "Jisoo Kim",
        "Sunghyun Lee",
        "Sangkyu Lee",
        "Junhyeok Kim",
        "Cheoljong Yang",
        "Youngjae Yu"
      ],
      "abstract": "Nonverbal communication is integral to human interaction, with gestures, facial expressions, and body language conveying critical aspects of intent and emotion. However, existing large language models (LLMs) fail to effectively incorporate these nonverbal elements, limiting their capacity to create fully immersive conversational experiences. We introduce MARS, a multimodal language model designed to understand and generate nonverbal cues alongside text, bridging this gap in conversational AI. Our key innovation is VENUS, a large-scale dataset comprising annotated videos with time-aligned text, facial expressions, and body language. Leveraging VENUS, we train MARS with a next-token prediction objective, combining text with vector-quantized nonverbal representations to achieve multimodal understanding and generation within a unified framework. Based on various analyses of the VENUS datasets, we validate its substantial scale and high effectiveness. Our quantitative and qualitative results demonstrate that MARS successfully generates text and nonverbal languages, corresponding to conversational input.",
      "arxiv_url": "https://arxiv.org/abs/2506.00958",
      "pdf_url": "https://arxiv.org/pdf/2506.00958",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.20279",
      "title": "Sudo Rm -rf Agentic_security",
      "authors": [
        "Sejin Lee",
        "Jian Kim",
        "Haon Park",
        "Ashkan Yousefpour",
        "Sangyoon Yu",
        "Min Song"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed as computer-use agents, autonomously performing tasks within real desktop or web environments. While this evolution greatly expands practical use cases for humans, it also creates serious security exposures. We present SUDO (Screen-based Universal Detox2Tox Offense), a novel attack framework that systematically bypasses refusal-trained safeguards in commercial computer-use agents, such as Claude for Computer Use. The core mechanism, Detox2Tox, transforms harmful requests (that agents initially reject) into seemingly benign requests via detoxification, secures detailed instructions from advanced vision language models (VLMs), and then reintroduces malicious content via toxification just before execution. Unlike conventional jailbreaks, SUDO iteratively refines its attacks based on a built-in refusal feedback, making it increasingly effective against robust policy filters. In extensive tests spanning 50 real-world tasks and multiple state-of-the-art VLMs, SUDO achieves a stark attack success rate of 24.41% (with no refinement), and up to 41.33% (by its iterative refinement) in Claude for Computer Use. By revealing these vulnerabilities and demonstrating the ease with which they can be exploited in real-world computing environments, this paper highlights an immediate need for robust, context-aware safeguards. WARNING: This paper includes harmful or offensive model outputs",
      "arxiv_url": "https://arxiv.org/abs/2503.20279",
      "pdf_url": "https://arxiv.org/pdf/2503.20279",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11147",
      "title": "RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning",
      "authors": [
        "Junhao Hu",
        "Wenrui Huang",
        "Weidong Wang",
        "Zhenwen Li",
        "Tiancheng Hu",
        "Zhixia Liu",
        "Xusheng Chen",
        "Tao Xie",
        "Yizhou Shan"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities across various domains, with recent advancements in challenging reasoning tasks such as mathematics and programming. However, solving reasoning tasks often requires an LLM to generate long sequences, incurring $O(N)$ time and memory complexities per token, where $N$ is the current sequence length. To reduce complexities, existing sparsity-based algorithms propose to retain Key-Value (KV) vectors, the intermediate representations of only the most critical tokens. However, these algorithms struggle with the\"impossible trinity\"of accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache budget, $L \\ll N$). To address the\"impossible trinity\", in this paper, we identify a new attention pattern during the decode stage of reasoning tasks, where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are utilized, and then become unimportant afterward. Based on this pattern, we propose a new algorithm RaaS that identifies milestone tokens and retains their KV vectors until they are no longer needed, achieving high accuracy with $O(L)$ time and $O(L)$ memory complexities.",
      "arxiv_url": "https://arxiv.org/abs/2502.11147",
      "pdf_url": "https://arxiv.org/pdf/2502.11147",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.15594",
      "title": "WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts",
      "authors": [
        "Negar Foroutan",
        "Angelika Romanou",
        "Matin Ansaripour",
        "J. Eisenschlos",
        "Karl Aberer",
        "R. Lebret"
      ],
      "abstract": "Documents are fundamental to preserving and disseminating information, often incorporating complex layouts, tables, and charts that pose significant challenges for automatic document understanding (DU). While vision-language large models (VLLMs) have demonstrated improvements across various tasks, their effectiveness in processing long-context vision inputs remains unclear. This paper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice questions (MCQs) designed to evaluate cross-modal reasoning over tables and charts extracted from 4,000 Wikipedia pages spanning seven distinct topics. Unlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring models to synthesize information from multiple modalities. We evaluate 12 state-of-the-art vision-language models, revealing that while proprietary models achieve ~70% accuracy when provided with direct context, their performance deteriorates significantly when retrieval from long documents is required. Among these, GPT-4-o is the only model exceeding 50% accuracy in this setting, whereas open-source models perform considerably worse, with a maximum accuracy of 27%. These findings underscore the challenges of long-context, multi-modal reasoning and establish WikiMixQA as a crucial benchmark for advancing document understanding research.",
      "arxiv_url": "https://arxiv.org/abs/2506.15594",
      "pdf_url": "https://arxiv.org/pdf/2506.15594",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "74359ff299321bf34892a114357c238e6fe29787",
      "title": "Extending LLM Context Window with Adaptive Grouped Positional Encoding: A Training-Free Method",
      "authors": [
        "Xinhao Xu",
        "Jiaxin Li",
        "Hui Chen",
        "Zijia Lin",
        "Jungong Han",
        "Guiguang Ding"
      ],
      "abstract": "Processing long input remains a significant challenge for large language models (LLMs) due to the scarcity of large-scale long-context training data and the high computational cost of training models for extended context windows. In this paper, we propose Ada ptive Gro uped P ositional E ncoding (AdaGroPE), a training-free, plug-and-play method to enhance long-context understanding in existing LLMs. AdaGroPE progressively increases the reuse count of relative positions as the distance grows and dynamically adapts the positional encoding mapping to sequence length, thereby fully exploiting the range of pre-trained position embeddings. Its design is consistent with the principles of rotary position embedding (RoPE) and aligns with human perception of relative distance, enabling robust performance in real-world settings with variable-length inputs. Extensive experiments across various benchmarks demonstrate that our AdaGroPE consistently achieves state-of-the-art performance, surpassing baseline methods and even outperforming LLMs inherently designed for long-context processing on certain tasks.",
      "arxiv_url": "https://www.semanticscholar.org/paper/74359ff299321bf34892a114357c238e6fe29787",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03310",
      "title": "The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing",
      "authors": [
        "Guillermo Marco",
        "Julio Gonzalo",
        "Víctor Fresno-Fernández"
      ],
      "abstract": "Recent studies comparing AI-generated and human-authored literary texts have produced conflicting results: some suggest AI already surpasses human quality, while others argue it still falls short. We start from the hypothesis that such divergences can be largely explained by genuine differences in how readers interpret and value literature, rather than by an intrinsic quality of the texts evaluated. Using five public datasets (1,471 stories, 101 annotators including critics, students, and lay readers), we (i) extract 17 reference-less textual features (e.g., coherence, emotional variance, average sentence length...); (ii) model individual reader preferences, deriving feature importance vectors that reflect their textual priorities; and (iii) analyze these vectors in a shared\"preference space\". Reader vectors cluster into two profiles: 'surface-focused readers' (mainly non-experts), who prioritize readability and textual richness; and 'holistic readers' (mainly experts), who value thematic development, rhetorical variety, and sentiment dynamics. Our results quantitatively explain how measurements of literary quality are a function of how text features align with each reader's preferences. These findings advocate for reader-sensitive evaluation frameworks in the field of creative text generation.",
      "arxiv_url": "https://arxiv.org/abs/2506.03310",
      "pdf_url": "https://arxiv.org/pdf/2506.03310",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18116",
      "title": "Bayesian Optimization for Controlled Image Editing via LLMs",
      "authors": [
        "Chengkun Cai",
        "Haoliang Liu",
        "Xu Zhao",
        "Zhongyu Jiang",
        "Tianfang Zhang",
        "Zongkai Wu",
        "Jenq-Neng Hwang",
        "Serge Belongie",
        "Lei Li"
      ],
      "abstract": "In the rapidly evolving field of image generation, achieving precise control over generated content and maintaining semantic consistency remain significant limitations, particularly concerning grounding techniques and the necessity for model fine-tuning. To address these challenges, we propose BayesGenie, an off-the-shelf approach that integrates Large Language Models (LLMs) with Bayesian Optimization to facilitate precise and user-friendly image editing. Our method enables users to modify images through natural language descriptions without manual area marking, while preserving the original image's semantic integrity. Unlike existing techniques that require extensive pre-training or fine-tuning, our approach demonstrates remarkable adaptability across various LLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian optimization strategy to automatically refine the inference process parameters, achieving high-precision image editing with minimal user intervention. Through extensive experiments across diverse scenarios, we demonstrate that our framework significantly outperforms existing methods in both editing accuracy and semantic preservation, as validated using different LLMs including Claude3 and GPT-4.",
      "arxiv_url": "https://arxiv.org/abs/2502.18116",
      "pdf_url": "https://arxiv.org/pdf/2502.18116",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.08292",
      "title": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them",
      "authors": [
        "Abhilasha Ravichander",
        "Shrusti Ghela",
        "David Wadden",
        "Yejin Choi"
      ],
      "abstract": "Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. However, measuring hallucination can be challenging, as having humans verify model generations on-the-fly is both expensive and time-consuming. In this work, we release HALoGEN, a comprehensive hallucination benchmark consisting of: (1) 10,923 prompts for generative models spanning nine domains including programming, scientific attribution, and summarization, and (2) automatic high-precision verifiers for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source. We use this framework to evaluate ~150,000 generations from 14 language models, finding that even the best-performing models are riddled with hallucinations (sometimes up to 86% of generated atomic facts depending on the domain). We further define a novel error classification for LLM hallucinations based on whether they likely stem from incorrect recollection of training data (Type A errors), or incorrect knowledge in training data (Type B errors), or are fabrication (Type C errors). We hope our framework provides a foundation to enable the principled study of why generative models hallucinate, and advances the development of trustworthy large language models.",
      "arxiv_url": "https://arxiv.org/abs/2501.08292",
      "pdf_url": "https://arxiv.org/pdf/2501.08292",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "748d154acc0a432890424ae483e2f2cb21039c76",
      "title": "Going Beyond Your Expectations in Latency Metrics for Simultaneous Speech Translation",
      "authors": [
        "Jorge Iranzo-Sánchez",
        "Javier Iranzo-Sánchez",
        "Adrià Giménez",
        "Jorge Civera"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/748d154acc0a432890424ae483e2f2cb21039c76",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22591",
      "title": "Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning",
      "authors": [
        "Tom B. Brown",
        "Benjamin Mann",
        "N. Ryder",
        "Aditya Ramesh",
        "Daniel M. Ziegler",
        "Jeffrey Wu",
        "Clemens Winter",
        "Chris Hesse",
        "Mark Chen",
        "Eric Sigler",
        "Ma-teusz Litwin",
        "Scott Gray",
        "Benjamin Chess",
        "J. Clark",
        "Christopher Berner",
        "Sam Mc-315 Candlish",
        "Alec Radford",
        "I. Sutskever",
        "K. Cobbe",
        "Vineet Kosaraju",
        "Mo Bavarian",
        "Hee-woo Jun",
        "Lukasz Kaiser",
        "Matthias Plappert",
        "Jerry Tworek",
        "Jacob Hilton",
        "Reiichiro Nakano"
      ],
      "abstract": "Although large language models demonstrate strong performance across various domains, they still struggle with numerous bad cases in mathematical reasoning. Previous approaches to learning from errors synthesize training data by solely extrapolating from isolated bad cases, thereby failing to generalize the extensive patterns inherent within these cases. This paper presents Self-Error-Instruct (SEI), a framework that addresses these model weaknesses and synthesizes more generalized targeted training data. Specifically, we explore a target model on two mathematical datasets, GSM8K and MATH, to pinpoint bad cases. Then, we generate error keyphrases for these cases based on the instructor model's (GPT-4o) analysis and identify error types by clustering these keyphrases. Next, we sample a few bad cases during each generation for each identified error type and input them into the instructor model, which synthesizes additional training data using a self-instruct approach. This new data is refined through a one-shot learning process to ensure that only the most effective examples are kept. Finally, we use these curated data to fine-tune the target model, iteratively repeating the process to enhance performance. We apply our framework to various models and observe improvements in their reasoning abilities across both in-domain and out-of-domain mathematics datasets. These results demonstrate the effectiveness of self-error instruction in improving LLMs' mathematical reasoning through error generalization.",
      "arxiv_url": "https://arxiv.org/abs/2505.22591",
      "pdf_url": "https://arxiv.org/pdf/2505.22591",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.19999",
      "title": "A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior",
      "authors": [
        "Francesco Ignazio Re",
        "Andreas Opedal",
        "Glib Manaiev",
        "Mario Giulianelli",
        "Ryan Cotterell"
      ],
      "abstract": "Reading is a process that unfolds across space and time, alternating between fixations where a reader focuses on a specific point in space, and saccades where a reader rapidly shifts their focus to a new point. An ansatz of psycholinguistics is that modeling a reader's fixations and saccades yields insight into their online sentence processing. However, standard approaches to such modeling rely on aggregated eye-tracking measurements and models that impose strong assumptions, ignoring much of the spatio-temporal dynamics that occur during reading. In this paper, we propose a more general probabilistic model of reading behavior, based on a marked spatio-temporal point process, that captures not only how long fixations last, but also where they land in space and when they take place in time. The saccades are modeled using a Hawkes process, which captures how each fixation excites the probability of a new fixation occurring near it in time and space. The duration time of fixation events is modeled as a function of fixation-specific predictors convolved across time, thus capturing spillover effects. Empirically, our Hawkes process model exhibits a better fit to human saccades than baselines. With respect to fixation durations, we observe that incorporating contextual surprisal as a predictor results in only a marginal improvement in the model's predictive accuracy. This finding suggests that surprisal theory struggles to explain fine-grained eye movements.",
      "arxiv_url": "https://arxiv.org/abs/2506.19999",
      "pdf_url": "https://arxiv.org/pdf/2506.19999",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "74e213aa78b90207652bf0e0f09b53c10b46fe00",
      "title": "Activation Steering Decoding: Mitigating Hallucination in Large Vision-Language Models through Bidirectional Hidden State Intervention",
      "authors": [
        "Zhiyang Chen",
        "Yousong Zhu",
        "Yufei Zhan",
        "Ailin Deng",
        "Zhirui Chen",
        "Bryan Hooi. 2024",
        "Seeing",
        "Chaoya Jiang",
        "Haiyang Xu",
        "Mengfan Dong",
        "Wei Chen",
        "Ming Ye",
        "Qinghao Yan",
        "Ji Ye",
        "Zhang",
        "Qirui Jiao",
        "Daoyuan Chen",
        "Yilun Huang",
        "Yaliang Li",
        "Linxi Zhao",
        "Yihe Deng",
        "Weitong Zhang",
        "Yiyang Zhou",
        "Chenhang Cui",
        "Rafael Rafailov",
        "Linjun Jaehong Yoon",
        "Zhun Zhang",
        "Chelsea Deng",
        "Mohit Finn",
        "Bansal",
        "Andy Zou",
        "Long Phan",
        "Sarah Chen",
        "James Campbell",
        "Mantas Yin",
        "Ann-Kathrin Mazeika",
        "Dombrowski"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities in multi-modal understanding, but they frequently suffer from hallucination - generating content inconsistent with visual inputs. In this work, we explore a novel perspective on hallucination mitigation by examining the intermediate activations of LVLMs during generation. Our investigation reveals that hallucinated content manifests as distinct, identifiable patterns in the model’s hidden state space. Motivated by this finding, we propose A ctivation S teering D ecoding (ASD), a training-free approach that mitigates hallucination through targeted intervention in the model’s intermediate activations. ASD operates by first identifying directional patterns of hallucination in the activation space using a small calibration set, then employing a contrast decoding mechanism that computes the difference between positive and negative steering predictions. This approach effectively suppresses hallucination patterns while preserving the model’s general capabilities. Extensive experiments demonstrate that our method significantly reduces hallucination across multiple benchmarks while maintaining performance on general visual understanding tasks. Notably, our approach requires no model re-training or architectural modifications, making it readily applicable to existing deployed models.",
      "arxiv_url": "https://www.semanticscholar.org/paper/74e213aa78b90207652bf0e0f09b53c10b46fe00",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7505858e40a6f653c01388e4d88fa10b1fa2bcce",
      "title": "FGDGNN: Fine-Grained Dynamic Graph Neural Network for Rumor Detection on Social Media",
      "authors": [
        "Mei Guo",
        "Chen Chen",
        "Chunyan Hou",
        "Yike Wu",
        "Xiao-hui Yuan"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/7505858e40a6f653c01388e4d88fa10b1fa2bcce",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02544",
      "title": "CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG",
      "authors": [
        "Yang Tian",
        "Fan Liu",
        "Jingyuan Zhang",
        "W. Victoria",
        "Yupeng Hu",
        "Liqiang Nie"
      ],
      "abstract": "Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to enhance Multimodal Large Language Models by incorporating externally retrieved multimodal knowledge, but it introduces two challenges: Parametric-Retrieved Knowledge Inconsistency (PRKI), where discrepancies between parametric and retrieved knowledge create uncertainty in determining reliability, and Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between visual and textual sources disrupts entity representation. To address these challenges, we propose Cross-source knowledge \\textbf{Re}conciliation for Multimodal RAG (CoRe-MMRAG), a novel end-to-end framework that effectively reconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage pipeline: it first generates an internal response from parametric knowledge, then selects the most relevant multimodal evidence via joint similarity assessment, generates an external response, and finally integrates both to produce a reliable answer. Additionally, a specialized training paradigm enhances knowledge source discrimination, multimodal integration, and unified answer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG achieves substantial improvements over baseline methods, achieving 5.6% and 9.3% performance gains on InfoSeek and Encyclopedic-VQA, respectively.",
      "arxiv_url": "https://arxiv.org/abs/2506.02544",
      "pdf_url": "https://arxiv.org/pdf/2506.02544",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Multi-Modal RAG",
        "RAG"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "75680a6b965af2c83380f33a4ed889bb1a11ad00",
      "title": "Faster Speculative Decoding via Effective Draft Decoder with Pruned Candidate Tree",
      "authors": [
        "Huanran Zheng",
        "Xiaoling Wang"
      ],
      "abstract": "Speculative Decoding (SD) is a promising method for reducing the inference latency of large language models (LLMs). A well-designed draft model and an effective draft candidate tree construction method are key to enhancing the acceleration effect of SD. In this paper, we first propose the Effective Draft De-coder (EDD), which treats the LLM as a powerful encoder and generates more accurate draft tokens by leveraging the encoding results as soft prompts. Furthermore, we use KL divergence instead of the standard cross-entropy loss to better align the draft model’s output with the LLM. Next, we introduce the Pruned Candidate Tree (PCT) algorithm to construct a more efficient candidate tree. Specifically, we found that the confidence scores predicted by the draft model are well-calibrated with the acceptance probability of draft tokens. Therefore, PCT estimates the expected time gain for each node in the candidate tree based on confidence scores and retains only the nodes that contribute to acceleration, pruning away redundant nodes. We conducted extensive experiments with various LLMs across four datasets. The experimental results verify the effectiveness of our proposed method, which significantly improves the performance of SD and reduces the inference latency of LLMs.",
      "arxiv_url": "https://www.semanticscholar.org/paper/75680a6b965af2c83380f33a4ed889bb1a11ad00",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10695",
      "title": "Introducing Verification Task of Set Consistency with Set-Consistency Energy Networks",
      "authors": [
        "Mooho Song",
        "H. Son",
        "Jay-Yoon Lee"
      ],
      "abstract": "Examining logical inconsistencies among multiple statements (such as collections of sentences or question-answer pairs) is a crucial challenge in machine learning, particularly for ensuring the safety and reliability of models. Traditional methods that rely on pairwise comparisons often fail to capture inconsistencies that only emerge when more than two statements are evaluated collectively. To address this gap, we introduce the task of set-consistency verification, an extension of natural language inference (NLI) that assesses the logical coherence of entire sets rather than isolated pairs. Building on this task, we present the Set-Consistency Energy Network (SC-Energy), a novel model that employs a contrastive loss framework to learn the compatibility among a collection of statements. Our approach not only efficiently verifies inconsistencies and pinpoints the specific statements responsible for logical contradictions, but also significantly outperforms existing methods including prompting-based LLM models. Furthermore, we release two new datasets: Set-LConVQA and Set-SNLI for set-consistency verification task.",
      "arxiv_url": "https://arxiv.org/abs/2503.10695",
      "pdf_url": "https://arxiv.org/pdf/2503.10695",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22165",
      "title": "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes",
      "authors": [
        "Bocheng Li",
        "Zhujin Gao",
        "Linli Xu"
      ],
      "abstract": "Diffusion models have emerged as a promising approach for text generation, with recent works falling into two main categories: discrete and continuous diffusion models. Discrete diffusion models apply token corruption independently using categorical distributions, allowing for different diffusion progress across tokens but lacking fine-grained control. Continuous diffusion models map tokens to continuous spaces and apply fine-grained noise, but the diffusion progress is uniform across tokens, limiting their ability to capture semantic nuances. To address these limitations, we propose \\textbf{\\underline{N}}on-simultan\\textbf{\\underline{e}}ous C\\textbf{\\underline{o}}ntinuous \\textbf{\\underline{Diff}}usion Models (NeoDiff), a novel diffusion model that integrates the strengths of both discrete and continuous approaches. NeoDiff introduces a Poisson diffusion process for the forward process, enabling a flexible and fine-grained noising paradigm, and employs a time predictor for the reverse process to adaptively modulate the denoising progress based on token semantics. Furthermore, NeoDiff utilizes an optimized schedule for inference to ensure more precise noise control and improved performance. Our approach unifies the theories of discrete and continuous diffusion models, offering a more principled and effective framework for text generation. Experimental results on several text generation tasks demonstrate NeoDiff's superior performance compared to baselines of non-autoregressive continuous and discrete diffusion models, iterative-based methods and autoregressive diffusion-based methods. These results highlight NeoDiff's potential as a powerful tool for generating high-quality text and advancing the field of diffusion-based text generation.",
      "arxiv_url": "https://arxiv.org/abs/2505.22165",
      "pdf_url": "https://arxiv.org/pdf/2505.22165",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "75b172dd940e3a2b4a537a6f808f827f93a46828",
      "title": "Making RALM Robust to Irrelevant Contexts via Layer Knowledge Guided Attention",
      "authors": [
        "Weijie Shi",
        "Hao Chen",
        "Jiaming Li",
        "Yao Zhao",
        "Yazhong Zhang",
        "Qijin Chen",
        "Jipeng Zhang",
        "Ruiyuan Zhang",
        "Jia Zhu",
        "Jiajie Xu",
        "Xiaofang Zhou"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/75b172dd940e3a2b4a537a6f808f827f93a46828",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.15475",
      "title": "Speculative Sampling via Exponential Races",
      "authors": [
        "S. Kobus",
        "Deniz Gündüz"
      ],
      "abstract": "Speculative decoding accelerates large language model inference using a smaller draft model. In this paper, we establish a surprising connection between speculative decoding and channel simulation, which aims at simulating a noisy channel using as few bits as possible. This connection allows us to provide an information-theoretic analysis of the speed up that can be achieved by speculative decoding. Leveraging this link, we derive an explicit relation between generation speed-up and the number of tokens $k$ generated by the draft model for large $k$, which serves as an upper bound for all $k$. We also propose a novel speculative decoding method via exponential race ERSD that matches state-of-the-art performance.",
      "arxiv_url": "https://arxiv.org/abs/2504.15475",
      "pdf_url": "https://arxiv.org/pdf/2504.15475",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.13369",
      "title": "Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions",
      "authors": [
        "Wan Ju Kang",
        "Eunki Kim",
        "Na Min An",
        "Sangryul Kim",
        "Haemin Choi",
        "Ki Hoon Kwak",
        "James Thorne"
      ],
      "abstract": "Often, the needs and visual abilities differ between the annotator group and the end user group. Generating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain. Sighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly, bias-prone, and somewhat lacking by BLV standards. In this study, we ask sighted individuals to assess -- rather than produce -- diagram descriptions generated by vision-language models (VLM) that have been guided with latent supervision via a multi-pass inference. The sighted assessments prove effective and useful to professional educators who are themselves BLV and teach visually impaired learners. We release Sightation, a collection of diagram description datasets spanning 5k diagrams and 137k samples for completion, preference, retrieval, question answering, and reasoning training purposes and demonstrate their fine-tuning potential in various downstream tasks.",
      "arxiv_url": "https://arxiv.org/abs/2503.13369",
      "pdf_url": "https://arxiv.org/pdf/2503.13369",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.08648",
      "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities",
      "authors": [
        "Savya Khosla",
        "Aditi Tiwari",
        "Kushal Kafle",
        "Simon Jenni",
        "Handong Zhao",
        "John P. Collomosse",
        "Jing Shi"
      ],
      "abstract": "While originally designed for unidirectional generative modeling, decoder-only large language models (LLMs) are increasingly being adapted for bidirectional modeling. However, unidirectional and bidirectional models are typically trained separately with distinct objectives (generation and representation learning). This separation overlooks the opportunity for developing a more versatile language model and for these objectives to complement each other. In this work, we propose MAGNET, a method for adapting decoder-only LLMs to generate robust representations and infill missing text spans. MAGNET employs three self-supervised training objectives and introduces an attention mechanism that combines bidirectional and causal attention, enabling unified training across all objectives. Our results demonstrate that LLMs adapted with MAGNET (1) surpass strong text encoders on token-level and sentence-level representation learning tasks, (2) generate contextually appropriate text infills by leveraging past and future contexts, (3) perform open-ended text generation without excessive repetition of words or phrases, and (4) preserve the knowledge and reasoning capability gained by the LLM during pretraining.",
      "arxiv_url": "https://arxiv.org/abs/2501.08648",
      "pdf_url": "https://arxiv.org/pdf/2501.08648",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.10893",
      "title": "ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search",
      "authors": [
        "Yize Zhang",
        "Tianshu Wang",
        "Sirui Chen",
        "Kun Wang",
        "Xingyu Zeng",
        "Hongyu Lin",
        "Xianpei Han",
        "Le Sun",
        "Chaochao Lu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive capabilities and are receiving increasing attention to enhance their reasoning through scaling test--time compute. However, their application in open--ended, knowledge--intensive, complex reasoning scenarios is still limited. Reasoning--oriented methods struggle to generalize to open--ended scenarios due to implicit assumptions of complete world knowledge. Meanwhile, knowledge--augmented reasoning (KAR) methods fail to address two core challenges: 1) error propagation, where errors in early steps cascade through the chain, and 2) verification bottleneck, where the explore--exploit tradeoff arises in multi--branch decision processes. To overcome these limitations, we introduce ARise, a novel framework that integrates risk assessment of intermediate reasoning states with dynamic retrieval--augmented generation (RAG) within a Monte Carlo tree search paradigm. This approach enables effective construction and optimization of reasoning plans across multiple maintained hypothesis branches. Experimental results show that ARise significantly outperforms the state--of--the--art KAR methods by up to 23.10%, and the latest RAG-equipped large reasoning models by up to 25.37%. Our project page is at https://opencausalab.github.io/ARise.",
      "arxiv_url": "https://arxiv.org/abs/2504.10893",
      "pdf_url": "https://arxiv.org/pdf/2504.10893",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-04-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.19741",
      "title": "Basic Reading Distillation",
      "authors": [
        "Zhi-Ming Zhou",
        "Sirui Miao",
        "Xiangyu Duan",
        "Hao Yang",
        "Min Zhang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable abilities in various natural language processing areas, but they demand high computation resources which limits their deployment in real-world. Distillation is one technique to solve this problem through either knowledge distillation or task distillation. Both distillation approaches train small models to imitate specific features of LLMs, but they all neglect basic reading education for small models on generic texts that are \\emph{unrelated} to downstream tasks. In this paper, we propose basic reading distillation (BRD) which educates a small model to imitate LLMs basic reading behaviors, such as named entity recognition, question raising and answering, on each sentence. After such basic education, we apply the small model on various tasks including language inference benchmarks and BIG-bench tasks. It shows that the small model can outperform or perform comparable to over 20x bigger LLMs. Analysis reveals that BRD effectively influences the probability distribution of the small model, and has orthogonality to either knowledge distillation or task distillation.",
      "arxiv_url": "https://arxiv.org/abs/2507.19741",
      "pdf_url": "https://arxiv.org/pdf/2507.19741",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "763027fd058877e271cdf7542ea0dafdf7879b4b",
      "title": "Powerformer: Efficient and High-Accuracy Privacy-Preserving Language Model with Homomorphic Encryption",
      "authors": [
        "Dongjin Park",
        "Eunsang Lee",
        "Joon-Woo Lee"
      ],
      "abstract": "We propose Powerformer , an efficient homomorphic encryption (HE)-based privacy-preserving language model (PPLM) designed to reduce computational overhead while maintaining model performance. Powerformer incorporates three key techniques to optimize encrypted computations: 1) A novel distillation technique that replaces softmax and layer normalization with computationally efficient power and linear functions, ensuring no performance degradation while enabling seamless encrypted computation. 2) A pseudo-sign composite approximation method that accurately approximates GELU and tanh functions with minimal computational overhead. 3) A homomorphic matrix multiplication algorithm specifically optimized for Transformer models, enhancing efficiency in encrypted environments. By integrating these techniques, Powerformer based on the BERT-base model achieves a 45% reduction in computation time compared to the state-of-the-art HE-based PPLM without any loss in accuracy.",
      "arxiv_url": "https://www.semanticscholar.org/paper/763027fd058877e271cdf7542ea0dafdf7879b4b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00418",
      "title": "Dual Debiasing for Noisy In-Context Learning for Text Generation",
      "authors": [
        "Siqi Liang",
        "Sumyeong Ahn",
        "Paramveer S. Dhillon",
        "Jiayu Zhou"
      ],
      "abstract": "In-context learning (ICL) relies heavily on high-quality demonstrations drawn from large annotated corpora. Existing approaches detect noisy annotations by ranking local perplexities, presuming that noisy samples yield higher perplexities than their clean counterparts. However, this assumption breaks down when the noise ratio is high and many demonstrations are flawed. We re-examine the perplexity-based paradigm for text generation under noisy annotations, highlighting two sources of bias in perplexity: the annotation itself and the domain-specific knowledge inherent in large language models (LLMs). To overcome these biases, we introduce a dual-debiasing framework that uses synthesized neighbors to explicitly correct perplexity estimates, yielding a robust Sample Cleanliness Score. This metric uncovers absolute sample cleanliness regardless of the overall corpus noise level. Extensive experiments demonstrate our method's superior noise-detection capabilities and show that its final ICL performance is comparable to that of a fully clean demonstration corpus. Moreover, our approach remains robust even when noise ratios are extremely high.",
      "arxiv_url": "https://arxiv.org/abs/2506.00418",
      "pdf_url": "https://arxiv.org/pdf/2506.00418",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.09440",
      "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture",
      "authors": [
        "GigaChat team Mamedov Valentin",
        "Evgenii Kosarev",
        "Gregory Leleytner",
        "Ilya Shchuckin",
        "Valeriy Berezovskiy",
        "Daniil Smirnov",
        "Dmitry Kozlov",
        "Sergei Averkiev",
        "Lukyanenko Ivan",
        "Aleksandr Proshunin",
        "Ainur Israfilova",
        "Ivan Baskov",
        "Artem Chervyakov",
        "Emil Shakirov",
        "Mikhail Kolesov",
        "Daria Khomich",
        "Darya Latortseva",
        "Sergei Porkhun",
        "Yury Fedorov",
        "O. Kutuzov",
        "Polina Kudriavtseva",
        "S. Soldatova",
        "Kolodin Egor",
        "Stanislav Pyatkin",
        "Dzmitry Menshykh",
        "Grafov Sergei",
        "Eldar Damirov",
        "Karlov Vladimir",
        "Ruslan Gaitukiev",
        "Arkadiy Shatenov",
        "Alena Fenogenova",
        "Nikita Savushkin",
        "Fedor Minkin"
      ],
      "abstract": "Generative large language models (LLMs) have become crucial for modern NLP research and applications across various languages. However, the development of foundational models specifically tailored to the Russian language has been limited, primarily due to the significant computational resources required. This paper introduces the GigaChat family of Russian LLMs, available in various sizes, including base models and instruction-tuned versions. We provide a detailed report on the model architecture, pre-training process, and experiments to guide design choices. In addition, we evaluate their performance on Russian and English benchmarks and compare GigaChat with multilingual analogs. The paper presents a system demonstration of the top-performing models accessible via an API, a Telegram bot, and a Web interface. Furthermore, we have released three open GigaChat models in open-source (https://huggingface.co/ai-sage), aiming to expand NLP research opportunities and support the development of industrial solutions for the Russian language.",
      "arxiv_url": "https://arxiv.org/abs/2506.09440",
      "pdf_url": "https://arxiv.org/pdf/2506.09440",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13063",
      "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity",
      "authors": [
        "Yuri Kuratov",
        "M. Arkhipov",
        "A. Bulatov",
        "M. Burtsev"
      ],
      "abstract": "A range of recent works addresses the problem of compression of sequence of tokens into a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches are focused on reduction of the amount of compute in existing language models rather than minimization of number of bits needed to store text. Despite relying on powerful models as encoders, the maximum attainable lossless compression ratio is typically not higher than x10. This fact is highly intriguing because, in theory, the maximum information capacity of large real-valued vectors is far beyond the presented rates even for 16-bit precision and a modest vector size. In this work, we explore the limits of compression by replacing the encoder with a per-sample optimization procedure. We show that vectors with compression ratios up to x1500 exist, which highlights two orders of magnitude gap between existing and practically attainable solutions. Furthermore, we empirically show that the compression limits are determined not by the length of the input but by the amount of uncertainty to be reduced, namely, the cross-entropy loss on this sequence without any conditioning. The obtained limits highlight the substantial gap between the theoretical capacity of input embeddings and their practical utilization, suggesting significant room for optimization in model design.",
      "arxiv_url": "https://arxiv.org/abs/2502.13063",
      "pdf_url": "https://arxiv.org/pdf/2502.13063",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12913",
      "title": "GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning",
      "authors": [
        "Sifan Zhou",
        "Shuo Wang",
        "Zhihang Yuan",
        "Mingjia Shi",
        "Yuzhang Shang",
        "Dawei Yang"
      ],
      "abstract": "Large Language Models (LLMs) fine-tuning technologies have achieved remarkable results. However, traditional LLM fine-tuning approaches face significant challenges: they require large Floating Point (FP) computation, raising privacy concerns when handling sensitive data, and are impractical for resource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT) techniques reduce trainable parameters, their reliance on floating-point arithmetic creates fundamental incompatibilities with edge hardware. In this work, we introduce a novel framework for on-device LLM fine-tuning that eliminates the need for floating-point operations in both inference and training, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer format, which efficiently represents model parameters in integer format using shared exponents among parameter groups. When combined with LoRA-like adapters, this enables fully integer-based fine-tuning that is both memory and compute efficient. We demonstrate that our approach achieves accuracy comparable to BF16-based fine-tuning while significantly reducing 1.85x memory usage. Moreover, compared to FP8, our method can reduce 5x power consumption and 11x chip area with same performance, making large-scale model adaptation feasible on edge devices.",
      "arxiv_url": "https://arxiv.org/abs/2502.12913",
      "pdf_url": "https://arxiv.org/pdf/2502.12913",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13131",
      "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis",
      "authors": [
        "Feng Luo",
        "Rui Yang",
        "Hao Sun",
        "Chunyuan Deng",
        "Jiarui Yao",
        "Jingyan Shen",
        "Huan Zhang",
        "Hanjie Chen"
      ],
      "abstract": "Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment. Our code is available at https://github.com/amandaluof/DRMs.",
      "arxiv_url": "https://arxiv.org/abs/2502.13131",
      "pdf_url": "https://arxiv.org/pdf/2502.13131",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.20527",
      "title": "StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of 7,000+ Real-World APIs",
      "authors": [
        "Zhicheng Guo",
        "Sijie Cheng",
        "Yuchen Niu",
        "Hao Wang",
        "Sicheng Zhou",
        "Wenbing Huang",
        "Yang Liu"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) has spurred significant interest in tool learning, where LLMs are augmented with external tools to tackle complex tasks. However, existing tool environments face challenges in balancing stability, scalability, and realness, particularly for benchmarking purposes. To address this problem, we propose MirrorAPI, a novel framework that trains specialized LLMs to accurately simulate real API responses, effectively acting as\"mirrors\"to tool environments. Using a comprehensive dataset of request-response pairs from 7,000+ APIs, we employ supervised fine-tuning and chain-of-thought reasoning to enhance simulation fidelity. MirrorAPI achieves superior accuracy and stability compared to state-of-the-art methods, as demonstrated by its performance on the newly constructed MirrorAPI-Bench and its integration into StableToolBench.",
      "arxiv_url": "https://arxiv.org/abs/2503.20527",
      "pdf_url": "https://arxiv.org/pdf/2503.20527",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "76f759323f58fd535fa13080709d9ef54f4ee9b4",
      "title": "Drift: Enhancing LLM Faithfulness in Rationale Generation via Dual-Reward Probabilistic Inference",
      "authors": [
        "Jiazheng Li",
        "Hanqi Yan",
        "Yulan He"
      ],
      "abstract": "As Large Language Models (LLMs) are increasingly applied to complex reasoning tasks, achieving both accurate task performance and faithful explanations becomes crucial. However, LLMs often generate unfaithful explanations, partly because they do not consistently adhere closely to the provided context. Existing approaches to this problem either rely on superficial calibration methods, such as decomposed Chain-of-Thought prompting, or require costly retraining to improve model faith-fulness. In this work, we propose a probabilistic inference paradigm that leverages task-specific and lookahead rewards to ensure that LLM-generated rationales are more faithful to model decisions and align better with input context. These rewards are derived from a domain-specific proposal distribution, allowing for optimized sequential Monte Carlo approximations. Our evaluations across three different reasoning tasks show that this method, which allows for controllable generation during inference, improves both accuracy and faithfulness of LLMs. This method offers a promising path towards making LLMs more reliable for reasoning tasks without sacrificing performance.",
      "arxiv_url": "https://www.semanticscholar.org/paper/76f759323f58fd535fa13080709d9ef54f4ee9b4",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "77184b8fc9955b021b9d6329846fbdd071b14434",
      "title": "VMLU Benchmarks: A comprehensive benchmark toolkit for Vietnamese LLMs",
      "authors": [
        "Cuc Thi Bui",
        "Nguyen Truong Son",
        "T. Truong",
        "V. Phung",
        "Pham Nhut Huy",
        "Hoang Anh Le",
        "Quoc Huu Van",
        "Phong Nguyen-Thuan Do",
        "Van Le Tran Truc",
        "D. Chau",
        "Le-Minh Nguyen"
      ],
      "abstract": "The evolution of Large Language Models (LLMs) has underscored the necessity for benchmarks designed for various languages and cultural contexts. To address this need for Vietnamese, we present the first Vietnamese Multitask Language Understanding ( VMLU ) Benchmarks. The VMLU benchmarks consist of four datasets that assess different capabilities of LLMs, including general knowledge, reading comprehension, reasoning, and conversational skills. This paper also provides an insightful overview of the current state of some dominant LLMs, such as Llama-3 (Grattafiori et al., 2024), Qwen2.5 (Qwen et al., 2025), and GPT-4, highlighting their performances and limitations when measured against these benchmarks. Furthermore, we provide insights into how prompt design can influence VMLU’s evaluation outcomes, as well as suggest that open-source LLMs can serve as effective, cost-efficient evaluators within the Vietnamese context. By offering a comprehensive and accessible benchmarking framework, the VMLU Benchmarks aim to foster the development and fine-tuning of Vietnamese LLMs, thereby establishing a foundation for their practical applications in language-specific domains.",
      "arxiv_url": "https://www.semanticscholar.org/paper/77184b8fc9955b021b9d6329846fbdd071b14434",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "771c38980af4be312b18cc60ee61bc246a5d34b4",
      "title": "Synthetic Data in the Era of Large Language Models",
      "authors": [
        "Vijay Viswanathan",
        "Xiang Yue",
        "Alisa Liu",
        "Yizhong Wang",
        "Graham Neubig"
      ],
      "abstract": "in natural language processing",
      "arxiv_url": "https://www.semanticscholar.org/paper/771c38980af4be312b18cc60ee61bc246a5d34b4",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11656",
      "title": "Uncovering the Impact of Chain-of-Thought Reasoning for Direct Preference Optimization: Lessons from Text-to-SQL",
      "authors": [
        "Hanbing Liu",
        "Haoyang Li",
        "Xiaokang Zhang",
        "Ruotong Chen",
        "Haiyong Xu",
        "Tian Tian",
        "Qi Qi",
        "Jing Zhang"
      ],
      "abstract": "Direct Preference Optimization (DPO) has proven effective in complex reasoning tasks like math word problems and code generation. However, when applied to Text-to-SQL datasets, it often fails to improve performance and can even degrade it. Our investigation reveals the root cause: unlike math and code tasks, which naturally integrate Chain-of-Thought (CoT) reasoning with DPO, Text-to-SQL datasets typically include only final answers (gold SQL queries) without detailed CoT solutions. By augmenting Text-to-SQL datasets with synthetic CoT solutions, we achieve, for the first time, consistent and significant performance improvements using DPO. Our analysis shows that CoT reasoning is crucial for unlocking DPO's potential, as it mitigates reward hacking, strengthens discriminative capabilities, and improves scalability. These findings offer valuable insights for building more robust Text-to-SQL models. To support further research, we publicly release the code and CoT-enhanced datasets.",
      "arxiv_url": "https://arxiv.org/abs/2502.11656",
      "pdf_url": "https://arxiv.org/pdf/2502.11656",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.02802",
      "title": "Consistent Client Simulation for Motivational Interviewing-based Counseling",
      "authors": [
        "Yizhe Yang",
        "Palakorn Achananuparp",
        "Heyan Huang",
        "Jing Jiang",
        "John Pinto",
        "Jenny Giam",
        "Kit Phey Leng",
        "Nicholas Gabriel Lim",
        "Cameron Tan Shi Ern",
        "Ee-Peng Lim"
      ],
      "abstract": "Simulating human clients in mental health counseling is crucial for training and evaluating counselors (both human or simulated) in a scalable manner. Nevertheless, past research on client simulation did not focus on complex conversation tasks such as mental health counseling. In these tasks, the challenge is to ensure that the client's actions (i.e., interactions with the counselor) are consistent with with its stipulated profiles and negative behavior settings. In this paper, we propose a novel framework that supports consistent client simulation for mental health counseling. Our framework tracks the mental state of a simulated client, controls its state transitions, and generates for each state behaviors consistent with the client's motivation, beliefs, preferred plan to change, and receptivity. By varying the client profile and receptivity, we demonstrate that consistent simulated clients for different counseling scenarios can be effectively created. Both our automatic and expert evaluations on the generated counseling sessions also show that our client simulation method achieves higher consistency than previous methods.",
      "arxiv_url": "https://arxiv.org/abs/2502.02802",
      "pdf_url": "https://arxiv.org/pdf/2502.02802",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7751bd5d3c8007c43828bac4efda930fbdfff139",
      "title": "Answering Complex Geographic Questions by Adaptive Reasoning with Visual Context and External Commonsense Knowledge",
      "authors": [
        "Fan Li",
        "Jianxing Yu",
        "Jielong Tang",
        "Wenqing Chen",
        "Hanjiang Lai",
        "Yanghui Rao",
        "Jian Yin"
      ],
      "abstract": "This paper focuses on a new task of answering geographic reasoning questions based on the given image (called GeoVQA ). Unlike traditional VQA tasks, GeoVQA asks for details about the image-related culture, landscape, etc. This requires not only the identification of the objects in the image, their properties and relations, but also the understanding of the geographic knowledge of the objects, such as location, transportation, landmark, cuisine, etc. This background knowledge does not explicitly appear in the image, nor is there an extra-textual description. Without this missing but necessary knowledge, it is difficult for existing matching-based methods to infer the correct answer. To tackle these challenges, we pro-pose a new geographic reasoning framework for our task. We first analyze the image and describe its fine-grained content by text and keywords using a multi-modal retrieval augmented technique, so as to deduce an answer in a unified textual modality. Next, we retrieve the crucial geographic commonsense knowledge. To reduce the retrieval complexity, we design a dynamic method that can adaptively collect the relevant clues for each reasoning step. The step in the incorrect direction will be pruned according to some judgment criteria. The remaining steps can help us form a reasoning chain to derive a",
      "arxiv_url": "https://www.semanticscholar.org/paper/7751bd5d3c8007c43828bac4efda930fbdfff139",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Multi-Modal RAG",
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.15282",
      "title": "Exploring In-Image Machine Translation with Real-World Background",
      "authors": [
        "Yanzhi Tian",
        "Zeming Liu",
        "Zhengyang Liu",
        "Yuhang Guo"
      ],
      "abstract": "In-Image Machine Translation (IIMT) aims to translate texts within images from one language to another. Previous research on IIMT was primarily conducted on simplified scenarios such as images of one-line text with black font in white backgrounds, which is far from reality and impractical for applications in the real world. To make IIMT research practically valuable, it is essential to consider a complex scenario where the text backgrounds are derived from real-world images. To facilitate research of complex scenario IIMT, we design an IIMT dataset that includes subtitle text with real-world background. However previous IIMT models perform inadequately in complex scenarios. To address the issue, we propose the DebackX model, which separates the background and text-image from the source image, performs translation on text-image directly, and fuses the translated text-image with the background, to generate the target image. Experimental results show that our model achieves improvements in both translation quality and visual effect.",
      "arxiv_url": "https://arxiv.org/abs/2505.15282",
      "pdf_url": "https://arxiv.org/pdf/2505.15282",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.18102",
      "title": "InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating",
      "authors": [
        "Fuyu Wang",
        "Jiangtong Li",
        "Kun Zhu",
        "Changjun Jiang"
      ],
      "abstract": "With the rapid advancements in large language models (LLMs), debating tasks, such as argument quality assessment and debate process simulation, have made significant progress. However, existing LLM-based debating systems focus on responding to specific arguments while neglecting objective assessments such as authenticity and logical validity. Furthermore, these systems lack a structured approach to optimize across various dimensions$-$including evaluation metrics, chain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby limiting their effectiveness. To address these interconnected challenges, we propose a dual-component framework: (1) $\\textbf{InspireScore}$, a novel evaluation system that establishes a multi-dimensional assessment architecture incorporating four subjective criteria (emotional appeal, argument clarity, argument arrangement, and topic relevance) alongside two objective metrics (fact authenticity and logical validity); and (2) $\\textbf{InspireDebate}$, an optimized debating framework employing a phased optimization approach through CoT reasoning enhancement, multi-dimensional Direct Preference Optimization (DPO), and real-time knowledge grounding via web-based Retrieval Augmented Generation (Web-RAG). Empirical evaluations demonstrate that $\\textbf{InspireScore}$ achieves 44$\\%$ higher correlation with expert judgments compared to existing methods, while $\\textbf{InspireDebate}$ shows significant improvements, outperforming baseline models by 57$\\%$. Source code is available at https://github.com/fywang12/InspireDebate.",
      "arxiv_url": "https://arxiv.org/abs/2506.18102",
      "pdf_url": "https://arxiv.org/pdf/2506.18102",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-06-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.06838",
      "title": "Shifting from Ranking to Set Selection for Retrieval Augmented Generation",
      "authors": [
        "Dahyun Lee",
        "Yongrae Jo",
        "Haeju Park",
        "Moontae Lee"
      ],
      "abstract": "Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved passages are not only individually relevant but also collectively form a comprehensive set. Existing approaches primarily rerank top-k passages based on their individual relevance, often failing to meet the information needs of complex queries in multi-hop question answering. In this work, we propose a set-wise passage selection approach and introduce SETR, which explicitly identifies the information requirements of a query through Chain-of-Thought reasoning and selects an optimal set of passages that collectively satisfy those requirements. Experiments on multi-hop RAG benchmarks show that SETR outperforms both proprietary LLM-based rerankers and open-source baselines in terms of answer correctness and retrieval quality, providing an effective and efficient alternative to traditional rerankers in RAG systems. The code is available at https://github.com/LGAI-Research/SetR",
      "arxiv_url": "https://arxiv.org/abs/2507.06838",
      "pdf_url": "https://arxiv.org/pdf/2507.06838",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-07-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7857ef0e705e8491f301116cdaed6dc94d741c7d",
      "title": "What Language Do Non-English-Centric Large Language Models Think in?",
      "authors": [
        "Chengzhi Zhong",
        "Qianying Liu",
        "Fei Cheng",
        "Junfeng Jiang",
        "Zhen Wan",
        "Chenhui Chu",
        "Yugo Murawaki",
        "Sadao Kurohashi"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/7857ef0e705e8491f301116cdaed6dc94d741c7d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.04113",
      "title": "Advancing Zero-shot Text-to-Speech Intelligibility across Diverse Domains via Preference Alignment",
      "authors": [
        "Xueyao Zhang",
        "Yuancheng Wang",
        "Chaoren Wang",
        "Ziniu Li",
        "Zhuo Chen",
        "Zhizheng Wu"
      ],
      "abstract": "Modern zero-shot text-to-speech (TTS) systems, despite using extensive pre-training, often struggle in challenging scenarios such as tongue twisters, repeated words, code-switching, and cross-lingual synthesis, leading to intelligibility issues. To address these limitations, this paper leverages preference alignment techniques, which enable targeted construction of out-of-pretraining-distribution data to enhance performance. We introduce a new dataset, named the Intelligibility Preference Speech Dataset (INTP), and extend the Direct Preference Optimization (DPO) framework to accommodate diverse TTS architectures. After INTP alignment, in addition to intelligibility, we observe overall improvements including naturalness, similarity, and audio quality for multiple TTS models across diverse domains. Based on that, we also verify the weak-to-strong generalization ability of INTP for more intelligible models such as CosyVoice 2 and Ints. Moreover, we showcase the potential for further improvements through iterative alignment based on Ints. Audio samples are available at https://intalign.github.io/.",
      "arxiv_url": "https://arxiv.org/abs/2505.04113",
      "pdf_url": "https://arxiv.org/pdf/2505.04113",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01751",
      "title": "SAKE: Steering Activations for Knowledge Editing",
      "authors": [
        "Marco Scialanga",
        "Thibault Laugel",
        "Vincent Grari",
        "Marcin Detyniecki"
      ],
      "abstract": "As Large Langue Models have been shown to memorize real-world facts, the need to update this knowledge in a controlled and efficient manner arises. Designed with these constraints in mind, Knowledge Editing (KE) approaches propose to alter specific facts in pretrained models. However, they have been shown to suffer from several limitations, including their lack of contextual robustness and their failure to generalize to logical implications related to the fact. To overcome these issues, we propose SAKE, a steering activation method that models a fact to be edited as a distribution rather than a single prompt. Leveraging Optimal Transport, SAKE alters the LLM behavior over a whole fact-related distribution, defined as paraphrases and logical implications. Several numerical experiments demonstrate the effectiveness of this method: SAKE is thus able to perform more robust edits than its existing counterparts.",
      "arxiv_url": "https://arxiv.org/abs/2503.01751",
      "pdf_url": "https://arxiv.org/pdf/2503.01751",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13311",
      "title": "Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors",
      "authors": [
        "Jian Wang",
        "Yinpei Dai",
        "Yichi Zhang",
        "Ziqiao Ma",
        "Wenjie Li",
        "Joyce Chai"
      ],
      "abstract": "Intelligent tutoring agents powered by large language models (LLMs) have been increasingly explored to deliver personalized knowledge in areas such as language learning and science education. However, their capabilities in guiding users to solve complex real-world tasks remain underexplored. To address this limitation, in this work, we focus on coding tutoring, a challenging problem that requires tutors to proactively guide students towards completing predefined coding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER), which combines knowledge tracing to estimate a student's knowledge state and turn-by-turn verification to ensure effective guidance toward task completion. We introduce DICT, an automatic evaluation protocol that assesses tutor agents using controlled student simulation and code generation tests. Extensive experiments reveal the challenges of coding tutoring and demonstrate that TRAVER achieves a significantly higher success rate. Although we use code tutoring as an example in this paper, our approach can be extended beyond coding, providing valuable insights into advancing tutoring agents for human task learning.",
      "arxiv_url": "https://arxiv.org/abs/2502.13311",
      "pdf_url": "https://arxiv.org/pdf/2502.13311",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19015",
      "title": "Can Multimodal Large Language Models Understand Spatial Relations?",
      "authors": [
        "Jingping Liu",
        "Ziyan Liu",
        "Zhedong Cen",
        "Yan Zhou",
        "Yinan Zou",
        "Weiyan Zhang",
        "Haiyun Jiang",
        "Tong Ruan"
      ],
      "abstract": "Spatial relation reasoning is a crucial task for multimodal large language models (MLLMs) to understand the objective world. However, current benchmarks have issues like relying on bounding boxes, ignoring perspective substitutions, or allowing questions to be answered using only the model's prior knowledge without image understanding. To address these issues, we introduce SpatialMQA, a human-annotated spatial relation reasoning benchmark based on COCO2017, which enables MLLMs to focus more on understanding images in the objective world. To ensure data quality, we design a well-tailored annotation procedure, resulting in SpatialMQA consisting of 5,392 samples. Based on this benchmark, a series of closed- and open-source MLLMs are implemented and the results indicate that the current state-of-the-art MLLM achieves only 48.14% accuracy, far below the human-level accuracy of 98.40%. Extensive experimental analyses are also conducted, suggesting the future research directions. The benchmark and codes are available at https://github.com/ziyan-xiaoyu/SpatialMQA.git.",
      "arxiv_url": "https://arxiv.org/abs/2505.19015",
      "pdf_url": "https://arxiv.org/pdf/2505.19015",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "78edabd2289b5a01c2fbc8abab9f2377b2193ac3",
      "title": "Cross-lingual Multimodal Sentiment Analysis for Low-Resource Languages via Language Family Disentanglement and Rethinking Transfer",
      "authors": [
        "Long Chen",
        "Shuoyu Guan",
        "Xiaohua Huang",
        "Wen-Jing Wang",
        "Cai Xu",
        "Ziyu Guan",
        "Wei Zhao"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/78edabd2289b5a01c2fbc8abab9f2377b2193ac3",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17510",
      "title": "Recurrent Knowledge Identification and Fusion for Language Model Continual Learning",
      "authors": [
        "Yujie Feng",
        "Xujia Wang",
        "Zexin Lu",
        "Shenghong Fu",
        "Guangyuan Shi",
        "Yongxin Xu",
        "Yasha Wang",
        "Philip S. Yu",
        "Xu Chu",
        "Xiao-Ming Wu"
      ],
      "abstract": "Continual learning (CL) is crucial for deploying large language models (LLMs) in dynamic real-world environments without costly retraining. While recent model ensemble and model merging methods guided by parameter importance have gained popularity, they often struggle to balance knowledge transfer and forgetting, mainly due to the reliance on static importance estimates during sequential training. In this paper, we present Recurrent-KIF, a novel CL framework for Recurrent Knowledge Identification and Fusion, which enables dynamic estimation of parameter importance distributions to enhance knowledge transfer. Inspired by human continual learning, Recurrent-KIF employs an inner loop that rapidly adapts to new tasks while identifying important parameters, coupled with an outer loop that globally manages the fusion of new and historical knowledge through redundant knowledge pruning and key knowledge merging. These inner-outer loops iteratively perform multiple rounds of fusion, allowing Recurrent-KIF to leverage intermediate training information and adaptively adjust fusion strategies based on evolving importance distributions. Extensive experiments on two CL benchmarks with various model sizes (from 770M to 13B) demonstrate that Recurrent-KIF effectively mitigates catastrophic forgetting and enhances knowledge transfer.",
      "arxiv_url": "https://arxiv.org/abs/2502.17510",
      "pdf_url": "https://arxiv.org/pdf/2502.17510",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "794999a6525eb6bf4e8b1a6386dc74627236f8de",
      "title": "A Perspective on LLM Data Generation with Few-shot Examples: from Intent to Kubernetes Manifest",
      "authors": [
        "Antonino Angi",
        "Liubov Nedoshivina",
        "Alessio Sacco",
        "Stefano Braghin",
        "Mark Purcell"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/794999a6525eb6bf4e8b1a6386dc74627236f8de",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "794bf13836bdbeccca94084fb3e96f9c094f6b85",
      "title": "CONSENSAGENT: Towards Efficient and Effective Consensus in Multi-Agent LLM Interactions Through Sycophancy Mitigation",
      "authors": [
        "Priya Pitre",
        "Naren Ramakrishnan",
        "Xuan Wang"
      ],
      "abstract": "Multi-agent large language model (LLM) systems have shown remarkable performance in tasks such as reasoning, planning, and decision-making. However, their applicability is limited by challenges such as high computational costs and robustness issues. In this work, we identify and systematically evaluate a critical yet overlooked challenge: sycophancy, where agents reinforce each other’s responses instead of critically engaging with the debate. This behavior inflates computational costs by requiring additional debate rounds to reach consensus, limiting the efficiency of multi-agent LLM systems. Through experiments on six benchmark reasoning datasets across three models, we analyze the impact of sycophancy and its role in reducing the reliability of multi-agent debate. Motivated by our findings, we propose C ONSENS A GENT , a novel framework that dynamically refines prompts based on agent interactions to mitigate sycophancy. C ONSEN - S A GENT improves accuracy of the debate while maintaining efficiency. It significantly outperforms both single-agent and multi-agent base-lines, achieving state-of-the-art results across all benchmark datasets. Our findings highlight the crucial role of structured prompt optimization in multi-agent setups and establish a foundation for more reliable, efficient multi-agent LLM systems in real-world applications. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/794bf13836bdbeccca94084fb3e96f9c094f6b85",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.09706",
      "title": "Domain Adaptation of Foundation LLMs for e-Commerce",
      "authors": [
        "Christian Herold",
        "Michael Kozielski",
        "Tala Bazazo",
        "Pavel Petrushkov",
        "Hadi Hashemi",
        "Patrycja Cieplicka",
        "Dominika Basaj",
        "Shahram Khadivi"
      ],
      "abstract": "We present the e-Llama models: 8 billion and 70 billion parameter large language models that are adapted towards the e-commerce domain. These models are meant as foundation models with deep knowledge about e-commerce, that form a base for instruction- and fine-tuning. The e-Llama models are obtained by continuously pretraining the Llama 3.1 base models on 1 trillion tokens of domain-specific data. We discuss our approach and motivate our choice of hyperparameters with a series of ablation studies. To quantify how well the models have been adapted to the e-commerce domain, we define and implement a set of multilingual, e-commerce specific evaluation tasks. We show that, when carefully choosing the training setup, the Llama 3.1 models can be adapted towards the new domain without sacrificing significant performance on general domain tasks. We also explore the possibility of merging the adapted model and the base model for a better control of the performance trade-off between domains.",
      "arxiv_url": "https://arxiv.org/abs/2501.09706",
      "pdf_url": "https://arxiv.org/pdf/2501.09706",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10673",
      "title": "ZeroSumEval: An Extensible Framework For Scaling LLM Evaluation with Inter-Model Competition",
      "authors": [
        "H. A. Alyahya",
        "Haidar Khan",
        "Yazeed Alnumay",
        "M. Saiful",
        "Bülent Yener",
        "Abhimanyu Dubey",
        "Abhinav Jauhri",
        "Abhinav Pandey",
        "Abhishek Kadian",
        "Ahmad Al-Dahle",
        "Aiesha Letman",
        "Akhil Mathur",
        "A. Schelten",
        "Amy Yang",
        "Angela Fan",
        "Anirudh Goyal",
        "A. Hartshorn",
        "Aobo Yang",
        "Archi Mitra",
        "A. Sravankumar",
        "A. Korenev",
        "Arthur Hinsvark",
        "Arun Rao",
        "Aston Zhang",
        "Aur'elien Rodriguez",
        "Austen Gregerson",
        "Ava Spataru",
        "Baptiste Rozière",
        "Bethany Biron",
        "Binh Tang",
        "Bobbie Chern",
        "Charlotte Caucheteux",
        "Chaya Nayak",
        "Chloe Bi",
        "Chris Marra",
        "Chris McConnell",
        "Christian Keller",
        "C. Touret",
        "Chunyang Wu",
        "Corinne Wong",
        "Cristian Canton Ferrer",
        "Cyrus Nikolaidis",
        "Damien Al-lonsius",
        "Daniel Song",
        "Danielle Pintz",
        "Danny Livshits",
        "David Esiobu",
        "Dhruv Choudhary",
        "Dhruv Mahajan",
        "Diego Garcia-Olano",
        "Diego Perino",
        "Dieuwke Hupkes",
        "Egor Lakomkin",
        "Ehab A. AlBadawy",
        "E. Lobanova",
        "Emily Dinan",
        "Eric Michael Smith",
        "Filip Radenovic",
        "Frank Zhang",
        "Gabriele Synnaeve",
        "Gabrielle Lee",
        "Georgia Lewis Anderson",
        "Graeme Nail",
        "Grégoire Mi-alon",
        "Guan Pang",
        "Guillem Cucurell",
        "Hailey Nguyen",
        "Hannah Korevaar",
        "Hu Xu",
        "Hugo Touvron",
        "Imanol Iliyan Zarov",
        "Arrieta Ibarra",
        "Isabel M. Kloumann",
        "Ishan Misra",
        "Ivan Evtimov",
        "Jade Copet",
        "Jaewon Lee",
        "Jan Geffert",
        "Jana Vranes",
        "Jason Park",
        "Jay Mahadeokar",
        "Jeet Shah",
        "J. V. D. Linde",
        "Jennifer Billock",
        "Jenny Hong",
        "Jenya Lee",
        "J. Fu",
        "Jianfeng Chi",
        "Jianyu Huang",
        "Jiawen Liu",
        "Jie Wang",
        "Jiecao Yu",
        "Joanna Bitton",
        "Joe Spisak",
        "Jongsoo Park",
        "Joseph Rocca",
        "J. Johnstun",
        "Joshua Saxe",
        "Junteng Jia",
        "Kalyan Vasuden Alwala",
        "K. Upasani",
        "Kate Plawiak",
        "Keqian Li",
        "Kenneth Heafield",
        "Kevin R. Stone",
        "Khalid El-Arini",
        "Krithika Iyer",
        "Kshitiz Malik",
        "Kuen-ley Chiu",
        "Kunal Bhalla",
        "Lauren Rantala-Yeary",
        "Laurens van der Maaten",
        "Lawrence Chen",
        "Liang Tan",
        "Liz Jenkins",
        "Louis Martin",
        "Lovish Madaan",
        "Lubo Malo",
        "Lukas Blecher",
        "Lukas Landzaat",
        "Luke de Oliveira",
        "M. Muzzi",
        "Ma-hesh Pasupuleti",
        "Mannat Singh",
        "Manohar Paluri",
        "Marcin Kardas",
        "Mathew Oldham",
        "Mathieu Rita",
        "Maya Pavlova",
        "M. Kambadur",
        "Mike Lewis",
        "Mitesh Min Si",
        "Kumar Singh",
        "Mona Hassan",
        "Naman Goyal",
        "Narjes Torabi",
        "Nikolay Bash-lykov",
        "Nikolay Bogoychev",
        "Niladri S. Chatterji",
        "Olivier Duchenne",
        "Onur Çelebi",
        "Patrick Alrassy",
        "Pengchuan Zhang",
        "Pengwei Li",
        "Petar Vasić",
        "Peter Weng",
        "Prajjwal Bhargava",
        "P. Dubal",
        "Praveen Krishnan Punit",
        "S. Koura",
        "Puxin Xu",
        "Qing He",
        "Qingxiao Dong",
        "Ragavan Srinivasan",
        "Raj Ganapathy",
        "Ramon Calderer",
        "Ricardo Silveira Cabral",
        "Robert Stojnic",
        "R. Raileanu",
        "Rohit Girdhar",
        "Rohit Patel",
        "Romain Sauvestre",
        "Ron-nie Polidoro",
        "Roshan Sumbaly",
        "Ross Taylor",
        "Ruan Silva",
        "Rui Hou",
        "Rui Wang",
        "S. Hosseini",
        "Sa-hana Chennabasappa",
        "Sanjay Singh",
        "Seohyun Sean Bell",
        "Sonia Kim",
        "Sergey Edunov",
        "Shaoliang Nie",
        "Sharan Narang",
        "S. Raparthy",
        "Sheng Shen",
        "Shengye Wan",
        "Shruti Bhosale",
        "Shun Zhang",
        "Simon Vandenhende",
        "Soumya Batra",
        "Spencer Whitman",
        "Sten Sootla",
        "S. Collot",
        "Suchin Gu-rurangan",
        "S. Borodinsky",
        "Tamar Herman",
        "T. Fowler",
        "Tarek Sheasha",
        "Thomas Georgiou",
        "Thomas Scialom",
        "Tobias Speckbacher",
        "Todor Mihaylov",
        "Tong Xiao",
        "Ujjwal Karn",
        "Vedanuj Goswami",
        "Vibhor Gupta",
        "Vignesh Ramanathan",
        "Viktor Kerkez",
        "Vincent Gonguet",
        "Vir-ginie Do",
        "Vish Vogeti",
        "Vladan Petro-vic",
        "Weiwei Chu",
        "Wenhan Xiong",
        "Wenyin Fu",
        "Whit-ney Meers",
        "Xavier Martinet",
        "Xiaodong Wang",
        "Ellen Tan",
        "Xinfeng Xie",
        "Xuchao Jia",
        "Xuewei Wang",
        "Yaelle Goldschlag",
        "Yashesh Gaur",
        "Yasmine Babaei",
        "Yiwen Wen",
        "Yiwen Song",
        "Yuchen Zhang",
        "Yue Li",
        "Yuning Mao",
        "Zacharie Delpierre Coudert",
        "Zhengxu Yan",
        "Zhengxing Chen",
        "Zoe Papakipos",
        "Aaditya K. Singh",
        "Aaron Grattafiori",
        "Abha Jain",
        "Adam Kelsey",
        "Adam Shajnfeld",
        "Adi Gangidi",
        "Adolfo Victoria",
        "Ahuva Goldstand",
        "A. Menon",
        "Ajay Sharma",
        "Alex Boesen-berg",
        "Alex Vaughan",
        "Alexei Baevski",
        "Allie Feinstein",
        "Amanda Kallet",
        "Amit Sangani",
        "Anam Yunus",
        "An-drei Lupu",
        "Andres Alvarado",
        "A. Caples",
        "Andrew Gu",
        "Andrew Ho",
        "Andrew Poulton",
        "Andrew Ryan",
        "Ankit Ramchandani",
        "Annie Franco",
        "Apara-jita Saraf",
        "Arkabandhu Chowdhury",
        "Ashley Gabriel",
        "Ashwin Bharambe",
        "Assaf Eisenman",
        "Azadeh Yaz-dan",
        "Beau James",
        "Ben Maurer",
        "B. Leonhardi",
        "Bernie Huang",
        "Beth Loyd",
        "Beto de Paola",
        "Bhargavi Paranjape",
        "Bing Liu",
        "Bo Wu",
        "B. Ni",
        "Braden Han-cock",
        "Bram Wasti",
        "Brandon Spence",
        "B. Stojkovic",
        "Brian Gamido",
        "Britt Montalvo",
        "Carl Parker",
        "Carly Burton",
        "Catalina Mejia",
        "Changhan Wang",
        "Changkyu Kim",
        "Chao Zhou",
        "Chester Hu",
        "Ching-Hsiang Chu",
        "Chris Cai",
        "Chris Tindal",
        "Christoph Feichtenhofer",
        "Damon Civin",
        "Dana Beaty",
        "Daniel Kreymer",
        "Daniel Li",
        "Gil Halpern",
        "Govind Thattai",
        "Grant Herman",
        "Grigory Sizov",
        "Guangyi",
        "Guna Zhang",
        "Lakshminarayanan Hamid",
        "Han Shojanazeri",
        "Han Zou",
        "Hanwen Wang",
        "Haroun Zha",
        "Harrison Habeeb",
        "Helen Rudolph",
        "Henry Suk",
        "Hunter Aspegren",
        "Ibrahim Goldman",
        "Igor Damlaj",
        "Igor Molybog",
        "Irina-Elena Tufanov",
        "Itai Veliche",
        "Jake Gat",
        "James Weissman",
        "Geboski James",
        "Japhet Kohli",
        "Jean-Baptiste Asher",
        "Gaya Jeff",
        "Jeff Marcus",
        "Jennifer Tang",
        "Jenny Chan",
        "Zhen Jeremy",
        "J. Reizenstein",
        "Jessica Teboul",
        "Zhong Jian",
        "Jingyi Jin",
        "Joe Yang",
        "Jon Cummings",
        "Carvill Jon",
        "Jon Shepard",
        "J. McPhie",
        "Torres Josh",
        "Junjie Ginsburg",
        "Kai Wang",
        "Kam Wu",
        "Hou Karan",
        "Karthik Saxena",
        "Kartikay Prasad",
        "Katayoun Khan-delwal",
        "Kathy Zand",
        "Kaushik Matosich",
        "Kelly Veeraraghavan",
        "Keqian Michelena",
        "Kun Li",
        "Kunal Huang",
        "Kushal Chawla",
        "Kyle Lakhotia",
        "Lailin Huang",
        "Lakshya Chen",
        "Lavender A Leandro Garg",
        "Lee Silva",
        "Lei Bell",
        "Liangpeng Zhang",
        "Licheng Guo",
        "Liron Yu",
        "L. Moshkovich",
        "Madian Wehrstedt",
        "Manav Khabsa",
        "Manish Avalani",
        "Maria Bhatt",
        "Martynas Tsim-poukelli",
        "Matan Mankus",
        "Matthew Hasson",
        "M. Lennie",
        "M. Reso",
        "M. Groshev",
        "Maya Naumov",
        "Meghan Lathi",
        "Michael L Keneally",
        "Michal Seltzer",
        "M. Valko",
        "Mihir Restrepo",
        "Mik Patel",
        "Mik Vyatskov",
        "M. Samvelyan",
        "Mike Clark",
        "M. Macey",
        "Miquel Wang",
        "Jubert Mo",
        "Mo Metanat",
        "Mun-ish Rastegari",
        "Nandhini Bansal",
        "Natascha Santhanam",
        "Parks Natasha",
        "Navyata White",
        "Nayan Bawa",
        "Nick Singhal",
        "Nicolas Egebo",
        "Usunier",
        "Nikolay Pavlovich",
        "Laptev Ning",
        "Ning Dong",
        "Norman Zhang",
        "Oleg Cheng",
        "Olivia Chernoguz",
        "Omkar Hart",
        "Ozlem Salpekar",
        "Parkin Kalinli",
        "Parth Kent",
        "Paul Parekh",
        "Paul Saab",
        "Pedro Balaji",
        "Philip Rittner",
        "Pierre Bontrager",
        "Piotr Roux",
        "P. Dollar",
        "P. Zvyagina",
        "P. Yuvraj",
        "Qian Liang",
        "Rachad Alao",
        "Rachel Rodriguez",
        "Rafi Ayub",
        "Raghotham Murthy",
        "Raghu Nayani",
        "Rahul Mitra",
        "Raymond Li",
        "Rebekkah Hogan",
        "Robin Battey",
        "Rocky Wang",
        "Rohan Mah-eswari",
        "Russ Howes",
        "Ruty Rinott",
        "Sai Jayesh",
        "Bondu Samyak",
        "Sara Datta",
        "Sara Chugh",
        "Sargun Hunt",
        "Sasha Dhillon",
        "Satadru Sidorov",
        "Saurabh Pan",
        "Verma Seiji",
        "Sharadh Yamamoto",
        "Shaun Ramaswamy",
        "Shaun Lind-say",
        "Sheng Lindsay",
        "Sheng Feng",
        "Lin Shengxin Cindy",
        "Shiva Zha",
        "Shuqiang Shankar",
        "Shuqiang Zhang",
        "Sinong Zhang",
        "S. Wang",
        "Soji Agar-wal",
        "S. Sajuyigbe",
        "S. Chintala",
        "Stephen Max",
        "Steve Chen",
        "Steve Kehoe",
        "S. Sudarshan",
        "S. Govindaprasad",
        "Sungmin Gupta",
        "Sunny Cho",
        "Suraj Virk",
        "Sy Subramanian",
        "Choudhury Sydney",
        "Tal Goldman",
        "T. Remez",
        "Tamara Glaser",
        "Thilo Best",
        "Thomas Kohler",
        "Tianhe Robinson",
        "Tianjun Li",
        "Tim Zhang",
        "Tim Matthews",
        "Tzook Chou",
        "Varun Shaked",
        "Victoria Vontimitta",
        "Victoria Ajayi",
        "Vijai Montanez",
        "V. Mohan",
        "Vishal Kumar",
        "Vítor Mangla",
        "Vlad Albiero",
        "Vlad Ionescu",
        "Poenaru Vlad Tiberiu",
        "Vlad T. Mihailescu",
        "Wei Ivanov",
        "Li",
        "Yilin Zhang",
        "Ying Zhang",
        "Yossi Adi",
        "Youngjin Nam",
        "Wang Yu",
        "Yuchen Hao",
        "Yundi Qian",
        "Yuzi He",
        "Zach Rait",
        "Zachary DeVito",
        "Zef Rosnbrick",
        "Zhaoduo Wen"
      ],
      "abstract": "We introduce ZeroSumEval, a dynamic, competition-based, and evolving evaluation framework for Large Language Models (LLMs) that leverages competitive games. ZeroSumEval encompasses a diverse suite of games, including security challenges (Capture the Flag), classic board games (chess), and knowledge tests (MathQuiz). These games are designed to evaluate a range of capabilities such as strategic reasoning, planning, knowledge application, safety, and adaptability. Building upon recent studies that highlight the effectiveness of game-based evaluations for LLMs, ZeroSumEval enhances these approaches by providing a standardized and extensible framework for easily implementing games and leverages DSPy to provide a better abstraction for LLM player strategies.",
      "arxiv_url": "https://arxiv.org/abs/2503.10673",
      "pdf_url": "https://arxiv.org/pdf/2503.10673",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00071",
      "title": "I see what you mean: Co-Speech Gestures for Reference Resolution in Multimodal Dialogue",
      "authors": [
        "E. Ghaleb",
        "Bulat Khaertdinov",
        "Asli Ozyurek",
        "R. Fern'andez"
      ],
      "abstract": "In face-to-face interaction, we use multiple modalities, including speech and gestures, to communicate information and resolve references to objects. However, how representational co-speech gestures refer to objects remains understudied from a computational perspective. In this work, we address this gap by introducing a multimodal reference resolution task centred on representational gestures, while simultaneously tackling the challenge of learning robust gesture embeddings. We propose a self-supervised pre-training approach to gesture representation learning that grounds body movements in spoken language. Our experiments show that the learned embeddings align with expert annotations and have significant predictive power. Moreover, reference resolution accuracy further improves when (1) using multimodal gesture representations, even when speech is unavailable at inference time, and (2) leveraging dialogue history. Overall, our findings highlight the complementary roles of gesture and speech in reference resolution, offering a step towards more naturalistic models of human-machine interaction.",
      "arxiv_url": "https://arxiv.org/abs/2503.00071",
      "pdf_url": "https://arxiv.org/pdf/2503.00071",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7981222953c00283192902f0a939cc665e6385d1",
      "title": "ZeroNER: Fueling Zero-Shot Named Entity Recognition via Entity Type Descriptions",
      "authors": [
        "Alessio Cocchieri",
        "Marcos Martínez Galindo",
        "Giacomo Frisoni",
        "Gianluca Moro",
        "Claudio Sartori",
        "Giuseppe Tagliavini"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/7981222953c00283192902f0a939cc665e6385d1",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.13328",
      "title": "Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges",
      "authors": [
        "Hongru Wang",
        "Wenyu Huang",
        "Yufei Wang",
        "Yuanhao Xi",
        "Jianqiao Lu",
        "Huan Zhang",
        "Nan Hu",
        "Zeming Liu",
        "Jeff Z. Pan",
        "Kam-Fai Wong"
      ],
      "abstract": "Existing benchmarks that assess Language Models (LMs) as Language Agents (LAs) for tool use primarily focus on stateless, single-turn interactions or partial evaluations, such as tool selection in a single turn, overlooking the inherent stateful nature of interactions in multi-turn applications. To fulfill this gap, we propose \\texttt{DialogTool}, a multi-turn dialogue dataset with stateful tool interactions considering the whole life cycle of tool use, across six key tasks in three stages: 1) \\textit{tool creation}; 2) \\textit{tool utilization}: tool awareness, tool selection, tool execution; and 3) \\textit{role-consistent response}: response generation and role play. Furthermore, we build \\texttt{VirtualMobile} -- an embodied virtual mobile evaluation environment to simulate API calls and assess the robustness of the created APIs\\footnote{We will use tools and APIs alternatively, there are no significant differences between them in this paper.}. Taking advantage of these artifacts, we conduct comprehensive evaluation on 13 distinct open- and closed-source LLMs and provide detailed analysis at each stage, revealing that the existing state-of-the-art LLMs still cannot perform well to use tools over long horizons.",
      "arxiv_url": "https://arxiv.org/abs/2505.13328",
      "pdf_url": "https://arxiv.org/pdf/2505.13328",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.21098",
      "title": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry",
      "authors": [
        "Qinwen Chen",
        "Wenbiao Tao",
        "Zhiwei Zhu",
        "Mingfan Xi",
        "Liangzhong Guo",
        "Yuan Wang",
        "Wei Wang",
        "Yunshi Lan"
      ],
      "abstract": "Community Question Answering (CQA) platforms can be deemed as important knowledge bases in community, but effectively leveraging historical interactions and domain knowledge in real-time remains a challenge. Existing methods often underutilize external knowledge, fail to incorporate dynamic historical QA context, or lack memory mechanisms suited for industrial deployment. We propose ComRAG, a retrieval-augmented generation framework for real-time industrial CQA that integrates static knowledge with dynamic historical QA pairs via a centroid-based memory mechanism designed for retrieval, generation, and efficient storage. Evaluated on three industrial CQA datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9% improvement in vector similarity, reducing latency by 8.7% to 23.3%, and lowering chunk growth from 20.23% to 2.06% over iterations.",
      "arxiv_url": "https://arxiv.org/abs/2506.21098",
      "pdf_url": "https://arxiv.org/pdf/2506.21098",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-06-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.13288",
      "title": "Task-Oriented Automatic Fact-Checking with Frame-Semantics",
      "authors": [
        "J. Devasier",
        "A. Putta",
        "Rishabh Mediratta",
        "Chengkai Li"
      ],
      "abstract": "We propose a novel paradigm for automatic fact-checking that leverages frame semantics to enhance the structured understanding of claims and guide the process of fact-checking them. To support this, we introduce a pilot dataset of real-world claims extracted from PolitiFact, specifically annotated for large-scale structured data. This dataset underpins two case studies: the first investigates voting-related claims using the Vote semantic frame, while the second explores various semantic frames based on data sources from the Organisation for Economic Co-operation and Development (OECD). Our findings demonstrate the effectiveness of frame semantics in improving evidence retrieval and explainability for fact-checking. Finally, we conducted a survey of frames evoked in fact-checked claims, identifying high-impact frames to guide future work in this direction.",
      "arxiv_url": "https://arxiv.org/abs/2501.13288",
      "pdf_url": "https://arxiv.org/pdf/2501.13288",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7a20a9c7865d4804d4fa4c8df72284f37339efef",
      "title": "Think Both Ways: Teacher-Student Bidirectional Reasoning Enhances MCQ Generation and Distractor Quality",
      "authors": [
        "Yimiao Qiu",
        "Yang Deng",
        "Quanming Yao",
        "Zhimeng Zhang",
        "Zhiang Dong",
        "Chang Yao",
        "Jingyuan Chen"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/7a20a9c7865d4804d4fa4c8df72284f37339efef",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7a3dd57d828a7be6f5196d3d0424aa5c70ac22c2",
      "title": "Event Pattern-Instance Graph: A Multi-Round Role Representation Learning Strategy for Document-Level Event Argument Extraction",
      "authors": [
        "Qizhi Wan",
        "Liu Tao",
        "Changxuan Wan",
        "Rong Hu",
        "Keli Xiao",
        "Yuxin Shuai"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/7a3dd57d828a7be6f5196d3d0424aa5c70ac22c2",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.05288",
      "title": "A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models",
      "authors": [
        "Shuliang Liu",
        "Hongyi Liu",
        "Aiwei Liu",
        "Bingchen Duan",
        "Zheng Qi",
        "Yibo Yan",
        "He Geng",
        "Peijie Jiang",
        "Jia Liu",
        "Xuming Hu"
      ],
      "abstract": "The widespread deployment of large language models (LLMs) across critical domains has amplified the societal risks posed by algorithmically generated misinformation. Unlike traditional false content, LLM-generated misinformation can be self-reinforcing, highly plausible, and capable of rapid propagation across multiple languages, which traditional detection methods fail to mitigate effectively. This paper introduces a proactive defense paradigm, shifting from passive post hoc detection to anticipatory mitigation strategies. We propose a Three Pillars framework: (1) Knowledge Credibility, fortifying the integrity of training and deployed data; (2) Inference Reliability, embedding self-corrective mechanisms during reasoning; and (3) Input Robustness, enhancing the resilience of model interfaces against adversarial attacks. Through a comprehensive survey of existing techniques and a comparative meta-analysis, we demonstrate that proactive defense strategies offer up to 63\\% improvement over conventional methods in misinformation prevention, despite non-trivial computational overhead and generalization challenges. We argue that future research should focus on co-designing robust knowledge foundations, reasoning certification, and attack-resistant interfaces to ensure LLMs can effectively counter misinformation across varied domains.",
      "arxiv_url": "https://arxiv.org/abs/2507.05288",
      "pdf_url": "https://arxiv.org/pdf/2507.05288",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00441",
      "title": "K-order Ranking Preference Optimization for Large Language Models",
      "authors": [
        "Shihao Cai",
        "Chongming Gao",
        "Yang Zhang",
        "Wentao Shi",
        "Jizhi Zhang",
        "Keqin Bao",
        "Qifan Wang",
        "Fuli Feng"
      ],
      "abstract": "To adapt large language models (LLMs) to ranking tasks, existing list-wise methods, represented by list-wise Direct Preference Optimization (DPO), focus on optimizing partial-order or full-order list ranking consistency for LLMs to enhance their ranking abilities. However, we argue that optimizing top-K ranking consistency could be more appropriate for real-world applications. There are two main reasons: (1) users are typically concerned with only the top-K results, making top-K ranking more important, and (2) tail items often lack precise feedback, making top-K ranking more reliable. Based on this, we propose K-order Ranking Preference Optimization (KPO) by extending the DPO's Plackett-Luce model to accommodate top-K rankings. Additionally, recognizing that the number of important items can vary across queries, we extend KPO to dynamically determine appropriate K for different samples and introduce a curriculum learning strategy to boost training efficiency. Extensive experiments demonstrate the effectiveness of KPO, highlighting its high sample efficiency and robustness to noise. The code is available at https://github.com/Lanyu0303/KPO.",
      "arxiv_url": "https://arxiv.org/abs/2506.00441",
      "pdf_url": "https://arxiv.org/pdf/2506.00441",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07671",
      "title": "GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation",
      "authors": [
        "I. Sorodoc",
        "Leonardo F. R. Ribeiro",
        "Rexhina Blloshmi",
        "Christopher Davis",
        "A. D. Gispert"
      ],
      "abstract": "We present GaRAGe, a large RAG benchmark with human-curated long-form answers and annotations of each grounding passage, allowing a fine-grained evaluation of whether LLMs can identify relevant grounding when generating RAG answers. Our benchmark contains 2366 questions of diverse complexity, dynamism, and topics, and includes over 35K annotated passages retrieved from both private document sets and the Web, to reflect real-world RAG use cases. This makes it an ideal test bed to evaluate an LLM's ability to identify only the relevant information necessary to compose a response, or provide a deflective response when there is insufficient information. Evaluations of multiple state-of-the-art LLMs on GaRAGe show that the models tend to over-summarise rather than (a) ground their answers strictly on the annotated relevant passages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b) deflect when no relevant grounding is available (reaching at most 31% true positive rate in deflections). The F1 in attribution to relevant sources is at most 58.9%, and we show that performance is particularly reduced when answering time-sensitive questions and when having to draw knowledge from sparser private grounding sources.",
      "arxiv_url": "https://arxiv.org/abs/2506.07671",
      "pdf_url": "https://arxiv.org/pdf/2506.07671",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.15697",
      "title": "DeepRTL2: A Versatile Model for RTL-Related Tasks",
      "authors": [
        "Yi Liu",
        "Hongji Zhang",
        "Yunhao Zhou",
        "Zheng-Hao Shi",
        "Changran Xu",
        "Qiang Xu"
      ],
      "abstract": "The integration of large language models (LLMs) into electronic design automation (EDA) has significantly advanced the field, offering transformative benefits, particularly in register transfer level (RTL) code generation and understanding. While previous studies have demonstrated the efficacy of fine-tuning LLMs for these generation-based tasks, embedding-based tasks, which are equally critical to EDA workflows, have been largely overlooked. These tasks, including natural language code search, RTL code functionality equivalence checking, and performance prediction, are essential for accelerating and optimizing the hardware design process. To address this gap, we present DeepRTL2, a family of versatile LLMs that unifies both generation- and embedding-based tasks related to RTL. By simultaneously tackling a broad range of tasks, DeepRTL2 represents the first model to provide a comprehensive solution to the diverse challenges in EDA. Through extensive experiments, we show that DeepRTL2 achieves state-of-the-art performance across all evaluated tasks.",
      "arxiv_url": "https://arxiv.org/abs/2506.15697",
      "pdf_url": "https://arxiv.org/pdf/2506.15697",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24671",
      "title": "Multiple LLM Agents Debate for Equitable Cultural Alignment",
      "authors": [
        "Dayeon Ki",
        "Rachel Rudinger",
        "Tianyi Zhou",
        "Marine Carpuat"
      ],
      "abstract": "Large Language Models (LLMs) need to adapt their predictions to diverse cultural contexts to benefit diverse communities across the world. While previous efforts have focused on single-LLM, single-turn approaches, we propose to exploit the complementary strengths of multiple LLMs to promote cultural adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based agents debate over a cultural scenario and collaboratively reach a final decision. We propose two variants: one where either LLM agents exclusively debate and another where they dynamically choose between self-reflection and debate during their turns. We evaluate these approaches on 7 open-weight LLMs (and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette norms in 75 countries. Experiments show that debate improves both overall accuracy and cultural group parity over single-LLM baselines. Notably, multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies comparable to that of a much larger model (27B parameters).",
      "arxiv_url": "https://arxiv.org/abs/2505.24671",
      "pdf_url": "https://arxiv.org/pdf/2505.24671",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12685",
      "title": "Theoretical Guarantees for Minimum Bayes Risk Decoding",
      "authors": [
        "Yuki Ichihara",
        "Yuu Jinnai",
        "Kaito Ariu",
        "Tetsuro Morimura",
        "Eiji Uchibe"
      ],
      "abstract": "Minimum Bayes Risk (MBR) decoding optimizes output selection by maximizing the expected utility value of an underlying human distribution. While prior work has shown the effectiveness of MBR decoding through empirical evaluation, few studies have analytically investigated why the method is effective. As a result of our analysis, we show that, given the size $n$ of the reference hypothesis set used in computation, MBR decoding approaches the optimal solution with high probability at a rate of $O\\left(n^{-\\frac{1}{2}}\\right)$, under certain assumptions, even though the language space $Y$ is significantly larger $|Y|\\gg n$. This result helps to theoretically explain the strong performance observed in several prior empirical studies on MBR decoding. In addition, we provide the performance gap for maximum-a-posteriori (MAP) decoding and compare it to MBR decoding. The result of this paper indicates that MBR decoding tends to converge to the optimal solution faster than MAP decoding in several cases.",
      "arxiv_url": "https://arxiv.org/abs/2502.12685",
      "pdf_url": "https://arxiv.org/pdf/2502.12685",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.07920",
      "title": "Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural Vision-Language Dataset for Southeast Asia",
      "authors": [
        "Samuel Cahyawijaya",
        "Holy Lovenia",
        "Joel Ruben Antony Moniz",
        "Tack Hwa Wong",
        "MohammadRifqi Farhansyah",
        "Thant Thiri Maung",
        "Frederikus Hudi",
        "David Anugraha",
        "Muhammad Ravi Shulthan Habibi",
        "Muhammad Reza Qorib",
        "Amit Agarwal",
        "Joseph Marvin Imperial",
        "Hitesh Laxmichand Patel",
        "Vicky Feliren",
        "B. Nasution",
        "Manuel Antonio Rufino",
        "Genta Indra Winata",
        "R. Rajagede",
        "Carlos Rafael Catalan",
        "Mohamed Fazli Mohamed Imam",
        "Priyaranjan Pattnayak",
        "Salsabila Zahirah Pranida",
        "K. Pratama",
        "Yeshil Bangera",
        "Adisai Na-Thalang",
        "Patricia Nicole Monderin",
        "Yueqi Song",
        "Christian Simon",
        "Lynnette Hui Xian Ng",
        "Richardy Lobo' Sapan",
        "Taki Hasan Rafi",
        "Bin Wang",
        "Supryadi",
        "Kanyakorn Veerakanjana",
        "Piyalitt Ittichaiwong",
        "Matthew Theodore Roque",
        "Karissa Vincentio",
        "Takdanai Kreangphet",
        "Phakphum Artkaew",
        "K. Palgunadi",
        "Yanzhi Yu",
        "Rochana Prih Hastuti",
        "William Nixon",
        "Mithil Bangera",
        "A. X. W. Lim",
        "A. Khine",
        "Hanif Muhammad Zhafran",
        "Teddy Ferdinan",
        "Audra Aurora Izzani",
        "Ayushman Singh",
        "Evan",
        "Jauza Akbar Krito",
        "M. Anugraha",
        "Fenal Ashokbhai Ilasariya",
        "Haochen Li",
        "John Amadeo Daniswara",
        "Filbert Aurelian Tjiaranata",
        "Eryawan Presma Yulianrifat",
        "Can Udomcharoenchaikit",
        "Fadil Risdian Ansori",
        "Mahardika Krisna Ihsani",
        "Giang Nguyen",
        "A. Barik",
        "Dan John Velasco",
        "R. Genadi",
        "Saptarshi Saha",
        "Chengwei Wei",
        "Isaiah Flores",
        "Kenneth Ko Han Chen",
        "Anjela Gail Santos",
        "Wan Shen Lim",
        "Kaung Si Phyo",
        "Tim Santos",
        "M. Dwiastuti",
        "Jiayun Luo",
        "Jan Christian Blaise Cruz",
        "Ming Shan Hee",
        "Ikhlasul Akmal Hanif",
        "M.Alif Al Hakim",
        "Muhammad Rizky Sya'ban",
        "Kun Kerdthaisong",
        "Lester James V. Miranda",
        "Fajri Koto",
        "Tirana Noor Fatyanosa",
        "Alham Fikri Aji",
        "Jostin Jerico Rosal",
        "Jun Kevin",
        "Robert Wijaya",
        "Onno P. Kampman",
        "Ruochen Zhang",
        "Börje F. Karlsson",
        "Peerat Limkonchotiwat"
      ],
      "abstract": "Southeast Asia (SEA) is a region of extraordinary linguistic and cultural diversity, yet it remains significantly underrepresented in vision-language (VL) research. This often results in artificial intelligence (AI) models that fail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an open-source initiative dedicated to developing high-quality, culturally relevant data for SEA languages. By involving contributors from SEA countries, SEA-VL aims to ensure better cultural relevance and diversity, fostering greater inclusivity of underrepresented languages in VL research. Beyond crowdsourcing, our initiative goes one step further in the exploration of the automatic collection of culturally relevant images through crawling and image generation. First, we find that image crawling achieves approximately ~85% cultural relevance while being more cost- and time-efficient than crowdsourcing. Second, despite the substantial progress in generative vision models, synthetic images remain unreliable in accurately reflecting SEA cultures. The generated images often fail to reflect the nuanced traditions and cultural contexts of the region. Collectively, we gather 1.28M SEA culturally-relevant images, more than 50 times larger than other existing datasets. Through SEA-VL, we aim to bridge the representation gap in SEA, fostering the development of more inclusive AI systems that authentically represent diverse cultures across SEA.",
      "arxiv_url": "https://arxiv.org/abs/2503.07920",
      "pdf_url": "https://arxiv.org/pdf/2503.07920",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.13282",
      "title": "Rank, Chunk and Expand: Lineage-Oriented Reasoning for Taxonomy Expansion",
      "authors": [
        "Sahil Mishra",
        "Kumar Arjun",
        "Tanmoy Chakraborty"
      ],
      "abstract": "Taxonomies are hierarchical knowledge graphs crucial for recommendation systems, and web applications. As data grows, expanding taxonomies is essential, but existing methods face key challenges: (1) discriminative models struggle with representation limits and generalization, while (2) generative methods either process all candidates at once, introducing noise and exceeding context limits, or discard relevant entities by selecting noisy candidates. We propose LORex (Lineage-Oriented Reasoning for Taxonomy Expansion), a plug-and-play framework that combines discriminative ranking and generative reasoning for efficient taxonomy expansion. Unlike prior methods, LORex ranks and chunks candidate terms into batches, filtering noise and iteratively refining selections by reasoning candidates' hierarchy to ensure contextual efficiency. Extensive experiments across four benchmarks and twelve baselines show that LORex improves accuracy by 12% and Wu&Palmer similarity by 5% over state-of-the-art methods.",
      "arxiv_url": "https://arxiv.org/abs/2505.13282",
      "pdf_url": "https://arxiv.org/pdf/2505.13282",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24778",
      "title": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?",
      "authors": [
        "Jiayu Liu",
        "Qing Zong",
        "Weiqi Wang",
        "Yangqiu Song"
      ],
      "abstract": "As large language models (LLMs) are increasingly used in high-stakes domains, accurately assessing their confidence is crucial. Humans typically express confidence through epistemic markers (e.g.,\"fairly confident\") instead of numerical values. However, it remains unclear whether LLMs consistently use these markers to reflect their intrinsic confidence due to the difficulty of quantifying uncertainty associated with various markers. To address this gap, we first define marker confidence as the observed accuracy when a model employs an epistemic marker. We evaluate its stability across multiple question-answering datasets in both in-distribution and out-of-distribution settings for open-source and proprietary LLMs. Our results show that while markers generalize well within the same distribution, their confidence is inconsistent in out-of-distribution scenarios. These findings raise significant concerns about the reliability of epistemic markers for confidence estimation, underscoring the need for improved alignment between marker based confidence and actual model uncertainty. Our code is available at https://github.com/HKUST-KnowComp/MarCon.",
      "arxiv_url": "https://arxiv.org/abs/2505.24778",
      "pdf_url": "https://arxiv.org/pdf/2505.24778",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14856",
      "title": "FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling",
      "authors": [
        "Weilin Zhao",
        "Tengyu Pan",
        "Xu Han",
        "Yudi Zhang",
        "Ao Sun",
        "Yuxiang Huang",
        "Kaihuo Zhang",
        "Weilun Zhao",
        "Yuxuan Li",
        "Jianyong Wang",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Speculative sampling has emerged as an important technique for accelerating the auto-regressive generation process of large language models (LLMs) by utilizing a draft-then-verify mechanism to produce multiple tokens per forward pass. While state-of-the-art speculative sampling methods use only a single layer and a language modeling (LM) head as the draft model to achieve impressive layer compression, their efficiency gains are substantially reduced for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens. To address this, we present FR-Spec, a frequency-ranked speculative sampling framework that optimizes draft candidate selection through vocabulary space compression. By constraining the draft search to a frequency-prioritized token subset, our method reduces LM Head computation overhead by 75% while ensuring the equivalence of the final output distribution. Experiments across multiple datasets demonstrate an average of 1.12$\\times$ speedup over the state-of-the-art speculative sampling method EAGLE-2. Code available at https://github.com/thunlp/FR-Spec.",
      "arxiv_url": "https://arxiv.org/abs/2502.14856",
      "pdf_url": "https://arxiv.org/pdf/2502.14856",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.22813",
      "title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models",
      "authors": [
        "Zhuojun Ding",
        "Wei Wei",
        "Chenghao Fan"
      ],
      "abstract": "Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domain-specific models is costly. Existing works typically train a unified model across multiple domains, but such approaches lack adaptation and scalability since not all training data benefits target domains and scaling trained models remains challenging. We propose the SaM framework, which dynamically Selects and Merges expert models at inference time. Specifically, for a target domain, we select domain-specific experts pre-trained on existing domains based on (i) domain similarity to the target domain and (ii) performance on sampled instances, respectively. The experts are then merged to create task-specific models optimized for the target domain. By dynamically merging experts beneficial to target domains, we improve generalization across various domains without extra training. Additionally, experts can be added or removed conveniently, leading to great scalability. Extensive experiments on multiple benchmarks demonstrate our framework's effectiveness, which outperforms the unified model by an average of 10%. We further provide insights into potential improvements, practical experience, and extensions of our framework.",
      "arxiv_url": "https://arxiv.org/abs/2506.22813",
      "pdf_url": "https://arxiv.org/pdf/2506.22813",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7b92fa2b962fbfa066455acb8e53729eb75a718a",
      "title": "Automated Fine-Grained Mixture-of-Experts Quantization",
      "authors": [
        "Zhanhao Xie",
        "Yuexiao Ma",
        "Xiawu Zheng",
        "Fei Chao",
        "Wanchen Sui",
        "Yong Li",
        "Shen Li",
        "Rongrong Ji"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/7b92fa2b962fbfa066455acb8e53729eb75a718a",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.05491",
      "title": "Statistical Deficiency for Task Inclusion Estimation",
      "authors": [
        "Loic Fosse",
        "Fr'ed'eric B'echet",
        "Benoit Favre",
        "G'eraldine Damnati",
        "Gw'enol'e Lecorv'e",
        "Maxime Darrin",
        "Philippe Formont",
        "Pablo Piantanida"
      ],
      "abstract": "Tasks are central in machine learning, as they are the most natural objects to assess the capabilities of current models. The trend is to build general models able to address any task. Even though transfer learning and multitask learning try to leverage the underlying task space, no well-founded tools are available to study its structure. This study proposes a theoretically grounded setup to define the notion of task and to compute the {\\bf inclusion} between two tasks from a statistical deficiency point of view. We propose a tractable proxy as information sufficiency to estimate the degree of inclusion between tasks, show its soundness on synthetic data, and use it to reconstruct empirically the classic NLP pipeline.",
      "arxiv_url": "https://arxiv.org/abs/2503.05491",
      "pdf_url": "https://arxiv.org/pdf/2503.05491",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7bca50a59d0f2a8f6b4b136afe91fe193355bb15",
      "title": "Training Multi-Modal LLMs through Dialogue Planning for HRI",
      "authors": [
        "C. D. Hromei",
        "Federico Borazio",
        "Andrea Sensi",
        "Elisa Passone",
        "Danilo Croce",
        "Roberto Basili"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/7bca50a59d0f2a8f6b4b136afe91fe193355bb15",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.13573",
      "title": "Improving Contextual Faithfulness of Large Language Models via Retrieval Heads-Induced Optimization",
      "authors": [
        "Lei Huang",
        "Xiaocheng Feng",
        "Weitao Ma",
        "Yuchun Fan",
        "Xiachong Feng",
        "Yangfan Ye",
        "Weihong Zhong",
        "Yuxuan Gu",
        "Baoxin Wang",
        "Dayong Wu",
        "Guoping Hu",
        "Bing Qin"
      ],
      "abstract": "Ensuring contextual faithfulness in retrieval-augmented large language models (LLMs) is crucial for building trustworthy information-seeking systems, particularly in long-form question-answering (LFQA) scenarios. In this work, we identify a salient correlation between LFQA faithfulness and retrieval heads, a set of attention heads responsible for retrieving contextual information. Leveraging this insight, we propose RHIO, a framework designed to teach LLMs to explicitly discriminate between faithful and unfaithful generations. RHIO first augments unfaithful samples that simulate realistic model-intrinsic errors by selectively masking retrieval heads. Then, these samples are incorporated into joint training, enabling the model to distinguish unfaithful outputs from faithful ones conditioned on control tokens. Furthermore, these control tokens are leveraged to self-induce contrastive outputs, amplifying their difference through contrastive decoding. Additionally, to facilitate the evaluation of contextual faithfulness, we also introduce GroundBench, a comprehensive benchmark compiled from five existing LFQA datasets. Extensive experimental results on GroundBench demonstrate that RHIO significantly improves faithfulness, even outperforming GPT-4o.",
      "arxiv_url": "https://arxiv.org/abs/2501.13573",
      "pdf_url": "https://arxiv.org/pdf/2501.13573",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.06792",
      "title": "On the Mutual Influence of Gender and Occupation in LLM Representations",
      "authors": [
        "Haozhe An",
        "Connor Baumler",
        "Abhilasha Sancheti",
        "Rachel Rudinger"
      ],
      "abstract": "We examine LLM representations of gender for first names in various occupational contexts to study how occupations and the gender perception of first names in LLMs influence each other mutually. We find that LLMs' first-name gender representations correlate with real-world gender statistics associated with the name, and are influenced by the co-occurrence of stereotypically feminine or masculine occupations. Additionally, we study the influence of first-name gender representations on LLMs in a downstream occupation prediction task and their potential as an internal metric to identify extrinsic model biases. While feminine first-name embeddings often raise the probabilities for female-dominated jobs (and vice versa for male-dominated jobs), reliably using these internal gender representations for bias detection remains challenging.",
      "arxiv_url": "https://arxiv.org/abs/2503.06792",
      "pdf_url": "https://arxiv.org/pdf/2503.06792",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.10852",
      "title": "Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages",
      "authors": [
        "Zeli Su",
        "Ziyin Zhang",
        "Guixian Xu",
        "Jianing Liu",
        "XU Han",
        "Ting Zhang",
        "Yushuang Dong"
      ],
      "abstract": "While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.",
      "arxiv_url": "https://arxiv.org/abs/2502.10852",
      "pdf_url": "https://arxiv.org/pdf/2502.10852",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7ca3b61c0fc1b30fce6b89a7898cfc9f3da15dfd",
      "title": "RelationalCoder: Rethinking Complex Tables via Programmatic Relational Transformation",
      "authors": [
        "Haoyu Dong",
        "Yue Hu",
        "Huailiang Peng",
        "Yanan Cao"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/7ca3b61c0fc1b30fce6b89a7898cfc9f3da15dfd",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04378",
      "title": "HelpSteer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks",
      "authors": [
        "Zhilin Wang",
        "Jiaqi Zeng",
        "Olivier Delalleau",
        "Daniel Egert",
        "Ellie Evans",
        "Hoo-Chang Shin",
        "Felipe Soares",
        "Yi Dong",
        "Oleksii Kuchaiev"
      ],
      "abstract": "Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect HelpSteer3 data to train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3.",
      "arxiv_url": "https://arxiv.org/abs/2503.04378",
      "pdf_url": "https://arxiv.org/pdf/2503.04378",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7d0b80c17c682cf1c75b588056d71a7a0c6a4170",
      "title": "There’s No Such Thing as Simple Reasoning for LLMs",
      "authors": [
        "Nurul Fajrin Ariyani",
        "Zied Bouraoui",
        "Richard Booth",
        "S. Schockaert"
      ],
      "abstract": "Large Language Models (LLMs) have been widely found to struggle with logical reasoning, where even fine-tuned models fail dramatically on out-of-distribution problems. However, existing work has focused on relatively complex “many-hop” reasoning problems. In this paper, we analyse the performance of fine-tuned LLMs on simple reasoning problems, all of which can be solved in at most three inference steps. Due to the simplicity of these problems, the model cannot encounter test problems that are fundamentally different from those it has seen during training. Unfortunately, however, we find that the models remain highly brittle, being susceptible to seemingly innocent perturbations, such as the addition of duplicates to the set of premises and shuffling the order in which the premises are presented.",
      "arxiv_url": "https://www.semanticscholar.org/paper/7d0b80c17c682cf1c75b588056d71a7a0c6a4170",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7d267538bffb4358e6998eacb4a5cddb4c1e6c08",
      "title": "Multi-level Association Refinement Network for Dialogue Aspect-based Sentiment Quadruple Analysis",
      "authors": [
        "Zeliang Tong",
        "Wei Wei",
        "Xiaoye Qu",
        "Rikui Huang",
        "Zhixin Chen",
        "Xingyu Yan"
      ],
      "abstract": "Dialogue Aspect-based Sentiment Quadruple (DiaASQ) analysis aims to identify all quadru-ples ( i.e. , <target, aspect, opinion, sentiment>) from the dialogue. This task is challenging as different elements within a quadruple may manifest in different utterances, requiring precise handling of associations at both the utterance and word levels. However, most existing meth-ods tackling it predominantly leverage predefined dialogue structure ( e.g. , reply) and word semantics, resulting in a surficial understanding of the deep sentiment association between utterances and words. In this paper, we propose a novel M ulti-level A ssociation R efinement N etwork (MARN) designed to achieve more accurate and comprehensive sentiment associations between utterances and words. Specifically, for utterances, we dynamically capture their associations with enriched semantic features through a holistic understanding of the dialogue, aligning them more closely with sentiment associations within elements in quadru-ples. For words, we develop a novel cross-utterance syntax parser (CU-Parser) that fully exploits syntactic information to enhance the association between word pairs within and across utterances. Moreover, to address the scarcity of labeled data in DiaASQ, we further introduce a multi-view data augmentation strategy to enhance the performance",
      "arxiv_url": "https://www.semanticscholar.org/paper/7d267538bffb4358e6998eacb4a5cddb4c1e6c08",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.07659",
      "title": "Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent",
      "authors": [
        "E. Wilcox",
        "Cui Ding",
        "Giovanni Acampa",
        "Tiago Pimentel",
        "Alex Warstadt",
        "Tamar I. Regev"
      ],
      "abstract": "This paper argues that the relationship between lexical identity and prosody -- one well-studied parameter of linguistic variation -- can be characterized using information theory. We predict that languages that use prosody to make lexical distinctions should exhibit a higher mutual information between word identity and prosody, compared to languages that don't. We test this hypothesis in the domain of pitch, which is used to make lexical distinctions in tonal languages, like Cantonese. We use a dataset of speakers reading sentences aloud in ten languages across five language families to estimate the mutual information between the text and their pitch curves. We find that, across languages, pitch curves display similar amounts of entropy. However, these curves are easier to predict given their associated text in the tonal languages, compared to pitch- and stress-accent languages, and thus the mutual information is higher in these languages, supporting our hypothesis. Our results support perspectives that view linguistic typology as gradient, rather than categorical.",
      "arxiv_url": "https://arxiv.org/abs/2505.07659",
      "pdf_url": "https://arxiv.org/pdf/2505.07659",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04482",
      "title": "Understanding and Meeting Practitioner Needs When Measuring Representational Harms Caused by LLM-Based Systems",
      "authors": [
        "Emma Harvey",
        "Emily Sheng",
        "Su Lin Blodgett",
        "Alexandra Chouldechova",
        "J. Garcia-Gathright",
        "Alexandra Olteanu",
        "Hanna M. Wallach"
      ],
      "abstract": "The NLP research community has made publicly available numerous instruments for measuring representational harms caused by large language model (LLM)-based systems. These instruments have taken the form of datasets, metrics, tools, and more. In this paper, we examine the extent to which such instruments meet the needs of practitioners tasked with evaluating LLM-based systems. Via semi-structured interviews with 12 such practitioners, we find that practitioners are often unable to use publicly available instruments for measuring representational harms. We identify two types of challenges. In some cases, instruments are not useful because they do not meaningfully measure what practitioners seek to measure or are otherwise misaligned with practitioner needs. In other cases, instruments - even useful instruments - are not used by practitioners due to practical and institutional barriers impeding their uptake. Drawing on measurement theory and pragmatic measurement, we provide recommendations for addressing these challenges to better meet practitioner needs.",
      "arxiv_url": "https://arxiv.org/abs/2506.04482",
      "pdf_url": "https://arxiv.org/pdf/2506.04482",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17817",
      "title": "Predicting Through Generation: Why Generation Is Better for Prediction",
      "authors": [
        "Md. Kowsher",
        "Nusrat Jahan Prottasha",
        "Prakash Bhat",
        "Chun-Nam Yu",
        "Mojtaba Soltanalian",
        "Ivan Garibay",
        "Ozlem O. Garibay",
        "Chen Chen",
        "Niloofar Yousefi"
      ],
      "abstract": "This paper argues that generating output tokens is more effective than using pooled representations for prediction tasks because token-level generation retains more mutual information. Since LLMs are trained on massive text corpora using next-token prediction, generation aligns naturally with their learned behavior. Using the Data Processing Inequality (DPI), we provide both theoretical and empirical evidence supporting this claim. However, autoregressive models face two key challenges when used for prediction: (1) exposure bias, where the model sees ground truth tokens during training but relies on its own predictions during inference, leading to errors, and (2) format mismatch, where discrete tokens do not always align with the tasks required output structure. To address these challenges, we introduce PredGen(Predicting Through Generating), an end to end framework that (i) uses scheduled sampling to reduce exposure bias, and (ii) introduces a task adapter to convert the generated tokens into structured outputs. Additionally, we introduce Writer-Director Alignment Loss (WDAL), which ensures consistency between token generation and final task predictions, improving both text coherence and numerical accuracy. We evaluate PredGen on multiple classification and regression benchmarks. Our results show that PredGen consistently outperforms standard baselines, demonstrating its effectiveness in structured prediction tasks.",
      "arxiv_url": "https://arxiv.org/abs/2502.17817",
      "pdf_url": "https://arxiv.org/pdf/2502.17817",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.15755",
      "title": "VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service",
      "authors": [
        "Xiasi Wang",
        "Tianliang Yao",
        "Simin Chen",
        "Runqi Wang",
        "Lei Ye",
        "Kuofeng Gao",
        "Yi Huang",
        "Yuan Yao"
      ],
      "abstract": "Vision-Language Models (VLMs) have demonstrated great potential in real-world applications. While existing research primarily focuses on improving their accuracy, the efficiency remains underexplored. Given the real-time demands of many applications and the high inference overhead of VLMs, efficiency robustness is a critical issue. However, previous studies evaluate efficiency robustness under unrealistic assumptions, requiring access to the model architecture and parameters -- an impractical scenario in ML-as-a-service settings, where VLMs are deployed via inference APIs. To address this gap, we propose VLMInferSlow, a novel approach for evaluating VLM efficiency robustness in a realistic black-box setting. VLMInferSlow incorporates fine-grained efficiency modeling tailored to VLM inference and leverages zero-order optimization to search for adversarial examples. Experimental results show that VLMInferSlow generates adversarial images with imperceptible perturbations, increasing the computational cost by up to 128.47%. We hope this research raises the community's awareness about the efficiency robustness of VLMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.15755",
      "pdf_url": "https://arxiv.org/pdf/2506.15755",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7ddbf66fb19a7bf4bb823662bb9b395fce07bbb8",
      "title": "Crab: A Novel Configurable Role-Playing LLM with Assessing Benchmark",
      "authors": [
        "Kai He",
        "Yucheng Huang",
        "Wenqing Wang",
        "Delong Ran",
        "Dongming Sheng",
        "Junxuan Huang",
        "Qika Lin",
        "Jiaxing Xu",
        "Wenqiang Liu",
        "Mengling Feng"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/7ddbf66fb19a7bf4bb823662bb9b395fce07bbb8",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7de535fb497bd13dd1512a213d4d7c822ef0b29e",
      "title": "AutoChunker: Structured Text Chunking and its Evaluation",
      "authors": [
        "Arihant Jain",
        "Purav Aggarwal",
        "Anoop Saladi"
      ],
      "abstract": "Text chunking is fundamental to modern retrieval-augmented systems, yet existing meth-ods often struggle with maintaining semantic coherence, both within and across chunks, while dealing with document structure and noise. We present AutoChunker, a bottom-up approach for text chunking that combines document structure awareness with noise elimination. AutoChunker leverages language models to identify and segregate logical units of information (a chunk) while preserving document hierarchy through a tree-based representation. To evaluate the chunking operator, we introduce a comprehensive evaluation framework based on five core tenets: noise reduction, completeness, context coherence, task relevance, and retrieval performance. Experimental results on Support and Wikipedia articles demonstrate that AutoChunker significantly outperforms existing methods, reducing noise while improving chunk completeness compared to state-of-the-art baselines. When integrated with an online product support system, our approach led to improvements in retrieval performance and customer return rates. Our work not only advances the state of text chunking but also provides a standardized framework for evaluating chunking strategies, addressing a critical gap in the field.",
      "arxiv_url": "https://www.semanticscholar.org/paper/7de535fb497bd13dd1512a213d4d7c822ef0b29e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17184",
      "title": "Measuring Data Diversity for Instruction Tuning: A Systematic Analysis and A Reliable Metric",
      "authors": [
        "Yuming Yang",
        "Yang Nan",
        "Junjie Ye",
        "Shihan Dou",
        "Xiao Wang",
        "Shuo Li",
        "Huijie Lv",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by evaluating their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information density in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level\"novelty.\"Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric. The code is available at https://github.com/UmeanNever/NovelSum.",
      "arxiv_url": "https://arxiv.org/abs/2502.17184",
      "pdf_url": "https://arxiv.org/pdf/2502.17184",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.09289",
      "title": "UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench",
      "authors": [
        "Boxi Yu",
        "Yuxuan Zhu",
        "Pinjia He",
        "Daniel Kang"
      ],
      "abstract": "The advent of Large Language Models (LLMs) has spurred the development of coding agents for real-world code generation. As a widely used benchmark for evaluating the code generation capabilities of these agents, SWE-Bench uses real-world problems based on GitHub issues and their corresponding pull requests. However, the manually written test cases included in these pull requests are often insufficient, allowing generated patches to pass the tests without resolving the underlying issue. To address this challenge, we introduce UTGenerator, an LLM-driven test case generator that automatically analyzes codebases and dependencies to generate test cases for real-world Python projects. Building on UTGenerator, we propose UTBoost, a comprehensive framework for test case augmentation. In our evaluation, we identified 36 task instances with insufficient test cases and uncovered 345 erroneous patches incorrectly labeled as passed in the original SWE Bench. These corrections, impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard entries, yield 18 and 11 ranking changes, respectively.",
      "arxiv_url": "https://arxiv.org/abs/2506.09289",
      "pdf_url": "https://arxiv.org/pdf/2506.09289",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.14321",
      "title": "Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach",
      "authors": [
        "Xingyu Li",
        "Chen Gong",
        "Guohong Fu"
      ],
      "abstract": "Multimodal coreference resolution (MCR) aims to identify mentions referring to the same entity across different modalities, such as text and visuals, and is essential for understanding multimodal content. In the era of rapidly growing mutimodal content and social media, MCR is particularly crucial for interpreting user interactions and bridging text-visual references to improve communication and personalization. However, MCR research for real-world dialogues remains unexplored due to the lack of sufficient data resources. To address this gap, we introduce TikTalkCoref, the first Chinese multimodal coreference dataset for social media in real-world scenarios, derived from the popular Douyin short-video platform. This dataset pairs short videos with corresponding textual dialogues from user comments and includes manually annotated coreference clusters for both person mentions in the text and the coreferential person head regions in the corresponding video frames. We also present an effective benchmark approach for MCR, focusing on the celebrity domain, and conduct extensive experiments on our dataset, providing reliable benchmark results for this newly constructed dataset. We will release the TikTalkCoref dataset to facilitate future research on MCR for real-world social media dialogues.",
      "arxiv_url": "https://arxiv.org/abs/2504.14321",
      "pdf_url": "https://arxiv.org/pdf/2504.14321",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Personalization"
      ],
      "published_date": "2025-04-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7e108134982ddeb412c53bb6eeb22ef1d906a0ff",
      "title": "Rhetorical Device-Aware Sarcasm Detection with Counterfactual Data Augmentation",
      "authors": [
        "Qingqing Hong",
        "Dongyu Zhang",
        "Jiayi Lin",
        "Dapeng Yin",
        "Shuyue Zhu",
        "Junli Wang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/7e108134982ddeb412c53bb6eeb22ef1d906a0ff",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12054",
      "title": "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
      "authors": [
        "Xinyu Zhang",
        "Yuxuan Dong",
        "Yanrui Wu",
        "Jiaxing Huang",
        "Chengyou Jia",
        "Basura Fernando",
        "Mike Zheng Shou",
        "Lingling Zhang",
        "Jun Liu"
      ],
      "abstract": "Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physics-based reasoning - a complex task requiring physics theorems and constraints. We present PhysReason, a 1,200-problem benchmark comprising knowledge-based (25%) and reasoning-based (75%) problems, where the latter are divided into three difficulty levels (easy, medium, hard). Notably, problems require an average of 8.1 solution steps, with hard requiring 15.6, reflecting the complexity of physics-based reasoning. We propose the Physics Solution Auto Scoring Framework, incorporating efficient answer-level and comprehensive step-level evaluations. Top-performing models like Deepseek-R1, Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on answer-level evaluation, with performance dropping from knowledge questions (75.11%) to hard problems (31.95%). Through step-level evaluation, we identified four key bottlenecks: Physics Theorem Application, Physics Process Understanding, Calculation, and Physics Condition Analysis. These findings position PhysReason as a novel and comprehensive benchmark for evaluating physics-based reasoning capabilities in large language models. Our code and data will be published at https:/dxzxy12138.github.io/PhysReason.",
      "arxiv_url": "https://arxiv.org/abs/2502.12054",
      "pdf_url": "https://arxiv.org/pdf/2502.12054",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7e8e5c69ee68f3bb865efec9fd3bb0f65a1ab58d",
      "title": "Towards Robust Universal Information Extraction: Dataset, Evaluation, and Solution",
      "authors": [
        "Jizhao Zhu",
        "Akang Shi",
        "Zixuan Li",
        "Long Bai",
        "Xiaolong Jin",
        "Jiafeng Guo",
        "Xueqi Cheng"
      ],
      "abstract": "In this paper, we aim to enhance the robustness of Universal Information Extraction (UIE) by introducing a new benchmark dataset, a comprehensive evaluation, and a feasible solution. Existing robust benchmark datasets have two key limitations: 1) They generate only a limited range of perturbations for a single Information Extraction (IE) task, which fails to evaluate the robustness of UIE models effectively; 2) They rely on small models or handcrafted rules to generate perturbations, often resulting in unnatural adversarial examples. Considering the powerful generation capabilities of Large Language Models (LLMs), we introduce a new benchmark dataset for Robust UIE, called RUIE-Bench, which utilizes LLMs to generate more diverse and realistic perturbations across different IE tasks. Based on this dataset, we comprehensively evaluate existing UIE models and reveal that both LLM-based models and other models suffer from significant performance drops. To improve robustness and reduce training costs, we pro-pose a data-augmentation solution that dynamically selects hard samples for iterative training based on the model’s inference loss. Experimental results show that training with only 15% of the data leads to an average 8.1% relative performance improvement across three IE tasks. Our code and dataset are available at: https://github.com/ICT-GoKnow/RobustUIE.",
      "arxiv_url": "https://www.semanticscholar.org/paper/7e8e5c69ee68f3bb865efec9fd3bb0f65a1ab58d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.03499",
      "title": "State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for State Space Models",
      "authors": [
        "Wonjun Kang",
        "Kevin Galim",
        "Yuchen Zeng",
        "Minjae Lee",
        "Hyung Il Koo",
        "Namik Cho"
      ],
      "abstract": "State Space Models (SSMs) have emerged as efficient alternatives to Transformers, mitigating their quadratic computational cost. However, the application of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains largely unexplored. In particular, prompt-based methods like Prompt Tuning and Prefix-Tuning, which are widely used in Transformers, do not perform well on SSMs. To address this, we propose state-based methods as a superior alternative to prompt-based methods. This new family of methods naturally stems from the architectural characteristics of SSMs. State-based methods adjust state-related features directly instead of depending on external prompts. Furthermore, we introduce a novel state-based PEFT method: State-offset Tuning. At every timestep, our method directly affects the state at the current step, leading to more effective adaptation. Through extensive experiments across diverse datasets, we demonstrate the effectiveness of our method. Code is available at https://github.com/furiosa-ai/ssm-state-tuning.",
      "arxiv_url": "https://arxiv.org/abs/2503.03499",
      "pdf_url": "https://arxiv.org/pdf/2503.03499",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.16856",
      "title": "Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification",
      "authors": [
        "Alexander Shvets"
      ],
      "abstract": "Most datasets for sentiment analysis lack context in which an opinion was expressed, often crucial for emotion understanding, and are mainly limited by a few emotion categories. Foundation large language models (LLMs) like GPT-4 suffer from over-predicting emotions and are too resource-intensive. We design an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b, for the generation of training examples for more accessible, lightweight BERT-type encoder models. We focus on enlarging the semantic diversity of examples and propose grounding the generation into a corpus of narratives to produce non-repetitive story-character-centered utterances with unique contexts over 28 emotion classes. By running 700K inferences in 450 GPU hours, we contribute with the dataset of 100K contextual and also 300K context-less examples to cover both scenarios. We use it for fine-tuning pre-trained encoders, which results in several Emo Pillars models. We show that Emo Pillars models are highly adaptive to new domains when tuned to specific tasks such as GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on the first three. We also validate our dataset, conducting statistical analysis and human evaluation, and confirm the success of our measures in utterance diversification (although less for the neutral class) and context personalization, while pointing out the need for improved handling of out-of-taxonomy labels within the pipeline.",
      "arxiv_url": "https://arxiv.org/abs/2504.16856",
      "pdf_url": "https://arxiv.org/pdf/2504.16856",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-04-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.13439",
      "title": "D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model",
      "authors": [
        "Grace Byun",
        "Jinho D. Choi"
      ],
      "abstract": "Evaluating generative models with open-ended generation is challenging due to inconsistencies in response formats. Multiple-choice (MC) evaluation mitigates this issue, but generating high-quality distractors is time-consuming and labor-intensive. We introduce D-GEN, the first open-source distractor generator model that transforms open-ended data into an MC format. To evaluate distractor quality, we propose two novel methods: (1) ranking alignment, ensuring generated distractors retain the discriminatory power of ground-truth distractors, and (2) entropy analysis, comparing model confidence distributions. Our results show that D-GEN preserves ranking consistency (Spearman's rho 0.99, Kendall's tau 0.94) and closely matches the entropy distribution of ground-truth distractors. Human evaluation further confirms the fluency, coherence, distractiveness, and incorrectness. Our work advances robust and efficient distractor generation with automated evaluation, setting a new standard for MC evaluation.",
      "arxiv_url": "https://arxiv.org/abs/2504.13439",
      "pdf_url": "https://arxiv.org/pdf/2504.13439",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7edfee383b265796431c7347d3cb6670dbffc1e0",
      "title": "V-ALPHASOCIAL: Benchmark and Self-Reflective Chain-of-Thought Generation for Visual Social Commonsense Reasoning",
      "authors": [
        "Zongyu Lin",
        "Zhikun Xu",
        "Xiaohan Song",
        "Yixin Wan",
        "Xingcheng Yao",
        "Tsung-Han Lin",
        "Selina Song",
        "Pranav Subbaraman",
        "Ben Zhou",
        "Kai-Wei Chang",
        "Yizhou Sun"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/7edfee383b265796431c7347d3cb6670dbffc1e0",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7efd0e8f775fd2b826757fe65b9a788ae3234cf7",
      "title": "AdaEdit: Advancing Continuous Knowledge Editing For Large Language Models",
      "authors": [
        "Qi Li",
        "Xiaowen Chu"
      ],
      "abstract": "Knowledge editing (KE) has emerged as a prominent alternative that enables efficient and precise information modification inside language models. However, a critical challenge arises in continuous language model editing — a significant performance decline both in knowledge update and retention when the number of edits increases. By dissecting the perturbation weight of language model in continuous KE, we uncover that disentangled and sparsi-fied knowledge representation can significantly alleviate the performance decline. Building on these insights, we introduce AdaEdit, a novel knowledge editing method. Extensive empirical evaluations on multiple LLMs demonstrate that our proposed methods can enhance the performance of edited LLMs in large-size continuous editing regimes, outperforming existing ones without substantially compromising the general abilities of these models.",
      "arxiv_url": "https://www.semanticscholar.org/paper/7efd0e8f775fd2b826757fe65b9a788ae3234cf7",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7f1c65bfd9988bbbe0aa54a1567a96a26dbe9864",
      "title": "Document-Level Event-Argument Data Augmentation for Challenging Role Types",
      "authors": [
        "Joseph Gatto",
        "Omar Sharif",
        "Parker Seegmiller",
        "S. Preum"
      ],
      "abstract": "Event Argument Extraction (EAE) is a daunting information extraction problem — with significant limitations in few-shot cross-domain (FSCD) settings. A common solution to FSCD modeling is data augmentation. Unfortunately, existing augmentation methods are not well-suited to a variety of real-world EAE contexts, including (i) modeling long documents (documents with over 10 sentences), and (ii) modeling challenging role types (i.e., event roles with little to no training data and semantically outlying roles). We introduce two novel LLM-powered data augmentation methods for generating extractive document-level EAE samples using zero in-domain training data . We validate the generalizability of our approach on four datasets — showing significant performance increases in low-resource settings. Our highest performing models provide a 13-pt increase in F1 score on zero-shot role extraction in FSCD evaluation.",
      "arxiv_url": "https://www.semanticscholar.org/paper/7f1c65bfd9988bbbe0aa54a1567a96a26dbe9864",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01784",
      "title": "iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering",
      "authors": [
        "Shuai Wang",
        "Yinan Yu"
      ],
      "abstract": "Large Language Models (LLMs) excel in many natural language processing tasks but often exhibit factual inconsistencies in knowledge-intensive settings. Integrating external knowledge resources, particularly knowledge graphs (KGs), provides a transparent and updatable foundation for more reliable reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons over KGs, is central to this effort, especially for complex, multi-hop queries. However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop connections. To tackle these challenges, we introduce iQUEST, a question-guided KBQA framework that iteratively decomposes complex queries into simpler sub-questions, ensuring a structured and focused reasoning trajectory. Additionally, we integrate a Graph Neural Network (GNN) to look ahead and incorporate 2-hop neighbor information at each reasoning step. This dual approach strengthens the reasoning process, enabling the model to explore viable paths more effectively. Detailed experiments demonstrate the consistent improvement delivered by iQUEST across four benchmark datasets and four LLMs. The code is publicly available at: https://github.com/Wangshuaiia/iQUEST.",
      "arxiv_url": "https://arxiv.org/abs/2506.01784",
      "pdf_url": "https://arxiv.org/pdf/2506.01784",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.23487",
      "title": "Large Language and Reasoning Models are Shallow Disjunctive Reasoners",
      "authors": [
        "Irtaza Khalid",
        "Amir Masoud Nourollah",
        "S. Schockaert"
      ],
      "abstract": "Large Language Models (LLMs) have been found to struggle with systematic reasoning. Even on tasks where they appear to perform well, their performance often depends on shortcuts, rather than on genuine reasoning abilities, leading them to collapse on out-of-distribution (OOD) examples. Post-training strategies based on reinforcement learning and chain-of-thought prompting have recently been hailed as a step change. However, little is known about the potential of the resulting ``Large Reasoning Models'' (LRMs) beyond maths and programming-based problem solving, where genuine OOD problems can be sparse. In this paper, we focus on tasks that require systematic relational composition for qualitative spatial and temporal reasoning. The setting allows fine control over problem difficulty to precisely measure OOD generalization. We find that, zero-shot LRMs generally outperform their LLM counterparts in single-path reasoning tasks but struggle in the multi-path setting. Whilst showing comparatively better results, fine-tuned LLMs are also not capable of multi-path generalization. We also provide evidence for the behavioral interpretation for this, i.e., that LRMs are shallow disjunctive reasoners.",
      "arxiv_url": "https://arxiv.org/abs/2503.23487",
      "pdf_url": "https://arxiv.org/pdf/2503.23487",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.05036",
      "title": "nvAgent: Automated Data Visualization from Natural Language via Collaborative Agent Workflow",
      "authors": [
        "Geliang Ouyang",
        "Jingyao Chen",
        "Zhihe Nie",
        "Yi Gui",
        "Yao Wan",
        "Hongyu Zhang",
        "Dongping Chen"
      ],
      "abstract": "Natural Language to Visualization (NL2Vis) seeks to convert natural-language descriptions into visual representations of given tables, empowering users to derive insights from large-scale data. Recent advancements in Large Language Models (LLMs) show promise in automating code generation to transform tabular data into accessible visualizations. However, they often struggle with complex queries that require reasoning across multiple tables. To address this limitation, we propose a collaborative agent workflow, termed nvAgent, for NL2Vis. Specifically, nvAgent comprises three agents: a processor agent for database processing and context filtering, a composer agent for planning visualization generation, and a validator agent for code translation and output verification. Comprehensive evaluations on the new VisEval benchmark demonstrate that nvAgent consistently surpasses state-of-the-art baselines, achieving a 7.88% improvement in single-table and a 9.23% improvement in multi-table scenarios. Qualitative analyses further highlight that nvAgent maintains nearly a 20% performance margin over previous models, underscoring its capacity to produce high-quality visual representations from complex, heterogeneous data sources.",
      "arxiv_url": "https://arxiv.org/abs/2502.05036",
      "pdf_url": "https://arxiv.org/pdf/2502.05036",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03519",
      "title": "An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals",
      "authors": [
        "Yangyang Zhao",
        "Ben Niu",
        "Libo Qin",
        "Shihan Wang"
      ],
      "abstract": "Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue systems to optimize dialogue policy, but it struggles to balance exploration and exploitation due to the high dimensionality of state and action spaces. This challenge often results in local optima or poor convergence. Evolutionary Algorithms (EAs) have been proven to effectively explore the solution space of neural networks by maintaining population diversity. Inspired by this, we innovatively combine the global search capabilities of EA with the local optimization of DRL to achieve a balance between exploration and exploitation. Nevertheless, the inherent flexibility of natural language in dialogue tasks complicates this direct integration, leading to prolonged evolutionary times. Thus, we further propose an elite individual injection mechanism to enhance EA's search efficiency by adaptively introducing best-performing individuals into the population. Experiments across four datasets show that our approach significantly improves the balance between exploration and exploitation, boosting performance. Moreover, the effectiveness of the EII mechanism in reducing exploration time has been demonstrated, achieving an efficient integration of EA and DRL on task-oriented dialogue policy tasks.",
      "arxiv_url": "https://arxiv.org/abs/2506.03519",
      "pdf_url": "https://arxiv.org/pdf/2506.03519",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7f948e5b5effbc220ea558620979122665882737",
      "title": "Can We Trust AI Doctors? A Survey of Medical Hallucination in Large Language and Large Vision-Language Models",
      "authors": [
        "Zhihong Zhu",
        "Yunyan Zhang",
        "Xianwei Zhuang",
        "Fan Zhang",
        "Zhongwei Wan",
        "Yuyan Chen",
        "Qingqing Long",
        "Yefeng Zheng",
        "Xian Wu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/7f948e5b5effbc220ea558620979122665882737",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12988",
      "title": "Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in LLMs",
      "authors": [
        "Zixiao Wang",
        "Duzhen Zhang",
        "Ishita Agrawal",
        "Shen Gao",
        "Le Song",
        "Xiuyi Chen"
      ],
      "abstract": "Previous approaches to persona simulation large language models (LLMs) have typically relied on learning basic biographical information, or using limited role-play dialogue datasets to capture a character's responses. However, a holistic representation of an individual goes beyond surface-level facts or conversations to deeper thoughts and thinking. In this work, we introduce CharacterBot, a model designed to replicate both the linguistic patterns and distinctive thought processes of a character. Using Lu Xun, a renowned Chinese writer, as a case study, we propose four training tasks derived from his 17 essay collections. These include a pre-training task focused on mastering external linguistic structures and knowledge, as well as three fine-tuning tasks: multiple-choice question answering, generative question answering, and style transfer, each aligning the LLM with Lu Xun's internal ideation and writing style. To optimize learning across these tasks, we introduce a CharLoRA parameter updating mechanism, where a general linguistic style expert collaborates with other task-specific experts to better study both the language style and the understanding of deeper thoughts. We evaluate CharacterBot on three tasks for linguistic accuracy and opinion comprehension, demonstrating that it significantly outperforms the baselines on our adapted metrics. We hope that this work inspires future research on deep character persona simulation LLM.",
      "arxiv_url": "https://arxiv.org/abs/2502.12988",
      "pdf_url": "https://arxiv.org/pdf/2502.12988",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20508",
      "title": "TripCraft: A Benchmark for Spatio-Temporally Fine Grained Travel Planning",
      "authors": [
        "Soumyabrata Chaudhuri",
        "Pranav Purkar",
        "Ritwik Raghav",
        "Shubhojit Mallick",
        "Manish Gupta",
        "Abhik Jana",
        "Shreya Ghosh"
      ],
      "abstract": "Recent advancements in probing Large Language Models (LLMs) have explored their latent potential as personalized travel planning agents, yet existing benchmarks remain limited in real world applicability. Existing datasets, such as TravelPlanner and TravelPlanner+, suffer from semi synthetic data reliance, spatial inconsistencies, and a lack of key travel constraints, making them inadequate for practical itinerary generation. To address these gaps, we introduce TripCraft, a spatiotemporally coherent travel planning dataset that integrates real world constraints, including public transit schedules, event availability, diverse attraction categories, and user personas for enhanced personalization. To evaluate LLM generated plans beyond existing binary validation methods, we propose five continuous evaluation metrics, namely Temporal Meal Score, Temporal Attraction Score, Spatial Score, Ordering Score, and Persona Score which assess itinerary quality across multiple dimensions. Our parameter informed setting significantly enhances meal scheduling, improving the Temporal Meal Score from 61% to 80% in a 7 day scenario. TripCraft establishes a new benchmark for LLM driven personalized travel planning, offering a more realistic, constraint aware framework for itinerary generation. Dataset and Codebase will be made publicly available upon acceptance.",
      "arxiv_url": "https://arxiv.org/abs/2502.20508",
      "pdf_url": "https://arxiv.org/pdf/2502.20508",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11275",
      "title": "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest",
      "authors": [
        "Letian Peng",
        "Zilong Wang",
        "Feng Yao",
        "Jingbo Shang"
      ],
      "abstract": "Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE models can act as free riders on LLM resources by reframing next-token \\emph{prediction} into \\emph{extraction} for tokens already present in the context. Specifically, our proposed next tokens extraction (NTE) paradigm learns a versatile IE model, \\emph{Cuckoo}, with 102.6M extractive data converted from LLM's pre-training and post-training data. Under the few-shot setting, Cuckoo adapts effectively to traditional and complex instruction-following IE with better performance than existing pre-trained IE models. As a free rider, Cuckoo can naturally evolve with the ongoing advancements in LLM data preparation, benefiting from improvements in LLM training pipelines without additional manual effort.",
      "arxiv_url": "https://arxiv.org/abs/2502.11275",
      "pdf_url": "https://arxiv.org/pdf/2502.11275",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "7fd9c5505f200d9df33b290157e6a78c50054995",
      "title": "Teaching Text Agents to Learn Sequential Decision Making from Failure",
      "authors": [
        "Canasai Kruengkrai",
        "Koichiro Yoshino"
      ],
      "abstract": "Text-based reinforcement-learning agents improve their policies by interacting with their environments to collect more training data. However, these self-collected data inevitably contain intermediate failed actions caused by at-tempting physically infeasible behaviors and/or hallucinations. Directly learning a policy from such trajectories can reinforce incorrect behaviors and reduce task success rates. In this paper, we propose a failed action-aware ob-jective that suppresses the negative impact of failed actions during training by assigning zero return based on textual feedback. Building on this objective, we introduce a perturbation method that leverages unsuccessful trajectories to construct new successful ones that share the same goal. This allows agents to benefit from diverse experiences without further interaction with the environment. Experiments in ALFWorld and ScienceWorld demonstrate that our method significantly outperforms strong baselines and generalizes across environments. Code is available at https://github.com/ riken-grp/text-agent .",
      "arxiv_url": "https://www.semanticscholar.org/paper/7fd9c5505f200d9df33b290157e6a78c50054995",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.08468",
      "title": "Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?",
      "authors": [
        "Md Tahmid Rahman Laskar",
        "Mohammed Saidul Islam",
        "Ridwan Mahbub",
        "Ahmed Masry",
        "Mizanur Rahman",
        "Amran Bhuiyan",
        "Mir Tafseer Nayeem",
        "Shafiq Joty",
        "Enamul Hoque",
        "Jimmy X. Huang"
      ],
      "abstract": "Charts are ubiquitous as they help people understand and reason with data. Recently, various downstream tasks, such as chart question answering, chart2text, and fact-checking, have emerged. Large Vision-Language Models (LVLMs) show promise in tackling these tasks, but their evaluation is costly and time-consuming, limiting real-world deployment. While using LVLMs as judges to assess the chart comprehension capabilities of other LVLMs could streamline evaluation processes, challenges like proprietary datasets, restricted access to powerful models, and evaluation costs hinder their adoption in industrial settings. To this end, we present a comprehensive evaluation of 13 open-source LVLMs as judges for diverse chart comprehension and reasoning tasks. We design both pairwise and pointwise evaluation tasks covering criteria like factual correctness, informativeness, and relevancy. Additionally, we analyze LVLM judges based on format adherence, positional consistency, length bias, and instruction-following. We focus on cost-effective LVLMs (<10B parameters) suitable for both research and commercial use, following a standardized evaluation protocol and rubric to measure the LVLM judge's accuracy. Experimental results reveal notable variability: while some open LVLM judges achieve GPT-4-level evaluation performance (about 80% agreement with GPT-4 judgments), others struggle (below ~10% agreement). Our findings highlight that state-of-the-art open-source LVLMs can serve as cost-effective automatic evaluators for chart-related tasks, though biases such as positional preference and length bias persist.",
      "arxiv_url": "https://arxiv.org/abs/2505.08468",
      "pdf_url": "https://arxiv.org/pdf/2505.08468",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17710",
      "title": "Mind the Gesture: Evaluating AI Sensitivity to Culturally Offensive Non-Verbal Gestures",
      "authors": [
        "Akhila Yerukola",
        "Saadia Gabriel",
        "Nanyun Peng",
        "Maarten Sap"
      ],
      "abstract": "Gestures are an integral part of non-verbal communication, with meanings that vary across cultures, and misinterpretations that can have serious social and diplomatic consequences. As AI systems become more integrated into global applications, ensuring they do not inadvertently perpetuate cultural offenses is critical. To this end, we introduce Multi-Cultural Set of Inappropriate Gestures and Nonverbal Signs (MC-SIGNS), a dataset of 288 gesture-country pairs annotated for offensiveness, cultural significance, and contextual factors across 25 gestures and 85 countries. Through systematic evaluation using MC-SIGNS, we uncover critical limitations: text-to-image (T2I) systems exhibit strong US-centric biases, performing better at detecting offensive gestures in US contexts than in non-US ones; large language models (LLMs) tend to over-flag gestures as offensive; and vision-language models (VLMs) default to US-based interpretations when responding to universal concepts like wishing someone luck, frequently suggesting culturally inappropriate gestures. These findings highlight the urgent need for culturally-aware AI safety mechanisms to ensure equitable global deployment of AI technologies.",
      "arxiv_url": "https://arxiv.org/abs/2502.17710",
      "pdf_url": "https://arxiv.org/pdf/2502.17710",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "80924aa30152d050d47dd726251bd38e36926b94",
      "title": "ODDA: An OODA-Driven Diverse Data Augmentation Framework for Low-Resource Relation Extraction",
      "authors": [
        "Yijie Zhong",
        "Yunfan Gao",
        "Xiaolian Zhang",
        "Haofen Wang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/80924aa30152d050d47dd726251bd38e36926b94",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12278",
      "title": "Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure",
      "authors": [
        "Zheyuan Yang",
        "Zexi Kuang",
        "Xue Xia",
        "Yilun Zhao"
      ],
      "abstract": "We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs in test-case generation. TestCase-Eval includes 500 algorithm problems and 100,000 human-crafted solutions from the Codeforces platform. It focuses on two pivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test sets probe diverse input scenarios and cover a wide range of potential failure modes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored test input that reveals a specific incorrect code implementation. We provide a comprehensive assessment of 19 state-of-the-art open-source and proprietary LLMs on TestCase-Eval, offering insights into their strengths and limitations in generating effective test cases for algorithm problems.",
      "arxiv_url": "https://arxiv.org/abs/2506.12278",
      "pdf_url": "https://arxiv.org/pdf/2506.12278",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "80a18e02501c74a2026b2dafc0afe11ae6ca5790",
      "title": "Dually Self-Improved Counterfactual Data Augmentation Using Large Language Model",
      "authors": [
        "Luhao Zhang",
        "Xinyu Zhang",
        "Linmei Hu",
        "Dandan Song",
        "Liqiang Nie"
      ],
      "abstract": "Counterfactual data augmentation, which generates minimally edited tokens to alter labels, has become a key approach to improving model robustness in natural language processing. It is usually implemented by first identifying the causal terms and then modifying these terms to create counterfactual candidates. The emergence of large language models (LLMs) has effectively facilitated the task of counterfactual data augmentation. However, existing LLM-based approaches still face some challenges in 1) accurately extracting the task-specific causal terms, and 2) the quality of LLM-generated counterfacts. To address the issues, we propose a dually self-improved counterfactual data augmentation method using LLM. On the one hand, we design a self-improved strategy employing the attention distribution of the task model to identify the task-specific causal terms, which is lightweight and task-specific. On the other hand, a second self-improved strategy based on direct preference optimization is utilized to refine LLM-generated counterfacts, achieving high-quality counterfacts. Finally, a balanced loss preventing over-emphasis on augmentated data is proposed to retrain the task model on the fusion of existing data and generated coun-terfacts. Extensive experiments on multiple benchmarks demonstrate the effectiveness of our proposed method in generating high-quality counterfacts for improving task performance.",
      "arxiv_url": "https://www.semanticscholar.org/paper/80a18e02501c74a2026b2dafc0afe11ae6ca5790",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "80c2582057dc3769e68818109583f56a42e60b92",
      "title": "In the LLM era, Word Sense Induction remains unsolved",
      "authors": [
        "Anna Mosolova",
        "Marie Candito",
        "Carlos Ramisch"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/80c2582057dc3769e68818109583f56a42e60b92",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.15442",
      "title": "On the Generalization vs Fidelity Paradox in Knowledge Distillation",
      "authors": [
        "S. K. Ramesh",
        "Ayan Sengupta",
        "Tanmoy Chakraborty"
      ],
      "abstract": "Knowledge distillation (KD) is a key technique for compressing large language models into smaller ones while preserving performance. Despite the recent traction of KD research, its effectiveness for smaller language models (LMs) and the mechanisms driving knowledge transfer remain underexplored. In this work, we present the first large-scale empirical and statistical analysis of KD across models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks in a zero-shot setting. Our findings reveal that KD can improve the average performance of smaller models by up to $10\\%$, with a peak task specific gain of $22\\%$, while providing only marginal benefits ($\\sim 1.3\\%$) for larger models. Surprisingly, teacher performance has a minimal impact on student outcomes, while teacher task expertise impacts KD effectiveness. A correlation study indicates that smaller LMs benefit more from KD, whereas larger LMs show diminished gains. Additionally, we uncover a misalignment between improvements in student performance and reasoning fidelity, suggesting that while KD enhances accuracy, it does not always maintain the structured decision-making processes of the teacher. Our ablation study further highlights the importance of teacher signals and logit smoothing in influencing students'performance after distillation. Overall, our study offers a comprehensive empirical and statistical assessment of KD, highlighting both its benefits and trade-offs when distilling knowledge from larger to smaller LMs.",
      "arxiv_url": "https://arxiv.org/abs/2505.15442",
      "pdf_url": "https://arxiv.org/pdf/2505.15442",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "80c9d2feb76b374d2430633e5de2722595ad62fb",
      "title": "LTRAG: Enhancing Autoformalization and Self-refinement for Logical Reasoning with Thought-Guided RAG",
      "authors": [
        "Ruikang Hu",
        "Shaoyu Lin",
        "Yeliang Xiu",
        "Yongmei Liu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/80c9d2feb76b374d2430633e5de2722595ad62fb",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19759",
      "title": "Does Your Voice Assistant Remember? Analyzing Conversational Context Recall and Utilization in Voice Interaction Models",
      "authors": [
        "Heeseung Kim",
        "Che Hyun Lee",
        "Sangkwon Park",
        "Ji-Ran Yeom",
        "Nohil Park",
        "Sangwon Yu",
        "Sungroh Yoon"
      ],
      "abstract": "Recent advancements in multi-turn voice interaction models have improved user-model communication. However, while closed-source models effectively retain and recall past utterances, whether open-source models share this ability remains unexplored. To fill this gap, we systematically evaluate how well open-source interaction models utilize past utterances using ContextDialog, a benchmark we proposed for this purpose. Our findings show that speech-based models have more difficulty than text-based ones, especially when recalling information conveyed in speech, and even with retrieval-augmented generation, models still struggle with questions about past utterances. These insights highlight key limitations in open-source models and suggest ways to improve memory retention and retrieval robustness.",
      "arxiv_url": "https://arxiv.org/abs/2502.19759",
      "pdf_url": "https://arxiv.org/pdf/2502.19759",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13604",
      "title": "BeamLoRA: Beam-Constraint Low-Rank Adaptation",
      "authors": [
        "Naibin Gu",
        "Zhenyu Zhang",
        "Xiyu Liu",
        "Peng Fu",
        "Zheng Lin",
        "Shuohuan Wang",
        "Yu Sun",
        "Hua Wu",
        "Weiping Wang",
        "Haifeng Wang"
      ],
      "abstract": "Due to the demand for efficient fine-tuning of large language models, Low-Rank Adaptation (LoRA) has been widely adopted as one of the most effective parameter-efficient fine-tuning methods. Nevertheless, while LoRA improves efficiency, there remains room for improvement in accuracy. Herein, we adopt a novel perspective to assess the characteristics of LoRA ranks. The results reveal that different ranks within the LoRA modules not only exhibit varying levels of importance but also evolve dynamically throughout the fine-tuning process, which may limit the performance of LoRA. Based on these findings, we propose BeamLoRA, which conceptualizes each LoRA module as a beam where each rank naturally corresponds to a potential sub-solution, and the fine-tuning process becomes a search for the optimal sub-solution combination. BeamLoRA dynamically eliminates underperforming sub-solutions while expanding the parameter space for promising ones, enhancing performance with a fixed rank. Extensive experiments across three base models and 12 datasets spanning math reasoning, code generation, and commonsense reasoning demonstrate that BeamLoRA consistently enhances the performance of LoRA, surpassing the other baseline methods.",
      "arxiv_url": "https://arxiv.org/abs/2502.13604",
      "pdf_url": "https://arxiv.org/pdf/2502.13604",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.23342",
      "title": "ATGen: A Framework for Active Text Generation",
      "authors": [
        "A. Tsvigun",
        "Daniil Vasilev",
        "Ivan Tsvigun",
        "Ivan Lysenko",
        "Talgat Bektleuov",
        "Aleksandr Medvedev",
        "Uliana Vinogradova",
        "Nikita Severin",
        "Mikhail Mozikov",
        "Andrey Savchenko",
        "Rostislav Grigorev",
        "Ramil Kuleev",
        "Fedor Zhdanov",
        "Artem Shelmanov",
        "Ilya Makarov"
      ],
      "abstract": "Active learning (AL) has demonstrated remarkable potential in reducing the annotation effort required for training machine learning models. However, despite the surging popularity of natural language generation (NLG) tasks in recent years, the application of AL to NLG has been limited. In this paper, we introduce Active Text Generation (ATGen) - a comprehensive framework that bridges AL with text generation tasks, enabling the application of state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered annotation in NLG tasks using both human annotators and automatic annotation agents based on large language models (LLMs). The framework supports LLMs deployed as services, such as ChatGPT and Claude, or operated on-premises. Furthermore, ATGen provides a unified platform for smooth implementation and benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present evaluation results for state-of-the-art AL strategies across diverse settings and multiple text generation tasks. We show that ATGen reduces both the effort of human annotators and costs associated with API calls to LLM-based annotation agents. The code of the framework is available on GitHub under the MIT license. The video presentation is available at http://atgen-video.nlpresearch.group",
      "arxiv_url": "https://arxiv.org/abs/2506.23342",
      "pdf_url": "https://arxiv.org/pdf/2506.23342",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.11110",
      "title": "Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective",
      "authors": [
        "Yiyao Yu",
        "Yuxiang Zhang",
        "Dongdong Zhang",
        "Xiao Liang",
        "Hengyuan Zhang",
        "Xingxing Zhang",
        "Ziyi Yang",
        "Mahmoud Khademi",
        "H. Awadalla",
        "Junjie Wang",
        "Yujiu Yang",
        "Furu Wei"
      ],
      "abstract": "Large Language Models (LLMs) have made notable progress in mathematical reasoning, yet often rely on single-paradigm reasoning, limiting their effectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a novel unified framework integrating multiple reasoning paradigms--Natural Language Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning (SR)--to enable synergistic collaboration. CoR generates multiple potential answers via different reasoning paradigms and synthesizes them into a coherent final solution. We propose a Progressive Paradigm Training (PPT) strategy for models to progressively master these paradigms, leading to CoR-Math-7B. Experimental results demonstrate that CoR-Math-7B significantly outperforms current SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o in theorem proving and a 15.0% improvement over RL-based methods on the MATH benchmark in arithmetic tasks. These results show the enhanced mathematical comprehension ability of our model, enabling zero-shot generalization across tasks.",
      "arxiv_url": "https://arxiv.org/abs/2501.11110",
      "pdf_url": "https://arxiv.org/pdf/2501.11110",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.07114",
      "title": "ChatBench: From Static Benchmarks to Human-AI Evaluation",
      "authors": [
        "Serina Chang",
        "Ashton Anderson",
        "Jake M. Hofman"
      ],
      "abstract": "With the rapid adoption of LLM-based chatbots, there is a pressing need to evaluate what humans and LLMs can achieve together. However, standard benchmarks, such as MMLU, measure LLM capabilities in isolation (i.e.,\"AI-alone\"). Here, we design and conduct a user study to convert MMLU questions into user-AI conversations, by seeding the user with the question and having them carry out a conversation with the LLM to answer their question. We release ChatBench, a new dataset with AI-alone, user-alone, and user-AI data for 396 questions and two LLMs, including 144K answers and 7,336 user-AI conversations. We find that AI-alone accuracy fails to predict user-AI accuracy, with significant differences across multiple subjects (math, physics, and moral reasoning), and we analyze the user-AI conversations to provide insight into how they diverge from AI-alone benchmarks. Finally, we show that fine-tuning a user simulator on a subset of ChatBench improves its ability to estimate user-AI accuracies, increasing correlation on held-out questions by more than 20 points, creating possibilities for scaling interactive evaluation.",
      "arxiv_url": "https://arxiv.org/abs/2504.07114",
      "pdf_url": "https://arxiv.org/pdf/2504.07114",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01435",
      "title": "Redundancy, Isotropy, and Intrinsic Dimensionality of Prompt-based Text Embeddings",
      "authors": [
        "Hayato Tsukagoshi",
        "Ryohei Sasano"
      ],
      "abstract": "Prompt-based text embedding models, which generate task-specific embeddings upon receiving tailored prompts, have recently demonstrated remarkable performance. However, their resulting embeddings often have thousands of dimensions, leading to high storage costs and increased computational costs of embedding-based operations. In this paper, we investigate how post-hoc dimensionality reduction applied to the embeddings affects the performance of various tasks that leverage these embeddings, specifically classification, clustering, retrieval, and semantic textual similarity (STS) tasks. Our experiments show that even a naive dimensionality reduction, which keeps only the first 25% of the dimensions of the embeddings, results in a very slight performance degradation, indicating that these embeddings are highly redundant. Notably, for classification and clustering, even when embeddings are reduced to less than 0.5% of the original dimensionality the performance degradation is very small. To quantitatively analyze this redundancy, we perform an analysis based on the intrinsic dimensionality and isotropy of the embeddings. Our analysis reveals that embeddings for classification and clustering, which are considered to have very high dimensional redundancy, exhibit lower intrinsic dimensionality and less isotropy compared with those for retrieval and STS.",
      "arxiv_url": "https://arxiv.org/abs/2506.01435",
      "pdf_url": "https://arxiv.org/pdf/2506.01435",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.17095",
      "title": "Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation",
      "authors": [
        "Kristine Ann M. Carandang",
        "Jasper Meynard P. Arana",
        "Ethan Robert A. Casin",
        "Christopher P. Monterola",
        "D. S. Tan",
        "Jesus Felix B. Valenzuela",
        "Christian M. Alis"
      ],
      "abstract": "Due to the legal and ethical responsibilities of healthcare providers (HCPs) for accurate documentation and protection of patient data privacy, the natural variability in the responses of large language models (LLMs) presents challenges for incorporating clinical note generation (CNG) systems, driven by LLMs, into real-world clinical processes. The complexity is further amplified by the detailed nature of texts in CNG. To enhance the confidence of HCPs in tools powered by LLMs, this study evaluates the reliability of 12 open-weight and proprietary LLMs from Anthropic, Meta, Mistral, and OpenAI in CNG in terms of their ability to generate notes that are string equivalent (consistency rate), have the same meaning (semantic consistency) and are correct (semantic similarity), across several iterations using the same prompt. The results show that (1) LLMs from all model families are stable, such that their responses are semantically consistent despite being written in various ways, and (2) most of the LLMs generated notes close to the corresponding notes made by experts. Overall, Meta's Llama 70B was the most reliable, followed by Mistral's Small model. With these findings, we recommend the local deployment of these relatively smaller open-weight models for CNG to ensure compliance with data privacy regulations, as well as to improve the efficiency of HCPs in clinical documentation.",
      "arxiv_url": "https://arxiv.org/abs/2505.17095",
      "pdf_url": "https://arxiv.org/pdf/2505.17095",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15612",
      "title": "LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models",
      "authors": [
        "Hugo Pitorro",
        "Marcos V. Treviso"
      ],
      "abstract": "State space models (SSMs), such as Mamba, have emerged as an efficient alternative to transformers for long-context sequence modeling. However, despite their growing adoption, SSMs lack the interpretability tools that have been crucial for understanding and improving attention-based architectures. While recent efforts provide insights into Mamba's internal mechanisms, they do not explicitly decompose token-wise contributions, leaving gaps in understanding how Mamba selectively processes sequences across layers. In this work, we introduce LaTIM, a novel token-level decomposition method for both Mamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively evaluate our method across diverse tasks, including machine translation, copying, and retrieval-based generation, demonstrating its effectiveness in revealing Mamba's token-to-token interaction patterns.",
      "arxiv_url": "https://arxiv.org/abs/2502.15612",
      "pdf_url": "https://arxiv.org/pdf/2502.15612",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.01652",
      "title": "MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive Environments",
      "authors": [
        "Yin Cai",
        "Zhouhong Gu",
        "Zhaohan Du",
        "Zheyu Ye",
        "Shaosheng Cao",
        "Yiqian Xu",
        "Hongwei Feng",
        "Ping Chen"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in environmental perception, reasoning-based decision-making, and simulating complex human behaviors, particularly in interactive role-playing contexts. This paper introduces the Multiverse Interactive Role-play Ability General Evaluation (MIRAGE), a comprehensive framework designed to assess LLMs'proficiency in portraying advanced human behaviors through murder mystery games. MIRAGE features eight intricately crafted scripts encompassing diverse themes and styles, providing a rich simulation. To evaluate LLMs'performance, MIRAGE employs four distinct methods: the Trust Inclination Index (TII) to measure dynamics of trust and suspicion, the Clue Investigation Capability (CIC) to measure LLMs'capability of conducting information, the Interactivity Capability Index (ICI) to assess role-playing capabilities and the Script Compliance Index (SCI) to assess LLMs'capability of understanding and following instructions. Our experiments indicate that even popular models like GPT-4 face significant challenges in navigating the complexities presented by the MIRAGE. The datasets and simulation codes are available in \\href{https://github.com/lime728/MIRAGE}{github}.",
      "arxiv_url": "https://arxiv.org/abs/2501.01652",
      "pdf_url": "https://arxiv.org/pdf/2501.01652",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04089",
      "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment",
      "authors": [
        "Anastasiia Ivanova",
        "Eva Bakaeva",
        "Z. Volovikova",
        "A. Kovalev",
        "Aleksandr I. Panov"
      ],
      "abstract": "As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at https://github.com/cog-model/AmbiK-dataset.",
      "arxiv_url": "https://arxiv.org/abs/2506.04089",
      "pdf_url": "https://arxiv.org/pdf/2506.04089",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05828",
      "title": "FinanceReasoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging",
      "authors": [
        "Zichen Tang",
        "Ziyan Ma",
        "Haoyang He",
        "Jiacheng Liu",
        "Zhongjun Yang",
        "Zihua Rong",
        "Rongjin Li",
        "Kun Ji",
        "Qing Huang",
        "Xinyang Hu",
        "Yang Liu",
        "Qianhe Zheng",
        "Mark Chen",
        "Jerry Tworek",
        "Hee-woo Jun",
        "Qim-ing Yuan",
        "Henrique Pondé",
        "Oliveira Pinto",
        "Jared Ka-plan",
        "Harri Edwards",
        "Yuri Burda",
        "Nicholas Joseph",
        "Greg Brockman",
        "Alex Ray",
        "Raul Puri",
        "Gretchen Krueger",
        "Michael Petrov",
        "Heidy Khlaaf",
        "Girish Sas-try",
        "Pamela Mishkin",
        "Brooke Chan",
        "Scott Gray",
        "N. Ryder",
        "Mikhail Pavlov",
        "Alethea Power",
        "Lukasz Kaiser",
        "Mo Bavarian",
        "Clemens Winter",
        "P. Tillet",
        "F. Such",
        "Dave Cum-mings",
        "Matthias Plappert",
        "Fotios Chantzis",
        "Eliza-beth Barnes",
        "Ariel Herbert-Voss",
        "William H. Guss",
        "Alex Nichol",
        "A. Paino",
        "N. Tezak",
        "Jie Tang",
        "Igor Babuschkin",
        "S. Balaji",
        "Shantanu Jain",
        "William Saunders",
        "Chris Hesse",
        "Andrew N Carr",
        "Jan Leike",
        "Josh Achiam",
        "Vedant Misra",
        "Evan Morikawa",
        "Alec Radford",
        "Matthew Knight",
        "M. Brundage",
        "Mira Murati",
        "Katie Mayer",
        "Peter Welinder",
        "Bob McGrew",
        "Dario Amodei",
        "Sam McCandlish",
        "Wenhu Chen",
        "Xueguang Ma",
        "Xinyi Wang",
        "William W. Cohen",
        "Ming Yin",
        "Max W.F. Ku",
        "Pan Lu",
        "Yixin Wan",
        "Jianyu Xu",
        "Tony Xia",
        "TheoremQA",
        "Zhiyu Chen",
        "Charese H. Smiley",
        "Sameena Shah",
        "Iana Borova",
        "Dylan Langdon",
        "Reema Moussa",
        "Matt Beane",
        "Ting-Hao Huang",
        "Bryan Routledge",
        "William Yang Wang",
        "Shiyang Li",
        "Zhiqiang Ma",
        "William Yang",
        "ConvFinQA",
        "Daya Guo",
        "Dejian Yang",
        "Haowei Zhang",
        "Jun-Mei Song",
        "Ruoyu Zhang",
        "Runxin Xu",
        "Qihao Zhu",
        "Shirong Ma",
        "Peiyi Wang",
        "Xiaoling Bi",
        "Xiaokang Zhang",
        "Xingkai Yu",
        "Yu Wu",
        "Z. F. Wu",
        "Zhibin Gou",
        "Zhihong Shao",
        "Zhuoshu Li",
        "Ziyi Gao",
        "A. Liu",
        "Bing Xue",
        "Bing-Li Wang",
        "Bochao Wu",
        "Bei Feng",
        "Chengda Lu",
        "Chenggang Zhao",
        "C. Deng",
        "Chenyu Zhang",
        "C. Ruan",
        "Damai Dai",
        "Deli Chen",
        "Dong-Li Ji",
        "Erhang Li",
        "Fangyun Lin",
        "Fucong Dai",
        "Fuli Luo",
        "Guangbo Hao",
        "Guanting Chen",
        "Guowei Li",
        "H. Zhang",
        "Han Bao",
        "Hanwei Xu",
        "Haocheng Wang",
        "Honghui Ding",
        "Huajian Xin",
        "Huazuo Gao",
        "Hui Qu",
        "Hui Li",
        "Jianzhong Guo",
        "Jiashi Li",
        "Jiawei Wang",
        "JingChang Chen",
        "Jingyang Yuan",
        "Junjie Qiu",
        "Junlong Li",
        "J. Cai",
        "J. Ni",
        "Jian Liang",
        "Jin Chen",
        "Kai Dong",
        "Kai Hu",
        "Kaige Gao",
        "Kang Guan",
        "Kexin Huang",
        "K. Yu",
        "Lean Wang",
        "Lecong Zhang",
        "Liang Zhao",
        "Litong Wang",
        "Liyue Zhang",
        "Lei Xu",
        "Leyi Xia",
        "Mingchuan Zhang",
        "Minghua Zhang",
        "M. Tang",
        "Meng Li",
        "Miaojun Wang",
        "Mingming Li",
        "Ning Tian",
        "Panpan Huang",
        "Peng Zhang",
        "Qiancheng Wang",
        "Qinyu Chen",
        "Qiushi Du",
        "Ruiqi Ge",
        "Ruisong Zhang",
        "Ruizhe Pan",
        "Runji Wang",
        "R. J. Chen",
        "R. Jin",
        "Ruyi Chen",
        "Shanghao Lu",
        "Shangyan Zhou",
        "Shanhuang Chen",
        "Wen Liu",
        "W. Liang",
        "Wenjun Gao",
        "Wen-xuan Yu",
        "Wentao Zhang",
        "W. Xiao",
        "Wei An",
        "Xiaodong Liu",
        "Xiaohan Wang",
        "Xiaokang Chen",
        "X. Nie",
        "Xin Cheng",
        "Xin Liu",
        "Xin Xie",
        "Xingchao Liu",
        "Xinyu Yang",
        "Xinyuan Li",
        "Xuecheng Su",
        "Xuheng Lin",
        "Xiangyu Jin",
        "Xi-Cheng Shen",
        "Xiaosha Chen",
        "Chengqi Zhao",
        "Chenyu Deng",
        "Chong Zhang",
        "Ruan Damai",
        "Daya Dai",
        "Dejian Guo",
        "Deli Yang",
        "Dongjie Chen",
        "Erhang Ji",
        "Fangyun Li",
        "Fucong Lin",
        "Dai Fuli",
        "Guangbo Luo",
        "Guanting Hao",
        "Guowei Chen",
        "Han Zhang",
        "Haocheng Xu",
        "Haowei Wang",
        "Honghui Zhang",
        "Huajian Ding",
        "Huazuo Xin",
        "Hui Gao",
        "J. Qu",
        "Jiaqi Guo",
        "Jiashi Ni",
        "Jiawei Li",
        "Wang Jin",
        "Jingyang Chen",
        "Jun-jie Yuan",
        "Junlong Qiu",
        "Junxiao Li",
        "Kai Song",
        "Kang Gao",
        "Kexin Guan",
        "Kuai Huang",
        "Peiyi Huang",
        "Peng Wang",
        "Qiancheng Zhang",
        "Wang Qihao",
        "Qin-Feng Zhu",
        "Qiushi Chen",
        "R. J. Du",
        "L. ChenR.",
        "Ruisong Ge",
        "Zhang",
        "Tian Pei",
        "Tianyu Sun",
        "Wangding Zeng",
        "Wanjia Zhao",
        "Xianzu Wang",
        "Xiaokang Chen",
        "X. Nie",
        "Yunxian Xiong",
        "Yuting Ma",
        "Yuxiang Yan",
        "Yuxi-ang Luo",
        "Yuxuan You",
        "Yuyang Liu",
        "Z. F. Zhou",
        "Z. Z. Wu",
        "Z. Ren",
        "Zhangli Ren",
        "Zheyi Sha",
        "Zhe Fu",
        "Xu Zhen",
        "Zhen Huang",
        "Zhenda Zhang",
        "Zhengyan Xie",
        "Zhewen Zhang",
        "Zhibin Hao",
        "Zhicheng Gou",
        "Zhiqiang Ma",
        "Zhihong Yan",
        "Zhipeng Shao",
        "Zhiyu Xu",
        "Zhongyu Wu",
        "Zhuoshu Zhang",
        "Zihui Li",
        "Zijia Gu",
        "Zhu Zijun",
        "Zilin Liu",
        "Ziwei Li",
        "Xie"
      ],
      "abstract": "We introduce FinanceReasoning, a novel benchmark designed to evaluate the reasoning capabilities of large reasoning models (LRMs) in financial numerical reasoning problems. Compared to existing benchmarks, our work provides three key advancements. (1) Credibility: We update 15.6% of the questions from four public datasets, annotating 908 new questions with detailed Python solutions and rigorously refining evaluation standards. This enables an accurate assessment of the reasoning improvements of LRMs. (2) Comprehensiveness: FinanceReasoning covers 67.8% of financial concepts and formulas, significantly surpassing existing datasets. Additionally, we construct 3,133 Python-formatted functions, which enhances LRMs'financial reasoning capabilities through refined knowledge (e.g., 83.2% $\\rightarrow$ 91.6% for GPT-4o). (3) Challenge: Models are required to apply multiple financial formulas for precise numerical reasoning on 238 Hard problems. The best-performing model (i.e., OpenAI o1 with PoT) achieves 89.1% accuracy, yet LRMs still face challenges in numerical precision. We demonstrate that combining Reasoner and Programmer models can effectively enhance LRMs'performance (e.g., 83.2% $\\rightarrow$ 87.8% for DeepSeek-R1). Our work paves the way for future research on evaluating and improving LRMs in domain-specific complex reasoning tasks.",
      "arxiv_url": "https://arxiv.org/abs/2506.05828",
      "pdf_url": "https://arxiv.org/pdf/2506.05828",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.08427",
      "title": "ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains",
      "authors": [
        "Zilu Dong",
        "Xiangqing Shen",
        "Zinong Yang",
        "Rui Xia"
      ],
      "abstract": "Current knowledge editing methods for large language models (LLMs) struggle to maintain logical consistency when propagating ripple effects to associated facts. We propose ChainEdit, a framework that synergizes knowledge graph-derived logical rules with LLM logical reasoning capabilities to enable systematic chain updates. By automatically extracting logical patterns from structured knowledge bases and aligning them with LLMs'internal logics, ChainEdit dynamically generates and edits logically connected knowledge clusters. Experiments demonstrate an improvement of more than 30% in logical generalization over baselines while preserving editing reliability and specificity. We further address evaluation biases in existing benchmarks through knowledge-aware protocols that disentangle external dependencies. This work establishes new state-of-the-art performance on ripple effect while ensuring internal logical consistency after knowledge editing.",
      "arxiv_url": "https://arxiv.org/abs/2507.08427",
      "pdf_url": "https://arxiv.org/pdf/2507.08427",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02561",
      "title": "Pruning General Large Language Models into Customized Expert Models",
      "authors": [
        "Yirao Zhao",
        "Guizhen Chen",
        "Kenji Kawaguchi",
        "Li Bing",
        "Wenxuan Zhang"
      ],
      "abstract": "Large language models (LLMs) have revolutionized natural language processing, yet their substantial model sizes often require substantial computational resources. To preserve computing resources and accelerate inference speed, it is crucial to prune redundant parameters, especially for experienced users who often need compact expert models tailored to specific downstream scenarios. However, most existing pruning methods focus on preserving the model's general capabilities, often requiring extensive post-training or suffering from degraded performance due to coarse-grained pruning. In this work, we design a $\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to prune a large general model into a smaller lightweight expert model, which is positioned along the\"language\",\"domain\"and\"task\"dimensions. By identifying and pruning irrelevant neurons of each dimension, $\\texttt{Cus-Prun}$ creates expert models without any post-training. Our experiments demonstrate that $\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal loss in both expert and general capabilities across various models from different model families and sizes.",
      "arxiv_url": "https://arxiv.org/abs/2506.02561",
      "pdf_url": "https://arxiv.org/pdf/2506.02561",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.06908",
      "title": "MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection",
      "authors": [
        "Ziyan Liu",
        "Chunxiao Fan",
        "Haoran Lou",
        "Yuexin Wu",
        "Kaiwei Deng"
      ],
      "abstract": "The rapid expansion of memes on social media has highlighted the urgent need for effective approaches to detect harmful content. However, traditional data-driven approaches struggle to detect new memes due to their evolving nature and the lack of up-to-date annotated data. To address this issue, we propose MIND, a multi-agent framework for zero-shot harmful meme detection that does not rely on annotated data. MIND implements three key strategies: 1) We retrieve similar memes from an unannotated reference set to provide contextual information. 2) We propose a bi-directional insight derivation mechanism to extract a comprehensive understanding of similar memes. 3) We then employ a multi-agent debate mechanism to ensure robust decision-making through reasoned arbitration. Extensive experiments on three meme datasets demonstrate that our proposed framework not only outperforms existing zero-shot approaches but also shows strong generalization across different model architectures and parameter scales, providing a scalable solution for harmful meme detection. The code is available at https://github.com/destroy-lonely/MIND.",
      "arxiv_url": "https://arxiv.org/abs/2507.06908",
      "pdf_url": "https://arxiv.org/pdf/2507.06908",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2025-07-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.09589",
      "title": "Logical forms complement probability in understanding language model (and human) performance",
      "authors": [
        "Yixuan Wang",
        "Freda Shi"
      ],
      "abstract": "With the increasing interest in using large language models (LLMs) for planning in natural language, understanding their behaviors becomes an important research question. This work conducts a systematic investigation of LLMs' ability to perform logical reasoning in natural language. We introduce a controlled dataset of hypothetical and disjunctive syllogisms in propositional and modal logic and use it as the testbed for understanding LLM performance. Our results lead to novel insights in predicting LLM behaviors: in addition to the probability of input (Gonen et al., 2023; McCoy et al., 2024), logical forms should be considered as important factors. In addition, we show similarities and discrepancies between the logical reasoning performances of humans and LLMs by collecting and comparing behavioral data from both.",
      "arxiv_url": "https://arxiv.org/abs/2502.09589",
      "pdf_url": "https://arxiv.org/pdf/2502.09589",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02347",
      "title": "STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation",
      "authors": [
        "Jiaming Li",
        "Yukun Chen",
        "Ziqiang Liu",
        "Minghuan Tan",
        "Lei Zhang",
        "Yunshui Li",
        "Run Luo",
        "Longze Chen",
        "Jing Luo",
        "A. Argha",
        "Hamid Alinejad-Rokny",
        "Wei Zhou",
        "Min Yang"
      ],
      "abstract": "Stories are central to human culture, serving to share ideas, preserve traditions, and foster connections. Automatic story generation, a key advancement in artificial intelligence (AI), offers new possibilities for creating personalized content, exploring creative ideas, and enhancing interactive experiences. However, existing methods struggle to maintain narrative coherence and logical consistency. This disconnect compromises the overall storytelling experience, underscoring the need for substantial improvements. Inspired by human cognitive processes, we introduce Storyteller, a novel approach that systemically improves the coherence and consistency of automatically generated stories. Storyteller introduces a plot node structure based on linguistically grounded subject verb object (SVO) triplets, which capture essential story events and ensure a consistent logical flow. Unlike previous methods, Storyteller integrates two dynamic modules, the STORYLINE and narrative entity knowledge graph (NEKG),that continuously interact with the story generation process. This integration produces structurally sound, cohesive and immersive narratives. Extensive experiments demonstrate that Storyteller significantly outperforms existing approaches, achieving an 84.33% average win rate through human preference evaluation. At the same time, it is also far ahead in other aspects including creativity, coherence, engagement, and relevance.",
      "arxiv_url": "https://arxiv.org/abs/2506.02347",
      "pdf_url": "https://arxiv.org/pdf/2506.02347",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Personalization"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.18673",
      "title": "Can Third-parties Read Our Emotions?",
      "authors": [
        "Jiayi Li",
        "Yingfan Zhou",
        "Pranav Narayanan Venkit",
        "Halima Binte Islam",
        "Sneha Arya",
        "Shomir Wilson",
        "Sarah M. Rajtmajer"
      ],
      "abstract": "Natural Language Processing tasks that aim to infer an author's private states, e.g., emotions and opinions, from their written text, typically rely on datasets annotated by third-party annotators. However, the assumption that third-party annotators can accurately capture authors' private states remains largely unexamined. In this study, we present human subjects experiments on emotion recognition tasks that directly compare third-party annotations with first-party (author-provided) emotion labels. Our findings reveal significant limitations in third-party annotations-whether provided by human annotators or large language models (LLMs)-in faithfully representing authors' private states. However, LLMs outperform human annotators nearly across the board. We further explore methods to improve third-party annotation quality. We find that demographic similarity between first-party authors and third-party human annotators enhances annotation performance. While incorporating first-party demographic information into prompts leads to a marginal but statistically significant improvement in LLMs' performance. We introduce a framework for evaluating the limitations of third-party annotations and call for refined annotation practices to accurately represent and model authors' private states.",
      "arxiv_url": "https://arxiv.org/abs/2504.18673",
      "pdf_url": "https://arxiv.org/pdf/2504.18673",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14767",
      "title": "Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis",
      "authors": [
        "Priyanka Kargupta",
        "Ishika Agarwal",
        "Tal August",
        "Jiawei Han"
      ],
      "abstract": "With the exponential growth of research facilitated by modern technology and improved accessibility, scientific discoveries have become increasingly fragmented within and across fields. This makes it challenging to assess the significance, novelty, incremental findings, and equivalent ideas between related works, particularly those from different research communities. Large language models (LLMs) have recently demonstrated strong quantitative and qualitative reasoning abilities, and multi-agent LLM debates have shown promise in handling complex reasoning tasks by exploring diverse perspectives and reasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a framework which converts scientific papers into LLM personas that debate their respective novelties. To emphasize structured, critical reasoning rather than focusing solely on outcomes, ToD dynamically constructs a debate tree, enabling fine-grained analysis of independent novelty arguments within scholarly articles. Through experiments on scientific literature across various domains, evaluated by expert researchers, we demonstrate that ToD generates informative arguments, effectively contrasts papers, and supports researchers in their literature review.",
      "arxiv_url": "https://arxiv.org/abs/2502.14767",
      "pdf_url": "https://arxiv.org/pdf/2502.14767",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06009",
      "title": "Unlocking Recursive Thinking of LLMs: Alignment via Refinement",
      "authors": [
        "Haoke Zhang",
        "Xiaobo Liang",
        "Cunxiang Wang",
        "Juntao Li",
        "Min Zhang"
      ],
      "abstract": "The OpenAI o1-series models have demonstrated that leveraging long-form Chain of Thought (CoT) can substantially enhance performance. However, the recursive thinking capabilities of Large Language Models (LLMs) remain limited, particularly in the absence of expert-curated data for distillation. In this paper, we propose \\textbf{AvR}: \\textbf{Alignment via Refinement}, a novel method aimed at unlocking the potential of LLMs for recursive reasoning through long-form CoT. AvR introduces a refinement process that integrates criticism and improvement actions, guided by differentiable learning techniques to optimize \\textbf{refinement-aware rewards}. As a result, the synthesized multi-round data can be organized as a long refinement thought, further enabling test-time scaling. Experimental results show that AvR significantly outperforms conventional preference optimization methods. Notably, with only 3k synthetic samples, our method boosts the performance of the LLaMA-3-8B-Instruct model by over 20\\% in win rate on AlpacaEval 2.0. Our code is available at Github (https://github.com/Banner-Z/AvR.git).",
      "arxiv_url": "https://arxiv.org/abs/2506.06009",
      "pdf_url": "https://arxiv.org/pdf/2506.06009",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04463",
      "title": "Aligning Large Language Models with Implicit Preferences from User-Generated Content",
      "authors": [
        "Zhaoxuan Tan",
        "Zheng Li",
        "Tianyi Liu",
        "Haodong Wang",
        "Hyokun Yun",
        "Ming Zeng",
        "Pei Chen",
        "Zhihan Zhang",
        "Yifan Gao",
        "Ruijie Wang",
        "Priyanka Nigam",
        "Bing Yin",
        "Menghan Jiang"
      ],
      "abstract": "Learning from preference feedback is essential for aligning large language models (LLMs) with human values and improving the quality of generated responses. However, existing preference learning methods rely heavily on curated data from humans or advanced LLMs, which is costly and difficult to scale. In this work, we present PUGC, a novel framework that leverages implicit human Preferences in unlabeled User-Generated Content (UGC) to generate preference data. Although UGC is not explicitly created to guide LLMs in generating human-preferred responses, it often reflects valuable insights and implicit preferences from its creators that has the potential to address readers' questions. PUGC transforms UGC into user queries and generates responses from the policy model. The UGC is then leveraged as a reference text for response scoring, aligning the model with these implicit preferences. This approach improves the quality of preference data while enabling scalable, domain-specific alignment. Experimental results on Alpaca Eval 2 show that models trained with DPO and PUGC achieve a 9.37% performance improvement over traditional methods, setting a 35.93% state-of-the-art length-controlled win rate using Mistral-7B-Instruct. Further studies highlight gains in reward quality, domain-specific alignment effectiveness, robustness against UGC quality, and theory of mind capabilities. Our code and dataset are available at https://zhaoxuan.info/PUGC.github.io/",
      "arxiv_url": "https://arxiv.org/abs/2506.04463",
      "pdf_url": "https://arxiv.org/pdf/2506.04463",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "82a91a873fbd0fe3c8ce1359f7c57dd867cc132b",
      "title": "M2PA: A Multi-Memory Planning Agent for Open Worlds Inspired by Cognitive Theory",
      "authors": [
        "Yanfang Zhou",
        "Xiaodong Li",
        "Yuntao Liu",
        "Yongqiang Zhao",
        "Xintong Wang",
        "Zhenyu Li",
        "Jinlong Tian",
        "Xinhai Xu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/82a91a873fbd0fe3c8ce1359f7c57dd867cc132b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.19571",
      "title": "Has Machine Translation Evaluation Achieved Human Parity? The Human Reference and the Limits of Progress",
      "authors": [
        "Lorenzo Proietti",
        "Stefano Perrella",
        "Roberto Navigli"
      ],
      "abstract": "In Machine Translation (MT) evaluation, metric performance is assessed based on agreement with human judgments. In recent years, automatic metrics have demonstrated increasingly high levels of agreement with humans. To gain a clearer understanding of metric performance and establish an upper bound, we incorporate human baselines in the MT meta-evaluation, that is, the assessment of MT metrics'capabilities. Our results show that human annotators are not consistently superior to automatic metrics, with state-of-the-art metrics often ranking on par with or higher than human baselines. Despite these findings suggesting human parity, we discuss several reasons for caution. Finally, we explore the broader implications of our results for the research field, asking: Can we still reliably measure improvements in MT evaluation? With this work, we aim to shed light on the limits of our ability to measure progress in the field, fostering discussion on an issue that we believe is crucial to the entire MT evaluation community.",
      "arxiv_url": "https://arxiv.org/abs/2506.19571",
      "pdf_url": "https://arxiv.org/pdf/2506.19571",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05385",
      "title": "LLMs Can Also Do Well! Breaking Barriers in Semantic Role Labeling via Large Language Models",
      "authors": [
        "Xinxin Li",
        "Huiyao Chen",
        "Chengjun Liu",
        "Jing Li",
        "Meishan Zhang",
        "Jun Yu",
        "Min Zhang"
      ],
      "abstract": "Semantic role labeling (SRL) is a crucial task of natural language processing (NLP). Although generative decoder-based large language models (LLMs) have achieved remarkable success across various NLP tasks, they still lag behind state-of-the-art encoder-decoder (BERT-like) models in SRL. In this work, we seek to bridge this gap by equipping LLMs for SRL with two mechanisms: (a) retrieval-augmented generation and (b) self-correction. The first mechanism enables LLMs to leverage external linguistic knowledge such as predicate and argument structure descriptions, while the second allows LLMs to identify and correct inconsistent SRL outputs. We conduct extensive experiments on three widely-used benchmarks of SRL (CPB1.0, CoNLL-2009, and CoNLL-2012). Results demonstrate that our method achieves state-of-the-art performance in both Chinese and English, marking the first successful application of LLMs to surpass encoder-decoder approaches in SRL.",
      "arxiv_url": "https://arxiv.org/abs/2506.05385",
      "pdf_url": "https://arxiv.org/pdf/2506.05385",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "833b29b492109ad083e7bf59ead2915b133265d5",
      "title": "Fairness Beyond Performance: Revealing Reliability Disparities Across Groups in Legal NLP",
      "authors": [
        "Santosh T.Y.S.S",
        "Irtiza Chowdhury"
      ],
      "abstract": "Fairness in NLP must extend beyond performance parity to encompass equitable reliability across groups. This study exposes a critical blind spot: models often make less reliable or overconfident predictions for marginalized groups, even when overall performance appears fair. Using the FairLex benchmark as a case study in legal NLP, we systematically evaluate both performance and reliability disparities across demographic, regional, and legal attributes spanning four jurisdictions. We show that domain-specific pre-training consistently improves both performance and reliability, especially for underrepresented groups. However, common bias mitigation methods frequently worsen reliability disparities, revealing a trade-off not captured by performance metrics alone. Our results call for a rethinking of fairness in high-stakes NLP: To ensure equitable treat-ment, models must not only be accurate, but also reliably self-aware across all groups.",
      "arxiv_url": "https://www.semanticscholar.org/paper/833b29b492109ad083e7bf59ead2915b133265d5",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.21538",
      "title": "Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval",
      "authors": [
        "Hani Alomari",
        "Anushka Sivakumar",
        "Andrew Zhang",
        "Chris Thomas"
      ],
      "abstract": "Cross-modal image-text retrieval is challenging because of the diverse possible associations between content from different modalities. Traditional methods learn a single-vector embedding to represent semantics of each sample, but struggle to capture nuanced and diverse relationships that can exist across modalities. Set-based approaches, which represent each sample with multiple embeddings, offer a promising alternative, as they can capture richer and more diverse relationships. In this paper, we show that, despite their promise, these set-based representations continue to face issues including sparse supervision and set collapse, which limits their effectiveness. To address these challenges, we propose Maximal Pair Assignment Similarity to optimize one-to-one matching between embedding sets which preserve semantic diversity within the set. We also introduce two loss functions to further enhance the representations: Global Discriminative Loss to enhance distinction among embeddings, and Intra-Set Divergence Loss to prevent collapse within each set. Our method achieves state-of-the-art performance on MS-COCO and Flickr30k without relying on external data.",
      "arxiv_url": "https://arxiv.org/abs/2506.21538",
      "pdf_url": "https://arxiv.org/pdf/2506.21538",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Multi-Modal RAG"
      ],
      "published_date": "2025-06-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19273",
      "title": "Eta-WavLM: Efficient Speaker Identity Removal in Self-Supervised Speech Representations Using a Simple Linear Equation",
      "authors": [
        "Giuseppe Ruggiero",
        "Matteo Testa",
        "J. V. D. Walle",
        "Luigi Di Caro"
      ],
      "abstract": "Self-supervised learning (SSL) has reduced the reliance on expensive labeling in speech technologies by learning meaningful representations from unannotated data. Since most SSL-based downstream tasks prioritize content information in speech, ideal representations should disentangle content from unwanted variations like speaker characteristics in the SSL representations. However, removing speaker information often degrades other speech components, and existing methods either fail to fully disentangle speaker identity or require resource-intensive models. In this paper, we propose a novel disentanglement method that linearly decomposes SSL representations into speaker-specific and speaker-independent components, effectively generating speaker disentangled representations. Comprehensive experiments show that our approach achieves speaker independence and as such, when applied to content-driven tasks such as voice conversion, our representations yield significant improvements over state-of-the-art methods.",
      "arxiv_url": "https://arxiv.org/abs/2505.19273",
      "pdf_url": "https://arxiv.org/pdf/2505.19273",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "838463a782ee7c4cb0fa6403adb2308563ab29cd",
      "title": "Stepwise Reasoning Disruption Attack of LLMs",
      "authors": [
        "Jingyu Peng",
        "Maolin Wang",
        "Xiangyu Zhao",
        "Kai Zhang",
        "Wanyu Wang",
        "Pengyue Jia",
        "Qidong Liu",
        "Ruocheng Guo",
        "Qi Liu"
      ],
      "abstract": "Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain unexplored, particularly in third-party platforms that facilitate user interactions via APIs. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the S tepwise r E asoning E rror D isruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED’s effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: https://github. com/Applied-Machine-Learning-Lab/ SEED-Attack",
      "arxiv_url": "https://www.semanticscholar.org/paper/838463a782ee7c4cb0fa6403adb2308563ab29cd",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.15925",
      "title": "Reranking-based Generation for Unbiased Perspective Summarization",
      "authors": [
        "Narutatsu Ri",
        "Nicholas Deas",
        "Kathleen McKeown"
      ],
      "abstract": "Generating unbiased summaries in real-world settings such as political perspective summarization remains a crucial application of Large Language Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics for measuring key attributes such as coverage and faithfulness without verifying their applicability, and efforts to develop improved summarizers are still nascent. We address these gaps by (1) identifying reliable metrics for measuring perspective summary quality, and (2) investigating the efficacy of LLM-based methods beyond zero-shot inference. Namely, we build a test set for benchmarking metric reliability using human annotations and show that traditional metrics underperform compared to language model-based metrics, which prove to be strong evaluators. Using these metrics, we show that reranking-based methods yield strong results, and preference tuning with synthetically generated and reranking-labeled data further boosts performance. Our findings aim to contribute to the reliable evaluation and development of perspective summarization methods.",
      "arxiv_url": "https://arxiv.org/abs/2506.15925",
      "pdf_url": "https://arxiv.org/pdf/2506.15925",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.12722",
      "title": "SimUSER: Simulating User Behavior with Large Language Models for Recommender System Evaluation",
      "authors": [
        "Nicolas Bougie",
        "Narimasa Watanabe"
      ],
      "abstract": "Recommender systems play a central role in numerous real-life applications, yet evaluating their performance remains a significant challenge due to the gap between offline metrics and online behaviors. Given the scarcity and limits (e.g., privacy issues) of real user data, we introduce SimUSER, an agent framework that serves as believable and cost-effective human proxies. SimUSER first identifies self-consistent personas from historical data, enriching user profiles with unique backgrounds and personalities. Then, central to this evaluation are users equipped with persona, memory, perception, and brain modules, engaging in interactions with the recommender system. SimUSER exhibits closer alignment with genuine humans than prior work, both at micro and macro levels. Additionally, we conduct insightful experiments to explore the effects of thumbnails on click rates, the exposure effect, and the impact of reviews on user engagement. Finally, we refine recommender system parameters based on offline A/B test results, resulting in improved user engagement in the real world.",
      "arxiv_url": "https://arxiv.org/abs/2504.12722",
      "pdf_url": "https://arxiv.org/pdf/2504.12722",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2025-04-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.23393",
      "title": "Hierarchical Memory Organization for Wikipedia Generation",
      "authors": [
        "Eugene J. Yu",
        "Dawei Zhu",
        "Yifan Song",
        "Xiangyu Wong",
        "Jiebin Zhang",
        "Wenxuan Shi",
        "Xiaoguang Li",
        "Qun Liu",
        "Sujian Li"
      ],
      "abstract": "Generating Wikipedia articles autonomously is a challenging task requiring the integration of accurate, comprehensive, and well-structured information from diverse sources. This paper introduces the Memory Organization-based Generation (MOG) framework, a novel approach to address these challenges by leveraging a hierarchical memory architecture. MOG extracts fine-grained memory units from web documents, recursively organizes them into a Wikipedia-style hierarchical structure, and uses this structure to guide the generation process. This ensures alignment between memory and the article outline, improving both informativeness and verifiability while minimizing hallucinations. Additionally, a citation module is implemented to enhance traceability by linking every generated sentence to specific memory units. Evaluations on our newly created WikiStart dataset demonstrate that MOG outperforms baseline methods in producing informative and reliable articles, making it particularly robust in real-world scenarios.",
      "arxiv_url": "https://arxiv.org/abs/2506.23393",
      "pdf_url": "https://arxiv.org/pdf/2506.23393",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00085",
      "title": "COSMIC: Generalized Refusal Direction Identification in LLM Activations",
      "authors": [
        "Vincent Siu",
        "Nicholas Crispino",
        "Zihao Yu",
        "Sam Pan",
        "Zhun Wang",
        "Yang Liu",
        "D. Song",
        "Chenguang Wang"
      ],
      "abstract": "Large Language Models (LLMs) encode behaviors such as refusal within their activation space, yet identifying these behaviors remains a significant challenge. Existing methods often rely on predefined refusal templates detectable in output tokens or require manual analysis. We introduce \\textbf{COSMIC} (Cosine Similarity Metrics for Inversion of Concepts), an automated framework for direction selection that identifies viable steering directions and target layers using cosine similarity - entirely independent of model outputs. COSMIC achieves steering performance comparable to prior methods without requiring assumptions about a model's refusal behavior, such as the presence of specific refusal tokens. It reliably identifies refusal directions in adversarial settings and weakly aligned models, and is capable of steering such models toward safer behavior with minimal increase in false refusals, demonstrating robustness across a wide range of alignment conditions.",
      "arxiv_url": "https://arxiv.org/abs/2506.00085",
      "pdf_url": "https://arxiv.org/pdf/2506.00085",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00927",
      "title": "In-the-wild Audio Spatialization with Flexible Text-guided Localization",
      "authors": [
        "Tianrui Pan",
        "Jie Liu",
        "Zewen Huang",
        "Jie Tang",
        "Gangshan Wu"
      ],
      "abstract": "To enhance immersive experiences, binaural audio offers spatial awareness of sounding objects in AR, VR, and embodied AI applications. While existing audio spatialization methods can generally map any available monaural audio to binaural audio signals, they often lack the flexible and interactive control needed in complex multi-object user-interactive environments. To address this, we propose a Text-guided Audio Spatialization (TAS) framework that utilizes flexible text prompts and evaluates our model from unified generation and comprehension perspectives. Due to the limited availability of premium and large-scale stereo data, we construct the SpatialTAS dataset, which encompasses 376,000 simulated binaural audio samples to facilitate the training of our model. Our model learns binaural differences guided by 3D spatial location and relative position prompts, augmented by flipped-channel audio. It outperforms existing methods on both simulated and real-recorded datasets, demonstrating superior generalization and accuracy. Besides, we develop an assessment model based on Llama-3.1-8B, which evaluates the spatial semantic coherence between our generated binaural audio and text prompts through a spatial reasoning task. Results demonstrate that text prompts provide flexible and interactive control to generate binaural audio with excellent quality and semantic consistency in spatial locations. Dataset is available at \\href{https://github.com/Alice01010101/TASU}",
      "arxiv_url": "https://arxiv.org/abs/2506.00927",
      "pdf_url": "https://arxiv.org/pdf/2506.00927",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15957",
      "title": "R3Mem: Bridging Memory Retention and Retrieval via Reversible Compression",
      "authors": [
        "Xiaoqiang Wang",
        "Suyuchen Wang",
        "Yun Zhu",
        "Bang Liu"
      ],
      "abstract": "Memory plays a key role in enhancing LLMs' performance when deployed to real-world applications. Existing solutions face trade-offs: explicit memory designs based on external storage require complex management and incur storage overhead, while implicit memory designs that store information via parameters struggle with reliable retrieval. In this paper, we propose R$^3$Mem, a memory network that optimizes both information Retention and Retrieval through Reversible context compression. Specifically, R$^3$Mem employs virtual memory tokens to compress and encode infinitely long histories, further enhanced by a hierarchical compression strategy that refines information from document- to entity-level for improved assimilation across granularities. For retrieval, R$^3$Mem employs a reversible architecture, reconstructing raw data by invoking the model backward with compressed information. Implemented via parameter-efficient fine-tuning, it can integrate seamlessly with any Transformer-based model. Experiments demonstrate that our memory design achieves state-of-the-art performance in long-context language modeling and retrieval-augmented generation tasks. It also significantly outperforms conventional memory modules in long-horizon interaction tasks like conversational agents, showcasing its potential for next-generation retrieval systems.",
      "arxiv_url": "https://arxiv.org/abs/2502.15957",
      "pdf_url": "https://arxiv.org/pdf/2502.15957",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00569",
      "title": "AutoMixAlign: Adaptive Data Mixing for Multi-Task Preference Optimization in LLMs",
      "authors": [
        "Nicholas E. Corrado",
        "Julian Katz-Samuels",
        "Adithya M. Devraj",
        "Hyokun Yun",
        "Chao Zhang",
        "Yi Xu",
        "Yi Pan",
        "Bing Yin",
        "Trishul M. Chilimbi"
      ],
      "abstract": "When aligning large language models (LLMs), their performance on various tasks (such as being helpful, harmless, and honest) depends heavily on the composition of their training data. However, selecting a data mixture that achieves strong performance across all tasks is challenging. Existing approaches rely on large ablation studies, heuristics, or human intuition, but these can be prohibitively expensive and suboptimal. We study this problem in the setting of preference optimization via DPO and introduce AutoMixAlign (AMA), a theoretically-grounded algorithm that adaptively mixes datasets during training to balance performance across tasks. AMA first trains \\textit{specialist models} for each task to determine losses that correspond to strong task performance. Then, it trains a generalist model using a novel minimax optimization that prioritizes tasks for which generalist model losses deviate most from specialist model losses. To optimize this problem, we propose two algorithms: (1) AMA-R, which adaptively reweights the objective to prioritize tasks, and (2) AMA-S, which adaptively adjusts how much data is sampled from each task to prioritize tasks. Both algorithms achieve a convergence rate of $O(1/\\sqrt{T})$ in the convex case. AMA-R's convergence result follows from Sagawa et al. (2019), and we provide a convergence proof for AMA-S using online learning techniques such as EXP3. We evaluate AMA on several multitask alignment setups and find that AMA outperforms the standard alignment approach -- which simply optimizes the total loss across all tasks -- and also outperforms model merging methods.",
      "arxiv_url": "https://arxiv.org/abs/2506.00569",
      "pdf_url": "https://arxiv.org/pdf/2506.00569",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.11630",
      "title": "The time scale of redundancy between prosody and linguistic context",
      "authors": [
        "Tamar I. Regev",
        "Chiebuka Ohams",
        "Shaylee Xie",
        "Lukas Wolf",
        "Evelina Fedorenko",
        "Alex Warstadt",
        "E. Wilcox",
        "Tiago Pimentel"
      ],
      "abstract": "In spoken communication, information is transmitted not only via words, but also through a rich array of non-verbal signals, including prosody--the non-segmental auditory features of speech. Do these different communication channels carry distinct information? Prior work has shown that the information carried by prosodic features is substantially redundant with that carried by the surrounding words. Here, we systematically examine the time scale of this relationship, studying how it varies with the length of past and future contexts. We find that a word's prosodic features require an extended past context (3-8 words across different features) to be reliably predicted. Given that long-scale contextual information decays in memory, prosody may facilitate communication by adding information that is locally unique. We also find that a word's prosodic features show some redundancy with future words, but only with a short scale of 1-2 words, consistent with reports of incremental short-term planning in language production. Thus, prosody may facilitate communication by helping listeners predict upcoming material. In tandem, our results highlight potentially distinct roles that prosody plays in facilitating integration of words into past contexts and in helping predict upcoming words.",
      "arxiv_url": "https://arxiv.org/abs/2503.11630",
      "pdf_url": "https://arxiv.org/pdf/2503.11630",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06619",
      "title": "BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs",
      "authors": [
        "Jesse Woo",
        "Fateme Hashemi Chaleshtori",
        "Ana Marasovi'c",
        "Kenneth Marino"
      ],
      "abstract": "A core part of legal work that has been under-explored in Legal NLP is the writing and editing of legal briefs. This requires not only a thorough understanding of the law of a jurisdiction, from judgments to statutes, but also the ability to make new arguments to try to expand the law in a new direction and make novel and creative arguments that are persuasive to judges. To capture and evaluate these legal skills in language models, we introduce BRIEFME, a new dataset focused on legal briefs. It contains three tasks for language models to assist legal professionals in writing briefs: argument summarization, argument completion, and case retrieval. In this work, we describe the creation of these tasks, analyze them, and show how current models perform. We see that today's large language models (LLMs) are already quite good at the summarization and guided completion tasks, even beating human-generated headings. Yet, they perform poorly on other tasks in our benchmark: realistic argument completion and retrieving relevant legal cases. We hope this dataset encourages more development in Legal NLP in ways that will specifically aid people in performing legal work.",
      "arxiv_url": "https://arxiv.org/abs/2506.06619",
      "pdf_url": "https://arxiv.org/pdf/2506.06619",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "84a99865c3aa1a1732b0329b372cb5bbc5aefe61",
      "title": "Length Controlled Generation for Black-box LLMs",
      "authors": [
        "Yuxuan Gu",
        "Wenjie Wang",
        "Xiaocheng Feng",
        "Weihong Zhong",
        "Kun Zhu",
        "Lei Huang",
        "Ting Liu",
        "Bing Qin",
        "Tat-Seng Chua"
      ],
      "abstract": "Large language models (L LM s) have demonstrated impressive instruction following capabilities, while still struggling to accurately manage the length of the generated text, which is a fundamental requirement in many real-world applications. Existing length control methods involve fine-tuning the parameters of L LM s, which is inefficient and suboptimal for practical use. In this paper, we propose a novel iterative sampling framework for text length control, integrating the Metropolis-Hastings algorithm with an importance sampling acceleration strategy. This framework efficiently and reliably regulates L LM s to generate length-constrained text without modifying the underlying parameters, thereby preserving the original capabilities of L LM s. Experimental results demonstrate that our framework achieves almost 100% success rates of length control on L LAMA 3.1 for tasks such as length-controlled abstractive summarization and length-constrained instruction following, with minimal additional computational overhead. This also highlights the significant potential of our method for precise length control across a broader range of applications, without compromising the versatility of L LM s.",
      "arxiv_url": "https://www.semanticscholar.org/paper/84a99865c3aa1a1732b0329b372cb5bbc5aefe61",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12384",
      "title": "Model Merging for Knowledge Editing",
      "authors": [
        "Zichuan Fu",
        "Xian Wu",
        "Guojing Li",
        "Yingying Zhang",
        "Yefeng Zheng",
        "Tianshi Ming",
        "Yejing Wang",
        "Wanyu Wang",
        "Xiangyu Zhao"
      ],
      "abstract": "Large Language Models (LLMs) require continuous updates to maintain accurate and current knowledge as the world evolves. While existing knowledge editing approaches offer various solutions for knowledge updating, they often struggle with sequential editing scenarios and harm the general capabilities of the model, thereby significantly hampering their practical applicability. This paper proposes a two-stage framework combining robust supervised fine-tuning (R-SFT) with model merging for knowledge editing. Our method first fine-tunes the LLM to internalize new knowledge fully, then merges the fine-tuned model with the original foundation model to preserve newly acquired knowledge and general capabilities. Experimental results demonstrate that our approach significantly outperforms existing methods in sequential editing while better preserving the original performance of the model, all without requiring any architectural changes. Code is available at: https://github.com/Applied-Machine-Learning-Lab/MM4KE.",
      "arxiv_url": "https://arxiv.org/abs/2506.12384",
      "pdf_url": "https://arxiv.org/pdf/2506.12384",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10167",
      "title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding",
      "authors": [
        "Hyunbin Jin",
        "J. Yeom",
        "Seunghyun Bae",
        "Taesup Kim"
      ],
      "abstract": "Large language models (LLMs) exhibit strong reasoning abilities, often attributed to few-shot or zero-shot chain-of-thought (CoT) prompting. While effective, these methods require labor-intensive prompt engineering, raising the question of whether reasoning can be induced without reliance on explicit prompts. In this work, we unlock the reasoning capabilities of LLMs without explicit prompting. Inspired by zero-shot CoT and CoT-decoding, we propose a novel decoding strategy that systematically nudges LLMs to continue reasoning, thereby preventing immature reasoning processes. Specifically, we monitor the model's generation and inject a designated phrase whenever it is likely to conclude its response prematurely, before completing the reasoning process. Our experimental evaluations on diverse reasoning benchmarks demonstrate that our proposed strategy substantially improves LLM reasoning capabilities, highlighting the potential of decoding-based interventions as an alternative to traditional prompting techniques.",
      "arxiv_url": "https://arxiv.org/abs/2503.10167",
      "pdf_url": "https://arxiv.org/pdf/2503.10167",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20620",
      "title": "Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning",
      "authors": [
        "Ayana Niwa",
        "Masahiro Kaneko",
        "Kentaro Inui"
      ],
      "abstract": "Large language models (LLMs) can exhibit advanced reasoning yet still generate incorrect answers. We hypothesize that such errors frequently stem from spurious beliefs, propositions the model internally considers true but are incorrect. To address this, we propose a method to rectify the belief space by suppressing these spurious beliefs while simultaneously enhancing true ones, thereby enabling more reliable inferences. Our approach first identifies the beliefs that lead to incorrect or correct answers by prompting the model to generate textual explanations, using our Forward-Backward Beam Search (FBBS). We then apply unlearning to suppress the identified spurious beliefs and enhance the true ones, effectively rectifying the model's belief space. Empirical results on multiple QA datasets and LLMs show that our method corrects previously misanswered questions without harming overall model performance. Furthermore, our approach yields improved generalization on unseen data, suggesting that rectifying a model's belief space is a promising direction for mitigating errors and enhancing overall reliability.",
      "arxiv_url": "https://arxiv.org/abs/2502.20620",
      "pdf_url": "https://arxiv.org/pdf/2502.20620",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "85a5365dc6964e68362a942d8f9fe3b2bcb4cccd",
      "title": "A Dual-Mind Framework for Strategic and Expressive Negotiation Agent",
      "authors": [
        "Yutong Liu",
        "Lida Shi",
        "Rui Song",
        "Hao Xu"
      ],
      "abstract": "Negotiation agents need to influence the attitudes or intentions of users to reach a consensus. Strategy planning and expressive optimization are crucial aspects of effective negotiations. However, previous studies have typically focused on only one of these aspects, neglecting the fact that their combined synergistic effect can lead to better performance. Inspired by the dual-process theory in human cognition, we propose a Dual-Mind Negotiation Agent (DMNA) framework. This framework integrates an intuitive module for rapid, experience-based response and a deliberative module for slow, expression optimization. The intuitive module is trained using Monte Carlo Tree Search (MCTS) and Direct Preference Optimization (DPO), enabling it to make suitable strategic planning and expression. The deliberative module employs a multifaceted reflexion mechanism to enhance the quality of expression. Experiments conducted on negotiation datasets confirm that DMNA achieves state-of-the-art results, demonstrating an enhancement in the negotiation ability of agents 1 .",
      "arxiv_url": "https://www.semanticscholar.org/paper/85a5365dc6964e68362a942d8f9fe3b2bcb4cccd",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.06680",
      "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation",
      "authors": [
        "Wei Li",
        "Xin Zhang",
        "Zhongxin Guo",
        "Shaoguang Mao",
        "Wen Luo",
        "Guangyue Peng",
        "Yangyu Huang",
        "Houfeng Wang",
        "Scarlett Li"
      ],
      "abstract": "Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs'automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.",
      "arxiv_url": "https://arxiv.org/abs/2503.06680",
      "pdf_url": "https://arxiv.org/pdf/2503.06680",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "85fb10109fbca6ea0da9071d51785c41865c0b2c",
      "title": "Exploring Layer-wise Representations of English and Chinese Homonymy in Pre-trained Language Models",
      "authors": [
        "M. K. Ma",
        "Chenwei Xie",
        "Wenbo Wang",
        "William Shi-Yuan Wang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/85fb10109fbca6ea0da9071d51785c41865c0b2c",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "8622d2946f83658ce255ee16667b75103547a077",
      "title": "Listening to Patients: Detecting and Mitigating Patient Misreport in Medical Dialogue System",
      "authors": [
        "Lang Qin",
        "Yao Zhang",
        "Hongru Liang",
        "Adam Jatowt",
        "Zhenglu Yang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/8622d2946f83658ce255ee16667b75103547a077",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18148",
      "title": "NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts",
      "authors": [
        "Farid Adilazuarda",
        "M. Wijanarko",
        "Lucky Susanto",
        "Khumaisa Nur'Aini",
        "Derry Wijaya",
        "Alham Fikri Aji"
      ],
      "abstract": "Indonesia is rich in languages and scripts. However, most NLP progress has been made using romanized text. In this paper, we present NusaAksara, a novel public benchmark for Indonesian languages that includes their original scripts. Our benchmark covers both text and image modalities and encompasses diverse tasks such as image segmentation, OCR, transliteration, translation, and language identification. Our data is constructed by human experts through rigorous steps. NusaAksara covers 8 scripts across 7 languages, including low-resource languages not commonly seen in NLP benchmarks. Although unsupported by Unicode, the Lampung script is included in this dataset. We benchmark our data across several models, from LLMs and VLMs such as GPT-4o, Llama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and show that most NLP technologies cannot handle Indonesia's local scripts, with many achieving near-zero performance.",
      "arxiv_url": "https://arxiv.org/abs/2502.18148",
      "pdf_url": "https://arxiv.org/pdf/2502.18148",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.15707",
      "title": "Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?",
      "authors": [
        "Seok Hwan Song",
        "Mohna Chakraborty",
        "Qi Li",
        "Wallapak Tavanapong"
      ],
      "abstract": "Large Language Models (LLMs) have been evaluated using diverse question types, e.g., multiple-choice, true/false, and short/long answers. This study answers an unexplored question about the impact of different question types on LLM accuracy on reasoning tasks. We investigate the performance of five LLMs on three different types of questions using quantitative and deductive reasoning tasks. The performance metrics include accuracy in the reasoning steps and choosing the final answer. Key Findings: (1) Significant differences exist in LLM performance across different question types. (2) Reasoning accuracy does not necessarily correlate with the final selection accuracy. (3) The number of options and the choice of words, influence LLM performance.",
      "arxiv_url": "https://arxiv.org/abs/2507.15707",
      "pdf_url": "https://arxiv.org/pdf/2507.15707",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "86a2030fb34e5be9ef7cd8aa397b12af8ec900b0",
      "title": "Neuron Activation Modulation for Text Style Transfer: Guiding Large Language Models",
      "authors": [
        "Chaona Kong",
        "Jianyi Liu",
        "Yifan Tang",
        "Ru Zhang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/86a2030fb34e5be9ef7cd8aa397b12af8ec900b0",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20921",
      "title": "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
      "authors": [
        "Injae Na",
        "Keonwoong Noh",
        "Woohwan Jung"
      ],
      "abstract": "LLM providers typically offer multiple LLM tiers, varying in performance and price. As NLP tasks become more complex and modularized, selecting the suitable LLM tier for each subtask is a key challenge to balance between cost and performance. To address the problem, we introduce LLM Automatic Transmission (LLM-AT) framework that automatically selects LLM tiers without training. LLM-AT consists of Starter, Generator, and Judge. The starter selects the initial LLM tier expected to solve the given question, the generator produces a response using the LLM of the selected tier, and the judge evaluates the validity of the response. If the response is invalid, LLM-AT iteratively upgrades to a higher-tier model, generates a new response, and re-evaluates until a valid response is obtained. Additionally, we propose accuracy estimator, which enables the suitable initial LLM tier selection without training. Given an input question, accuracy estimator estimates the expected accuracy of each LLM tier by computing the valid response rate across top-k similar queries from past inference records. Experiments demonstrate that LLM-AT achieves superior performance while reducing costs, making it a practical solution for real-world applications.",
      "arxiv_url": "https://arxiv.org/abs/2505.20921",
      "pdf_url": "https://arxiv.org/pdf/2505.20921",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.05750",
      "title": "CSTRL: Context-Driven Sequential Transfer Learning for Abstractive Radiology Report Summarization",
      "authors": [
        "Mst. Fahmida Naznin",
        "Adnan Ibney Faruq",
        "Mostafa Rifat Tazwar",
        "Md Jobayer",
        "M. M. H. Shawon",
        "Md Rakibul Hasan"
      ],
      "abstract": "A radiology report comprises several sections, including the Findings and Impression of the diagnosis. Automatically generating the Impression from the Findings is crucial for reducing radiologists'workload and improving diagnostic accuracy. Pretrained models that excel in common abstractive summarization problems encounter challenges when applied to specialized medical domains largely due to the complex terminology and the necessity for accurate clinical context. Such tasks in medical domains demand extracting core information, avoiding context shifts, and maintaining proper flow. Misuse of medical terms can lead to drastic clinical errors. To address these issues, we introduce a sequential transfer learning that ensures key content extraction and coherent summarization. Sequential transfer learning often faces challenges like initial parameter decay and knowledge loss, which we resolve with the Fisher matrix regularization. Using MIMIC-CXR and Open-I datasets, our model, CSTRL - Context-driven Sequential TRansfer Learning - achieved state-of-the-art performance, showing 56.2% improvement in BLEU-1, 40.5% in BLEU-2, 84.3% in BLEU-3, 28.9% in ROUGE-1, 41.0% in ROUGE-2 and 26.5% in ROGUE-3 score over benchmark studies. We also analyze factual consistency scores while preserving the medical context. Our code is publicly available at https://github.com/fahmidahossain/Report_Summarization.",
      "arxiv_url": "https://arxiv.org/abs/2503.05750",
      "pdf_url": "https://arxiv.org/pdf/2503.05750",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19254",
      "title": "Unveiling Dual Quality in Product Reviews: An NLP-Based Approach",
      "authors": [
        "Rafal Poswiata",
        "Marcin Michal Miro'nczuk",
        "Slawomir Dadas",
        "Malgorzata Grkebowiec",
        "Michał Perełkiewicz"
      ],
      "abstract": "Consumers often face inconsistent product quality, particularly when identical products vary between markets, a situation known as the dual quality problem. To identify and address this issue, automated techniques are needed. This paper explores how natural language processing (NLP) can aid in detecting such discrepancies and presents the full process of developing a solution. First, we describe in detail the creation of a new Polish-language dataset with 1,957 reviews, 540 highlighting dual quality issues. We then discuss experiments with various approaches like SetFit with sentence-transformers, transformer-based encoders, and LLMs, including error analysis and robustness verification. Additionally, we evaluate multilingual transfer using a subset of opinions in English, French, and German. The paper concludes with insights on deployment and practical applications.",
      "arxiv_url": "https://arxiv.org/abs/2505.19254",
      "pdf_url": "https://arxiv.org/pdf/2505.19254",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.03746",
      "title": "Process-based Self-Rewarding Language Models",
      "authors": [
        "Shimao Zhang",
        "Xiao Liu",
        "Xin Zhang",
        "Junxiao Liu",
        "Zheheng Luo",
        "Shujian Huang",
        "Yeyun Gong"
      ],
      "abstract": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' performance, which is constrained by the upper limit of human performance. Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs. However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to a decline in performance. In this work, we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference optimization within the self-rewarding paradigm. Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding, demonstrating the immense potential of self-rewarding to achieve LLM reasoning that may surpass human capabilities.",
      "arxiv_url": "https://arxiv.org/abs/2503.03746",
      "pdf_url": "https://arxiv.org/pdf/2503.03746",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.01225",
      "title": "A Conformal Risk Control Framework for Granular Word Assessment and Uncertainty Calibration of CLIPScore Quality Estimates",
      "authors": [
        "Gonccalo Gomes",
        "Chrysoula Zerva",
        "Bruno Martins"
      ],
      "abstract": "This study explores current limitations of learned image captioning evaluation metrics, specifically the lack of granular assessments for errors within captions, and the reliance on single-point quality estimates without considering uncertainty. To address the limitations, we propose a simple yet effective strategy for generating and calibrating distributions of CLIPScore values. Leveraging a model-agnostic conformal risk control framework, we calibrate CLIPScore values for task-specific control variables, tackling the aforementioned limitations. Experimental results demonstrate that using conformal risk control, over score distributions produced with simple methods such as input masking, can achieve competitive performance compared to more complex approaches. Our method effectively detects erroneous words, while providing formal guarantees aligned with desired risk levels. It also improves the correlation between uncertainty estimations and prediction errors, thus enhancing the overall reliability of caption evaluation metrics.",
      "arxiv_url": "https://arxiv.org/abs/2504.01225",
      "pdf_url": "https://arxiv.org/pdf/2504.01225",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23174",
      "title": "Map&Make: Schema Guided Text to Table Generation",
      "authors": [
        "Naman Ahuja",
        "Fenil Bardoliya",
        "Chitta Baral",
        "Vivek Gupta"
      ],
      "abstract": "Transforming dense, detailed, unstructured text into an interpretable and summarised table, also colloquially known as Text-to-Table generation, is an essential task for information retrieval. Current methods, however, miss out on how and what complex information to extract; they also lack the ability to infer data from the text. In this paper, we introduce a versatile approach, Map&Make, which\"dissects\"text into propositional atomic statements. This facilitates granular decomposition to extract the latent schema. The schema is then used to populate the tables that capture the qualitative nuances and the quantitative facts in the original text. Our approach is tested against two challenging datasets, Rotowire, renowned for its complex and multi-table schema, and Livesum, which demands numerical aggregation. By carefully identifying and correcting hallucination errors in Rotowire, we aim to achieve a cleaner and more reliable benchmark. We evaluate our method rigorously on a comprehensive suite of comparative and referenceless metrics. Our findings demonstrate significant improvement results across both datasets with better interpretability in Text-to-Table generation. Moreover, through detailed ablation studies and analyses, we investigate the factors contributing to superior performance and validate the practicality of our framework in structured summarization tasks.",
      "arxiv_url": "https://arxiv.org/abs/2505.23174",
      "pdf_url": "https://arxiv.org/pdf/2505.23174",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06887",
      "title": "Mixture of Small and Large Models for Chinese Spelling Check",
      "authors": [
        "Ziheng Qiao",
        "Houquan Zhou",
        "Zhenghua Li"
      ],
      "abstract": "In the era of large language models (LLMs), the Chinese Spelling Check (CSC) task has seen various LLM methods developed, yet their performance remains unsatisfactory. In contrast, fine-tuned BERT-based models, relying on high-quality in-domain data, show excellent performance but suffer from edit pattern overfitting. This paper proposes a novel dynamic mixture approach that effectively combines the probability distributions of small models and LLMs during the beam search decoding phase, achieving a balanced enhancement of precise corrections from small models and the fluency of LLMs. This approach also eliminates the need for fine-tuning LLMs, saving significant time and resources, and facilitating domain adaptation. Comprehensive experiments demonstrate that our mixture approach significantly boosts error correction capabilities, achieving state-of-the-art results across multiple datasets. Our code is available at https://github.com/zhqiao-nlp/MSLLM.",
      "arxiv_url": "https://arxiv.org/abs/2506.06887",
      "pdf_url": "https://arxiv.org/pdf/2506.06887",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.07572",
      "title": "Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation",
      "authors": [
        "Yupu Liang",
        "Yaping Zhang",
        "Zhiyang Zhang",
        "Yang Zhao",
        "Lu Xiang",
        "Chengqing Zong",
        "Yu Zhou"
      ],
      "abstract": "Document Image Machine Translation (DIMT) aims to translate text within document images, facing generalization challenges due to limited training data and the complex interplay between visual and textual information. To address these challenges, we introduce M4Doc, a novel single-to-mix modality alignment framework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an image-only encoder with the multimodal representations of an MLLM, pre-trained on large-scale document image datasets. This alignment enables a lightweight DIMT model to learn crucial visual-textual correlations during training. During inference, M4Doc bypasses the MLLM, maintaining computational efficiency while benefiting from its multimodal knowledge. Comprehensive experiments demonstrate substantial improvements in translation quality, especially in cross-domain generalization and challenging document image scenarios.",
      "arxiv_url": "https://arxiv.org/abs/2507.07572",
      "pdf_url": "https://arxiv.org/pdf/2507.07572",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "880c48200ec2b16129816fa91aa252f9c76598ef",
      "title": "AgentGym: Evaluating and Training Large Language Model-based Agents across Diverse Environments",
      "authors": [
        "Zhiheng Xi",
        "Yiwen Ding",
        "Wenxiang Chen",
        "Boyang Hong",
        "Honglin Guo",
        "Junzhe Wang",
        "Xin Guo",
        "Dingwen Yang",
        "Chenyang Liao",
        "Wei He",
        "Songyang Gao",
        "Luyao Chen",
        "Rui Zheng",
        "Yicheng Zou",
        "Tao Gui",
        "Qi Zhang",
        "Xipeng Qiu",
        "Xuanjing Huang",
        "Zuxuan Wu",
        "Yu-Gang Jiang"
      ],
      "abstract": "Large language models (LLMs) have emerged as a promising foundation to build generally-capable agents (LLM-based agents) that can handle multi-turn decision-making tasks across various environments. However, the community lacks a unified interactive framework that covers diverse environments for comprehensive evaluation of agents, and enables exploration and learning for their self-improvement. To address this, we propose A GENT G YM , a framework featuring 7 real-world scenarios, 14 environments, and 89 tasks for unified, real-time, and concurrent agent interaction. We construct expanded instruction set, high-quality trajectories, and comprehensive benchmarking suite for developing LLM-based agents. Moreover, A GENT G YM supports interactive exploration and learning for agents through multi-turn interactions and real-time feedback. Based on A GENT G YM , we take the initial step to develop LLM-based agents that can handle diverse tasks via methods like self-improvement or reinforcement learning. Experimental re-sults show that the trained agents can achieve results comparable to commercial models. We hope our work can help the community develop more advanced LLM-based agents. We release the code, dataset, benchmark, and checkpoints at https://agentgym.github.io/.",
      "arxiv_url": "https://www.semanticscholar.org/paper/880c48200ec2b16129816fa91aa252f9c76598ef",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.06892",
      "title": "Language Fusion for Parameter-Efficient Cross-lingual Transfer",
      "authors": [
        "Philipp Borchert",
        "Ivan Vuli'c",
        "Marie-Francine Moens",
        "Jochen De Weerdt"
      ],
      "abstract": "Limited availability of multilingual text corpora for training language models often leads to poor performance on downstream tasks due to undertrained representation spaces for languages other than English. This 'under-representation' has motivated recent cross-lingual transfer methods to leverage the English representation space by e.g. mixing English and 'non-English' tokens at the input level or extending model parameters to accommodate new languages. However, these approaches often come at the cost of increased computational complexity. We propose Fusion forLanguage Representations (FLARE) in adapters, a novel method that enhances representation quality and downstream performance for languages other than English while maintaining parameter efficiency. FLARE integrates source and target language representations within low-rank (LoRA) adapters using lightweight linear transformations, maintaining parameter efficiency while improving transfer performance. A series of experiments across representative cross-lingual natural language understanding tasks, including natural language inference, question-answering and sentiment analysis, demonstrate FLARE's effectiveness. FLARE achieves performance improvements of 4.9% for Llama 3.1 and 2.2% for Gemma~2 compared to standard LoRA fine-tuning on question-answering tasks, as measured by the exact match metric.",
      "arxiv_url": "https://arxiv.org/abs/2501.06892",
      "pdf_url": "https://arxiv.org/pdf/2501.06892",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.18539",
      "title": "Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented LLM-based Retrieval Method",
      "authors": [
        "P. Chen",
        "Yi Zhang",
        "Michael J. Cafarella",
        "Dan Roth"
      ],
      "abstract": "Real-world open-domain questions can be complicated, particularly when answering them involves information from multiple information sources. LLMs have demonstrated impressive performance in decomposing complex tasks into simpler steps, and previous work has used it for better retrieval in support of complex questions. However, LLM's decomposition of questions is unaware of what data is available and how data is organized, often leading to a sub-optimal retrieval performance. Recent effort in agentic RAG proposes to perform retrieval in an iterative fashion, where a followup query is derived as an action based on previous rounds of retrieval. While this provides one way of interacting with the data collection, agentic RAG's exploration of data is inefficient because successive queries depend on previous results rather than being guided by the organization of available data in the collection. To address this problem, we propose an LLM-based retrieval method -- ARM, that aims to better align the question with the organization of the data collection by exploring relationships among data objects beyond matching the utterance of the query, thus leading to a retrieve-all-at-once solution for complex queries. We evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms standard RAG with query decomposition by up to 5.2 pt in execution accuracy and agentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and 19.3 pt higher F1 match scores compared to these approaches.",
      "arxiv_url": "https://arxiv.org/abs/2501.18539",
      "pdf_url": "https://arxiv.org/pdf/2501.18539",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM",
        "RAG"
      ],
      "published_date": "2025-01-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "88a5beb79f263e6b3429f33726dbf27a37f7b9bd",
      "title": "Doc-React: Multi-page Heterogeneous Document Question-answering",
      "authors": [
        "Junda Wu",
        "Yu Xia",
        "Tong Yu",
        "Xiang Chen",
        "Sai Sree Harsha",
        "Akash V. Maharaj",
        "Ruiyi Zhang",
        "Victor S. Bursztyn",
        "Sungchul Kim",
        "Ryan A. Rossi",
        "Julian J. McAuley",
        "Yunyao Li",
        "Ritwik Sinha"
      ],
      "abstract": ": Answering questions over multi-page, multimodal documents, including text and figures, is a critical challenge for applications that require answers to integrate information across multiple modalities and contextual dependencies. Existing methods, such as single-turn retrieval-augmented generation (RAG), struggle to retrieve fine-grained and contextually relevant information from large, heterogeneous documents, leading to suboptimal performance. Inspired by iterative frameworks like ReAct, which refine retrieval through feedback, we propose Doc-React, an adaptive iterative framework that balances information gain and uncertainty reduction at each step. Doc-React leverages InfoNCE-guided retrieval to approximate mutual information, enabling dynamic sub-query generation and refinement. A large language model (LLM) serves as both a judge and generator, providing structured feedback to iteratively improve retrieval. By combining mutual information optimization with entropy-aware selection, Doc-React systematically captures relevant multimodal content, achieving strong performance on complex QA tasks.",
      "arxiv_url": "https://www.semanticscholar.org/paper/88a5beb79f263e6b3429f33726dbf27a37f7b9bd",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "88bc5f6647ac47e450987b7a203ad663c38b9e19",
      "title": "How Much Do Encoder Models Know About Word Senses?",
      "authors": [
        "Simone Teglia",
        "Simone Tedeschi",
        "Roberto Navigli"
      ],
      "abstract": "Word Sense Disambiguation (WSD) is a key task in Natural Language Processing (NLP), involving selecting the correct meaning of a word based on its context. With Pretrained Language Models (PLMs) like BERT and DeBERTa now well established, significant progress has been made in understanding contextual semantics. Nevertheless, how well these models inherently disambiguate word senses remains uncertain. In this work, we evaluate several encoder-only PLMs across two popular inventories (i.e. WordNet and the Oxford Dictionary of English) by analyzing their ability to separate word senses without any task-specific fine-tuning. We compute centroids of word senses and measure similarity to assess performance across different layers. Our results show that DeBERTa-v3 delivers the best performance on the task, with the middle layers (specifically the 7th and 8th layers) achieving the highest accuracy, outperforming the output layer by approximately 15 percentage points. Our experiments also explore the inherent structure of Word-Net and ODE sense inventories, highlighting their influence on the overall model behavior and performance. Finally, based on our findings, we develop a small, efficient model for the WSD task that attains robust performance while significantly reducing the carbon foot-print. We publicly release our software at http: //github.com/SapienzaNLP/wsd-probing .",
      "arxiv_url": "https://www.semanticscholar.org/paper/88bc5f6647ac47e450987b7a203ad663c38b9e19",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "88c16454f5b069fe04f2929e0442421bd53ccb42",
      "title": "ReKG-MCTS: Reinforcing LLM Reasoning on Knowledge Graphs via Training-Free Monte Carlo Tree Search",
      "authors": [
        "Xiaozhuang Song",
        "Shufei Zhang",
        "Tianshu Yu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/88c16454f5b069fe04f2929e0442421bd53ccb42",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16107",
      "title": "MPL: Multiple Programming Languages with Large Language Models for Information Extraction",
      "authors": [
        "Bo Li",
        "Gexiang Fang",
        "Wei Ye",
        "Zhenghua Xu",
        "Jinglei Zhang",
        "Hao Cheng",
        "Shikun Zhang"
      ],
      "abstract": "Recent research in information extraction (IE) focuses on utilizing code-style inputs to enhance structured output generation. The intuition behind this is that the programming languages (PLs) inherently exhibit greater structural organization than natural languages (NLs). This structural advantage makes PLs particularly suited for IE tasks. Nevertheless, existing research primarily focuses on Python for code-style simulation, overlooking the potential of other widely-used PLs (e.g., C++ and Java) during the supervised fine-tuning (SFT) phase. In this research, we propose \\textbf{M}ultiple \\textbf{P}rogramming \\textbf{L}anguages with large language models for information extraction (abbreviated as \\textbf{MPL}), a novel framework that explores the potential of incorporating different PLs in the SFT phase. Additionally, we introduce \\texttt{function-prompt} with virtual running to simulate code-style inputs more effectively and efficiently. Experimental results on a wide range of datasets demonstrate the effectiveness of MPL. Furthermore, we conduct extensive experiments to provide a comprehensive analysis. We have released our code for future research.",
      "arxiv_url": "https://arxiv.org/abs/2505.16107",
      "pdf_url": "https://arxiv.org/pdf/2505.16107",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02172",
      "title": "Different Speech Translation Models Encode and Translate Speaker Gender Differently",
      "authors": [
        "Dennis Fucci",
        "Marco Gaido",
        "Matteo Negri",
        "L. Bentivogli",
        "Andr'e F. T. Martins",
        "Giuseppe Attanasio"
      ],
      "abstract": "Recent studies on interpreting the hidden states of speech models have shown their ability to capture speaker-specific features, including gender. Does this finding also hold for speech translation (ST) models? If so, what are the implications for the speaker's gender assignment in translation? We address these questions from an interpretability perspective, using probing methods to assess gender encoding across diverse ST models. Results on three language directions (English-French/Italian/Spanish) indicate that while traditional encoder-decoder models capture gender information, newer architectures -- integrating a speech encoder with a machine translation system via adapters -- do not. We also demonstrate that low gender encoding capabilities result in systems' tendency toward a masculine default, a translation bias that is more pronounced in newer architectures.",
      "arxiv_url": "https://arxiv.org/abs/2506.02172",
      "pdf_url": "https://arxiv.org/pdf/2506.02172",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.15289",
      "title": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification",
      "authors": [
        "Junnan Zhu",
        "Min Xiao",
        "Yining Wang",
        "Feifei Zhai",
        "Yu Zhou",
        "Chengqing Zong"
      ],
      "abstract": "LLMs have achieved remarkable fluency and coherence in text generation, yet their widespread adoption has raised concerns about content reliability and accountability. In high-stakes domains, it is crucial to understand where and how the content is created. To address this, we introduce the Text pROVEnance (TROVE) challenge, designed to trace each sentence of a target text back to specific source sentences within potentially lengthy or multi-document inputs. Beyond identifying sources, TROVE annotates the fine-grained relationships (quotation, compression, inference, and others), providing a deep understanding of how each target sentence is formed. To benchmark TROVE, we construct our dataset by leveraging three public datasets covering 11 diverse scenarios (e.g., QA and summarization) in English and Chinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+), emphasizing the multi-document and long-document settings essential for provenance. To ensure high-quality data, we employ a three-stage annotation process: sentence retrieval, GPT-4o provenance, and human provenance. We evaluate 11 LLMs under direct prompting and retrieval-augmented paradigms, revealing that retrieval is essential for robust performance, larger models perform better in complex relationship classification, and closed-source models often lead, yet open-source models show significant promise, particularly with retrieval augmentation. We make our dataset available here: https://github.com/ZNLP/ZNLP-Dataset.",
      "arxiv_url": "https://arxiv.org/abs/2503.15289",
      "pdf_url": "https://arxiv.org/pdf/2503.15289",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "890faaad7887f5041b3724decf16ad8c05837c92",
      "title": "UQ-Merge: Uncertainty Guided Multimodal Large Language Model Merging",
      "authors": [
        "Huaizhi Qu",
        "Xinyu Zhao",
        "Jie Peng",
        "Kwonjoon Lee",
        "Behzad Dariush",
        "Tianlong Chen"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/890faaad7887f5041b3724decf16ad8c05837c92",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11107",
      "title": "Revisiting Weak-to-Strong Generalization in Theory and Practice: Reverse KL vs. Forward KL",
      "authors": [
        "Wei Yao",
        "Wenkai Yang",
        "Ziqiao Wang",
        "Yankai Lin",
        "Yong Liu"
      ],
      "abstract": "As large language models advance toward superhuman performance, ensuring their alignment with human values and abilities grows increasingly complex. Weak-to-strong generalization offers a promising approach by leveraging predictions from weaker models to guide stronger systems, but its effectiveness could be constrained by the inherent noise and inaccuracies in these weak predictions. To address this, we propose a theoretically grounded approach that replaces forward KL divergence-whose mass-covering behavior risks overfitting to imperfect weak signals-with reverse KL divergence. Reverse KL divergence's zero-forcing effect prioritizes high-confidence predictions, effectively mitigating the influence of unreliable weak supervision. Theoretically, we extend existing bounds and derive tighter lower bounds for both forward and reverse KL divergence, establishing that reverse KL achieves at least comparable guarantees to forward KL. Notably, when a sufficiently pre-trained strong model is fine-tuned on the last linear layer, reverse KL guarantees that it outperforms its weak supervisor by the magnitude of their disagreement. Empirically, we demonstrate that reverse KL and reverse cross-entropy enable strong models to successfully outperform those trained with forward KL and standard cross-entropy across most settings, highlighting the practical advantages of these reverse losses.",
      "arxiv_url": "https://arxiv.org/abs/2502.11107",
      "pdf_url": "https://arxiv.org/pdf/2502.11107",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "8954267078736418a3e6e4beb7d5c2c3aaa34fce",
      "title": "SemanticCamo: Jailbreaking Large Language Models through Semantic Camouflage",
      "authors": [
        "Jihui Yan",
        "Xiaocui Yang",
        "Daling Wang",
        "Shi Feng",
        "Yifei Zhang",
        "Yinzhi Zhao"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/8954267078736418a3e6e4beb7d5c2c3aaa34fce",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11677",
      "title": "Towards Fully Exploiting LLM Internal States to Enhance Knowledge Boundary Perception",
      "authors": [
        "Shiyu Ni",
        "Keping Bi",
        "Jiafeng Guo",
        "Lulu Yu",
        "Baolong Bi",
        "Xueqi Cheng"
      ],
      "abstract": "Large language models (LLMs) exhibit impressive performance across diverse tasks but often struggle to accurately gauge their knowledge boundaries, leading to confident yet incorrect responses. This paper explores leveraging LLMs'internal states to enhance their perception of knowledge boundaries from efficiency and risk perspectives. We investigate whether LLMs can estimate their confidence using internal states before response generation, potentially saving computational resources. Our experiments on datasets like Natural Questions, HotpotQA, and MMLU reveal that LLMs demonstrate significant pre-generation perception, which is further refined post-generation, with perception gaps remaining stable across varying conditions. To mitigate risks in critical domains, we introduce Confidence Consistency-based Calibration ($C^3$), which assesses confidence consistency through question reformulation. $C^3$ significantly improves LLMs'ability to recognize their knowledge gaps, enhancing the unknown perception rate by 5.6% on NQ and 4.9% on HotpotQA. Our findings suggest that pre-generation confidence estimation can optimize efficiency, while $C^3$ effectively controls output risks, advancing the reliability of LLMs in practical applications.",
      "arxiv_url": "https://arxiv.org/abs/2502.11677",
      "pdf_url": "https://arxiv.org/pdf/2502.11677",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01935",
      "title": "MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents",
      "authors": [
        "Kunlun Zhu",
        "Hongyi Du",
        "Zhaochen Hong",
        "Xiaocheng Yang",
        "Shuyi Guo",
        "Zhe Wang",
        "Zhenhailong Wang",
        "Cheng Qian",
        "Xiangru Tang",
        "Heng Ji",
        "Jiaxuan You"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE.",
      "arxiv_url": "https://arxiv.org/abs/2503.01935",
      "pdf_url": "https://arxiv.org/pdf/2503.01935",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2510.19367",
      "title": "Sign Language Translation with Sentence Embedding Supervision",
      "authors": [
        "Yasser Hamidullah",
        "Josef van Genabith",
        "C. España-Bonet"
      ],
      "abstract": "State-of-the-art sign language translation (SLT) systems facilitate the learning process through gloss annotations, either in an end2end manner or by involving an intermediate step. Unfortunately, gloss labelled sign language data is usually not available at scale and, when available, gloss annotations widely differ from dataset to dataset. We present a novel approach using sentence embeddings of the target sentences at training time that take the role of glosses. The new kind of supervision does not need any manual annotation but it is learned on raw textual data. As our approach easily facilitates multilinguality, we evaluate it on datasets covering German (PHOENIX-2014T) and American (How2Sign) sign languages and experiment with mono- and multilingual sentence embeddings and translation systems. Our approach significantly outperforms other gloss-free approaches, setting the new state-of-the-art for data sets where glosses are not available and when no additional SLT datasets are used for pretraining, diminishing the gap between gloss-free and gloss-dependent systems.",
      "arxiv_url": "https://arxiv.org/abs/2510.19367",
      "pdf_url": "https://arxiv.org/pdf/2510.19367",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-10-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11501",
      "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem?",
      "authors": [
        "Zichen Wen",
        "Yifeng Gao",
        "Weijia Li",
        "Conghui He",
        "Linfeng Zhang"
      ],
      "abstract": "Multimodal large language models (MLLMs) have shown remarkable performance for cross-modal understanding and generation, yet still suffer from severe inference costs. Recently, abundant works have been proposed to solve this problem with token pruning, which identifies the redundant tokens in MLLMs and then prunes them to reduce the computation and KV storage costs, leading to significant acceleration without training. While these methods claim efficiency gains, critical questions about their fundamental design and evaluation remain unanswered: Why do many existing approaches underperform even compared to naive random token selection? Are attention-based scoring sufficient for reliably identifying redundant tokens? Is language information really helpful during token pruning? What makes a good trade-off between token importance and duplication? Are current evaluation protocols comprehensive and unbiased? The ignorance of previous research on these problems hinders the long-term development of token pruning. In this paper, we answer these questions one by one, providing insights into the design of future token pruning methods.",
      "arxiv_url": "https://arxiv.org/abs/2502.11501",
      "pdf_url": "https://arxiv.org/pdf/2502.11501",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "89e91965747a059b7dbfbb75313accd31003ac4a",
      "title": "Unlocking LLMs' Self-Improvement Capacity with Autonomous Learning for Domain Adaptation",
      "authors": [
        "Ke Ji",
        "Junying Chen",
        "Anningzhe Gao",
        "Wenya Xie",
        "Xiang Wan",
        "Benyou Wang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/89e91965747a059b7dbfbb75313accd31003ac4a",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "8a00a50be9d0dfeb46cac2688a064e09597c3af8",
      "title": "ALW: Adaptive Layer-Wise contrastive decoding enhancing reasoning ability in Large Language Models",
      "authors": [
        "Yuechi Zhou",
        "Chuyue Zhou",
        "Jianxin Zhang",
        "Juntao Li",
        "Min Zhang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/8a00a50be9d0dfeb46cac2688a064e09597c3af8",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16520",
      "title": "Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs",
      "authors": [
        "Giovanni Servedio",
        "A. D. Bellis",
        "Dario Di Palma",
        "V. W. Anelli",
        "T. D. Noia"
      ],
      "abstract": "Factual hallucinations are a major challenge for Large Language Models (LLMs). They undermine reliability and user trust by generating inaccurate or fabricated content. Recent studies suggest that when generating false statements, the internal states of LLMs encode information about truthfulness. However, these studies often rely on synthetic datasets that lack realism, which limits generalization when evaluating the factual accuracy of text generated by the model itself. In this paper, we challenge the findings of previous work by investigating truthfulness encoding capabilities, leading to the generation of a more realistic and challenging dataset. Specifically, we extend previous work by introducing: (1) a strategy for sampling plausible true-false factoid sentences from tabular data and (2) a procedure for generating realistic, LLM-dependent true-false datasets from Question Answering collections. Our analysis of two open-source LLMs reveals that while the findings from previous studies are partially validated, generalization to LLM-generated datasets remains challenging. This study lays the groundwork for future research on factuality in LLMs and offers practical guidelines for more effective evaluation.",
      "arxiv_url": "https://arxiv.org/abs/2505.16520",
      "pdf_url": "https://arxiv.org/pdf/2505.16520",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.02901",
      "title": "Hide and Seek in Noise Labels: Noise-Robust Collaborative Active Learning with LLMs-Powered Assistance",
      "authors": [
        "Bo Yuan",
        "Yulin Chen",
        "Yin Zhang",
        "Wei Jiang"
      ],
      "abstract": "Learning from noisy labels (LNL) is a challenge that arises in many real-world scenarios where collected training data can contain incorrect or corrupted labels. Most existing solutions identify noisy labels and adopt active learning to query human experts on them for denoising. In the era of large language models (LLMs), although we can reduce the human effort to improve these methods, their performances are still subject to accurately separating the clean and noisy samples from noisy data. In this paper, we propose an innovative collaborative learning framework NoiseAL based on active learning to combine LLMs and small models (SMs) for learning from noisy labels. During collaborative training, we first adopt two SMs to form a co-prediction network and propose a dynamic-enhanced threshold strategy to divide the noisy data into different subsets, then select the clean and noisy samples from these subsets to feed the active annotator LLMs to rectify noisy samples. Finally, we employ different optimization objectives to conquer subsets with different degrees of label noises. Extensive experiments on synthetic and real-world noise datasets further demonstrate the superiority of our framework over state-of-the-art baselines.",
      "arxiv_url": "https://arxiv.org/abs/2504.02901",
      "pdf_url": "https://arxiv.org/pdf/2504.02901",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.01802",
      "title": "The Anatomy of Evidence: An Investigation Into Explainable ICD Coding",
      "authors": [
        "Katharina Beckh",
        "Elisa Studeny",
        "Sujan Sai Gannamaneni",
        "Dario Antweiler",
        "Stefan Rüping"
      ],
      "abstract": "Automatic medical coding has the potential to ease documentation and billing processes. For this task, transparency plays an important role for medical coders and regulatory bodies, which can be achieved using explainability methods. However, the evaluation of these approaches has been mostly limited to short text and binary settings due to a scarcity of annotated data. Recent efforts by Cheng et al. (2023) have introduced the MDACE dataset, which provides a valuable resource containing code evidence in clinical records. In this work, we conduct an in-depth analysis of the MDACE dataset and perform plausibility evaluation of current explainable medical coding systems from an applied perspective. With this, we contribute to a deeper understanding of automatic medical coding and evidence extraction. Our findings reveal that ground truth evidence aligns with code descriptions to a certain degree. An investigation into state-of-the-art approaches shows a high overlap with ground truth evidence. We propose match measures and highlight success and failure cases. Based on our findings, we provide recommendations for developing and evaluating explainable medical coding systems.",
      "arxiv_url": "https://arxiv.org/abs/2507.01802",
      "pdf_url": "https://arxiv.org/pdf/2507.01802",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.10728",
      "title": "Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims",
      "authors": [
        "Priyanka Kargupta",
        "Runchu Tian",
        "Jiawei Han"
      ],
      "abstract": "Claims made by individuals or entities are oftentimes nuanced and cannot be clearly labeled as entirely\"true\"or\"false\"-- as is frequently the case with scientific and political claims. However, a claim (e.g.,\"vaccine A is better than vaccine B\") can be dissected into its integral aspects and sub-aspects (e.g., efficacy, safety, distribution), which are individually easier to validate. This enables a more comprehensive, structured response that provides a well-rounded perspective on a given problem while also allowing the reader to prioritize specific angles of interest within the claim (e.g., safety towards children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based framework for automatically constructing a hierarchy of aspects typically considered when addressing a claim and enriching them with corpus-specific perspectives. This structure hierarchically partitions an input corpus to retrieve relevant segments, which assist in discovering new sub-aspects. Moreover, these segments enable the discovery of varying perspectives towards an aspect of the claim (e.g., support, neutral, or oppose) and their respective prevalence (e.g.,\"how many biomedical papers believe vaccine A is more transportable than B?\"). We apply ClaimSpect to a wide variety of real-world scientific and political claims featured in our constructed dataset, showcasing its robustness and accuracy in deconstructing a nuanced claim and representing perspectives within a corpus. Through real-world case studies and human evaluation, we validate its effectiveness over multiple baselines.",
      "arxiv_url": "https://arxiv.org/abs/2506.10728",
      "pdf_url": "https://arxiv.org/pdf/2506.10728",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-06-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "8ac1cc502e97a7940225720499c8cceb3c3b7530",
      "title": "Open-World Authorship Attribution",
      "authors": [
        "Xinhao Tan",
        "Songhua Liu",
        "Xia Cong",
        "Kunjun Li",
        "Xinchao Wang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/8ac1cc502e97a7940225720499c8cceb3c3b7530",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.08488",
      "title": "EtiCor++: Towards Understanding Etiquettical Bias in LLMs",
      "authors": [
        "Ashutosh Dwivedi",
        "Siddhant Singh",
        "Ashutosh Modi"
      ],
      "abstract": "In recent years, researchers have started analyzing the cultural sensitivity of LLMs. In this respect, Etiquettes have been an active area of research. Etiquettes are region-specific and are an essential part of the culture of a region; hence, it is imperative to make LLMs sensitive to etiquettes. However, there needs to be more resources in evaluating LLMs for their understanding and bias with regard to etiquettes. In this resource paper, we introduce EtiCor++, a corpus of etiquettes worldwide. We introduce different tasks for evaluating LLMs for knowledge about etiquettes across various regions. Further, we introduce various metrics for measuring bias in LLMs. Extensive experimentation with LLMs shows inherent bias towards certain regions.",
      "arxiv_url": "https://arxiv.org/abs/2506.08488",
      "pdf_url": "https://arxiv.org/pdf/2506.08488",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13738",
      "title": "Enhancing Input-Label Mapping in In-Context Learning with Contrastive Decoding",
      "authors": [
        "Keqin Peng",
        "Liang Ding",
        "Yuanxin Ouyang",
        "Meng Fang",
        "Yancheng Yuan",
        "D. Tao"
      ],
      "abstract": "Large language models (LLMs) excel at a range of tasks through in-context learning (ICL), where only a few task examples guide their predictions. However, prior research highlights that LLMs often overlook input-label mapping information in ICL, relying more on their pre-trained knowledge. To address this issue, we introduce In-Context Contrastive Decoding (ICCD), a novel method that emphasizes input-label mapping by contrasting the output distributions between positive and negative in-context examples. Experiments on 7 natural language understanding (NLU) tasks show that our ICCD method brings consistent and significant improvement (up to +1.8 improvement on average) upon 6 different scales of LLMs without requiring additional training. Our approach is versatile, enhancing performance with various demonstration selection methods, demonstrating its broad applicability and effectiveness. The code and scripts are released at https://github.com/Romainpkq/CD_ICL.",
      "arxiv_url": "https://arxiv.org/abs/2502.13738",
      "pdf_url": "https://arxiv.org/pdf/2502.13738",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "8afcd11c1565e0cfa4bba0059d6b07527d2c061c",
      "title": "HG-InsightLog: Context Prioritization and Reduction for Question Answering with Non-Natural Language Construct Log Data",
      "authors": [
        "Supriya Bajpai",
        "Athira Gopal",
        "Chandrakant Harjpal",
        "Niraj Kumar"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/8afcd11c1565e0cfa4bba0059d6b07527d2c061c",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.23948",
      "title": "AI2Agent: An End-to-End Framework for Deploying AI Projects as Autonomous Agents",
      "authors": [
        "Jiaxiang Chen",
        "Jingwei Shi",
        "Lei Gan",
        "Jiale Zhang",
        "Qingyu Zhang",
        "Dongqian Zhang",
        "Xin Pang",
        "Zhucong Li",
        "Yinghui Xu"
      ],
      "abstract": "As AI technology advances, it is driving innovation across industries, increasing the demand for scalable AI project deployment. However, deployment remains a critical challenge due to complex environment configurations, dependency conflicts, cross-platform adaptation, and debugging difficulties, which hinder automation and adoption. This paper introduces AI2Agent, an end-to-end framework that automates AI project deployment through guideline-driven execution, self-adaptive debugging, and case \\&solution accumulation. AI2Agent dynamically analyzes deployment challenges, learns from past cases, and iteratively refines its approach, significantly reducing human intervention. To evaluate its effectiveness, we conducted experiments on 30 AI deployment cases, covering TTS, text-to-image generation, image editing, and other AI applications. Results show that AI2Agent significantly reduces deployment time and improves success rates. The code and demo video are now publicly accessible.",
      "arxiv_url": "https://arxiv.org/abs/2503.23948",
      "pdf_url": "https://arxiv.org/pdf/2503.23948",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02077",
      "title": "Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition",
      "authors": [
        "Yoonjun Cho",
        "Soeun Kim",
        "Dongjae Jeon",
        "Kyelim Lee",
        "Beomsoo Lee",
        "Albert No"
      ],
      "abstract": "Decomposing weight matrices into quantization and low-rank components ($\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$) is a widely used technique for compressing large language models (LLMs). Existing joint optimization methods iteratively alternate between quantization and low-rank approximation. However, these methods tend to prioritize one component at the expense of the other, resulting in suboptimal decompositions that fail to leverage each component's unique strengths. In this work, we introduce Outlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank components the specific role of capturing activation-sensitive weights. This structured decomposition mitigates outliers' negative impact on quantization, enabling more effective balance between quantization and low-rank approximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B demonstrate that incorporating ODLRI into the joint optimization framework consistently reduces activation-aware error, minimizes quantization scale, and improves perplexity and zero-shot accuracy in low-bit settings.",
      "arxiv_url": "https://arxiv.org/abs/2506.02077",
      "pdf_url": "https://arxiv.org/pdf/2506.02077",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "8b43153a6ae14c61f53dd93aa8cf597d90725de6",
      "title": "Towards LLM-powered Attentive Listener: A Pragmatic Approach through Quantity Self-Repair",
      "authors": [
        "Junlin Li",
        "Bo Peng",
        "Yu-Yin Hsu"
      ],
      "abstract": "Grice’s Quantity Maxims dictate that human speakers aim for the optimal quantity of information during conversation. To empower LLMs to self-repair their responses toward optimal quantity and improve their attentive listening skills, we propose Q-Tuning and Q-Traveling , which draw on heuristic path-finding to enable decoder-only LLMs to travel among multiple “Q-alternatives” (Quantity Al-ternatives) and search for the optimal quantity in coordination with a conversation goal. Automatic and human evaluations demonstrate the effectiveness of Q-Tuning and Q-Traveling in constructing human-like, user-centered conversation agents. Our repository is open-sourced via https://github.com/ CN-Eyetk/QTraveling .",
      "arxiv_url": "https://www.semanticscholar.org/paper/8b43153a6ae14c61f53dd93aa8cf597d90725de6",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24593",
      "title": "Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis",
      "authors": [
        "Junzhuo Li",
        "Bo Wang",
        "Xiuze Zhou",
        "Peijie Jiang",
        "Jia Liu",
        "Xuming Hu"
      ],
      "abstract": "The interpretability of Mixture-of-Experts (MoE) models, especially those with heterogeneous designs, remains underexplored. Existing attribution methods for dense models fail to capture dynamic routing-expert interactions in sparse MoE architectures. To address this issue, we propose a cross-level attribution algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE, Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mistral-7B). Results show MoE models achieve 37% higher per-layer efficiency via a\"mid-activation, late-amplification\"pattern: early layers screen experts, while late layers refine knowledge collaboratively. Ablation studies reveal a\"basic-refinement\"framework--shared experts handle general tasks (entity recognition), while routed experts specialize in domain-specific processing (geographic attributes). Semantic-driven routing is evidenced by strong correlations between attention heads and experts (r=0.68), enabling task-aware coordination. Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10 experts) through shared expert redundancy, whereas shallow OLMoE suffers severe degradation (76% drop). Task sensitivity further guides design: core-sensitive tasks (geography) require concentrated expertise, while distributed-tolerant tasks (object attributes) leverage broader participation. These insights advance MoE interpretability, offering principles to balance efficiency, specialization, and robustness.",
      "arxiv_url": "https://arxiv.org/abs/2505.24593",
      "pdf_url": "https://arxiv.org/pdf/2505.24593",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.05280",
      "title": "Revealing Hidden Mechanisms of Cross-Country Content Moderation with Natural Language Processing",
      "authors": [
        "Neemesh Yadav",
        "Jiarui Liu",
        "Francesco Ortu",
        "Roya Ensafi",
        "Zhijing Jin",
        "Rada Mihalcea"
      ],
      "abstract": "The ability of Natural Language Processing (NLP) methods to categorize text into multiple classes has motivated their use in online content moderation tasks, such as hate speech and fake news detection. However, there is limited understanding of how or why these methods make such decisions, or why certain content is moderated in the first place. To investigate the hidden mechanisms behind content moderation, we explore multiple directions: 1) training classifiers to reverse-engineer content moderation decisions across countries; 2) explaining content moderation decisions by analyzing Shapley values and LLM-guided explanations. Our primary focus is on content moderation decisions made across countries, using pre-existing corpora sampled from the Twitter Stream Grab. Our experiments reveal interesting patterns in censored posts, both across countries and over time. Through human evaluations of LLM-generated explanations across three LLMs, we assess the effectiveness of using LLMs in content moderation. Finally, we discuss potential future directions, as well as the limitations and ethical considerations of this work. Our code and data are available at https://github.com/causalNLP/censorship",
      "arxiv_url": "https://arxiv.org/abs/2503.05280",
      "pdf_url": "https://arxiv.org/pdf/2503.05280",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.00218",
      "title": "Agents Under Siege: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks",
      "authors": [
        "Rana Muhammad Shahroz Khan",
        "Zhen Tan",
        "Sukwon Yun",
        "Charles Flemming",
        "Tianlong Chen"
      ],
      "abstract": "Most discussions about Large Language Model (LLM) safety have focused on single-agent settings but multi-agent LLM systems now create novel adversarial risks because their behavior depends on communication between agents and decentralized reasoning. In this work, we innovatively focus on attacking pragmatic systems that have constrains such as limited token bandwidth, latency between message delivery, and defense mechanisms. We design a $\\textit{permutation-invariant adversarial attack}$ that optimizes prompt distribution across latency and bandwidth-constraint network topologies to bypass distributed safety mechanisms within the system. Formulating the attack path as a problem of $\\textit{maximum-flow minimum-cost}$, coupled with the novel $\\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage graph-based optimization to maximize attack success rate while minimizing detection risk. Evaluating across models including $\\texttt{Llama}$, $\\texttt{Mistral}$, $\\texttt{Gemma}$, $\\texttt{DeepSeek}$ and other variants on various datasets like $\\texttt{JailBreakBench}$ and $\\texttt{AdversarialBench}$, our method outperforms conventional attacks by up to $7\\times$, exposing critical vulnerabilities in multi-agent systems. Moreover, we demonstrate that existing defenses, including variants of $\\texttt{Llama-Guard}$ and $\\texttt{PromptGuard}$, fail to prohibit our attack, emphasizing the urgent need for multi-agent specific safety mechanisms.",
      "arxiv_url": "https://arxiv.org/abs/2504.00218",
      "pdf_url": "https://arxiv.org/pdf/2504.00218",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23029",
      "title": "Uncovering Visual-Semantic Psycholinguistic Properties from the Distributional Structure of Text Embedding Spac",
      "authors": [
        "Si Wu",
        "Sebastian Bruch"
      ],
      "abstract": "Imageability (potential of text to evoke a mental image) and concreteness (perceptibility of text) are two psycholinguistic properties that link visual and semantic spaces. It is little surprise that computational methods that estimate them do so using parallel visual and semantic spaces, such as collections of image-caption pairs or multi-modal models. In this paper, we work on the supposition that text itself in an image-caption dataset offers sufficient signals to accurately estimate these properties. We hypothesize, in particular, that the peakedness of the neighborhood of a word in the semantic embedding space reflects its degree of imageability and concreteness. We then propose an unsupervised, distribution-free measure, which we call Neighborhood Stability Measure (NSM), that quantifies the sharpness of peaks. Extensive experiments show that NSM correlates more strongly with ground-truth ratings than existing unsupervised methods, and is a strong predictor of these properties for classification. Our code and data are available on GitHub (https://github.com/Artificial-Memory-Lab/imageability).",
      "arxiv_url": "https://arxiv.org/abs/2505.23029",
      "pdf_url": "https://arxiv.org/pdf/2505.23029",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05598",
      "title": "SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models in LLMs",
      "authors": [
        "Michael J. Ryan",
        "Omar Shaikh",
        "Aditri Bhagirath",
        "Daniel Frees",
        "William B. Held",
        "Diyi Yang"
      ],
      "abstract": "Recent calls for pluralistic alignment of Large Language Models (LLMs) encourage adapting models to diverse user preferences. However, most prior work on personalized reward models heavily rely on additional identity information, such as demographic details or a predefined set of preference categories. To this end, we introduce SynthesizeMe, an approach to inducing synthetic user personas from user interactions for personalized reward modeling. SynthesizeMe first generates and verifies reasoning to explain user preferences, then induces synthetic user personas from that reasoning, and finally filters to informative prior user interactions in order to build personalized prompts for a particular user. We show that using SynthesizeMe induced prompts improves personalized LLM-as-a-judge accuracy by 4.4% on Chatbot Arena. Combining SynthesizeMe derived prompts with a reward model achieves top performance on PersonalRewardBench: a new curation of user-stratified interactions with chatbots collected from 854 users of Chatbot Arena and PRISM.",
      "arxiv_url": "https://arxiv.org/abs/2506.05598",
      "pdf_url": "https://arxiv.org/pdf/2506.05598",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.08914",
      "title": "Diffusion Models Through a Global Lens: Are They Culturally Inclusive?",
      "authors": [
        "Zahra Bayramli",
        "Ayhan Suleymanzade",
        "Na Min An",
        "Huzama Ahmad",
        "Eunsu Kim",
        "Junyeong Park",
        "James Thorne",
        "Alice Oh"
      ],
      "abstract": "Text-to-image diffusion models have recently enabled the creation of visually compelling, detailed images from textual prompts. However, their ability to accurately represent various cultural nuances remains an open question. In our work, we introduce CultDiff benchmark, evaluating state-of-the-art diffusion models whether they can generate culturally specific images spanning ten countries. We show that these models often fail to generate cultural artifacts in architecture, clothing, and food, especially for underrepresented country regions, by conducting a fine-grained analysis of different similarity aspects, revealing significant disparities in cultural relevance, description fidelity, and realism compared to real-world reference images. With the collected human evaluations, we develop a neural-based image-image similarity metric, namely, CultDiff-S, to predict human judgment on real and generated images with cultural artifacts. Our work highlights the need for more inclusive generative AI systems and equitable dataset representation over a wide range of cultures.",
      "arxiv_url": "https://arxiv.org/abs/2502.08914",
      "pdf_url": "https://arxiv.org/pdf/2502.08914",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.12619",
      "title": "Quantification of Large Language Model Distillation",
      "authors": [
        "Sunbowen Lee",
        "Junting Zhou",
        "Chang Ao",
        "Kaige Li",
        "Xinrun Du",
        "Sirui He",
        "Haihong Wu",
        "Tianci Liu",
        "Jiaheng Liu",
        "Hamid Alinejad-Rokny",
        "Min Yang",
        "Yitao Liang",
        "Zhoufutu Wen",
        "Shiwen Ni"
      ],
      "abstract": "Model distillation is a fundamental technique in building large language models (LLMs), transferring knowledge from a teacher model to a student model. However, distillation can lead to model homogenization, reducing diversity among models and impairing their ability to robustly handle complex or novel tasks. These limitations underscore the need to systematically quantify the distillation process and its impact. In this work, we propose a framework to evaluate and quantify model distillation. Our method addresses two key aspects: (1) Identifying identity cognition contradictions to assess discrepancies in how models perceive and represent identity-related information, and (2) Analyzing multi-granularity response similarities across models to measure the extent of homogenization. Experimental results demonstrate two key insights: (1) Well-known closed-source and open-source LLMs usually exhibit high distillation degrees, except for Claude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees compared to aligned LLMs. By offering a systematic approach to improve the transparency of LLM data distillation, we call for LLMs with more independent development and more transparent technical reports to improve LLMs' robustness and safety. The code and data are available under https://github.com/Aegis1863/LLMs-Distillation-Quantification.",
      "arxiv_url": "https://arxiv.org/abs/2501.12619",
      "pdf_url": "https://arxiv.org/pdf/2501.12619",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15226",
      "title": "Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews",
      "authors": [
        "Mengqiao Liu",
        "Tevin Wang",
        "Cassandra A. Cohen",
        "Sarah Li",
        "Chenyan Xiong"
      ],
      "abstract": "Which large language model (LLM) is better? Every evaluation tells a story, but what do users really think about current LLMs? This paper presents CLUE, an LLM-powered interviewer that conducts in-the-moment user experience interviews, right after users interact with LLMs, and automatically gathers insights about user opinions from massive interview logs. We conduct a study with thousands of users to understand user opinions on mainstream LLMs, recruiting users to first chat with a target LLM and then be interviewed by CLUE. Our experiments demonstrate that CLUE captures interesting user opinions, e.g., the bipolar views on the displayed reasoning process of DeepSeek-R1 and demands for information freshness and multi-modality. Our code and data are at https://github.com/cxcscmu/LLM-Interviewer.",
      "arxiv_url": "https://arxiv.org/abs/2502.15226",
      "pdf_url": "https://arxiv.org/pdf/2502.15226",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18855",
      "title": "Inference Compute-Optimal Video Vision Language Models",
      "authors": [
        "Peiqi Wang",
        "ShengYun Peng",
        "Xuewen Zhang",
        "Hanchao Yu",
        "Yibo Yang",
        "Lifu Huang",
        "Fujun Liu",
        "Qifan Wang"
      ],
      "abstract": "This work investigates the optimal allocation of inference compute across three key scaling factors in video vision language models: language model size, frame count, and the number of visual tokens per frame. While prior works typically focuses on optimizing model efficiency or improving performance without considering resource constraints, we instead identify optimal model configuration under fixed inference compute budgets. We conduct large-scale training sweeps and careful parametric modeling of task performance to identify the inference compute-optimal frontier. Our experiments reveal how task performance depends on scaling factors and finetuning data size, as well as how changes in data size shift the compute-optimal frontier. These findings translate to practical tips for selecting these scaling factors.",
      "arxiv_url": "https://arxiv.org/abs/2505.18855",
      "pdf_url": "https://arxiv.org/pdf/2505.18855",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.05319",
      "title": "LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review",
      "authors": [
        "Cheng Yuan",
        "Xinkai Rui",
        "Yongqi Fan",
        "Yawei Fan",
        "Boyang Zhong",
        "Jiacheng Wang",
        "Weiyan Zhang",
        "Tong Ruan"
      ],
      "abstract": "Despite the remarkable performance of Large Language Models (LLMs) in automated discharge summary generation, they still suffer from hallucination issues, such as generating inaccurate content or fabricating information without valid sources. In addition, electronic medical records (EMRs) typically consist of long-form data, making it challenging for LLMs to attribute the generated content to the sources. To address these challenges, we propose LCDS, a Logic-Controlled Discharge Summary generation system. LCDS constructs a source mapping table by calculating textual similarity between EMRs and discharge summaries to constrain the scope of summarized content. Moreover, LCDS incorporates a comprehensive set of logical rules, enabling it to generate more reliable silver discharge summaries tailored to different clinical fields. Furthermore, LCDS supports source attribution for generated content, allowing experts to efficiently review, provide feedback, and rectify errors. The resulting golden discharge summaries are subsequently recorded for incremental fine-tuning of LLMs. Our project and demo video are in the GitHub repository https://github.com/ycycyc02/LCDS.",
      "arxiv_url": "https://arxiv.org/abs/2507.05319",
      "pdf_url": "https://arxiv.org/pdf/2507.05319",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "8cd4f6ba5632d0c5d2c943c409c3861cbb71572f",
      "title": "MPVStance: Mitigating Hallucinations in Stance Detection with Multi-Perspective Verification",
      "authors": [
        "Zhaodan Zhang",
        "Zhaodan Zhang",
        "Jin Zhang",
        "Hui Xu",
        "Xueqi Cheng"
      ],
      "abstract": "Stance detection is a pivotal task in Natural Language Processing (NLP), identifying textual attitudes toward various targets. Despite advances in using Large Language Models (LLMs), challenges persist due to hallucination-models generating plausible yet inaccurate content. Addressing these challenges, we introduce MPVStance, a framework that incorporates Multi-Perspective Verification (MPV) with Retrieval-Augmented Generation (RAG) across a structured five-step verification process. Our method enhances stance detection by rigorously validating each response from factual accuracy, logical consistency, contextual relevance, and other perspectives. Extensive testing on the SemEval-2016 and VAST datasets, including scenarios that challenge existing methods and comprehensive ablation studies, demonstrates that MPVStance significantly outperforms current models. It effectively mitigates hallucination issues and sets new benchmarks for reliability and accuracy in stance detection, particularly in zero-shot, few-shot, and challenging scenarios.",
      "arxiv_url": "https://www.semanticscholar.org/paper/8cd4f6ba5632d0c5d2c943c409c3861cbb71572f",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11190",
      "title": "ReLearn: Unlearning via Learning for Large Language Models",
      "authors": [
        "Haoming Xu",
        "Ningyuan Zhao",
        "Liming Yang",
        "Sendong Zhao",
        "Shumin Deng",
        "Mengru Wang",
        "Bryan Hooi",
        "Nay Oo",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.",
      "arxiv_url": "https://arxiv.org/abs/2502.11190",
      "pdf_url": "https://arxiv.org/pdf/2502.11190",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11425",
      "title": "Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models",
      "authors": [
        "Jongho Kim",
        "Seung-won Hwang"
      ],
      "abstract": "Despite the advanced capabilities of large language models (LLMs), their temporal reasoning ability remains underdeveloped. Prior works have highlighted this limitation, particularly in maintaining temporal consistency when understanding events. For example, models often confuse mutually exclusive temporal relations like ``before'' and ``after'' between events and make inconsistent predictions. In this work, we tackle the issue of temporal inconsistency in LLMs by proposing a novel counterfactual prompting approach. Our method generates counterfactual questions and enforces collective constraints, enhancing the model's consistency. We evaluate our method on multiple datasets, demonstrating significant improvements in event ordering for explicit and implicit events and temporal commonsense understanding by effectively addressing temporal inconsistencies.",
      "arxiv_url": "https://arxiv.org/abs/2502.11425",
      "pdf_url": "https://arxiv.org/pdf/2502.11425",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.08672",
      "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework For Advanced Reasoning",
      "authors": [
        "Fangzhi Xu",
        "Hang Yan",
        "Chang Ma",
        "Haiteng Zhao",
        "Qiushi Sun",
        "Kanzhi Cheng",
        "Junxian He",
        "Jun Liu",
        "Zhiyong Wu"
      ],
      "abstract": "Advancing LLM reasoning skills has captivated wide interest. However, current post-training techniques rely heavily on supervisory signals, such as outcome supervision or auxiliary reward models, which face the problem of scalability and high annotation costs. This motivates us to enhance LLM reasoning without the need for external supervision. We introduce a generalizable and purely unsupervised self-training framework, named Genius. Without external auxiliary, Genius requires to seek the optimal response sequence in a stepwise manner and optimize the LLM. To explore the potential steps and exploit the optimal ones, Genius introduces a stepwise foresight re-sampling strategy to sample and estimate the step value by simulating future outcomes. Further, we recognize that the unsupervised setting inevitably induces the intrinsic noise and uncertainty. To provide a robust optimization, we propose an advantage-calibrated optimization (ACO) loss function to mitigate estimation inconsistencies. Combining these techniques together, Genius provides an advanced initial step towards self-improve LLM reasoning with general queries and without supervision, revolutionizing reasoning scaling laws given the vast availability of general queries. The code will be released at https://github.com/xufangzhi/Genius.",
      "arxiv_url": "https://arxiv.org/abs/2504.08672",
      "pdf_url": "https://arxiv.org/pdf/2504.08672",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00088",
      "title": "HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs",
      "authors": [
        "Qing Li",
        "Jiahui Geng",
        "Zongxiong Chen",
        "Derui Zhu",
        "Yuxia Wang",
        "Congbo Ma",
        "Chenyang Lyu",
        "Fakhri Karray"
      ],
      "abstract": "In recent years, large language models (LLMs) have made remarkable advancements, yet hallucination, where models produce inaccurate or non-factual statements, remains a significant challenge for real-world deployment. Although current classification-based methods, such as SAPLMA, are highly efficient in mitigating hallucinations, they struggle when non-factual information arises in the early or mid-sequence of outputs, reducing their reliability. To address these issues, we propose Hallucination Detection-Neural Differential Equations (HD-NDEs), a novel method that systematically assesses the truthfulness of statements by capturing the full dynamics of LLMs within their latent space. Our approaches apply neural differential equations (Neural DEs) to model the dynamic system in the latent space of LLMs. Then, the sequence in the latent space is mapped to the classification space for truth assessment. The extensive experiments across five datasets and six widely used LLMs demonstrate the effectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC on the True-False dataset compared to state-of-the-art techniques.",
      "arxiv_url": "https://arxiv.org/abs/2506.00088",
      "pdf_url": "https://arxiv.org/pdf/2506.00088",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18407",
      "title": "AgentRM: Enhancing Agent Generalization with Reward Modeling",
      "authors": [
        "Yu Xia",
        "Jingru Fan",
        "Weize Chen",
        "Siyu Yan",
        "Xin Cong",
        "Zhong Zhang",
        "Ya-Ting Lu",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Existing LLM-based agents have achieved strong performance on held-in tasks, but their generalizability to unseen tasks remains poor. Hence, some recent work focus on fine-tuning the policy model with more diverse tasks to improve the generalizability. In this work, we find that finetuning a reward model to guide the policy model is more robust than directly finetuning the policy model. Based on this finding, we propose AgentRM, a generalizable reward model, to guide the policy model for effective test-time search. We comprehensively investigate three approaches to construct the reward model, including explicit reward modeling, implicit reward modeling and LLM-as-a-judge. We then use AgentRM to guide the answer generation with Best-of-N sampling and step-level beam search. On four types of nine agent tasks, AgentRM enhances the base policy model by $8.8$ points on average, surpassing the top general agent by $4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding greater improvement of $12.6$ on LLaMA-3-70B policy model. As for the specializability, AgentRM can also boost a finetuned policy model and outperform the top specialized agent by $11.4$ on three held-in tasks. Further analysis verifies its effectiveness in test-time scaling. Codes will be released to facilitate the research in this area.",
      "arxiv_url": "https://arxiv.org/abs/2502.18407",
      "pdf_url": "https://arxiv.org/pdf/2502.18407",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "8da765a0bc68ecdfdec249c6e6809341f2d2908e",
      "title": "Global Eye: Breaking the \"Fixed Thinking Pattern\" during the Instruction Expansion Process",
      "authors": [
        "Wenxuan Lu",
        "Wei Liu",
        "Jian Luan",
        "Bin Wang",
        "Songhao Jiang",
        "Tianning Zang"
      ],
      "abstract": "An extensive high-quality instruction dataset is crucial for the instruction tuning process of Large Language Models (LLMs). Recent instruction expansion methods have demonstrated their capability to improve the quality and quantity of existing datasets, by prompting high-performance LLM to generate multiple new instructions from the original ones. However, existing methods focus on constructing multi-perspective prompts (e.g., increasing complexity or difficulty) to expand instructions, overlooking the “Fixed Thinking Pattern\" issue of LLMs. This issue arises when repeatedly using the same set of prompts, causing LLMs to rely on a limited set of certain expressions to expand all instructions, potentially compromising the diversity of the final expanded dataset. This paper theoretically analyzes the causes of the “Fixed Thinking Pattern\", and corroborates this phenomenon through multi-faceted empirical research. Furthermore, we propose a novel method based on dynamic prompt updating: Global Eye. Specifically, after a fixed number of instruction expansions, we analyze the statistical characteristics of newly generated instructions and then update the prompts. Experimental results show that our method enables LLaMA3-8B and LLaMA2-13B to surpass the performance of open-source LLMs and GPT3.5 across various metrics.",
      "arxiv_url": "https://www.semanticscholar.org/paper/8da765a0bc68ecdfdec249c6e6809341f2d2908e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "8dae8968e1c787d6ad9875bcd5f32858fec194f4",
      "title": "DiffSkip: Differential Layer Skipping in Large Language Models",
      "authors": [
        "Xuan Luo",
        "Weizhi Wang",
        "Xifeng Yan"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/8dae8968e1c787d6ad9875bcd5f32858fec194f4",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.09130",
      "title": "VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search",
      "authors": [
        "Yikun Wang",
        "Siyin Wang",
        "Qinyuan Cheng",
        "Zhaoye Fei",
        "Liang Ding",
        "Qipeng Guo",
        "D. Tao",
        "Xipeng Qiu"
      ],
      "abstract": "Recent advancements in Large Vision-Language Models have showcased remarkable capabilities. However, they often falter when confronted with complex reasoning tasks that humans typically address through visual aids and deliberate, step-by-step thinking. While existing methods have explored text-based slow thinking or rudimentary visual assistance, they fall short of capturing the intricate, interleaved nature of human visual-verbal reasoning processes. To overcome these limitations and inspired by the mechanisms of slow thinking in human cognition, we introduce VisuoThink, a novel framework that seamlessly integrates visuospatial and linguistic domains. VisuoThink facilitates multimodal slow thinking by enabling progressive visual-textual reasoning and incorporates test-time scaling through look-ahead tree search. Extensive experiments demonstrate that VisuoThink significantly enhances reasoning capabilities via inference-time scaling, even without fine-tuning, achieving state-of-the-art performance in tasks involving geometry and spatial reasoning.",
      "arxiv_url": "https://arxiv.org/abs/2504.09130",
      "pdf_url": "https://arxiv.org/pdf/2504.09130",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14272",
      "title": "Capturing Nuanced Preferences: Preference-Aligned Distillation for Small Language Models",
      "authors": [
        "Yanggan Gu",
        "Junzhuo Li",
        "Sirui Huang",
        "Xin Zou",
        "Zhenghua Li",
        "Xuming Hu"
      ],
      "abstract": "Aligning small language models (SLMs) with human values typically involves distilling preference knowledge from large language models (LLMs). However, existing distillation methods model preference knowledge in teacher LLMs by comparing pairwise responses, overlooking the extent of difference between responses. This limitation hinders student SLMs from capturing the nuanced preferences for multiple responses. In this paper, we propose a Preference-Aligned Distillation (PAD) framework, which models teacher's preference knowledge as a probability distribution over all potential preferences, thereby providing more nuanced supervisory signals. Our insight in developing PAD is rooted in the demonstration that language models can serve as reward functions, reflecting their intrinsic preferences. Based on this, PAD comprises three key steps: (1) sampling diverse responses using high-temperature; (2) computing rewards for both teacher and student to construct their intrinsic preference; and (3) training the student's intrinsic preference distribution to align with the teacher's. Experiments on four mainstream alignment benchmarks demonstrate that PAD consistently and significantly outperforms existing approaches, achieving over 20\\% improvement on AlpacaEval 2 and Arena-Hard, indicating superior alignment with human preferences. Notably, on MT-Bench, using the \\textsc{Gemma} model family, the student trained by PAD surpasses its teacher, further validating the effectiveness of our PAD.",
      "arxiv_url": "https://arxiv.org/abs/2502.14272",
      "pdf_url": "https://arxiv.org/pdf/2502.14272",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "8e2153f6dda2b8bc09e77a27ee9003940b8856e5",
      "title": "From Complexity to Clarity: AI/NLP's Role in Regulatory Compliance",
      "authors": [
        "Jivitesh Jain",
        "Nivedhitha Dhanasekaran",
        "Mona T. Diab"
      ],
      "abstract": "Regulatory data compliance is a cornerstone of trust and accountability in critical sectors like finance, healthcare, and technology, yet its complexity poses significant challenges for organizations worldwide. Recent advances in nat-ural language processing, particularly large language models, have demonstrated remarkable capabilities in text analysis and reasoning, offering promising solutions for automating compliance processes. This survey examines the current state of automated data compliance, analyzing key challenges and approaches across problem areas. We identify critical limitations in current datasets and techniques, including issues of adaptability, completeness, and trust. Looking ahead, we propose research directions to address these challenges, emphasizing standardized evaluation frameworks and balanced human-AI collaboration.",
      "arxiv_url": "https://www.semanticscholar.org/paper/8e2153f6dda2b8bc09e77a27ee9003940b8856e5",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.08503",
      "title": "Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?",
      "authors": [
        "Jiahe Jin",
        "Yanheng He",
        "Mingyan Yang"
      ],
      "abstract": "In this work, we identify the\"2D-Cheating\"problem in 3D LLM evaluation, where these tasks might be easily solved by VLMs with rendered images of point clouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We test VLM performance across multiple 3D LLM benchmarks and, using this as a reference, propose principles for better assessing genuine 3D understanding. We also advocate explicitly separating 3D abilities from 1D or 2D aspects when evaluating 3D LLMs. Code and data are available at https://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks",
      "arxiv_url": "https://arxiv.org/abs/2502.08503",
      "pdf_url": "https://arxiv.org/pdf/2502.08503",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00551",
      "title": "AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation",
      "authors": [
        "Ming Wang",
        "Peidong Wang",
        "Lin Wu",
        "Xiaocui Yang",
        "Daling Wang",
        "Shi Feng",
        "Yuxin Chen",
        "Bixuan Wang",
        "Yifei Zhang"
      ],
      "abstract": "Constrained by the cost and ethical concerns of involving real seekers in AI-driven mental health, researchers develop LLM-based conversational agents (CAs) with tailored configurations, such as profiles, symptoms, and scenarios, to simulate seekers. While these efforts advance AI in mental health, achieving more realistic seeker simulation remains hindered by two key challenges: dynamic evolution and multi-session memory. Seekers'mental states often fluctuate during counseling, which typically spans multiple sessions. To address this, we propose AnnaAgent, an emotional and cognitive dynamic agent system equipped with tertiary memory. AnnaAgent incorporates an emotion modulator and a complaint elicitor trained on real counseling dialogues, enabling dynamic control of the simulator's configurations. Additionally, its tertiary memory mechanism effectively integrates short-term and long-term memory across sessions. Evaluation results, both automated and manual, demonstrate that AnnaAgent achieves more realistic seeker simulation in psychological counseling compared to existing baselines. The ethically reviewed and screened code can be found on https://github.com/sci-m-wang/AnnaAgent.",
      "arxiv_url": "https://arxiv.org/abs/2506.00551",
      "pdf_url": "https://arxiv.org/pdf/2506.00551",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.14218",
      "title": "Understanding the Repeat Curse in Large Language Models from a Feature Perspective",
      "authors": [
        "Junchi Yao",
        "Shu Yang",
        "Jianhua Xu",
        "Lijie Hu",
        "Mengdi Li",
        "Di Wang"
      ],
      "abstract": "Large language models (LLMs) have made remarkable progress in various domains, yet they often suffer from repetitive text generation, a phenomenon we refer to as the\"Repeat Curse\". While previous studies have proposed decoding strategies to mitigate repetition, the underlying mechanism behind this issue remains insufficiently explored. In this work, we investigate the root causes of repetition in LLMs through the lens of mechanistic interpretability. Inspired by recent advances in Sparse Autoencoders (SAEs), which enable monosemantic feature extraction, we propose a novel approach,\"Duplicatus Charm\", to induce and analyze the Repeat Curse. Our method systematically identifies\"Repetition Features\"-the key model activations responsible for generating repetitive outputs. First, we locate the layers most involved in repetition through logit analysis. Next, we extract and stimulate relevant features using SAE-based activation manipulation. To validate our approach, we construct a repetition dataset covering token and paragraph level repetitions and introduce an evaluation pipeline to quantify the influence of identified repetition features. Furthermore, by deactivating these features, we have effectively mitigated the Repeat Curse. The source code of our work is publicly available at: https://github.com/kaustpradalab/repeat-curse-llm",
      "arxiv_url": "https://arxiv.org/abs/2504.14218",
      "pdf_url": "https://arxiv.org/pdf/2504.14218",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12266",
      "title": "The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs",
      "authors": [
        "Avinash Baidya",
        "Kamalika Das",
        "Xiang Gao"
      ],
      "abstract": "Large Language Model (LLM)-based agents have significantly impacted Task-Oriented Dialog Systems (TODS) but continue to face notable performance challenges, especially in zero-shot scenarios. While prior work has noted this performance gap, the behavioral factors driving the performance gap remain under-explored. This study proposes a comprehensive evaluation framework to quantify the behavior gap between AI agents and human experts, focusing on discrepancies in dialog acts, tool usage, and knowledge utilization. Our findings reveal that this behavior gap is a critical factor negatively impacting the performance of LLM agents. Notably, as task complexity increases, the behavior gap widens (correlation: 0.963), leading to a degradation of agent performance on complex task-oriented dialogs. For the most complex task in our study, even the GPT-4o-based agent exhibits low alignment with human behavior, with low F1 scores for dialog acts (0.464), excessive and often misaligned tool usage with a F1 score of 0.139, and ineffective usage of external knowledge. Reducing such behavior gaps leads to significant performance improvement (24.3% on average). This study highlights the importance of comprehensive behavioral evaluations and improved alignment strategies to enhance the effectiveness of LLM-based TODS in handling complex tasks.",
      "arxiv_url": "https://arxiv.org/abs/2506.12266",
      "pdf_url": "https://arxiv.org/pdf/2506.12266",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-06-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19776",
      "title": "Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification",
      "authors": [
        "Akram Elbouanani",
        "Evan Dufraisse",
        "Adrian Popescu"
      ],
      "abstract": "Political biases encoded by LLMs might have detrimental effects on downstream applications. Existing bias analysis methods rely on small-size intermediate tasks (questionnaire answering or political content generation) and rely on the LLMs themselves for analysis, thus propagating bias. We propose a new approach leveraging the observation that LLM sentiment predictions vary with the target entity in the same sentence. We define an entropy-based inconsistency metric to encode this prediction variability. We insert 1319 demographically and politically diverse politician names in 450 political sentences and predict target-oriented sentiment using seven models in six widely spoken languages. We observe inconsistencies in all tested combinations and aggregate them in a statistically robust analysis at different granularity levels. We observe positive and negative bias toward left and far-right politicians and positive correlations between politicians with similar alignment. Bias intensity is higher for Western languages than for others. Larger models exhibit stronger and more consistent biases and reduce discrepancies between similar languages. We partially mitigate LLM unreliability in target-oriented sentiment classification (TSC) by replacing politician names with fictional but plausible counterparts.",
      "arxiv_url": "https://arxiv.org/abs/2505.19776",
      "pdf_url": "https://arxiv.org/pdf/2505.19776",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.09081",
      "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation",
      "authors": [
        "Zheqi He",
        "Yesheng Liu",
        "Jingshu Zheng",
        "Xuejing Li",
        "Jin-Ge Yao",
        "Bowen Qin",
        "Richeng Xuan",
        "Xi Yang"
      ],
      "abstract": "We present FlagEvalMM, an open-source evaluation framework designed to comprehensively assess multimodal models across a diverse range of vision-language understanding and generation tasks, such as visual question answering, text-to-image/video generation, and image-text retrieval. We decouple model inference from evaluation through an independent evaluation service, thus enabling flexible resource allocation and seamless integration of new tasks and models. Moreover, FlagEvalMM utilizes advanced inference acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to significantly enhance evaluation efficiency. Extensive experiments show that FlagEvalMM offers accurate and efficient insights into model strengths and limitations, making it a valuable tool for advancing multimodal research. The framework is publicly accessible at https://github.com/flageval-baai/FlagEvalMM.",
      "arxiv_url": "https://arxiv.org/abs/2506.09081",
      "pdf_url": "https://arxiv.org/pdf/2506.09081",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.13554",
      "title": "Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation",
      "authors": [
        "Zhanglin Wu",
        "Daimeng Wei",
        "Xiaoyu Chen",
        "Hengchao Shang",
        "Jiaxin Guo",
        "Zongyao Li",
        "Yuanchang Luo",
        "Jinlong Yang",
        "Zhiqiang Rao",
        "Hao Yang"
      ],
      "abstract": "Large language model (LLM) shows promising performances in a variety of downstream tasks, such as machine translation (MT). However, using LLMs for translation suffers from high computational costs and significant latency. Based on our evaluation, in most cases, translations using LLMs are comparable to that generated by neural machine translation (NMT) systems. Only in particular scenarios, LLM and NMT models show respective advantages. As a result, integrating NMT and LLM for translation and using LLM only when necessary seems to be a sound solution. A scheduling policy that optimizes translation result while ensuring fast speed and as little LLM usage as possible is thereby required. We compare several scheduling policies and propose a novel and straightforward decider that leverages source sentence features. We conduct extensive experiments on multilingual test sets and the result shows that we can achieve optimal translation performance with minimal LLM usage, demonstrating effectiveness of our decider.",
      "arxiv_url": "https://arxiv.org/abs/2505.13554",
      "pdf_url": "https://arxiv.org/pdf/2505.13554",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.17896",
      "title": "VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL",
      "authors": [
        "Shubham Mohole",
        "Sainyam Galhotra"
      ],
      "abstract": "Application systems using natural language interfaces to databases (NLIDBs) have democratized data analysis. This positive development has also brought forth an urgent challenge to help users who might use these systems without a background in statistical analysis to formulate bias-free analytical questions. Although significant research has focused on text-to-SQL generation accuracy, addressing cognitive biases in analytical questions remains underexplored. We present VeriMinder, https://veriminder.ai, an interactive system for detecting and mitigating such analytical vulnerabilities. Our approach introduces three key innovations: (1) a contextual semantic mapping framework for biases relevant to specific analysis contexts (2) an analytical framework that operationalizes the Hard-to-Vary principle and guides users in systematic data analysis (3) an optimized LLM-powered system that generates high-quality, task-specific prompts using a structured process involving multiple candidates, critic feedback, and self-reflection. User testing confirms the merits of our approach. In direct user experience evaluation, 82.5% participants reported positively impacting the quality of the analysis. In comparative evaluation, VeriMinder scored significantly higher than alternative approaches, at least 20% better when considered for metrics of the analysis's concreteness, comprehensiveness, and accuracy. Our system, implemented as a web application, is set to help users avoid\"wrong question\"vulnerability during data analysis. VeriMinder code base with prompts, https://reproducibility.link/veriminder, is available as an MIT-licensed open-source software to facilitate further research and adoption within the community.",
      "arxiv_url": "https://arxiv.org/abs/2507.17896",
      "pdf_url": "https://arxiv.org/pdf/2507.17896",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "8f65800549e6ba0b4fd2a9aafdb798eb4bde022d",
      "title": "RQT: Hierarchical Residual Quantization for Multi-Model Compression",
      "authors": [
        "Tianqi Chen",
        "Peisong Wang",
        "Weixiang Xu",
        "Zeyu Zhu",
        "Jian Cheng"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/8f65800549e6ba0b4fd2a9aafdb798eb4bde022d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06404",
      "title": "Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights",
      "authors": [
        "Sooyung Choi",
        "Jaehyeok Lee",
        "Xiaoyuan Yi",
        "Jing Yao",
        "Xing Xie",
        "J. Bak"
      ],
      "abstract": "The application scope of Large Language Models (LLMs) continues to expand, leading to increasing interest in personalized LLMs that align with human values. However, aligning these models with individual values raises significant safety concerns, as certain values may correlate with harmful information. In this paper, we identify specific safety risks associated with value-aligned LLMs and investigate the psychological principles behind these challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are more prone to harmful behavior compared to non-fine-tuned models and exhibit slightly higher risks in traditional safety evaluations than other fine-tuned models. (2) These safety issues arise because value-aligned LLMs genuinely generate text according to the aligned values, which can amplify harmful outcomes. Using a dataset with detailed safety categories, we find significant correlations between value alignment and safety risks, supported by psychological hypotheses. This study offers insights into the\"black box\"of value alignment and proposes in-context alignment methods to enhance the safety of value-aligned LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.06404",
      "pdf_url": "https://arxiv.org/pdf/2506.06404",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-06-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20826",
      "title": "AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset",
      "authors": [
        "Soichiro Murakami",
        "Peinan Zhang",
        "Hidetaka Kamigaito",
        "Hiroya Takamura",
        "Manabu Okumura"
      ],
      "abstract": "Identifying factors that make ad text attractive is essential for advertising success. This study proposes AdParaphrase v2.0, a dataset for ad text paraphrasing, containing human preference data, to enable the analysis of the linguistic factors and to support the development of methods for generating attractive ad texts. Compared with v1.0, this dataset is 20 times larger, comprising 16,460 ad text paraphrase pairs, each annotated with preference data from ten evaluators, thereby enabling a more comprehensive and reliable analysis. Through the experiments, we identified multiple linguistic features of engaging ad texts that were not observed in v1.0 and explored various methods for generating attractive ad texts. Furthermore, our analysis demonstrated the relationships between human preference and ad performance, and highlighted the potential of reference-free metrics based on large language models for evaluating ad text attractiveness. The dataset is publicly available at: https://github.com/CyberAgentAILab/AdParaphrase-v2.0.",
      "arxiv_url": "https://arxiv.org/abs/2505.20826",
      "pdf_url": "https://arxiv.org/pdf/2505.20826",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "8fffd885f74136a9a71f8a739decc2f5fa147f26",
      "title": "BenNumEval: A Benchmark to Assess LLMs' Numerical Reasoning Capabilities in Bengali",
      "authors": [
        "Kawsar Ahmed",
        "Md Osama",
        "Omar Sharif",
        "E. Hossain",
        "M. M. Hoque"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/8fffd885f74136a9a71f8a739decc2f5fa147f26",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11817",
      "title": "AnalyticKWS: Towards Exemplar-Free Analytic Class Incremental Learning for Small-footprint Keyword Spotting",
      "authors": [
        "Yang Xiao",
        "Tianyi Peng",
        "Rohan Kumar Das",
        "Yuchen Hu",
        "Huiping Zhuang"
      ],
      "abstract": "Keyword spotting (KWS) offers a vital mechanism to identify spoken commands in voice-enabled systems, where user demands often shift, requiring models to learn new keywords continually over time. However, a major problem is catastrophic forgetting, where models lose their ability to recognize earlier keywords. Although several continual learning methods have proven their usefulness for reducing forgetting, most existing approaches depend on storing and revisiting old data to combat catastrophic forgetting. Though effective, these methods face two practical challenges: 1) privacy risks from keeping user data and 2) large memory and time consumption that limit deployment on small devices. To address these issues, we propose an exemplar-free Analytic Continual Learning (AnalyticKWS) method that updates model parameters without revisiting earlier data. Inspired by efficient learning principles, AnalyticKWS computes a closed-form analytical solution for model updates and requires only a single epoch of adaptation for incoming keywords. AnalyticKWS demands fewer computational resources by avoiding gradient-based updates and does not store old data. By eliminating the need for back-propagation during incremental learning, the model remains lightweight and efficient. As a result, AnalyticKWS meets the challenges mentioned earlier and suits resource-limited settings well. Extensive experiments on various datasets and settings show that AnalyticKWS consistently outperforms existing continual learning methods.",
      "arxiv_url": "https://arxiv.org/abs/2505.11817",
      "pdf_url": "https://arxiv.org/pdf/2505.11817",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.05874",
      "title": "VideoRAG: Retrieval-Augmented Generation over Video Corpus",
      "authors": [
        "Soyeong Jeong",
        "Kangsan Kim",
        "Jinheon Baek",
        "Sung Ju Hwang"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) is a powerful strategy for improving the factual accuracy of models by retrieving external knowledge relevant to queries and incorporating it into the generation process. However, existing approaches primarily focus on text, with some recent advancements considering images, and they largely overlook videos, a rich source of multimodal knowledge capable of representing contextual details more effectively than any other modality. While very recent studies explore the use of videos in response generation, they either predefine query-associated videos without retrieval or convert videos into textual descriptions losing multimodal richness. To tackle these, we introduce VideoRAG, a framework that not only dynamically retrieves videos based on their relevance with queries but also utilizes both visual and textual information. The operation of VideoRAG is powered by recent Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and the seamless integration of retrieved videos jointly with queries for response generation. Also, inspired by that the context size of LVLMs may not be sufficient to process all frames in extremely long videos and not all frames are equally important, we introduce a video frame selection mechanism to extract the most informative subset of frames, along with a strategy to extract textual information from videos (as it can aid the understanding of video content) when their subtitles are not available. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines. Code is available at https://github.com/starsuzi/VideoRAG.",
      "arxiv_url": "https://arxiv.org/abs/2501.05874",
      "pdf_url": "https://arxiv.org/pdf/2501.05874",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-01-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19510",
      "title": "LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study",
      "authors": [
        "Dongil Yang",
        "Minjin Kim",
        "Sunghwan Kim",
        "Beong-woo Kwak",
        "Minjun Park",
        "Jinseok Hong",
        "Woontack Woo",
        "Jinyoung Yeo"
      ],
      "abstract": "The remarkable reasoning and generalization capabilities of Large Language Models (LLMs) have paved the way for their expanding applications in embodied AI, robotics, and other real-world tasks. To effectively support these applications, grounding in spatial and temporal understanding in multimodal environments is essential. To this end, recent works have leveraged scene graphs, a structured representation that encodes entities, attributes, and their relationships in a scene. However, a comprehensive evaluation of LLMs' ability to utilize scene graphs remains limited. In this work, we introduce Text-Scene Graph (TSG) Bench, a benchmark designed to systematically assess LLMs' ability to (1) understand scene graphs and (2) generate them from textual narratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models perform well on scene graph understanding, they struggle with scene graph generation, particularly for complex narratives. Our analysis indicates that these models fail to effectively decompose discrete scenes from a complex narrative, leading to a bottleneck when generating scene graphs. These findings underscore the need for improved methodologies in scene graph generation and provide valuable insights for future research. The demonstration of our benchmark is available at https://tsg-bench.netlify.app. Additionally, our code and evaluation data are publicly available at https://github.com/docworlds/tsg-bench.",
      "arxiv_url": "https://arxiv.org/abs/2505.19510",
      "pdf_url": "https://arxiv.org/pdf/2505.19510",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03887",
      "title": "Pre3: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation",
      "authors": [
        "Junyi Chen",
        "Shihao Bai",
        "Zaijun Wang",
        "Siyu Wu",
        "Chuheng Du",
        "Hailong Yang",
        "Ruihao Gong",
        "Shengzhong Liu",
        "Fan Wu",
        "Guihai Chen"
      ],
      "abstract": "Extensive LLM applications demand efficient structured generations, particularly for LR(1) grammars, to produce outputs in specified formats (e.g., JSON). Existing methods primarily parse LR(1) grammars into a pushdown automaton (PDA), leading to runtime execution overhead for context-dependent token processing, especially inefficient under large inference batches. To address these issues, we propose Pre$^3$ that exploits deterministic pushdown automata (DPDA) to optimize the constrained LLM decoding efficiency. First, by precomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables ahead-of-time edge analysis and thus makes parallel transition processing possible. Second, by leveraging the prefix-conditioned edges, Pre$^3$ introduces a novel approach that transforms LR(1) transition graphs into DPDA, eliminating the need for runtime path exploration and achieving edge transitions with minimal overhead. Pre$^3$ can be seamlessly integrated into standard LLM inference frameworks, reducing time per output token (TPOT) by up to 40% and increasing throughput by up to 36% in our experiments. Our code is available at https://github.com/ModelTC/lightllm.",
      "arxiv_url": "https://arxiv.org/abs/2506.03887",
      "pdf_url": "https://arxiv.org/pdf/2506.03887",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "903545a646dda9a8ca040a4adb58783af483acfe",
      "title": "SQLGenie: A Practical LLM based System for Reliable and Efficient SQL Generation",
      "authors": [
        "Pushpendu Ghosh",
        "Aryan Jain",
        "Promod Yenigalla"
      ],
      "abstract": "Large Language Models (LLMs) enable natural language to SQL conversion, allowing users to query databases without SQL expertise. However, generating accurate, efficient queries is challenging due to ambiguous intent, domain knowledge requirements, and database constraints. Extensive reasoning improves SQL quality but increases computational costs and latency. We propose SQLGenie, a practical sys-tem for reliable SQL generation. It consists of three components: (1) Table Onboarder , which analyzes new tables, optimizes indexing, partitions data, identifies foreign key relationships, and stores schema details for SQL generation; (2) SQL Generator , an LLM-based system producing accurate SQL; and (3) Feedback Augmentation , which filters correct query-SQL pairs, leverages multiple LLM agents for complex SQL, and stores verified examples. SQLGenie achieves state-of-the-art performance on public benchmarks (92.8% execution accuracy on WikiSQL, 82.1% on Spider, 73.8% on BIRD) and internal datasets, surpassing the best single-LLM baseline by 21.5% and the strongest pipeline competitor by 5.3%. Its hybrid variant optimally balances accuracy and efficiency, reducing generation time by 64% compared to traditional multi-LLM approaches while maintaining competitive accuracy.",
      "arxiv_url": "https://www.semanticscholar.org/paper/903545a646dda9a8ca040a4adb58783af483acfe",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.15629",
      "title": "CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction",
      "authors": [
        "Harsh Maheshwari",
        "S. Tenneti",
        "Alwarappan Nakkiran"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) has emerged as a powerful application of Large Language Models (LLMs), revolutionizing information search and consumption. RAG systems combine traditional search capabilities with LLMs to generate comprehensive answers to user queries, ideally with accurate citations. However, in our experience of developing a RAG product, LLMs often struggle with source attribution, aligning with other industry studies reporting citation accuracy rates of only about 74% for popular generative search engines. To address this, we present efficient post-processing algorithms to improve citation accuracy in LLM-generated responses, with minimal impact on latency and cost. Our approaches cross-check generated citations against retrieved articles using methods including keyword + semantic matching, fine tuned model with BERTScore, and a lightweight LLM-based technique. Our experimental results demonstrate a relative improvement of 15.46% in the overall accuracy metrics of our RAG system. This significant enhancement potentially enables a shift from our current larger language model to a relatively smaller model that is approximately 12x more cost-effective and 3x faster in inference time, while maintaining comparable performance. This research contributes to enhancing the reliability and trustworthiness of AI-generated content in information retrieval and summarization tasks which is critical to gain customer trust especially in commercial products.",
      "arxiv_url": "https://arxiv.org/abs/2504.15629",
      "pdf_url": "https://arxiv.org/pdf/2504.15629",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-04-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07064",
      "title": "Com2: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models",
      "authors": [
        "Kai Xiong",
        "Xiao Ding",
        "Yixin Cao",
        "Yuxiong Yan",
        "Li Du",
        "Yufei Zhang",
        "Jin-Fang Gao",
        "Jiaqian Liu",
        "Bing Qin",
        "Ting Liu"
      ],
      "abstract": "Large language models (LLMs) have mastered abundant simple and explicit commonsense knowledge through pre-training, enabling them to achieve human-like performance in simple commonsense reasoning. Nevertheless, LLMs struggle to reason with complex and implicit commonsense knowledge that is derived from simple ones (such as understanding the long-term effects of certain events), an aspect humans tend to focus on more. Existing works focus on complex tasks like math and code, while complex commonsense reasoning remains underexplored due to its uncertainty and lack of structure. To fill this gap and align with real-world concerns, we propose a benchmark Com$^2$ focusing on complex commonsense reasoning. We first incorporate causal event graphs to serve as structured complex commonsense. Then we adopt causal theory~(e.g., intervention) to modify the causal event graphs and obtain different scenarios that meet human concerns. Finally, an LLM is employed to synthesize examples with slow thinking, which is guided by the logical relationships in the modified causal graphs. Furthermore, we use detective stories to construct a more challenging subset. Experiments show that LLMs struggle in reasoning depth and breadth, while post-training and slow thinking can alleviate this. The code and data are available at https://github.com/Waste-Wood/Com2.",
      "arxiv_url": "https://arxiv.org/abs/2506.07064",
      "pdf_url": "https://arxiv.org/pdf/2506.07064",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17823",
      "title": "A General Framework to Enhance Fine-tuning-based LLM Unlearning",
      "authors": [
        "Jie Ren",
        "Zhenwei Dai",
        "Xianfeng Tang",
        "Hui Liu",
        "Jingying Zeng",
        "Zhen Li",
        "R. Goutam",
        "Suhang Wang",
        "Yue Xing",
        "Qi He"
      ],
      "abstract": "Unlearning has been proposed to remove copyrighted and privacy-sensitive data from Large Language Models (LLMs). Existing approaches primarily rely on fine-tuning-based methods, which can be categorized into gradient ascent-based (GA-based) and suppression-based methods. However, they often degrade model utility (the ability to respond to normal prompts). In this work, we aim to develop a general framework that enhances the utility of fine-tuning-based unlearning methods. To achieve this goal, we first investigate the common property between GA-based and suppression-based methods. We unveil that GA-based methods unlearn by distinguishing the target data (i.e., the data to be removed) and suppressing related generations, which is essentially the same strategy employed by suppression-based methods. Inspired by this finding, we introduce Gated Representation UNlearning (GRUN) which has two components: a soft gate function for distinguishing target data and a suppression module using Representation Fine-tuning (ReFT) to adjust representations rather than model parameters. Experiments show that GRUN significantly improves the unlearning and utility. Meanwhile, it is general for fine-tuning-based methods, efficient and promising for sequential unlearning.",
      "arxiv_url": "https://arxiv.org/abs/2502.17823",
      "pdf_url": "https://arxiv.org/pdf/2502.17823",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.03460",
      "title": "Visualising Policy-Reward Interplay to Inform Zeroth-Order Preference Optimisation of Large Language Models",
      "authors": [
        "Alessio Galatolo",
        "Zhenbang Dai",
        "Katie Winkle",
        "Meriem Beloucif"
      ],
      "abstract": "Fine-tuning Large Language Models (LLMs) with first-order methods like back-propagation is computationally intensive. Zeroth-Order (ZO) optimisation uses function evaluations instead of gradients, reducing memory usage, but suffers from slow convergence in high-dimensional models. As a result, ZO research in LLMs has mostly focused on classification, overlooking more complex generative tasks. In this paper, we introduce ZOPrO, a novel ZO algorithm designed for Preference Optimisation in LLMs. We begin by analysing the interplay between policy and reward models during traditional (first-order) Preference Optimisation, uncovering patterns in their relative updates. Guided by these insights, we adapt Simultaneous Perturbation Stochastic Approximation (SPSA) with a targeted sampling strategy to accelerate convergence. Through experiments on summarisation, machine translation, and conversational assistants, we demonstrate that our method consistently enhances reward signals while achieving convergence times comparable to first-order methods. While it falls short of some state-of-the-art methods, our work is the first to apply Zeroth-Order methods to Preference Optimisation in LLMs, going beyond classification tasks and paving the way for a largely unexplored research direction. Code and visualisations are available at https://github.com/alessioGalatolo/VisZOPrO",
      "arxiv_url": "https://arxiv.org/abs/2503.03460",
      "pdf_url": "https://arxiv.org/pdf/2503.03460",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "90d7b874448631c25746c390fdc927ff9b059029",
      "title": "Synergistic Augmentation: Enhancing Cross-Domain Zero-Shot Slot Filling with Small Model-Assisted Large Language Models",
      "authors": [
        "Weizhen Li",
        "Junbao Huang",
        "Peijie Huang",
        "Yuhong Xu",
        "Jiekun Fan"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/90d7b874448631c25746c390fdc927ff9b059029",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16491",
      "title": "LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing",
      "authors": [
        "Dario Di Palma",
        "A. D. Bellis",
        "Giovanni Servedio",
        "V. W. Anelli",
        "F. Narducci",
        "T. D. Noia"
      ],
      "abstract": "Large Language Models (LLMs) have rapidly become central to NLP, demonstrating their ability to adapt to various tasks through prompting techniques, including sentiment analysis. However, we still have a limited understanding of how these models capture sentiment-related information. This study probes the hidden layers of Llama models to pinpoint where sentiment features are most represented and to assess how this affects sentiment analysis. Using probe classifiers, we analyze sentiment encoding across layers and scales, identifying the layers and pooling methods that best capture sentiment signals. Our results show that sentiment information is most concentrated in mid-layers for binary polarity tasks, with detection accuracy increasing up to 14% over prompting techniques. Additionally, we find that in decoder-only models, the last token is not consistently the most informative for sentiment encoding. Finally, this approach enables sentiment tasks to be performed with memory requirements reduced by an average of 57%. These insights contribute to a broader understanding of sentiment in LLMs, suggesting layer-specific probing as an effective approach for sentiment tasks beyond prompting, with potential to enhance model utility and reduce memory requirements.",
      "arxiv_url": "https://arxiv.org/abs/2505.16491",
      "pdf_url": "https://arxiv.org/pdf/2505.16491",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "90e9f0d3dbe194a62bef2309d5dfea95a8544dd6",
      "title": "The Million Authors Corpus: A Cross-Lingual and Cross-Domain Wikipedia Dataset for Authorship Verification",
      "authors": [
        "Abraham Israeli",
        "Shuai Liu",
        "Jonathan May",
        "David Jurgens"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/90e9f0d3dbe194a62bef2309d5dfea95a8544dd6",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22304",
      "title": "CADReview: Automatically Reviewing CAD Programs with Error Detection and Correction",
      "authors": [
        "Jiali Chen",
        "Xusen Hei",
        "Hongfei Liu",
        "Yuancheng Wei",
        "Zikun Deng",
        "Jiayuan Xie",
        "Yi Cai",
        "Qing Li"
      ],
      "abstract": "Computer-aided design (CAD) is crucial in prototyping 3D objects through geometric instructions (i.e., CAD programs). In practical design workflows, designers often engage in time-consuming reviews and refinements of these prototypes by comparing them with reference images. To bridge this gap, we introduce the CAD review task to automatically detect and correct potential errors, ensuring consistency between the constructed 3D objects and reference images. However, recent advanced multimodal large language models (MLLMs) struggle to recognize multiple geometric components and perform spatial geometric operations within the CAD program, leading to inaccurate reviews. In this paper, we propose the CAD program repairer (ReCAD) framework to effectively detect program errors and provide helpful feedback on error correction. Additionally, we create a dataset, CADReview, consisting of over 20K program-image pairs, with diverse errors for the CAD review task. Extensive experiments demonstrate that our ReCAD significantly outperforms existing MLLMs, which shows great potential in design applications.",
      "arxiv_url": "https://arxiv.org/abs/2505.22304",
      "pdf_url": "https://arxiv.org/pdf/2505.22304",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "913b8d6ebe467043c6f10c454d25b78f72c876dd",
      "title": "Coordinating Chaos: A Structured Review of Linguistic Coordination Methodologies",
      "authors": [
        "Benjamin Litterer",
        "David Jurgens",
        "Dallas Card"
      ],
      "abstract": "Linguistic coordination—a phenomenon where conversation partners end up having similar patterns of language use—has been established across a variety of contexts and for multiple linguistic features. However, the study of language coordination has been accompanied by a diverse and inconsistently applied set of measures and theoretical perspectives. This diversity has significant consequences, as replication studies have highlighted the brittleness of certain measures and called influential findings into question. While prior work has addressed specific modeling decisions and model types, linguistic coordination research has yet to fully examine, synthesize, and critique the space of modeling choices available. In this work, we present a framework to organize the linguistic coordination literature. Using this schema, we provide a high-level overview of the choices involved in the measurement process and synthesize relevant critiques. Based on both gaps and limitations surfaced from this review, we suggest directions for further exploration and evaluation. In doing so, we provide the clarity required for linguistic coordination research to arrive at interpretable and sound conclusions.",
      "arxiv_url": "https://www.semanticscholar.org/paper/913b8d6ebe467043c6f10c454d25b78f72c876dd",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.08767",
      "title": "SelfElicit: Your Language Model Secretly Knows Where is the Relevant Evidence",
      "authors": [
        "Zhining Liu",
        "Rana Ali Amjad",
        "Ravinarayana Adkathimar",
        "Tianxin Wei",
        "Hanghang Tong"
      ],
      "abstract": "Providing Language Models (LMs) with relevant evidence in the context (either via retrieval or user-provided) can significantly improve their ability to provide better-grounded responses. However, recent studies have found that LMs often struggle to fully comprehend and utilize key evidence from the context, especially when it contains noise and irrelevant information, an issue common in real-world scenarios. To address this, we propose SelfElicit, an inference-time approach that helps LMs focus on key contextual evidence through self-guided explicit highlighting. By leveraging the inherent evidence-finding capabilities of LMs using the attention scores of deeper layers, our method automatically identifies and emphasizes key evidence within the input context, facilitating more accurate and grounded responses without additional training or iterative prompting. We demonstrate that SelfElicit brings consistent and significant improvement on multiple evidence-based QA tasks for various LM families while maintaining computational efficiency. Our code and documentation are available at https://github.com/ZhiningLiu1998/SelfElicit.",
      "arxiv_url": "https://arxiv.org/abs/2502.08767",
      "pdf_url": "https://arxiv.org/pdf/2502.08767",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24369",
      "title": "Adversarial Preference Learning for Robust LLM Alignment",
      "authors": [
        "Yuanfu Wang",
        "Pengyu Wang",
        "Chenyang Xi",
        "Bo Tang",
        "Junyi Zhu",
        "Wenqiang Wei",
        "Chen Chen",
        "Chao Yang",
        "Jingfeng Zhang",
        "Chaochao Lu",
        "Yijun Niu",
        "Keming Mao",
        "Zhiyu Li",
        "Feiyu Xiong",
        "Jie Hu",
        "Mingchuan Yang"
      ],
      "abstract": "Modern language models often rely on Reinforcement Learning from Human Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to adversarial attacks due to three key limitations: (1) the inefficiency and high cost of human annotation, (2) the vast diversity of potential adversarial attacks, and (3) the risk of feedback bias and reward hacking. To address these challenges, we introduce Adversarial Preference Learning (APL), an iterative adversarial training method incorporating three key innovations. First, a direct harmfulness metric based on the model's intrinsic preference probabilities, eliminating reliance on external assessment. Second, a conditional generative attacker that synthesizes input-specific adversarial variations. Third, an iterative framework with automated closed-loop feedback, enabling continuous adaptation through vulnerability discovery and mitigation. Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly enhances robustness, achieving 83.33% harmlessness win rate over the base model (evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured by LLaMA-Guard), and lowering attack success rate by up to 65% according to HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against the base model.",
      "arxiv_url": "https://arxiv.org/abs/2505.24369",
      "pdf_url": "https://arxiv.org/pdf/2505.24369",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "91734989154e339b4914d29c300afa0852a6a614",
      "title": "MERIT: Multi-Agent Collaboration for Unsupervised Time Series Representation Learning",
      "authors": [
        "Shu Zhou",
        "Yunyang Xuan",
        "Yuxuan Ao",
        "Xin Wang",
        "Tao Fan",
        "Hao Wang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/91734989154e339b4914d29c300afa0852a6a614",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.05108",
      "title": "Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration",
      "authors": [
        "Yuyi Zhang",
        "Peirong Zhang",
        "Zhenhua Yang",
        "Pengyu Yan",
        "Yongxin Shi",
        "Pengwei Liu",
        "Fengjun Guo",
        "Lianwen Jin"
      ],
      "abstract": "Historical documents represent an invaluable cultural heritage, yet have undergone significant degradation over time through tears, water erosion, and oxidation. Existing Historical Document Restoration (HDR) methods primarily focus on single modality or limited-size restoration, failing to meet practical needs. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel automated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and 6,543 synthetic images with character-level and line-level locations, as well as character annotations in different damage grades. AutoHDR mimics historians'restoration workflows through a three-stage approach: OCR-assisted damage localization, vision-language context text prediction, and patch autoregressive appearance restoration. The modular architecture of AutoHDR enables seamless human-machine collaboration, allowing for flexible intervention and optimization at each restoration stage. Experiments demonstrate AutoHDR's remarkable performance in HDR. When processing severely damaged documents, our method improves OCR accuracy from 46.83% to 84.05%, with further enhancement to 94.25% through human-machine collaboration. We believe this work represents a significant advancement in automated historical document restoration and contributes substantially to cultural heritage preservation. The model and dataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.",
      "arxiv_url": "https://arxiv.org/abs/2507.05108",
      "pdf_url": "https://arxiv.org/pdf/2507.05108",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00455",
      "title": "PodAgent: A Comprehensive Framework for Podcast Generation",
      "authors": [
        "Yujia Xiao",
        "Lei He",
        "Haohan Guo",
        "Fenglong Xie",
        "Tan Lee"
      ],
      "abstract": "Existing Existing automatic audio generation methods struggle to generate podcast-like audio programs effectively. The key challenges lie in in-depth content generation, appropriate and expressive voice production. This paper proposed PodAgent, a comprehensive framework for creating audio programs. PodAgent 1) generates informative topic-discussion content by designing a Host-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for suitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis method to generate expressive conversational speech. Given the absence of standardized evaluation criteria for podcast-like audio generation, we developed comprehensive assessment guidelines to effectively evaluate the model's performance. Experimental results demonstrate PodAgent's effectiveness, significantly surpassing direct GPT-4 generation in topic-discussion dialogue content, achieving an 87.4% voice-matching accuracy, and producing more expressive speech through LLM-guided synthesis. Demo page: https://podcast-agent.github.io/demo/. Source code: https://github.com/yujxx/PodAgent.",
      "arxiv_url": "https://arxiv.org/abs/2503.00455",
      "pdf_url": "https://arxiv.org/pdf/2503.00455",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11051",
      "title": "MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models",
      "authors": [
        "Jiahao Huo",
        "Yibo Yan",
        "Xu Zheng",
        "Yuanhuiyi Lyu",
        "Xin Zou",
        "Zhihua Wei",
        "Xuming Hu"
      ],
      "abstract": "Recent progress in Machine Unlearning (MU) has introduced solutions for the selective removal of private or sensitive information encoded within deep neural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs) remains in its nascent phase. Therefore, we propose to reformulate the task of multimodal MU in the era of MLLMs, which aims to erase only the visual patterns associated with a given entity while preserving the corresponding textual knowledge encoded within the original parameters of the language model backbone. Furthermore, we develop a novel geometry-constrained gradient ascent method MMUnlearner. It updates the weights of MLLMs with a weight saliency map jointly restricted by the remaining concepts and textual knowledge during unlearning, thereby preserving parameters essential for non-target knowledge. Extensive experiments demonstrate that MMUnlearner surpasses baselines that finetuning MLLMs with VQA data directly through Gradient Ascent (GA) or Negative Preference Optimization (NPO), across all evaluation dimensions. Our code can be found in [this URL](https://github.com/Z1zs/MMUnlearner).",
      "arxiv_url": "https://arxiv.org/abs/2502.11051",
      "pdf_url": "https://arxiv.org/pdf/2502.11051",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16978",
      "title": "HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar Generation",
      "authors": [
        "Weizhi Tang",
        "Yixuan Li",
        "Chris Sypherd",
        "Elizabeth Polgreen",
        "Vaishak Belle"
      ],
      "abstract": "Grammar plays a critical role in natural language processing and text/code generation by enabling the definition of syntax, the creation of parsers, and guiding structured outputs. Although large language models (LLMs) demonstrate impressive capabilities across domains, their ability to infer and generate grammars has not yet been thoroughly explored. In this paper, we aim to study and improve the ability of LLMs for few-shot grammar generation, where grammars are inferred from sets of a small number of positive and negative examples and generated in Backus-Naur Form. To explore this, we introduced a novel dataset comprising 540 structured grammar generation challenges, devised 6 metrics, and evaluated 8 various LLMs against it. Our findings reveal that existing LLMs perform sub-optimally in grammar generation. To address this, we propose an LLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar generation. HyGenar achieves substantial improvements in both the syntactic and semantic correctness of generated grammars across LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2505.16978",
      "pdf_url": "https://arxiv.org/pdf/2505.16978",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.22232",
      "title": "Leveraging In-Context Learning for Political Bias Testing of LLMs",
      "authors": [
        "Patrick Haller",
        "Jannis Vamvas",
        "Rico Sennrich",
        "Lena A. Jäger"
      ],
      "abstract": "A growing body of work has been querying LLMs with political questions to evaluate their potential biases. However, this probing method has limited stability, making comparisons between models unreliable. In this paper, we argue that LLMs need more context. We propose a new probing task, Questionnaire Modeling (QM), that uses human survey data as in-context examples. We show that QM improves the stability of question-based bias evaluation, and demonstrate that it may be used to compare instruction-tuned models to their base versions. Experiments with LLMs of various sizes indicate that instruction tuning can indeed change the direction of bias. Furthermore, we observe a trend that larger models are able to leverage in-context examples more effectively, and generally exhibit smaller bias scores in QM. Data and code are publicly available.",
      "arxiv_url": "https://arxiv.org/abs/2506.22232",
      "pdf_url": "https://arxiv.org/pdf/2506.22232",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.20840",
      "title": "CodeTool: Enhancing Programmatic Tool Invocation of LLMs via Process Supervision",
      "authors": [
        "Yifei Lu",
        "F. Ye",
        "Jian Li",
        "Q. Gao",
        "Cheng Liu",
        "Haibo Luo",
        "Nan Du",
        "Xiaolong Li",
        "Feiliang Ren"
      ],
      "abstract": "Tool invocation significantly enhances the capabilities of Large Language Models (LLMs), yet challenges persist, particularly in complex task scenarios. Current methods, such as instruction-enhanced reasoning and supervised fine-tuning, often result in unnecessarily long reasoning paths and face difficulties in verifying the correctness of intermediate steps. In this paper, we propose CodeTool, a novel framework for stepwise code generation that improves LLM tool invocation by leveraging the concise and easily verifiable nature of code. CodeTool incorporates two distinct process rewards: the On-the-spot Reward, which provides immediate feedback on the accuracy of each tool invocation, and the Latent Reward, which assesses the contribution of each step toward overall task completion. By maximizing the cumulative reward of the On-the-spot and Latend Rewards at each step, LLMs are guided to follow efficient and accurate reasoning paths. Extensive experiments on StableToolBench and RestBench-TMDB demonstrate the superiority of CodeTool over existing approaches.",
      "arxiv_url": "https://arxiv.org/abs/2503.20840",
      "pdf_url": "https://arxiv.org/pdf/2503.20840",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2508.10039",
      "title": "Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries",
      "authors": [
        "Wenqiang Wang",
        "Yan Xiao",
        "Hao Lin",
        "Yangshijie Zhang",
        "Xiaochun Cao"
      ],
      "abstract": "Current multi-task adversarial text attacks rely on abundant access to shared internal features and numerous queries, often limited to a single task type. As a result, these attacks are less effective against practical scenarios involving black-box feedback APIs, limited queries, or multiple task types. To bridge this gap, we propose \\textbf{C}luster and \\textbf{E}nsemble \\textbf{M}ulti-task Text Adversarial \\textbf{A}ttack (\\textbf{CEMA}), an effective black-box attack that exploits the transferability of adversarial texts across different tasks. CEMA simplifies complex multi-task scenarios by using a \\textit{deep-level substitute model} trained in a \\textit{plug-and-play} manner for text classification, enabling attacks without mimicking the victim model. This approach requires only a few queries for training, converting multi-task attacks into classification attacks and allowing attacks across various tasks. CEMA generates multiple adversarial candidates using different text classification methods and selects the one that most effectively attacks substitute models. In experiments involving multi-task models with two, three, or six tasks--spanning classification, translation, summarization, and text-to-image generation--CEMA demonstrates significant attack success with as few as 100 queries. Furthermore, CEMA can target commercial APIs (e.g., Baidu and Google Translate), large language models (e.g., ChatGPT 4o), and image-generation models (e.g., Stable Diffusion V2), showcasing its versatility and effectiveness in real-world applications.",
      "arxiv_url": "https://arxiv.org/abs/2508.10039",
      "pdf_url": "https://arxiv.org/pdf/2508.10039",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-08-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "92a8687b4908f0ccdc8f45d267cb590605c784a7",
      "title": "Error Comparison Optimization for Large Language Models on Aspect-Based Sentiment Analysis",
      "authors": [
        "Qianlong Wang",
        "Keyang Ding",
        "Hengxin Gao",
        "Hui Wang",
        "Ruifeng Xu"
      ],
      "abstract": "Supervised fine-tuning (SFT) has enabled large language models (LLMs) to exhibit promising performance on various tasks. However, this fine-tuning process only compares current predictions and labels on each sample, yet fails to perceive and understand its error outputs from different degrees, which may potentially produce a large percentage of serious errors. This poses a problem for aspect-based sentiment analysis (ABSA), in that these serious errors bring a greater negative impact than slight ones. Humans tend to compare mistakes to understand the varying degrees of mistakes, thus avoiding major bad decisions. Inspired by this, we propose a simple yet effective framework, which could understand the degree of different errors by learning from comparative error pairs. It utilizes the SFT model to yield multiple outputs on each sample and selects slight and severe errors based on the acceptable scores. Together with the labels, we construct two comparative error pairs and exploit their calibration losses to optimize parameters. We conduct comprehensive experiments on ABSA datasets to demonstrate the effectiveness of our framework over baselines.",
      "arxiv_url": "https://www.semanticscholar.org/paper/92a8687b4908f0ccdc8f45d267cb590605c784a7",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "92aa593b7063256573762b33f5371f5567c82366",
      "title": "Meta-Tool: Unleash Open-World Function Calling Capabilities of General-Purpose Large Language Models",
      "authors": [
        "Shengqian Qin",
        "Yakun Zhu",
        "Linjie Mu",
        "Shaoting Zhang",
        "Xiaofan Zhang"
      ],
      "abstract": "Large language models (LLMs) have show-cased remarkable capabilities as autonomous agents when augmented with external tools. Equipped with fixed tool sets, LLMs struggle with addressing diverse user inquiries in open-world tasks. To evaluate and boost the performance of LLMs in dealing with complex demands in the real-world, we propose open-world function calling, where LLMs need to retrieve suitable tools from a pre-defined external tool library and use retrieved tools to resolve the user’s problem. We introduce Meta-Tool, a versatile and plug-and-play tool retrieval system as the access of LLMs to external tool library. Drawing inspiration from the myriad of enhanced approaches associated with Retrieval-Augmented Generation (RAG), Meta-Tool employs a hypothesize-retrieve-invoke framework. We further pro-pose Meta-Bench, a comprehensive benchmark for evaluating LLMs in open-world function calling and associated tasks. Meta-Bench encompasses 2 , 800 dialogues and 7 , 361 tools, spanning ten distinct scenarios to provide robust and diverse test categories. In conjunction, we present MT-LLaMA, a finetuned version of LLaMA-3.1, which exhibits remarkable performance improvements. Our empirical experiments reveal that Meta-Tool significantly enhances the ability of advanced LLMs to retrieve and leverage the most suitable tools compared to previous tool retrieval methods. Moreover, our fine-tuning enables even smaller-sized LLMs to achieve comparable even exceeding",
      "arxiv_url": "https://www.semanticscholar.org/paper/92aa593b7063256573762b33f5371f5567c82366",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM",
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17571",
      "title": "Towards Conditioning Clinical Text Generation for User Control",
      "authors": [
        "Osman Alperen Koras",
        "Rabi Bahnan",
        "J. Kleesiek",
        "Amin Dada"
      ],
      "abstract": "Deploying natural language generation systems in clinical settings remains challenging despite advances in Large Language Models (LLMs), which continue to exhibit hallucinations and factual inconsistencies, necessitating human oversight. This paper explores automated dataset augmentation using LLMs as human proxies to condition LLMs for clinician control without increasing cognitive workload. On the BioNLP ACL'24 Discharge Me! Shared Task, we achieve new state-of-the-art results with simpler methods than prior submissions through more efficient training, yielding a 9\\% relative improvement without augmented training and up to 34\\% with dataset augmentation. Preliminary human evaluation further supports the effectiveness of our approach, highlighting the potential of augmenting clinical text generation for control to enhance relevance, accuracy, and factual consistency.",
      "arxiv_url": "https://arxiv.org/abs/2502.17571",
      "pdf_url": "https://arxiv.org/pdf/2502.17571",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12439",
      "title": "Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games",
      "authors": [
        "Jinming Zhang",
        "Yunfei Long"
      ],
      "abstract": "Interactive Fiction games (IF games) are where players interact through natural language commands. While recent advances in Artificial Intelligence agents have reignited interest in IF games as a domain for studying decision-making, existing approaches prioritize task-specific performance metrics over human-like comprehension of narrative context and gameplay logic. This work presents a cognitively inspired framework that guides Large Language Models (LLMs) to learn and play IF games systematically. Our proposed **L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three key components: (1) structured map building to capture spatial and narrative relationships, (2) action learning to identify context-appropriate commands, and (3) feedback-driven experience analysis to refine decision-making over time. By aligning LLMs-based agents' behavior with narrative intent and commonsense constraints, LPLH moves beyond purely exploratory strategies to deliver more interpretable, human-like performance. Crucially, this approach draws on cognitive science principles to more closely simulate how human players read, interpret, and respond within narrative worlds. As a result, LPLH reframes the IF games challenge as a learning problem for LLMs-based agents, offering a new path toward robust, context-aware gameplay in complex text-based environments.",
      "arxiv_url": "https://arxiv.org/abs/2505.12439",
      "pdf_url": "https://arxiv.org/pdf/2505.12439",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12464",
      "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models",
      "authors": [
        "Seanie Lee",
        "Dong Bok Lee",
        "Dominik Wagner",
        "Minki Kang",
        "Haebin Seong",
        "Tobias Bocklet",
        "Juho Lee",
        "Sung Ju Hwang"
      ],
      "abstract": "Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, but they often underperform on\"hard\"examples where the larger model provides accurate predictions. We observe that many inputs can be reliably handled by the smaller model, while only a small fraction require the larger model's capacity. Motivated by this, we propose SafeRoute, a binary router that distinguishes hard examples from easy ones. Our method selectively applies the larger safety guard model to the data that the router considers hard, improving efficiency while maintaining accuracy compared to solely using the larger safety guard model. Experimental results on multiple benchmark datasets demonstrate that our adaptive model selection significantly enhances the trade-off between computational cost and safety performance, outperforming relevant baselines.",
      "arxiv_url": "https://arxiv.org/abs/2502.12464",
      "pdf_url": "https://arxiv.org/pdf/2502.12464",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.14154",
      "title": "SConU: Selective Conformal Uncertainty in Large Language Models",
      "authors": [
        "Zhiyuan Wang",
        "Qingni Wang",
        "Yue Zhang",
        "Tianlong Chen",
        "Xiaofeng Zhu",
        "Xiaoshuang Shi",
        "Kaidi Xu"
      ],
      "abstract": "As large language models are increasingly utilized in real-world applications, guarantees of task-specific metrics are essential for their reliable deployment. Previous studies have introduced various criteria of conformal uncertainty grounded in split conformal prediction, which offer user-specified correctness coverage. However, existing frameworks often fail to identify uncertainty data outliers that violate the exchangeability assumption, leading to unbounded miscoverage rates and unactionable prediction sets. In this paper, we propose a novel approach termed Selective Conformal Uncertainty (SConU), which, for the first time, implements significance tests, by developing two conformal p-values that are instrumental in determining whether a given sample deviates from the uncertainty distribution of the calibration set at a specific manageable risk level. Our approach not only facilitates rigorous management of miscoverage rates across both single-domain and interdisciplinary contexts, but also enhances the efficiency of predictions. Furthermore, we comprehensively analyze the components of the conformal procedures, aiming to approximate conditional coverage, particularly in high-stakes question-answering tasks.",
      "arxiv_url": "https://arxiv.org/abs/2504.14154",
      "pdf_url": "https://arxiv.org/pdf/2504.14154",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.15724",
      "title": "MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference",
      "authors": [
        "Kunxi Li",
        "Zhonghua Jiang",
        "Zhouzhou Shen",
        "Zhaode Wang",
        "Chengfei Lv",
        "Shengyu Zhang",
        "Fan Wu",
        "Fei Wu"
      ],
      "abstract": "This paper introduces MadaKV, a modality-adaptive key-value (KV) cache eviction strategy designed to enhance the efficiency of multimodal large language models (MLLMs) in long-context inference. In multimodal scenarios, attention heads exhibit varying preferences for different modalities, resulting in significant disparities in modality importance across attention heads. Traditional KV cache eviction methods, which are tailored for unimodal settings, fail to capture modality-specific information, thereby yielding suboptimal performance. MadaKV addresses these challenges through two key components: modality preference adaptation and hierarchical compression compensation. By dynamically sensing modality information within attention heads and adaptively retaining critical tokens, MadaKV achieves substantial reductions in KV cache memory footprint and model inference decoding latency (1.3 to 1.5 times improvement) while maintaining high accuracy across various multimodal long-context tasks. Extensive experiments on representative MLLMs and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to existing KV cache eviction methods.",
      "arxiv_url": "https://arxiv.org/abs/2506.15724",
      "pdf_url": "https://arxiv.org/pdf/2506.15724",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00264",
      "title": "MultiHoax: A Dataset of Multi-hop False-Premise Questions",
      "authors": [
        "Mohammadamin Shafiei",
        "Hamidreza Saffari",
        "N. Moosavi"
      ],
      "abstract": "As Large Language Models are increasingly deployed in high-stakes domains, their ability to detect false assumptions and reason critically is crucial for ensuring reliable outputs. False-premise questions (FPQs) serve as an important evaluation method by exposing cases where flawed assumptions lead to incorrect responses. While existing benchmarks focus on single-hop FPQs, real-world reasoning often requires multi-hop inference, where models must verify consistency across multiple reasoning steps rather than relying on surface-level cues. To address this gap, we introduce MultiHoax, a benchmark for evaluating LLMs' ability to handle false premises in complex, multi-step reasoning tasks. Our dataset spans seven countries and ten diverse knowledge categories, using Wikipedia as the primary knowledge source to enable factual reasoning across regions. Experiments reveal that state-of-the-art LLMs struggle to detect false premises across different countries, knowledge categories, and multi-hop reasoning types, highlighting the need for improved false premise detection and more robust multi-hop reasoning capabilities in LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.00264",
      "pdf_url": "https://arxiv.org/pdf/2506.00264",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18713",
      "title": "Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer",
      "authors": [
        "Guodong Du",
        "Zitao Fang",
        "Jing Li",
        "Junlin Li",
        "Runhua Jiang",
        "Shuyang Yu",
        "Yifei Guo",
        "Yangneng Chen",
        "Sim Kuan Goh",
        "Ho-Kin Tang",
        "Daojing He",
        "Honghai Liu",
        "Min Zhang"
      ],
      "abstract": "Foundation models and their checkpoints have significantly advanced deep learning, boosting performance across various applications. However, fine-tuned models often struggle outside their specific domains and exhibit considerable redundancy. Recent studies suggest that combining a pruned fine-tuned model with the original pre-trained model can mitigate forgetting, reduce interference when merging model parameters across tasks, and improve compression efficiency. In this context, developing an effective pruning strategy for fine-tuned models is crucial. Leveraging the advantages of the task vector mechanism, we preprocess fine-tuned models by calculating the differences between them and the original model. Recognizing that different task vector subspaces contribute variably to model performance, we introduce a novel method called Neural Parameter Search (NPS-Pruning) for slimming down fine-tuned models. This method enhances pruning efficiency by searching through neural parameters of task vectors within low-rank subspaces. Our method has three key applications: enhancing knowledge transfer through pairwise model interpolation, facilitating effective knowledge fusion via model merging, and enabling the deployment of compressed models that retain near-original performance while significantly reducing storage costs. Extensive experiments across vision, NLP, and multi-modal benchmarks demonstrate the effectiveness and robustness of our approach, resulting in substantial performance gains. The code is publicly available at: https://github.com/duguodong7/NPS-Pruning.",
      "arxiv_url": "https://arxiv.org/abs/2505.18713",
      "pdf_url": "https://arxiv.org/pdf/2505.18713",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05949",
      "title": "NameTag 3: A Tool and a Service for Multilingual/Multitagset NER",
      "authors": [
        "Jana Strakov'a",
        "Milan Straka"
      ],
      "abstract": "We introduce NameTag 3, an open-source tool and cloud-based web service for multilingual, multidataset, and multitagset named entity recognition (NER), supporting both flat and nested entities. NameTag 3 achieves state-of-the-art results on 21 test datasets in 15 languages and remains competitive on the rest, even against larger models. It is available as a command-line tool and as a cloud-based service, enabling use without local installation. NameTag 3 web service currently provides flat NER for 17 languages, trained on 21 corpora and three NE tagsets, all powered by a single 355M-parameter fine-tuned model; and nested NER for Czech, powered by a 126M fine-tuned model. The source code is licensed under open-source MPL 2.0, while the models are distributed under non-commercial CC BY-NC-SA 4.0. Documentation is available at https://ufal.mff.cuni.cz/nametag, source code at https://github.com/ufal/nametag3, and trained models via https://lindat.cz. The REST service and the web application can be found at https://lindat.mff.cuni.cz/services/nametag/. A demonstration video is available at https://www.youtube.com/watch?v=-gaGnP0IV8A.",
      "arxiv_url": "https://arxiv.org/abs/2506.05949",
      "pdf_url": "https://arxiv.org/pdf/2506.05949",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16710",
      "title": "Training Long-Context LLMs Efficiently via Chunk-wise Optimization",
      "authors": [
        "Wenhao Li",
        "Yuxin Zhang",
        "Gen Luo",
        "Daohai Yu",
        "Rongrong Ji"
      ],
      "abstract": "While long-context large language models (LLMs) exhibit remarkable document processing capabilities, their prohibitively high training costs often hinder customized applications. To mitigate this issue, we propose \\textit{Sequential Chunk-wise Optimization} (SeCO), a memory-efficient training paradigm that partitions lengthy inputs into manageable chunks. Each chunk independently constructs its computational graph and performs localized backpropagation, ensuring that only one chunk's forward activations are stored in memory. Building on SeCO, we further introduce \\textit{Sparse Chunk-wise Optimization} (SpaCO), which reduces computational overhead by selectively propagating gradients to specific chunks and incorporates a carefully designed compensation factor to ensure unbiased gradient estimation. SpaCO decouples the computational cost of backpropagation from the context length, enabling training time to gradually converge to inference time as sequences become longer. Implemented as lightweight training wrappers, both SeCO and SpaCO offer substantial practical benefits. For example, when fine-tuning an 8B model with LoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to 16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up to 3x faster than SeCO under the same experimental setup. These innovations provide new insights into optimizing long-context models, making them more accessible for practical applications. We have open-sourced the code at \\href{https://github.com/wenhaoli-xmu/seco}{here}.",
      "arxiv_url": "https://arxiv.org/abs/2505.16710",
      "pdf_url": "https://arxiv.org/pdf/2505.16710",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "93cfb4ccd42b7dde1b813d2e9b9adfe0876f64cb",
      "title": "Blinded by Context: Unveiling the Halo Effect of MLLM in AI Hiring",
      "authors": [
        "Kyusik Kim",
        "Jeongwoo Ryu",
        "Hyeonseok Jeon",
        "Bongwon Suh"
      ],
      "abstract": "This study investigates the halo effect in AI-driven hiring evaluations using Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Through experiments with hypothetical job applications, we examined how these models’ evaluations are influenced by non-job-related information, including extracurricular activities and social media images. By analyzing models’ responses to Likert-scale questions across different competency dimensions, we found that AI models exhibit significant halo effects, particularly in image-based evaluations, while text-based assessments showed more resistance to bias. The findings demonstrate that supplementary multi-modal information can substantially influence AI hiring decisions, highlighting potential risks in AI-based recruitment systems.",
      "arxiv_url": "https://www.semanticscholar.org/paper/93cfb4ccd42b7dde1b813d2e9b9adfe0876f64cb",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "93fcf1bf064455b4fc4d56333714df4df6600762",
      "title": "Word2Passage: Word-level Importance Re-weighting for Query Expansion",
      "authors": [
        "Jeonghwan Choi",
        "Minjeong Ban",
        "Minseok Kim",
        "Hwanjun Song"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/93fcf1bf064455b4fc4d56333714df4df6600762",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.02173",
      "title": "The Efficiency vs. Accuracy Trade-off: Optimizing RAG-Enhanced LLM Recommender Systems Using Multi-Head Early Exit",
      "authors": [
        "Huixue Zhou",
        "Hengrui Gu",
        "Xi Liu",
        "Kaixiong Zhou",
        "Mingfu Liang",
        "Yongkang Xiao",
        "Srinivas Govindan",
        "Piyush Chawla",
        "Jiyan Yang",
        "Xiangfei Meng",
        "Huayu Li",
        "Buyun Zhang",
        "Liang Luo",
        "Wen-Yen Chen",
        "Yiping Han",
        "Bo Long",
        "Rui Zhang",
        "Tianlong Chen"
      ],
      "abstract": "The deployment of Large Language Models (LLMs) in recommender systems for predicting Click-Through Rates (CTR) necessitates a delicate balance between computational efficiency and predictive accuracy. This paper presents an optimization framework that combines Retrieval-Augmented Generation (RAG) with an innovative multi-head early exit architecture to concurrently enhance both aspects. By integrating Graph Convolutional Networks (GCNs) as efficient retrieval mechanisms, we are able to significantly reduce data retrieval times while maintaining high model performance. The early exit strategy employed allows for dynamic termination of model inference, utilizing real-time predictive confidence assessments across multiple heads. This not only quickens the responsiveness of LLMs but also upholds or improves their accuracy, making it ideal for real-time application scenarios. Our experiments demonstrate how this architecture effectively decreases computation time without sacrificing the accuracy needed for reliable recommendation delivery, establishing a new standard for efficient, real-time LLM deployment in commercial systems.",
      "arxiv_url": "https://arxiv.org/abs/2501.02173",
      "pdf_url": "https://arxiv.org/pdf/2501.02173",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-01-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "948d331f5248b47c7c9ac9cc0e898b004e14a978",
      "title": "Open-World Attribute Mining for E-Commerce Products with Multimodal Self-Correction Instruction Tuning",
      "authors": [
        "Jiaqi Li",
        "Yanming Li",
        "Xiaoli Shen",
        "Chuanyi Zhang",
        "Guilin Qi",
        "Sheng Bi"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/948d331f5248b47c7c9ac9cc0e898b004e14a978",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2508.08550",
      "title": "Fine-grained Video Dubbing Duration Alignment with Segment Supervised Preference Optimization",
      "authors": [
        "Chaoqun Cui",
        "Liangbin Huang",
        "Shijing Wang",
        "Zhe Tong",
        "Zhaolong Huang",
        "Xiao Zeng",
        "Xiaofeng Liu"
      ],
      "abstract": "Video dubbing aims to translate original speech in visual media programs from the source language to the target language, relying on neural machine translation and text-to-speech technologies. Due to varying information densities across languages, target speech often mismatches the source speech duration, causing audio-video synchronization issues that significantly impact viewer experience. In this study, we approach duration alignment in LLM-based video dubbing machine translation as a preference optimization problem. We propose the Segment Supervised Preference Optimization (SSPO) method, which employs a segment-wise sampling strategy and fine-grained loss to mitigate duration mismatches between source and target lines. Experimental results demonstrate that SSPO achieves superior performance in duration alignment tasks.",
      "arxiv_url": "https://arxiv.org/abs/2508.08550",
      "pdf_url": "https://arxiv.org/pdf/2508.08550",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-08-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.06885",
      "title": "ProBench: Judging Multimodal Foundation Models on Open-ended Multi-domain Expert Tasks",
      "authors": [
        "Yan Yang",
        "Dongxu Li",
        "Haoning Wu",
        "Bei Chen",
        "Liu Liu",
        "Liyuan Pan",
        "Junnan Li"
      ],
      "abstract": "Solving expert-level multimodal tasks is a key milestone towards general intelligence. As the capabilities of multimodal large language models (MLLMs) continue to improve, evaluation of such advanced multimodal intelligence becomes necessary yet challenging. In this work, we introduce ProBench, a benchmark of open-ended user queries that require professional expertise and advanced reasoning. ProBench consists of 4,000 high-quality samples independently submitted by professionals based on their daily productivity demands. It spans across 10 fields and 56 sub-fields, including science, arts, humanities, coding, mathematics, and creative writing. Experimentally, we evaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal that although the best open-source models rival the proprietary ones, ProBench presents significant challenges in visual perception, textual understanding, domain knowledge and advanced reasoning, thus providing valuable directions for future multimodal AI research efforts.",
      "arxiv_url": "https://arxiv.org/abs/2503.06885",
      "pdf_url": "https://arxiv.org/pdf/2503.06885",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18673",
      "title": "Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models",
      "authors": [
        "Zixiang Xu",
        "Yanbo Wang",
        "Yue Huang",
        "Xiuying Chen",
        "Jieyu Zhao",
        "Meng Jiang",
        "Xiangliang Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable success in Natural Language Processing (NLP), yet their cross-lingual performance consistency remains a significant challenge. This paper introduces a novel methodology for efficiently identifying inherent cross-lingual weaknesses in LLMs. Our approach leverages beam search and LLM-based simulation to generate bilingual question pairs that expose performance discrepancies between English and target languages. We construct a new dataset of over 6,000 bilingual pairs across 16 languages using this methodology, demonstrating its effectiveness in revealing weaknesses even in state-of-the-art models. The extensive experiments demonstrate that our method precisely and cost-effectively pinpoints cross-lingual weaknesses, consistently revealing over 50\\% accuracy drops in target languages across a wide range of models. Moreover, further experiments investigate the relationship between linguistic similarity and cross-lingual weaknesses, revealing that linguistically related languages share similar performance patterns and benefit from targeted post-training. Code is available at https://github.com/xzx34/Cross-Lingual-Pitfalls.",
      "arxiv_url": "https://arxiv.org/abs/2505.18673",
      "pdf_url": "https://arxiv.org/pdf/2505.18673",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18316",
      "title": "WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging",
      "authors": [
        "Ahmed Elhady",
        "Eneko Agirre",
        "Mikel Artetxe"
      ],
      "abstract": "We introduce WiCkeD, a simple method to increase the complexity of existing multiple-choice benchmarks by randomly replacing a choice with\"None of the above\", a method often used in educational tests. We show that WiCkeD can be automatically applied to any existing benchmark, making it more challenging. We apply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight LLMs. The performance of the models drops 12.1 points on average with respect to the original versions of the datasets. When using chain-of-thought on 3 MMLU datasets, the performance drop for the WiCkeD variant is similar to the one observed when using the LLMs directly, showing that WiCkeD is also challenging for models with enhanced reasoning abilities. WiCkeD also uncovers that some models are more sensitive to the extra reasoning required, providing additional information with respect to the original benchmarks. We relase our code and data at https://github.com/ahmedselhady/wicked-benchmarks.",
      "arxiv_url": "https://arxiv.org/abs/2502.18316",
      "pdf_url": "https://arxiv.org/pdf/2502.18316",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24613",
      "title": "When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue Generation",
      "authors": [
        "Daniela Occhipinti",
        "Marco Guerini",
        "Malvina Nissim"
      ],
      "abstract": "Endowing dialogue agents with persona information has proven to significantly improve the consistency and diversity of their generations. While much focus has been placed on aligning dialogues with provided personas, the adaptation to the interlocutor's profile remains largely underexplored. In this work, we investigate three key aspects: (1) a model's ability to align responses with both the provided persona and the interlocutor's; (2) its robustness when dealing with familiar versus unfamiliar interlocutors and topics, and (3) the impact of additional fine-tuning on specific persona-based dialogues. We evaluate dialogues generated with diverse speaker pairings and topics, framing the evaluation as an author identification task and employing both LLM-as-a-judge and human evaluations. By systematically masking or disclosing information about the interlocutor, we assess its impact on dialogue generation. Results show that access to the interlocutor's persona improves the recognition of the target speaker, while masking it does the opposite. Although models generalise well across topics, they struggle with unfamiliar interlocutors. Finally, we found that in zero-shot settings, LLMs often copy biographical details, facilitating identification but trivialising the task.",
      "arxiv_url": "https://arxiv.org/abs/2505.24613",
      "pdf_url": "https://arxiv.org/pdf/2505.24613",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19726",
      "title": "Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training",
      "authors": [
        "Toan Tran",
        "Ruixuan Liu",
        "Li Xiong"
      ],
      "abstract": "Large language models (LLMs) have become the backbone of modern natural language processing but pose privacy concerns about leaking sensitive training data. Membership inference attacks (MIAs), which aim to infer whether a sample is included in a model's training dataset, can serve as a foundation for broader privacy threats. Existing defenses designed for traditional classification models do not account for the sequential nature of text data. As a result, they either require significant computational resources or fail to effectively mitigate privacy risks in LLMs. In this work, we propose \\methodname, a lightweight yet effective empirical privacy defense for protecting training data of language models by leveraging token-specific characteristics. By analyzing token dynamics during training, we propose a token selection strategy that categorizes tokens into hard tokens for learning and memorized tokens for unlearning. Subsequently, our training-phase defense optimizes a novel dual-purpose token-level loss to achieve a Pareto-optimal balance between utility and privacy. Extensive experiments demonstrate that our approach not only provides strong protection against MIAs but also improves language modeling performance by around 10\\% across various LLM architectures and datasets compared to the baselines.",
      "arxiv_url": "https://arxiv.org/abs/2502.19726",
      "pdf_url": "https://arxiv.org/pdf/2502.19726",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "95040eb3ac8b28795e571bb0a98cef7822a9bdf4",
      "title": "Demystifying Small Language Models for Edge Deployment",
      "authors": [
        "Zhenyan Lu",
        "Xiang Li",
        "Dongqi Cai",
        "Rongjie Yi",
        "Fangming Liu",
        "Wei Liu",
        "Jian Luan",
        "Xiwen Zhang",
        "N. Lane",
        "Mengwei Xu"
      ],
      "abstract": ".",
      "arxiv_url": "https://www.semanticscholar.org/paper/95040eb3ac8b28795e571bb0a98cef7822a9bdf4",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24174",
      "title": "Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation",
      "authors": [
        "Ryota Miyano",
        "Yuki Arase"
      ],
      "abstract": "This study proposes a simple yet effective LoRA merge method to achieve LLM adaptation for low-resource language generation tasks. The LoRA merge technique, which integrates multiple LoRA modules trained on different tasks, has gained attention as an effective and efficient approach for adapting LLMs to target tasks. However, previous methods are limited in adaptability as they keep the LoRA parameters frozen. Additionally, the low-resource problem has been out of their scope. We propose a LoRA merge method that updates and prunes LoRA parameters through fine-tuning with minimal target task data, which allows finer-grained adjustments of LoRA parameters and enhancement of task adaptability. Extensive experiments have been conducted taking summarization as a benchmark task. Our datasets cover various domains and multiple languages of English and Japanese. The results confirm that the proposed method achieves significant and consistent improvements in task adaptability over the previous methods.",
      "arxiv_url": "https://arxiv.org/abs/2505.24174",
      "pdf_url": "https://arxiv.org/pdf/2505.24174",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.15471",
      "title": "CoLA: Collaborative Low-Rank Adaptation",
      "authors": [
        "Yiyun Zhou",
        "Chang Yao",
        "Jingyuan Chen"
      ],
      "abstract": "The scaling law of Large Language Models (LLMs) reveals a power-law relationship, showing diminishing return on performance as model scale increases. While training LLMs from scratch is resource-intensive, fine-tuning a pre-trained model for specific tasks has become a practical alternative. Full fine-tuning (FFT) achieves strong performance; however, it is computationally expensive and inefficient. Parameter-efficient fine-tuning (PEFT) methods, like LoRA, have been proposed to address these challenges by freezing the pre-trained model and adding lightweight task-specific modules. LoRA, in particular, has proven effective, but its application to multi-task scenarios is limited by interference between tasks. Recent approaches, such as Mixture-of-Experts (MOE) and asymmetric LoRA, have aimed to mitigate these issues but still struggle with sample scarcity and noise interference due to their fixed structure. In response, we propose CoLA, a more flexible LoRA architecture with an efficient initialization scheme, and introduces three collaborative strategies to enhance performance by better utilizing the quantitative relationships between matrices $A$ and $B$. Our experiments demonstrate the effectiveness and robustness of CoLA, outperforming existing PEFT methods, especially in low-sample scenarios. Our data and code are fully publicly available at https://github.com/zyy-2001/CoLA.",
      "arxiv_url": "https://arxiv.org/abs/2505.15471",
      "pdf_url": "https://arxiv.org/pdf/2505.15471",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.04066",
      "title": "LLAMAPIE: Proactive In-Ear Conversation Assistants",
      "authors": [
        "Tuochao Chen",
        "Nicholas Batchelder",
        "Alisa Liu",
        "Noah A. Smith",
        "Shyamnath Gollakota"
      ],
      "abstract": "We introduce LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. We address several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, we construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. We evaluate our approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with our assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive model, highlighting the potential of LlamaPie to enhance live conversations.",
      "arxiv_url": "https://arxiv.org/abs/2505.04066",
      "pdf_url": "https://arxiv.org/pdf/2505.04066",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2508.06570",
      "title": "ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos",
      "authors": [
        "Mohammad Zia Ur Rehman",
        "Anukriti Bhatnagar",
        "Omkar Kabde",
        "Shubhi Bansal",
        "Nagendra Kumar"
      ],
      "abstract": "The existing research has primarily focused on text and image-based hate speech detection, video-based approaches remain underexplored. In this work, we introduce a novel dataset, ImpliHateVid, specifically curated for implicit hate speech detection in videos. ImpliHateVid consists of 2,009 videos comprising 509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos, making it one of the first large-scale video datasets dedicated to implicit hate detection. We also propose a novel two-stage contrastive learning framework for hate speech detection in videos. In the first stage, we train modality-specific encoders for audio, text, and image using contrastive loss by concatenating features from the three encoders. In the second stage, we train cross-encoders using contrastive learning to refine multimodal representations. Additionally, we incorporate sentiment, emotion, and caption-based features to enhance implicit hate detection. We evaluate our method on two datasets, ImpliHateVid for implicit hate speech detection and another dataset for general hate speech detection in videos, HateMM dataset, demonstrating the effectiveness of the proposed multimodal contrastive learning for hateful content detection in videos and the significance of our dataset.",
      "arxiv_url": "https://arxiv.org/abs/2508.06570",
      "pdf_url": "https://arxiv.org/pdf/2508.06570",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-08-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14846",
      "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation",
      "authors": [
        "Yue Yang",
        "Ajay Patel",
        "Matt Deitke",
        "Tanmay Gupta",
        "Luca Weihs",
        "Andrew Head",
        "Mark Yatskar",
        "Christopher Callison-Burch",
        "Ranjay Krishna",
        "Aniruddha Kembhavi",
        "Christopher Clark"
      ],
      "abstract": "Reasoning about images with rich text, such as charts and documents, is a critical application of vision-language models (VLMs). However, VLMs often struggle in these domains due to the scarcity of diverse text-rich vision-language data. To address this challenge, we present CoSyn, a framework that leverages the coding capabilities of text-only large language models (LLMs) to automatically create synthetic text-rich multimodal data. Given input text describing a target domain (e.g.,\"nutrition fact labels\"), CoSyn prompts an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic images. With the underlying code as textual representations of the synthetic images, CoSyn can generate high-quality instruction-tuning data, again relying on a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K images and 2.7M rows of vision-language instruction-tuning data. Comprehensive experiments on seven benchmarks demonstrate that models trained on our synthetic data achieve state-of-the-art performance among competitive open-source models, including Llama 3.2, and surpass proprietary models such as GPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing data, enabling VLMs to ground information within input images, showcasing its potential for developing multimodal agents capable of acting in real-world environments.",
      "arxiv_url": "https://arxiv.org/abs/2502.14846",
      "pdf_url": "https://arxiv.org/pdf/2502.14846",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "958f00ad0d4781320d5690bac19e80cdb1c73874",
      "title": "Pretraining Context Compressor for Large Language Models with Embedding-Based Memory",
      "authors": [
        "Yuhong Dai",
        "Jianxun Lian",
        "Yitian Huang",
        "Wei Zhang",
        "Mingyang Zhou",
        "Mingqi Wu",
        "Xing Xie",
        "Hao Liao"
      ],
      "abstract": "Efficient processing of long contexts in large language models (LLMs) is essential for real-world applications like retrieval-augmented generation and in-context learning, especially in resource-constrained environments such as edge computing. This paper explores the embedding-based context compression to reduce inference costs while preserving the down-stream LLM configurations. We propose a decoupled compressor-LLM framework, pre-trained on text reconstruction and completion tasks, designed to effectively preserve essential contextual information within condensed embedding representations. Our extensive experiments investigate pretraining, model configurations, compression rates, efficiency across tasks, and adaptability to various LLMs. Re-sults demonstrate that our approach outperforms competitive baselines in three domains and across eight datasets while being adaptable to different downstream LLMs. We find that thorough pretraining and carefully selected compression rates, such as 4x and 16x, enable a lightweight compressor to achieve a good balance between accuracy and speed. These findings underscore the potential of embedding-based compression to enhance LLM efficiency and motivate further research in this area.",
      "arxiv_url": "https://www.semanticscholar.org/paper/958f00ad0d4781320d5690bac19e80cdb1c73874",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM",
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "95d3ed2a4f20a41381bea4327ed9e26bbf8ff758",
      "title": "Boosting Long-Context Information Seeking via Query-Guided Activation Refilling",
      "authors": [
        "Hongjin Qian",
        "Zheng Liu",
        "Peitian Zhang",
        "Zhicheng Dou",
        "Defu Lian"
      ],
      "abstract": "Processing long contexts poses a significant challenge for large language models (LLMs) due to their inherent context-window limitations and the computational burden of extensive key-value (KV) activations, which severely impact efficiency. For information-seeking tasks, full context perception is often unnecessary, as a query’s information needs can dynamically range from localized details to a global perspective, depending on its complexity. However, existing methods struggle to adapt effectively to these dynamic information needs. In the paper, we propose a method for processing long-context information-seeking tasks via query-guided AC tivation RE filling (ACRE). ACRE constructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache compactly captures global information, and the layer-2 (L2) cache provides detailed and localized information. ACRE establishes a proxying relationship between the two caches, allowing the input query to attend to the L1 cache and dynamically refill it with relevant entries from the L2 cache. This mechanism integrates global understanding with query-specific local details, thus improving answer decoding. Experiments on a variety of long-context information-seeking datasets demonstrate ACRE’s effectiveness, achieving improvements in both performance and efficiency. We will release our source codes in this repository .",
      "arxiv_url": "https://www.semanticscholar.org/paper/95d3ed2a4f20a41381bea4327ed9e26bbf8ff758",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.12432",
      "title": "Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation",
      "authors": [
        "Dongsheng Zhu",
        "Weixian Shi",
        "Zhengliang Shi",
        "Zhaochun Ren",
        "Shuaiqiang Wang",
        "Lingyong Yan",
        "Dawei Yin"
      ],
      "abstract": "Although current Large Language Models (LLMs) exhibit impressive capabilities, performing complex real-world tasks still requires tool learning. Mainstream methods, such as CoT/ReAct, rely on step-by-step tool invocation to interact with external environments, but they are limited in perceptual scope and lack adequate task-planning capability. To address these limitations, other studies introduce the first Search-based Decision Tree (DFSDT), which still suffers from the high computational cost. In this paper, we introduce a novel parallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama). First, we transform traditional tree-based tool search paths into Directed Acyclic Graph (DAG) structure, generating a high-quality parallel tool invocation dataset. The DTA-Llama is then trained on the dataset to learn to iteratively divide the current task into several parallel tool invocation sub-tasks and aggregate the invocation results to decide the next actions. Furthermore, we introduce an efficient inference framework inspired by the Process/Threads mechanism when applying the DTA-Llama to practical tasks. Experimental results show that our approach substantially enhances task performance while reducing token consumption and inference time. Llama2-7B, using our method, is comparable to the official parallel function calling method of GPT-3.5. The relevant code, dataset, and model weights are available at https://corn0205.github.io/",
      "arxiv_url": "https://arxiv.org/abs/2501.12432",
      "pdf_url": "https://arxiv.org/pdf/2501.12432",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-01-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04534",
      "title": "Is It JUST Semantics? A Case Study of Discourse Particle Understanding in LLMs",
      "authors": [
        "William Sheffield",
        "Kanishka Misra",
        "Valentina Pyatkin",
        "Ashwini Deo",
        "Kyle Mahowald",
        "Junyi Jessy Li"
      ],
      "abstract": "Discourse particles are crucial elements that subtly shape the meaning of text. These words, often polyfunctional, give rise to nuanced and often quite disparate semantic/discourse effects, as exemplified by the diverse uses of the particle\"just\"(e.g., exclusive, temporal, emphatic). This work investigates the capacity of LLMs to distinguish the fine-grained senses of English\"just\", a well-studied example in formal semantics, using data meticulously created and labeled by expert linguists. Our findings reveal that while LLMs exhibit some ability to differentiate between broader categories, they struggle to fully capture more subtle nuances, highlighting a gap in their understanding of discourse particles.",
      "arxiv_url": "https://arxiv.org/abs/2506.04534",
      "pdf_url": "https://arxiv.org/pdf/2506.04534",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00842",
      "title": "Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience",
      "authors": [
        "Jiawei Gu",
        "Ziting Xian",
        "Yuanzhen Xie",
        "Ye Liu",
        "Enjie Liu",
        "Ruichao Zhong",
        "Mochi Gao",
        "Yunzhi Tan",
        "Bo Hu",
        "Zang Li"
      ],
      "abstract": "Large language models (LLMs) achieve strong performance on plain text tasks but underperform on structured data like tables and databases. Potential challenges arise from their underexposure during pre-training and rigid text-to-structure transfer mechanisms. Unlike humans who seamlessly apply learned patterns across data modalities, LLMs struggle to infer implicit relationships embedded in tabular formats, especially in the absence of explicit structural guidance. To bridge this cognitive gap, we introduce Contrastive Retrieval-Augmented Generation on Experience (CoRE), a framework that builds experience memory representations and enhances generalization through contrastive In-Context Learning (ICL) to simulate human-like knowledge transfer. Experiments on Text-to-SQL and TableQA show CoRE significantly improves performance, achieving average gains of 3.44% and 4.24%, with up to 17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated Experience Memory expands training data 8-9x, enhancing diversity and domain coverage. This training-free and continual method propels LLMs toward structured knowledge expertise.",
      "arxiv_url": "https://arxiv.org/abs/2506.00842",
      "pdf_url": "https://arxiv.org/pdf/2506.00842",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11183",
      "title": "Don't Get Lost in the Trees: Streamlining LLM Reasoning by Overcoming Tree Search Exploration Pitfalls",
      "authors": [
        "Ante Wang",
        "Linfeng Song",
        "Ye Tian",
        "Dian Yu",
        "Haitao Mi",
        "Xiangyu Duan",
        "Zhaopeng Tu",
        "Jinsong Su",
        "Dong Yu"
      ],
      "abstract": "Recent advancements in tree search algorithms guided by verifiers have significantly enhanced the reasoning capabilities of large language models (LLMs), but at the cost of increased computational resources. In this work, we identify two key challenges contributing to this inefficiency: $\\textit{over-exploration}$ due to redundant states with semantically equivalent content, and $\\textit{under-exploration}$ caused by high variance in verifier scoring leading to frequent trajectory switching. To address these issues, we propose FETCH, an e$\\textbf{f}$fici$\\textbf{e}$nt $\\textbf{t}$ree sear$\\textbf{ch}$ framework, which is a flexible, plug-and-play system compatible with various tree search algorithms. Our framework mitigates over-exploration by merging semantically similar states using agglomerative clustering of text embeddings obtained from a fine-tuned SimCSE model. To tackle under-exploration, we enhance verifiers by incorporating temporal difference learning with adjusted $\\lambda$-returns during training to reduce variance, and employing a verifier ensemble to aggregate scores during inference. Experiments on GSM8K, GSM-Plus, and MATH datasets demonstrate that our methods significantly improve reasoning accuracy and computational efficiency across four different tree search algorithms, paving the way for more practical applications of LLM-based reasoning. The code is available at https://github.com/Soistesimmer/Fetch.",
      "arxiv_url": "https://arxiv.org/abs/2502.11183",
      "pdf_url": "https://arxiv.org/pdf/2502.11183",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.18063",
      "title": "Dynamic Task Vector Grouping for Efficient Multi-Task Prompt Tuning",
      "authors": [
        "Pieyi Zhang",
        "Richong Zhang",
        "Zhijie Nie"
      ],
      "abstract": "Multi-task prompt tuning utilizes multiple high-resource source tasks to improve performance on low-source target tasks. Existing approaches transfer the soft prompt trained by combining all source tasks or a single ``high-similar'' source task one-time-only. However, we find that the optimal transfer performance often comes from a combination of source tasks, which is neither one nor all. Further, we find that the similarity between source and target tasks also changes dynamically during fine-tuning after transfering, making similarity calculation in the initiation stage inadequate. To address these issues, we propose a method called Dynamic Task Vector Grouping (DTVG), whose core ideas contain (1) measuring the task similarity with task vectors instead of soft prompt, (2) grouping the optimal source task combination based on two metrics: {\\it target similarity} and {\\it knowledge consistency}; (3) dynamically updating the combination in each iteration step. Extensive experiments on the 26 NLP datasets under different settings demonstrate that DTVG effectively groups similar source tasks while reducing negative transfer, achieving the start-of-art performance.",
      "arxiv_url": "https://arxiv.org/abs/2503.18063",
      "pdf_url": "https://arxiv.org/pdf/2503.18063",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.17509",
      "title": "Follow-up Question Generation For Enhanced Patient-Provider Conversations",
      "authors": [
        "Joseph Gatto",
        "Parker Seegmiller",
        "Timothy Burdick",
        "Inas S. Khayal",
        "Sarah DeLozier",
        "S. Preum"
      ],
      "abstract": "Follow-up question generation is an essential feature of dialogue systems as it can reduce conversational ambiguity and enhance modeling complex interactions. Conversational contexts often pose core NLP challenges such as (i) extracting relevant information buried in fragmented data sources, and (ii) modeling parallel thought processes. These two challenges occur frequently in medical dialogue as a doctor asks questions based not only on patient utterances but also their prior EHR data and current diagnostic hypotheses. Asking medical questions in asynchronous conversations compounds these issues as doctors can only rely on static EHR information to motivate follow-up questions. To address these challenges, we introduce FollowupQ, a novel framework for enhancing asynchronous medical conversation. FollowupQ is a multi-agent framework that processes patient messages and EHR data to generate personalized follow-up questions, clarifying patient-reported medical conditions. FollowupQ reduces requisite provider follow-up communications by 34%. It also improves performance by 17% and 5% on real and synthetic data, respectively. We also release the first public dataset of asynchronous medical messages with linked EHR data alongside 2,300 follow-up questions written by clinical experts for the wider NLP research community.",
      "arxiv_url": "https://arxiv.org/abs/2503.17509",
      "pdf_url": "https://arxiv.org/pdf/2503.17509",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "Personalization"
      ],
      "published_date": "2025-03-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.03835",
      "title": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification",
      "authors": [
        "Yindu Su",
        "Huike Zou",
        "Ling Sun",
        "Ting Zhang",
        "Haiyang Yang",
        "Liyu Chen",
        "David Lo",
        "Qingheng Zhang",
        "Shuguang Han",
        "Jufeng Chen"
      ],
      "abstract": "Product Attribute Value Identification (PAVI) involves identifying attribute values from product profiles, a key task for improving product search, recommendation, and business analytics on e-commerce platforms. However, existing PAVI methods face critical challenges, such as inferring implicit values, handling out-of-distribution (OOD) values, and producing normalized outputs. To address these limitations, we introduce Taxonomy-Aware Contrastive Learning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR formulates PAVI as an information retrieval task by encoding product profiles and candidate values into embeddings and retrieving values based on their similarity. It leverages contrastive training with taxonomy-aware hard negative sampling and employs adaptive inference with dynamic thresholds. TACLR offers three key advantages: (1) it effectively handles implicit and OOD values while producing normalized outputs; (2) it scales to thousands of categories, tens of thousands of attributes, and millions of values; and (3) it supports efficient inference for high-load industrial deployment. Extensive experiments on proprietary and public datasets validate the effectiveness and efficiency of TACLR. Further, it has been successfully deployed on the real-world e-commerce platform Xianyu, processing millions of product listings daily with frequently updated, large-scale attribute taxonomies. We release the code to facilitate reproducibility and future research at https://github.com/SuYindu/TACLR.",
      "arxiv_url": "https://arxiv.org/abs/2501.03835",
      "pdf_url": "https://arxiv.org/pdf/2501.03835",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00964",
      "title": "ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness",
      "authors": [
        "Dren Fazlija",
        "Arkadij Orlov",
        "Sandipan Sikdar"
      ],
      "abstract": "Large language models (LLMs) are increasingly becoming valuable to corporate data management due to their ability to process text from various document formats and facilitate user interactions through natural language queries. However, LLMs must consider the sensitivity of information when communicating with employees, especially given access restrictions. Simple filtering based on user clearance levels can pose both performance and privacy challenges. To address this, we propose the concept of sensitivity awareness (SA), which enables LLMs to adhere to predefined access rights rules. In addition, we developed a benchmarking environment called ACCESS DENIED INC to evaluate SA. Our experimental findings reveal significant variations in model behavior, particularly in managing unauthorized data requests while effectively addressing legitimate queries. This work establishes a foundation for benchmarking sensitivity-aware language models and provides insights to enhance privacy-centric AI systems in corporate environments.",
      "arxiv_url": "https://arxiv.org/abs/2506.00964",
      "pdf_url": "https://arxiv.org/pdf/2506.00964",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00319",
      "title": "SkillVerse : Assessing and Enhancing LLMs with Tree Evaluation",
      "authors": [
        "Yufei Tian",
        "Jiao Sun",
        "Nanyun Peng",
        "Zizhao Zhang"
      ],
      "abstract": "As language models evolve to tackle complex, multifaceted tasks, their evaluation must adapt to capture this intricacy. A granular, skill-specific understanding of model capabilities can empower researchers to make informed model development plans. In this paper, we introduce SkillVerse, an unsupervised tree-structured diagnosis framework for understanding model proficiency in specific abilities. With LLM as a judge, SkillVerse first critiques the model responses, and then organizes them into a hierarchical structure termed dendrogram. Given proficiency at arbitrary levels of granularity, SkillVerse is flexible to produce insights of behaviors of modern large models. We also demonstrate its efficacy in two downstream tasks: 1) improving model in-context learning by 25% using a tree-search algorithm to select more informative few-shot demonstrations, and 2) accurately predicting new model weaknesses with a 55% success rate, 22% higher than without SkillVerse.",
      "arxiv_url": "https://arxiv.org/abs/2506.00319",
      "pdf_url": "https://arxiv.org/pdf/2506.00319",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "974142bd4496caf1b725bedb81c87b5a8c6f94eb",
      "title": "Enhancing Complex Reasoning in Knowledge Graph Question Answering through Query Graph Approximation",
      "authors": [
        "Hongjun Jeong",
        "Minji Kim",
        "Heesoo Jung",
        "Ko Keun Kim",
        "Hogun Park"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/974142bd4496caf1b725bedb81c87b5a8c6f94eb",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.08662",
      "title": "RoToR: Towards More Reliable Responses for Order-Invariant Inputs",
      "authors": [
        "Soyoung Yoon",
        "Dongha Ahn",
        "Youngwon Lee",
        "Minkyu Jung",
        "HyungJoo Jang",
        "Seung-won Hwang"
      ],
      "abstract": "Mitigating positional bias of language models (LMs) for listwise inputs is a well-known and important problem (e.g., lost-in-the-middle). While zero-shot order-invariant LMs have been proposed to solve this issue, their success on practical listwise problems has been limited. In this work, as a first contribution, we identify and overcome two limitations to make zero-shot invariant LMs more practical: (1) training and inference distribution mismatch arising from modifying positional ID assignments to enforce invariance, and (2) failure to adapt to mixture of order-invariant and sensitive inputs in practical listwise problems. Then, to overcome these issues we propose (1) RoToR, a zero-shot invariant LM for genuinely order-invariant inputs with minimal modifications of positional IDs, and (2) Selective Routing, an adaptive framework that handles both order-invariant and order-sensitive inputs in listwise tasks. On the Lost in the middle (LitM), Knowledge Graph QA (KGQA), and MMLU benchmarks, we show that RoToR with Selective Routing can effectively handle practical listwise input tasks in a zero-shot manner (https://github.com/soyoung97/RoToR)",
      "arxiv_url": "https://arxiv.org/abs/2502.08662",
      "pdf_url": "https://arxiv.org/pdf/2502.08662",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.09089",
      "title": "LocAgent: Graph-Guided LLM Agents for Code Localization",
      "authors": [
        "Zhaoling Chen",
        "Xiangru Tang",
        "Gangda Deng",
        "Fang Wu",
        "Jialong Wu",
        "Zhiwei Jiang",
        "Viktor K. Prasanna",
        "Arman Cohan",
        "Xingyao Wang"
      ],
      "abstract": "Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.",
      "arxiv_url": "https://arxiv.org/abs/2503.09089",
      "pdf_url": "https://arxiv.org/pdf/2503.09089",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.02614",
      "title": "Personalized Generation In Large Model Era: A Survey",
      "authors": [
        "Yiyan Xu",
        "Jinghao Zhang",
        "Alireza Salemi",
        "Xinting Hu",
        "Wenjie Wang",
        "Fuli Feng",
        "Hamed Zamani",
        "Xiangnan He",
        "Tat-Seng Chua"
      ],
      "abstract": "In the era of large models, content generation is gradually shifting to Personalized Generation (PGen), tailoring content to individual preferences and needs. This paper presents the first comprehensive survey on PGen, investigating existing research in this rapidly growing field. We conceptualize PGen from a unified perspective, systematically formalizing its key components, core objectives, and abstract workflows. Based on this unified perspective, we propose a multi-level taxonomy, offering an in-depth review of technical advancements, commonly used datasets, and evaluation metrics across multiple modalities, personalized contexts, and tasks. Moreover, we envision the potential applications of PGen and highlight open challenges and promising directions for future exploration. By bridging PGen research across multiple modalities, this survey serves as a valuable resource for fostering knowledge sharing and interdisciplinary collaboration, ultimately contributing to a more personalized digital landscape.",
      "arxiv_url": "https://arxiv.org/abs/2503.02614",
      "pdf_url": "https://arxiv.org/pdf/2503.02614",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Personalization"
      ],
      "published_date": "2025-03-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20362",
      "title": "VSCBench: Bridging the Gap in Vision-Language Model Safety Calibration",
      "authors": [
        "Jiahui Geng",
        "Qing Li",
        "Zongxiong Chen",
        "Yuxia Wang",
        "Derui Zhu",
        "Zhuohan Xie",
        "Chenyang Lyu",
        "Xiuying Chen",
        "Preslav Nakov",
        "Fakhri Karray"
      ],
      "abstract": "The rapid advancement of vision-language models (VLMs) has brought a lot of attention to their safety alignment. However, existing methods have primarily focused on model undersafety, where the model responds to hazardous queries, while neglecting oversafety, where the model refuses to answer safe queries. In this paper, we introduce the concept of $\\textit{safety calibration}$, which systematically addresses both undersafety and oversafety. Specifically, we present $\\textbf{VSCBench}$, a novel dataset of 3,600 image-text pairs that are visually or textually similar but differ in terms of safety, which is designed to evaluate safety calibration across image-centric and text-centric scenarios. Based on our benchmark, we evaluate safety calibration across eleven widely used VLMs. Our extensive experiments revealed major issues with both undersafety and oversafety. We further investigated four approaches to improve the model's safety calibration. We found that even though some methods effectively calibrated the models' safety problems, these methods also lead to the degradation of models' utility. This trade-off underscores the urgent need for advanced calibration methods, and our benchmark provides a valuable tool for evaluating future approaches. Our code and data are available at https://github.com/jiahuigeng/VSCBench.git.",
      "arxiv_url": "https://arxiv.org/abs/2505.20362",
      "pdf_url": "https://arxiv.org/pdf/2505.20362",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.15568",
      "title": "Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for Evaluating Gender Diversity in Large Language Models",
      "authors": [
        "Zhengyang Shan",
        "Emily Ruth Diana",
        "Jiawei Zhou"
      ],
      "abstract": "We present a comprehensive evaluation of gender fairness in large language models (LLMs), focusing on their ability to handle both binary and non-binary genders. While previous studies primarily focus on binary gender distinctions, we introduce the Gender Inclusivity Fairness Index (GIFI), a novel and comprehensive metric that quantifies the diverse gender inclusivity of LLMs. GIFI consists of a wide range of evaluations at different levels, from simply probing the model with respect to provided gender pronouns to testing various aspects of model generation and cognitive behaviors under different gender assumptions, revealing biases associated with varying gender identifiers. We conduct extensive evaluations with GIFI on 22 prominent open-source and proprietary LLMs of varying sizes and capabilities, discovering significant variations in LLMs' gender inclusivity. Our study highlights the importance of improving LLMs' inclusivity, providing a critical benchmark for future advancements in gender fairness in generative models.",
      "arxiv_url": "https://arxiv.org/abs/2506.15568",
      "pdf_url": "https://arxiv.org/pdf/2506.15568",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "98075e51c5ba49e723a2d4f8f15b6199bd7e4d92",
      "title": "ASTRO: Automatic Strategy Optimization For Non-Cooperative Dialogues",
      "authors": [
        "Yikuan Hu",
        "Chen Huang",
        "Wenqiang Lei"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/98075e51c5ba49e723a2d4f8f15b6199bd7e4d92",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "980821bf3db02de118e5de566e96ede7cf3fcd3e",
      "title": "GLiM: Integrating Graph Transformer and LLM for Document-Level Biomedical Relation Extraction with Incomplete Labeling",
      "authors": [
        "Hao Fang",
        "Yuejie Zhang",
        "Rui Feng",
        "Yingwen Wang",
        "Qing Wang",
        "Wen He",
        "Xiaobo Zhang",
        "Tao Zhang",
        "Shang Gao"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/980821bf3db02de118e5de566e96ede7cf3fcd3e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23166",
      "title": "Tell, Don't Show: Leveraging Language Models' Abstractive Retellings to Model Literary Themes",
      "authors": [
        "Li Lucy",
        "Camilla Griffiths",
        "Sarah Levine",
        "Jennifer L. Eberhardt",
        "Dorottya Demszky",
        "David Bamman"
      ],
      "abstract": "Conventional bag-of-words approaches for topic modeling, like latent Dirichlet allocation (LDA), struggle with literary text. Literature challenges lexical methods because narrative language focuses on immersive sensory details instead of abstractive description or exposition: writers are advised to\"show, don't tell.\"We propose Retell, a simple, accessible topic modeling approach for literature. Here, we prompt resource-efficient, generative language models (LMs) to tell what passages show, thereby translating narratives' surface forms into higher-level concepts and themes. By running LDA on LMs' retellings of passages, we can obtain more precise and informative topics than by running LDA alone or by directly asking LMs to list topics. To investigate the potential of our method for cultural analytics, we compare our method's outputs to expert-guided annotations in a case study on racial/cultural identity in high school English language arts books.",
      "arxiv_url": "https://arxiv.org/abs/2505.23166",
      "pdf_url": "https://arxiv.org/pdf/2505.23166",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14059",
      "title": "Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting",
      "authors": [
        "Hao Feng",
        "Shubo Wei",
        "Xiang Fei",
        "Wei Shi",
        "Yingdong Han",
        "Lei Liao",
        "Jinghui Lu",
        "Binghong Wu",
        "Qi Liu",
        "Chunhui Lin",
        "Jingqun Tang",
        "Hao Liu",
        "Can Huang"
      ],
      "abstract": "Document image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables. Current approaches either assemble specialized expert models or directly generate page-level content autoregressively, facing integration overhead, efficiency bottlenecks, and layout structure degradation despite their decent performance. To address these limitations, we present \\textit{Dolphin} (\\textit{\\textbf{Do}cument Image \\textbf{P}arsing via \\textbf{H}eterogeneous Anchor Prompt\\textbf{in}g}), a novel multimodal document image parsing model following an analyze-then-parse paradigm. In the first stage, Dolphin generates a sequence of layout elements in reading order. These heterogeneous elements, serving as anchors and coupled with task-specific prompts, are fed back to Dolphin for parallel content parsing in the second stage. To train Dolphin, we construct a large-scale dataset of over 30 million samples, covering multi-granularity parsing tasks. Through comprehensive evaluations on both prevalent benchmarks and self-constructed ones, Dolphin achieves state-of-the-art performance across diverse page-level and element-level settings, while ensuring superior efficiency through its lightweight architecture and parallel parsing mechanism. The code and pre-trained models are publicly available at https://github.com/ByteDance/Dolphin",
      "arxiv_url": "https://arxiv.org/abs/2505.14059",
      "pdf_url": "https://arxiv.org/pdf/2505.14059",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00894",
      "title": "CODEMENV: Benchmarking Large Language Models on Code Migration",
      "authors": [
        "Keyuan Cheng",
        "Xudong Shen",
        "Yihao Yang",
        "Tengyue Wang",
        "Yang Cao",
        "Muhammad Asif Ali",
        "Hanbin Wang",
        "Lijie Hu",
        "Di Wang"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable capabilities across various software engineering tasks; however, their effectiveness in code migration, adapting code to run in different environments, remains insufficiently studied. In this work, we introduce CODEMENV: Code Migration Across Environment, a new benchmark specifically designed to assess LLMs' abilities in code migration scenarios. CODEMENV consists of 922 examples spanning 19 Python and Java packages, and covers three core tasks: (1) identifying functions incompatible with specific versions, (2) detecting changes in function definitions, and (3) adapting code to target environments. Experimental evaluation with seven LLMs on CODEMENV yields an average pass@1 rate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings include: (i) LLMs tend to be more proficient with newer function versions, which aids in migrating legacy code, and (ii) LLMs sometimes exhibit logical inconsistencies by identifying function changes irrelevant to the intended migration environment. The datasets are available at https://github.com/xdshen-ai/Benchmark-of-Code-Migration.",
      "arxiv_url": "https://arxiv.org/abs/2506.00894",
      "pdf_url": "https://arxiv.org/pdf/2506.00894",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00386",
      "title": "Adaptive-VP: A Framework for LLM-Based Virtual Patients that Adapts to Trainees' Dialogue to Facilitate Nurse Communication Training",
      "authors": [
        "Keyeun Lee",
        "Seolhee Lee",
        "E. Kim",
        "Yena Ko",
        "Jinsu Eun",
        "Dahee Kim",
        "Hyewon Cho",
        "Haiyi Zhu",
        "Robert E. Kraut",
        "Eunyoung Suh",
        "Eun-mee Kim",
        "Hajin Lim"
      ],
      "abstract": "Effective communication training is essential to preparing nurses for high-quality patient care. While standardized patient (SP) simulations provide valuable experiential learning, they are often costly and inflexible. Virtual patient (VP) systems offer a scalable alternative, but most fail to adapt to the varying communication skills of trainees. In particular, when trainees respond ineffectively, VPs should escalate in hostility or become uncooperative--yet this level of adaptive interaction remains largely unsupported. To address this gap, we introduce Adaptive-VP, a VP dialogue generation framework that leverages large language models (LLMs) to dynamically adapt VP behavior based on trainee input. The framework features a pipeline for constructing clinically grounded yet flexible VP scenarios and a modular system for assessing trainee communication and adjusting VP responses in real time, while ensuring learner safety. We validated Adaptive-VP by simulating challenging patient conversations. Automated evaluation using a corpus from practicing nurses showed that our communication skill evaluation mechanism reflected real-world proficiency levels. Expert nurses further confirmed that Adaptive-VP produced more natural and realistic interactions than existing approaches, demonstrating its potential as a scalable and effective tool for nursing communication training.",
      "arxiv_url": "https://arxiv.org/abs/2506.00386",
      "pdf_url": "https://arxiv.org/pdf/2506.00386",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.01718",
      "title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis",
      "authors": [
        "Huaye Zeng",
        "Dongfu Jiang",
        "Haozhe Wang",
        "Ping Nie",
        "Xiaotong Chen",
        "Wenhu Chen"
      ],
      "abstract": "Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25\\% and MBPP-plus by 6\\% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models.",
      "arxiv_url": "https://arxiv.org/abs/2502.01718",
      "pdf_url": "https://arxiv.org/pdf/2502.01718",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12821",
      "title": "Pitfalls of Scale: Investigating the Inverse Task of Redefinition in Large Language Models",
      "authors": [
        "Elena Stringli",
        "Maria Lymperaiou",
        "Giorgos Filandrianos",
        "G. Stamou"
      ],
      "abstract": "Inverse tasks can uncover potential reasoning gaps as Large Language Models (LLMs) scale up. In this work, we explore the redefinition task, in which we assign alternative values to well-known physical constants and units of measure, prompting LLMs to respond accordingly. Our findings show that not only does model performance degrade with scale, but its false confidence also rises. Moreover, while factors such as prompting strategies or response formatting are influential, they do not preclude LLMs from anchoring to memorized values.",
      "arxiv_url": "https://arxiv.org/abs/2502.12821",
      "pdf_url": "https://arxiv.org/pdf/2502.12821",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12568",
      "title": "A Cognitive Writing Perspective for Constrained Long-Form Text Generation",
      "authors": [
        "Kaiyang Wan",
        "Honglin Mu",
        "Rui Hao",
        "Haoran Luo",
        "Tianle Gu",
        "Xiuying Chen"
      ],
      "abstract": "Like humans, Large Language Models (LLMs) struggle to generate high-quality long-form text that adheres to strict requirements in a single pass. This challenge is unsurprising, as successful human writing, according to the Cognitive Writing Theory, is a complex cognitive process involving iterative planning, translating, reviewing, and monitoring. Motivated by these cognitive principles, we aim to equip LLMs with human-like cognitive writing capabilities through CogWriter, a novel training-free framework that transforms LLM constrained long-form text generation into a systematic cognitive writing paradigm. Our framework consists of two key modules: (1) a Planning Agent that performs hierarchical planning to decompose the task, and (2) multiple Generation Agents that execute these plans in parallel. The system maintains quality via continuous monitoring and reviewing mechanisms, which evaluate outputs against specified requirements and trigger necessary revisions. CogWriter demonstrates exceptional performance on LongGenBench, a benchmark for complex constrained long-form text generation. Even when using Qwen-2.5-14B as its backbone, CogWriter surpasses GPT-4o by 22% in complex instruction completion accuracy while reliably generating texts exceeding 10,000 words. We hope this cognitive science-inspired approach provides a paradigm for LLM writing advancements: \\href{https://github.com/KaiyangWan/CogWriter}{CogWriter}.",
      "arxiv_url": "https://arxiv.org/abs/2502.12568",
      "pdf_url": "https://arxiv.org/pdf/2502.12568",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "992fd2d43ca1d489ec168b04c313a6cb66331343",
      "title": "Dynamic Evaluation with Cognitive Reasoning for Multi-turn Safety of Large Language Models",
      "authors": [
        "Lanxue Zhang",
        "Yanan Cao",
        "Yuqiang Xie",
        "Fang Fang",
        "Yangxi Li"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) poses significant challenges for safety evaluation. Current static datasets struggle to identify emerging vulnerabilities due to three limitations: (1) they risk being exposed in model training data, leading to evaluation bias; (2) their limited prompt diversity fails to capture real-world application scenarios; (3) they are limited to provide human-like multi-turn interactions. To address these limitations, we propose a dynamic evaluation framework, CogSafe , for comprehensive and automated multi-turn safety assessment of LLMs. We introduce CogSafe based on cognitive theories to simulate the real chatting process. To enhance assessment diversity, we introduce scenario simulation and strategy decision to guide the dynamic generation, enabling coverage of application situations. Furthermore, we incorporate the cognitive process to simulate multi-turn dialogues that reflect the cognitive dynamics of real-world interactions. Extensive experiments demonstrate the scalability and effectiveness of our framework, which has been applied to evaluate the safety of widely used LLMs. The evaluation dataset and code are available at https://github.com/Qlanxue/CogSafe.",
      "arxiv_url": "https://www.semanticscholar.org/paper/992fd2d43ca1d489ec168b04c313a6cb66331343",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04381",
      "title": "TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for LLM-as-a-Judge",
      "authors": [
        "Cheng-Han Chiang",
        "Hung-yi Lee",
        "Michal Lukasik"
      ],
      "abstract": "The LLM-as-a-judge paradigm uses large language models (LLMs) for automated text evaluation, where a numerical assessment is assigned by an LLM to the input text following scoring rubrics. Existing methods for LLM-as-a-judge use cross-entropy (CE) loss for fine-tuning, which neglects the numeric nature of score prediction. Recent work addresses numerical prediction limitations of LLM fine-tuning through regression-aware fine-tuning, which, however, does not consider chain-of-thought (CoT) reasoning for score prediction. In this paper, we introduce TRACT (Two-stage Regression-Aware fine-tuning with CoT), a method combining CoT reasoning with regression-aware training. TRACT consists of two stages: first, seed LLM is fine-tuned to generate CoTs, which serve as supervision for the second stage fine-tuning. The training objective of TRACT combines the CE loss for learning the CoT reasoning capabilities, and the regression-aware loss for the score prediction. Experiments across four LLM-as-a-judge datasets and two LLMs show that TRACT significantly outperforms existing methods. Extensive ablation studies validate the importance of each component in TRACT.",
      "arxiv_url": "https://arxiv.org/abs/2503.04381",
      "pdf_url": "https://arxiv.org/pdf/2503.04381",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01150",
      "title": "MiLiC-Eval: Benchmarking Multilingual LLMs for China's Minority Languages",
      "authors": [
        "Chen Zhang",
        "Mingxu Tao",
        "Zhiyuan Liao",
        "Yansong Feng"
      ],
      "abstract": "Large language models (LLMs) excel in high-resource languages but struggle with low-resource languages (LRLs), particularly those spoken by minority communities in China, such as Tibetan, Uyghur, Kazakh, and Mongolian. To systematically track the progress in these languages, we introduce MiLiC-Eval, a benchmark designed for minority languages in China, featuring 24K instances across 9 tasks. MiLiC-Eval focuses on underrepresented writing systems. Its parallelism between tasks and languages can provide a faithful and fine-grained assessment of linguistic and problem-solving skills. Our evaluation reveals that open-source LLMs perform poorly on syntax-intensive tasks and multi-script languages. We further demonstrate how MiLiC-Eval can help advance LRL research in handling diverse writing systems and understanding the process of language adaptation.",
      "arxiv_url": "https://arxiv.org/abs/2503.01150",
      "pdf_url": "https://arxiv.org/pdf/2503.01150",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12212",
      "title": "Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning",
      "authors": [
        "Shaobo Wang",
        "Xiangqi Jin",
        "Ziming Wang",
        "Jize Wang",
        "Jiajun Zhang",
        "Kaixin Li",
        "Zichen Wen",
        "Zhong Li",
        "Conghui He",
        "Xuming Hu",
        "Linfeng Zhang"
      ],
      "abstract": "Fine-tuning large language models (LLMs) on task-specific data is essential for their effective deployment. As dataset sizes grow, efficiently selecting optimal subsets for training becomes crucial to balancing performance and computational costs. Traditional data selection methods often require fine-tuning a scoring model on the target dataset, which is time-consuming and resource-intensive, or rely on heuristics that fail to fully leverage the model's predictive capabilities. To address these challenges, we propose Data Whisperer, an efficient, training-free, attention-based method that leverages few-shot in-context learning with the model to be fine-tuned. Comprehensive evaluations were conducted on both raw and synthetic datasets across diverse tasks and models. Notably, Data Whisperer achieves superior performance compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just 10% of the data, and outperforms existing methods with a 3.1-point improvement and a 7.4$\\times$ speedup. The code is available at https://github.com/gszfwsb/Data-Whisperer.",
      "arxiv_url": "https://arxiv.org/abs/2505.12212",
      "pdf_url": "https://arxiv.org/pdf/2505.12212",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "997eee68d3315851d455d0408ba188ed276be6b7",
      "title": "Chinese Inertial GAN for Handwriting Signal Generation and Recognition",
      "authors": [
        "Yifeng Wang",
        "Yi Zhao"
      ],
      "abstract": "Keyboard-based interaction may not accommodate various needs, especially for individuals with disabilities. While inertial sensor-based writing recognition is promising due to the sensors’ small size, wearability, and low cost, accurate recognition in the Chinese context is hampered by the difficulty of collecting extensive inertial signal samples for the vast number of characters. Therefore, we design a Chinese Inertial GAN (CI-GAN) containing Chinese glyph encoding (CGE), forced optimal transport (FOT), and semantic relevance alignment (SRA) to acquire unlimited high-quality training samples. Unlike existing vectorization methods focusing on the meaning of Chinese characters, CGE represents shape and stroke features, providing glyph guidance for writing signal generation. FOT establishes a triple-consistency constraint between the input prompt, output signal features, and real signal features, ensuring the authenticity and semantic accuracy of the generated signals. SRA aligns semantic relationships between multiple outputs and their input prompts, ensuring that similar inputs correspond to similar outputs (and vice versa), alleviating model hallu-cination. The three modules guide the generator while also interacting with each other, forming a coupled system. By utilizing the massive training samples provided by CI-GAN, the performance of six widely used classifiers is improved from 6.7% to 98.4%, indicating that CI-GAN constructs a flexible and efficient data platform for Chinese inertial writing recognition. Furthermore, we release the first Chinese inertial writing dataset on GitHub.",
      "arxiv_url": "https://www.semanticscholar.org/paper/997eee68d3315851d455d0408ba188ed276be6b7",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.19835",
      "title": "MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration",
      "authors": [
        "Yucheng Zhou",
        "Lingran Song",
        "Jianbing Shen"
      ],
      "abstract": "Recent advancements in medical Large Language Models (LLMs) have showcased their powerful reasoning and diagnostic capabilities. Despite their success, current unified multimodal medical LLMs face limitations in knowledge update costs, comprehensiveness, and flexibility. To address these challenges, we introduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis (MAM). Inspired by our empirical findings highlighting the benefits of role assignment and diagnostic discernment in LLMs, MAM decomposes the medical diagnostic process into specialized roles: a General Practitioner, Specialist Team, Radiologist, Medical Assistant, and Director, each embodied by an LLM-based agent. This modular and collaborative framework enables efficient knowledge updates and leverages existing medical LLMs and knowledge bases. Extensive experimental evaluations conducted on a wide range of publicly accessible multimodal medical datasets, incorporating text, image, audio, and video modalities, demonstrate that MAM consistently surpasses the performance of modality-specific LLMs. Notably, MAM achieves significant performance improvements ranging from 18% to 365% compared to baseline models. Our code is released at https://github.com/yczhou001/MAM.",
      "arxiv_url": "https://arxiv.org/abs/2506.19835",
      "pdf_url": "https://arxiv.org/pdf/2506.19835",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-06-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.12075",
      "title": "BOOKCOREF: Coreference Resolution at Book Scale",
      "authors": [
        "Giuliano Martinelli",
        "Tommaso Bonomo",
        "Pere-Lluís Huguet Cabot",
        "Roberto Navigli"
      ],
      "abstract": "Coreference Resolution systems are typically evaluated on benchmarks containing small- to medium-scale documents. When it comes to evaluating long texts, however, existing benchmarks, such as LitBank, remain limited in length and do not adequately assess system capabilities at the book scale, i.e., when co-referring mentions span hundreds of thousands of tokens. To fill this gap, we first put forward a novel automatic pipeline that produces high-quality Coreference Resolution annotations on full narrative texts. Then, we adopt this pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with an average document length of more than 200,000 tokens. We carry out a series of experiments showing the robustness of our automatic procedure and demonstrating the value of our resource, which enables current long-document coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full books. Moreover, we report on the new challenges introduced by this unprecedented book-scale setting, highlighting that current models fail to deliver the same performance they achieve on smaller documents. We release our data and code to encourage research and development of new book-scale Coreference Resolution systems at https://github.com/sapienzanlp/bookcoref.",
      "arxiv_url": "https://arxiv.org/abs/2507.12075",
      "pdf_url": "https://arxiv.org/pdf/2507.12075",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "99f370e55fa54d0d0e2cb703aeecbff96ad5c34a",
      "title": "Not All Terms Matter: Recall-Oriented Adaptive Learning for PLM-aided Query Expansion in Open-Domain Question Answering",
      "authors": [
        "Xinran Chen",
        "Ben He",
        "Xuanang Chen",
        "Le Sun"
      ],
      "abstract": "The effectiveness of open-domain question answering (ODQA), particularly those employing a retriever-reader architecture, depends on the ability to recall relevant documents - a critical step that enables the reader to accurately extract answers. To enhance this retrieval phase, current query expansion (QE) techniques leverage pre-trained language models (PLM) to mitigate word mismatches and improve the re-call of relevant documents. Despite their advancements, these techniques often treat all expanded terms uniformly, which can lead to less-than-optimal retrieval outcomes. In response, we propose a novel Re call-oriented A daptive L earning (ReAL) method, which iteratively adjusts the importance weights of QE terms based on their relevance, thereby refining term distinction and enhancing the separation of relevant terms. Specifically, ReAL employs a similarity-based model to classify documents into pseudo-relevant and pseudo-irrelevant sets, and then optimizes term weights via two tailored loss functions to maximize the scoring gap between them. Experiments on four ODQA datasets and five QE methods show that ReAL consistently enhances retrieval accuracy and overall end-to-end QA performance, providing a robust and efficient solution for improving QE strategies in ODQA scenarios.",
      "arxiv_url": "https://www.semanticscholar.org/paper/99f370e55fa54d0d0e2cb703aeecbff96ad5c34a",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11541",
      "title": "MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training",
      "authors": [
        "Hui Huang",
        "Jiaheng Liu",
        "Yancheng He",
        "Shilong Li",
        "Bing Xu",
        "Conghui Zhu",
        "Muyun Yang",
        "Tiejun Zhao"
      ],
      "abstract": "Complex instruction-following with elaborate constraints is imperative for Large Language Models (LLMs). While existing methods have constructed data for complex instruction alignment, they all rely on a more advanced model, especially GPT-4, limiting their application. In this paper, we propose a Multi-granularity Self-Contrastive Training (MuSC) framework, to improve the complex instruction alignment without relying on a stronger model. Our method is conducted on both coarse and fine granularity. On coarse-granularity, we construct constraint-aware preference data based on instruction decomposition and recombination. On fine-granularity, we perform token-aware preference optimization with dynamic token-level supervision. Our method is evaluated on open-sourced models, and experiment results show our method achieves significant improvement on both complex and general instruction-following benchmarks, surpassing previous self-alignment methods.",
      "arxiv_url": "https://arxiv.org/abs/2502.11541",
      "pdf_url": "https://arxiv.org/pdf/2502.11541",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20897",
      "title": "Beyond Demographics: Fine-tuning Large Language Models to Predict Individuals' Subjective Text Perceptions",
      "authors": [
        "Matthias Orlikowski",
        "Jiaxin Pei",
        "Paul Rottger",
        "Philipp Cimiano",
        "David Jurgens",
        "Dirk Hovy"
      ],
      "abstract": "People naturally vary in their annotations for subjective questions and some of this variation is thought to be due to the person's sociodemographic characteristics. LLMs have also been used to label data, but recent work has shown that models perform poorly when prompted with sociodemographic attributes, suggesting limited inherent sociodemographic knowledge. Here, we ask whether LLMs can be trained to be accurate sociodemographic models of annotator variation. Using a curated dataset of five tasks with standardized sociodemographics, we show that models do improve in sociodemographic prompting when trained but that this performance gain is largely due to models learning annotator-specific behaviour rather than sociodemographic patterns. Across all tasks, our results suggest that models learn little meaningful connection between sociodemographics and annotation, raising doubts about the current use of LLMs for simulating sociodemographic variation and behaviour.",
      "arxiv_url": "https://arxiv.org/abs/2502.20897",
      "pdf_url": "https://arxiv.org/pdf/2502.20897",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.04362",
      "title": "LLMs can be easily Confused by Instructional Distractions",
      "authors": [
        "Yerin Hwang",
        "Yongi-Mi Kim",
        "Jahyun Koo",
        "Taegwan Kang",
        "Hyunkyung Bae",
        "Kyomin Jung"
      ],
      "abstract": "Despite the fact that large language models (LLMs) show exceptional skill in instruction following tasks, this strength can turn into a vulnerability when the models are required to disregard certain instructions. Instruction-following tasks typically involve a clear task description and input text containing the target data to be processed. However, when the input itself resembles an instruction, confusion may arise, even if there is explicit prompting to distinguish between the task instruction and the input. We refer to this phenomenon as instructional distraction. In this paper, we introduce a novel benchmark, named DIM-Bench, specifically designed to assess LLMs'performance under instructional distraction. The benchmark categorizes real-world instances of instructional distraction and evaluates LLMs across four instruction tasks: rewriting, proofreading, translation, and style transfer -- alongside five input tasks: reasoning, code generation, mathematical reasoning, bias detection, and question answering. Our experimental results reveal that even the most advanced LLMs are susceptible to instructional distraction, often failing to accurately follow user intent in such cases.",
      "arxiv_url": "https://arxiv.org/abs/2502.04362",
      "pdf_url": "https://arxiv.org/pdf/2502.04362",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15266",
      "title": "A Training-free LLM-based Approach to General Chinese Character Error Correction",
      "authors": [
        "Houquan Zhou",
        "Bo Zhang",
        "Zhenghua Li",
        "Ming Yan",
        "Min Zhang"
      ],
      "abstract": "Chinese spelling correction (CSC) is a crucial task that aims to correct character errors in Chinese text. While conventional CSC focuses on character substitution errors caused by mistyping, two other common types of character errors, missing and redundant characters, have received less attention. These errors are often excluded from CSC datasets during the annotation process or ignored during evaluation, even when they have been annotated. This issue limits the practicality of the CSC task. To address this issue, we introduce the task of General Chinese Character Error Correction (C2EC), which focuses on all three types of character errors. We construct a high-quality C2EC benchmark by combining and manually verifying data from CCTC and Lemon datasets. We extend the training-free prompt-free CSC method to C2EC by using Levenshtein distance for handling length changes and leveraging an additional prompt-based large language model (LLM) to improve performance. Experiments show that our method enables a 14B-parameter LLM to be on par with models nearly 50 times larger on both conventional CSC and C2EC tasks, without any fine-tuning.",
      "arxiv_url": "https://arxiv.org/abs/2502.15266",
      "pdf_url": "https://arxiv.org/pdf/2502.15266",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17701",
      "title": "From Perceptions to Decisions: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed LLMs",
      "authors": [
        "Ruxiao Chen",
        "Chenguang Wang",
        "Yuran Sun",
        "Xilei Zhao",
        "Susu Xu"
      ],
      "abstract": "Evacuation decision prediction is critical for efficient and effective wildfire response by helping emergency management anticipate traffic congestion and bottlenecks, allocate resources, and minimize negative impacts. Traditional statistical methods for evacuation decision prediction fail to capture the complex and diverse behavioral logic of different individuals. In this work, for the first time, we introduce FLARE, short for facilitating LLM for advanced reasoning on wildfire evacuation decision prediction, a Large Language Model (LLM)-based framework that integrates behavioral theories and models to streamline the Chain-of-Thought (CoT) reasoning and subsequently integrate with memory-based Reinforcement Learning (RL) module to provide accurate evacuation decision prediction and understanding. Our proposed method addresses the limitations of using existing LLMs for evacuation behavioral predictions, such as limited survey data, mismatching with behavioral theory, conflicting individual preferences, implicit and complex mental states, and intractable mental state-behavior mapping. Experiments on three post-wildfire survey datasets show an average of 20.47% performance improvement over traditional theory-informed behavioral models, with strong cross-event generalizability. Our complete code is publicly available at https://github.com/SusuXu-s-Lab/FLARE",
      "arxiv_url": "https://arxiv.org/abs/2502.17701",
      "pdf_url": "https://arxiv.org/pdf/2502.17701",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "9a9bd245343b7d013b51d593b16eeb659e387ca3",
      "title": "scRAG: Hybrid Retrieval-Augmented Generation for LLM-based Cross-Tissue Single-Cell Annotation",
      "authors": [
        "Zhiyin Yu",
        "Chao Zheng",
        "Chong Chen",
        "Xian-Sheng Hua",
        "Xiao Luo"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/9a9bd245343b7d013b51d593b16eeb659e387ca3",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21850",
      "title": "Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task",
      "authors": [
        "Yanbei Jiang",
        "Yihao Ding",
        "Chao Lei",
        "Jiayang Ao",
        "Jey Han Lau",
        "Krista A. Ehinger"
      ],
      "abstract": "Current Multimodal Large Language Models (MLLMs) excel in general visual reasoning but remain underexplored in Abstract Visual Reasoning (AVR), which demands higher-order reasoning to identify abstract rules beyond simple perception. Existing AVR benchmarks focus on single-step reasoning, emphasizing the end result but neglecting the multi-stage nature of reasoning process. Past studies found MLLMs struggle with these benchmarks, but it doesn't explain how they fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR benchmark, based on RAVEN, designed to assess reasoning across varying levels of complexity. Additionally, existing metrics like accuracy only focus on the final outcomes while do not account for the correctness of intermediate steps. Therefore, we propose a novel metric, MSEval, which considers the correctness of intermediate steps in addition to the final outcomes. We conduct comprehensive experiments on MultiStAR using 17 representative close-source and open-source MLLMs. The results reveal that while existing MLLMs perform adequately on basic perception tasks, they continue to face challenges in more complex rule detection stages.",
      "arxiv_url": "https://arxiv.org/abs/2505.21850",
      "pdf_url": "https://arxiv.org/pdf/2505.21850",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "9ab569d44bd58b26263bf57eabafa7edca276abc",
      "title": "Visibility as Survival: Generalizing NLP for Native Alaskan Language Identification",
      "authors": [
        "Ivory Yang",
        "Chunhui Zhang",
        "Yuxin Wang",
        "Z. Ouyang",
        "Soroush Vosoughi"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/9ab569d44bd58b26263bf57eabafa7edca276abc",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19163",
      "title": "TestNUC: Enhancing Test-Time Computing Approaches through Neighboring Unlabeled Data Consistency",
      "authors": [
        "Henry Peng Zou",
        "Zhengyao Gu",
        "Yue Zhou",
        "Yankai Chen",
        "Weizhi Zhang",
        "Liancheng Fang",
        "Yibo Wang",
        "Yangning Li",
        "Kay Liu",
        "Philip S. Yu"
      ],
      "abstract": "Test-time computing approaches, which leverage additional computational resources during inference, have been proven effective in enhancing large language model performance. This work introduces a novel, linearly scaling approach, TestNUC, that improves test-time predictions by leveraging the local consistency of neighboring unlabeled data-it classifies an input instance by considering not only the model's prediction on that instance but also on neighboring unlabeled instances. We evaluate TestNUC across eight diverse datasets, spanning intent classification, topic mining, domain discovery, and emotion detection, demonstrating its consistent superiority over baseline methods such as standard prompting and self-consistency. Furthermore, TestNUC can be seamlessly integrated with existing test-time computing approaches, substantially boosting their performance. Our analysis reveals that TestNUC scales effectively with increasing amounts of unlabeled data and performs robustly across different embedding models, making it practical for real-world applications. Our code is available at https://github.com/HenryPengZou/TestNUC.",
      "arxiv_url": "https://arxiv.org/abs/2502.19163",
      "pdf_url": "https://arxiv.org/pdf/2502.19163",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24211",
      "title": "Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?",
      "authors": [
        "Jiwan Chung",
        "Janghan Yoon",
        "Junhyeong Park",
        "Sangeyl Lee",
        "Joowon Yang",
        "Sooyeon Park",
        "Youngjae Yu"
      ],
      "abstract": "Any-to-any generative models aim to enable seamless interpretation and generation across multiple modalities within a unified framework, yet their ability to preserve relationships across modalities remains uncertain. Do unified models truly achieve cross-modal coherence, or is this coherence merely perceived? To explore this, we introduce ACON, a dataset of 1,000 images (500 newly contributed) paired with captions, editing instructions, and Q&A pairs to evaluate cross-modal transfers rigorously. Using three consistency criteria-cyclic consistency, forward equivariance, and conjugated equivariance-our experiments reveal that any-to-any models do not consistently demonstrate greater cross-modal consistency than specialized models in pointwise evaluations such as cyclic consistency. However, equivariance evaluations uncover weak but observable consistency through structured analyses of the intermediate latent space enabled by multiple editing operations. We release our code and data at https://github.com/JiwanChung/ACON.",
      "arxiv_url": "https://arxiv.org/abs/2505.24211",
      "pdf_url": "https://arxiv.org/pdf/2505.24211",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.13773",
      "title": "Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions",
      "authors": [
        "Pu Jian",
        "Donglei Yu",
        "Wen Yang",
        "Shuo Ren",
        "Jiajun Zhang"
      ],
      "abstract": "In visual question answering (VQA) context, users often pose ambiguous questions to visual language models (VLMs) due to varying expression habits. Existing research addresses such ambiguities primarily by rephrasing questions. These approaches neglect the inherently interactive nature of user interactions with VLMs, where ambiguities can be clarified through user feedback. However, research on interactive clarification faces two major challenges: (1) Benchmarks are absent to assess VLMs'capacity for resolving ambiguities through interaction; (2) VLMs are trained to prefer answering rather than asking, preventing them from seeking clarification. To overcome these challenges, we introduce \\textbf{ClearVQA} benchmark, which targets three common categories of ambiguity in VQA context, and encompasses various VQA scenarios.",
      "arxiv_url": "https://arxiv.org/abs/2507.13773",
      "pdf_url": "https://arxiv.org/pdf/2507.13773",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.05193",
      "title": "Memory-augmented Query Reconstruction for LLM-based Knowledge Graph Reasoning",
      "authors": [
        "Mufan Xu",
        "Gewen Liang",
        "Kehai Chen",
        "Wei Wang",
        "Xun Zhou",
        "Muyun Yang",
        "Tiejun Zhao",
        "Min Zhang"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable performance on knowledge graph question answering (KGQA) tasks by planning and interacting with knowledge graphs. However, existing methods often confuse tool utilization with knowledge reasoning, harming readability of model outputs and giving rise to hallucinatory tool invocations, which hinder the advancement of KGQA. To address this issue, we propose Memory-augmented Query Reconstruction for LLM-based Knowledge Graph Reasoning (MemQ) to decouple LLM from tool invocation tasks using LLM-built query memory. By establishing a memory module with explicit descriptions of query statements, the proposed MemQ facilitates the KGQA process with natural language reasoning and memory-augmented query reconstruction. Meanwhile, we design an effective and readable reasoning to enhance the LLM's reasoning capability in KGQA. Experimental results that MemQ achieves state-of-the-art performance on widely used benchmarks WebQSP and CWQ.",
      "arxiv_url": "https://arxiv.org/abs/2503.05193",
      "pdf_url": "https://arxiv.org/pdf/2503.05193",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.06987",
      "title": "Social Bias Benchmark for Generation: A Comparison of Generation and QA-Based Evaluations",
      "authors": [
        "Jiho Jin",
        "Woosung Kang",
        "Junho Myung",
        "Alice Oh"
      ],
      "abstract": "Measuring social bias in large language models (LLMs) is crucial, but existing bias evaluation methods struggle to assess bias in long-form generation. We propose a Bias Benchmark for Generation (BBG), an adaptation of the Bias Benchmark for QA (BBQ), designed to evaluate social bias in long-form generation by having LLMs generate continuations of story prompts. Building our benchmark in English and Korean, we measure the probability of neutral and biased generations across ten LLMs. We also compare our long-form story generation evaluation results with multiple-choice BBQ evaluation, showing that the two approaches produce inconsistent results.",
      "arxiv_url": "https://arxiv.org/abs/2503.06987",
      "pdf_url": "https://arxiv.org/pdf/2503.06987",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.11115",
      "title": "Incorporating Domain Knowledge into Materials Tokenization",
      "authors": [
        "Yerim Oh",
        "Jun-Hyung Park",
        "Junho Kim",
        "SungHo Kim",
        "SangKeun Lee"
      ],
      "abstract": "While language models are increasingly utilized in materials science, typical models rely on frequency-centric tokenization methods originally developed for natural language processing. However, these methods frequently produce excessive fragmentation and semantic loss, failing to maintain the structural and semantic integrity of material concepts. To address this issue, we propose MATTER, a novel tokenization approach that integrates material knowledge into tokenization. Based on MatDetector trained on our materials knowledge base and a re-ranking method prioritizing material concepts in token merging, MATTER maintains the structural integrity of identified material concepts and prevents fragmentation during tokenization, ensuring their semantic meaning remains intact. The experimental results demonstrate that MATTER outperforms existing tokenization methods, achieving an average performance gain of $4\\%$ and $2\\%$ in the generation and classification tasks, respectively. These results underscore the importance of domain knowledge for tokenization strategies in scientific text processing. Our code is available at https://github.com/yerimoh/MATTER",
      "arxiv_url": "https://arxiv.org/abs/2506.11115",
      "pdf_url": "https://arxiv.org/pdf/2506.11115",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00197",
      "title": "When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs",
      "authors": [
        "Xinyue Shen",
        "Yun Shen",
        "Michael Backes",
        "Yang Zhang"
      ],
      "abstract": "Knowledge files have been widely used in large language model (LLM) agents, such as GPTs, to improve response quality. However, concerns about the potential leakage of knowledge files have grown significantly. Existing studies demonstrate that adversarial prompts can induce GPTs to leak knowledge file content. Yet, it remains uncertain whether additional leakage vectors exist, particularly given the complex data flows across clients, servers, and databases in GPTs. In this paper, we present a comprehensive risk assessment of knowledge file leakage, leveraging a novel workflow inspired by Data Security Posture Management (DSPM). Through the analysis of 651,022 GPT metadata, 11,820 flows, and 1,466 responses, we identify five leakage vectors: metadata, GPT initialization, retrieval, sandboxed execution environments, and prompts. These vectors enable adversaries to extract sensitive knowledge file data such as titles, content, types, and sizes. Notably, the activation of the built-in tool Code Interpreter leads to a privilege escalation vulnerability, enabling adversaries to directly download original knowledge files with a 95.95% success rate. Further analysis reveals that 28.80% of leaked files are copyrighted, including digital copies from major publishers and internal materials from a listed company. In the end, we provide actionable solutions for GPT builders and platform providers to secure the GPT data supply chain.",
      "arxiv_url": "https://arxiv.org/abs/2506.00197",
      "pdf_url": "https://arxiv.org/pdf/2506.00197",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "9b56103675af6777d829eca813ddaa1f2ba00bf9",
      "title": "How to Compare Things Properly? A Study of Argument Relevance in Comparative Question Answering",
      "authors": [
        "Irina Nikishina",
        "Saba Anwar",
        "Nikolay Dolgov",
        "Maria Manina",
        "D. Ignatenko",
        "Artem Shelmanov",
        "Christian Biemann"
      ],
      "abstract": "Comparative Question Answering (CQA) lies at the intersection of Question Answering, Ar-gument Mining, and Summarization. It poses unique challenges due to the inherently subjective nature of many questions and the need to integrate diverse perspectives. Although the CQA task can be addressed using recently emerged instruction-following Large Language Models (LLMs), challenges such as hallucinations in their outputs and the lack of transparent argument provenance remain significant limitations. To address these challenges, we construct a manually curated dataset comprising arguments annotated with their relevance. These arguments are further used to answer comparative questions, enabling precise traceability and faithfulness. Furthermore, we define explicit criteria for an “ideal” comparison and introduce a benchmark for evaluating the outputs of various Retrieval-Augmented Generation (RAG) models with respect to argument relevance. All code and data are publicly released to support further research 1 .",
      "arxiv_url": "https://www.semanticscholar.org/paper/9b56103675af6777d829eca813ddaa1f2ba00bf9",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.11921",
      "title": "RePanda: Pandas-powered Tabular Verification and Reasoning",
      "authors": [
        "Atoosa Malemir Chegini",
        "Keivan Rezaei",
        "Hamid Eghbalzadeh",
        "S. Feizi"
      ],
      "abstract": "Fact-checking tabular data is essential for ensuring the accuracy of structured information. However, existing methods often rely on black-box models with opaque reasoning. We introduce RePanda, a structured fact verification approach that translates claims into executable pandas queries, enabling interpretable and verifiable reasoning. To train RePanda, we construct PanTabFact, a structured dataset derived from the TabFact train set, where claims are paired with executable queries generated using DeepSeek-Chat and refined through automated error correction. Fine-tuning DeepSeek-coder-7B-instruct-v1.5 on PanTabFact, RePanda achieves 84.09% accuracy on the TabFact test set. To evaluate Out-of-Distribution (OOD) generalization, we interpret question-answer pairs from WikiTableQuestions as factual claims and refer to this dataset as WikiFact. Without additional fine-tuning, RePanda achieves 84.72% accuracy on WikiFact, significantly outperforming all other baselines and demonstrating strong OOD robustness. Notably, these results closely match the zero-shot performance of DeepSeek-Chat (671B), indicating that our fine-tuning approach effectively distills structured reasoning from a much larger model into a compact, locally executable 7B model. Beyond fact verification, RePanda extends to tabular question answering by generating executable queries that retrieve precise answers. To support this, we introduce PanWiki, a dataset mapping WikiTableQuestions to pandas queries. Fine-tuning on PanWiki, RePanda achieves 75.1% accuracy in direct answer retrieval. These results highlight the effectiveness of structured execution-based reasoning for tabular verification and question answering. We have publicly released the dataset on Hugging Face at datasets/AtoosaChegini/PanTabFact.",
      "arxiv_url": "https://arxiv.org/abs/2503.11921",
      "pdf_url": "https://arxiv.org/pdf/2503.11921",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "9bcf24ab745c4cbf1db5a15a05f670b6891efb01",
      "title": "Toward Automatic Discovery of a Canine Phonetic Alphabet",
      "authors": [
        "Theron Wang",
        "Xingyuan Li",
        "Hridayesh Lekhak",
        "Tuan M. Dang",
        "Mengyue Wu",
        "Ke Zhu"
      ],
      "abstract": "Dogs communicate intelligently through vo-calizations, yet the phonetic properties of their communication remain poorly understood. This paper introduces an iterative algorithm inspired by human phonetic discovery, leveraging minimal pairs to identify distinct canine cognitive vocal units and construct a comprehensive phonetic alphabet. Additionally, the algorithm derives canine vocal pattern that exhibit structured correlations with specific environments and activities, suggesting potential meaningful communicative patterns. Our approach provides a novel framework for analyzing non-human vocalization systems and offers potential applications beyond canines to other animal species, advancing the study of cross-species communication.",
      "arxiv_url": "https://www.semanticscholar.org/paper/9bcf24ab745c4cbf1db5a15a05f670b6891efb01",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.15551",
      "title": "Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack",
      "authors": [
        "Murong Yue",
        "Ziyu Yao"
      ],
      "abstract": "Batch prompting, which combines a batch of multiple queries sharing the same context in one inference, has emerged as a promising solution to reduce inference costs. However, our study reveals a significant security vulnerability in batch prompting: malicious users can inject attack instructions into a batch, leading to unwanted interference across all queries, which can result in the inclusion of harmful content, such as phishing links, or the disruption of logical reasoning. In this paper, we construct BATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of two types and 8k batch instances, to study the batch prompting vulnerability systematically. Our evaluation of both closed-source and open-weight LLMs demonstrates that all LLMs are susceptible to batch-prompting attacks. We then explore multiple defending approaches. While the prompting-based defense shows limited effectiveness for smaller LLMs, the probing-based approach achieves about 95% accuracy in detecting attacks. Additionally, we perform a mechanistic analysis to understand the attack and identify attention heads that are responsible for it.",
      "arxiv_url": "https://arxiv.org/abs/2503.15551",
      "pdf_url": "https://arxiv.org/pdf/2503.15551",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19548",
      "title": "When Large Language Models Meet Speech: A Survey on Integration Approaches",
      "authors": [
        "Zhengdong Yang",
        "Shuichiro Shimizu",
        "Yahan Yu",
        "Chenhui Chu"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have spurred interest in expanding their application beyond text-based tasks. A large number of studies have explored integrating other modalities with LLMs, notably speech modality, which is naturally related to text. This paper surveys the integration of speech with LLMs, categorizing the methodologies into three primary approaches: text-based, latent-representation-based, and audio-token-based integration. We also demonstrate how these methods are applied across various speech-related applications and highlight the challenges in this field to offer inspiration for",
      "arxiv_url": "https://arxiv.org/abs/2502.19548",
      "pdf_url": "https://arxiv.org/pdf/2502.19548",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.12941",
      "title": "HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of Multimodal Large Language Model",
      "authors": [
        "Haiyang Guo",
        "Fanhu Zeng",
        "Ziwei Xiang",
        "Fei Zhu",
        "Da-Han Wang",
        "Xu-Yao Zhang",
        "Cheng-Lin Liu"
      ],
      "abstract": "Instruction tuning is widely used to improve a pre-trained Multimodal Large Language Model (MLLM) by training it on curated task-specific datasets, enabling better comprehension of human instructions. However, it is infeasible to collect all possible instruction datasets simultaneously in real-world scenarios. Thus, enabling MLLM with continual instruction tuning is essential for maintaining their adaptability. However, existing methods often trade off memory efficiency for performance gains, significantly compromising overall efficiency. In this paper, we propose a task-specific expansion and task-general fusion framework based on the variations in Centered Kernel Alignment (CKA) similarity across different model layers when trained on diverse datasets. Furthermore, we analyze the information leakage present in the existing benchmark and propose a new and more challenging benchmark to rationally evaluate the performance of different methods. Comprehensive experiments showcase a significant performance improvement of our method compared to existing state-of-the-art methods. Code and dataset are released at https://github.com/Ghy0501/HiDe-LLaVA.",
      "arxiv_url": "https://arxiv.org/abs/2503.12941",
      "pdf_url": "https://arxiv.org/pdf/2503.12941",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14297",
      "title": "Cross-Lingual Optimization for Language Transfer in Large Language Models",
      "authors": [
        "Jungseob Lee",
        "Seongtae Hong",
        "Hyeonseok Moon",
        "Heu-Jeoung Lim"
      ],
      "abstract": "Adapting large language models to other languages typically employs supervised fine-tuning (SFT) as a standard approach. However, it often suffers from an overemphasis on English performance, a phenomenon that is especially pronounced in data-constrained environments. To overcome these challenges, we propose \\textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an English-centric LLM to a target language while preserving its English capabilities. CLO utilizes publicly available English SFT data and a translation model to enable cross-lingual transfer. We conduct experiments using five models on six languages, each possessing varying levels of resource. Our results show that CLO consistently outperforms SFT in both acquiring target language proficiency and maintaining English performance. Remarkably, in low-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400 samples, demonstrating that CLO can achieve better performance with less data. Furthermore, we find that SFT is particularly sensitive to data quantity in medium and low-resource languages, whereas CLO remains robust. Our comprehensive analysis emphasizes the limitations of SFT and incorporates additional training strategies in CLO to enhance efficiency.",
      "arxiv_url": "https://arxiv.org/abs/2505.14297",
      "pdf_url": "https://arxiv.org/pdf/2505.14297",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.03266",
      "title": "LexGenie: Automated Generation of Structured Reports for European Court of Human Rights Case Law",
      "authors": [
        "Santosh T.Y.S.S",
        "Mahmoud Aly",
        "O. Ichim",
        "Matthias Grabmair"
      ],
      "abstract": "Analyzing large volumes of case law to uncover evolving legal principles, across multiple cases, on a given topic is a demanding task for legal professionals. Structured topical reports provide an effective solution by summarizing key issues, principles, and judgments, enabling comprehensive legal analysis on a particular topic. While prior works have advanced query-based individual case summarization, none have extended to automatically generating multi-case structured reports. To address this, we introduce LexGenie, an automated LLM-based pipeline designed to create structured reports using the entire body of case law on user-specified topics within the European Court of Human Rights jurisdiction. LexGenie retrieves, clusters, and organizes relevant passages by topic to generate a structured outline and cohesive content for each section. Expert evaluation confirms LexGenie's utility in producing structured reports that enhance efficient, scalable legal analysis.",
      "arxiv_url": "https://arxiv.org/abs/2503.03266",
      "pdf_url": "https://arxiv.org/pdf/2503.03266",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.14370",
      "title": "Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits",
      "authors": [
        "Amrit Poudel",
        "Yifan Ding",
        "Jurgen Pfeffer",
        "Tim Weninger"
      ],
      "abstract": "Search engines play a crucial role as digital gatekeepers, shaping the visibility of Web and social media content through algorithmic curation. This study investigates how search engines like Google selectively promotes or suppresses certain hashtags and subreddits, impacting the information users encounter. By comparing search engine results with nonsampled data from Reddit and Twitter/X, we reveal systematic biases in content visibility. Google's algorithms tend to suppress subreddits and hashtags related to sexually explicit material, conspiracy theories, advertisements, and cryptocurrencies, while promoting content associated with higher engagement. These findings suggest that Google's gatekeeping practices influence public discourse by curating the social media narratives available to users.",
      "arxiv_url": "https://arxiv.org/abs/2506.14370",
      "pdf_url": "https://arxiv.org/pdf/2506.14370",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15079",
      "title": "Can Hallucination Correction Improve Video-Language Alignment?",
      "authors": [
        "Lingjun Zhao",
        "Mingyang Xie",
        "Paola Cascante-Bonilla",
        "Hal Daum'e",
        "Kwonjoon Lee"
      ],
      "abstract": "Large Vision-Language Models often generate hallucinated content that is not grounded in its visual inputs. While prior work focuses on mitigating hallucinations, we instead explore leveraging hallucination correction as a training objective to improve video-language alignment. We introduce HACA, a self-training framework learning to correct hallucinations in descriptions that do not align with the video content. By identifying and correcting inconsistencies, HACA enhances the model's ability to align video and textual representations for spatio-temporal reasoning. Our experimental results show consistent gains in video-caption binding and text-to-video retrieval tasks, demonstrating that hallucination correction-inspired tasks serve as an effective strategy for improving vision and language alignment.",
      "arxiv_url": "https://arxiv.org/abs/2502.15079",
      "pdf_url": "https://arxiv.org/pdf/2502.15079",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "9c3631b7049bfd56537a7925d21021049db58da3",
      "title": "KE-MHISTO: Towards a Multilingual Historical Knowledge Extraction Benchmark for Addressing the Long-Tail Problem",
      "authors": [
        "Arianna Graciotti",
        "L. piano",
        "Nicolas Lazzari",
        "Enrico Daga",
        "Rocco Tripodi",
        "Valentina Presutti",
        "Livio Pompianu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/9c3631b7049bfd56537a7925d21021049db58da3",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.07527",
      "title": "Supervised Optimism Correction: Be Confident When LLMs Are Sure",
      "authors": [
        "Junjie Zhang",
        "Rushuai Yang",
        "Shunyu Liu",
        "Ting-En Lin",
        "Fei Huang",
        "Yi Chen",
        "Yongbin Li",
        "Dacheng Tao"
      ],
      "abstract": "In this work, we establish a novel theoretical connection between supervised fine-tuning and offline reinforcement learning under the token-level Markov decision process, revealing that large language models indeed learn an implicit $Q$-function for inference. Through this theoretical lens, we demonstrate that the widely used beam search method suffers from unacceptable over-optimism, where inference errors are inevitably amplified due to inflated $Q$-value estimations of suboptimal steps. To address this limitation, we propose Supervised Optimism Correction(SOC), which introduces a simple yet effective auxiliary loss for token-level $Q$-value estimations during supervised fine-tuning. Specifically, the auxiliary loss employs implicit value regularization to boost model confidence in expert-demonstrated responses, thereby suppressing over-optimism toward insufficiently supervised responses. Extensive experiments on mathematical reasoning benchmarks, including GSM8K, MATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search across a series of open-source models.",
      "arxiv_url": "https://arxiv.org/abs/2504.07527",
      "pdf_url": "https://arxiv.org/pdf/2504.07527",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "9c4ecfe594be92e52994c13056acd15cc3c523e8",
      "title": "From English to Second Language Mastery: Enhancing LLMs with Cross-Lingual Continued Instruction Tuning",
      "authors": [
        "Linjuan Wu",
        "Haoran Wei",
        "Baosong Yang",
        "Weiming Lu"
      ],
      "abstract": "Supervised Fine-Tuning (SFT) with translated instruction data effectively adapts Large Language Models (LLMs) from English to non-English languages. We introduce Cross-Lingual Continued Instruction Tuning (X-CIT), which fully leverages translation-based parallel instruction data to enhance cross-lingual adapt-ability. X-CIT emulates the human process of second language acquisition and is guided by Chomsky’s Principles and Parameters Theory. It first fine-tunes the LLM on English instruction data to establish foundational capabilities (i.e. Principles), then continues with target language translation and customized chat-instruction data to adjust \"parameters\" specific to the target language. This chat-instruction data captures alignment information in translated parallel data, guiding the model to initially think and respond in its native language before transitioning to the target language. To further mimic human learning progression, we incorporate Self-Paced Learning (SPL) during continued training, allowing the model to advance from simple to complex tasks. Implemented on Llama-2-7B across five languages, X-CIT was evaluated against three objective benchmarks and an LLM-as-a-judge benchmark, improving the strongest baseline by an average of 1.97% and 8.2% in these two benchmarks, respectively.",
      "arxiv_url": "https://www.semanticscholar.org/paper/9c4ecfe594be92e52994c13056acd15cc3c523e8",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20730",
      "title": "DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking",
      "authors": [
        "Zhuoqun Li",
        "Haiyang Yu",
        "Xuanang Chen",
        "Hongyu Lin",
        "Yaojie Lu",
        "Fei Huang",
        "Xianpei Han",
        "Yongbin Li",
        "Le Sun"
      ],
      "abstract": "Designing solutions for complex engineering challenges is crucial in human production activities. However, previous research in the retrieval-augmented generation (RAG) field has not sufficiently addressed tasks related to the design of complex engineering solutions. To fill this gap, we introduce a new benchmark, SolutionBench, to evaluate a system's ability to generate complete and feasible solutions for engineering problems with multiple complex constraints. To further advance the design of complex engineering solutions, we propose a novel system, SolutionRAG, that leverages the tree-based exploration and bi-point thinking mechanism to generate reliable solutions. Extensive experimental results demonstrate that SolutionRAG achieves state-of-the-art (SOTA) performance on the SolutionBench, highlighting its potential to enhance the automation and reliability of complex engineering solution design in real-world applications.",
      "arxiv_url": "https://arxiv.org/abs/2502.20730",
      "pdf_url": "https://arxiv.org/pdf/2502.20730",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-02-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.07030",
      "title": "UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations",
      "authors": [
        "Fengran Mo",
        "Yifan Gao",
        "Chuan Meng",
        "Xin Liu",
        "Zhuofeng Wu",
        "Kelong Mao",
        "Zhengyang Wang",
        "Pei Chen",
        "Zheng Li",
        "Xian Li",
        "Bing Yin",
        "Menghan Jiang"
      ],
      "abstract": "The rapid advancement of conversational search systems revolutionizes how information is accessed by enabling the multi-turn interaction between the user and the system. Existing conversational search systems are usually built with two different models. This separation restricts the system from leveraging the intrinsic knowledge of the models simultaneously, which cannot ensure the effectiveness of retrieval benefiting the generation. The existing studies for developing unified models cannot fully address the aspects of understanding conversational context, managing retrieval independently, and generating responses. In this paper, we explore how to unify dense retrieval and response generation for large language models in conversation. We conduct joint fine-tuning with different objectives and design two mechanisms to reduce the inconsistency risks while mitigating data discrepancy. The evaluations on five conversational search datasets demonstrate that our unified model can mutually improve both tasks and outperform the existing baselines.",
      "arxiv_url": "https://arxiv.org/abs/2507.07030",
      "pdf_url": "https://arxiv.org/pdf/2507.07030",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.14432",
      "title": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play",
      "authors": [
        "Wei-Wen Fang",
        "Yang Zhang",
        "Kaizhi Qian",
        "James Glass",
        "Yada Zhu"
      ],
      "abstract": "Large language models (LLMs) are increasingly integrated with specialized external tools, yet many tasks demand zero-shot tool usage with minimal or noisy documentation. Existing solutions rely on manual rewriting or labeled data for validation, making them inapplicable in true zero-shot settings. To address these challenges, we propose PLAY2PROMPT, an automated framework that systematically\"plays\"with each tool to explore its input-output behaviors. Through this iterative trial-and-error process, PLAY2PROMPT refines tool documentation and generates usage examples without any labeled data. These examples not only guide LLM inference but also serve as validation to further enhance tool utilization. Extensive experiments on real-world tasks demonstrate that PLAY2PROMPT significantly improves zero-shot tool performance across both open and closed models, offering a scalable and effective solution for domain-specific tool integration.",
      "arxiv_url": "https://arxiv.org/abs/2503.14432",
      "pdf_url": "https://arxiv.org/pdf/2503.14432",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21242",
      "title": "Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings",
      "authors": [
        "Gunjan Balde",
        "Soumyadeep Roy",
        "Mainack Mondal",
        "Niloy Ganguly"
      ],
      "abstract": "Large Language Models (LLMs) recently achieved great success in medical text summarization by simply using in-context learning. However, these recent efforts do not perform fine-grained evaluations under difficult settings where LLMs might fail. They typically report performance scores over the entire dataset. Through our benchmarking study, we show that LLMs show a significant performance drop for data points with high concentration of out-of-vocabulary (OOV) words or with high novelty. Vocabulary adaptation is an intuitive solution to this vocabulary mismatch issue where the LLM vocabulary gets updated with certain expert domain (here, medical) words or subwords. An interesting finding from our study is that Llama-3.1, even with a vocabulary size of around 128K tokens, still faces over-fragmentation issue with medical words. To that end, we show vocabulary adaptation helps improve the LLM summarization performance even in difficult settings. Through extensive experimentation of multiple vocabulary adaptation strategies, two continual pretraining strategies, and three benchmark medical summarization datasets, we gain valuable insights into the role of vocabulary adaptation strategies for customizing LLMs to the medical domain. We also performed a human evaluation study with medical experts where they found that vocabulary adaptation results in more relevant and faithful summaries. Our codebase is made publicly available at https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark.",
      "arxiv_url": "https://arxiv.org/abs/2505.21242",
      "pdf_url": "https://arxiv.org/pdf/2505.21242",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14100",
      "title": "Towards Context-Robust LLMs: A Gated Representation Fine-tuning Approach",
      "authors": [
        "Shenglai Zeng",
        "Pengfei He",
        "Kai Guo",
        "Tianqi Zheng",
        "Hanqing Lu",
        "Yue Xing",
        "Hui Liu"
      ],
      "abstract": "Large Language Models (LLMs) enhanced with external contexts, such as through retrieval-augmented generation (RAG), often face challenges in handling imperfect evidence. They tend to over-rely on external knowledge, making them vulnerable to misleading and unhelpful contexts. To address this, we propose the concept of context-robust LLMs, which can effectively balance internal knowledge with external context, similar to human cognitive processes. Specifically, context-robust LLMs should rely on external context only when lacking internal knowledge, identify contradictions between internal and external knowledge, and disregard unhelpful contexts. To achieve this goal, we introduce Grft, a lightweight and plug-and-play gated representation fine-tuning approach. Grft consists of two key components: a gating mechanism to detect and filter problematic inputs, and low-rank representation adapters to adjust hidden representations. By training a lightweight intervention function with only 0.0004\\% of model size on fewer than 200 examples, Grft can effectively adapt LLMs towards context-robust behaviors.",
      "arxiv_url": "https://arxiv.org/abs/2502.14100",
      "pdf_url": "https://arxiv.org/pdf/2502.14100",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06987",
      "title": "Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors",
      "authors": [
        "Senqi Yang",
        "Dongyu Zhang",
        "Jing Ren",
        "Ziqi Xu",
        "Xiuzhen Zhang",
        "Yiliao Song",
        "Hongfei Lin",
        "Feng Xia"
      ],
      "abstract": "Metaphors are pervasive in communication, making them crucial for natural language processing (NLP). Previous research on automatic metaphor processing predominantly relies on training data consisting of English samples, which often reflect Western European or North American biases. This cultural skew can lead to an overestimation of model performance and contributions to NLP progress. However, the impact of cultural bias on metaphor processing, particularly in multimodal contexts, remains largely unexplored. To address this gap, we introduce MultiMM, a Multicultural Multimodal Metaphor dataset designed for cross-cultural studies of metaphor in Chinese and English. MultiMM consists of 8,461 text-image advertisement pairs, each accompanied by fine-grained annotations, providing a deeper understanding of multimodal metaphors beyond a single cultural domain. Additionally, we propose Sentiment-Enriched Metaphor Detection (SEMD), a baseline model that integrates sentiment embeddings to enhance metaphor comprehension across cultural backgrounds. Experimental results validate the effectiveness of SEMD on metaphor detection and sentiment analysis tasks. We hope this work increases awareness of cultural bias in NLP research and contributes to the development of fairer and more inclusive language models. Our dataset and code are available at https://github.com/DUTIR-YSQ/MultiMM.",
      "arxiv_url": "https://arxiv.org/abs/2506.06987",
      "pdf_url": "https://arxiv.org/pdf/2506.06987",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "9cd5f245a430d350df20523120547bedc286cc61",
      "title": "Make Imagination Clearer! Stable Diffusion-based Visual Imagination for Multimodal Machine Translation",
      "authors": [
        "Andong Chen",
        "Yuchen Song",
        "Kehai Chen",
        "Xuefeng Bai",
        "Muyun Yang",
        "Liqiang Nie",
        "Jie Liu",
        "Tiejun Zhao",
        "Min Zhang"
      ],
      "abstract": "Visual information has been introduced for enhancing machine translation (MT), and its effectiveness heavily relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we propose a stable diffusion-based imagination network integrated into a multimodal large language model (MLLM) to explicitly generate an image for each source sentence, thereby advancing multimodal MT. Particularly, we build heuristic feedback with reinforcement learning to ensure the consistency of the generated image with the source sentence without the supervision of visual information, which breaks the high-cost bottleneck of image annotation in MT. Furthermore, the proposed method enables imaginative visual information to be integrated into text-only MT in addition to multimodal MT. Experimental results show that our model significantly outperforms existing multimodal MT and text-only MT, especially achieving an average improvement of more than 12 BLEU points on Multi30K and MSCOCO multimodal MT benchmarks. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/9cd5f245a430d350df20523120547bedc286cc61",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01622",
      "title": "DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards Meaningful LLM Evaluation",
      "authors": [
        "Eliya Habba",
        "Ofir Arviv",
        "Itay Itzhak",
        "Yotam Perlitz",
        "Elron Bandel",
        "Leshem Choshen",
        "Michal Shmueli-Scheuer",
        "Gabriel Stanovsky"
      ],
      "abstract": "Recent work found that LLMs are sensitive to a wide range of arbitrary prompt dimensions, including the type of delimiters, answer enumerators, instruction wording, and more. This throws into question popular single-prompt evaluation practices. We present DOVE (Dataset Of Variation Evaluation) a large-scale dataset containing prompt perturbations of various evaluation benchmarks. In contrast to previous work, we examine LLM sensitivity from an holistic perspective, and assess the joint effects of perturbations along various dimensions, resulting in thousands of perturbations per instance. We evaluate several model families against DOVE, leading to several findings, including efficient methods for choosing well-performing prompts, observing that few-shot examples reduce sensitivity, and identifying instances which are inherently hard across all perturbations. DOVE consists of more than 250M prompt perturbations and model outputs, which we make publicly available to spur a community-wide effort toward meaningful, robust, and efficient evaluation. Browse the data, contribute, and more: https://slab-nlp.github.io/DOVE/",
      "arxiv_url": "https://arxiv.org/abs/2503.01622",
      "pdf_url": "https://arxiv.org/pdf/2503.01622",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.06914",
      "title": "The Distracting Effect: Understanding Irrelevant Passages in RAG",
      "authors": [
        "Chen Amiraz",
        "Florin Cuconasu",
        "Simone Filice",
        "Zohar S. Karnin"
      ],
      "abstract": "A well-known issue with Retrieval Augmented Generation (RAG) is that retrieved passages that are irrelevant to the query sometimes distract the answer-generating LLM, causing it to provide an incorrect response. In this paper, we shed light on this core issue and formulate the distracting effect of a passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the distracting effect of a passage and demonstrate its robustness across LLMs. Our research introduces novel methods for identifying and using hard distracting passages to improve RAG systems. By fine-tuning LLMs with these carefully selected distracting passages, we achieve up to a 7.5% increase in answering accuracy compared to counterparts fine-tuned on conventional RAG datasets. Our contribution is two-fold: first, we move beyond the simple binary classification of irrelevant passages as either completely unrelated vs. distracting, and second, we develop and analyze multiple methods for finding hard distracting passages. To our knowledge, no other research has provided such a comprehensive framework for identifying and utilizing hard distracting passages.",
      "arxiv_url": "https://arxiv.org/abs/2505.06914",
      "pdf_url": "https://arxiv.org/pdf/2505.06914",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-05-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.09723",
      "title": "QueryAttack: Jailbreaking Aligned Large Language Models Using Structured Non-natural Query Language",
      "authors": [
        "Qingsong Zou",
        "Jingyu Xiao",
        "Qing Li",
        "Zhi Yan",
        "Yuhang Wang",
        "Li Xu",
        "Wenxuan Wang",
        "Kuofeng Gao",
        "Ruoyu Li",
        "Yong Jiang"
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated remarkable potential in the field of natural language processing. Unfortunately, LLMs face significant security and ethical risks. Although techniques such as safety alignment are developed for defense, prior researches reveal the possibility of bypassing such defenses through well-designed jailbreak attacks. In this paper, we propose QueryAttack, a novel framework to examine the generalizability of safety alignment. By treating LLMs as knowledge databases, we translate malicious queries in natural language into structured non-natural query language to bypass the safety alignment mechanisms of LLMs. We conduct extensive experiments on mainstream LLMs, and the results show that QueryAttack not only can achieve high attack success rates (ASRs), but also can jailbreak various defense methods. Furthermore, we tailor a defense method against QueryAttack, which can reduce ASR by up to $64\\%$ on GPT-4-1106. Our code is available at https://github.com/horizonsinzqs/QueryAttack.",
      "arxiv_url": "https://arxiv.org/abs/2502.09723",
      "pdf_url": "https://arxiv.org/pdf/2502.09723",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.01377",
      "title": "Improving Medical Large Vision-Language Models with Abnormal-Aware Feedback",
      "authors": [
        "Yucheng Zhou",
        "Lingran Song",
        "Jianbing Shen"
      ],
      "abstract": "Existing Medical Large Vision-Language Models (Med-LVLMs), encapsulating extensive medical knowledge, demonstrate excellent capabilities in understanding medical images. However, there remain challenges in visual localization in medical images, which is crucial for abnormality detection and interpretation. To address these issues, we propose a novel UMed-LVLM designed to unveil medical abnormalities. Specifically, we collect a Medical Abnormalities Unveiling (MAU) dataset and propose a two-stage training method for UMed-LVLM training. To collect MAU dataset, we propose a prompt method utilizing the GPT-4V to generate diagnoses based on identified abnormal areas in medical images. Moreover, the two-stage training method includes Abnormal-Aware Instruction Tuning and Abnormal-Aware Rewarding, comprising Relevance Reward, Abnormal Localization Reward and Vision Relevance Reward. Experimental results demonstrate that our UMed-LVLM significantly outperforms existing Med-LVLMs in identifying and understanding medical abnormalities, achieving a 58% improvement over the baseline. In addition, this work shows that enhancing the abnormality detection capabilities of Med-LVLMs significantly improves their understanding of medical images and generalization capability.",
      "arxiv_url": "https://arxiv.org/abs/2501.01377",
      "pdf_url": "https://arxiv.org/pdf/2501.01377",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18343",
      "title": "Model Editing with Graph-Based External Memory",
      "authors": [
        "Yash Kumar Atri",
        "Ahmed M. Alaa",
        "Thomas Hartvigsen"
      ],
      "abstract": "Large language models (LLMs) have revolutionized natural language processing, yet their practical utility is often limited by persistent issues of hallucinations and outdated parametric knowledge. Although post-training model editing offers a pathway for dynamic updates, existing methods frequently suffer from overfitting and catastrophic forgetting. To tackle these challenges, we propose a novel framework that leverages hyperbolic geometry and graph neural networks for precise and stable model edits. We introduce HYPE (HYperbolic Parameter Editing), which comprises three key components: (i) Hyperbolic Graph Construction, which uses Poincar\\'e embeddings to represent knowledge triples in hyperbolic space, preserving hierarchical relationships and preventing unintended side effects by ensuring that edits to parent concepts do not inadvertently affect child concepts; (ii) M\\\"obius-Transformed Updates, which apply hyperbolic addition to propagate edits while maintaining structural consistency within the hyperbolic manifold, unlike conventional Euclidean updates that distort relational distances; and (iii) Dual Stabilization, which combines gradient masking and periodic GNN parameter resetting to prevent catastrophic forgetting by focusing updates on critical parameters and preserving long-term knowledge. Experiments on CounterFact, CounterFact+, and MQuAKE with GPT-J and GPT2-XL demonstrate that HYPE significantly enhances edit stability, factual accuracy, and multi-hop reasoning.",
      "arxiv_url": "https://arxiv.org/abs/2505.18343",
      "pdf_url": "https://arxiv.org/pdf/2505.18343",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.16853",
      "title": "Imagine to Hear: Auditory Knowledge Generation can be an Effective Assistant for Language Models",
      "authors": [
        "Suho Yoo",
        "Hyunjong Ok",
        "Jaeho Lee"
      ],
      "abstract": "Language models pretrained on text-only corpora often struggle with tasks that require auditory commonsense knowledge. Previous work addresses this problem by augmenting the language model to retrieve knowledge from external audio databases. This approach has several limitations, such as the potential lack of relevant audio in databases and the high costs associated with constructing the databases. To address these issues, we propose Imagine to Hear, a novel approach that dynamically generates auditory knowledge using generative models. Our framework detects multiple audio-related textual spans from the given prompt and generates corresponding auditory knowledge. We develop several mechanisms to efficiently process multiple auditory knowledge, including a CLAP-based rejection sampler and a language-audio fusion module. Our experiments show that our method achieves state-of-the-art performance on AuditoryBench without relying on external databases, highlighting the effectiveness of our generation-based approach.",
      "arxiv_url": "https://arxiv.org/abs/2503.16853",
      "pdf_url": "https://arxiv.org/pdf/2503.16853",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.18085",
      "title": "Temporal Relation Extraction in Clinical Texts: A Span-based Graph Transformer Approach",
      "authors": [
        "Rochana Chaturvedi",
        "Peyman Baghershahi",
        "Sourav Medya",
        "Barbara Di Eugenio"
      ],
      "abstract": "Temporal information extraction from unstructured text is essential for contextualizing events and deriving actionable insights, particularly in the medical domain. We address the task of extracting clinical events and their temporal relations using the well-studied I2B2 2012 Temporal Relations Challenge corpus. This task is inherently challenging due to complex clinical language, long documents, and sparse annotations. We introduce GRAPHTREX, a novel method integrating span-based entity-relation extraction, clinical large pre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT) to capture local and global dependencies. Our HGT component facilitates information propagation across the document through innovative global landmarks that bridge distant entities. Our method improves the state-of-the-art with 5.5% improvement in the tempeval $F_1$ score over the previous best and up to 8.9% improvement on long-range relations, which presents a formidable challenge. We further demonstrate generalizability by establishing a strong baseline on the E3C corpus. This work not only advances temporal information extraction but also lays the groundwork for improved diagnostic and prognostic models through enhanced temporal reasoning.",
      "arxiv_url": "https://arxiv.org/abs/2503.18085",
      "pdf_url": "https://arxiv.org/pdf/2503.18085",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14910",
      "title": "TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis",
      "authors": [
        "Yu Zhang",
        "Wenxiang Guo",
        "Changhao Pan",
        "Dongyu Yao",
        "Zhiyuan Zhu",
        "Ziyue Jiang",
        "Yuhan Wang",
        "Tao Jin",
        "Zhou Zhao"
      ],
      "abstract": "Customizable multilingual zero-shot singing voice synthesis (SVS) has various potential applications in music composition and short video dubbing. However, existing SVS models overly depend on phoneme and note boundary annotations, limiting their robustness in zero-shot scenarios and producing poor transitions between phonemes and notes. Moreover, they also lack effective multi-level style control via diverse prompts. To overcome these challenges, we introduce TCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer and style control based on various prompts. TCSinger 2 mainly includes three key modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration, extends content embedding, and applies masking to the boundaries to enable smooth transitions. 2) Custom Audio Encoder, uses contrastive learning to extract aligned representations from singing, speech, and textual prompts. 3) Flow-based Custom Transformer, leverages Cus-MOE, with F0 supervision, enhancing both the synthesis quality and style modeling of the generated singing voice. Experimental results show that TCSinger 2 outperforms baseline models in both subjective and objective metrics across multiple related tasks. Singing voice samples are available at https://aaronz345.github.io/TCSinger2Demo/.",
      "arxiv_url": "https://arxiv.org/abs/2505.14910",
      "pdf_url": "https://arxiv.org/pdf/2505.14910",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01711",
      "title": "MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation Alignment",
      "authors": [
        "Weicong Qin",
        "Yi Xu",
        "Weijie Yu",
        "Chenglei Shen",
        "Ming He",
        "Jianpin Fan",
        "Xiao Zhang",
        "Jun Xu"
      ],
      "abstract": "Personalized product search aims to retrieve and rank items that match users' preferences and search intent. Despite their effectiveness, existing approaches typically assume that users' query fully captures their real motivation. However, our analysis of a real-world e-commerce platform reveals that users often engage in relevant consultations before searching, indicating they refine intents through consultations based on motivation and need. The implied motivation in consultations is a key enhancing factor for personalized search. This unexplored area comes with new challenges including aligning contextual motivations with concise queries, bridging the category-text gap, and filtering noise within sequence history. To address these, we propose a Motivation-Aware Personalized Search (MAPS) method. It embeds queries and consultations into a unified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE) to prioritize critical semantics, and introduces dual alignment: (1) contrastive learning aligns consultations, reviews, and product features; (2) bidirectional attention integrates motivation-aware embeddings with user preferences. Extensive experiments on real and synthetic data show MAPS outperforms existing methods in both retrieval and ranking tasks.",
      "arxiv_url": "https://arxiv.org/abs/2503.01711",
      "pdf_url": "https://arxiv.org/pdf/2503.01711",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "9e179068e5eaa4b1036a154dfd3b9e47cf804827",
      "title": "Introducing Graph Context into Language Models through Parameter-Efficient Fine-Tuning for Lexical Relation Mining",
      "authors": [
        "Jingwen Sun",
        "Zhiyi Tian",
        "Yu He",
        "Jingwei Sun",
        "Guangzhong Sun"
      ],
      "abstract": "Lexical relation refers to the way words are re-lated within a language. Prior work has demonstrated that pretrained language models (PLMs) can effectively mine lexical relations between word pairs. However, they overlook the potential of graph structures composed of lexical relations, which can be integrated with the semantic knowledge of PLMs. In this work, we propose a parameter-efficient fine-tuning method through graph context, which integrates graph features and semantic representations for lexical relation classification (LRC) and lexical entailment (LE) tasks. Our experiments show that graph features can help PLMs better understand more complex lexical relations, establishing a new state-of-the-art for LRC and LE. Finally, we perform an error analysis, identifying the bottlenecks of language models in lexical relation mining tasks and providing insights for future improvements.",
      "arxiv_url": "https://www.semanticscholar.org/paper/9e179068e5eaa4b1036a154dfd3b9e47cf804827",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.17439",
      "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
      "authors": [
        "Zhuoshi Pan",
        "Yu Li",
        "Honglin Lin",
        "Qizhi Pei",
        "Zinan Tang",
        "Wei Wu",
        "Chenlin Ming",
        "H. V. Zhao",
        "Conghui He",
        "Lijun Wu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning capability in solving mathematical problems. However, existing approaches primarily focus on improving the quality of correct training data, e.g., distilling high-quality correct solutions from advanced models, neglecting the value contained in error data, potentially hindering the model's reflective ability. Though some studies attempt to leverage error data, they often involve complex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error nodes. In this work, we propose to enhance LLMs' reasoning ability by Learning from Errors for Mathematical Advancement (LEMMA). LEMMA constructs data consisting of an incorrect solution with an erroneous step and a reflection connection to a correct solution for fine-tuning. Specifically, we systematically analyze the model-generated error types and introduce an error-type grounded mistake augmentation method to collect diverse and representative errors. Correct solutions are either from fixing the errors or generating a fresh start. Through a model-aware smooth reflection connection, the erroneous solution is transferred to the correct one. By fine-tuning on the constructed dataset, the model is able to self-correct errors autonomously within the generation process without relying on external critique models. Experimental results demonstrate that LEMMA achieves significant performance improvements over other strong baselines.",
      "arxiv_url": "https://arxiv.org/abs/2503.17439",
      "pdf_url": "https://arxiv.org/pdf/2503.17439",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "9ebdfed10bc8c959e343072f64a6ad65d931dfe2",
      "title": "DTCRS: Dynamic Tree Construction for Recursive Summarization",
      "authors": [
        "Guanran Luo",
        "Zhongquan Jian",
        "Wentao Qiu",
        "Meihong Wang",
        "Qingqiang Wu"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) mitigates the hallucination issues of large language models (LLMs) by integrating external knowledge. For abstractive questions involving multi-step reasoning, knowledge from multiple sections is often required. To address this issue, recent research has introduced recursive summarization, which constructs a hierarchical sum-mary tree by clustering text chunks, integrating information from various parts of the document to provide evidence for abstractive questions. However, summary trees often contain a large number of redundant summary nodes, which not only increase construction time but may also negatively impact question answering. Moreover, recursive summarization is not suitable for all types of questions. We introduce DTCRS, a method that dynamically generates summary trees based on document structure and query semantics. DTCRS determines whether a summary tree is necessary by analyzing the question type. It then decomposes the question and uses the embeddings of sub-questions as initial cluster centers, reducing redundant summaries while improving the relevance between summaries and the question. Our approach significantly reduces summary tree construction time and achieves substantial improvements across three QA tasks. Additionally, we investigate the applicability of recursive summarization to different question types, providing valuable insights for future research.",
      "arxiv_url": "https://www.semanticscholar.org/paper/9ebdfed10bc8c959e343072f64a6ad65d931dfe2",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24715",
      "title": "CoRet: Improved Retriever for Code Editing",
      "authors": [
        "Fabio Fehr",
        "Prabhu Teja Sivaprasad",
        "Luca Franceschi",
        "Giovanni Zappella"
      ],
      "abstract": "In this paper, we introduce CoRet, a dense retrieval model designed for code-editing tasks that integrates code semantics, repository structure, and call graph dependencies. The model focuses on retrieving relevant portions of a code repository based on natural language queries such as requests to implement new features or fix bugs. These retrieved code chunks can then be presented to a user or to a second code-editing model or agent. To train CoRet, we propose a loss function explicitly designed for repository-level retrieval. On SWE-bench and Long Code Arena's bug localisation datasets, we show that our model substantially improves retrieval recall by at least 15 percentage points over existing models, and ablate the design choices to show their importance in achieving these results.",
      "arxiv_url": "https://arxiv.org/abs/2505.24715",
      "pdf_url": "https://arxiv.org/pdf/2505.24715",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22757",
      "title": "Pre-Training Curriculum for Multi-Token Prediction in Language Models",
      "authors": [
        "Ansar Aynetdinov",
        "Alan Akbik"
      ],
      "abstract": "Multi-token prediction (MTP) is a recently proposed pre-training objective for language models. Rather than predicting only the next token (NTP), MTP predicts the next $k$ tokens at each prediction step, using multiple prediction heads. MTP has shown promise in improving downstream performance, inference speed, and training efficiency, particularly for large models. However, prior work has shown that smaller language models (SLMs) struggle with the MTP objective. To address this, we propose a curriculum learning strategy for MTP training, exploring two variants: a forward curriculum, which gradually increases the complexity of the pre-training objective from NTP to MTP, and a reverse curriculum, which does the opposite. Our experiments show that the forward curriculum enables SLMs to better leverage the MTP objective during pre-training, improving downstream NTP performance and generative output quality, while retaining the benefits of self-speculative decoding. The reverse curriculum achieves stronger NTP performance and output quality, but fails to provide any self-speculative decoding benefits.",
      "arxiv_url": "https://arxiv.org/abs/2505.22757",
      "pdf_url": "https://arxiv.org/pdf/2505.22757",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.15620",
      "title": "Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in Contextual Settings",
      "authors": [
        "Austin Xu",
        "Srijan Bansal",
        "Yifei Ming",
        "Semih Yavuz",
        "Shafiq Joty"
      ],
      "abstract": "The large language model (LLM)-as-judge paradigm has been used to meet the demand for a cheap, reliable, and fast evaluation of model outputs during AI system development and post-deployment monitoring. While judge models -- LLMs finetuned to specialize in assessing and critiquing model outputs -- have been touted as general purpose evaluators, they are typically evaluated only on non-contextual scenarios, such as instruction following. The omission of contextual settings -- those where external information is used as context to generate an output -- is surprising given the increasing prevalence of retrieval-augmented generation (RAG) and summarization use cases. Contextual assessment is uniquely challenging, as evaluation often depends on practitioner priorities, leading to conditional evaluation criteria (e.g., comparing responses based on factuality and then considering completeness if they are equally factual). To address the gap, we propose ContextualJudgeBench, a judge benchmark with 2,000 challenging response pairs across eight splits inspired by real-world contextual evaluation scenarios. We build our benchmark with a multi-pronged data construction pipeline that leverages both existing human annotations and model-based perturbations. Our comprehensive study across 11 judge models and 9 general purpose models, reveals that the contextual information and its assessment criteria present a significant challenge to even state-of-the-art models. For example, OpenAI's o1, the best-performing model, barely reaches 55% consistent accuracy.",
      "arxiv_url": "https://arxiv.org/abs/2503.15620",
      "pdf_url": "https://arxiv.org/pdf/2503.15620",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-03-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "9f7671ff968ce21ccc1ad18bc61d9a094f9544f2",
      "title": "From Perception to Reasoning: Enhancing Vision-Language Models for Mobile UI Understanding",
      "authors": [
        "S. Sravanthi",
        "Ankit Mishra",
        "Debjyoti Mondal",
        "Subhadarshi Panda",
        "Rituraj Singh",
        "Pushpak Bhattacharyya"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/9f7671ff968ce21ccc1ad18bc61d9a094f9544f2",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.04644",
      "title": "Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning with Agentic Tools",
      "authors": [
        "Junde Wu",
        "Jiayuan Zhu",
        "Yuyuan Liu",
        "Min Xu",
        "Yueming Jin"
      ],
      "abstract": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Agentic Reasoning dynamically leverages web search, code execution, and structured memory to address complex problems requiring deep research. A key innovation in our framework is the Mind-Map agent, which constructs a structured knowledge graph to store reasoning context and track logical relationships, ensuring coherence in long reasoning chains with extensive tool usage. Additionally, we conduct a comprehensive exploration of the Web-Search agent, leading to a highly effective search mechanism that surpasses all prior approaches. When deployed on DeepSeek-R1, our method achieves a new state-of-the-art (SOTA) among public models and delivers performance comparable to OpenAI Deep Research, the leading proprietary model in this domain. Extensive ablation studies validate the optimal selection of agentic tools and confirm the effectiveness of our Mind-Map and Web-Search agents in enhancing LLM reasoning. The code is at: https://github.com/theworldofagents/Agentic-Reasoning",
      "arxiv_url": "https://arxiv.org/abs/2502.04644",
      "pdf_url": "https://arxiv.org/pdf/2502.04644",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM",
        "Search Agent"
      ],
      "published_date": "2025-02-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03939",
      "title": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning",
      "authors": [
        "Junqi Gao",
        "Xiang Zou",
        "Ying Ai",
        "Dong Li",
        "Yichen Niu",
        "Biqing Qi",
        "Jianxing Liu"
      ],
      "abstract": "Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external knowledge integration capabilities by explicitly modeling knowledge relationships, thereby improving the factual accuracy and generation quality of Large Language Models (LLMs) in specialized domains. However, existing methods suffer from two inherent limitations: 1) Inefficient Information Aggregation: They rely on a single agent and fixed iterative patterns, making it difficult to adaptively capture multi-level textual, structural, and degree information within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning schemes, which cannot dynamically adjust reasoning depth nor achieve precise semantic correction. To overcome these limitations, we propose Graph Counselor, an GraphRAG method based on multi-agent collaboration. This method uses the Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought, and Execution Agents work together to precisely model complex graph structures and dynamically adjust information extraction strategies, addressing the challenges of multi-level dependency modeling and adaptive reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves the accuracy and semantic consistency of reasoning results through self-reflection and backward reasoning mechanisms. Experiments demonstrate that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, exhibiting higher reasoning accuracy and generalization ability. Our code is available at https://github.com/gjq100/Graph-Counselor.git.",
      "arxiv_url": "https://arxiv.org/abs/2506.03939",
      "pdf_url": "https://arxiv.org/pdf/2506.03939",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07636",
      "title": "SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling",
      "authors": [
        "Haoran Wang",
        "Zhenyu Hou",
        "Yao Wei",
        "Jie Tang",
        "Yuxiao Dong"
      ],
      "abstract": "Large language models (LLMs) have advanced rapidly from conversational problem solving to addressing real-world tasks involving tool use, such as software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex and Cursor, have offered end-to-end automation of the software development process. However, building effective SWE agents remains challenging due to the lack of high-quality training data and effective test cases. To address this issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we develop a robust pipeline to synthesize test cases for patch evaluation. Second, we scale up agent trajectories to construct the training data for building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the SWE-Dev models can achieve top performance among all open SWE agents. Specifically, the success rates of the SWE-Dev 7B and 32B parameter models reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source models. All code, models, and datasets are publicly available at https://github.com/THUDM/SWE-Dev.",
      "arxiv_url": "https://arxiv.org/abs/2506.07636",
      "pdf_url": "https://arxiv.org/pdf/2506.07636",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10460",
      "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond",
      "authors": [
        "Liang Wen",
        "Yunke Cai",
        "Fenrui Xiao",
        "Xin He",
        "Qi An",
        "Zhenyu Duan",
        "Yimin Du",
        "Junchen Liu",
        "Lifu Tang",
        "Xiaowei Lv",
        "Haosheng Zou",
        "Yongchao Deng",
        "Shousheng Jia",
        "Xiangzheng Zhang"
      ],
      "abstract": "This paper introduces Light-R1, an open-source suite for training long reasoning models using reproducible and cost-effective methodology. Given the proprietary nature of data used in the DeepSeek-R1 series, we develop an alternative approach leveraging exclusively public data and models. Our curriculum training progressively increases data difficulty, combined with multi-staged post-training. Our Light-R1-32B model, trained from Qwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math reasoning. Experimental results show that this curriculum approach becomes more effective when distinct, diverse datasets are available for different training stages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on proprietary data) with 3,000 challenging examples from our curriculum dataset yielded state-of-the-art 7B and 14B models, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1. Furthermore, we extend our work by applying GRPO on long reasoning models. Our final Light-R1-14B-DS achieves SOTA performance among 14B models in math, with AIME24&25 scores of 74.0 and 60.2 respectively, surpassing many 32B models and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training, Light-R1-14B-DS demonstrates strong cross-domain generalization. Light-R1 represents a significant advancement in making sophisticated reasoning models more accessible and implementable in real-world applications. Our models, training data and code have been made available at https://github.com/Qihoo360/Light-R1.",
      "arxiv_url": "https://arxiv.org/abs/2503.10460",
      "pdf_url": "https://arxiv.org/pdf/2503.10460",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00493",
      "title": "LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement",
      "authors": [
        "Boyi Kang",
        "Xinfa Zhu",
        "Zihan Zhang",
        "Zhen Ye",
        "Mingshuai Liu",
        "Ziqian Wang",
        "Yike Zhu",
        "Guobin Ma",
        "Jun Chen",
        "Longshuai Xiao",
        "Chao Weng",
        "Wei Xue",
        "Lei Xie"
      ],
      "abstract": "Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area.",
      "arxiv_url": "https://arxiv.org/abs/2503.00493",
      "pdf_url": "https://arxiv.org/pdf/2503.00493",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.05714",
      "title": "How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond",
      "authors": [
        "Chen Huang",
        "Yang Deng",
        "Wenqiang Lei",
        "Jiancheng Lv",
        "Tat-Seng Chua",
        "Xiangji Huang"
      ],
      "abstract": "With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard.",
      "arxiv_url": "https://arxiv.org/abs/2501.05714",
      "pdf_url": "https://arxiv.org/pdf/2501.05714",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.10198",
      "title": "DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation",
      "authors": [
        "Hanghui Guo",
        "Jia Zhu",
        "Shimin Di",
        "Weijie Shi",
        "Zhangze Chen",
        "Jiajie Xu"
      ],
      "abstract": "Dynamic Retrieval-augmented Generation (RAG) has shown great success in mitigating hallucinations in large language models (LLMs) during generation. However, existing dynamic RAG methods face significant limitations in two key aspects: 1) Lack of an effective mechanism to control retrieval triggers, and 2) Lack of effective scrutiny of retrieval content. To address these limitations, we propose an innovative dynamic RAG method, DioR (Adaptive Cognitive Detection and Contextual Retrieval Optimization), which consists of two main components: adaptive cognitive detection and contextual retrieval optimization, specifically designed to determine when retrieval is needed and what to retrieve for LLMs is useful. Experimental results demonstrate that DioR achieves superior performance on all tasks, demonstrating the effectiveness of our work.",
      "arxiv_url": "https://arxiv.org/abs/2504.10198",
      "pdf_url": "https://arxiv.org/pdf/2504.10198",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-04-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14645",
      "title": "Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs",
      "authors": [
        "Yuchen Wu",
        "Liang Ding",
        "Li Shen",
        "D. Tao"
      ],
      "abstract": "Knowledge editing allows for efficient adaptation of large language models (LLMs) to new information or corrections without requiring full retraining. However, prior methods typically focus on either single-language editing or basic multilingual editing, failing to achieve true cross-linguistic knowledge synchronization. To address this, we present a simple and practical state-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE), designed to propagate knowledge from a dominant language to other languages effectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition Instruction Tuning (XE-IT), which fine-tunes the model on a curated parallel dataset to modify in-scope knowledge while preserving unrelated information, and (ii) Target-language Preference Optimization (TL-PO), which applies advanced optimization techniques to ensure consistency across languages, fostering the transfer of updates. Additionally, we contribute a high-quality, cross-lingual dataset, specifically designed to enhance knowledge transfer across languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks show that X-KDE significantly enhances cross-lingual performance, achieving an average improvement of +8.19%, while maintaining high accuracy in monolingual settings.",
      "arxiv_url": "https://arxiv.org/abs/2502.14645",
      "pdf_url": "https://arxiv.org/pdf/2502.14645",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.06157",
      "title": "UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces",
      "authors": [
        "Baining Zhao",
        "Jianjie Fang",
        "Zichao Dai",
        "Ziyou Wang",
        "Jirong Zha",
        "Weichen Zhang",
        "Chen Gao",
        "Yue Wang",
        "Jinqiang Cui",
        "Xinlei Chen",
        "Yong Li"
      ],
      "abstract": "Large multimodal models exhibit remarkable intelligence, yet their embodied cognitive abilities during motion in open-ended urban 3D space remain to be explored. We introduce a benchmark to evaluate whether video-large language models (Video-LLMs) can naturally process continuous first-person visual observations like humans, enabling recall, perception, reasoning, and navigation. We have manually control drones to collect 3D embodied motion video data from real-world cities and simulated environments, resulting in 1.5k video clips. Then we design a pipeline to generate 5.2k multiple-choice questions. Evaluations of 17 widely-used Video-LLMs reveal current limitations in urban embodied cognition. Correlation analysis provides insight into the relationships between different tasks, showing that causal reasoning has a strong correlation with recall, perception, and navigation, while the abilities for counterfactual and associative reasoning exhibit lower correlation with other tasks. We also validate the potential for Sim-to-Real transfer in urban embodiment through fine-tuning.",
      "arxiv_url": "https://arxiv.org/abs/2503.06157",
      "pdf_url": "https://arxiv.org/pdf/2503.06157",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a0a78de905b98bfdad5b638f21dc05f99530a855",
      "title": "Measuring Bias and Agreement in Large Language Model Presupposition Judgments",
      "authors": [
        "Katherine Atwell",
        "Mandy Simons",
        "Malihe Alikhani"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/a0a78de905b98bfdad5b638f21dc05f99530a855",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a1022d405a08e3ace394c3a074b1f9c63a6eeb8e",
      "title": "DynaQuest: A Dynamic Question Answering Dataset Reflecting Real-World Knowledge Updates",
      "authors": [
        "Qian Lin",
        "Junyi Li",
        "Hwee Tou Ng"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/a1022d405a08e3ace394c3a074b1f9c63a6eeb8e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.09988",
      "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits",
      "authors": [
        "Ron Yosef",
        "Moran Yanuka",
        "Yonatan Bitton",
        "Dani Lischinski"
      ],
      "abstract": "Text-guided image editing, fueled by recent advancements in generative AI, is becoming increasingly widespread. This trend highlights the need for a comprehensive framework to verify text-guided edits and assess their quality. To address this need, we introduce EditInspector, a novel benchmark for evaluation of text-guided image edits, based on human annotations collected using an extensive template for edit verification. We leverage EditInspector to evaluate the performance of state-of-the-art (SoTA) vision and language models in assessing edits across various dimensions, including accuracy, artifact detection, visual quality, seamless integration with the image scene, adherence to common sense, and the ability to describe edit-induced changes. Our findings indicate that current models struggle to evaluate edits comprehensively and frequently hallucinate when describing the changes. To address these challenges, we propose two novel methods that outperform SoTA models in both artifact detection and difference caption generation.",
      "arxiv_url": "https://arxiv.org/abs/2506.09988",
      "pdf_url": "https://arxiv.org/pdf/2506.09988",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.21252",
      "title": "Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents",
      "authors": [
        "Tianyi Men",
        "Zhuoran Jin",
        "Pengfei Cao",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "As Multimodal Large Language Models (MLLMs) advance, multimodal agents show promise in real-world tasks like web navigation and embodied intelligence. However, due to limitations in a lack of external feedback, these agents struggle with self-correction and generalization. A promising approach is to use reward models as external feedback, but there is no clear on how to select reward models for agents. Thus, there is an urgent need to build a reward bench targeted at agents. To address these challenges, we propose Agent-RewardBench, a benchmark designed to evaluate reward modeling ability in MLLMs. The benchmark is characterized by three key features: (1) Multiple dimensions and real-world agent scenarios evaluation. It covers perception, planning, and safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the assessment of agent capabilities at the individual steps of a task, providing a more granular view of performance during the planning process; and (3) Appropriately difficulty and high-quality. We carefully sample from 10 diverse models, difficulty control to maintain task challenges, and manual verification to ensure the integrity of the data. Experiments demonstrate that even state-of-the-art multimodal models show limited performance, highlighting the need for specialized training in agent reward modeling. Code is available at github.",
      "arxiv_url": "https://arxiv.org/abs/2506.21252",
      "pdf_url": "https://arxiv.org/pdf/2506.21252",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2509.26210",
      "title": "Dia-Lingle: A Gamified Interface for Dialectal Data Collection",
      "authors": [
        "Jiugeng Sun",
        "R. Sevastjanova",
        "Sina Ahmadi",
        "Rico Sennrich",
        "Mennatallah El-Assady"
      ],
      "abstract": "Dialects suffer from the scarcity of computational textual resources as they exist predominantly in spoken rather than written form and exhibit remarkable geographical diversity. Collecting dialect data and subsequently integrating it into current language technologies present significant obstacles. Gamification has been proven to facilitate remote data collection processes with great ease and on a substantially wider scale. This paper introduces Dia-Lingle, a gamified interface aimed to improve and facilitate dialectal data collection tasks such as corpus expansion and dialect labelling. The platform features two key components: the first challenges users to rewrite sentences in their dialects, identifies them through a classifier and solicits feedback, and the other one asks users to match sentences to their geographical locations. Dia-Lingle combines active learning with gamified difficulty levels, strategically encouraging prolonged user engagement while efficiently enriching the dialect corpus. Usability evaluation shows that our interface demonstrates high levels of user satisfaction. We provide the link to Dia-Lingle: https://dia-lingle.ivia.ch/, and demo video: https://youtu.be/0QyJsB8ym64.",
      "arxiv_url": "https://arxiv.org/abs/2509.26210",
      "pdf_url": "https://arxiv.org/pdf/2509.26210",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-09-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.02095",
      "title": "LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information",
      "authors": [
        "Bowen Ping",
        "Jiali Zeng",
        "Fandong Meng",
        "Shuo Wang",
        "Jie Zhou",
        "Shanghang Zhang"
      ],
      "abstract": "Long-form generation is crucial for academic writing papers and repo-level code generation. Despite this, current models, including GPT-4o, still exhibit unsatisfactory performance. Existing methods that utilize preference learning with outcome supervision often fail to provide detailed feedback for extended contexts. This shortcoming can lead to content that does not fully satisfy query requirements, resulting in issues like length deviations, and diminished quality. In this paper, we propose enhancing long-form generation by incorporating process supervision. We employ Monte Carlo Tree Search to gather stepwise preference pairs, utilizing a global memory pool to maintain consistency. To address the issue of suboptimal candidate selection, we integrate external critiques to refine and improve the quality of the preference pairs. Finally, we apply step-level DPO using the collected stepwise preference pairs. Experimental results show that our method improves length and quality on long-form generation benchmarks, with almost lossless performance on general benchmarks across various model backbones.",
      "arxiv_url": "https://arxiv.org/abs/2502.02095",
      "pdf_url": "https://arxiv.org/pdf/2502.02095",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.03474",
      "title": "Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues",
      "authors": [
        "Varsha Suresh",
        "M. Mughal",
        "C. Theobalt",
        "Vera Demberg"
      ],
      "abstract": "Research in linguistics shows that non-verbal cues, such as gestures, play a crucial role in spoken discourse. For example, speakers perform hand gestures to indicate topic shifts, helping listeners identify transitions in discourse. In this work, we investigate whether the joint modeling of gestures using human motion sequences and language can improve spoken discourse modeling in language models. To integrate gestures into language models, we first encode 3D human motion sequences into discrete gesture tokens using a VQ-VAE. These gesture token embeddings are then aligned with text embeddings through feature alignment, mapping them into the text embedding space. To evaluate the gesture-aligned language model on spoken discourse, we construct text infilling tasks targeting three key discourse cues grounded in linguistic research: discourse connectives, stance markers, and quantifiers. Results show that incorporating gestures enhances marker prediction accuracy across the three tasks, highlighting the complementary information that gestures can offer in modeling spoken discourse. We view this work as an initial step toward leveraging non-verbal cues to advance spoken language modeling in language models.",
      "arxiv_url": "https://arxiv.org/abs/2503.03474",
      "pdf_url": "https://arxiv.org/pdf/2503.03474",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.18243",
      "title": "DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering",
      "authors": [
        "Rong Cheng",
        "Jinyi Liu",
        "Yan Zheng",
        "Fei Ni",
        "Jiazhen Du",
        "Hangyu Mao",
        "Fuzheng Zhang",
        "Bo Wang",
        "Jianye Hao"
      ],
      "abstract": "Multi-Hop Question Answering (MHQA) tasks permeate real-world applications, posing challenges in orchestrating multi-step reasoning across diverse knowledge domains. While existing approaches have been improved with iterative retrieval, they still struggle to identify and organize dynamic knowledge. To address this, we propose DualRAG, a synergistic dual-process framework that seamlessly integrates reasoning and retrieval. DualRAG operates through two tightly coupled processes: Reasoning-augmented Querying (RaQ) and progressive Knowledge Aggregation (pKA). They work in concert: as RaQ navigates the reasoning path and generates targeted queries, pKA ensures that newly acquired knowledge is systematically integrated to support coherent reasoning. This creates a virtuous cycle of knowledge enrichment and reasoning refinement. Through targeted fine-tuning, DualRAG preserves its sophisticated reasoning and retrieval capabilities even in smaller-scale models, demonstrating its versatility and core advantages across different scales. Extensive experiments demonstrate that this dual-process approach substantially improves answer accuracy and coherence, approaching, and in some cases surpassing, the performance achieved with oracle knowledge access. These results establish DualRAG as a robust and efficient solution for complex multi-hop reasoning tasks.",
      "arxiv_url": "https://arxiv.org/abs/2504.18243",
      "pdf_url": "https://arxiv.org/pdf/2504.18243",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a1d3cc4dedd033bf0a1bba08abf049e3cfc937f7",
      "title": "Word-Level Detection of Code-Mixed Hate Speech with Multilingual Domain Transfer",
      "authors": [
        "Karin Niederreiter",
        "Dagmar Gromann"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/a1d3cc4dedd033bf0a1bba08abf049e3cfc937f7",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11651",
      "title": "MMXU: A Multi-Modal and Multi-X-ray Understanding Dataset for Disease Progression",
      "authors": [
        "Linjie Mu",
        "Zhongzhen Huang",
        "Shengqian Qin",
        "Yakun Zhu",
        "Shaoting Zhang",
        "Xiaofan Zhang"
      ],
      "abstract": "Large vision-language models (LVLMs) have shown great promise in medical applications, particularly in visual question answering (MedVQA) and diagnosis from medical images. However, existing datasets and models often fail to consider critical aspects of medical diagnostics, such as the integration of historical records and the analysis of disease progression over time. In this paper, we introduce MMXU (Multimodal and MultiX-ray Understanding), a novel dataset for MedVQA that focuses on identifying changes in specific regions between two patient visits. Unlike previous datasets that primarily address single-image questions, MMXU enables multi-image questions, incorporating both current and historical patient data. We demonstrate the limitations of current LVLMs in identifying disease progression on MMXU-\\textit{test}, even those that perform well on traditional benchmarks. To address this, we propose a MedRecord-Augmented Generation (MAG) approach, incorporating both global and regional historical records. Our experiments show that integrating historical records significantly enhances diagnostic accuracy by at least 20\\%, bridging the gap between current LVLMs and human expert performance. Additionally, we fine-tune models with MAG on MMXU-\\textit{dev}, which demonstrates notable improvements. We hope this work could illuminate the avenue of advancing the use of LVLMs in medical diagnostics by emphasizing the importance of historical context in interpreting medical images. Our dataset is released at github: https://github.com/linjiemu/MMXU.",
      "arxiv_url": "https://arxiv.org/abs/2502.11651",
      "pdf_url": "https://arxiv.org/pdf/2502.11651",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a248a4b19b99211e7278058d82ef23919bbc5e3c",
      "title": "SocialCC: Interactive Evaluation for Cultural Competence in Language Agents",
      "authors": [
        "Jincenzi Wu",
        "Jianxun Lian",
        "Dingdong Wang",
        "Helen M. Meng"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed worldwide, yet their ability to navigate cultural nuances remains underex-plored. Misinterpreting cultural content can lead to AI-generated responses that are offensive or inappropriate, limiting their usability in global applications such as customer service, diplomatic communication, and online education. While prior research has evaluated cultural knowledge of LLMs, existing benchmarks fail to assess dynamic cultural competence — the ability to apply cultural knowledge effectively in real-world interactions. To address this gap, we introduce SocialCC , a novel benchmark designed to evaluate cultural competence through multi-turn interactive intercultural scenarios. It comprises 3,060 human-written scenarios spanning 60 countries across six continents. Through extensive experiments on eight prominent LLMs, our findings reveal a significant gap between the cultural knowledge stored in these models and their ability to apply it effectively in cross-cultural communication. We release our code and data at https: //github.com/jincenziwu/SocialCC .",
      "arxiv_url": "https://www.semanticscholar.org/paper/a248a4b19b99211e7278058d82ef23919bbc5e3c",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15806",
      "title": "A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos",
      "authors": [
        "Yang Yao",
        "Xuan Tong",
        "Ruofan Wang",
        "Yixu Wang",
        "Lujundong Li",
        "Liang Liu",
        "Yan Teng",
        "Yingchun Wang"
      ],
      "abstract": "Large Reasoning Models (LRMs) have significantly advanced beyond traditional Large Language Models (LLMs) with their exceptional logical reasoning capabilities, yet these improvements introduce heightened safety risks. When subjected to jailbreak attacks, their ability to generate more targeted and organized content can lead to greater harm. Although some studies claim that reasoning enables safer LRMs against existing LLM attacks, they overlook the inherent flaws within the reasoning process itself. To address this gap, we propose the first jailbreak attack targeting LRMs, exploiting their unique vulnerabilities stemming from the advanced reasoning capabilities. Specifically, we introduce a Chaos Machine, a novel component to transform attack prompts with diverse one-to-one mappings. The chaos mappings iteratively generated by the machine are embedded into the reasoning chain, which strengthens the variability and complexity and also promotes a more robust attack. Based on this, we construct the Mousetrap framework, which makes attacks projected into nonlinear-like low sample spaces with mismatched generalization enhanced. Also, due to the more competing objectives, LRMs gradually maintain the inertia of unpredictable iterative reasoning and fall into our trap. Success rates of the Mousetrap attacking o1-mini, Claude-Sonnet and Gemini-Thinking are as high as 96%, 86% and 98% respectively on our toxic dataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench, attacking Claude-Sonnet, well-known for its safety, Mousetrap can astonishingly achieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This paper contains inappropriate, offensive and harmful content.",
      "arxiv_url": "https://arxiv.org/abs/2502.15806",
      "pdf_url": "https://arxiv.org/pdf/2502.15806",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.05420",
      "title": "PreSumm: Predicting Summarization Performance Without Summarizing",
      "authors": [
        "Steven Koniaev",
        "Ori Ernst",
        "Jackie Chi Kit Cheung"
      ],
      "abstract": "Despite recent advancements in automatic summarization, state-of-the-art models do not summarize all documents equally well, raising the question: why? While prior research has extensively analyzed summarization models, little attention has been given to the role of document characteristics in influencing summarization performance. In this work, we explore two key research questions. First, do documents exhibit consistent summarization quality across multiple systems? If so, can we predict a document's summarization performance without generating a summary? We answer both questions affirmatively and introduce PreSumm, a novel task in which a system predicts summarization performance based solely on the source document. Our analysis sheds light on common properties of documents with low PreSumm scores, revealing that they often suffer from coherence issues, complex content, or a lack of a clear main theme. In addition, we demonstrate PreSumm's practical utility in two key applications: improving hybrid summarization workflows by identifying documents that require manual summarization and enhancing dataset quality by filtering outliers and noisy documents. Overall, our findings highlight the critical role of document properties in summarization performance and offer insights into the limitations of current systems that could serve as the basis for future improvements.",
      "arxiv_url": "https://arxiv.org/abs/2504.05420",
      "pdf_url": "https://arxiv.org/pdf/2504.05420",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24165",
      "title": "Tag-Evol: Achieving Efficient Instruction Evolving via Tag Injection",
      "authors": [
        "Yixuan Wang",
        "Shiqi Zhou",
        "Chuanzhe Guo",
        "Qingfu Zhu"
      ],
      "abstract": "Evol-Instruct has made significant improvements as a data synthesis method in several areas. Existing methods typically rely on a fixed set of strategies to evolve, which require manual design and are monolithic in form. In addition, iterative evolution also makes the acquisition of hard samples expensive. In view of this, we propose the Tag-Evol framework, a more diverse and efficient instruction evolving method. Specifically, Tag-Evol uses diverse and specific knowledge tags as strategies to achieve controlled evolution by injecting different combinations of tags into the original instructions. Experiments with multiple backbones in diverse domain benchmarks show that the proposed method generates significantly better evolved data than other methods. Furthermore, we conduct a thorough analysis of the evolved data, demonstrating that Tag-Evol is not only efficient but also generates more diverse and challenging data.",
      "arxiv_url": "https://arxiv.org/abs/2505.24165",
      "pdf_url": "https://arxiv.org/pdf/2505.24165",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a2bd8c533f867f9d35147077899fe4a2450ea68e",
      "title": "Live Football Commentary System Providing Background Information",
      "authors": [
        "Yuichiro Mori",
        "Chikara Tanaka",
        "Aru Maekawa",
        "Satoshi Kosugi",
        "Tatsuya Ishigaki",
        "Kotaro Funakoshi",
        "Hiroya Takamura",
        "Manabu Okumura"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/a2bd8c533f867f9d35147077899fe4a2450ea68e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07453",
      "title": "Understanding Cross-Domain Adaptation in Low-Resource Topic Modeling",
      "authors": [
        "Pritom Saha Akash",
        "Kevin Chen-Chuan Chang"
      ],
      "abstract": "Topic modeling plays a vital role in uncovering hidden semantic structures within text corpora, but existing models struggle in low-resource settings where limited target-domain data leads to unstable and incoherent topic inference. We address this challenge by formally introducing domain adaptation for low-resource topic modeling, where a high-resource source domain informs a low-resource target domain without overwhelming it with irrelevant content. We establish a finite-sample generalization bound showing that effective knowledge transfer depends on robust performance in both domains, minimizing latent-space discrepancy, and preventing overfitting to the data. Guided by these insights, we propose DALTA (Domain-Aligned Latent Topic Adaptation), a new framework that employs a shared encoder for domain-invariant features, specialized decoders for domain-specific nuances, and adversarial alignment to selectively transfer relevant information. Experiments on diverse low-resource datasets demonstrate that DALTA consistently outperforms state-of-the-art methods in terms of topic coherence, stability, and transferability.",
      "arxiv_url": "https://arxiv.org/abs/2506.07453",
      "pdf_url": "https://arxiv.org/pdf/2506.07453",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19773",
      "title": "What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs",
      "authors": [
        "Sangyeop Kim",
        "Yohan Lee",
        "Yongwoo Song",
        "Kimin Lee"
      ],
      "abstract": "We investigate long-context vulnerabilities in Large Language Models (LLMs) through Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of up to 128K tokens. Through comprehensive analysis with various many-shot attack settings with different instruction styles, shot density, topic, and format, we reveal that context length is the primary factor determining attack effectiveness. Critically, we find that successful attacks do not require carefully crafted harmful content. Even repetitive shots or random dummy text can circumvent model safety measures, suggesting fundamental limitations in long-context processing capabilities of LLMs. The safety behavior of well-aligned models becomes increasingly inconsistent with longer contexts. These findings highlight significant safety gaps in context expansion capabilities of LLMs, emphasizing the need for new safety mechanisms.",
      "arxiv_url": "https://arxiv.org/abs/2505.19773",
      "pdf_url": "https://arxiv.org/pdf/2505.19773",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16033",
      "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
      "authors": [
        "Qianqi Yan",
        "Yue Fan",
        "Hongquan Li",
        "Shan Jiang",
        "Yang Zhao",
        "Xinze Guan",
        "Ching-Chen Kuo",
        "Xin Eric Wang"
      ],
      "abstract": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting pairwise inconsistencies but struggle with inconsistencies confined to single elements in complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency.",
      "arxiv_url": "https://arxiv.org/abs/2502.16033",
      "pdf_url": "https://arxiv.org/pdf/2502.16033",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12853",
      "title": "S2R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning",
      "authors": [
        "Ruotian Ma",
        "Peisong Wang",
        "Cheng Liu",
        "Xingyan Liu",
        "Jiaqi Chen",
        "Bang Zhang",
        "Xin Zhou",
        "Nan Du",
        "Jia Li"
      ],
      "abstract": "Recent studies have demonstrated the effectiveness of LLM test-time scaling. However, existing approaches to incentivize LLMs' deep thinking abilities generally require large-scale data or significant training efforts. Meanwhile, it remains unclear how to improve the thinking abilities of less powerful base models. In this work, we introduce S$^2$R, an efficient framework that enhances LLM reasoning by teaching models to self-verify and self-correct during inference. Specifically, we first initialize LLMs with iterative self-verification and self-correction behaviors through supervised fine-tuning on carefully curated data. The self-verification and self-correction skills are then further strengthened by both outcome-level and process-level reinforcement learning, with minimized resource requirements, enabling the model to adaptively refine its reasoning process during inference. Our results demonstrate that, with only 3.1k self-verifying and self-correcting behavior initialization samples, Qwen2.5-math-7B achieves an accuracy improvement from 51.0\\% to 81.6\\%, outperforming models trained on an equivalent amount of long-CoT distilled data. Extensive experiments and analysis based on three base models across both in-domain and out-of-domain benchmarks validate the effectiveness of S$^2$R. Our code and data are available at https://github.com/NineAbyss/S2R.",
      "arxiv_url": "https://arxiv.org/abs/2502.12853",
      "pdf_url": "https://arxiv.org/pdf/2502.12853",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10542",
      "title": "Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More",
      "authors": [
        "Arvid Frydenlund"
      ],
      "abstract": "This work concerns the path-star task, a minimal example of searching over a graph. The graph, $G$, is star-shaped with $D$ arms radiating from a start node, $s$. A language model (LM) is given $G$, $s$, and a target node $t$, which ends one of the arms and is tasked with generating the arm containing $t$. The minimal nature of this task means only a single choice needs to be made: which of the $D$ arms contains $t$? Decoder-only LMs fail to solve this elementary task above $1/D$ chance due to a learned shortcut that absorbs training supervision. We show how this pathology is caused by excess supervision and we present a series of solutions demonstrating that the task is solvable via decoder-only LMs. We find that the task's minimal nature causes its difficulty, as it prevents task decomposition. Our solutions provide insight into the pathology and its implications for LMs trained via next-token prediction.",
      "arxiv_url": "https://arxiv.org/abs/2503.10542",
      "pdf_url": "https://arxiv.org/pdf/2503.10542",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.10855",
      "title": "Towards Effective Extraction and Evaluation of Factual Claims",
      "authors": [
        "Dasha Metropolitansky",
        "Jonathan Larson"
      ],
      "abstract": "A common strategy for fact-checking long-form content generated by Large Language Models (LLMs) is extracting simple claims that can be verified independently. Since inaccurate or incomplete claims compromise fact-checking results, ensuring claim quality is critical. However, the lack of a standardized evaluation framework impedes assessment and comparison of claim extraction methods. To address this gap, we propose a framework for evaluating claim extraction in the context of fact-checking along with automated, scalable, and replicable methods for applying this framework, including novel approaches for measuring coverage and decontextualization. We also introduce Claimify, an LLM-based claim extraction method, and demonstrate that it outperforms existing methods under our evaluation framework. A key feature of Claimify is its ability to handle ambiguity and extract claims only when there is high confidence in the correct interpretation of the source text.",
      "arxiv_url": "https://arxiv.org/abs/2502.10855",
      "pdf_url": "https://arxiv.org/pdf/2502.10855",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a3787df6c01c943b4a1859c8aae3ec86ef020748",
      "title": "PAM: Paraphrase AMR-Centric Evaluation Metric",
      "authors": [
        "Afonso Sousa",
        "Henrique Lopes Cardoso"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/a3787df6c01c943b4a1859c8aae3ec86ef020748",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.10967",
      "title": "Advancing General Multimodal Capability of Vision-language Models with Pyramid-descent Visual Position Encoding",
      "authors": [
        "Zhanpeng Chen",
        "Mingxiao Li",
        "Ziyang Chen",
        "Nan Du",
        "Xiaolong Li",
        "Yuexian Zou"
      ],
      "abstract": "Vision-language Models (VLMs) have shown remarkable capabilities in advancing general artificial intelligence, yet the irrational encoding of visual positions persists in inhibiting the models' comprehensive perception performance across different levels of granularity. In this work, we propose Pyramid-descent Visual Position Encoding (PyPE), a novel approach designed to enhance the perception of visual tokens within VLMs. By assigning visual position indexes from the periphery to the center and expanding the central receptive field incrementally, PyPE addresses the limitations of traditional raster-scan methods and mitigates the long-term decay effects induced by Rotary Position Embedding (RoPE). Our method reduces the relative distance between interrelated visual elements and instruction tokens, promoting a more rational allocation of attention weights and allowing for a multi-granularity perception of visual elements and countering the over-reliance on anchor tokens. Extensive experimental evaluations demonstrate that PyPE consistently improves the general capabilities of VLMs across various sizes. Code is available at https://github.com/SakuraTroyChen/PyPE.",
      "arxiv_url": "https://arxiv.org/abs/2501.10967",
      "pdf_url": "https://arxiv.org/pdf/2501.10967",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12983",
      "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models",
      "authors": [
        "Jiaan Wang",
        "Fandong Meng",
        "Zengkui Sun",
        "Yunlong Liang",
        "Yuxuan Cao",
        "Jiarong Xu",
        "Haoxiang Shi",
        "Jie Zhou"
      ],
      "abstract": "Many-to-many summarization (M2MS) aims to process documents in any language and generate the corresponding summaries also in any language. Recently, large language models (LLMs) have shown strong multi-lingual abilities, giving them the potential to perform M2MS in real applications. This work presents a systematic empirical study on LLMs' M2MS ability. Specifically, we first reorganize M2MS data based on eight previous domain-specific datasets. The reorganized data contains 47.8K samples spanning five domains and six languages, which could be used to train and evaluate LLMs. Then, we benchmark 18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned traditional models (e.g., mBART) are also conducted for comparisons. Our experiments reveal that, zero-shot LLMs achieve competitive results with fine-tuned traditional models. After instruct-tuning, open-source LLMs can significantly improve their M2MS ability, and outperform zero-shot LLMs (including GPT-4) in terms of automatic evaluations. In addition, we demonstrate that this task-specific improvement does not sacrifice the LLMs' general task-solving abilities. However, as revealed by our human evaluation, LLMs still face the factuality issue, and the instruction tuning might intensify the issue. Thus, how to control factual errors becomes the key when building LLM summarizers in real applications, and is worth noting in future research.",
      "arxiv_url": "https://arxiv.org/abs/2505.12983",
      "pdf_url": "https://arxiv.org/pdf/2505.12983",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.10289",
      "title": "RT-VC: Real-Time Zero-Shot Voice Conversion with Speech Articulatory Coding",
      "authors": [
        "Yisi Liu",
        "Chenyang Wang",
        "Hanjo Kim",
        "Raniya Khan",
        "G. Anumanchipalli"
      ],
      "abstract": "Voice conversion has emerged as a pivotal technology in numerous applications ranging from assistive communication to entertainment. In this paper, we present RT-VC, a zero-shot real-time voice conversion system that delivers ultra-low latency and high-quality performance. Our approach leverages an articulatory feature space to naturally disentangle content and speaker characteristics, facilitating more robust and interpretable voice transformations. Additionally, the integration of differentiable digital signal processing (DDSP) enables efficient vocoding directly from articulatory features, significantly reducing conversion latency. Experimental evaluations demonstrate that, while maintaining synthesis quality comparable to the current state-of-the-art (SOTA) method, RT-VC achieves a CPU latency of 61.4 ms, representing a 13.3\\% reduction in latency.",
      "arxiv_url": "https://arxiv.org/abs/2506.10289",
      "pdf_url": "https://arxiv.org/pdf/2506.10289",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a423319c80883931edfb105b540deea2a83d91b7",
      "title": "Explicit Bayesian Inference to Uncover the Latent Themes of Large Language Models",
      "authors": [
        "Raymond Li",
        "Chuyuan Li",
        "Gabriel Murray",
        "Giuseppe Carenini"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/a423319c80883931edfb105b540deea2a83d91b7",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20368",
      "title": "Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents",
      "authors": [
        "Jaeyoung Choe",
        "Jihoon Kim",
        "Woohwan Jung"
      ],
      "abstract": "Retrieval-augmented generation (RAG) based large language models (LLMs) are widely used in finance for their excellent performance on knowledge-intensive tasks. However, standardized documents (e.g., SEC filing) share similar formats such as repetitive boilerplate texts, and similar table structures. This similarity forces traditional RAG methods to misidentify near-duplicate text, leading to duplicate retrieval that undermines accuracy and completeness. To address these issues, we propose the Hierarchical Retrieval with Evidence Curation (HiREC) framework. Our approach first performs hierarchical retrieval to reduce confusion among similar texts. It first retrieve related documents and then selects the most relevant passages from the documents. The evidence curation process removes irrelevant passages. When necessary, it automatically generates complementary queries to collect missing information. To evaluate our approach, we construct and release a Large-scale Open-domain Financial (LOFin) question answering benchmark that includes 145,897 SEC documents and 1,595 question-answer pairs. Our code and data are available at https://github.com/deep-over/LOFin-bench-HiREC.",
      "arxiv_url": "https://arxiv.org/abs/2505.20368",
      "pdf_url": "https://arxiv.org/pdf/2505.20368",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00488",
      "title": "Synergizing LLMs with Global Label Propagation for Multimodal Fake News Detection",
      "authors": [
        "Shuguo Hu",
        "Jun Hu",
        "Huaiwen Zhang"
      ],
      "abstract": "Large Language Models (LLMs) can assist multimodal fake news detection by predicting pseudo labels. However, LLM-generated pseudo labels alone demonstrate poor performance compared to traditional detection methods, making their effective integration non-trivial. In this paper, we propose Global Label Propagation Network with LLM-based Pseudo Labeling (GLPN-LLM) for multimodal fake news detection, which integrates LLM capabilities via label propagation techniques. The global label propagation can utilize LLM-generated pseudo labels, enhancing prediction accuracy by propagating label information among all samples. For label propagation, a mask-based mechanism is designed to prevent label leakage during training by ensuring that training nodes do not propagate their own labels back to themselves. Experimental results on benchmark datasets show that by synergizing LLMs with label propagation, our model achieves superior performance over state-of-the-art baselines.",
      "arxiv_url": "https://arxiv.org/abs/2506.00488",
      "pdf_url": "https://arxiv.org/pdf/2506.00488",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.12663",
      "title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment",
      "authors": [
        "Xiaotian Zhang",
        "Ruizhe Chen",
        "Yang Feng",
        "Zuozhu Liu"
      ],
      "abstract": "Aligning language models with human preferences presents significant challenges, particularly in achieving personalization without incurring excessive computational costs. Existing methods rely on reward signals and additional annotated data, limiting their scalability and adaptability to diverse human values. To address these challenges, we introduce Persona-judge, a novel discriminative paradigm that enables training-free personalized alignment with unseen preferences. Instead of optimizing policy parameters through external reward feedback, Persona-judge leverages the intrinsic preference judgment capabilities of the model. Specifically, a draft model generates candidate tokens conditioned on a given preference, while a judge model, embodying another preference, cross-validates the predicted tokens whether to be accepted. Experimental results demonstrate that Persona-judge, using the inherent preference evaluation mechanisms of the model, offers a scalable and computationally efficient solution to personalized alignment, paving the way for more adaptive customized alignment. Our code is available here.",
      "arxiv_url": "https://arxiv.org/abs/2504.12663",
      "pdf_url": "https://arxiv.org/pdf/2504.12663",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Personalization"
      ],
      "published_date": "2025-04-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23540",
      "title": "Probability-Consistent Preference Optimization for Enhanced LLM Reasoning",
      "authors": [
        "Yunqiao Yang",
        "Houxing Ren",
        "Zimu Lu",
        "Ke Wang",
        "Weikang Shi",
        "Aojun Zhou",
        "Junting Pan",
        "Mingjie Zhan",
        "Hongsheng Li"
      ],
      "abstract": "Recent advances in preference optimization have demonstrated significant potential for improving mathematical reasoning capabilities in large language models (LLMs). While current approaches leverage high-quality pairwise preference data through outcome-based criteria like answer correctness or consistency, they fundamentally neglect the internal logical coherence of responses. To overcome this, we propose Probability-Consistent Preference Optimization (PCPO), a novel framework that establishes dual quantitative metrics for preference selection: (1) surface-level answer correctness and (2) intrinsic token-level probability consistency across responses. Extensive experiments show that our PCPO consistently outperforms existing outcome-only criterion approaches across a diverse range of LLMs and benchmarks. Our code is publicly available at https://github.com/YunqiaoYang/PCPO.",
      "arxiv_url": "https://arxiv.org/abs/2505.23540",
      "pdf_url": "https://arxiv.org/pdf/2505.23540",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.05673",
      "title": "R-VLM: Region-Aware Vision Language Model for Precise GUI Grounding",
      "authors": [
        "Joonhyung Park",
        "Peng Tang",
        "Sagnik Das",
        "Srikar Appalaraju",
        "Kunwar Yashraj Singh",
        "R. Manmatha",
        "Shabnam Ghadar"
      ],
      "abstract": "Visual agent models for automating human activities on Graphical User Interfaces (GUIs) have emerged as a promising research direction, driven by advances in large Vision Language Models (VLMs). A critical challenge in GUI automation is the precise grounding of interface elements across diverse platforms. Existing vision-only GUI agents directly ground elements from large and cluttered screenshots, requiring them to process substantial irrelevant information that compromises their accuracy. In addition, these approaches typically employ basic cross-entropy loss for learning grounding objectives, which fails to effectively capture grounding quality compared to established object detection metrics like Intersection-over-Union (IoU). To address these issues, we introduce R-VLM, a novel GUI grounding approach that leverages zoomed-in region proposals for precise element localization. We also propose an IoU-aware objective function that facilitates model convergence toward high IoU predictions. Our approach bridges the gap between VLMs and conventional object detection techniques, improving the state-of-the-art grounding accuracy by 13% across diverse GUI platforms on the GUI grounding benchmarks ScreenSpot and AgentStudio. In addition, our R-VLM approach shows 3.2-9.7% absolute accuracy improvements in GUI navigation tasks on the AITW and Mind2Web benchmarks.",
      "arxiv_url": "https://arxiv.org/abs/2507.05673",
      "pdf_url": "https://arxiv.org/pdf/2507.05673",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13962",
      "title": "Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering",
      "authors": [
        "William Jurayj",
        "Jeffrey Cheng",
        "Benjamin Van Durme"
      ],
      "abstract": "Scaling the test-time compute of large language models has demonstrated impressive performance on reasoning benchmarks. However, existing evaluations of test-time scaling make the strong assumption that a reasoning system should always give an answer to any question provided. This overlooks concerns about whether a model is confident in its answer, and whether it is appropriate to always provide a response. To address these concerns, we extract confidence scores during reasoning for thresholding model responses. We find that increasing compute budget at inference time not only helps models answer more questions correctly, but also increases confidence in correct responses. We then extend the current paradigm of zero-risk responses during evaluation by considering settings with non-zero levels of response risk, and suggest a recipe for reporting evaluations under these settings.",
      "arxiv_url": "https://arxiv.org/abs/2502.13962",
      "pdf_url": "https://arxiv.org/pdf/2502.13962",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00806",
      "title": "Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering",
      "authors": [
        "Songtao Jiang",
        "Chenyi Zhou",
        "Yan Zhang",
        "Yeying Jin",
        "Zuozhu Liu"
      ],
      "abstract": "Multimodal large language models (MLLMs) still struggle with complex reasoning tasks in Visual Question Answering (VQA). While current methods have advanced by incorporating visual prompts, our study uncovers critical limitations: these approaches indiscriminately annotate all detected objects for every visual question, generating excessive visual markers that degrade task performance. This issue stems primarily from a lack of focus on key visual elements, raising two important questions: Are all objects equally important, and do all questions require visual prompts? Motivated by Dual Process Theory, which distinguishes between instinctive and deliberate cognitive modes in human reasoning, we propose FOCUS, a plug-and-play approach that dynamically adapts to the complexity of questions, combining fast intuitive judgments with deliberate analytical reasoning to enhance the vision-language reasoning capability of the MLLM. For straightforward questions, FOCUS supports efficient zero-shot reasoning. For more complex tasks, it employs the conceptualizing before observation strategy to highlight critical elements. Extensive experiments on four benchmarks, ScienceQA, TextQA, VizWiz, and MME, demonstrate that FOCUS consistently improves the performance of both open-source and black-box MLLMs, achieving significant gains across all datasets. Ablation studies further validate the importance of combining diverse cognitive strategies with refined visual information for superior performance. Code will be released.",
      "arxiv_url": "https://arxiv.org/abs/2506.00806",
      "pdf_url": "https://arxiv.org/pdf/2506.00806",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14405",
      "title": "Investigating and Enhancing the Robustness of Large Multimodal Models Against Temporal Inconsistency",
      "authors": [
        "Jiafeng Liang",
        "Shixin Jiang",
        "Xuan Dong",
        "Ning Wang",
        "Zheng Chu",
        "Hui Su",
        "Jinlan Fu",
        "Ming Liu",
        "See-Kiong Ng",
        "Bing Qin"
      ],
      "abstract": "Large Multimodal Models (LMMs) have recently demonstrated impressive performance on general video comprehension benchmarks. Nevertheless, for broader applications, the robustness of their temporal analysis capability needs to be thoroughly investigated yet predominantly ignored. Motivated by this, we propose a novel temporal robustness benchmark (TemRobBench), which introduces temporal inconsistency perturbations separately at the visual and textual modalities to assess the robustness of models. We evaluate 16 mainstream LMMs and find that they exhibit over-reliance on prior knowledge and textual context in adversarial environments, while ignoring the actual temporal dynamics in the video. To mitigate this issue, we design panoramic direct preference optimization (PanoDPO), which encourages LMMs to incorporate both visual and linguistic feature preferences simultaneously. Experimental results show that PanoDPO can effectively enhance the model's robustness and reliability in temporal analysis.",
      "arxiv_url": "https://arxiv.org/abs/2505.14405",
      "pdf_url": "https://arxiv.org/pdf/2505.14405",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.01715",
      "title": "Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach",
      "authors": [
        "Aditya Tomar",
        "Rudra Murthy",
        "Pushpak Bhattacharyya"
      ],
      "abstract": "Bias and stereotypes in language models can cause harm, especially in sensitive areas like content moderation and decision-making. This paper addresses bias and stereotype detection by exploring how jointly learning these tasks enhances model performance. We introduce StereoBias, a unique dataset labeled for bias and stereotype detection across five categories: religion, gender, socio-economic status, race, profession, and others, enabling a deeper study of their relationship. Our experiments compare encoder-only models and fine-tuned decoder-only models using QLoRA. While encoder-only models perform well, decoder-only models also show competitive results. Crucially, joint training on bias and stereotype detection significantly improves bias detection compared to training them separately. Additional experiments with sentiment analysis confirm that the improvements stem from the connection between bias and stereotypes, not multi-task learning alone. These findings highlight the value of leveraging stereotype information to build fairer and more effective AI systems.",
      "arxiv_url": "https://arxiv.org/abs/2507.01715",
      "pdf_url": "https://arxiv.org/pdf/2507.01715",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12921",
      "title": "Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison",
      "authors": [
        "George Saad",
        "Scott Sanner"
      ],
      "abstract": "Query-driven recommendation with unknown items poses a challenge for users to understand why certain items are appropriate for their needs. Query-driven Contrastive Summarization (QCS) is a methodology designed to address this issue by leveraging language-based item descriptions to clarify contrasts between them. However, existing state-of-the-art contrastive summarization methods such as STRUM-LLM fall short of this goal. To overcome these limitations, we introduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs debate-style prompting to generate focused and contrastive summarizations of item aspects relevant to a query. Leveraging modern large language models (LLMs) as powerful tools for generating debates, Q-STRUM Debate provides enhanced contrastive summaries. Experiments across three datasets demonstrate that Q-STRUM Debate yields significant performance improvements over existing methods on key contrastive summarization criteria, thus introducing a novel and performant debate prompting methodology for QCS.",
      "arxiv_url": "https://arxiv.org/abs/2502.12921",
      "pdf_url": "https://arxiv.org/pdf/2502.12921",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11187",
      "title": "TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking",
      "authors": [
        "Shahriar Kabir Nahin",
        "R. N. Nandi",
        "S. Sarker",
        "Quazi Sarwar Muhtaseem",
        "Md Kowsher",
        "Apu Chandraw Shill",
        "Md Ibrahim",
        "Mehadi Hasan Menon",
        "T. Muntasir",
        "Firoj Alam"
      ],
      "abstract": "In this paper, we present TituLLMs, the first large pretrained Bangla LLMs, available in 1b and 3b parameter sizes. Due to computational constraints during both training and inference, we focused on smaller models. To train TituLLMs, we collected a pretraining dataset of approximately ~37 billion tokens. We extended the Llama-3.2 tokenizer to incorporate language- and culture-specific knowledge, which also enables faster training and inference. There was a lack of benchmarking datasets to benchmark LLMs for Bangla. To address this gap, we developed five benchmarking datasets. We benchmarked various LLMs, including TituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual versions. However, this is not always the case, highlighting the complexities of language adaptation. Our work lays the groundwork for adapting existing multilingual open models to other low-resource languages. To facilitate broader adoption and further research, we have made the TituLLMs models and benchmarking datasets publicly available (https://huggingface.co/collections/hishab/titulm-llama-family-6718d31fc1b83529276f490a).",
      "arxiv_url": "https://arxiv.org/abs/2502.11187",
      "pdf_url": "https://arxiv.org/pdf/2502.11187",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a53655e93b973f35ab4b9ea9976cd2cf82fc3270",
      "title": "Text is All You Need: LLM-enhanced Incremental Social Event Detection",
      "authors": [
        "Zitai Qiu",
        "Congbo Ma",
        "Jia Wu",
        "Jian Yang"
      ],
      "abstract": "Social event detection (SED) is the task of identifying, categorizing, and tracking events from social data sources such as social media posts, news articles, and online discussions. Existing state-of-the-art (SOTA) SED models predominantly rely on graph neural networks (GNNs), which involve complex graph construction and time-consuming training processes, limiting their practicality in real-world scenarios. In this paper, we rethink the key challenge in SED: the informal expressions and abbreviations of short texts on social media platforms, which impact clustering accuracy. We propose a novel framework, LLM-enhanced Social Event Detection (LSED) , which leverages the rich background knowledge of LLMs to address this challenge. Specifically, LSED utilizes LLMs to formalize and disambiguate short texts by completing abbreviations and summarizing informal expressions. Furthermore, we introduce hyperbolic space embeddings, which are more suitable for natural language sentence representations, to enhance clustering performance. Extensive experiments on two challenging real-world datasets demonstrate that LSED outperforms existing SOTA models, achieving improvements in effectiveness , efficiency , and stability . Our work highlights the potential of LLMs in SED and provides a practical solu-tion for real-world applications. The code is available at GitHub 1 .",
      "arxiv_url": "https://www.semanticscholar.org/paper/a53655e93b973f35ab4b9ea9976cd2cf82fc3270",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.00026",
      "title": "Theory of Mind in Large Language Models: Assessment and Enhancement",
      "authors": [
        "Ruirui Chen",
        "Weifeng Jiang",
        "Chengwei Qin",
        "Cheston Tan"
      ],
      "abstract": "Theory of Mind (ToM)-the ability to reason about the mental states of oneself and others-is a cornerstone of human social intelligence. As Large Language Models (LLMs) become increasingly integrated into daily life, understanding their ability to interpret and respond to human mental states is crucial for enabling effective interactions. In this paper, we review LLMs'ToM capabilities by analyzing both evaluation benchmarks and enhancement strategies. For evaluation, we focus on recently proposed and widely used story-based benchmarks. For enhancement, we provide an in-depth analysis of recent methods aimed at improving LLMs'ToM abilities. Furthermore, we outline promising directions for future research to further advance these capabilities and better adapt LLMs to more realistic and diverse scenarios. Our survey serves as a valuable resource for researchers interested in evaluating and advancing LLMs'ToM capabilities.",
      "arxiv_url": "https://arxiv.org/abs/2505.00026",
      "pdf_url": "https://arxiv.org/pdf/2505.00026",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.17015",
      "title": "Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?",
      "authors": [
        "Arduin Findeis",
        "Floris Weers",
        "Guoli Yin",
        "Ke Ye",
        "Ruoming Pang",
        "Tom Gunter"
      ],
      "abstract": "Pairwise preferences over model responses are widely collected to evaluate and provide feedback to large language models (LLMs). Given two alternative model responses to the same input, a human or AI annotator selects the\"better\"response. This approach can provide feedback for domains where other hard-coded metrics are difficult to obtain (e.g., chat response quality), thereby helping model evaluation or training. However, for some domains high-quality pairwise comparisons can be tricky to obtain - from AI and humans. For example, for responses with many factual statements, annotators may disproportionately weigh writing quality rather than underlying facts. In this work, we explore augmenting standard AI annotator systems with additional tools to improve performance on three challenging response domains: long-form factual, math and code tasks. We propose a tool-using agentic system to provide higher quality feedback on these domains. Our system uses web-search and code execution to ground itself based on external validation, independent of the LLM's internal knowledge and biases. We provide extensive experimental results evaluating our method across the three targeted response domains as well as general annotation tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as three new datasets for domains with saturated pre-existing datasets. Our results indicate that external tools can indeed improve performance in many, but not all, cases. More generally, our experiments highlight the sensitivity of performance to simple parameters (e.g., prompt) and the need for improved (non-saturated) annotator benchmarks. We share our code at https://github.com/apple/ml-agent-evaluator.",
      "arxiv_url": "https://arxiv.org/abs/2507.17015",
      "pdf_url": "https://arxiv.org/pdf/2507.17015",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-07-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03532",
      "title": "GA-S3: Comprehensive Social Network Simulation with Group Agents",
      "authors": [
        "Yunyao Zhang",
        "Zikai Song",
        "Hang Zhou",
        "Wenfeng Ren",
        "Yi-Ping Phoebe Chen",
        "Junqing Yu",
        "Wei Yang"
      ],
      "abstract": "Social network simulation is developed to provide a comprehensive understanding of social networks in the real world, which can be leveraged for a wide range of applications such as group behavior emergence, policy optimization, and business strategy development. However, billions of individuals and their evolving interactions involved in social networks pose challenges in accurately reflecting real-world complexities. In this study, we propose a comprehensive Social Network Simulation System (GA-S3) that leverages newly designed Group Agents to make intelligent decisions regarding various online events. Unlike other intelligent agents that represent an individual entity, our group agents model a collection of individuals exhibiting similar behaviors, facilitating the simulation of large-scale network phenomena with complex interactions at a manageable computational cost. Additionally, we have constructed a social network benchmark from 2024 popular online events that contains fine-grained information on Internet traffic variations. The experiment demonstrates that our approach is capable of achieving accurate and highly realistic prediction results. Code is open at https://github.com/AI4SS/GAS-3.",
      "arxiv_url": "https://arxiv.org/abs/2506.03532",
      "pdf_url": "https://arxiv.org/pdf/2506.03532",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2508.18740",
      "title": "M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations",
      "authors": [
        "Qiao Liang",
        "Ying Shen",
        "Tiantian Chen",
        "Lin Zhang"
      ],
      "abstract": "Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has recently gained significant attention in social media analysis, aiming to extract emotion utterances, cause utterances, and emotion categories simultaneously. However, the scarcity of related datasets, with only one published dataset featuring highly uniform dialogue scenarios, hinders model development in this field. To address this, we introduce MECAD, the first multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56 TV series spanning a wide range of dialogue contexts. In addition, existing MECTEC methods fail to explicitly model emotional and causal contexts and neglect the fusion of semantic information at different levels, leading to performance degradation. In this paper, we propose M3HG, a novel model that explicitly captures emotional and causal contexts and effectively fuses contextual information at both inter- and intra-utterance levels via a multimodal heterogeneous graph. Extensive experiments demonstrate the effectiveness of M3HG compared with existing state-of-the-art methods. The codes and dataset are available at https://github.com/redifinition/M3HG.",
      "arxiv_url": "https://arxiv.org/abs/2508.18740",
      "pdf_url": "https://arxiv.org/pdf/2508.18740",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-08-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.05489",
      "title": "Mechanistic Interpretability of Emotion Inference in Large Language Models",
      "authors": [
        "Ala Nekouvaght Tak",
        "Amin Banayeeanzade",
        "Anahita Bolourani",
        "Mina Kian",
        "Robin Jia",
        "Jonathan Gratch"
      ],
      "abstract": "Large language models (LLMs) show promising capabilities in predicting human emotions from text. However, the mechanisms through which these models process emotional stimuli remain largely unexplored. Our study addresses this gap by investigating how autoregressive LLMs infer emotions, showing that emotion representations are functionally localized to specific regions in the model. Our evaluation includes diverse model families and sizes and is supported by robustness checks. We then show that the identified representations are psychologically plausible by drawing on cognitive appraisal theory, a well-established psychological framework positing that emotions emerge from evaluations (appraisals) of environmental stimuli. By causally intervening on construed appraisal concepts, we steer the generation and show that the outputs align with theoretical and intuitive expectations. This work highlights a novel way to causally intervene and precisely shape emotional text generation, potentially benefiting safety and alignment in sensitive affective domains.",
      "arxiv_url": "https://arxiv.org/abs/2502.05489",
      "pdf_url": "https://arxiv.org/pdf/2502.05489",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11829",
      "title": "Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks",
      "authors": [
        "Chenlu Wang",
        "Weimin Lyu",
        "Ritwik Banerjee"
      ],
      "abstract": "Detecting deviant language such as sexism, or nuanced language such as metaphors or sarcasm, is crucial for enhancing the safety, clarity, and interpretation of online social discourse. While existing classifiers deliver strong results on these tasks, they often come with significant computational cost and high data demands. In this work, we propose \\textbf{Cla}ss \\textbf{D}istillation (ClaD), a novel training paradigm that targets the core challenge: distilling a small, well-defined target class from a highly diverse and heterogeneous background. ClaD integrates two key innovations: (i) a loss function informed by the structural properties of class distributions, based on Mahalanobis distance, and (ii) an interpretable decision algorithm optimized for class separation. Across three benchmark detection tasks -- sexism, metaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with smaller language models and orders of magnitude fewer parameters, achieves performance comparable to several large language models (LLMs). These results demonstrate ClaD as an efficient tool for pragmatic language understanding tasks that require gleaning a small target class from a larger heterogeneous background.",
      "arxiv_url": "https://arxiv.org/abs/2505.11829",
      "pdf_url": "https://arxiv.org/pdf/2505.11829",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2509.04202",
      "title": "Explicit and Implicit Data Augmentation for Social Event Detection",
      "authors": [
        "Congbo Ma",
        "Yuxia Wang",
        "Jia Wu",
        "Jian Yang",
        "Jing Du",
        "Zitai Qiu",
        "Qing Li",
        "Hu Wang",
        "Preslav Nakov"
      ],
      "abstract": "Social event detection involves identifying and categorizing important events from social media, which relies on labeled data, but annotation is costly and labor-intensive. To address this problem, we propose Augmentation framework for Social Event Detection (SED-Aug), a plug-and-play dual augmentation framework, which combines explicit text-based and implicit feature-space augmentation to enhance data diversity and model robustness. The explicit augmentation utilizes large language models to enhance textual information through five diverse generation strategies. For implicit augmentation, we design five novel perturbation techniques that operate in the feature space on structural fused embeddings. These perturbations are crafted to keep the semantic and relational properties of the embeddings and make them more diverse. Specifically, SED-Aug outperforms the best baseline model by approximately 17.67% on the Twitter2012 dataset and by about 15.57% on the Twitter2018 dataset in terms of the average F1 score. The code is available at GitHub: https://github.com/congboma/SED-Aug.",
      "arxiv_url": "https://arxiv.org/abs/2509.04202",
      "pdf_url": "https://arxiv.org/pdf/2509.04202",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-09-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.22353",
      "title": "Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions",
      "authors": [
        "Yubo Li",
        "Yidi Miao",
        "Xueying Ding",
        "R. Krishnan",
        "R. Padman"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities across various tasks, but their deployment in high-stake domains requires consistent and coherent behavior across multiple rounds of user interaction. This paper introduces a comprehensive framework for evaluating and improving LLM response consistency, making three key contributions. Code and data are available at: https://github.com/yubol-bobo/MT-Consistency. First, we introduce Position-Weighted Consistency (PWC), a metric designed to capture both the importance of early-stage stability and recovery patterns in multi-turn interactions. Second, we present MT-Consistency, a carefully curated benchmark dataset spanning diverse domains and difficulty levels, specifically designed to evaluate LLM consistency under various challenging follow-up scenarios. Third, we introduce Confidence-Aware Response Generation (CARG), a framework that significantly improves response stability by explicitly integrating internal model confidence scores during the generation process. Experimental results demonstrate that CARG significantly improves response stability without sacrificing accuracy, offering a practical path toward more dependable LLM behavior in critical, real-world deployments.",
      "arxiv_url": "https://arxiv.org/abs/2503.22353",
      "pdf_url": "https://arxiv.org/pdf/2503.22353",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14316",
      "title": "Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion",
      "authors": [
        "Tiehan Cui",
        "Yanxu Mao",
        "Peipei Liu",
        "Congying Liu",
        "Datao You"
      ],
      "abstract": "Although large language models (LLMs) have achieved remarkable advancements, their security remains a pressing concern. One major threat is jailbreak attacks, where adversarial prompts bypass model safeguards to generate harmful or objectionable content. Researchers study jailbreak attacks to understand security and robustness of LLMs. However, existing jailbreak attack methods face two main challenges: (1) an excessive number of iterative queries, and (2) poor generalization across models. In addition, recent jailbreak evaluation datasets focus primarily on question-answering scenarios, lacking attention to text generation tasks that require accurate regeneration of toxic content. To tackle these challenges, we propose two contributions: (1) ICE, a novel black-box jailbreak method that employs Intent Concealment and divErsion to effectively circumvent security constraints. ICE achieves high attack success rates (ASR) with a single query, significantly improving efficiency and transferability across different models. (2) BiSceneEval, a comprehensive dataset designed for assessing LLM robustness in question-answering and text-generation tasks. Experimental results demonstrate that ICE outperforms existing jailbreak techniques, revealing critical vulnerabilities in current defense mechanisms. Our findings underscore the necessity of a hybrid security strategy that integrates predefined security mechanisms with real-time semantic decomposition to enhance the security of LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2505.14316",
      "pdf_url": "https://arxiv.org/pdf/2505.14316",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10870",
      "title": "Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate",
      "authors": [
        "Ziyang Huang",
        "Wangtao Sun",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "This paper systematically addresses the challenges of rule retrieval, a crucial yet underexplored area. Vanilla retrieval methods using sparse or dense retrievers to directly search for relevant rules to support downstream reasoning, often suffer from low accuracy. This is primarily due to a significant semantic gap between the instantiated facts in the queries and the abstract representations of the rules. Such misalignment results in suboptimal retrieval quality, which in turn negatively impacts reasoning performance. To overcome these challenges, we propose Self-Induction Augmented Retrieval (SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce potential inferential rules that might offer benefits for reasoning by abstracting the underlying knowledge and logical structure in queries. These induced rules are then used for query augmentation to improve retrieval effectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a method that re-estimates the relevance of retrieved rules by assessing whether the abstract knowledge they contain can be instantiated to align with the facts in the queries and the helpfulness for reasoning. Extensive experiments across various settings demonstrate the effectiveness and versatility of our proposed methods.",
      "arxiv_url": "https://arxiv.org/abs/2505.10870",
      "pdf_url": "https://arxiv.org/pdf/2505.10870",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a610fd1a3221ad5f771700395441f213fc48a172",
      "title": "Mitigating Non-Representative Prototypes and Representation Bias in Few-Shot Continual Relation Extraction",
      "authors": [
        "Thanh Duc Pham",
        "Nam Le Hai",
        "L. Van",
        "Nguyen Thi Ngoc Diep",
        "Sang Dinh",
        "T. Nguyen"
      ],
      "abstract": "To address the phenomenon of similar classes, existing methods in few-shot continual relation extraction (FCRE) face two main challenges: non-representative prototypes and representation bias, especially when the number of available samples is limited. In our work, we propose Minion to address these challenges. Firstly, we leverage the General Orthogonal Frame (GOF) structure, based on the concept of Neural Collapse, to create robust class pro-totypes with clear separation, even between analogous classes. Secondly, we utilize label description representations as global class representatives within the fast-slow contrastive learning paradigm. These representations consistently encapsulate the essential attributes of each relation, acting as global information that helps mitigate overfitting and reduces representation bias caused by the limited local few-shot examples within a class. Extensive experiments on well-known FCRE benchmarks show that our method outperforms state-of-the-art approaches, demonstrating its effectiveness for advancing RE system.",
      "arxiv_url": "https://www.semanticscholar.org/paper/a610fd1a3221ad5f771700395441f213fc48a172",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00059",
      "title": "Investigating and Enhancing Vision-Audio Capability in Omnimodal Large Language Models",
      "authors": [
        "Rui Hu",
        "Delai Qiu",
        "Shuyu Wei",
        "Jiaming Zhang",
        "Yining Wang",
        "Shengping Liu",
        "Jitao Sang"
      ],
      "abstract": "Omnimodal Large Language Models (OLLMs) have shown significant progress in integrating vision and text, but still struggle with integrating vision and audio, often exhibiting suboptimal performance when processing audio queries compared to text queries. This disparity is primarily due to insufficient alignment between vision and audio modalities during training, leading to inadequate attention to visual information when using audio queries. To mitigate this issue, we propose a Self-Knowledge Distillation (Self-KD) training method where the vision-text component of the OLLM serves as the teacher and the vision-audio component as the student. This enables the model to process audio in a manner analogous to its text processing. Our experimental results demonstrate that Self-KD is an effective method for enhancing the vision-audio capabilities of OLLMs by learning from the vision-text components, which subsequently improves the interaction between audio and images and results in improved performance on multimodal tasks.",
      "arxiv_url": "https://arxiv.org/abs/2503.00059",
      "pdf_url": "https://arxiv.org/pdf/2503.00059",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a626553642668c04e446014502ed49468b2caa36",
      "title": "Speech Act Patterns for Improving Generalizability of Explainable Politeness Detection Models",
      "authors": [
        "Ahmad Aljanaideh"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/a626553642668c04e446014502ed49468b2caa36",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.02832",
      "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation",
      "authors": [
        "Songming Zhang",
        "Xue Zhang",
        "Tong Zhang",
        "Bojie Hu",
        "Yufeng Chen",
        "Jinan Xu"
      ],
      "abstract": "In modern large language models (LLMs), LLM alignment is of crucial importance and is typically achieved through methods such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). However, in most existing methods for LLM alignment, all tokens in the response are optimized using a sparse, response-level reward or preference annotation. The ignorance of token-level rewards may erroneously punish high-quality tokens or encourage low-quality tokens, resulting in suboptimal performance and slow convergence speed. To address this issue, we propose AlignDistil, an RLHF-equivalent distillation method for token-level reward optimization. Specifically, we introduce the reward learned by DPO into the RLHF objective and theoretically prove the equivalence between this objective and a token-level distillation process, where the teacher distribution linearly combines the logits from the DPO model and a reference model. On this basis, we further bridge the accuracy gap between the reward from the DPO model and the pure reward model, by building a contrastive DPO reward with a normal and a reverse DPO model. Moreover, to avoid under- and over-optimization on different tokens, we design a token adaptive logit extrapolation mechanism to construct an appropriate teacher distribution for each token. Experimental results demonstrate the superiority of our AlignDistil over existing methods and showcase fast convergence due to its token-level distributional reward optimization.",
      "arxiv_url": "https://arxiv.org/abs/2503.02832",
      "pdf_url": "https://arxiv.org/pdf/2503.02832",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11177",
      "title": "The Mirage of Model Editing: Revisiting Evaluation in the Wild",
      "authors": [
        "Wanli Yang",
        "Fei Sun",
        "Jiajun Tan",
        "Xinyu Ma",
        "Qi Cao",
        "Dawei Yin",
        "Huawei Shen",
        "Xueqi Cheng"
      ],
      "abstract": "Despite near-perfect results reported in the literature, the effectiveness of model editing in real-world applications remains unclear. To bridge this gap, we introduce QAEdit, a new benchmark aligned with widely used question answering (QA) datasets, and WILD, a task-agnostic evaluation framework designed to better reflect real-world usage of model editing. Our single editing experiments show that current editing methods perform substantially worse than previously reported (38.5% vs. 96.8%). We demonstrate that it stems from issues in the synthetic evaluation practices of prior work. Among them, the most severe is the use of teacher forcing during testing, which leaks both content and length of the ground truth, leading to overestimated performance. Furthermore, we simulate practical deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits. This work calls for a shift in model editing research toward rigorous evaluation and the development of robust, scalable methods that can reliably update knowledge in LLMs for real-world use.",
      "arxiv_url": "https://arxiv.org/abs/2502.11177",
      "pdf_url": "https://arxiv.org/pdf/2502.11177",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.11041",
      "title": "Enhancing Semantic Consistency of Large Language Models through Model Editing: An Interpretability-Oriented Approach",
      "authors": [
        "Jingyuan Yang",
        "Dapeng Chen",
        "Yajing Sun",
        "Rong-Zhi Li",
        "Zhiyong Feng",
        "Wei Peng"
      ],
      "abstract": "A Large Language Model (LLM) tends to generate inconsistent and sometimes contradictory outputs when presented with a prompt that has equivalent semantics but is expressed differently from the original prompt. To achieve semantic consistency of an LLM, one of the key approaches is to finetune the model with prompt-output pairs with semantically equivalent meanings. Despite its effectiveness, a data-driven finetuning method incurs substantial computation costs in data preparation and model optimization. In this regime, an LLM is treated as a ``black box'', restricting our ability to gain deeper insights into its internal mechanism. In this paper, we are motivated to enhance the semantic consistency of LLMs through a more interpretable method (i.e., model editing) to this end. We first identify the model components (i.e., attention heads) that have a key impact on the semantic consistency of an LLM. We subsequently inject biases into the output of these model components along the semantic-consistency activation direction. It is noteworthy that these modifications are cost-effective, without reliance on mass manipulations of the original model parameters. Through comprehensive experiments on the constructed NLU and open-source NLG datasets, our method demonstrates significant improvements in the semantic consistency and task performance of LLMs. Additionally, our method exhibits promising generalization capabilities by performing well on tasks beyond the primary tasks.",
      "arxiv_url": "https://arxiv.org/abs/2501.11041",
      "pdf_url": "https://arxiv.org/pdf/2501.11041",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06884",
      "title": "FREE: Fast and Robust Vision Language Models with Early Exits",
      "authors": [
        "D. J. Bajpai",
        "M. Hanawal"
      ],
      "abstract": "In recent years, Vision-Language Models (VLMs) have shown remarkable performance improvements in Vision-Language tasks. However, their large size poses challenges for real-world applications where inference latency is a concern. To tackle this issue, we propose employing Early Exit (EE) strategies in VLMs. However, training exit classifiers in VLMs is challenging, particularly with limited labeled training data. To address this, we introduce FREE, an adversarial training approach within a GAN-based framework. Here, each exit consists of a transformer layer and a classifier. The transformer layer is adversarially trained to produce feature representations similar to the final layer, while a feature classifier serves as the discriminator. Our method focuses on performing input-adaptive inference that increases inference speed with minimal drop in performance. Experimental results demonstrate the effectiveness of our approach in enhancing accuracy and model robustness by mitigating overthinking and the phenomenon of mid-crisis that we highlight. We experimentally validate that our method speeds up the inference process by more than 1.51x while retaining comparable performance. The source code is available at https://github.com/Div290/FREE.",
      "arxiv_url": "https://arxiv.org/abs/2506.06884",
      "pdf_url": "https://arxiv.org/pdf/2506.06884",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a6ce61cdd3c6852ed997d3c1885a62f34add82df",
      "title": "UniLR: Unleashing the Power of LLMs on Multiple Legal Tasks with a Unified Legal Retriever",
      "authors": [
        "Ang Li",
        "Yiquan Wu",
        "Yifei Liu",
        "Ming Cai",
        "Lizhi Qing",
        "Shihang Wang",
        "Yangyang Kang",
        "Chengyuan Liu",
        "Fei Wu",
        "Kun Kuang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/a6ce61cdd3c6852ed997d3c1885a62f34add82df",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19838",
      "title": "FoodTaxo: Generating Food Taxonomies with Large Language Models",
      "authors": [
        "Pascal Wullschleger",
        "Majid Zarharan",
        "Donnacha Daly",
        "Marc Pouly",
        "Jennifer Foster"
      ],
      "abstract": "We investigate the utility of Large Language Models for automated taxonomy generation and completion specifically applied to taxonomies from the food technology industry. We explore the extent to which taxonomies can be completed from a seed taxonomy or generated without a seed from a set of known concepts, in an iterative fashion using recent prompting techniques. Experiments on five taxonomies using an open-source LLM (Llama-3), while promising, point to the difficulty of correctly placing inner nodes.",
      "arxiv_url": "https://arxiv.org/abs/2505.19838",
      "pdf_url": "https://arxiv.org/pdf/2505.19838",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10413",
      "title": "Hierarchical Document Refinement for Long-context Retrieval-augmented Generation",
      "authors": [
        "Jiajie Jin",
        "Xiaoxi Li",
        "Guanting Dong",
        "Yuyao Zhang",
        "Yutao Zhu",
        "Yongkang Wu",
        "Zhonghua Li",
        "Qi Ye",
        "Zhicheng Dou"
      ],
      "abstract": "Real-world RAG applications often encounter long-context input scenarios, where redundant information and noise results in higher inference costs and reduced performance. To address these challenges, we propose LongRefiner, an efficient plug-and-play refiner that leverages the inherent structural characteristics of long documents. LongRefiner employs dual-level query analysis, hierarchical document structuring, and adaptive refinement through multi-task learning on a single foundation model. Experiments on seven QA datasets demonstrate that LongRefiner achieves competitive performance in various scenarios while using 10x fewer computational costs and latency compared to the best baseline. Further analysis validates that LongRefiner is scalable, efficient, and effective, providing practical insights for real-world long-text RAG applications. Our code is available at https://github.com/ignorejjj/LongRefiner.",
      "arxiv_url": "https://arxiv.org/abs/2505.10413",
      "pdf_url": "https://arxiv.org/pdf/2505.10413",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-05-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01205",
      "title": "Trick or Neat: Adversarial Ambiguity and Language Model Evaluation",
      "authors": [
        "Antonia Karamolegkou",
        "Oliver Eberle",
        "Phillip Rust",
        "Carina Kauf",
        "Anders Søgaard"
      ],
      "abstract": "Detecting ambiguity is important for language understanding, including uncertainty estimation, humour detection, and processing garden path sentences. We assess language models' sensitivity to ambiguity by introducing an adversarial ambiguity dataset that includes syntactic, lexical, and phonological ambiguities along with adversarial variations (e.g., word-order changes, synonym replacements, and random-based alterations). Our findings show that direct prompting fails to robustly identify ambiguity, while linear probes trained on model representations can decode ambiguity with high accuracy, sometimes exceeding 90\\%. Our results offer insights into the prompting paradigm and how language models encode ambiguity at different layers. We release both our code and data: https://github.com/coastalcph/lm_ambiguity.",
      "arxiv_url": "https://arxiv.org/abs/2506.01205",
      "pdf_url": "https://arxiv.org/pdf/2506.01205",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.17781",
      "title": "Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models",
      "authors": [
        "Miguel Romero",
        "Shuoyang Ding",
        "Corey D. Barret",
        "Georgiana Dinu",
        "G. Karypis"
      ],
      "abstract": "Dense embeddings are fundamental to modern machine learning systems, powering Retrieval-Augmented Generation (RAG), information retrieval, and representation learning. While instruction-conditioning has become the dominant approach for embedding specialization, its direct application to low-capacity models imposes fundamental representational constraints that limit the performance gains derived from specialization. In this paper, we analyze these limitations and introduce the Mixture of Task Experts (MoTE) transformer block, which leverages task-specialized parameters trained with Task-Aware Contrastive Learning (\\tacl) to enhance the model ability to generate specialized embeddings. Empirical results show that MoTE achieves $64\\%$ higher performance gains in retrieval datasets ($+3.27 \\rightarrow +5.21$) and $43\\%$ higher performance gains across all datasets ($+1.81 \\rightarrow +2.60$). Critically, these gains are achieved without altering instructions, training data, inference time, or number of active parameters.",
      "arxiv_url": "https://arxiv.org/abs/2506.17781",
      "pdf_url": "https://arxiv.org/pdf/2506.17781",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-06-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.06932",
      "title": "Harnessing Large Language Models for Disaster Management: A Survey",
      "authors": [
        "Zhenyu Lei",
        "Yushun Dong",
        "Weiyu Li",
        "Rong Ding",
        "Qi Wang",
        "Jundong Li"
      ],
      "abstract": "Large language models (LLMs) have revolutionized scientific research with their exceptional capabilities and transformed various fields. Among their practical applications, LLMs have been playing a crucial role in mitigating threats to human life, infrastructure, and the environment. Despite growing research in disaster LLMs, there remains a lack of systematic review and in-depth analysis of LLMs for natural disaster management. To address the gap, this paper presents a comprehensive survey of existing LLMs in natural disaster management, along with a taxonomy that categorizes existing works based on disaster phases and application scenarios. By collecting public datasets and identifying key challenges and opportunities, this study aims to guide the professional community in developing advanced LLMs for disaster management to enhance the resilience against natural disasters.",
      "arxiv_url": "https://arxiv.org/abs/2501.06932",
      "pdf_url": "https://arxiv.org/pdf/2501.06932",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18802",
      "title": "Language Models Grow Less Humanlike beyond Phase Transition",
      "authors": [
        "Tatsuya Aoyama",
        "E. Wilcox"
      ],
      "abstract": "LMs'alignment with human reading behavior (i.e. psychometric predictive power; PPP) is known to improve during pretraining up to a tipping point, beyond which it either plateaus or degrades. Various factors, such as word frequency, recency bias in attention, and context size, have been theorized to affect PPP, yet there is no current account that explains why such a tipping point exists, and how it interacts with LMs'pretraining dynamics more generally. We hypothesize that the underlying factor is a pretraining phase transition, characterized by the rapid emergence of specialized attention heads. We conduct a series of correlational and causal experiments to show that such a phase transition is responsible for the tipping point in PPP. We then show that, rather than producing attention patterns that contribute to the degradation in PPP, phase transitions alter the subsequent learning dynamics of the model, such that further training keeps damaging PPP.",
      "arxiv_url": "https://arxiv.org/abs/2502.18802",
      "pdf_url": "https://arxiv.org/pdf/2502.18802",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04760",
      "title": "Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion",
      "authors": [
        "Lingyuan Liu",
        "Mengxiang Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have shown potential in generating hypothetical documents for query expansion, thereby enhancing information retrieval performance. However, the efficacy of this method is highly dependent on the quality of the generated documents, which often requires complex prompt strategies and the integration of advanced dense retrieval techniques. This can be both costly and computationally intensive. To mitigate these limitations, we explore the use of zero-shot LLM-based query expansion to improve sparse retrieval, particularly for learned sparse retrievers. We introduce a novel fusion ranking framework, Exp4Fuse, which enhances the performance of sparse retrievers through an indirect application of zero-shot LLM-based query expansion. Exp4Fuse operates by simultaneously considering two retrieval routes-one based on the original query and the other on the LLM-augmented query. It then generates two ranked lists using a sparse retriever and fuses them using a modified reciprocal rank fusion method. We conduct extensive evaluations of Exp4Fuse against leading LLM-based query expansion methods and advanced retrieval techniques on three MS MARCO-related datasets and seven low-resource datasets. Experimental results reveal that Exp4Fuse not only surpasses existing LLM-based query expansion methods in enhancing sparse retrievers but also, when combined with advanced sparse retrievers, achieves SOTA results on several benchmarks. This highlights the superior performance and effectiveness of Exp4Fuse in improving query expansion for sparse retrieval.",
      "arxiv_url": "https://arxiv.org/abs/2506.04760",
      "pdf_url": "https://arxiv.org/pdf/2506.04760",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.21508",
      "title": "skLEP: A Slovak General Language Understanding Benchmark",
      "authors": [
        "Marek Suppa",
        "Andrej Ridzik",
        "Daniel Hládek",
        "Tomas Javurek",
        "Viktoria Ondrejova",
        "Kristína Sásiková",
        "Martin Tamajka",
        "Marián Simko"
      ],
      "abstract": "In this work, we introduce skLEP, the first comprehensive benchmark specifically designed for evaluating Slovak natural language understanding (NLU) models. We have compiled skLEP to encompass nine diverse tasks that span token-level, sentence-pair, and document-level challenges, thereby offering a thorough assessment of model capabilities. To create this benchmark, we curated new, original datasets tailored for Slovak and meticulously translated established English NLU resources. Within this paper, we also present the first systematic and extensive evaluation of a wide array of Slovak-specific, multilingual, and English pre-trained language models using the skLEP tasks. Finally, we also release the complete benchmark data, an open-source toolkit facilitating both fine-tuning and evaluation of models, and a public leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering reproducibility and drive future research in Slovak NLU.",
      "arxiv_url": "https://arxiv.org/abs/2506.21508",
      "pdf_url": "https://arxiv.org/pdf/2506.21508",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11211",
      "title": "A Survey of LLM-based Agents in Medicine: How far are we from Baymax?",
      "authors": [
        "Wenxuan Wang",
        "Zizhan Ma",
        "Zheng Wang",
        "Chenghan Wu",
        "Wenting Chen",
        "Xiang Li",
        "Yixuan Yuan"
      ],
      "abstract": "Large Language Models (LLMs) are transforming healthcare through the development of LLM-based agents that can understand, reason about, and assist with medical tasks. This survey provides a comprehensive review of LLM-based agents in medicine, examining their architectures, applications, and challenges. We analyze the key components of medical agent systems, including system profiles, clinical planning mechanisms, medical reasoning frameworks, and external capacity enhancement. The survey covers major application scenarios such as clinical decision support, medical documentation, training simulations, and healthcare service optimization. We discuss evaluation frameworks and metrics used to assess these agents' performance in healthcare settings. While LLM-based agents show promise in enhancing healthcare delivery, several challenges remain, including hallucination management, multimodal integration, implementation barriers, and ethical considerations. The survey concludes by highlighting future research directions, including advances in medical reasoning inspired by recent developments in LLM architectures, integration with physical systems, and improvements in training simulations. This work provides researchers and practitioners with a structured overview of the current state and future prospects of LLM-based agents in medicine.",
      "arxiv_url": "https://arxiv.org/abs/2502.11211",
      "pdf_url": "https://arxiv.org/pdf/2502.11211",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a7da4f6321e88e0d44f5435f4a4424c96b595eb2",
      "title": "Continued Pretraining and Interpretability-Based Evaluation for Low-Resource Languages: A Galician Case Study",
      "authors": [
        "Pablo Rodríguez",
        "Silvia Paniagua Suárez",
        "Pablo Gamallo",
        "Susana Sotelo Docío"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/a7da4f6321e88e0d44f5435f4a4424c96b595eb2",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.07046",
      "title": "A Unified Agentic Framework for Evaluating Conditional Image Generation",
      "authors": [
        "Jifang Wang",
        "Xue Yang",
        "Longyue Wang",
        "Zhenran Xu",
        "Yiyu Wang",
        "Yaowei Wang",
        "Weihua Luo",
        "Kaifu Zhang",
        "Baotian Hu",
        "Min Zhang"
      ],
      "abstract": "Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEval's capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability.",
      "arxiv_url": "https://arxiv.org/abs/2504.07046",
      "pdf_url": "https://arxiv.org/pdf/2504.07046",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-04-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.15372",
      "title": "X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System",
      "authors": [
        "Peng Wang",
        "Ruihan Tao",
        "Qiguang Chen",
        "Mengkang Hu",
        "Libo Qin"
      ],
      "abstract": "Recently, large language model (LLM)-based agents have achieved significant success in interactive environments, attracting significant academic and industrial attention. Despite these advancements, current research predominantly focuses on English scenarios. In reality, there are over 7,000 languages worldwide, all of which demand access to comparable agentic services. Nevertheless, the development of language agents remains inadequate for meeting the diverse requirements of multilingual agentic applications. To fill this gap, we introduce X-WebAgentBench, a novel multilingual agent benchmark in an interactive web environment, which evaluates the planning and interaction performance of language agents across multiple languages, thereby contributing to the advancement of global agent intelligence. Additionally, we assess the performance of various LLMs and cross-lingual alignment methods, examining their effectiveness in enhancing agents. Our findings reveal that even advanced models like GPT-4o, when combined with cross-lingual techniques, fail to achieve satisfactory results. We hope that X-WebAgentBench can serve as a valuable benchmark for multilingual agent scenario in real-world applications.",
      "arxiv_url": "https://arxiv.org/abs/2505.15372",
      "pdf_url": "https://arxiv.org/pdf/2505.15372",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-05-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12799",
      "title": "Towards Text-Image Interleaved Retrieval",
      "authors": [
        "Xin Zhang",
        "Ziqi Dai",
        "Yongqing Li",
        "Yanzhao Zhang",
        "Dingkun Long",
        "Pengjun Xie",
        "Meishan Zhang",
        "Jun Yu",
        "Wenjie Li",
        "Min Zhang"
      ],
      "abstract": "Current multimodal information retrieval studies mainly focus on single-image inputs, which limits real-world applications involving multiple images and text-image interleaved content. In this work, we introduce the text-image interleaved retrieval (TIIR) task, where the query and document are interleaved text-image sequences, and the model is required to understand the semantics from the interleaved context for effective retrieval. We construct a TIIR benchmark based on naturally interleaved wikiHow tutorials, where a specific pipeline is designed to generate interleaved queries. To explore the task, we adapt several off-the-shelf retrievers and build a dense baseline by interleaved multimodal large language model (MLLM). We then propose a novel Matryoshka Multimodal Embedder (MME), which compresses the number of visual tokens at different granularity, to address the challenge of excessive visual tokens in MLLM-based TIIR models. Experiments demonstrate that simple adaption of existing models does not consistently yield effective results. Our MME achieves significant improvements over the baseline by substantially fewer visual tokens. We provide extensive analysis and will release the dataset and code to facilitate future research.",
      "arxiv_url": "https://arxiv.org/abs/2502.12799",
      "pdf_url": "https://arxiv.org/pdf/2502.12799",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.01801",
      "title": "Investigating and Scaling up Code-Switching for Multilingual Language Model Pre-Training",
      "authors": [
        "Zhijun Wang",
        "Jiahuan Li",
        "Hao Zhou",
        "Rongxiang Weng",
        "Jingang Wang",
        "Xin Huang",
        "Xue Han",
        "Junlan Feng",
        "Chao Deng",
        "Shujian Huang"
      ],
      "abstract": "Large language models (LLMs) exhibit remarkable multilingual capabilities despite the extreme language imbalance in the pre-training data. In this paper, we closely examine the reasons behind this phenomenon, focusing on the pre-training corpus. We find that the existence of code-switching, alternating between different languages within a context, is key to multilingual capabilities. We conduct an analysis to investigate code-switching in the pre-training corpus, examining its presence and categorizing it into four types within two quadrants. We then assess its impact on multilingual performance. These types of code-switching data are unbalanced in proportions and demonstrate different effects on facilitating language transfer. To better explore the power of code-switching for language alignment during pre-training, we investigate the strategy of synthetic code-switching. We continuously scale up the synthetic code-switching data and observe remarkable improvements in both benchmarks and representation space. Extensive experiments indicate that incorporating synthetic code-switching data enables better language alignment and generalizes well to high, medium, and low-resource languages with pre-training corpora of varying qualities.",
      "arxiv_url": "https://arxiv.org/abs/2504.01801",
      "pdf_url": "https://arxiv.org/pdf/2504.01801",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a864f48746d8276df2ff3ca31455a20fb81b19a0",
      "title": "Multilingual Arbitration: Optimizing Data Pools to Accelerate Multilingual Progress",
      "authors": [
        "Ayomide Odumakinde",
        "Daniel D'souza",
        "Pat Verga",
        "B. Ermiş",
        "Sara Hooker"
      ],
      "abstract": "Synthetic data has driven recent state-of-the-art advancements, but reliance on a single oracle teacher model can lead to model collapse and bias propagation. These issues are particularly severe in multilingual settings, where no single model excels across all languages. In this study, we propose multilingual arbitration , which exploits performance variations among multiple models for each language. By strategically routing samples through a diverse set of models, each with unique strengths, we mitigate these challenges and enhance multilingual performance. Extensive experiments with state-of-the-art models demonstrate that our approach significantly surpasses single-teacher distillation, achieving up to 80% win rates over proprietary and open-weight models like Gemma 2, Llama 3.1, and Mistral v0.3, with the largest improvements in low-resource languages.",
      "arxiv_url": "https://www.semanticscholar.org/paper/a864f48746d8276df2ff3ca31455a20fb81b19a0",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.07340",
      "title": "Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering",
      "authors": [
        "Shuzheng Si",
        "Haozhe Zhao",
        "Gang Chen",
        "Cheng Gao",
        "Yuzhuo Bai",
        "Zhitong Wang",
        "Kaikai An",
        "Kangyang Luo",
        "Chen Qian",
        "Fanchao Qi",
        "Baobao Chang",
        "Maosong Sun"
      ],
      "abstract": "Training LLMs on data containing unfamiliar knowledge during the instruction tuning stage can encourage hallucinations. To address this challenge, we introduce NOVA, a novel framework designed to identify high-quality data that aligns well with the LLM's learned knowledge to reduce hallucinations. NOVA includes Internal Consistency Probing (ICP) and Semantic Equivalence Identification (SEI) to measure how familiar the LLM is with instruction data. Specifically, ICP evaluates the LLM's understanding of the given instruction by calculating the tailored consistency among multiple self-generated responses. SEI further assesses the familiarity of the LLM with the target response by comparing it to the generated responses, using the proposed semantic clustering and well-designed voting strategy. Finally, to ensure the quality of selected samples, we introduce an expert-aligned reward model, considering characteristics beyond just familiarity. By considering data quality and avoiding unfamiliar data, we can utilize the selected data to effectively align LLMs to follow instructions and hallucinate less.",
      "arxiv_url": "https://arxiv.org/abs/2502.07340",
      "pdf_url": "https://arxiv.org/pdf/2502.07340",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.01572",
      "title": "PipeSpec: Breaking Stage Dependencies in Hierarchical LLM Decoding",
      "authors": [
        "Bradley McDanel",
        "Sai Qian Zhang",
        "Yunhai Hu",
        "Zining Liu"
      ],
      "abstract": "Speculative decoding accelerates large language model inference by using smaller draft models to generate candidate tokens for parallel verification. However, current approaches are limited by sequential stage dependencies that prevent full hardware utilization. We present PipeSpec, a framework that generalizes speculative decoding to $k$ models arranged in a hierarchical pipeline, enabling asynchronous execution with lightweight coordination for prediction verification and rollback. Our analytical model characterizes token generation rates across pipeline stages and proves guaranteed throughput improvements over traditional decoding for any non-zero acceptance rate. We further derive closed-form expressions for steady-state verification probabilities that explain the empirical benefits of pipeline depth. Experimental results show that PipeSpec achieves up to 2.54$\\times$ speedup while outperforming state-of-the-art methods. We validate PipeSpec across text summarization and code generation tasks using LLaMA 2 and 3 models, demonstrating that pipeline efficiency increases with model depth, providing a scalable approach to accelerating LLM inference on multi-device systems.",
      "arxiv_url": "https://arxiv.org/abs/2505.01572",
      "pdf_url": "https://arxiv.org/pdf/2505.01572",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11493",
      "title": "DAST: Context-Aware Compression in LLMs via Dynamic Allocation of Soft Tokens",
      "authors": [
        "Shaoshen Chen",
        "Yangning Li",
        "Zishan Xu",
        "Yinghui Li",
        "Xin Su",
        "Zifei Shan",
        "Hai-Tao Zheng"
      ],
      "abstract": "Large Language Models (LLMs) face computational inefficiencies and redundant processing when handling long context inputs, prompting a focus on compression techniques. While existing semantic vector-based compression methods achieve promising performance, these methods fail to account for the intrinsic information density variations between context chunks, instead allocating soft tokens uniformly across context chunks. This uniform distribution inevitably diminishes allocation to information-critical regions. To address this, we propose Dynamic Allocation of Soft Tokens (DAST), a simple yet effective method that leverages the LLM's intrinsic understanding of contextual relevance to guide compression. DAST combines perplexity-based local information with attention-driven global information to dynamically allocate soft tokens to the informative-rich chunks, enabling effective, context-aware compression. Experimental results across multiple benchmarks demonstrate that DAST surpasses state-of-the-art methods.",
      "arxiv_url": "https://arxiv.org/abs/2502.11493",
      "pdf_url": "https://arxiv.org/pdf/2502.11493",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14019",
      "title": "Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text Generation Systems",
      "authors": [
        "Myra Cheng",
        "Su Lin Blodgett",
        "Alicia DeVrio",
        "Lisa Egede",
        "Alexandra Olteanu"
      ],
      "abstract": "As text generation systems' outputs are increasingly anthropomorphic -- perceived as human-like -- scholars have also increasingly raised concerns about how such outputs can lead to harmful outcomes, such as users over-relying or developing emotional dependence on these systems. How to intervene on such system outputs to mitigate anthropomorphic behaviors and their attendant harmful outcomes, however, remains understudied. With this work, we aim to provide empirical and theoretical grounding for developing such interventions. To do so, we compile an inventory of interventions grounded both in prior literature and a crowdsourcing study where participants edited system outputs to make them less human-like. Drawing on this inventory, we also develop a conceptual framework to help characterize the landscape of possible interventions, articulate distinctions between different types of interventions, and provide a theoretical basis for evaluating the effectiveness of different interventions.",
      "arxiv_url": "https://arxiv.org/abs/2502.14019",
      "pdf_url": "https://arxiv.org/pdf/2502.14019",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a8ee36a75665860ff5f226118c7b64c85bc4ebcc",
      "title": "Decoding LLM Personality Measurement: Forced-Choice vs. Likert",
      "authors": [
        "Xiaoyu Li",
        "Haoran Shi",
        "Zengyi Yu",
        "Yukun Tu",
        "Chanjin Zheng"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/a8ee36a75665860ff5f226118c7b64c85bc4ebcc",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24788",
      "title": "Drop Dropout on Single-Epoch Language Model Pretraining",
      "authors": [
        "Houjun Liu",
        "John Bauer",
        "Christopher D. Manning"
      ],
      "abstract": "Originally, dropout was seen as a breakthrough regularization technique that reduced overfitting and improved performance in almost all applications of deep learning by reducing overfitting. Yet, single-epoch pretraining tasks common to modern LLMs yield minimal overfitting, leading to dropout not being used for large LLMs. Nevertheless, no thorough empirical investigation has been done on the role of dropout in LM pretraining. Through experiments in single-epoch pretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs with varying levels of dropout, we find that downstream performance in language modeling, morpho-syntax (BLiMP), question answering (SQuAD), and natural-language inference (MNLI) improves when dropout is not applied during pretraining. We additionally find that the recently-introduced\"early dropout\"also degrades performance over applying no dropout at all. We further investigate the models' editability, and find that models trained without dropout are more successful in gradient-based model editing (MEND) and equivalent in representation-based model editing (ReFT). Therefore, we advocate to drop dropout during single-epoch pretraining.",
      "arxiv_url": "https://arxiv.org/abs/2505.24788",
      "pdf_url": "https://arxiv.org/pdf/2505.24788",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.20648",
      "title": "TN-Eval: Rubric and Evaluation Protocols for Measuring the Quality of Behavioral Therapy Notes",
      "authors": [
        "Raj Sanjay Shah",
        "Lei Xu",
        "Qian Liu",
        "Jon Burnsky",
        "Drew Bertagnolli",
        "Chaitanya P. Shivade"
      ],
      "abstract": "Behavioral therapy notes are important for both legal compliance and patient care. Unlike progress notes in physical health, quality standards for behavioral therapy notes remain underdeveloped. To address this gap, we collaborated with licensed therapists to design a comprehensive rubric for evaluating therapy notes across key dimensions: completeness, conciseness, and faithfulness. Further, we extend a public dataset of behavioral health conversations with therapist-written notes and LLM-generated notes, and apply our evaluation framework to measure their quality. We find that: (1) A rubric-based manual evaluation protocol offers more reliable and interpretable results than traditional Likert-scale annotations. (2) LLMs can mimic human evaluators in assessing completeness and conciseness but struggle with faithfulness. (3) Therapist-written notes often lack completeness and conciseness, while LLM-generated notes contain hallucination. Surprisingly, in a blind test, therapists prefer and judge LLM-generated notes to be superior to therapist-written notes.",
      "arxiv_url": "https://arxiv.org/abs/2503.20648",
      "pdf_url": "https://arxiv.org/pdf/2503.20648",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00780",
      "title": "Do not Abstain! Identify and Solve the Uncertainty",
      "authors": [
        "Jingyu Liu",
        "Jingquan Peng",
        "Xiaopeng Wu",
        "Xubin Li",
        "Tiezheng Ge",
        "Bo Zheng",
        "Yong Liu"
      ],
      "abstract": "Despite the widespread application of Large Language Models (LLMs) across various domains, they frequently exhibit overconfidence when encountering uncertain scenarios, yet existing solutions primarily rely on evasive responses (e.g.,\"I don't know\") overlooks the opportunity of identifying and addressing the uncertainty to generate more satisfactory responses. To systematically investigate and improve LLMs' ability of recognizing and addressing the source of uncertainty, we introduce \\textbf{ConfuseBench}, a benchmark mainly focus on three types of uncertainty: document scarcity, limited capability, and query ambiguity. Experiments with ConfuseBench reveal that current LLMs struggle to accurately identify the root cause of uncertainty and solve it. They prefer to attribute uncertainty to query ambiguity while overlooking capability limitations, especially for those weaker models. To tackle this challenge, we first generate context-aware inquiries that highlight the confusing aspect of the original query. Then we judge the source of uncertainty based on the uniqueness of the inquiry's answer. Further we use an on-policy training method, InteractDPO to generate better inquiries. Experimental results demonstrate the efficacy of our approach.",
      "arxiv_url": "https://arxiv.org/abs/2506.00780",
      "pdf_url": "https://arxiv.org/pdf/2506.00780",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.02807",
      "title": "CAMI: A Counselor Agent Supporting Motivational Interviewing through State Inference and Topic Exploration",
      "authors": [
        "Yizhe Yang",
        "Palakorn Achananuparp",
        "Heyan Huang",
        "Jing Jiang",
        "Kit Phey Leng",
        "Nicholas Gabriel Lim",
        "Cameron Tan Shi Ern",
        "Ee-Peng Lim"
      ],
      "abstract": "Conversational counselor agents have become essential tools for addressing the rising demand for scalable and accessible mental health support. This paper introduces CAMI, a novel automated counselor agent grounded in Motivational Interviewing (MI) -- a client-centered counseling approach designed to address ambivalence and facilitate behavior change. CAMI employs a novel STAR framework, consisting of client's state inference, motivation topic exploration, and response generation modules, leveraging large language models (LLMs). These components work together to evoke change talk, aligning with MI principles and improving counseling outcomes for clients from diverse backgrounds. We evaluate CAMI's performance through both automated and manual evaluations, utilizing simulated clients to assess MI skill competency, client's state inference accuracy, topic exploration proficiency, and overall counseling success. Results show that CAMI not only outperforms several state-of-the-art methods but also shows more realistic counselor-like behavior. Additionally, our ablation study underscores the critical roles of state inference and topic exploration in achieving this performance.",
      "arxiv_url": "https://arxiv.org/abs/2502.02807",
      "pdf_url": "https://arxiv.org/pdf/2502.02807",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12408",
      "title": "On the Robust Approximation of ASR Metrics",
      "authors": [
        "Abdul Waheed",
        "Hanin Atwany",
        "Rita Singh",
        "Bhiksha Raj"
      ],
      "abstract": "Recent advances in speech foundation models are largely driven by scaling both model size and data, enabling them to perform a wide range of tasks, including speech recognition. Traditionally, ASR models are evaluated using metrics like Word Error Rate (WER) and Character Error Rate (CER), which depend on ground truth labels. As a result of limited labeled data from diverse domains and testing conditions, the true generalization capabilities of these models beyond standard benchmarks remain unclear. Moreover, labeling data is both costly and time-consuming. To address this, we propose a novel label-free approach for approximating ASR performance metrics, eliminating the need for ground truth labels. Our method utilizes multimodal embeddings in a unified space for speech and transcription representations, combined with a high-quality proxy model to compute proxy metrics. These features are used to train a regression model to predict key ASR metrics like Word Error Rate (WER) and Character Error Rate (CER). We experiment with over 40 models across 14 datasets representing both standard and in-the-wild testing conditions. Our results show that we approximate the metrics within a single-digit absolute difference across all experimental configurations, outperforming the most recent baseline by more than 50\\%.",
      "arxiv_url": "https://arxiv.org/abs/2502.12408",
      "pdf_url": "https://arxiv.org/pdf/2502.12408",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04579",
      "title": "Selecting Demonstrations for Many-Shot In-Context Learning via Gradient Matching",
      "authors": [
        "Jianfei Zhang",
        "Bei Li",
        "Jun Bai",
        "Rumei Li",
        "Yanmeng Wang",
        "Chenghua Lin",
        "Wenge Rong"
      ],
      "abstract": "In-Context Learning (ICL) empowers Large Language Models (LLMs) for rapid task adaptation without Fine-Tuning (FT), but its reliance on demonstration selection remains a critical challenge. While many-shot ICL shows promising performance through scaled demonstrations, the selection method for many-shot demonstrations remains limited to random selection in existing work. Since the conventional instance-level retrieval is not suitable for many-shot scenarios, we hypothesize that the data requirements for in-context learning and fine-tuning are analogous. To this end, we introduce a novel gradient matching approach that selects demonstrations by aligning fine-tuning gradients between the entire training set of the target task and the selected examples, so as to approach the learning effect on the entire training set within the selected examples. Through gradient matching on relatively small models, e.g., Qwen2.5-3B or Llama3-8B, our method consistently outperforms random selection on larger LLMs from 4-shot to 128-shot scenarios across 9 diverse datasets. For instance, it surpasses random selection by 4% on Qwen2.5-72B and Llama3-70B, and by around 2% on 5 closed-source LLMs. This work unlocks more reliable and effective many-shot ICL, paving the way for its broader application.",
      "arxiv_url": "https://arxiv.org/abs/2506.04579",
      "pdf_url": "https://arxiv.org/pdf/2506.04579",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a9b85bd869ccde971f21e2863689d77d4825af90",
      "title": "Learn to Memorize: Scalable Continual Learning in Semiparametric Models with Mixture-of-Neighbors Induction Memory",
      "authors": [
        "Guangyue Peng",
        "Tao Ge",
        "Wen Luo",
        "Wei Li",
        "Houfeng Wang"
      ],
      "abstract": "Semiparametric language models (LMs) have shown promise in various Natural Language Processing (NLP) tasks. However, they utilize non-parametric memory as static storage, which lacks learning capability and remains disconnected from the internal information flow of the parametric models, limiting scalability and efficiency. Based on recent interpretability theories of LMs, we reconceptualize the non-parametric memory represented by k NN-LM as a learnable Mixture-of-Neighbors Induction Memory (MoNIM), which synergizes the induction capabilities of attention heads with the memorization strength of feed-forward networks (FFN). By integrating into the model’s information flow, MoNIM functions as an FFN-like bypass layer within the Transformer architecture, enabling effective learning of new knowledge. Extensive experiments demonstrate that MoNIM is a retentive and scalable continual learner in both data-and model-wise, enhancing the scalability and continual learning performance of semiparametric LMs. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/a9b85bd869ccde971f21e2863689d77d4825af90",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.16858",
      "title": "Planning with Diffusion Models for Target-Oriented Dialogue Systems",
      "authors": [
        "Hanwen Du",
        "B. Peng",
        "Xia Ning"
      ],
      "abstract": "Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM era, where strategic dialogue planning is crucial for directing conversations toward specific targets. However, existing dialogue planning methods generate dialogue plans in a step-by-step sequential manner, and may suffer from compounding errors and myopic actions. To address these limitations, we introduce a novel dialogue planning framework, DiffTOD, which leverages diffusion models to enable non-sequential dialogue planning. DiffTOD formulates dialogue planning as a trajectory generation problem with conditional guidance, and leverages a diffusion language model to estimate the likelihood of the dialogue trajectory. To optimize the dialogue action strategies, DiffTOD introduces three tailored guidance mechanisms for different target types, offering flexible guidance toward diverse TOD targets at test time. Extensive experiments across three diverse TOD settings show that DiffTOD can effectively perform non-myopic lookahead exploration and optimize action strategies over a long horizon through non-sequential dialogue planning, and demonstrates strong flexibility across complex and diverse dialogue scenarios. Our code and data are accessible through https://github.com/ninglab/DiffTOD.",
      "arxiv_url": "https://arxiv.org/abs/2504.16858",
      "pdf_url": "https://arxiv.org/pdf/2504.16858",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.16728",
      "title": "IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery",
      "authors": [
        "Aniketh Garikaparthi",
        "Manasi S. Patwardhan",
        "L. Vig",
        "Arman Cohan"
      ],
      "abstract": "The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System",
      "arxiv_url": "https://arxiv.org/abs/2504.16728",
      "pdf_url": "https://arxiv.org/pdf/2504.16728",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "a9ed1ec80fd874a4b127b691437717b77047e13e",
      "title": "Weak-to-Strong Honesty Alignment via Learning-to-Rank Supervision",
      "authors": [
        "Yunfan Xie",
        "Lixin Zou",
        "Dan Luo",
        "Min Tang",
        "Chenliang Li"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/a9ed1ec80fd874a4b127b691437717b77047e13e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00737",
      "title": "Narrative Media Framing in Political Discourse",
      "authors": [
        "Yulia Otmakhova",
        "Lea Frermann"
      ],
      "abstract": "Narrative frames are a powerful way of conceptualizing and communicating complex, controversial ideas, however automated frame analysis to date has mostly overlooked this framing device. In this paper, we connect elements of narrativity with fundamental aspects of framing, and present a framework which formalizes and operationalizes such aspects. We annotate and release a data set of news articles in the climate change domain, analyze the dominance of narrative frame components across political leanings, and test LLMs in their ability to predict narrative frames and their components. Finally, we apply our framework in an unsupervised way to elicit components of narrative framing in a second domain, the COVID-19 crisis, where our predictions are congruent with prior theoretical work showing the generalizability of our approach.",
      "arxiv_url": "https://arxiv.org/abs/2506.00737",
      "pdf_url": "https://arxiv.org/pdf/2506.00737",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.13126",
      "title": "Preference Curriculum: LLMs Should Always Be Pretrained on Their Preferred Data",
      "authors": [
        "Xuemiao Zhang",
        "Liangyu Xu",
        "Feiyu Duan",
        "Yongwei Zhou",
        "Sirui Wang",
        "Jingang Wang",
        "Xunliang Cai"
      ],
      "abstract": "Large language models (LLMs) generally utilize a consistent data distribution throughout the pretraining process. However, as the model's capability improves, it is intuitive that its data preferences dynamically change, indicating the need for pretraining with different data at various training stages. To achieve it, we propose the Perplexity Difference (PD) based Preference Curriculum learning (PDPC) framework, which always perceives and uses the data preferred by LLMs to train and boost them. First, we introduce the PD metric to quantify the difference in how challenging a sample is for weak versus strong models. Samples with high PD are more challenging for weak models to learn and are more suitable to be arranged in the later stage of pretraining. Second, we propose the preference function to approximate and predict the data preference of the LLM at any training step, so as to complete the arrangement of the dataset offline and ensure continuous training without interruption. Experimental results on 1.3B and 3B models demonstrate that PDPC significantly surpasses baselines. Notably, the 3B model trained on 1T tokens achieves an increased average accuracy of over 8.1% across MMLU and CMMLU.",
      "arxiv_url": "https://arxiv.org/abs/2501.13126",
      "pdf_url": "https://arxiv.org/pdf/2501.13126",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.08042",
      "title": "A General Framework to Evaluate Methods for Assessing Dimensions of Lexical Semantic Change Using LLM-Generated Synthetic Data",
      "authors": [
        "Naomi Baes",
        "Raphael Merx",
        "Nick Haslam",
        "Ekaterina Vylomova",
        "Haim Dubossarsky"
      ],
      "abstract": "Lexical Semantic Change (LSC) provides insight into cultural and social dynamics. Yet, the validity of methods for measuring different kinds of LSC remains unestablished due to the absence of historical benchmark datasets. To address this gap, we propose LSC-Eval, a novel three-stage general-purpose evaluation framework to: (1) develop a scalable methodology for generating synthetic datasets that simulate theory-driven LSC using In-Context Learning and a lexical database; (2) use these datasets to evaluate the sensitivity of computational methods to synthetic change; and (3) assess their suitability for detecting change in specific dimensions and domains. We apply LSC-Eval to simulate changes along the Sentiment, Intensity, and Breadth (SIB) dimensions, as defined in the SIBling framework, using examples from psychology. We then evaluate the ability of selected methods to detect these controlled interventions. Our findings validate the use of synthetic benchmarks, demonstrate that tailored methods effectively detect changes along SIB dimensions, and reveal that a state-of-the-art LSC model faces challenges in detecting affective dimensions of LSC. LSC-Eval offers a valuable tool for dimension- and domain-specific benchmarking of LSC methods, with particular relevance to the social sciences.",
      "arxiv_url": "https://arxiv.org/abs/2503.08042",
      "pdf_url": "https://arxiv.org/pdf/2503.08042",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.02016",
      "title": "Mind the (Belief) Gap: Group Identity in the World of LLMs",
      "authors": [
        "Angana Borah",
        "Marwa Houalla",
        "Rada Mihalcea"
      ],
      "abstract": "Social biases and belief-driven behaviors can significantly impact Large Language Models (LLMs) decisions on several tasks. As LLMs are increasingly used in multi-agent systems for societal simulations, their ability to model fundamental group psychological characteristics remains critical yet under-explored. In this study, we present a multi-agent framework that simulates belief congruence, a classical group psychology theory that plays a crucial role in shaping societal interactions and preferences. Our findings reveal that LLMs exhibit amplified belief congruence compared to humans, across diverse contexts. We further investigate the implications of this behavior on two downstream tasks: (1) misinformation dissemination and (2) LLM learning, finding that belief congruence in LLMs increases misinformation dissemination and impedes learning. To mitigate these negative impacts, we propose strategies inspired by: (1) contact hypothesis, (2) accuracy nudges, and (3) global citizenship framework. Our results show that the best strategies reduce misinformation dissemination by up to 37% and enhance learning by 11%. Bridging social psychology and AI, our work provides insights to navigate real-world interactions using LLMs while addressing belief-driven biases.",
      "arxiv_url": "https://arxiv.org/abs/2503.02016",
      "pdf_url": "https://arxiv.org/pdf/2503.02016",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.07424",
      "title": "RomanLens: The Role Of Latent Romanization In Multilinguality In LLMs",
      "authors": [
        "Alan Saji",
        "Jaavid Aktar Husain",
        "Thanmay Jayakumar",
        "Raj Dabre",
        "Anoop Kunchukuttan",
        "Mitesh M. Khapra",
        "Ratish Puduppully Nilekani Centre at AI4Bharat",
        "Singapore University of Technology",
        "Design",
        "I. I. T. Madras",
        "India",
        "National Institute of Information",
        "Communications Technology",
        "Kyoto",
        "Japan",
        "I. Bombay",
        "Microsoft",
        "IT University of Copenhagen"
      ],
      "abstract": "Large Language Models (LLMs) exhibit strong multilingual performance despite being predominantly trained on English-centric corpora. This raises a fundamental question: How do LLMs achieve such multilingual capabilities? Focusing on languages written in non-Roman scripts, we investigate the role of Romanization - the representation of non-Roman scripts using Roman characters - as a potential bridge in multilingual processing. Using mechanistic interpretability techniques, we analyze next-token generation and find that intermediate layers frequently represent target words in Romanized form before transitioning to native script, a phenomenon we term Latent Romanization. Further, through activation patching experiments, we demonstrate that LLMs encode semantic concepts similarly across native and Romanized scripts, suggesting a shared underlying representation. Additionally, for translation into non-Roman script languages, our findings reveal that when the target language is in Romanized form, its representations emerge earlier in the model's layers compared to native script. These insights contribute to a deeper understanding of multilingual representation in LLMs and highlight the implicit role of Romanization in facilitating language transfer.",
      "arxiv_url": "https://arxiv.org/abs/2502.07424",
      "pdf_url": "https://arxiv.org/pdf/2502.07424",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11095",
      "title": "A Survey of Large Language Models in Psychotherapy: Current Landscape and Future Directions",
      "authors": [
        "Hongbin Na",
        "Yining Hua",
        "Zimu Wang",
        "Tao Shen",
        "Beibei Yu",
        "Lilin Wang",
        "Wei Wang",
        "J. Torous",
        "Ling Chen"
      ],
      "abstract": "Mental health is increasingly critical in contemporary healthcare, with psychotherapy demanding dynamic, context-sensitive interactions that traditional NLP methods struggle to capture. Large Language Models (LLMs) offer significant potential for addressing this gap due to their ability to handle extensive context and multi-turn reasoning. This review introduces a conceptual taxonomy dividing psychotherapy into interconnected stages--assessment, diagnosis, and treatment--to systematically examine LLM advancements and challenges. Our comprehensive analysis reveals imbalances in current research, such as a focus on common disorders, linguistic biases, fragmented methods, and limited theoretical integration. We identify critical challenges including capturing dynamic symptom fluctuations, overcoming linguistic and cultural biases, and ensuring diagnostic reliability. Highlighting future directions, we advocate for continuous multi-stage modeling, real-time adaptive systems grounded in psychological theory, and diversified research covering broader mental disorders and therapeutic approaches, aiming toward more holistic and clinically integrated psychotherapy LLMs systems.",
      "arxiv_url": "https://arxiv.org/abs/2502.11095",
      "pdf_url": "https://arxiv.org/pdf/2502.11095",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01357",
      "title": "KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors",
      "authors": [
        "Zhiyang Qi",
        "Takumasa Kaneko",
        "Keiko Takamizo",
        "Mariko Ukiyo",
        "Michimasa Inaba"
      ],
      "abstract": "Generating psychological counseling responses with language models relies heavily on high-quality datasets. Crowdsourced data collection methods require strict worker training, and data from real-world counseling environments may raise privacy and ethical concerns. While recent studies have explored using large language models (LLMs) to augment psychological counseling dialogue datasets, the resulting data often suffers from limited diversity and authenticity. To address these limitations, this study adopts a role-playing approach where trained counselors simulate counselor-client interactions, ensuring high-quality dialogues while mitigating privacy risks. Using this method, we construct KokoroChat, a Japanese psychological counseling dialogue dataset comprising 6,589 long-form dialogues, each accompanied by comprehensive client feedback. Experimental results demonstrate that fine-tuning open-source LLMs with KokoroChat improves both the quality of generated counseling responses and the automatic evaluation of counseling dialogues. The KokoroChat dataset is available at https://github.com/UEC-InabaLab/KokoroChat.",
      "arxiv_url": "https://arxiv.org/abs/2506.01357",
      "pdf_url": "https://arxiv.org/pdf/2506.01357",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18439",
      "title": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning",
      "authors": [
        "Chanwoo Park",
        "Seungju Han",
        "Xingzhi Guo",
        "A. Ozdaglar",
        "Kaiqing Zhang",
        "Joo-Kyung Kim"
      ],
      "abstract": "Leveraging multiple large language models (LLMs) to build collaborative multi-agentic workflows has demonstrated significant potential. However, most previous studies focus on prompting the out-of-the-box LLMs, relying on their innate capability for collaboration, which may not improve LLMs'performance as shown recently. In this paper, we introduce a new post-training paradigm MAPoRL (Multi-Agent Post-co-training for collaborative LLMs with Reinforcement Learning), to explicitly elicit the collaborative behaviors and further unleash the power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first generate their own responses independently and engage in a multi-turn discussion to collaboratively improve the final answer. In the end, a MAPoRL verifier evaluates both the answer and the discussion, by assigning a score that verifies the correctness of the answer, while adding incentives to encourage corrective and persuasive discussions. The score serves as the co-training reward, and is then maximized through multi-agent RL. Unlike existing LLM post-training paradigms, MAPoRL advocates the co-training of multiple LLMs together using RL for better generalization. Accompanied by analytical insights, our experiments demonstrate that training individual LLMs alone is insufficient to induce effective collaboration. In contrast, multi-agent co-training can boost the collaboration performance across benchmarks, with generalization to unseen domains.",
      "arxiv_url": "https://arxiv.org/abs/2502.18439",
      "pdf_url": "https://arxiv.org/pdf/2502.18439",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04595",
      "title": "Hierarchical-Task-Aware Multi-modal Mixture of Incremental LoRA Experts for Embodied Continual Learning",
      "authors": [
        "Ziqi Jia",
        "Anmin Wang",
        "Xiaoyang Qu",
        "Xiaowen Yang",
        "Jianzong Wang"
      ],
      "abstract": "Previous continual learning setups for embodied intelligence focused on executing low-level actions based on human commands, neglecting the ability to learn high-level planning and multi-level knowledge. To address these issues, we propose the Hierarchical Embodied Continual Learning Setups (HEC) that divide the agent's continual learning process into two layers: high-level instructions and low-level actions, and define five embodied continual learning sub-setups. Building on these setups, we introduce the Task-aware Mixture of Incremental LoRA Experts (Task-aware MoILE) method. This approach achieves task recognition by clustering visual-text embeddings and uses both a task-level router and a token-level router to select the appropriate LoRA experts. To effectively address the issue of catastrophic forgetting, we apply Singular Value Decomposition (SVD) to the LoRA parameters obtained from prior tasks, preserving key components while orthogonally training the remaining parts. The experimental results show that our method stands out in reducing the forgetting of old tasks compared to other methods, effectively supporting agents in retaining prior knowledge while continuously learning new tasks.",
      "arxiv_url": "https://arxiv.org/abs/2506.04595",
      "pdf_url": "https://arxiv.org/pdf/2506.04595",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00363",
      "title": "Adapting General-Purpose Embedding Models to Private Datasets Using Keyword-based Retrieval",
      "authors": [
        "Yubai Wei",
        "Jiale Han",
        "Yi Yang"
      ],
      "abstract": "Text embedding models play a cornerstone role in AI applications, such as retrieval-augmented generation (RAG). While general-purpose text embedding models demonstrate strong performance on generic retrieval benchmarks, their effectiveness diminishes when applied to private datasets (e.g., company-specific proprietary data), which often contain specialized terminology and lingo. In this work, we introduce BMEmbed, a novel method for adapting general-purpose text embedding models to private datasets. By leveraging the well-established keyword-based retrieval technique (BM25), we construct supervisory signals from the ranking of keyword-based retrieval results to facilitate model adaptation. We evaluate BMEmbed across a range of domains, datasets, and models, showing consistent improvements in retrieval performance. Moreover, we provide empirical insights into how BM25-based signals contribute to improving embeddings by fostering alignment and uniformity, highlighting the value of this approach in adapting models to domain-specific data. We release the source code available at https://github.com/BaileyWei/BMEmbed for the research community.",
      "arxiv_url": "https://arxiv.org/abs/2506.00363",
      "pdf_url": "https://arxiv.org/pdf/2506.00363",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11988",
      "title": "TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique Annotation in Cyber Threat Intelligence Text",
      "authors": [
        "Ahmed Lekssays",
        "Utsav Shukla",
        "H. Sencar",
        "Md. Rizwan Parvez"
      ],
      "abstract": "Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datasets and task-specific optimizations, such as custom hard-negative mining and denoising, resources rarely available in specialized domains. We propose TechniqueRAG, a domain-specific retrieval-augmented generation (RAG) framework that bridges this gap by integrating off-the-shelf retrievers, instruction-tuned LLMs, and minimal text-technique pairs. Our approach addresses data scarcity by fine-tuning only the generation component on limited in-domain examples, circumventing the need for resource-intensive retrieval training. While conventional RAG mitigates hallucination by coupling retrieval and generation, its reliance on generic retrievers often introduces noisy candidates, limiting domain-specific precision. To address this, we enhance retrieval quality and domain specificity through zero-shot LLM re-ranking, which explicitly aligns retrieved candidates with adversarial techniques. Experiments on multiple security benchmarks demonstrate that TechniqueRAG achieves state-of-the-art performance without extensive task-specific optimizations or labeled data, while comprehensive analysis provides further insights.",
      "arxiv_url": "https://arxiv.org/abs/2505.11988",
      "pdf_url": "https://arxiv.org/pdf/2505.11988",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11223",
      "title": "Asymmetric Conflict and Synergy in Post-training for LLM-based Multilingual Machine Translation",
      "authors": [
        "Tong Zheng",
        "Yan Wen",
        "Huiwen Bao",
        "Junfeng Guo",
        "Heng Huang"
      ],
      "abstract": "The emergence of Large Language Models (LLMs) has advanced the multilingual machine translation (MMT), yet the Curse of Multilinguality (CoM) remains a major challenge. Existing work in LLM-based MMT typically mitigates this issue via scaling up training and computation budget, which raises a critical question: Is scaling up the training and computation budget truly necessary for high-quality MMT, or can a deeper understanding of CoM provide a more efficient solution? To explore this problem, we analyze the linguistic conflicts and synergy, the underlying mechanism of CoM during post-training phase. We identify an asymmetric phenomenon in linguistic conflicts and synergy: the dominance of conflicts and synergy varies in different translation directions, leading to sub-optimal adaptation in existing post-training methods. We further find that a significant bottleneck in MMT appears to lie in post-training rather than multilingual pre-training, suggesting the need for more effective adaptation strategies. Building on these new insights, we propose a direction-aware training approach, combined with group-wise model merging, to address asymmetry in linguistic conflicts and synergy explicitly. Leveraging this strategy, our method fine-tunes X-ALMA-13B-Pretrain-trained only with multilingual pre-training-achieving comparable performance to XALMA-13B (only SFT) while using only 20B pretraining tokens and 17B parameters-5.5x fewer pretraining-tokens and 1.7x fewer model size-with just 0.85 COMET drop on Flores-200 testsets of 50 languages.",
      "arxiv_url": "https://arxiv.org/abs/2502.11223",
      "pdf_url": "https://arxiv.org/pdf/2502.11223",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.18337",
      "title": "TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance",
      "authors": [
        "Syed Mekael Wasti",
        "Shou-Yi Hung",
        "Christopher Collins",
        "En-Shiun Annie Lee"
      ],
      "abstract": "Machine translation (MT) post-editing and research data collection often rely on inefficient, disconnected workflows. We introduce TranslationCorrect, an integrated framework designed to streamline these tasks. TranslationCorrect combines MT generation using models like NLLB, automated error prediction using models like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive post-editing interface within a single environment. Built with human-computer interaction (HCI) principles in mind to minimize cognitive load, as confirmed by a user study. For translators, it enables them to correct errors and batch translate efficiently. For researchers, TranslationCorrect exports high-quality span-based annotations in the Error Span Annotation (ESA) format, using an error taxonomy inspired by Multidimensional Quality Metrics (MQM). These outputs are compatible with state-of-the-art error detection models and suitable for training MT or post-editing systems. Our user study confirms that TranslationCorrect significantly improves translation efficiency and user satisfaction over traditional annotation methods.",
      "arxiv_url": "https://arxiv.org/abs/2506.18337",
      "pdf_url": "https://arxiv.org/pdf/2506.18337",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.05328",
      "title": "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models",
      "authors": [
        "Anar Yeginbergen",
        "Maite Oronoz",
        "Rodrigo Agerri"
      ],
      "abstract": "This paper investigates the role of dynamic external knowledge integration in improving counter-argument generation using Large Language Models (LLMs). While LLMs have shown promise in argumentative tasks, their tendency to generate lengthy, potentially unfactual responses highlights the need for more controlled and evidence-based approaches. We introduce a new manually curated dataset of argument and counter-argument pairs specifically designed to balance argumentative complexity with evaluative feasibility. We also propose a new LLM-as-a-Judge evaluation methodology that shows a stronger correlation with human judgments compared to traditional reference-based metrics. Our experimental results demonstrate that integrating dynamic external knowledge from the web significantly improves the quality of generated counter-arguments, particularly in terms of relatedness, persuasiveness, and factuality. The findings suggest that combining LLMs with real-time external knowledge retrieval offers a promising direction for developing more effective and reliable counter-argumentation systems.",
      "arxiv_url": "https://arxiv.org/abs/2503.05328",
      "pdf_url": "https://arxiv.org/pdf/2503.05328",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21224",
      "title": "A Representation Level Analysis of NMT Model Robustness to Grammatical Errors",
      "authors": [
        "Abderrahmane Issam",
        "Yusuf Can Semerci",
        "Jan Scholtes",
        "Gerasimos Spanakis"
      ],
      "abstract": "Understanding robustness is essential for building reliable NLP systems. Unfortunately, in the context of machine translation, previous work mainly focused on documenting robustness failures or improving robustness. In contrast, we study robustness from a model representation perspective by looking at internal model representations of ungrammatical inputs and how they evolve through model layers. For this purpose, we perform Grammatical Error Detection (GED) probing and representational similarity analysis. Our findings indicate that the encoder first detects the grammatical error, then corrects it by moving its representation toward the correct form. To understand what contributes to this process, we turn to the attention mechanism where we identify what we term Robustness Heads. We find that Robustness Heads attend to interpretable linguistic units when responding to grammatical errors, and that when we fine-tune models for robustness, they tend to rely more on Robustness Heads for updating the ungrammatical word representation.",
      "arxiv_url": "https://arxiv.org/abs/2505.21224",
      "pdf_url": "https://arxiv.org/pdf/2505.21224",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ab19c23080065dd5f12ddda9184a3f9a82193779",
      "title": "Dense Retrieval with Quantity Comparison Intent",
      "authors": [
        "Prayas Agrawal",
        "Nandeesh Kumar",
        "M. Chelliah",
        "Surender Kumar",
        "Soumen Chakrabarti"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/ab19c23080065dd5f12ddda9184a3f9a82193779",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.06036",
      "title": "Multi-Sense Embeddings for Language Models and Knowledge Distillation",
      "authors": [
        "Qitong Wang",
        "Mohammed J. Zaki",
        "Georgios Kollias",
        "Vasileios Kalantzis"
      ],
      "abstract": "Transformer-based large language models (LLMs) rely on contextual embeddings which generate different (continuous) representations for the same token depending on its surrounding context. Nonetheless, words and tokens typically have a limited number of senses (or meanings). We propose multi-sense embeddings as a drop-in replacement for each token in order to capture the range of their uses in a language. To construct a sense embedding dictionary, we apply a clustering algorithm to embeddings generated by an LLM and consider the cluster centers as representative sense embeddings. In addition, we propose a novel knowledge distillation method that leverages the sense dictionary to learn a smaller student model that mimics the senses from the much larger base LLM model, offering significant space and inference time savings, while maintaining competitive performance. Via thorough experiments on various benchmarks, we showcase the effectiveness of our sense embeddings and knowledge distillation approach. We share our code at https://github.com/Qitong-Wang/SenseDict",
      "arxiv_url": "https://arxiv.org/abs/2504.06036",
      "pdf_url": "https://arxiv.org/pdf/2504.06036",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07818",
      "title": "WebUIBench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in WebUI-to-Code",
      "authors": [
        "Zhiyu Lin",
        "Zhengda Zhou",
        "Zhiyuan Zhao",
        "Tianrui Wan",
        "Yilun Ma",
        "Junyu Gao",
        "Xuelong Li"
      ],
      "abstract": "With the rapid advancement of Generative AI technology, Multimodal Large Language Models(MLLMs) have the potential to act as AI software engineers capable of executing complex web application development. Considering that the model requires a confluence of multidimensional sub-capabilities to address the challenges of various development phases, constructing a multi-view evaluation framework is crucial for accurately guiding the enhancement of development efficiency. However, existing benchmarks usually fail to provide an assessment of sub-capabilities and focus solely on webpage generation outcomes. In this work, we draw inspiration from the principles of software engineering and further propose WebUIBench, a benchmark systematically designed to evaluate MLLMs in four key areas: WebUI Perception, HTML Programming,WebUI-HTML Understanding, and WebUI-to-Code. WebUIBench comprises 21K high-quality question-answer pairs derived from over 0.7K real-world websites. The extensive evaluation of 29 mainstream MLLMs uncovers the skill characteristics and various weakness that models encountered during the development process.",
      "arxiv_url": "https://arxiv.org/abs/2506.07818",
      "pdf_url": "https://arxiv.org/pdf/2506.07818",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02354",
      "title": "RATE-Nav: Region-Aware Termination Enhancement for Zero-shot Object Navigation with Vision-Language Models",
      "authors": [
        "Junjie Li",
        "Nan Zhang",
        "Xiaoyang Qu",
        "Kai Lu",
        "Guokuan Li",
        "Jiguang Wan",
        "Jianzong Wang"
      ],
      "abstract": "Object Navigation (ObjectNav) is a fundamental task in embodied artificial intelligence. Although significant progress has been made in semantic map construction and target direction prediction in current research, redundant exploration and exploration failures remain inevitable. A critical but underexplored direction is the timely termination of exploration to overcome these challenges. We observe a diminishing marginal effect between exploration steps and exploration rates and analyze the cost-benefit relationship of exploration. Inspired by this, we propose RATE-Nav, a Region-Aware Termination-Enhanced method. It includes a geometric predictive region segmentation algorithm and region-Based exploration estimation algorithm for exploration rate calculation. By leveraging the visual question answering capabilities of visual language models (VLMs) and exploration rates enables efficient termination.RATE-Nav achieves a success rate of 67.8% and an SPL of 31.3% on the HM3D dataset. And on the more challenging MP3D dataset, RATE-Nav shows approximately 10% improvement over previous zero-shot methods.",
      "arxiv_url": "https://arxiv.org/abs/2506.02354",
      "pdf_url": "https://arxiv.org/pdf/2506.02354",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01253",
      "title": "CoRE: Condition-based Reasoning for Identifying Outcome Variance in Complex Events",
      "authors": [
        "Sai Vallurupalli",
        "Francis Ferraro"
      ],
      "abstract": "Knowing which latent conditions lead to a particular outcome is useful for critically examining claims made about complex event outcomes. Identifying implied conditions and examining their influence on an outcome is challenging. We handle this by combining and augmenting annotations from two existing datasets consisting of goals and states, and explore the influence of conditions through our research questions and Condition-based Reasoning tasks. We examine open and closed LLMs of varying sizes and intent-alignment on our reasoning tasks and find that conditions are useful when not all context is available. Models differ widely in their ability to generate and identify outcome-variant conditions which affects their performance on outcome validation when conditions are used to replace missing context. Larger models like GPT-4o, are more cautious in such less constrained situations.",
      "arxiv_url": "https://arxiv.org/abs/2506.01253",
      "pdf_url": "https://arxiv.org/pdf/2506.01253",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.05614",
      "title": "Two Intermediate Translations Are Better Than One: Fine-tuning LLMs for Document-level Translation Refinement",
      "authors": [
        "Yichen Dong",
        "Xinglin Lyu",
        "Junhui Li",
        "Daimeng Wei",
        "Min Zhang",
        "Shimin Tao",
        "Hao Yang"
      ],
      "abstract": "Recent research has shown that large language models (LLMs) can enhance translation quality through self-refinement. In this paper, we build on this idea by extending the refinement from sentence-level to document-level translation, specifically focusing on document-to-document (Doc2Doc) translation refinement. Since sentence-to-sentence (Sent2Sent) and Doc2Doc translation address different aspects of the translation process, we propose fine-tuning LLMs for translation refinement using two intermediate translations, combining the strengths of both Sent2Sent and Doc2Doc. Additionally, recognizing that the quality of intermediate translations varies, we introduce an enhanced fine-tuning method with quality awareness that assigns lower weights to easier translations and higher weights to more difficult ones, enabling the model to focus on challenging translation cases. Experimental results across ten translation tasks with LLaMA-3-8B-Instruct and Mistral-Nemo-Instruct demonstrate the effectiveness of our approach.",
      "arxiv_url": "https://arxiv.org/abs/2504.05614",
      "pdf_url": "https://arxiv.org/pdf/2504.05614",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.11807",
      "title": "Are Multimodal Large Language Models Pragmatically Competent Listeners in Simple Reference Resolution Tasks?",
      "authors": [
        "Simeon Junker",
        "Manar Ali",
        "Larissa Koch",
        "Sina Zarrieß",
        "Hendrik Buschmeier"
      ],
      "abstract": "We investigate the linguistic abilities of multimodal large language models in reference resolution tasks featuring simple yet abstract visual stimuli, such as color patches and color grids. Although the task may not seem challenging for today's language models, being straightforward for human dyads, we consider it to be a highly relevant probe of the pragmatic capabilities of MLLMs. Our results and analyses indeed suggest that basic pragmatic capabilities, such as context-dependent interpretation of color descriptions, still constitute major challenges for state-of-the-art MLLMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.11807",
      "pdf_url": "https://arxiv.org/pdf/2506.11807",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14276",
      "title": "STeCa: Step-level Trajectory Calibration for LLM Agent Learning",
      "authors": [
        "Hanlin Wang",
        "Jian Wang",
        "Chak Tou Leong",
        "Wenjie Li"
      ],
      "abstract": "Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations or preference learning through exploratory trajectory sampling. However, these methods often struggle to address long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. We finally leverage these calibrated trajectories with successful trajectories for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that timely calibration enables agents to complete tasks with greater robustness. Our code and data are available at https://github.com/WangHanLinHenry/STeCa.",
      "arxiv_url": "https://arxiv.org/abs/2502.14276",
      "pdf_url": "https://arxiv.org/pdf/2502.14276",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "abe241b09369ad1b2ade6a11c82a0cbacec240c9",
      "title": "Towards Medical Complex Reasoning with LLMs through Medical Verifiable Problems",
      "authors": [
        "Junying Chen",
        "Zhenyang Cai",
        "Ke Ji",
        "Xidong Wang",
        "Wanlong Liu",
        "Rongsheng Wang",
        "Benyou Wang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/abe241b09369ad1b2ade6a11c82a0cbacec240c9",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22172",
      "title": "Reverse Preference Optimization for Complex Instruction Following",
      "authors": [
        "Xiang Huang",
        "Ting-En Lin",
        "Feiteng Fang",
        "Yuchuan Wu",
        "Hangyu Li",
        "Yuzhong Qu",
        "Fei Huang",
        "Yongbin Li"
      ],
      "abstract": "Instruction following (IF) is a critical capability for large language models (LLMs). However, handling complex instructions with multiple constraints remains challenging. Previous methods typically select preference pairs based on the number of constraints they satisfy, introducing noise where chosen examples may fail to follow some constraints and rejected examples may excel in certain respects over the chosen ones. To address the challenge of aligning with multiple preferences, we propose a simple yet effective method called Reverse Preference Optimization (RPO). It mitigates noise in preference pairs by dynamically reversing the constraints within the instruction to ensure the chosen response is perfect, alleviating the burden of extensive sampling and filtering to collect perfect responses. Besides, reversal also enlarges the gap between chosen and rejected responses, thereby clarifying the optimization direction and making it more robust to noise. We evaluate RPO on two multi-turn IF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over the DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively. Moreover, RPO scales effectively across model sizes (8B to 70B parameters), with the 70B RPO model surpassing GPT-4o.",
      "arxiv_url": "https://arxiv.org/abs/2505.22172",
      "pdf_url": "https://arxiv.org/pdf/2505.22172",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.06594",
      "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for Machine Translation",
      "authors": [
        "Yingfeng Luo",
        "Tong Zheng",
        "Yongyu Mu",
        "Bei Li",
        "Qinghong Zhang",
        "Yongqi Gao",
        "Ziqiang Xu",
        "Peinan Feng",
        "Xiaoqian Liu",
        "Tong Xiao",
        "Jingbo Zhu"
      ],
      "abstract": "The field of neural machine translation (NMT) has changed with the advent of large language models (LLMs). Much of the recent emphasis in natural language processing (NLP) has been on modeling machine translation and many other problems using a single pre-trained Transformer decoder, while encoder-decoder architectures, which were the standard in earlier NMT models, have received relatively less attention. In this paper, we explore translation models that are universal, efficient, and easy to optimize, by marrying the world of LLMs with the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder unchanged. We also develop methods for adapting LLMs to work better with the NMT decoder. Furthermore, we construct a new dataset involving multiple tasks to assess how well the machine translation system generalizes across various tasks. Evaluations on the WMT and our datasets show that results using our method match or surpass a range of baselines in terms of translation quality, but achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in the memory footprint of the KV cache. It also demonstrates strong generalization across a variety of translation-related tasks.",
      "arxiv_url": "https://arxiv.org/abs/2503.06594",
      "pdf_url": "https://arxiv.org/pdf/2503.06594",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15197",
      "title": "TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding",
      "authors": [
        "Zhaoxuan Wu",
        "Zijian Zhou",
        "Arun Verma",
        "Alok Prakash",
        "Daniela Rus",
        "B. Low"
      ],
      "abstract": "We propose TETRIS, a novel method that optimizes the total throughput of batch speculative decoding in multi-request settings. Unlike existing methods that optimize for a single request or a group of requests as a whole, TETRIS actively selects the most promising draft tokens (for every request in a batch) to be accepted when verified in parallel, resulting in fewer rejected tokens and hence less wasted computing resources. Such an effective resource utilization to achieve fast inference in large language models (LLMs) is especially important to service providers with limited inference capacity. Compared to baseline speculative decoding, TETRIS yields a consistently higher acceptance rate and more effective utilization of the limited inference capacity. We show theoretically and empirically that TETRIS outperforms baseline speculative decoding and existing methods that dynamically select draft tokens, leading to a more efficient batch inference in LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2502.15197",
      "pdf_url": "https://arxiv.org/pdf/2502.15197",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ac4e71d92a9e7d10dd7e348bf82ccbacda653956",
      "title": "InImageTrans: Multimodal LLM-based Text Image Machine Translation",
      "authors": [
        "Fei Zuo",
        "Kehai Chen",
        "Yu Zhang",
        "Zhengshan Xue",
        "Min Zhang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/ac4e71d92a9e7d10dd7e348bf82ccbacda653956",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11754",
      "title": "Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation",
      "authors": [
        "Wenyu Huang",
        "P. Vougiouklis",
        "Mirella Lapata",
        "Jeff Z. Pan"
      ],
      "abstract": "Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant information but also employing multi-hop reasoning across the information sources. Although LMs perform well on traditional question-answering tasks, the causal mask can hinder their capacity to reason across complex contexts. In this paper, we explore how LMs respond to multi-hop questions by permuting search results (retrieved documents) under various configurations. Our study reveals interesting findings as follows: 1) Encoder-decoder models, such as the ones in the Flan-T5 family, generally outperform causal decoder-only LMs in MHQA tasks, despite being significantly smaller in size; 2) altering the order of gold documents reveals distinct trends in both Flan T5 models and fine-tuned decoder-only models, with optimal performance observed when the document order aligns with the reasoning chain order; 3) enhancing causal decoder-only models with bi-directional attention by modifying the causal mask can effectively boost their end performance. In addition to the above, we conduct a thorough investigation of the distribution of LM attention weights in the context of MHQA. Our experiments reveal that attention weights tend to peak at higher values when the resulting answer is correct. We leverage this finding to heuristically improve LMs' performance on this task. Our code is publicly available at https://github.com/hwy9855/MultiHopQA-Reasoning.",
      "arxiv_url": "https://arxiv.org/abs/2505.11754",
      "pdf_url": "https://arxiv.org/pdf/2505.11754",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.08208",
      "title": "ASTRID - An Automated and Scalable TRIaD for the Evaluation of RAG-based Clinical Question Answering Systems",
      "authors": [
        "Mohita Chowdhury",
        "Y. He",
        "A. Higham",
        "Ernest Lim"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive potential in clinical question answering (QA), with Retrieval Augmented Generation (RAG) emerging as a leading approach for ensuring the factual accuracy of model responses. However, current automated RAG metrics perform poorly in clinical and conversational use cases. Using clinical human evaluations of responses is expensive, unscalable, and not conducive to the continuous iterative development of RAG systems. To address these challenges, we introduce ASTRID - an Automated and Scalable TRIaD for evaluating clinical QA systems leveraging RAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy (RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is designed to better capture the faithfulness of a model's response to the knowledge base without penalising conversational elements. To validate our triad, we curate a dataset of over 200 real-world patient questions posed to an LLM-based QA agent during surgical follow-up for cataract surgery - the highest volume operation in the world - augmented with clinician-selected questions for emergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate that CF can predict human ratings of faithfulness better than existing definitions for conversational use cases. Furthermore, we show that evaluation using our triad consisting of CF, RA, and CR exhibits alignment with clinician assessment for inappropriate, harmful, or unhelpful responses. Finally, using nine different LLMs, we demonstrate that the three metrics can closely agree with human evaluations, highlighting the potential of these metrics for use in LLM-driven automated evaluation pipelines. We also publish the prompts and datasets for these experiments, providing valuable resources for further research and development.",
      "arxiv_url": "https://arxiv.org/abs/2501.08208",
      "pdf_url": "https://arxiv.org/pdf/2501.08208",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-01-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24147",
      "title": "Rationales Are Not Silver Bullets: Measuring the Impact of Rationales on Model Performance and Reliability",
      "authors": [
        "Chiwei Zhu",
        "Benfeng Xu",
        "An Yang",
        "Junyang Lin",
        "Quan Wang",
        "Chang Zhou",
        "Zhendong Mao"
      ],
      "abstract": "Training language models with rationales augmentation has been shown to be beneficial in many existing works. In this paper, we identify that such a prevailing view does not hold consistently. We conduct comprehensive investigations to thoroughly inspect the impact of rationales on model performance as well as a novel perspective of model reliability. The results lead to several key findings that add new insights upon existing understandings: 1) Rationales can, at times, deteriorate model performance; 2) Rationales can, at times, improve model reliability, even outperforming their untrained counterparts; 3) A linear correspondence exists in between the performance and reliability improvements, while both are driven by the intrinsic difficulty of the task. These findings provide informative regulations on the broad utilization of rationales and raise critical implications on the procedure of explicitly aligning language models with implicit human thoughts. Codes can be found at https://github.com/Ignoramus0817/rationales.",
      "arxiv_url": "https://arxiv.org/abs/2505.24147",
      "pdf_url": "https://arxiv.org/pdf/2505.24147",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12265",
      "title": "Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation",
      "authors": [
        "Chengwei Qin",
        "Wenxuan Zhou",
        "Karthik Abinav Sankararaman",
        "Nanshu Wang",
        "Tengyu Xu",
        "Alexander Radovic",
        "Eryk Helenowski",
        "Arya Talebzadeh",
        "Aditya Tayade",
        "Si-Yuan Wang",
        "Shafiq R. Joty",
        "Han Fang",
        "Hao Ma"
      ],
      "abstract": "Hallucination, the generation of factually incorrect information, remains a significant challenge for large language models (LLMs), especially in open-domain long-form generation. Existing approaches for detecting hallucination in long-form tasks either focus on limited domains or rely heavily on external fact-checking tools, which may not always be available. In this work, we systematically investigate reference-free hallucination detection in open-domain long-form responses. Our findings reveal that internal states (e.g., model's output probability and entropy) alone are insufficient for reliably (i.e., better than random guessing) distinguishing between factual and hallucinated content. To enhance detection, we explore various existing approaches, including prompting-based methods, probing, and fine-tuning, with fine-tuning proving the most effective. To further improve the accuracy, we introduce a new paradigm, named RATE-FT, that augments fine-tuning with an auxiliary task for the model to jointly learn with the main task of hallucination detection. With extensive experiments and analysis using a variety of model families&datasets, we demonstrate the effectiveness and generalizability of our method, e.g., +3% over general fine-tuning methods on LongFact.",
      "arxiv_url": "https://arxiv.org/abs/2505.12265",
      "pdf_url": "https://arxiv.org/pdf/2505.12265",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.01915",
      "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models",
      "authors": [
        "Chengao Li",
        "Hanyu Zhang",
        "Yunkun Xu",
        "Hongyan Xue",
        "Xiang Ao",
        "Qing He"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful technique for aligning large language models (LLMs) with human preferences. However, effectively aligning LLMs with diverse human preferences remains a significant challenge, particularly when they are conflict. To address this issue, we frame human value alignment as a multi-objective optimization problem, aiming to maximize a set of potentially conflicting objectives. We introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning paradigm that employs multiple-gradient descent to align LLMs with diverse preference distributions. GAPO adaptively rescales the gradients for each objective to determine an update direction that optimally balances the trade-offs between objectives. Additionally, we introduce P-GAPO, which incorporates user preferences across different objectives and achieves Pareto solutions that better align with the user's specific needs. Our theoretical analysis demonstrates that GAPO converges towards a Pareto optimal solution for multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms current state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness.",
      "arxiv_url": "https://arxiv.org/abs/2507.01915",
      "pdf_url": "https://arxiv.org/pdf/2507.01915",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.17692",
      "title": "Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering",
      "authors": [
        "Binquan Ji",
        "Haibo Luo",
        "Yifei Lu",
        "Lei Hei",
        "Jiaqi Wang",
        "Tingjing Liao",
        "Lingyu Wang",
        "Shichao Wang",
        "Feiliang Ren"
      ],
      "abstract": "Knowledge-intensive multi-hop question answering (QA) tasks, which require integrating evidence from multiple sources to address complex queries, often necessitate multiple rounds of retrieval and iterative generation by large language models (LLMs). However, incorporating many documents and extended contexts poses challenges -such as hallucinations and semantic drift-for lightweight LLMs with fewer parameters. This work proposes a novel framework called DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions into logically coherent subquestions to form a hallucination-free reasoning chain. It then iteratively refines these subquestions through context-aware rewriting to generate effective query formulations. For retrieval, we introduce a lightweight discriminative keyword extraction module that leverages extracted keywords to achieve targeted, precise document recall with relatively low computational overhead. Extensive experiments on three multi-hop QA datasets demonstrate that DEC performs on par with or surpasses state-of-the-art benchmarks while significantly reducing token consumption. Notably, our approach attains state-of-the-art results on models with 8B parameters, showcasing its effectiveness in various scenarios, particularly in resource-constrained environments.",
      "arxiv_url": "https://arxiv.org/abs/2506.17692",
      "pdf_url": "https://arxiv.org/pdf/2506.17692",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "accbbbce115b2e9b701d985e23b540028e5194db",
      "title": "Towards Better Value Principles for Large Language Model Alignment: A Systematic Evaluation and Enhancement",
      "authors": [
        "Bingbing Xu",
        "Jing Yao",
        "Xiaoyuan Yi",
        "Aishan Maoliniyazi",
        "Xing Xie",
        "Xiaofeng Meng"
      ],
      "abstract": "As Large Language Models (LLMs) advance, aligning them with human values is critical for their responsible development. Value principles serve as the foundation for clarifying alignment goals. Multiple sets of value principles have been proposed, such as HHH (helpful, honest, harmless) and instructions for data synthesis in reinforcement learning from AI feedback (RLAIF). However, most of them are heuristically crafted, without consideration of three primary challenges in practical LLM alignment: 1) Comprehensiveness to deal with diverse and even unforeseen scenarios in which LLMs could be applied; 2) Precision to provide LLMs with clear and actionable guidance in specific scenarios; and 3) Compatability to avoid internal contracts between principles. In this paper, we formalize quantitative metrics to evaluate value principles along the three desirable properties. Building on these metrics, we propose the Hi erarchical Va lue P rinciple framework ( HiVaP ) 1 , which constructs a hierarchical principle set and retrieves principles tailored to each scenario in a cascading way, addressing above challenges. Experimental re-sults validate that the three metrics capture the effectiveness of value principles for LLM alignment, and our HiVaP framework that enhances these metrics leads to superior alignment. Warning: This paper contains several toxic and offensive statements.",
      "arxiv_url": "https://www.semanticscholar.org/paper/accbbbce115b2e9b701d985e23b540028e5194db",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12340",
      "title": "Understanding Silent Data Corruption in LLM Training",
      "authors": [
        "Jeffrey Ma",
        "Hengzhi Pei",
        "Leonard Lausen",
        "G. Karypis"
      ],
      "abstract": "As the scale of training large language models (LLMs) increases, one emergent failure is silent data corruption (SDC), where hardware produces incorrect computations without explicit failure signals. In this work, we are the first to investigate the impact of real-world SDCs on LLM training by comparing model training between healthy production nodes and unhealthy nodes exhibiting SDCs. With the help from a cloud computing platform, we access the unhealthy nodes that were swept out from production by automated fleet management. Using deterministic execution via XLA compiler and our proposed synchronization mechanisms, we isolate and analyze the impact of SDC errors on these nodes at three levels: at each submodule computation, at a single optimizer step, and at a training period. Our results reveal that the impact of SDCs on computation varies on different unhealthy nodes. Although in most cases the perturbations from SDCs on submodule computation and gradients are relatively small, SDCs can lead models to converge to different optima with different weights and even cause spikes in the training loss. Our analysis sheds light on further understanding and mitigating the impact of SDCs.",
      "arxiv_url": "https://arxiv.org/abs/2502.12340",
      "pdf_url": "https://arxiv.org/pdf/2502.12340",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22552",
      "title": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM",
      "authors": [
        "Hoang Pham",
        "Thanh-Do Nguyen",
        "Khac-Hoai Nam Bui"
      ],
      "abstract": "Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of large language models (LLMs) is an emerging research challenge in claim verification. While KGs provide structured, semantically rich representations well-suited for reasoning, most existing verification methods rely on unstructured text corpora, limiting their ability to effectively leverage KGs. Additionally, despite possessing strong reasoning abilities, modern LLMs struggle with multi-step modular pipelines and reasoning over KGs without adaptation. To address these challenges, we propose ClaimPKG, an end-to-end framework that seamlessly integrates LLM reasoning with structured knowledge from KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight, specialized LLM to represent the input claim as pseudo-subgraphs, guiding a dedicated subgraph retrieval module to identify relevant KG subgraphs. These retrieved subgraphs are then processed by a general-purpose LLM to produce the final verdict and justification. Extensive experiments on the FactKG dataset demonstrate that ClaimPKG achieves state-of-the-art performance, outperforming strong baselines in this research field by 9%-12% accuracy points across multiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability to unstructured datasets such as HoVer and FEVEROUS, effectively combining structured knowledge from KGs with LLM reasoning across various LLM backbones.",
      "arxiv_url": "https://arxiv.org/abs/2505.22552",
      "pdf_url": "https://arxiv.org/pdf/2505.22552",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.07400",
      "title": "Talking Point based Ideological Discourse Analysis in News Events",
      "authors": [
        "Nishanth Nakshatri",
        "Nikhil Mehta",
        "Siyi Liu",
        "Sihao Chen",
        "Daniel J. Hopkins",
        "Dan Roth",
        "Dan Goldwasser"
      ],
      "abstract": "Analyzing ideological discourse even in the age of LLMs remains a challenge, as these models often struggle to capture the key elements that shape real-world narratives. Specifically, LLMs fail to focus on characteristic elements driving dominant discourses and lack the ability to integrate contextual information required for understanding abstract ideological views. To address these limitations, we propose a framework motivated by the theory of ideological discourse analysis to analyze news articles related to real-world events. Our framework represents the news articles using a relational structure - talking points, which captures the interaction between entities, their roles, and media frames along with a topic of discussion. It then constructs a vocabulary of repeating themes - prominent talking points, that are used to generate ideology-specific viewpoints (or partisan perspectives). We evaluate our framework's ability to generate these perspectives through automated tasks - ideology and partisan classification tasks, supplemented by human validation. Additionally, we demonstrate straightforward applicability of our framework in creating event snapshots, a visual way of interpreting event discourse. We release resulting dataset and model to the community to support further research.",
      "arxiv_url": "https://arxiv.org/abs/2504.07400",
      "pdf_url": "https://arxiv.org/pdf/2504.07400",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.03271",
      "title": "DPO Kernels: A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich Paradigm for Direct Preference Optimization",
      "authors": [
        "Amitava Das",
        "Suranjana Trivedy",
        "Danush Khanna",
        "Rajarshi Roy",
        "Gurpreet Singh",
        "Basab Ghosh",
        "Yaswanth Narsupalli",
        "Vinija Jain",
        "Vasu Sharma",
        "Aishwarya N. Reganti",
        "Aman Chadha"
      ],
      "abstract": "The rapid rise of large language models (LLMs) has unlocked many applications but also underscores the challenge of aligning them with diverse values and preferences. Direct Preference Optimization (DPO) is central to alignment but constrained by fixed divergences and limited feature transformations. We propose DPO-Kernels, which integrates kernel methods to address these issues through four key contributions: (i) Kernelized Representations with polynomial, RBF, Mahalanobis, and spectral kernels for richer transformations, plus a hybrid loss combining embedding-based and probability-based objectives; (ii) Divergence Alternatives (Jensen-Shannon, Hellinger, Renyi, Bhattacharyya, Wasserstein, and f-divergences) for greater stability; (iii) Data-Driven Selection metrics that automatically choose the best kernel-divergence pair; and (iv) a Hierarchical Mixture of Kernels for both local precision and global modeling. Evaluations on 12 datasets demonstrate state-of-the-art performance in factuality, safety, reasoning, and instruction following. Grounded in Heavy-Tailed Self-Regularization, DPO-Kernels maintains robust generalization for LLMs, offering a comprehensive resource for further alignment research.",
      "arxiv_url": "https://arxiv.org/abs/2501.03271",
      "pdf_url": "https://arxiv.org/pdf/2501.03271",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12215",
      "title": "Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?",
      "authors": [
        "Zhiyuan Zeng",
        "Qinyuan Cheng",
        "Zhangyue Yin",
        "Yunhua Zhou",
        "Xipeng Qiu"
      ],
      "abstract": "The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI's o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models truly possess test-time scaling capabilities remains underexplored. This study found that longer CoTs of these o1-like models do not consistently enhance accuracy; in fact, correct solutions are often shorter than incorrect ones for the same questions. Further investigation shows this phenomenon is closely related to models' self-revision capabilities - longer CoTs contain more self-revisions, which often lead to performance degradation. We then compare sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that parallel scaling achieves better coverage and scalability. Based on these insights, we propose Shortest Majority Vote, a method that combines parallel scaling strategies with CoT length characteristics, significantly improving models' test-time scalability compared to conventional majority voting approaches.",
      "arxiv_url": "https://arxiv.org/abs/2502.12215",
      "pdf_url": "https://arxiv.org/pdf/2502.12215",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22582",
      "title": "Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts",
      "authors": [
        "Xue Zhang",
        "Yunlong Liang",
        "Fandong Meng",
        "Songming Zhang",
        "Yufeng Chen",
        "Jinan Xu",
        "Jie Zhou"
      ],
      "abstract": "Continually expanding new languages for existing large language models (LLMs) is a promising yet challenging approach to building powerful multilingual LLMs. The biggest challenge is to make the model continuously learn new languages while preserving the proficient ability of old languages. To achieve this, recent work utilizes the Mixture-of-Experts (MoE) architecture to expand new languages by adding new experts and avoid catastrophic forgetting of old languages by routing corresponding tokens to the original model backbone (old experts). Although intuitive, this kind of method is parameter-costly when expanding new languages and still inevitably impacts the performance of old languages. To address these limitations, we analyze the language characteristics of different layers in LLMs and propose a layer-wise expert allocation algorithm (LayerMoE) to determine the appropriate number of new experts for each layer. Specifically, we find different layers in LLMs exhibit different representation similarities between languages and then utilize the similarity as the indicator to allocate experts for each layer, i.e., the higher similarity, the fewer experts. Additionally, to further mitigate the forgetting of old languages, we add a classifier in front of the router network on the layers with higher similarity to guide the routing of old language tokens. Experimental results show that our method outperforms the previous state-of-the-art baseline with 60% fewer experts in the single-expansion setting and with 33.3% fewer experts in the lifelong-expansion setting, demonstrating the effectiveness of our method.",
      "arxiv_url": "https://arxiv.org/abs/2505.22582",
      "pdf_url": "https://arxiv.org/pdf/2505.22582",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.08371",
      "title": "Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding",
      "authors": [
        "Zikai Xiao",
        "Ziyang Wang",
        "Wen Ma",
        "Yan Zhang",
        "Wei-lei Shen",
        "Yan Wang",
        "Luqi Gong",
        "Zuozhu Liu"
      ],
      "abstract": "While Large Language Models (LLMs) support long contexts, they struggle with performance degradation within the context window. Current solutions incur prohibitive training costs, leaving statistical behaviors and cost-effective approaches underexplored. From the decoding perspective, we identify the Posterior Salience Attenuation (PSA) phenomenon, where the salience ratio correlates with long-text performance degradation. Notably, despite the attenuation, gold tokens still occupy high-ranking positions in the decoding space. Motivated by it, we propose the training-free Positional Contrastive Decoding (PCD) that contrasts the logits derived from long-aware attention with those from designed local-aware attention, enabling the model to focus on the gains introduced by large-scale short-to-long training. Through the analysis of long-term decay simulation, we demonstrate that PCD effectively alleviates attention score degradation. Experimental results show that PCD achieves state-of-the-art performance on long-context benchmarks.",
      "arxiv_url": "https://arxiv.org/abs/2506.08371",
      "pdf_url": "https://arxiv.org/pdf/2506.08371",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-06-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00507",
      "title": "Exploring In-context Example Generation for Machine Translation",
      "authors": [
        "Dohyun Lee",
        "Seungil Chad Lee",
        "Chanwoo Yang",
        "Yujin Baek",
        "Jaegul Choo"
      ],
      "abstract": "Large language models (LLMs) have demonstrated strong performance across various tasks, leveraging their exceptional in-context learning ability with only a few examples. Accordingly, the selection of optimal in-context examples has been actively studied in the field of machine translation. However, these studies presuppose the presence of a demonstration pool with human-annotated pairs, making them less applicable to low-resource languages where such an assumption is challenging to meet. To overcome this limitation, this paper explores the research direction of in-context example generation for machine translation. Specifically, we propose Demonstration Augmentation for Translation (DAT), a simple yet effective approach that generates example pairs without relying on any external resources. This method builds upon two prior criteria, relevance and diversity, which have been highlighted in previous work as key factors for in-context example selection. Through experiments and analysis on low-resource languages where human-annotated pairs are scarce, we show that DAT achieves superior translation quality compared to the baselines. Furthermore, we investigate the potential of progressively accumulating generated pairs during test time to build and reuse a demonstration pool. Our implementation is publicly available at https://github.com/aiclaudev/DAT.",
      "arxiv_url": "https://arxiv.org/abs/2506.00507",
      "pdf_url": "https://arxiv.org/pdf/2506.00507",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07533",
      "title": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts",
      "authors": [
        "Wei Tao",
        "Haocheng Lu",
        "Xiaoyang Qu",
        "Bin Zhang",
        "Kai Lu",
        "Jiguang Wan",
        "Jianzong Wang"
      ],
      "abstract": "One of the primary challenges in optimizing large language models (LLMs) for long-context inference lies in the high memory consumption of the Key-Value (KV) cache. Existing approaches, such as quantization, have demonstrated promising results in reducing memory usage. However, current quantization methods cannot take both effectiveness and efficiency into account. In this paper, we propose MoQAE, a novel mixed-precision quantization method via mixture of quantization-aware experts. First, we view different quantization bit-width configurations as experts and use the traditional mixture of experts (MoE) method to select the optimal configuration. To avoid the inefficiency caused by inputting tokens one by one into the router in the traditional MoE method, we input the tokens into the router chunk by chunk. Second, we design a lightweight router-only fine-tuning process to train MoQAE with a comprehensive loss to learn the trade-off between model accuracy and memory usage. Finally, we introduce a routing freezing (RF) and a routing sharing (RS) mechanism to further reduce the inference overhead. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms state-of-the-art KV cache quantization approaches in both efficiency and effectiveness.",
      "arxiv_url": "https://arxiv.org/abs/2506.07533",
      "pdf_url": "https://arxiv.org/pdf/2506.07533",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.06664",
      "title": "SEE: Continual Fine-tuning with Sequential Ensemble of Experts",
      "authors": [
        "Zhilin Wang",
        "Yafu Li",
        "Xiaoye Qu",
        "Yu Cheng"
      ],
      "abstract": "Continual fine-tuning of large language models (LLMs) suffers from catastrophic forgetting. Rehearsal-based methods mitigate this problem by retaining a small set of old data. Nevertheless, they still suffer inevitable performance loss. Although training separate experts for each task can help prevent forgetting, effectively assembling them remains a challenge. Some approaches use routers to assign tasks to experts, but in continual learning, they often require retraining for optimal performance. To address these challenges, we introduce the Sequential Ensemble of Experts (SEE) framework. SEE removes the need for an additional router, allowing each expert to independently decide whether a query should be handled. The framework employs distributed routing, and during continual fine-tuning, SEE only requires the training of new experts for incoming tasks rather than retraining the entire system. Experiments reveal that the SEE outperforms prior approaches, including multi-task learning, in continual fine-tuning. It also demonstrates remarkable generalization ability, as the expert can effectively identify out-of-distribution queries, which can then be directed to a more generalized model for resolution. This work highlights the promising potential of integrating routing and response mechanisms within each expert, paving the way for the future of distributed model ensembling.",
      "arxiv_url": "https://arxiv.org/abs/2504.06664",
      "pdf_url": "https://arxiv.org/pdf/2504.06664",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23826",
      "title": "FinRipple: Aligning Large Language Models with Financial Market for Event Ripple Effect Awareness",
      "authors": [
        "Yuanjian Xu",
        "Jianing Hao",
        "Kunsheng Tang",
        "Jingnan Chen",
        "Anxian Liu",
        "Peng Liu",
        "Guang Zhang"
      ],
      "abstract": "Financial markets exhibit complex dynamics where localized events trigger ripple effects across entities. Previous event studies, constrained by static single-company analyses and simplistic assumptions, fail to capture these ripple effects. While large language models (LLMs) offer emergent reasoning capabilities, their direct application falters due to structural market unawareness and limited capacity to analyze ripple effects. We propose FinRipple, an elegant framework that empowers LLMs with the ability to analyze ripple effects through financial theory-guided large-scale reinforcement learning. We begin by relaxing the assumptions of previous methods, incorporating a time-varying knowledge graph to accurately represent market structure. By seamlessly integrating classical asset pricing theory, we align the LLM with the market, enabling it to predict ripple effects. To the best of our knowledge, we are the first to provide a standardized definition of ripple effect prediction, a task that is extremely important yet unexplored in the financial domain. Extensive experiments demonstrate that FinRipple provides a promising solution to this task.",
      "arxiv_url": "https://arxiv.org/abs/2505.23826",
      "pdf_url": "https://arxiv.org/pdf/2505.23826",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00032",
      "title": "KatFishNet: Detecting LLM-Generated Korean Text through Linguistic Feature Analysis",
      "authors": [
        "Shinwoo Park",
        "Shubin Kim",
        "Do-Kyung Kim",
        "Yo-Sub Han"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) increases the difficulty of distinguishing between human-written and LLM-generated text. Detecting LLM-generated text is crucial for upholding academic integrity, preventing plagiarism, protecting copyrights, and ensuring ethical research practices. Most prior studies on detecting LLM-generated text focus primarily on English text. However, languages with distinct morphological and syntactic characteristics require specialized detection approaches. Their unique structures and usage patterns can hinder the direct application of methods primarily designed for English. Among such languages, we focus on Korean, which has relatively flexible spacing rules, a rich morphological system, and less frequent comma usage compared to English. We introduce KatFish, the first benchmark dataset for detecting LLM-generated Korean text. The dataset consists of text written by humans and generated by four LLMs across three genres. By examining spacing patterns, part-of-speech diversity, and comma usage, we illuminate the linguistic differences between human-written and LLM-generated Korean text. Building on these observations, we propose KatFishNet, a detection method specifically designed for the Korean language. KatFishNet achieves an average of 19.78% higher AUROC compared to the best-performing existing detection method. Our code and data are available at https://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis.",
      "arxiv_url": "https://arxiv.org/abs/2503.00032",
      "pdf_url": "https://arxiv.org/pdf/2503.00032",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2508.05534",
      "title": "CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text Generation",
      "authors": [
        "Santosh T.Y.S.S",
        "Youssef Tarek Elkhayat",
        "O. Ichim",
        "Pranav Shetty",
        "Dongsheng Wang",
        "Zhiqiang Ma",
        "Armineh Nourbakhsh",
        "Xiaomo Liu"
      ],
      "abstract": "Due to their ability to process long and complex contexts, LLMs can offer key benefits to the Legal domain, but their adoption has been hindered by their tendency to generate unfaithful, ungrounded, or hallucinatory outputs. While Retrieval-Augmented Generation offers a promising solution by grounding generations in external knowledge, it offers no guarantee that the provided context will be effectively integrated. To address this, context-aware decoding strategies have been proposed to amplify the influence of relevant context, but they usually do not explicitly enforce faithfulness to the context. In this work, we introduce Confidence-guided Copy-based Decoding for Legal Text Generation (CoCoLex)-a decoding strategy that dynamically interpolates the model produced vocabulary distribution with a distribution derived based on copying from the context. CoCoLex encourages direct copying based on the model's confidence, ensuring greater fidelity to the source. Experimental results on five legal benchmarks demonstrate that CoCoLex outperforms existing context-aware decoding methods, particularly in long-form generation tasks.",
      "arxiv_url": "https://arxiv.org/abs/2508.05534",
      "pdf_url": "https://arxiv.org/pdf/2508.05534",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-08-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13576",
      "title": "Beyond One-Size-Fits-All: Tailored Benchmarks for Efficient Evaluation",
      "authors": [
        "Peiwen Yuan",
        "Yueqi Zhang",
        "Shaoxiong Feng",
        "Yiwei Li",
        "Xinglin Wang",
        "Jiayi Shi",
        "Chuyi Tan",
        "Boyuan Pan",
        "Yao Hu",
        "Kan Li"
      ],
      "abstract": "Evaluating models on large benchmarks is very resource-intensive, especially during the period of rapid model evolution. Existing efficient evaluation methods estimate the performance of target models by testing them only on a small and static coreset of the benchmark, which is derived from the publicly available evaluation results of source models. These methods rely on the assumption that target models have high prediction consistency with source models. However, we demonstrate that it doesn't generalize well in practice. To alleviate the inconsistency issue, we present TailoredBench, a method that conducts customized evaluation tailored to each target model. Specifically, a Global-coreset is first constructed as a probe to identify the most consistent source models for each target model with an adaptive source model selection strategy. Afterwards, a scalable K-Medoids clustering algorithm is proposed to extend the Global-coreset to a tailored Native-coreset for each target model. According to the predictions on Native-coresets, we obtain the performance of target models on the whole benchmark with a calibrated estimation strategy. Comprehensive experiments on 5 benchmarks across over 300 models demonstrate that compared to best performing baselines, TailoredBench achieves an average reduction of 31.4% in MAE of accuracy estimates under the same inference budgets, showcasing strong effectiveness and generalizability.",
      "arxiv_url": "https://arxiv.org/abs/2502.13576",
      "pdf_url": "https://arxiv.org/pdf/2502.13576",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "aebed4136eddc22e21005ad05d844069b4466528",
      "title": "Modal Dependency Parsing via Biaffine Attention with Self-Loop",
      "authors": [
        "Jayeol Chun",
        "Nianwen Xue"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/aebed4136eddc22e21005ad05d844069b4466528",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "aec44bcc970133acd604dae2e99229990c01bb38",
      "title": "RLKGF: Reinforcement Learning from Knowledge Graph Feedback Without Human Annotations",
      "authors": [
        "Lian Yan",
        "Chen Tang",
        "Yi Guan",
        "Haotian Wang",
        "Songyuan Wang",
        "Haifeng Liu",
        "Yang Yang",
        "Jingchi Jiang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/aec44bcc970133acd604dae2e99229990c01bb38",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "aed9fa27d9bb13608a0577bccbbff3d2f01e632f",
      "title": "AIGuard: A Benchmark and Lightweight Detection for E-commerce AIGC Risks",
      "authors": [
        "Wenhua Zhang",
        "Weicheng Li",
        "Xuanrong Rao",
        "Lixin Zou",
        "Xiangyang Luo",
        "Chubin Zhuang",
        "Yongjie Hong",
        "Zhen Qin",
        "Hengyun Chang",
        "Chenliang Li",
        "Bo Zheng"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/aed9fa27d9bb13608a0577bccbbff3d2f01e632f",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.19036",
      "title": "RedundancyLens: Revealing and Exploiting Visual Token Processing Redundancy for Efficient Decoder-Only MLLMs",
      "authors": [
        "Hongliang Li",
        "Jiaxin Zhang",
        "Wenhui Liao",
        "Dezhi Peng",
        "Kai Ding",
        "Lianwen Jin"
      ],
      "abstract": "Current Multimodal Large Language Model (MLLM) architectures face a critical tradeoff between performance and efficiency: decoder-only architectures achieve higher performance but lower efficiency, while cross-attention-based architectures offer greater efficiency but lower performance. The key distinction lies in how visual tokens are processed. Decoder-only architectures apply self-attention and FFN operations on visual tokens, while cross-attention architectures skip these computations. To investigate whether redundancy exists in this computationally expensive process, we propose a training-free framework for analyzing trained MLLMs. It consists of Probe-Activated Dynamic FFN and Hollow Attention, which enable adjustable reductions in computations for visual tokens, as well as a Layer Ranking Algorithm that prioritizes layers for these reductions. Extensive experiments demonstrate substantial, structured, and clustered redundancy unique to decoder-only MLLMs, offering valuable insights for future MLLM architecture design. Furthermore, by leveraging our reduction framework as a training-free inference acceleration approach, we achieve performance comparable to or better than state-of-the-art methods while remaining compatible with them. Code will be publicly available at https://github.com/L-Hugh/RedundancyLens.",
      "arxiv_url": "https://arxiv.org/abs/2501.19036",
      "pdf_url": "https://arxiv.org/pdf/2501.19036",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04490",
      "title": "Large Language Models in Bioinformatics: A Survey",
      "authors": [
        "Zhenyu Wang",
        "Zikang Wang",
        "Jiyue Jiang",
        "Pengan Chen",
        "Xiangyu Shi",
        "Yu Li"
      ],
      "abstract": "Large Language Models (LLMs) are revolutionizing bioinformatics, enabling advanced analysis of DNA, RNA, proteins, and single-cell data. This survey provides a systematic review of recent advancements, focusing on genomic sequence modeling, RNA structure prediction, protein function inference, and single-cell transcriptomics. Meanwhile, we also discuss several key challenges, including data scarcity, computational complexity, and cross-omics integration, and explore future directions such as multimodal learning, hybrid AI models, and clinical applications. By offering a comprehensive perspective, this paper underscores the transformative potential of LLMs in driving innovations in bioinformatics and precision medicine.",
      "arxiv_url": "https://arxiv.org/abs/2503.04490",
      "pdf_url": "https://arxiv.org/pdf/2503.04490",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "aeff4dd57c6f6a4a7f266806346f7ed0b93df72e",
      "title": "Vision Language Model Helps Private Information De-Identification in Vision Data",
      "authors": [
        "Tiejin Chen",
        "Pingzhi Li",
        "Kaixiong Zhou",
        "Tianlong Chen",
        "Hua Wei"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/aeff4dd57c6f6a4a7f266806346f7ed0b93df72e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.11549",
      "title": "Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas",
      "authors": [
        "Nishant Balepur",
        "Vishakh Padmakumar",
        "Fumeng Yang",
        "Shi Feng",
        "Rachel Rudinger",
        "J. Boyd-Graber"
      ],
      "abstract": "LLMs are aligned to follow input instructions by learning which of two responses users prefer for a prompt. However, such preference data do not convey why users prefer responses that are chosen or rejected, so LLMs trained on these datasets cannot tailor responses to varied user needs. To surface these parameters of personalization, we apply abductive reasoning to preference data, inferring needs and interests of users, i.e., personas, that may prefer either response. We test this idea in two steps: Persona Inference (PI), abductively inferring personas of users who prefer chosen or rejected outputs, and Persona Tailoring (PT), training models to tailor outputs to personas from PI. We show: 1) LLMs infer personas accurately explaining why different users may prefer both chosen or rejected outputs; 2) Training on preference data augmented with PI personas via PT boosts personalization and generalizes to supporting user-written personas; and 3) Rejected response personas form harder personalization evaluations, showing PT better aids users with uncommon preferences versus typical alignment methods. We argue for an abductive view of preferences for personalization, asking not only which response is better but when, why, and for whom.",
      "arxiv_url": "https://arxiv.org/abs/2501.11549",
      "pdf_url": "https://arxiv.org/pdf/2501.11549",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-01-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "af4f5d4ba38d6f1a4a98637862b6a7756771bbef",
      "title": "Tool learning via Inference-time Scaling and Cycle Verifier",
      "authors": [
        "Xiaobo Liang",
        "Wenjing Xie",
        "Juntao Li",
        "Wanfu Wang",
        "Yibin Chen",
        "Kehai Chen",
        "Min Zhang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/af4f5d4ba38d6f1a4a98637862b6a7756771bbef",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01900",
      "title": "LLM-Empowered Class Imbalanced Graph Prompt Learning for Online Drug Trafficking Detection",
      "authors": [
        "Tianyi Ma",
        "Y. Qian",
        "Zehong Wang",
        "Zheyuan Zhang",
        "Chuxu Zhang",
        "Yanfang Ye"
      ],
      "abstract": "As the market for illicit drugs remains extremely profitable, major online platforms have become direct-to-consumer intermediaries for illicit drug trafficking participants. These online activities raise significant social concerns that require immediate actions. Existing approaches to combating this challenge are generally impractical, due to the imbalance of classes and scarcity of labeled samples in real-world applications. To this end, we propose a novel Large Language Model-empowered Heterogeneous Graph Prompt Learning framework for illicit Drug Trafficking detection, called LLM-HetGDT, that leverages LLM to facilitate heterogeneous graph neural networks (HGNNs) to effectively identify drug trafficking activities in the class-imbalanced scenarios. Specifically, we first pre-train HGNN over a contrastive pretext task to capture the inherent node and structure information over the unlabeled drug trafficking heterogeneous graph (HG). Afterward, we employ LLM to augment the HG by generating high-quality synthetic user nodes in minority classes. Then, we fine-tune the soft prompts on the augmented HG to capture the important information in the minority classes for the downstream drug trafficking detection task. To comprehensively study online illicit drug trafficking activities, we collect a new HG dataset over Twitter, called Twitter-HetDrug. Extensive experiments on this dataset demonstrate the effectiveness, efficiency, and applicability of LLM-HetGDT.",
      "arxiv_url": "https://arxiv.org/abs/2503.01900",
      "pdf_url": "https://arxiv.org/pdf/2503.01900",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20014",
      "title": "Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation",
      "authors": [
        "Hoyun Song",
        "Huije Lee",
        "Jisu Shin",
        "Sukmin Cho",
        "Changgeon Ko",
        "Jong C. Park"
      ],
      "abstract": "The detection of mental health problems from social media and the interpretation of these results have been extensively explored. Research has shown that incorporating clinical symptom information into a model enhances domain expertise, improving its detection and interpretation performance. While large language models (LLMs) are shown to be effective for generating explanatory rationales in mental health detection, their substantially large parameter size and high computational cost limit their practicality. Reasoning distillation transfers this ability to smaller language models (SLMs), but inconsistencies in the relevance and domain alignment of LLM-generated rationales pose a challenge. This paper investigates how rationale quality impacts SLM performance in mental health detection and explanation generation. We hypothesize that ensuring high-quality and domain-relevant rationales enhances the distillation. To this end, we propose a framework that selects rationales based on their alignment with expert clinical reasoning. Experiments show that our quality-focused approach significantly enhances SLM performance in both mental disorder detection and rationale generation. This work highlights the importance of rationale quality and offers an insightful framework for knowledge transfer in mental health applications.",
      "arxiv_url": "https://arxiv.org/abs/2505.20014",
      "pdf_url": "https://arxiv.org/pdf/2505.20014",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12978",
      "title": "Multi-document Summarization through Multi-document Event Relation Graph Reasoning in LLMs: a case study in Framing Bias Mitigation",
      "authors": [
        "Yuanyuan Lei",
        "Ruihong Huang"
      ],
      "abstract": "Media outlets are becoming more partisan and polarized nowadays. Most previous work focused on detecting media bias. In this paper, we aim to mitigate media bias by generating a neutralized summary given multiple articles presenting different ideological views. Motivated by the critical role of events and event relations in media bias detection, we propose to increase awareness of bias in LLMs via multi-document events reasoning and use a multi-document event relation graph to guide the summarization process. This graph contains rich event information useful to reveal bias: four common types of in-doc event relations to reflect content framing bias, cross-doc event coreference relation to reveal content selection bias, and event-level moral opinions to highlight opinionated framing bias. We further develop two strategies to incorporate the multi-document event relation graph for neutralized summarization. Firstly, we convert a graph into natural language descriptions and feed the textualized graph into LLMs as a part of a hard text prompt. Secondly, we encode the graph with graph attention network and insert the graph embedding into LLMs as a soft prompt. Both automatic evaluation and human evaluation confirm that our approach effectively mitigates both lexical and informational media bias, and meanwhile improves content preservation.",
      "arxiv_url": "https://arxiv.org/abs/2506.12978",
      "pdf_url": "https://arxiv.org/pdf/2506.12978",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.14625",
      "title": "Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models",
      "authors": [
        "Chenchen Yuan",
        "Zheyu Zhang",
        "Shuo Yang",
        "Bardh Prenkaj",
        "Gjergji Kasneci"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs'moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.",
      "arxiv_url": "https://arxiv.org/abs/2506.14625",
      "pdf_url": "https://arxiv.org/pdf/2506.14625",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "afa53c2ec4ced0dc15a2709b4857788fc5e37b56",
      "title": "NewsInterview: a Dataset and a Playground to Evaluate LLMs' Grounding Gap via Informational Interviews",
      "authors": [
        "Alexander Spangher",
        "Michael Lu",
        "Sriya Kalyan",
        "Hyundong Justin Cho",
        "Tenghao Huang",
        "Weiyan Shi",
        "Jonathan May"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in generating coherent text but often struggle with strategic dialogue. To address this gap, we focus on journalistic interviews. We curate a dataset of 40,000 two-person informational interviews from major news organizations in scenarios where human interviewers employ strategies to coax information from sources. We then try to mimic these activities with LLMs and find striking differences; models are less likely to use acknowledgments and more likely to rabbit-hole and not pivot to other topics. Real-izing that a fundamental deficit exists in LLM multi-turn planning and strategic thinking, we develop a realistic simulated environment, incorporating source personas and persuasive elements, in order to facilitate the development of agents with long-horizon rewards. Our experiments show that mimicry failures are not two-sided; when posing as a source, models adequately reflect human behavior in information sharing, making our simulation a realistic benchmark. Interviewer-LLMs, however, struggle with engaging persuasively, leading to sub-optimal information extraction across model size and capability. This simulated game lays the groundwork for future work in enhancing LLMs’ strategic dialogue capabilities. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/afa53c2ec4ced0dc15a2709b4857788fc5e37b56",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.00589",
      "title": "Efficient Annotator Reliability Assessment with EffiARA",
      "authors": [
        "Owen Cook",
        "Jake Vasilakes",
        "Ian Roberts",
        "Xingyi Song"
      ],
      "abstract": "Data annotation is an essential component of the machine learning pipeline; it is also a costly and time-consuming process. With the introduction of transformer-based models, annotation at the document level is increasingly popular; however, there is no standard framework for structuring such tasks. The EffiARA annotation framework is, to our knowledge, the first project to support the whole annotation pipeline, from understanding the resources required for an annotation task to compiling the annotated dataset and gaining insights into the reliability of individual annotators as well as the dataset as a whole. The framework's efficacy is supported by two previous studies: one improving classification performance through annotator-reliability-based soft-label aggregation and sample weighting, and the other increasing the overall agreement among annotators through removing identifying and replacing an unreliable annotator. This work introduces the EffiARA Python package and its accompanying webtool, which provides an accessible graphical user interface for the system. We open-source the EffiARA Python package at https://github.com/MiniEggz/EffiARA and the webtool is publicly accessible at https://effiara.gate.ac.uk.",
      "arxiv_url": "https://arxiv.org/abs/2504.00589",
      "pdf_url": "https://arxiv.org/pdf/2504.00589",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.15372",
      "title": "COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for Summarization and Headline Generation",
      "authors": [
        "Raghvendra Kumar",
        "Mohammed Salman S A",
        "Aryan Sahu",
        "Tridib Nandi",
        "P. P",
        "Sriparna Saha",
        "Jose G Moreno"
      ],
      "abstract": "Despite progress in comment-aware multimodal and multilingual summarization for English and Chinese, research in Indian languages remains limited. This study addresses this gap by introducing COSMMIC, a pioneering comment-sensitive multimodal, multilingual dataset featuring nine major Indian languages. COSMMIC comprises 4,959 article-image pairs and 24,484 reader comments, with ground-truth summaries available in all included languages. Our approach enhances summaries by integrating reader insights and feedback. We explore summarization and headline generation across four configurations: (1) using article text alone, (2) incorporating user comments, (3) utilizing images, and (4) combining text, comments, and images. To assess the dataset's effectiveness, we employ state-of-the-art language models such as LLama3 and GPT-4. We conduct a comprehensive study to evaluate different component combinations, including identifying supportive comments, filtering out noise using a dedicated comment classifier using IndicBERT, and extracting valuable insights from images with a multilingual CLIP-based classifier. This helps determine the most effective configurations for natural language generation (NLG) tasks. Unlike many existing datasets that are either text-only or lack user comments in multimodal settings, COSMMIC uniquely integrates text, images, and user feedback. This holistic approach bridges gaps in Indian language resources, advancing NLP research and fostering inclusivity.",
      "arxiv_url": "https://arxiv.org/abs/2506.15372",
      "pdf_url": "https://arxiv.org/pdf/2506.15372",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19112",
      "title": "Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering",
      "authors": [
        "Zheng Chu",
        "Huiming Fan",
        "Jingchang Chen",
        "Qianyu Wang",
        "Mingda Yang",
        "Jiafeng Liang",
        "Zhongjie Wang",
        "Hao Li",
        "Guo Tang",
        "Ming Liu",
        "Bing Qin"
      ],
      "abstract": "Although large language models (LLMs) have demonstrated remarkable reasoning capabilities, they still face challenges in knowledge-intensive multi-hop reasoning. Recent work explores iterative retrieval to address complex problems. However, the lack of intermediate guidance often results in inaccurate retrieval and flawed intermediate reasoning, leading to incorrect reasoning. To address these, we propose Self-Critique Guided Iterative Reasoning (SiGIR), which uses self-critique feedback to guide the iterative reasoning process. Specifically, through end-to-end training, we enable the model to iteratively address complex problems via question decomposition. Additionally, the model is able to self-evaluate its intermediate reasoning steps. During iterative reasoning, the model engages in branching exploration and employs self-evaluation to guide the selection of promising reasoning trajectories. Extensive experiments on three multi-hop reasoning datasets demonstrate the effectiveness of our proposed method, surpassing the previous SOTA by $8.6\\%$. Furthermore, our thorough analysis offers insights for future research. Our code, data, and models are available at Github: https://github.com/zchuz/SiGIR-MHQA.",
      "arxiv_url": "https://arxiv.org/abs/2505.19112",
      "pdf_url": "https://arxiv.org/pdf/2505.19112",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16761",
      "title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
      "authors": [
        "Joseph Suh",
        "Erfan Jahanparast",
        "Suhong Moon",
        "Minwoo Kang",
        "Serina Chang"
      ],
      "abstract": "Large language models (LLMs) present novel opportunities in public opinion research by predicting survey responses in advance during the early stages of survey design. Prior methods steer LLMs via descriptions of subpopulations as LLMs' input prompt, yet such prompt engineering approaches have struggled to faithfully predict the distribution of survey responses from human subjects. In this work, we propose directly fine-tuning LLMs to predict response distributions by leveraging unique structural characteristics of survey data. To enable fine-tuning, we curate SubPOP, a significantly scaled dataset of 3,362 questions and 70K subpopulation-response pairs from well-established public opinion surveys. We show that fine-tuning on SubPOP greatly improves the match between LLM predictions and human responses across various subpopulations, reducing the LLM-human gap by up to 46% compared to baselines, and achieves strong generalization to unseen surveys and subpopulations. Our findings highlight the potential of survey-based fine-tuning to improve opinion prediction for diverse, real-world subpopulations and therefore enable more efficient survey designs. Our code is available at https://github.com/JosephJeesungSuh/subpop.",
      "arxiv_url": "https://arxiv.org/abs/2502.16761",
      "pdf_url": "https://arxiv.org/pdf/2502.16761",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11355",
      "title": "\"Nuclear Deployed!\": Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents",
      "authors": [
        "Rongwu Xu",
        "Xiaojian Li",
        "Shuo Chen",
        "Wei Xu"
      ],
      "abstract": "Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. We also show that these agents can violate instructions and superior commands. On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents. We release our code to foster further research.",
      "arxiv_url": "https://arxiv.org/abs/2502.11355",
      "pdf_url": "https://arxiv.org/pdf/2502.11355",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.01306",
      "title": "Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking",
      "authors": [
        "Xiaoxue Cheng",
        "Junyi Li",
        "Wayne Xin Zhao",
        "Jiahui Wen"
      ],
      "abstract": "Large language models (LLMs) demonstrate exceptional capabilities, yet still face the hallucination issue. Typical text generation approaches adopt an auto-regressive generation without deliberate reasoning, which often results in untrustworthy and factually inaccurate responses. In this paper, we propose HaluSearch, a novel framework that incorporates tree search-based algorithms (e.g. MCTS) to enable an explicit slow thinking generation process for mitigating hallucinations of LLMs during inference. Specifically, HaluSearch frames text generation as a step-by-step reasoning process, using a self-evaluation reward model to score each generation step and guide the tree search towards the most reliable generation pathway for fully exploiting the internal knowledge of LLMs. To balance efficiency and quality, we introduce a hierarchical thinking system switch mechanism inspired by the dual process theory in cognitive science, which dynamically alternates between fast and slow thinking modes at both the instance and step levels, adapting to the complexity of questions and reasoning states. We conduct extensive experiments on both English and Chinese datasets and the results show that our approach significantly outperforms baseline approaches.",
      "arxiv_url": "https://arxiv.org/abs/2501.01306",
      "pdf_url": "https://arxiv.org/pdf/2501.01306",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.10231",
      "title": "Classifying Unreliable Narrators with Large Language Models",
      "authors": [
        "Anneliese Brei",
        "Katharine Henry",
        "Abhisheik Sharma",
        "Shashank Srivastava",
        "Snigdha Chaturvedi"
      ],
      "abstract": "Often when we interact with a first-person account of events, we consider whether or not the narrator, the primary speaker of the text, is reliable. In this paper, we propose using computational methods to identify unreliable narrators, i.e. those who unintentionally misrepresent information. Borrowing literary theory from narratology to define different types of unreliable narrators based on a variety of textual phenomena, we present TUNa, a human-annotated dataset of narratives from multiple domains, including blog posts, subreddit posts, hotel reviews, and works of literature. We define classification tasks for intra-narrational, inter-narrational, and inter-textual unreliabilities and analyze the performance of popular open-weight and proprietary LLMs for each. We propose learning from literature to perform unreliable narrator classification on real-world text data. To this end, we experiment with few-shot, fine-tuning, and curriculum learning settings. Our results show that this task is very challenging, and there is potential for using LLMs to identify unreliable narrators. We release our expert-annotated dataset and code and invite future research in this area.",
      "arxiv_url": "https://arxiv.org/abs/2506.10231",
      "pdf_url": "https://arxiv.org/pdf/2506.10231",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14642",
      "title": "How Far are LLMs from Being Our Digital Twins? A Benchmark for Persona-Based Behavior Chain Simulation",
      "authors": [
        "Rui Li",
        "Heming Xia",
        "Xinfeng Yuan",
        "Qingxiu Dong",
        "Lei Sha",
        "Wenjie Li",
        "Zhifang Sui"
      ],
      "abstract": "Recently, LLMs have garnered increasing attention across academic disciplines for their potential as human digital twins, virtual proxies designed to replicate individuals and autonomously perform tasks such as decision-making, problem-solving, and reasoning on their behalf. However, current evaluations of LLMs primarily emphasize dialogue simulation while overlooking human behavior simulation, which is crucial for digital twins. To address this gap, we introduce BehaviorChain, the first benchmark for evaluating LLMs'ability to simulate continuous human behavior. BehaviorChain comprises diverse, high-quality, persona-based behavior chains, totaling 15,846 distinct behaviors across 1,001 unique personas, each with detailed history and profile metadata. For evaluation, we integrate persona metadata into LLMs and employ them to iteratively infer contextually appropriate behaviors within dynamic scenarios provided by BehaviorChain. Comprehensive evaluation results demonstrated that even state-of-the-art models struggle with accurately simulating continuous human behavior.",
      "arxiv_url": "https://arxiv.org/abs/2502.14642",
      "pdf_url": "https://arxiv.org/pdf/2502.14642",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b051360d0710dd3dd89b882f8f77312116faef5c",
      "title": "Mining Complex Patterns of Argumentative Reasoning in Natural Language Dialogue",
      "authors": [
        "Ramon Ruiz-Dolz",
        "Zlata Kikteva",
        "John Lawrence"
      ],
      "abstract": "Argumentation scheme mining is the task of automatically identifying reasoning mechanisms behind argument inferences. These mechanisms provide insights into underlying argu-ment structures and guide the assessment of natural language arguments. Research on argumentation scheme mining, however, has always been limited by the scarcity of large enough publicly available corpora containing scheme annotations. In this paper, we present the first state-of-the-art results for mining argumentation schemes in natural language dialogue. For this purpose, we create QT-S CHEMES , a new corpus of 441 arguments annotated with 24 argumentation schemes. Using this corpus, we leverage the capabilities of LLMs and Transformer-based models, pre-training them on a large corpus containing textbook-like argumentation schemes and validating their applicability in real-world scenarios.",
      "arxiv_url": "https://www.semanticscholar.org/paper/b051360d0710dd3dd89b882f8f77312116faef5c",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.03106",
      "title": "Monitoring Decoding: Mitigating Hallucination via Evaluating the Factuality of Partial Response during Generation",
      "authors": [
        "Yurui Chang",
        "Bochuan Cao",
        "Lu Lin"
      ],
      "abstract": "While large language models have demonstrated exceptional performance across a wide range of tasks, they remain susceptible to hallucinations -- generating plausible yet factually incorrect contents. Existing methods to mitigating such risk often rely on sampling multiple full-length generations, which introduces significant response latency and becomes ineffective when the model consistently produces hallucinated outputs with high confidence. To address these limitations, we introduce Monitoring Decoding (MD), a novel framework that dynamically monitors the generation process and selectively applies in-process interventions, focusing on revising crucial tokens responsible for hallucinations. Instead of waiting until completion of multiple full-length generations, we identify hallucination-prone tokens during generation using a monitor function, and further refine these tokens through a tree-based decoding strategy. This approach ensures an enhanced factual accuracy and coherence in the generated output while maintaining efficiency. Experimental results demonstrate that MD consistently outperforms self-consistency-based approaches in both effectiveness and efficiency, achieving higher factual accuracy while significantly reducing computational overhead.",
      "arxiv_url": "https://arxiv.org/abs/2503.03106",
      "pdf_url": "https://arxiv.org/pdf/2503.03106",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13794",
      "title": "LESA: Learnable LLM Layer Scaling-Up",
      "authors": [
        "Yifei Yang",
        "Zouying Cao",
        "Xinbei Ma",
        "Yao Yao",
        "Libo Qin",
        "Zhi Chen",
        "Hai Zhao"
      ],
      "abstract": "Training Large Language Models (LLMs) from scratch requires immense computational resources, making it prohibitively expensive. Model scaling-up offers a promising solution by leveraging the parameters of smaller models to create larger ones. However, existing depth scaling-up methods rely on empirical heuristic rules for layer duplication, which result in poorer initialization and slower convergence during continual pre-training. We propose \\textbf{LESA}, a novel learnable method for depth scaling-up. By concatenating parameters from each layer and applying Singular Value Decomposition, we uncover latent patterns between layers, suggesting that inter-layer parameters can be learned. LESA uses a neural network to predict the parameters inserted between adjacent layers, enabling better initialization and faster training. Experiments show that LESA outperforms existing baselines, achieving superior performance with less than half the computational cost during continual pre-training. Extensive analyses demonstrate its effectiveness across different model sizes and tasks.",
      "arxiv_url": "https://arxiv.org/abs/2502.13794",
      "pdf_url": "https://arxiv.org/pdf/2502.13794",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b10e992ca9ffc27a425313cdb83f2c42fc32dd3b",
      "title": "Eeyore: Realistic Depression Simulation via Expert-in-the-Loop Supervised and Preference Optimization",
      "authors": [
        "Siyang Liu",
        "Bianca Brie",
        "Wenda Li",
        "Laura Biester",
        "Andrew Lee",
        "James W. Pennebaker",
        "Rada Mihalcea"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/b10e992ca9ffc27a425313cdb83f2c42fc32dd3b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18413",
      "title": "When Benchmarks Talk: Re-Evaluating Code LLMs with Interactive Feedback",
      "authors": [
        "Jane Pan",
        "Ryan Shar",
        "Jacob Pfau",
        "Ameet Talwalkar",
        "He He",
        "Valerie Chen"
      ],
      "abstract": "Programming is a fundamentally interactive process, yet coding assistants are often evaluated using static benchmarks that fail to measure how well models collaborate with users. We introduce an interactive evaluation pipeline to examine how LLMs incorporate different types of feedback in a collaborative setting. Specifically, we perturb static coding benchmarks so that the code model must interact with a simulated user to retrieve key information about the problem. We find that interaction significantly affects model performance, as the relative rankings of 10 models across 3 datasets often vary between static and interactive settings, despite models being fairly robust to feedback that contains errors. We also observe that even when different feedback types are equally effective with respect to performance, they can impact model behaviors such as (1) how models respond to higher- vs. lower-quality feedback and (2) whether models prioritize aesthetic vs. functional edits. Our work aims to\"re-evaluate\"model coding capabilities through an interactive lens toward bridging the gap between existing evaluations and real-world usage.",
      "arxiv_url": "https://arxiv.org/abs/2502.18413",
      "pdf_url": "https://arxiv.org/pdf/2502.18413",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.05767",
      "title": "Migician: Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models",
      "authors": [
        "You Li",
        "Heyu Huang",
        "Chi Chen",
        "Kaiyu Huang",
        "Chao Huang",
        "Zonghao Guo",
        "Zhiyuan Liu",
        "Jinan Xu",
        "Yuhua Li",
        "Ruixuan Li",
        "Maosong Sun"
      ],
      "abstract": "The recent advancement of Multimodal Large Language Models (MLLMs) has significantly improved their fine-grained perception of single images and general comprehension across multiple images. However, existing MLLMs still face challenges in achieving precise grounding in complex multi-image scenarios. To address this, we first explore a Chain-of-Thought (CoT) framework that integrates single-image grounding with multi-image comprehension. While partially effective, it remains unstable and struggles to capture abstract visual information due to its non-end-to-end nature. Therefore, we introduce Migician, the first multi-image grounding model capable of performing free-form and accurate grounding across multiple images. To support this, we present the MGrounding-630k dataset, which comprises data for several multi-image grounding tasks derived from existing datasets, along with newly generated free-form grounding instruction-following data. Furthermore, we propose MIG-Bench, a comprehensive benchmark specifically designed for evaluating multi-image grounding capabilities. Experimental results demonstrate that our model achieves significantly superior multi-image grounding capabilities, outperforming the best existing MLLMs by 24.94% and even surpassing much larger 70B models. Our code, model, dataset, and benchmark are fully open-sourced at https://migician-vg.github.io/.",
      "arxiv_url": "https://arxiv.org/abs/2501.05767",
      "pdf_url": "https://arxiv.org/pdf/2501.05767",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.11155",
      "title": "Evaluating Multimodal Large Language Models on Video Captioning via Monte Carlo Tree Search",
      "authors": [
        "Linhao Yu",
        "Xinguang Ji",
        "Yahui Liu",
        "Fanheng Kong",
        "Chenxi Sun",
        "Jingyuan Zhang",
        "Hongzhi Zhang",
        "W. V.",
        "Fuzheng Zhang",
        "Deyi Xiong"
      ],
      "abstract": "Video captioning can be used to assess the video understanding capabilities of Multimodal Large Language Models (MLLMs). However, existing benchmarks and evaluation protocols suffer from crucial issues, such as inadequate or homogeneous creation of key points, exorbitant cost of data creation, and limited evaluation scopes. To address these issues, we propose an automatic framework, named AutoCaption, which leverages Monte Carlo Tree Search (MCTS) to construct numerous and diverse descriptive sentences (\\textit{i.e.}, key points) that thoroughly represent video content in an iterative way. This iterative captioning strategy enables the continuous enhancement of video details such as actions, objects' attributes, environment details, etc. We apply AutoCaption to curate MCTS-VCB, a fine-grained video caption benchmark covering video details, thereby enabling a comprehensive evaluation of MLLMs on the video captioning task. We evaluate more than 20 open- and closed-source MLLMs of varying sizes on MCTS-VCB. Results show that MCTS-VCB can effectively and comprehensively evaluate the video captioning capability, with Gemini-1.5-Pro achieving the highest F1 score of 71.2. Interestingly, we fine-tune InternVL2.5-8B with the AutoCaption-generated data, which helps the model achieve an overall improvement of 25.0% on MCTS-VCB and 16.3% on DREAM-1K, further demonstrating the effectiveness of AutoCaption. The code and data are available at https://github.com/tjunlp-lab/MCTS-VCB.",
      "arxiv_url": "https://arxiv.org/abs/2506.11155",
      "pdf_url": "https://arxiv.org/pdf/2506.11155",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12501",
      "title": "Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge",
      "authors": [
        "Qiyuan Zhang",
        "Yufei Wang",
        "Yuxin Jiang",
        "Liangyou Li",
        "Chuhan Wu",
        "Yasheng Wang",
        "Xin Jiang",
        "Lifeng Shang",
        "Ruiming Tang",
        "Fuyuan Lyu",
        "Chen Ma"
      ],
      "abstract": "LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly rely on majority voting or criteria expansion, which is insufficient to address the limitation in CoT. We propose Crowd-based Comparative Evaluation, which introduces additional crowd responses to compare with the candidate responses, thereby exposing deeper and more comprehensive details within the candidate responses. This process effectively guides LLM-as-a-Judge to provide a more detailed CoT judgment. Extensive experiments demonstrate that our approach enhances evaluation reliability, achieving an average accuracy gain of 6.7% across five benchmarks. Moreover, our method produces higher-quality CoTs that facilitate judge distillation and exhibit superior performance in rejection sampling for supervised fine-tuning (SFT), referred to as crowd rejection sampling, thereby enabling more efficient SFT. Our analysis confirms that CoTs generated by ours are more comprehensive and of higher quality, and evaluation accuracy improves as inference scales.",
      "arxiv_url": "https://arxiv.org/abs/2502.12501",
      "pdf_url": "https://arxiv.org/pdf/2502.12501",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.13077",
      "title": "Advancing Sequential Numerical Prediction in Autoregressive Models",
      "authors": [
        "Xiang Fei",
        "Jinghui Lu",
        "Qi Sun",
        "Hao Feng",
        "Yanjie Wang",
        "Wei Shi",
        "An-Lan Wang",
        "Jingqun Tang",
        "Can Huang"
      ],
      "abstract": "Autoregressive models have become the de facto choice for sequence generation tasks, but standard approaches treat digits as independent tokens and apply cross-entropy loss, overlooking the coherent structure of numerical sequences. This paper introduces Numerical Token Integrity Loss (NTIL) to address this gap. NTIL operates at two levels: (1) token-level, where it extends the Earth Mover's Distance (EMD) to preserve ordinal relationships between numerical values, and (2) sequence-level, where it penalizes the overall discrepancy between the predicted and actual sequences. This dual approach improves numerical prediction and integrates effectively with LLMs/MLLMs. Extensive experiments show significant performance improvements with NTIL.",
      "arxiv_url": "https://arxiv.org/abs/2505.13077",
      "pdf_url": "https://arxiv.org/pdf/2505.13077",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17390",
      "title": "Mitigating Bias in RAG: Controlling the Embedder",
      "authors": [
        "Taeyoun Kim",
        "Jacob Mitchell Springer",
        "Aditi Raghunathan",
        "Maarten Sap"
      ],
      "abstract": "In retrieval augmented generation (RAG) systems, each individual component -- the LLM, embedder, and corpus -- could introduce biases in the form of skews towards outputting certain perspectives or identities. In this work, we study the conflict between biases of each component and their relationship to the overall bias of the RAG system, which we call bias conflict. Examining both gender and political biases as case studies, we show that bias conflict can be characterized through a linear relationship among components despite its complexity in 6 different LLMs. Through comprehensive fine-tuning experiments creating 120 differently biased embedders, we demonstrate how to control bias while maintaining utility and reveal the importance of reverse-biasing the embedder to mitigate bias in the overall system. Additionally, we find that LLMs and tasks exhibit varying sensitivities to the embedder bias, a crucial factor to consider for debiasing. Our results underscore that a fair RAG system can be better achieved by carefully controlling the bias of the embedder rather than increasing its fairness.",
      "arxiv_url": "https://arxiv.org/abs/2502.17390",
      "pdf_url": "https://arxiv.org/pdf/2502.17390",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.13065",
      "title": "MotiveBench: How Far Are We From Human-Like Motivational Reasoning in Large Language Models?",
      "authors": [
        "Xixian Yong",
        "Jianxun Lian",
        "Xiaoyuan Yi",
        "Xiao Zhou",
        "Xing Xie"
      ],
      "abstract": "Large language models (LLMs) have been widely adopted as the core of agent frameworks in various scenarios, such as social simulations and AI companions. However, the extent to which they can replicate human-like motivations remains an underexplored question. Existing benchmarks are constrained by simplistic scenarios and the absence of character identities, resulting in an information asymmetry with real-world situations. To address this gap, we propose MotiveBench, which consists of 200 rich contextual scenarios and 600 reasoning tasks covering multiple levels of motivation. Using MotiveBench, we conduct extensive experiments on seven popular model families, comparing different scales and versions within each family. The results show that even the most advanced LLMs still fall short in achieving human-like motivational reasoning. Our analysis reveals key findings, including the difficulty LLMs face in reasoning about\"love&belonging\"motivations and their tendency toward excessive rationality and idealism. These insights highlight a promising direction for future research on the humanization of LLMs. The dataset, benchmark, and code are available at https://aka.ms/motivebench.",
      "arxiv_url": "https://arxiv.org/abs/2506.13065",
      "pdf_url": "https://arxiv.org/pdf/2506.13065",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11022",
      "title": "MultiTEND: A Multilingual Benchmark for Natural Language to NoSQL Query Translation",
      "authors": [
        "Zhiqian Qin",
        "Yuanfeng Song",
        "Jinwei Lu",
        "Yuanwei Song",
        "Shuaimin Li",
        "C. Zhang"
      ],
      "abstract": "Natural language interfaces for NoSQL databases are increasingly vital in the big data era, enabling users to interact with complex, unstructured data without deep technical expertise. However, most recent advancements focus on English, leaving a gap for multilingual support. This paper introduces MultiTEND, the first and largest multilingual benchmark for natural language to NoSQL query generation, covering six languages: English, German, French, Russian, Japanese and Mandarin Chinese. Using MultiTEND, we analyze challenges in translating natural language to NoSQL queries across diverse linguistic structures, including lexical and syntactic differences. Experiments show that performance accuracy in both English and non-English settings remains relatively low, with a 4%-6% gap across scenarios like fine-tuned SLM, zero-shot LLM, and RAG for LLM. To address the aforementioned challenges, we introduce MultiLink, a novel framework that bridges the multilingual input to NoSQL query generation gap through a Parallel Linking Process. It breaks down the task into multiple steps, integrating parallel multilingual processing, Chain-of-Thought (CoT) reasoning, and Retrieval-Augmented Generation (RAG) to tackle lexical and structural challenges inherent in multilingual NoSQL generation. MultiLink shows enhancements in all metrics for every language against the top baseline, boosting execution accuracy by about 15% for English and averaging a 10% improvement for non-English languages.",
      "arxiv_url": "https://arxiv.org/abs/2502.11022",
      "pdf_url": "https://arxiv.org/pdf/2502.11022",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.15654",
      "title": "People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text",
      "authors": [
        "Jenna Russell",
        "Marzena Karpinska",
        "Mohit Iyyer"
      ],
      "abstract": "In this paper, we study how well humans can detect text generated by commercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300 non-fiction English articles, label them as either human-written or AI-generated, and provide paragraph-length explanations for their decisions. Our experiments show that annotators who frequently use LLMs for writing tasks excel at detecting AI-generated text, even without any specialized training or feedback. In fact, the majority vote among five such\"expert\"annotators misclassifies only 1 of 300 articles, significantly outperforming most commercial and open-source detectors we evaluated even in the presence of evasion tactics like paraphrasing and humanization. Qualitative analysis of the experts' free-form explanations shows that while they rely heavily on specific lexical clues ('AI vocabulary'), they also pick up on more complex phenomena within the text (e.g., formality, originality, clarity) that are challenging to assess for automatic detectors. We release our annotated dataset and code to spur future research into both human and automated detection of AI-generated text.",
      "arxiv_url": "https://arxiv.org/abs/2501.15654",
      "pdf_url": "https://arxiv.org/pdf/2501.15654",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.12908",
      "title": "HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models",
      "authors": [
        "Xinyan Jiang",
        "Hang Ye",
        "Yongxin Zhu",
        "Xiaoying Zheng",
        "Zikang Chen",
        "Jun Gong"
      ],
      "abstract": "Large Language Models (LLMs) often generate hallucinations, producing outputs that are contextually inaccurate or factually incorrect. We introduce HICD, a novel method designed to induce hallucinations for contrastive decoding to mitigate hallucinations. Unlike existing contrastive decoding methods, HICD selects attention heads crucial to the model's prediction as inducing heads, then induces hallucinations by dispersing attention of these inducing heads and compares the hallucinated outputs with the original outputs to obtain the final result. Our approach significantly improves performance on tasks requiring contextual faithfulness, such as context completion, reading comprehension, and question answering. It also improves factuality in tasks requiring accurate knowledge recall. We demonstrate that our inducing heads selection and attention dispersion method leads to more\"contrast-effective\"hallucinations for contrastive decoding, outperforming other hallucination-inducing methods. Our findings provide a promising strategy for reducing hallucinations by inducing hallucinations in a controlled manner, enhancing the performance of LLMs in a wide range of tasks.",
      "arxiv_url": "https://arxiv.org/abs/2503.12908",
      "pdf_url": "https://arxiv.org/pdf/2503.12908",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.24016",
      "title": "EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations",
      "authors": [
        "Hyunjong Kim",
        "Sangyeop Kim",
        "Jongheon Jeong",
        "Yeongjae Cho",
        "Sungzoon Cho"
      ],
      "abstract": "Recent advances in large language models and vision-language models have led to growing interest in explainable evaluation metrics for image captioning. However, these metrics generate explanations without standardized criteria, and the overall quality of the generated explanations remains unverified. In this paper, we propose EXPERT, a reference-free evaluation metric that provides structured explanations based on three fundamental criteria: fluency, relevance, and descriptiveness. By constructing large-scale datasets of high-quality structured explanations, we develop a two-stage evaluation template to effectively supervise a vision-language model for both scoring and explanation generation. EXPERT achieves state-of-the-art results on benchmark datasets while providing significantly higher-quality explanations than existing metrics, as validated through comprehensive human evaluation. Our code and datasets are available at https://github.com/hjkim811/EXPERT.",
      "arxiv_url": "https://arxiv.org/abs/2506.24016",
      "pdf_url": "https://arxiv.org/pdf/2506.24016",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b1e79802a72689b61f186306551db6ad33885306",
      "title": "SurveyPilot: an Agentic Framework for Automated Human Opinion Collection from Social Media",
      "authors": [
        "Viet Thanh Pham",
        "Lizhen Qu",
        "Zhuang Li",
        "Suraj Sharma",
        "Gholamreza Haffari"
      ],
      "abstract": "Opinion survey research is a crucial method used by social scientists for understanding societal beliefs and behaviors. Traditional methodologies often entail high costs and limited scalability, while current automated methods such as opinion synthesis exhibit severe biases and lack traceability. In this paper, we introduce S UR - VEY P ILOT , a novel finite-state orchestrated agentic framework that automates the collection and analysis of human opinions from social media platforms. S URVEY P ILOT addresses the limitations of pioneering approaches by (i) providing transparency and traceability in each state of opinion collection and (ii) incorporating several techniques for mitigating biases, notably with a novel genetic algorithm for improving result diversity. Our extensive experiments reveal that S URVEY P ILOT achieves a close alignment with authentic survey re-sults across multiple domains, observing average relative improvements of 68.98% and 51.37% when comparing to opinion synthesis and agent-based approaches. Implementation of S URVEY P ILOT is available on https: //github.com/thanhpv2102/SurveyPilot",
      "arxiv_url": "https://www.semanticscholar.org/paper/b1e79802a72689b61f186306551db6ad33885306",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12835",
      "title": "Subword models struggle with word learning, but surprisal hides it",
      "authors": [
        "Bastian Bunzeck",
        "Sina Zarrieß"
      ],
      "abstract": "We study word learning in subword and character language models with the psycholinguistic lexical decision task. While subword LMs struggle to discern words and non-words with high accuracy, character LMs solve this task easily and consistently. Only when supplied with further contexts do subword LMs perform similarly to character models. Additionally, when looking at word-level and syntactic learning trajectories, we find that both processes are separable in character LMs. Word learning happens before syntactic learning, whereas both occur simultaneously in subword LMs. This raises questions about the adequacy of subword LMs for modeling language acquisition and positions character LMs as a viable alternative to study processes below the syntactic level.",
      "arxiv_url": "https://arxiv.org/abs/2502.12835",
      "pdf_url": "https://arxiv.org/pdf/2502.12835",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.08459",
      "title": "Diagnosing Failures in Large Language Models' Answers: Integrating Error Attribution into Evaluation Framework",
      "authors": [
        "Zishan Xu",
        "Shuyi Xie",
        "Qingsong Lv",
        "Shupei Xiao",
        "Linlin Song",
        "Wenjuan Sui",
        "Fan Lin"
      ],
      "abstract": "With the widespread application of Large Language Models (LLMs) in various tasks, the mainstream LLM platforms generate massive user-model interactions daily. In order to efficiently analyze the performance of models and diagnose failures in their answers, it is essential to develop an automated framework to systematically categorize and attribute errors. However, existing evaluation models lack error attribution capability. In this work, we establish a comprehensive Misattribution Framework with 6 primary and 15 secondary categories to facilitate in-depth analysis. Based on this framework, we present AttriData, a dataset specifically designed for error attribution, encompassing misattribution, along with the corresponding scores and feedback. We also propose MisAttributionLLM, a fine-tuned model on AttriData, which is the first general-purpose judge model capable of simultaneously generating score, misattribution, and feedback. Extensive experiments and analyses are conducted to confirm the effectiveness and robustness of our proposed method.",
      "arxiv_url": "https://arxiv.org/abs/2507.08459",
      "pdf_url": "https://arxiv.org/pdf/2507.08459",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14865",
      "title": "Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical and Cultural Artifacts",
      "authors": [
        "Sara Ghaboura",
        "Ketan More",
        "Ritesh Thawkar",
        "Wafa Al Ghallabi",
        "Omkar Thawakar",
        "F. Khan",
        "Hisham Cholakkal",
        "Salman H. Khan",
        "R. Anwer"
      ],
      "abstract": "Understanding historical and cultural artifacts demands human expertise and advanced computational techniques, yet the process remains complex and time-intensive. While large multimodal models offer promising support, their evaluation and improvement require a standardized benchmark. To address this, we introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning 266 distinct cultures across 10 major historical regions. Designed for AI-driven analysis of manuscripts, artworks, inscriptions, and archaeological discoveries, TimeTravel provides a structured dataset and robust evaluation framework to assess AI models' capabilities in classification, interpretation, and historical comprehension. By integrating AI with historical research, TimeTravel fosters AI-powered tools for historians, archaeologists, researchers, and cultural tourists to extract valuable insights while ensuring technology contributes meaningfully to historical discovery and cultural heritage preservation. We evaluate contemporary AI models on TimeTravel, highlighting their strengths and identifying areas for improvement. Our goal is to establish AI as a reliable partner in preserving cultural heritage, ensuring that technological advancements contribute meaningfully to historical discovery. Our code is available at: \\url{https://github.com/mbzuai-oryx/TimeTravel}.",
      "arxiv_url": "https://arxiv.org/abs/2502.14865",
      "pdf_url": "https://arxiv.org/pdf/2502.14865",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.13835",
      "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space",
      "authors": [
        "Yicheng Chen",
        "Yining Li",
        "Kai Hu",
        "Zerun Ma",
        "Haochen Ye",
        "Kai Chen"
      ],
      "abstract": "Data quality and diversity are key to the construction of effective instruction-tuning datasets. % With the increasing availability of open-source instruction-tuning datasets, it is advantageous to automatically select high-quality and diverse subsets from a vast amount of data. % Existing methods typically prioritize instance quality and use heuristic rules to maintain diversity. % However, this absence of a comprehensive view of the entire collection often leads to suboptimal results. % Moreover, heuristic rules generally focus on distance or clustering within the embedding space, which fails to accurately capture the intent of complex instructions in the semantic space. % To bridge this gap, we propose a unified method for quantifying the information content of datasets. This method models the semantic space by constructing a label graph and quantifies diversity based on the distribution of information within the graph. % Based on such a measurement, we further introduce an efficient sampling method that selects data samples iteratively to \\textbf{M}aximize the \\textbf{I}nformation \\textbf{G}ain (MIG) in semantic space. % Experiments on various datasets and base models demonstrate that MIG consistently outperforms state-of-the-art methods. % Notably, the model fine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance to the official SFT model trained on the full dataset, with improvements of +5.73\\% on AlpacaEval and +6.89\\% on Wildbench.",
      "arxiv_url": "https://arxiv.org/abs/2504.13835",
      "pdf_url": "https://arxiv.org/pdf/2504.13835",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.21876",
      "title": "Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation",
      "authors": [
        "Qiyue Gao",
        "Xinyu Pi",
        "Kevin Liu",
        "Junrong Chen",
        "Ruolan Yang",
        "Xinqi Huang",
        "Xinyu Fang",
        "Lu Sun",
        "Gautham Kishore",
        "Bo Ai",
        "Stone Tao",
        "Mengyang Liu",
        "Jiaxi Yang",
        "Chao-Jung Lai",
        "Chuanyang Jin",
        "Jiannan Xiang",
        "Benhao Huang",
        "Zeming Chen",
        "David Danks",
        "Hao Su",
        "Tianmin Shu",
        "Ziqiao Ma",
        "Lianhui Qin",
        "Zhiting Hu"
      ],
      "abstract": "Internal world models (WMs) enable agents to understand the world's state and predict transitions, serving as the basis for advanced deliberative reasoning. Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and Gemini, exhibit potential as general-purpose WMs. While the latest studies have evaluated and shown limitations in specific capabilities such as visual understanding, a systematic evaluation of VLMs'fundamental WM abilities remains absent. Drawing on comparative psychology and cognitive science, we propose a two-stage framework that assesses Perception (visual, spatial, temporal, quantitative, and motion) and Prediction (mechanistic simulation, transitive inference, compositional inference) to provide an atomic evaluation of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse simulated environments with controlled counterfactual simulations. Through 660 experiments on 15 latest commercial and open-source VLMs, we find that these models exhibit striking limitations in basic world modeling abilities. For instance, almost all models perform at near-random accuracy when distinguishing motion trajectories. Additionally, they lack disentangled understanding -- e.g., some models tend to believe blue objects move faster than green ones. More rich results and analyses reveal significant gaps between VLMs and human-level world modeling.",
      "arxiv_url": "https://arxiv.org/abs/2506.21876",
      "pdf_url": "https://arxiv.org/pdf/2506.21876",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b29da0efd8a12333aa5efe38470852dae46b24d4",
      "title": "Natural Language Reasoning in Large Language Models: Analysis and Evaluation",
      "authors": [
        "Debela Tesfaye Gemechu",
        "Ramon Ruiz-Dolz",
        "Henrike Beyer",
        "Chris Reed"
      ],
      "abstract": "While Large Language Models (LLMs) have demonstrated promising results on a range of reasoning benchmarks—particularly in formal logic, mathematical tasks, and Chain-of-Thought prompting—less is known about their capabilities in unconstrained natural language reasoning. Argumentative reasoning, a form of reasoning naturally expressed in language and central to everyday discourse, presents unique challenges for LLMs due to its reliance on context, implicit assumptions, and value judgments. This paper addresses a gap in the study of reasoning in LLMs by presenting the first large-scale evaluation of their unconstrained natural language reasoning capabilities based on natu-ral language argumentation. The paper offers three contributions: (i) the formalisation of a new strategy designed to evaluate argumentative reasoning in LLMs: argument-component selection; (ii) the creation of the Argument Reasoning Tasks (ART) dataset, a new benchmark for argument-component selection based on ar-gument structures for natural language reasoning; and (iii) an extensive experimental analysis involving four different models, demonstrating the limitations of LLMs on natural language reasoning tasks.",
      "arxiv_url": "https://www.semanticscholar.org/paper/b29da0efd8a12333aa5efe38470852dae46b24d4",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03032",
      "title": "TestAgent: An Adaptive and Intelligent Expert for Human Assessment",
      "authors": [
        "Junhao Yu",
        "Zhuang Yan",
        "Yuxuan Sun",
        "Weibo Gao",
        "Qi Liu",
        "Mingyue Cheng",
        "Zhenya Huang",
        "Enhong Chen"
      ],
      "abstract": "Accurately assessing internal human states is key to understanding preferences, offering personalized services, and identifying challenges in real-world applications. Originating from psychometrics, adaptive testing has become the mainstream method for human measurement and has now been widely applied in education, healthcare, sports, and sociology. It customizes assessments by selecting the fewest test questions . However, current adaptive testing methods face several challenges. The mechanized nature of most algorithms leads to guessing behavior and difficulties with open-ended questions. Additionally, subjective assessments suffer from noisy response data and coarse-grained test outputs, further limiting their effectiveness. To move closer to an ideal adaptive testing process, we propose TestAgent, a large language model (LLM)-powered agent designed to enhance adaptive testing through interactive engagement. This is the first application of LLMs in adaptive testing. TestAgent supports personalized question selection, captures test-takers' responses and anomalies, and provides precise outcomes through dynamic, conversational interactions. Experiments on psychological, educational, and lifestyle assessments show our approach achieves more accurate results with 20% fewer questions than state-of-the-art baselines, and testers preferred it in speed, smoothness, and other dimensions.",
      "arxiv_url": "https://arxiv.org/abs/2506.03032",
      "pdf_url": "https://arxiv.org/pdf/2506.03032",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.14693",
      "title": "Rethinking Table Instruction Tuning",
      "authors": [
        "Naihao Deng",
        "Rada Mihalcea"
      ],
      "abstract": "Recent advances in table understanding have focused on instruction-tuning large language models (LLMs) for table-related tasks. However, existing research has overlooked the impact of hyperparameter choices, and also lacks a comprehensive evaluation of the out-of-domain table understanding ability and the general capabilities of these table LLMs. In this paper, we evaluate these abilities in existing table LLMs, and find significant declines in both out-of-domain table understanding and general capabilities as compared to their base models. Through systematic analysis, we show that hyperparameters, such as learning rate, can significantly influence both table-specific and general capabilities. Contrary to the previous table instruction-tuning work, we demonstrate that smaller learning rates and fewer training instances can enhance table understanding while preserving general capabilities. Based on our findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B Instruct, which achieves performance on par with, or surpassing GPT-3.5 and GPT-4 on table tasks, while maintaining strong out-of-domain generalization and general capabilities. Our findings highlight the potential for reduced data annotation costs and more efficient model development through careful hyperparameter selection. We open-source the project and our models.",
      "arxiv_url": "https://arxiv.org/abs/2501.14693",
      "pdf_url": "https://arxiv.org/pdf/2501.14693",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b2d08bba4e53d5068c0391e263d5503f64b6aed2",
      "title": "ConLoan: A Contrastive Multilingual Dataset for Evaluating Loanwords",
      "authors": [
        "Sina Ahmadi",
        "Micha David Hess",
        "Elena Álvarez Mellado",
        "Alessia Battisti",
        "Cui Ding",
        "Anne Göhring",
        "Yingqiang Gao",
        "Zifan Jiang",
        "Andrianos Michail",
        "Peshmerge Morad",
        "Joel Niklaus",
        "Maria Christina Panagiotopoulou",
        "Stefano Perrella",
        "Juri Opitz",
        "Anastassia Shaitarova",
        "Rico Sennrich"
      ],
      "abstract": "Lexical borrowing, the adoption of words from one language into another, is a ubiquitous linguistic phenomenon influenced by geopolitical, societal, and technological factors. This paper introduces ConLoan–a novel contrastive dataset comprising sentences with and without loanwords across 10 languages. Through systematic evaluation using this dataset, we investigate how state-of-the-art machine translation and language models process loanwords compared to their native alternatives. Our experiments reveal that these systems show systematic preferences for loanwords over native terms and exhibit varying performance across languages. These findings provide valuable insights for developing more linguistically robust NLP systems.",
      "arxiv_url": "https://www.semanticscholar.org/paper/b2d08bba4e53d5068c0391e263d5503f64b6aed2",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18581",
      "title": "Removal of Hallucination on Hallucination: Debate-Augmented RAG",
      "authors": [
        "Wentao Hu",
        "Wengyu Zhang",
        "Yiyang Jiang",
        "Chen Jason Zhang",
        "Xiaoyong Wei",
        "Qing Li"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating external knowledge, yet it introduces a critical issue: erroneous or biased retrieval can mislead generation, compounding hallucinations, a phenomenon we term Hallucination on Hallucination. To address this, we propose Debate-Augmented RAG (DRAG), a training-free framework that integrates Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages. In retrieval, DRAG employs structured debates among proponents, opponents, and judges to refine retrieval quality and ensure factual reliability. In generation, DRAG introduces asymmetric information roles and adversarial debates, enhancing reasoning robustness and mitigating factual inconsistencies. Evaluations across multiple tasks demonstrate that DRAG improves retrieval reliability, reduces RAG-induced hallucinations, and significantly enhances overall factual accuracy. Our code is available at https://github.com/Huenao/Debate-Augmented-RAG.",
      "arxiv_url": "https://arxiv.org/abs/2505.18581",
      "pdf_url": "https://arxiv.org/pdf/2505.18581",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-05-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.08342",
      "title": "Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization",
      "authors": [
        "Itai Mondshine",
        "Tzuf Paz-Argaman",
        "Reut Tsarfaty"
      ],
      "abstract": "Automatic n-gram based metrics such as ROUGE are widely used for evaluating generative tasks such as summarization. While these metrics are considered indicative (even if imperfect) of human evaluation for English, their suitability for other languages remains unclear. To address this, we systematically assess evaluation metrics for generation both n-gram-based and neural based to evaluate their effectiveness across languages and tasks. Specifically, we design a large-scale evaluation suite across eight languages from four typological families: agglutinative, isolating, low-fusional, and high-fusional, spanning both low- and high-resource settings, to analyze their correlation with human judgments. Our findings highlight the sensitivity of evaluation metrics to the language type. For example, in fusional languages, n-gram-based metrics show lower correlation with human assessments compared to isolating and agglutinative languages. We also demonstrate that proper tokenization can significantly mitigate this issue for morphologically rich fusional languages, sometimes even reversing negative trends. Additionally, we show that neural-based metrics specifically trained for evaluation, such as COMET, consistently outperform other neural metrics and better correlate with human judgments in low-resource languages. Overall, our analysis highlights the limitations of n-gram metrics for fusional languages and advocates for greater investment in neural-based metrics trained for evaluation tasks.",
      "arxiv_url": "https://arxiv.org/abs/2507.08342",
      "pdf_url": "https://arxiv.org/pdf/2507.08342",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19572",
      "title": "DocMEdit: Towards Document-Level Model Editing",
      "authors": [
        "Li Zeng",
        "Zeming Liu",
        "Chong Feng",
        "Heyan Huang",
        "Yuhang Guo"
      ],
      "abstract": "Model editing aims to correct errors and outdated knowledge in the Large language models (LLMs) with minimal cost. Prior research has proposed a variety of datasets to assess the effectiveness of these model editing methods. However, most existing datasets only require models to output short phrases or sentences, overlooks the widespread existence of document-level tasks in the real world, raising doubts about their practical usability. Aimed at addressing this limitation and promoting the application of model editing in real-world scenarios, we propose the task of document-level model editing. To tackle such challenges and enhance model capabilities in practical settings, we introduce \\benchmarkname, a dataset focused on document-level model editing, characterized by document-level inputs and outputs, extrapolative, and multiple facts within a single edit. We propose a series of evaluation metrics and experiments. The results show that the difficulties in document-level model editing pose challenges for existing model editing methods.",
      "arxiv_url": "https://arxiv.org/abs/2505.19572",
      "pdf_url": "https://arxiv.org/pdf/2505.19572",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b3586399a57942cb30f3239d6b301761f6e7853e",
      "title": "Beyond Negative Stereotypes - Non-Negative Abusive Utterances about Identity Groups and Their Semantic Variants",
      "authors": [
        "Tina Lommel",
        "Elisabeth Eder",
        "Josef Ruppenhofer",
        "Michael Wiegand"
      ],
      "abstract": "We investigate a specific subtype of implicitly abusive language, focusing on non-negative sentences about identity groups (e.g. Women make good cooks ). We introduce a novel data-set comprising such utterances. It not only profiles abusive sentences but also includes various semantic variants of the same characteristic attributed to an identity group, allowing us to systematically examine the impact of different degrees of generalization and perspective framing. Thus we demonstrate that specific variants significantly intensify the perception of abusive-ness. By switching identity groups, we high-light how the characteristics often described in stereotypes are not inherently abusive. We also report on classification experiments.",
      "arxiv_url": "https://www.semanticscholar.org/paper/b3586399a57942cb30f3239d6b301761f6e7853e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20616",
      "title": "PersonaBench: Evaluating AI Models on Understanding Personal Information through Accessing (Synthetic) Private User Data",
      "authors": [
        "Juntao Tan",
        "Liangwei Yang",
        "Zuxin Liu",
        "Zhiwei Liu",
        "Rithesh Murthy",
        "T. Awalgaonkar",
        "Jianguo Zhang",
        "Weiran Yao",
        "Ming Zhu",
        "Shirley Kokane",
        "Silvio Savarese",
        "Huan Wang",
        "Caiming Xiong",
        "Shelby Heinecke"
      ],
      "abstract": "Personalization is critical in AI assistants, particularly in the context of private AI models that work with individual users. A key scenario in this domain involves enabling AI models to access and interpret a user's private data (e.g., conversation history, user-AI interactions, app usage) to understand personal details such as biographical information, preferences, and social connections. However, due to the sensitive nature of such data, there are no publicly available datasets that allow us to assess an AI model's ability to understand users through direct access to personal information. To address this gap, we introduce a synthetic data generation pipeline that creates diverse, realistic user profiles and private documents simulating human activities. Leveraging this synthetic data, we present PersonaBench, a benchmark designed to evaluate AI models'performance in understanding personal information derived from simulated private user data. We evaluate Retrieval-Augmented Generation (RAG) pipelines using questions directly related to a user's personal information, supported by the relevant private documents provided to the models. Our results reveal that current retrieval-augmented AI models struggle to answer private questions by extracting personal information from user documents, highlighting the need for improved methodologies to enhance personalization capabilities in AI.",
      "arxiv_url": "https://arxiv.org/abs/2502.20616",
      "pdf_url": "https://arxiv.org/pdf/2502.20616",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Personalization",
        "RAG"
      ],
      "published_date": "2025-02-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20122",
      "title": "Self-Training Elicits Concise Reasoning in Large Language Models",
      "authors": [
        "Tergel Munkhbat",
        "Namgyu Ho",
        "Seohyun Kim",
        "Yongjin Yang",
        "Yujin Kim",
        "Se-young Yun"
      ],
      "abstract": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs. Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concisely, relative to their default behavior. To elicit this capability, we propose simple fine-tuning methods which leverage self-generated concise reasoning paths obtained by best-of-N sampling and few-shot conditioning, in task-specific settings. Our combined method achieves a 30% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy. By exploiting the fundamental stochasticity and in-context learning capabilities of LLMs, our self-training approach robustly elicits concise reasoning on a wide range of models, including those with extensive post-training. Code is available at https://github.com/TergelMunkhbat/concise-reasoning",
      "arxiv_url": "https://arxiv.org/abs/2502.20122",
      "pdf_url": "https://arxiv.org/pdf/2502.20122",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13246",
      "title": "When People are Floods: Analyzing Dehumanizing Metaphors in Immigration Discourse with Large Language Models",
      "authors": [
        "Julia Mendelsohn",
        "Ceren Budak"
      ],
      "abstract": "Metaphor, discussing one concept in terms of another, is abundant in politics and can shape how people understand important issues. We develop a computational approach to measure metaphorical language, focusing on immigration discourse on social media. Grounded in qualitative social science research, we identify seven concepts evoked in immigration discourse (e.g.\"water\"or\"vermin\"). We propose and evaluate a novel technique that leverages both word-level and document-level signals to measure metaphor with respect to these concepts. We then study the relationship between metaphor, political ideology, and user engagement in 400K US tweets about immigration. While conservatives tend to use dehumanizing metaphors more than liberals, this effect varies widely across concepts. Moreover, creature-related metaphor is associated with more retweets, especially for liberal authors. Our work highlights the potential for computational methods to complement qualitative approaches in understanding subtle and implicit language in political discourse.",
      "arxiv_url": "https://arxiv.org/abs/2502.13246",
      "pdf_url": "https://arxiv.org/pdf/2502.13246",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00732",
      "title": "Bregman Conditional Random Fields: Sequence Labeling with Parallelizable Inference Algorithms",
      "authors": [
        "Caio Corro",
        "Mathieu Lacroix",
        "Joseph Le Roux"
      ],
      "abstract": "We propose a novel discriminative model for sequence labeling called Bregman conditional random fields (BCRF). Contrary to standard linear-chain conditional random fields, BCRF allows fast parallelizable inference algorithms based on iterative Bregman projections. We show how such models can be learned using Fenchel-Young losses, including extension for learning from partial labels. Experimentally, our approach delivers comparable results to CRF while being faster, and achieves better results in highly constrained settings compared to mean field, another parallelizable alternative.",
      "arxiv_url": "https://arxiv.org/abs/2506.00732",
      "pdf_url": "https://arxiv.org/pdf/2506.00732",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.11277",
      "title": "From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning Method for LLMs",
      "authors": [
        "Guocong Li",
        "L",
        "Yihang Wu",
        "Ping Wang",
        "Shuaihan Huang",
        "Hongxia Xu",
        "Jian Wu"
      ],
      "abstract": "Large language models (LLMs) exhibit excellent performance in natural language processing (NLP), but remain highly sensitive to the quality of input queries, especially when these queries contain misleading or inaccurate information. Existing methods focus on correcting the output, but they often overlook the potential of improving the ability of LLMs to detect and correct misleading content in the input itself. In this paper, we propose a novel three-stage fine-tuning method that enhances the ability of LLMs to detect and correct misleading information in the input, further improving response accuracy and reducing hallucinations. Specifically, the three stages include (1) training LLMs to identify misleading information, (2) training LLMs to correct the misleading information using built-in or external knowledge, and (3) training LLMs to generate accurate answers based on the corrected queries. To evaluate our method, we conducted experiments on three datasets for the hallucination detection task and the question answering~(QA) task, as well as two datasets containing misleading information that we constructed. The experimental results demonstrate that our method significantly improves the accuracy and factuality of LLM responses, while also enhancing the ability to detect hallucinations and reducing the generation of hallucinations in the output, particularly when the query contains misleading information.",
      "arxiv_url": "https://arxiv.org/abs/2504.11277",
      "pdf_url": "https://arxiv.org/pdf/2504.11277",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19130",
      "title": "Voting or Consensus? Decision-Making in Multi-Agent Debate",
      "authors": [
        "Lars Benedikt Kaesberg",
        "Jonas Becker",
        "Jan Philip Wahle",
        "Terry Ruas",
        "Bela Gipp"
      ],
      "abstract": "Much of the success of multi-agent debates depends on carefully choosing the right parameters. The decision-making protocol stands out as it can highly impact final model answers, depending on how decisions are reached. Systematic comparison of decision protocols is difficult because many studies alter multiple discussion parameters beyond the protocol. So far, it has been largely unknown how decision-making influences different tasks. This work systematically evaluates the impact of seven decision protocols (e.g., majority voting, unanimity consensus). We change only one variable at a time - the decision protocol - to analyze how different methods affect the collaboration between agents and measure differences in knowledge and reasoning tasks. Our results show that voting protocols improve performance by 13.2% in reasoning tasks and consensus protocols by 2.8% in knowledge tasks compared to other decision protocols. Increasing the number of agents improves performance, while more discussion rounds before voting reduce it. To improve decision-making by increasing answer diversity, we propose two new methods, All-Agents Drafting (AAD) and Collective Improvement (CI). Our methods improve task performance by up to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the importance of decision-making in multi-agent debates beyond scaling.",
      "arxiv_url": "https://arxiv.org/abs/2502.19130",
      "pdf_url": "https://arxiv.org/pdf/2502.19130",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.05752",
      "title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models",
      "authors": [
        "Sungjae Lee",
        "Hyejin Park",
        "Jaechang Kim",
        "Jungseul Ok"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer from computational inefficiency and redundancy. First, they overlook the diversity of task difficulties, leading to unnecessarily extensive searches even for easy tasks. Second, they neglect the semantics of reasoning paths, resulting in redundant exploration of semantically identical paths. To address these limitations, we propose Semantic Exploration with Adaptive Gating (SEAG), a computationally efficient method. SEAG employs an adaptive gating mechanism that dynamically decides whether to conduct a tree search, based on the confidence level of answers from a preceding simple reasoning method. Furthermore, its tree-based exploration consolidates semantically identical reasoning steps, reducing redundant explorations while maintaining or even improving accuracy. Our extensive experiments demonstrate that SEAG significantly improves accuracy by 4.3% on average while requiring only 31% of computational costs compared to existing tree search-based methods on complex reasoning benchmarks including GSM8K and ARC with diverse language models such as Llama2, Llama3, and Mistral. Our code is available at https://github.com/ml-postech/SEAG-semantic-exploration-with-adaptive-gating .",
      "arxiv_url": "https://arxiv.org/abs/2501.05752",
      "pdf_url": "https://arxiv.org/pdf/2501.05752",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10688",
      "title": "CULEMO: Cultural Lenses on Emotion - Benchmarking LLMs for Cross-Cultural Emotion Understanding",
      "authors": [
        "Tadesse Destaw Belay",
        "Ahmed Haj Ahmed",
        "Alvin Grissom",
        "Iqra Ameer",
        "Grigori Sidorov",
        "Olga Kolesnikova",
        "Seid Muhie Yimam"
      ],
      "abstract": "NLP research has increasingly focused on subjective tasks such as emotion analysis. However, existing emotion benchmarks suffer from two major shortcomings: (1) they largely rely on keyword-based emotion recognition, overlooking crucial cultural dimensions required for deeper emotion understanding, and (2) many are created by translating English-annotated data into other languages, leading to potentially unreliable evaluation. To address these issues, we introduce Cultural Lenses on Emotion (CuLEmo), the first benchmark designed to evaluate culture-aware emotion prediction across six languages: Amharic, Arabic, English, German, Hindi, and Spanish. CuLEmo comprises 400 crafted questions per language, each requiring nuanced cultural reasoning and understanding. We use this benchmark to evaluate several state-of-the-art LLMs on culture-aware emotion prediction and sentiment analysis tasks. Our findings reveal that (1) emotion conceptualizations vary significantly across languages and cultures, (2) LLMs performance likewise varies by language and cultural context, and (3) prompting in English with explicit country context often outperforms in-language prompts for culture-aware emotion and sentiment understanding. The dataset and evaluation code are publicly available.",
      "arxiv_url": "https://arxiv.org/abs/2503.10688",
      "pdf_url": "https://arxiv.org/pdf/2503.10688",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.17362",
      "title": "A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit",
      "authors": [
        "Zafarullah Mahmood",
        "Soliman Ali",
        "Jiading Zhu",
        "Mohamed Abdelwahab",
        "Michelle Yu Collins",
        "Sihan Chen",
        "Yi Cheng Zhao",
        "Jodi Wolff",
        "Osnat C. Melamed",
        "N. Minian",
        "Marta M. Maslej",
        "Carolynne Cooper",
        "Matt Ratto",
        "Peter Selby",
        "Jonathan Rose"
      ],
      "abstract": "The conversational capabilities of Large Language Models (LLMs) suggest that they may be able to perform as automated talk therapists. It is crucial to know if these systems would be effective and adhere to known standards. We present a counsellor chatbot that focuses on motivating tobacco smokers to quit smoking. It uses a state-of-the-art LLM and a widely applied therapeutic approach called Motivational Interviewing (MI), and was evolved in collaboration with clinician-scientists with expertise in MI. We also describe and validate an automated assessment of both the chatbot's adherence to MI and client responses. The chatbot was tested on 106 participants, and their confidence that they could succeed in quitting smoking was measured before the conversation and one week later. Participants' confidence increased by an average of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed adherence to MI standards in 98% of utterances, higher than human counsellors. The chatbot scored well on a participant-reported metric of perceived empathy but lower than typical human counsellors. Furthermore, participants' language indicated a good level of motivation to change, a key goal in MI. These results suggest that the automation of talk therapy with a modern LLM has promise.",
      "arxiv_url": "https://arxiv.org/abs/2505.17362",
      "pdf_url": "https://arxiv.org/pdf/2505.17362",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.09570",
      "title": "LLMs Can Achieve High-quality Simultaneous Machine Translation as Efficiently as Offline",
      "authors": [
        "Biao Fu",
        "Minpeng Liao",
        "Kai Fan",
        "Chengxi Li",
        "Liang Zhang",
        "Yidong Chen",
        "Xiaodong Shi"
      ],
      "abstract": "When the complete source sentence is provided, Large Language Models (LLMs) perform excellently in offline machine translation even with a simple prompt\"Translate the following sentence from [src lang] into [tgt lang]:\". However, in many real scenarios, the source tokens arrive in a streaming manner and simultaneous machine translation (SiMT) is required, then the efficiency and performance of decoder-only LLMs are significantly limited by their auto-regressive nature. To enable LLMs to achieve high-quality SiMT as efficiently as offline translation, we propose a novel paradigm that includes constructing supervised fine-tuning (SFT) data for SiMT, along with new training and inference strategies. To replicate the token input/output stream in SiMT, the source and target tokens are rearranged into an interleaved sequence, separated by special tokens according to varying latency requirements. This enables powerful LLMs to learn read and write operations adaptively, based on varying latency prompts, while still maintaining efficient auto-regressive decoding. Experimental results show that, even with limited SFT data, our approach achieves state-of-the-art performance across various SiMT benchmarks, and preserves the original abilities of offline translation. Moreover, our approach generalizes well to document-level SiMT setting without requiring specific fine-tuning, even beyond the offline translation model.",
      "arxiv_url": "https://arxiv.org/abs/2504.09570",
      "pdf_url": "https://arxiv.org/pdf/2504.09570",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.02953",
      "title": "Cultural Learning-Based Culture Adaptation of Language Models",
      "authors": [
        "Chen Cecilia Liu",
        "Anna Korhonen",
        "Iryna Gurevych"
      ],
      "abstract": "Adapting large language models (LLMs) to diverse cultural values is a challenging task, as existing LLMs often reflect the values of specific groups by default, and potentially causing harm to others. In this paper, we present CLCA, a novel framework for enhancing LLM alignment with cultural values based on cultural learning. The framework leverages simulated social interactions to generate conversations in which LLMs engage in role-playing within culturally adapted social scenarios, capturing implicit cultural norms for model fine-tuning. CLCA improves cultural value alignment across various model architectures measured using World Value Survey data, demonstrating the effectiveness of our proposed approach. Our results provide early evidence that understanding intent and social interactions can enhance cultural value adaptation in LLMs, highlighting the promise of training approaches based on cultural learning.",
      "arxiv_url": "https://arxiv.org/abs/2504.02953",
      "pdf_url": "https://arxiv.org/pdf/2504.02953",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11142",
      "title": "NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM",
      "authors": [
        "Zihan Wang",
        "Yaohui Zhu",
        "Gim Hee Lee",
        "Yachun Fan"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) is an essential skill for embodied agents, allowing them to navigate in 3D environments following natural language instructions. High-performance navigation models require a large amount of training data, the high cost of manually annotating data has seriously hindered this field. Therefore, some previous methods translate trajectory videos into step-by-step instructions for expanding data, but such instructions do not match well with users' communication styles that briefly describe destinations or state specific needs. Moreover, local navigation trajectories overlook global context and high-level task planning. To address these issues, we propose NavRAG, a retrieval-augmented generation (RAG) framework that generates user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical scene description tree for 3D scene understanding from global layout to local details, then simulates various user roles with specific demands to retrieve from the scene tree, generating diverse instructions with LLM. We annotate over 2 million navigation instructions across 861 scenes and evaluate the data quality and navigation performance of trained models.",
      "arxiv_url": "https://arxiv.org/abs/2502.11142",
      "pdf_url": "https://arxiv.org/pdf/2502.11142",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03781",
      "title": "Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models",
      "authors": [
        "Seungcheol Park",
        "Jeongin Bae",
        "Beomseok Kwon",
        "Minjun Kim",
        "Byeongwook Kim",
        "S. Kwon",
        "U. Kang",
        "Dongsoo Lee"
      ],
      "abstract": "How can we quantize large language models while preserving accuracy? Quantization is essential for deploying large language models (LLMs) efficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are promising quantization schemes that have strong expressiveness and optimizability, respectively. However, neither scheme leverages both advantages. In this paper, we propose UniQuanF (Unified Quantization with Flexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses both strong expressiveness and optimizability by unifying the flexible mapping technique in UQ and non-uniform quantization levels of BCQ. We propose unified initialization, and local and periodic mapping techniques to optimize the parameters in UniQuanF precisely. After optimization, our unification theorem removes computational and memory overhead, allowing us to utilize the superior accuracy of UniQuanF without extra deployment costs induced by the unification. Experimental results demonstrate that UniQuanF outperforms existing UQ and BCQ methods, achieving up to 4.60% higher accuracy on GSM8K benchmark.",
      "arxiv_url": "https://arxiv.org/abs/2506.03781",
      "pdf_url": "https://arxiv.org/pdf/2506.03781",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b4c23421a6302aeb738c00ce2be6dddf01f71560",
      "title": "Entrospect: Information-Theoretic Self-Reflection Elicits Better Response Refinement of Small Language Models",
      "authors": [
        "Tianqiang Yan",
        "Ziqiao Lin",
        "Lin Zhang",
        "Zhenglong Sun",
        "Yuan Gao"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/b4c23421a6302aeb738c00ce2be6dddf01f71560",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b4d9836c6e8a26693d5519e6ea144b03856a82dd",
      "title": "Beyond Sequences: Two-dimensional Representation and Dependency Encoding for Code Generation",
      "authors": [
        "Xiangyu Zhang",
        "Yu Zhou",
        "Guang Yang",
        "Wei Cheng",
        "Taolue Chen"
      ],
      "abstract": "The advent of large language models has significantly advanced automatic code generation, transforming the way programmers writing code. Inspired by natural language processing, mainstream code generation approaches represent code as a linear sequence of tokens. In this paper, we propose to represent code snip-pets as two-dimensional entities, where both code lines and tokens within lines are explicitly modeled. This representation allows us to capture the hierarchical and spatial structure of code, especially the dependencies between code lines. Our method CoDE introduces a dependency encoding approach that leverages dictionary learning to perform semantic matching between code lines. As such, it avoids the reliance on strict position indices, leading to better generalization to code with diverse context and lengths. We thoroughly evaluate CoDE based on four categories of tasks. The experimental results showcase its generalizability, context understanding and retrieval, as well as interpretability in code generation.",
      "arxiv_url": "https://www.semanticscholar.org/paper/b4d9836c6e8a26693d5519e6ea144b03856a82dd",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02713",
      "title": "Open-Set Living Need Prediction with Large Language Models",
      "authors": [
        "Xiaochong Lan",
        "J. Feng",
        "Yizhou Sun",
        "Chen Gao",
        "Jia Lei",
        "Xinlei Shi",
        "Hengliang Luo",
        "Yong Li"
      ],
      "abstract": "Living needs are the needs people generate in their daily lives for survival and well-being. On life service platforms like Meituan, user purchases are driven by living needs, making accurate living need predictions crucial for personalized service recommendations. Traditional approaches treat this prediction as a closed-set classification problem, severely limiting their ability to capture the diversity and complexity of living needs. In this work, we redefine living need prediction as an open-set classification problem and propose PIGEON, a novel system leveraging large language models (LLMs) for unrestricted need prediction. PIGEON first employs a behavior-aware record retriever to help LLMs understand user preferences, then incorporates Maslow's hierarchy of needs to align predictions with human living needs. For evaluation and application, we design a recall module based on a fine-tuned text embedding model that links flexible need descriptions to appropriate life services. Extensive experiments on real-world datasets demonstrate that PIGEON significantly outperforms closed-set approaches on need-based life service recall by an average of 19.37%. Human evaluation validates the reasonableness and specificity of our predictions. Additionally, we employ instruction tuning to enable smaller LLMs to achieve competitive performance, supporting practical deployment.",
      "arxiv_url": "https://arxiv.org/abs/2506.02713",
      "pdf_url": "https://arxiv.org/pdf/2506.02713",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b505b7f8de49294ed30838f49e4500bb58fc596d",
      "title": "RuleEdit: Towards Rule-Level Knowledge Generalization to Mitigate Over-Editing in Large Language Models",
      "authors": [
        "Bihan Zhou",
        "Haopeng Ren",
        "Li Yuan",
        "Yi Cai",
        "Liuwen Cao",
        "Zikun Deng"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/b505b7f8de49294ed30838f49e4500bb58fc596d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b547dfd468064b63349e71fcdd5970db099f1208",
      "title": "NOVA: An Iterative Planning Framework for Enhancing Scientific Innovation with Large Language Models",
      "authors": [
        "Xiang Hu",
        "Hongyu Fu",
        "Jinge Wang",
        "Yifeng Wang",
        "Zhikun Li",
        "Renjun Xu",
        "Yu Lu",
        "Yaochu Jin",
        "Lili Pan",
        "Zhenzhong Lan"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/b547dfd468064b63349e71fcdd5970db099f1208",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b56e5922874ef61ec556dceb55ceb9078561aba5",
      "title": "BI-Bench : A Comprehensive Benchmark Dataset and Unsupervised Evaluation for BI Systems",
      "authors": [
        "Ankush Gupta",
        "Aniya Aggarwal",
        "Shivangi Bithel",
        "Arvind Agarwal"
      ],
      "abstract": "A comprehensive benchmark is crucial for evaluating automated Business Intelligence (BI) systems and their real-world effectiveness. We propose BI-Bench , a holistic, end-to-end benchmarking framework that assesses BI systems based on the quality, relevance, and depth of insights. It categorizes queries into descriptive, diagnostic, predictive, and prescriptive types, aligning with practical BI needs. Our fully automated approach enables custom benchmark generation tailored to specific datasets. Additionally, we introduce an automated evaluation mechanism within BI-Bench that removes reliance on strict ground truth, ensuring scalable and adaptable assessments. By addressing key limitations, it offers a flexible and robust, user-centered methodology for advancing next-generation BI systems.",
      "arxiv_url": "https://www.semanticscholar.org/paper/b56e5922874ef61ec556dceb55ceb9078561aba5",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b590a26956f3057a348f180731ece3e44b30c5a9",
      "title": "WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data",
      "authors": [
        "Jingtan Wang",
        "Xinyang Lu",
        "Zitong Zhao",
        "Zhongxiang Dai",
        "Chuan-Sheng Foo",
        "See-Kiong Ng",
        "K. H. Low"
      ],
      "abstract": "The impressive performances of large language models (LLMs) and their immense potential for commercialization have given rise to serious concerns over the intellectual property (IP) of their training data. In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs. To this end, it is imperative to be able to (a) identify the data provider who contributed to the generation of a synthetic text by an LLM (source attribution) and (b) verify whether the text data from a data provider has been used to train an LLM (data provenance). In this paper, we show that both problems can be solved by watermarking, i.e., by enabling an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s). We identify the key properties of such watermarking frameworks (e.g., source attribution accuracy, robustness against adversaries), and propose a WAtermarking for Source Attribution (WASA) framework that satisfies these key properties due to our algorithmic designs. Our WASA framework enables an LLM to learn an accurate mapping from the texts of different data providers to their corresponding unique watermarks, which sets the foundation for effective source attribution (and hence data provenance). Extensive empirical evaluations show that our WASA framework achieves effective source attribution and data provenance.",
      "arxiv_url": "https://www.semanticscholar.org/paper/b590a26956f3057a348f180731ece3e44b30c5a9",
      "pdf_url": "https://arxiv.org/pdf/2310.00646",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.14269",
      "title": "DARS: Dynamic Action Re-Sampling to Enhance Coding Agent Performance by Adaptive Tree Traversal",
      "authors": [
        "Vaibhav Aggarwal",
        "Ojasv Kamal",
        "Abhinav Japesh",
        "Zhijing Jin",
        "Bernhard Schölkopf"
      ],
      "abstract": "Large Language Models (LLMs) have revolutionized various domains, including natural language processing, data analysis, and software development, by enabling automation. In software engineering, LLM-powered coding agents have garnered significant attention due to their potential to automate complex development tasks, assist in debugging, and enhance productivity. However, existing approaches often struggle with sub-optimal decision-making, requiring either extensive manual intervention or inefficient compute scaling strategies. To improve coding agent performance, we present Dynamic Action Re-Sampling (DARS), a novel inference time compute scaling approach for coding agents, that is faster and more effective at recovering from sub-optimal decisions compared to baselines. While traditional agents either follow linear trajectories or rely on random sampling for scaling compute, our approach DARS works by branching out a trajectory at certain key decision points by taking an alternative action given the history of the trajectory and execution feedback of the previous attempt from that point. We evaluate our approach on SWE-Bench Lite benchmark, demonstrating that this scaling strategy achieves a pass@k score of 55% with Claude 3.5 Sonnet V2. Our framework achieves a pass@1 rate of 47%, outperforming state-of-the-art (SOTA) open-source frameworks.",
      "arxiv_url": "https://arxiv.org/abs/2503.14269",
      "pdf_url": "https://arxiv.org/pdf/2503.14269",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04369",
      "title": "Lost in Literalism: How Supervised Training Shapes Translationese in LLMs",
      "authors": [
        "Yafu Li",
        "Ronghao Zhang",
        "Zhilin Wang",
        "Huajian Zhang",
        "Leyang Cui",
        "Yongjing Yin",
        "Tong Xiao",
        "Yue Zhang"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable success in machine translation, demonstrating impressive performance across diverse languages. However, translationese, characterized by overly literal and unnatural translations, remains a persistent challenge in LLM-based translation systems. Despite their pre-training on vast corpora of natural utterances, LLMs exhibit translationese errors and generate unexpected unnatural translations, stemming from biases introduced during supervised fine-tuning (SFT). In this work, we systematically evaluate the prevalence of translationese in LLM-generated translations and investigate its roots during supervised training. We introduce methods to mitigate these biases, including polishing golden references and filtering unnatural training instances. Empirical evaluations demonstrate that these approaches significantly reduce translationese while improving translation naturalness, validated by human evaluations and automatic metrics. Our findings highlight the need for training-aware adjustments to optimize LLM translation outputs, paving the way for more fluent and target-language-consistent translations. We release the data and code at https://github.com/yafuly/LLM_Translationese.",
      "arxiv_url": "https://arxiv.org/abs/2503.04369",
      "pdf_url": "https://arxiv.org/pdf/2503.04369",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12852",
      "title": "MVL-SIB: A Massively Multilingual Vision-Language Benchmark for Cross-Modal Topical Matching",
      "authors": [
        "Fabian David Schmidt",
        "Florian Schneider",
        "Chris Biemann",
        "Goran Glavavs"
      ],
      "abstract": "Existing multilingual vision-language (VL) benchmarks often only cover a handful of languages. Consequently, evaluations of large vision-language models (LVLMs) predominantly target high-resource languages, underscoring the need for evaluation data for low-resource languages. To address this limitation, we introduce MVL-SIB, a massively multilingual vision-language benchmark that evaluates both cross-modal and text-only topical matching across 205 languages -- over 100 more than the most multilingual existing VL benchmarks encompass. We then benchmark a range of of open-weight LVLMs together with GPT-4o(-mini) on MVL-SIB. Our results reveal that LVLMs struggle in cross-modal topic matching in lower-resource languages, performing no better than chance on languages like N'Koo. Our analysis further reveals that VL support in LVLMs declines disproportionately relative to textual support for lower-resource languages, as evidenced by comparison of cross-modal and text-only topical matching performance. We further observe that open-weight LVLMs do not benefit from representing a topic with more than one image, suggesting that these models are not yet fully effective at handling multi-image tasks. By correlating performance on MVL-SIB with other multilingual VL benchmarks, we highlight that MVL-SIB serves as a comprehensive probe of multilingual VL understanding in LVLMs.",
      "arxiv_url": "https://arxiv.org/abs/2502.12852",
      "pdf_url": "https://arxiv.org/pdf/2502.12852",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18366",
      "title": "Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems",
      "authors": [
        "Hansa Meghwani",
        "Amit Agarwal",
        "Priyaranjan Pattnayak",
        "Hitesh Laxmichand Patel",
        "Srikant Panda"
      ],
      "abstract": "Enterprise search systems often struggle to retrieve accurate, domain-specific information due to semantic mismatches and overlapping terminologies. These issues can degrade the performance of downstream applications such as knowledge management, customer support, and retrieval-augmented generation agents. To address this challenge, we propose a scalable hard-negative mining framework tailored specifically for domain-specific enterprise data. Our approach dynamically selects semantically challenging but contextually irrelevant documents to enhance deployed re-ranking models. Our method integrates diverse embedding models, performs dimensionality reduction, and uniquely selects hard negatives, ensuring computational efficiency and semantic precision. Evaluation on our proprietary enterprise corpus (cloud services domain) demonstrates substantial improvements of 15\\% in MRR@3 and 19\\% in MRR@10 compared to state-of-the-art baselines and other negative sampling techniques. Further validation on public domain-specific datasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability and readiness for real-world applications.",
      "arxiv_url": "https://arxiv.org/abs/2505.18366",
      "pdf_url": "https://arxiv.org/pdf/2505.18366",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-05-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23224",
      "title": "MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration",
      "authors": [
        "Zhitao He",
        "Sandeep Polisetty",
        "Zhiyuan Fan",
        "Yuchen Huang",
        "Shujin Wu",
        "Yi R. Fung"
      ],
      "abstract": "In recent years, multimodal large language models (MLLMs) have made significant progress but continue to face inherent challenges in multimodal reasoning, which requires multi-level (e.g., perception, reasoning) and multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior work on estimating model confidence tends to focus on the overall response for training and calibration, but fails to assess confidence in each reasoning step, leading to undesirable hallucination snowballing. In this work, we present MMBoundary, a novel framework that advances the knowledge boundary awareness of MLLMs through reasoning step confidence calibration. To achieve this, we propose to incorporate complementary textual and cross-modal self-rewarding signals to estimate confidence at each step of the MLLM reasoning process. In addition to supervised fine-tuning MLLM on this set of self-rewarded confidence estimation signal for initial confidence expression warm-up, we introduce a reinforcement learning stage with multiple reward functions for further aligning model knowledge and calibrating confidence at each reasoning step, enhancing reasoning chain self-correction. Empirical results show that MMBoundary significantly outperforms existing methods across diverse domain datasets and metrics, achieving an average of 7.5% reduction in multimodal confidence calibration errors and up to 8.3% improvement in task performance.",
      "arxiv_url": "https://arxiv.org/abs/2505.23224",
      "pdf_url": "https://arxiv.org/pdf/2505.23224",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b69d0f97bbd4126f3f580908684e8efdc4b4532e",
      "title": "CrisisTS: Coupling Social Media Textual Data and Meteorological Time Series for Urgency Classification",
      "authors": [
        "Romain Meunier",
        "Farah Benamara",
        "Véronique Moriceau",
        "Zhongzheng Qiao",
        "Savitha Ramasamy"
      ],
      "abstract": "This paper proposes C RISIS TS, the first multimodal and multilingual dataset for urgency classification composed of benchmark crisis datasets that have been mapped with open source geocoded meteorological time series data. This mapping is based on a simple and effective strategy that allows for temporal and location alignment even in the absence of location mention in the text. A set of multimodal experiments have been conducted relying on transformers and LLMs to improve overall performances while ensuring model generalizability. Our results show that modality fusion out-performs text-only models.",
      "arxiv_url": "https://www.semanticscholar.org/paper/b69d0f97bbd4126f3f580908684e8efdc4b4532e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.15451",
      "title": "STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity Extraction in Chinese Hate Speech Detection",
      "authors": [
        "Zewen Bai",
        "Yuanyuan Sun",
        "Shengdi Yin",
        "Junyu Lu",
        "Jingjie Zeng",
        "Hao Zhu",
        "Liang Yang",
        "Hongfei Lin"
      ],
      "abstract": "The proliferation of hate speech has caused significant harm to society. The intensity and directionality of hate are closely tied to the target and argument it is associated with. However, research on hate speech detection in Chinese has lagged behind, and existing datasets lack span-level fine-grained annotations. Furthermore, the lack of research on Chinese hateful slang poses a significant challenge. In this paper, we provide a solution for fine-grained detection of Chinese hate speech. First, we construct a dataset containing Target-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first span-level Chinese hate speech dataset. Secondly, we evaluate the span-level hate speech detection performance of existing models using STATE ToxiCN. Finally, we conduct the first study on Chinese hateful slang and evaluate the ability of LLMs to detect such expressions. Our work contributes valuable resources and insights to advance span-level hate speech detection in Chinese.",
      "arxiv_url": "https://arxiv.org/abs/2501.15451",
      "pdf_url": "https://arxiv.org/pdf/2501.15451",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13842",
      "title": "Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking",
      "authors": [
        "Yilong Chen",
        "Junyuan Shang",
        "Zhenyu Zhang",
        "Yanxi Xie",
        "Jiawei Sheng",
        "Tingwen Liu",
        "Shuohuan Wang",
        "Yu Sun",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "abstract": "Large language models (LLMs) face inherent performance bottlenecks under parameter constraints, particularly in processing critical tokens that demand complex reasoning. Empirical analysis reveals challenging tokens induce abrupt gradient spikes across layers, exposing architectural stress points in standard Transformers. Building on this insight, we propose Inner Thinking Transformer (ITT), which reimagines layer computations as implicit thinking steps. ITT dynamically allocates computation through Adaptive Token Routing, iteratively refines representations via Residual Thinking Connections, and distinguishes reasoning phases using Thinking Step Encoding. ITT enables deeper processing of critical tokens without parameter expansion. Evaluations across 162M-466M parameter models show ITT achieves 96.5\\% performance of a 466M Transformer using only 162M parameters, reduces training data by 43.2\\%, and outperforms Transformer/Loop variants in 11 benchmarks. By enabling elastic computation allocation during inference, ITT balances performance and efficiency through architecture-aware optimization of implicit thinking pathways.",
      "arxiv_url": "https://arxiv.org/abs/2502.13842",
      "pdf_url": "https://arxiv.org/pdf/2502.13842",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b771cedbbcb7636ead3d983f059c643186554e29",
      "title": "ProtoLens: Advancing Prototype Learning for Fine-Grained Interpretability in Text Classification",
      "authors": [
        "Bowen Wei",
        "Ziwei Zhu"
      ],
      "abstract": "Deep neural networks have achieved remarkable performance in various text-based tasks but often lack interpretability, making them less suitable for applications where transparency is critical. To address this, we propose ProtoLens, a novel prototype-based model that provides fine-grained, sub-sentence level interpretability for text classification. ProtoLens uses a Prototype-aware Span Extraction module to identify relevant text spans associated with learned prototypes and a Prototype Alignment mechanism to ensure prototypes are semantically meaningful throughout training. By aligning the prototype embeddings with human-understandable examples, ProtoLens provides interpretable predictions while maintaining competitive accuracy. Extensive experiments demonstrate that ProtoLens outperforms both prototype-based and non-interpretable baselines on multiple text classification benchmarks. Code and data are available at https://anonymous.4open.science/r/ProtoLens-CE0B/ .",
      "arxiv_url": "https://www.semanticscholar.org/paper/b771cedbbcb7636ead3d983f059c643186554e29",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.23111",
      "title": "FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes",
      "authors": [
        "J. Nawale",
        "Mohammed Safi Ur Rahman Khan",
        "D. Janani",
        "Mansi Gupta",
        "Danish Pruthi",
        "Mitesh M. Khapra"
      ],
      "abstract": "Existing studies on fairness are largely Western-focused, making them inadequate for culturally diverse countries such as India. To address this gap, we introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to evaluate fairness of LLMs across 85 identity groups encompassing diverse castes, religions, regions, and tribes. We first consult domain experts to curate over 1,800 socio-cultural topics spanning behaviors and situations, where biases and stereotypes are likely to emerge. Grounded in these topics, we generate and manually validate 20,000 real-world scenario templates to probe LLMs for fairness. We structure these templates into three evaluation tasks: plausibility, judgment, and generation. Our evaluation of 14 popular LLMs on these tasks reveals strong negative biases against marginalized identities, with models frequently reinforcing common stereotypes. Additionally, we find that models struggle to mitigate bias even when explicitly asked to rationalize their decision. Our evaluation provides evidence of both allocative and representational harms that current LLMs could cause towards Indian identities, calling for a more cautious usage in practical applications. We release INDIC-BIAS as an open-source benchmark to advance research on benchmarking and mitigating biases and stereotypes in the Indian context.",
      "arxiv_url": "https://arxiv.org/abs/2506.23111",
      "pdf_url": "https://arxiv.org/pdf/2506.23111",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.08441",
      "title": "Better Embeddings with Coupled Adam",
      "authors": [
        "Felix Stollenwerk",
        "Tobias Stollenwerk"
      ],
      "abstract": "Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets.",
      "arxiv_url": "https://arxiv.org/abs/2502.08441",
      "pdf_url": "https://arxiv.org/pdf/2502.08441",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.04577",
      "title": "Position-aware Automatic Circuit Discovery",
      "authors": [
        "Tal Haklay",
        "Hadas Orgad",
        "David Bau",
        "Aaron Mueller",
        "Yonatan Belinkov"
      ],
      "abstract": "A widely used strategy to discover and understand language model mechanisms is circuit analysis. A circuit is a minimal subgraph of a model's computation graph that executes a specific task. We identify a gap in existing circuit discovery methods: they assume circuits are position-invariant, treating model components as equally relevant across input positions. This limits their ability to capture cross-positional interactions or mechanisms that vary across positions. To address this gap, we propose two improvements to incorporate positionality into circuits, even on tasks containing variable-length examples. First, we extend edge attribution patching, a gradient-based method for circuit discovery, to differentiate between token positions. Second, we introduce the concept of a dataset schema, which defines token spans with similar semantics across examples, enabling position-aware circuit discovery in datasets with variable length examples. We additionally develop an automated pipeline for schema generation and application using large language models. Our approach enables fully automated discovery of position-sensitive circuits, yielding better trade-offs between circuit size and faithfulness compared to prior work.",
      "arxiv_url": "https://arxiv.org/abs/2502.04577",
      "pdf_url": "https://arxiv.org/pdf/2502.04577",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10480",
      "title": "World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning",
      "authors": [
        "Siyin Wang",
        "Zhaoye Fei",
        "Qinyuan Cheng",
        "Shiduo Zhang",
        "Panpan Cai",
        "Jinlan Fu",
        "Xipeng Qiu"
      ],
      "abstract": "Recent advances in large vision-language models (LVLMs) have shown promise for embodied task planning, yet they struggle with fundamental challenges like dependency constraints and efficiency. Existing approaches either solely optimize action selection or leverage world models during inference, overlooking the benefits of learning to model the world as a way to enhance planning capabilities. We propose Dual Preference Optimization (D$^2$PO), a new learning framework that jointly optimizes state prediction and action selection through preference learning, enabling LVLMs to understand environment dynamics for better planning. To automatically collect trajectories and stepwise preference data without human annotation, we introduce a tree search mechanism for extensive exploration via trial-and-error. Extensive experiments on VoTa-Bench demonstrate that our D$^2$PO-based method significantly outperforms existing methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and LLaMA-3.2 (11B), achieving superior task success rates with more efficient execution paths.",
      "arxiv_url": "https://arxiv.org/abs/2503.10480",
      "pdf_url": "https://arxiv.org/pdf/2503.10480",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.01705",
      "title": "The Essence of Contextual Understanding in Theory of Mind: A Study on Question Answering with Story Characters",
      "authors": [
        "Chulun Zhou",
        "Qiujing Wang",
        "Mo Yu",
        "Xiaoqian Yue",
        "Rui Lu",
        "Jiangnan Li",
        "Yifan Zhou",
        "Shunchi Zhang",
        "Jie Zhou",
        "Wai Lam"
      ],
      "abstract": "Theory-of-Mind (ToM) is a fundamental psychological capability that allows humans to understand and interpret the mental states of others. Humans infer others' thoughts by integrating causal cues and indirect clues from broad contextual information, often derived from past interactions. In other words, human ToM heavily relies on the understanding about the backgrounds and life stories of others. Unfortunately, this aspect is largely overlooked in existing benchmarks for evaluating machines' ToM capabilities, due to their usage of short narratives without global context, especially personal background of characters. In this paper, we verify the importance of comprehensive contextual understanding about personal backgrounds in ToM and assess the performance of LLMs in such complex scenarios. To achieve this, we introduce CharToM benchmark, comprising 1,035 ToM questions based on characters from classic novels. Our human study reveals a significant disparity in performance: the same group of educated participants performs dramatically better when they have read the novels compared to when they have not. In parallel, our experiments on state-of-the-art LLMs, including the very recent o1 and DeepSeek-R1 models, show that LLMs still perform notably worse than humans, despite that they have seen these stories during pre-training. This highlights the limitations of current LLMs in capturing the nuanced contextual information required for ToM reasoning.",
      "arxiv_url": "https://arxiv.org/abs/2501.01705",
      "pdf_url": "https://arxiv.org/pdf/2501.01705",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16169",
      "title": "Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs under Noisy Observations",
      "authors": [
        "Chunyang Li",
        "Weiqi Wang",
        "Tianshi ZHENG",
        "Yangqiu Song"
      ],
      "abstract": "Inductive reasoning, a cornerstone of human cognition, enables generalization from limited data but hasn't yet been fully achieved by large language models (LLMs). While modern LLMs excel at reasoning tasks, their ability to maintain stable and consistent rule abstraction under imperfect observations remains underexplored. To fill this gap, in this work, we introduce Robust Rule Induction, a task that evaluates LLMs' capability in inferring rules from data that are fused with noisy examples. To address this task, we further propose Sample-steered Rule Refinement (SRR), a method enhancing reasoning stability via observation diversification and execution-guided feedback. Experiments across arithmetic, cryptography, and list functions reveal: (1) SRR outperforms other methods with minimal performance degradation under noise; (2) Despite slight accuracy variation, LLMs exhibit instability under noise (e.g., 0% accuracy change with only 70% consistent score); (3) Counterfactual task gaps highlight LLMs' reliance on memorized patterns over genuine abstraction. Our findings challenge LLMs' reasoning robustness, revealing susceptibility to hypothesis drift and pattern overfitting, while providing empirical evidence critical for developing human-like inductive systems. Code and data are available at https://github.com/HKUST-KnowComp/Robust-Rule-Induction.",
      "arxiv_url": "https://arxiv.org/abs/2502.16169",
      "pdf_url": "https://arxiv.org/pdf/2502.16169",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.16787",
      "title": "Revisiting LoRA through the Lens of Parameter Redundancy: Spectral Encoding Helps",
      "authors": [
        "Jiashun Cheng",
        "Aochuan Chen",
        "Nuo Chen",
        "Ziqi Gao",
        "Yuhan Li",
        "Jia Li",
        "F. Tsung"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) has emerged as a prominent technique for fine-tuning large foundation models. Despite its successes, the substantial parameter redundancy, which limits the capacity and efficiency of LoRA, has been recognized as a bottleneck. In this work, we systematically investigate the impact of redundancy in fine-tuning LoRA and reveal that reducing density redundancy does not degrade expressiveness. Based on this insight, we introduce \\underline{S}pectral-\\underline{e}ncoding \\underline{L}ow-\\underline{R}ank \\underline{A}daptation (SeLoRA), which harnesses the robust expressiveness of spectral bases to re-parameterize LoRA from a sparse spectral subspace. Designed with simplicity, SeLoRA enables seamless integration with various LoRA variants for performance boosting, serving as a scalable plug-and-play framework. Extensive experiments substantiate that SeLoRA achieves greater efficiency with fewer parameters, delivering superior performance enhancements over strong baselines on various downstream tasks, including commonsense reasoning, math reasoning, and code generation.",
      "arxiv_url": "https://arxiv.org/abs/2506.16787",
      "pdf_url": "https://arxiv.org/pdf/2506.16787",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05227",
      "title": "Improving Low-Resource Morphological Inflection via Self-Supervised Objectives",
      "authors": [
        "Adam Wiemerslage",
        "K. Wense"
      ],
      "abstract": "Self-supervised objectives have driven major advances in NLP by leveraging large-scale unlabeled data, but such resources are scarce for many of the world's languages. Surprisingly, they have not been explored much for character-level tasks, where smaller amounts of data have the potential to be beneficial. We investigate the effectiveness of self-supervised auxiliary tasks for morphological inflection -- a character-level task highly relevant for language documentation -- in extremely low-resource settings, training encoder-decoder transformers for 19 languages and 13 auxiliary objectives. Autoencoding yields the best performance when unlabeled data is very limited, while character masked language modeling (CMLM) becomes more effective as data availability increases. Though objectives with stronger inductive biases influence model predictions intuitively, they rarely outperform standard CMLM. However, sampling masks based on known morpheme boundaries consistently improves performance, highlighting a promising direction for low-resource morphological modeling.",
      "arxiv_url": "https://arxiv.org/abs/2506.05227",
      "pdf_url": "https://arxiv.org/pdf/2506.05227",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.15821",
      "title": "Just Put a Human in the Loop? Investigating LLM-Assisted Annotation for Subjective Tasks",
      "authors": [
        "Hope Schroeder",
        "Deb Roy",
        "Jad Kabbara"
      ],
      "abstract": "LLM use in annotation is becoming widespread, and given LLMs'overall promising performance and speed, simply\"reviewing\"LLM annotations in interpretive tasks can be tempting. In subjective annotation tasks with multiple plausible answers, reviewing LLM outputs can change the label distribution, impacting both the evaluation of LLM performance, and analysis using these labels in a social science task downstream. We conducted a pre-registered experiment with 410 unique annotators and over 7,000 annotations testing three AI assistance conditions against controls, using two models, and two datasets. We find that presenting crowdworkers with LLM-generated annotation suggestions did not make them faster, but did improve their self-reported confidence in the task. More importantly, annotators strongly took the LLM suggestions, significantly changing the label distribution compared to the baseline. When these labels created with LLM assistance are used to evaluate LLM performance, reported model performance significantly increases. We believe our work underlines the importance of understanding the impact of LLM-assisted annotation on subjective, qualitative tasks, on the creation of gold data for training and testing, and on the evaluation of NLP systems on subjective tasks.",
      "arxiv_url": "https://arxiv.org/abs/2507.15821",
      "pdf_url": "https://arxiv.org/pdf/2507.15821",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.15351",
      "title": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models",
      "authors": [
        "I-Fan Lin",
        "Faegheh Hasibi",
        "Suzan Verberne"
      ],
      "abstract": "In this paper, we propose Selection and Pooling with Large Language Models (SPILL), an intuitive and domain-adaptive method for intent clustering without fine-tuning. Existing embeddings-based clustering methods rely on a few labeled examples or unsupervised fine-tuning to optimize results for each new dataset, which makes them less generalizable to multiple datasets. Our goal is to make these existing embedders more generalizable to new domain datasets without further fine-tuning. Inspired by our theoretical derivation and simulation results on the effectiveness of sampling and pooling techniques, we view the clustering task as a small-scale selection problem. A good solution to this problem is associated with better clustering performance. Accordingly, we propose a two-stage approach: First, for each utterance (referred to as the seed), we derive its embedding using an existing embedder. Then, we apply a distance metric to select a pool of candidates close to the seed. Because the embedder is not optimized for new datasets, in the second stage, we use an LLM to further select utterances from these candidates that share the same intent as the seed. Finally, we pool these selected candidates with the seed to derive a refined embedding for the seed. We found that our method generally outperforms directly using an embedder, and it achieves comparable results to other state-of-the-art studies, even those that use much larger models and require fine-tuning, showing its strength and efficiency. Our results indicate that our method enables existing embedders to be further improved without additional fine-tuning, making them more adaptable to new domain datasets. Additionally, viewing the clustering task as a small-scale selection problem gives the potential of using LLMs to customize clustering tasks according to the user's goals.",
      "arxiv_url": "https://arxiv.org/abs/2503.15351",
      "pdf_url": "https://arxiv.org/pdf/2503.15351",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18859",
      "title": "Writing Like the Best: Exemplar-Based Expository Text Generation",
      "authors": [
        "Yuxiang Liu",
        "Kevin Chen-Chuan Chang"
      ],
      "abstract": "We introduce the Exemplar-Based Expository Text Generation task, aiming to generate an expository text on a new topic using an exemplar on a similar topic. Current methods fall short due to their reliance on extensive exemplar data, difficulty in adapting topic-specific content, and issues with long-text coherence. To address these challenges, we propose the concept of Adaptive Imitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA leverages large language models (LLMs) for effective adaptive imitation through a fine-grained plan-then-adapt process. RePA also enables recurrent segment-by-segment imitation, supported by two memory structures that enhance input clarity and output coherence. We also develop task-specific evaluation metrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as evaluators. Experimental results across our collected three diverse datasets demonstrate that RePA surpasses existing baselines in producing factual, consistent, and relevant texts for this task.",
      "arxiv_url": "https://arxiv.org/abs/2505.18859",
      "pdf_url": "https://arxiv.org/pdf/2505.18859",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b90675d31be1d603ad322624e90ad1e3ad9a5375",
      "title": "RL-Guider: Leveraging Historical Decisions and Feedback for Drug Editing with Large Language Models",
      "authors": [
        "Xufeng Liu",
        "Yixuan Ding",
        "Jingxiang Qu",
        "Yichi Zhang",
        "Wenhan Gao",
        "Yi Liu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/b90675d31be1d603ad322624e90ad1e3ad9a5375",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b929323b914aa177d466296cf1b5578c2e9c9955",
      "title": "Less Mature is More Adaptable for Sentence-level Language Modeling",
      "authors": [
        "Abhilasha Sancheti",
        "David Dale",
        "Artyom Kozhevnikov",
        "Maha Elbayad"
      ],
      "abstract": "This work investigates sentence-level models ( i.e., models that operate at the sentence-level) to study how sentence representations from various encoders influence downstream task performance, and which syntactic, semantic, and discourse-level properties are essential for strong performance. Our experiments encompass encoders with diverse training regimes and pretraining domains, as well as various pooling strategies applied to multi-sentence input tasks (including sentence ordering, sentiment classification, and natural language inference) requiring coarse-to-fine-grained reasoning. We find that “less mature” representations ( e.g., mean-pooled representations from BERT’s first or last layer, or representations from encoders with limited fine-tuning) exhibit greater generalizability and adaptability to downstream tasks compared to representations from extensively fine-tuned models ( e.g., , SBERT or Sim-CSE). These findings are consistent across different pretraining seed initializations for BERT. Our probing analysis reveals that syntactic and discourse-level properties are stronger indicators of downstream performance than MTEB scores or decodability. Furthermore, the data and time efficiency of sentence-level models, often outperforming token-level models, under-scores their potential for future research.",
      "arxiv_url": "https://www.semanticscholar.org/paper/b929323b914aa177d466296cf1b5578c2e9c9955",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23006",
      "title": "A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs",
      "authors": [
        "Chiwan Park",
        "Wonjun Jang",
        "Daeryong Kim",
        "Aelim Ahn",
        "Kichang Yang",
        "Woosung Hwang",
        "Jihyeon Roh",
        "Hyerin Park",
        "Hyosun Wang",
        "Min Seok Kim",
        "Jihoon Kang"
      ],
      "abstract": "The advancement of Large Language Models (LLMs) has led to significant improvements in various service domains, including search, recommendation, and chatbot applications. However, applying state-of-the-art (SOTA) research to industrial settings presents challenges, as it requires maintaining flexible conversational abilities while also strictly complying with service-specific constraints. This can be seen as two conflicting requirements due to the probabilistic nature of LLMs. In this paper, we propose our approach to addressing this challenge and detail the strategies we employed to overcome their inherent limitations in real-world applications. We conduct a practical case study of a conversational agent designed for the e-commerce domain, detailing our implementation workflow and optimizations. Our findings provide insights into bridging the gap between academic research and real-world application, introducing a framework for developing scalable, controllable, and reliable AI-driven agents.",
      "arxiv_url": "https://arxiv.org/abs/2505.23006",
      "pdf_url": "https://arxiv.org/pdf/2505.23006",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.11420",
      "title": "Reinforcing Compositional Retrieval: Retrieving Step-by-Step for Composing Informative Contexts",
      "authors": [
        "Quanyu Long",
        "Jianda Chen",
        "Zhengyuan Liu",
        "Nancy F. Chen",
        "Wenya Wang",
        "Sinno Jialin Pan"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous tasks, yet they often rely on external context to handle complex tasks. While retrieval-augmented frameworks traditionally focus on selecting top-ranked documents in a single pass, many real-world scenarios demand compositional retrieval, where multiple sources must be combined in a coordinated manner. In this work, we propose a tri-encoder sequential retriever that models this process as a Markov Decision Process (MDP), decomposing the probability of retrieving a set of elements into a sequence of conditional probabilities and allowing each retrieval step to be conditioned on previously selected examples. We train the retriever in two stages: first, we efficiently construct supervised sequential data for initial policy training; we then refine the policy to align with the LLM's preferences using a reward grounded in the structural correspondence of generated programs. Experimental results show that our method consistently and significantly outperforms baselines, underscoring the importance of explicitly modeling inter-example dependencies. These findings highlight the potential of compositional retrieval for tasks requiring multiple pieces of evidence or examples.",
      "arxiv_url": "https://arxiv.org/abs/2504.11420",
      "pdf_url": "https://arxiv.org/pdf/2504.11420",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.17147",
      "title": "MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming",
      "authors": [
        "Weiyang Guo",
        "Jing Li",
        "Wenya Wang",
        "Yu Li",
        "Daojing He",
        "Jun Yu",
        "Min Zhang"
      ],
      "abstract": "The proliferation of jailbreak attacks against large language models (LLMs) highlights the need for robust security measures. However, in multi-round dialogues, malicious intentions may be hidden in interactions, leading LLMs to be more prone to produce harmful responses. In this paper, we propose the \\textbf{M}ulti-\\textbf{T}urn \\textbf{S}afety \\textbf{A}lignment (\\ourapproach) framework, to address the challenge of securing LLMs in multi-round interactions. It consists of two stages: In the thought-guided attack learning stage, the red-team model learns about thought-guided multi-round jailbreak attacks to generate adversarial prompts. In the adversarial iterative optimization stage, the red-team model and the target model continuously improve their respective capabilities in interaction. Furthermore, we introduce a multi-turn reinforcement learning algorithm based on future rewards to enhance the robustness of safety alignment. Experimental results show that the red-team model exhibits state-of-the-art attack capabilities, while the target model significantly improves its performance on safety benchmarks.",
      "arxiv_url": "https://arxiv.org/abs/2505.17147",
      "pdf_url": "https://arxiv.org/pdf/2505.17147",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01114",
      "title": "Reconsidering LLM Uncertainty Estimation Methods in the Wild",
      "authors": [
        "Yavuz Faruk Bakman",
        "D. Yaldiz",
        "Sungmin Kang",
        "Tuo Zhang",
        "Baturalp Buyukates",
        "S. Avestimehr",
        "Sai Praneeth Karimireddy"
      ],
      "abstract": "Large Language Model (LLM) Uncertainty Estimation (UE) methods have become a crucial tool for detecting hallucinations in recent years. While numerous UE methods have been proposed, most existing studies evaluate them in isolated short-form QA settings using threshold-independent metrics such as AUROC or PRR. However, real-world deployment of UE methods introduces several challenges. In this work, we systematically examine four key aspects of deploying UE methods in practical settings. Specifically, we assess (1) the sensitivity of UE methods to decision threshold selection, (2) their robustness to query transformations such as typos, adversarial prompts, and prior chat history, (3) their applicability to long-form generation, and (4) strategies for handling multiple UE scores for a single query. Our evaluations on 19 UE methods reveal that most of them are highly sensitive to threshold selection when there is a distribution shift in the calibration dataset. While these methods generally exhibit robustness against previous chat history and typos, they are significantly vulnerable to adversarial prompts. Additionally, while existing UE methods can be adapted for long-form generation through various strategies, there remains considerable room for improvement. Lastly, ensembling multiple UE scores at test time provides a notable performance boost, which highlights its potential as a practical improvement strategy. Code is available at: https://github.com/duygunuryldz/uncertainty_in_the_wild.",
      "arxiv_url": "https://arxiv.org/abs/2506.01114",
      "pdf_url": "https://arxiv.org/pdf/2506.01114",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01796",
      "title": "Read it in Two Steps: Translating Extremely Low-Resource Languages with Code-Augmented Grammar Books",
      "authors": [
        "Chen Zhang",
        "Jiuheng Lin",
        "Xiao Liu",
        "Zekai Zhang",
        "Yansong Feng"
      ],
      "abstract": "While large language models (LLMs) have shown promise in translating extremely low-resource languages using resources like dictionaries, the effectiveness of grammar books remains debated. This paper investigates the role of grammar books in translating extremely low-resource languages by decomposing it into two key steps: grammar rule retrieval and application. To facilitate the study, we introduce ZhuangRules, a modularized dataset of grammar rules and their corresponding test sentences. Our analysis reveals that rule retrieval constitutes a primary bottleneck in grammar-based translation. Moreover, although LLMs can apply simple rules for translation when explicitly provided, they encounter difficulties in handling more complex rules. To address these challenges, we propose representing grammar rules as code functions, considering their similarities in structure and the benefit of code in facilitating LLM reasoning. Our experiments show that using code rules significantly boosts both rule retrieval and application, ultimately resulting in a 13.1% BLEU improvement in translation.",
      "arxiv_url": "https://arxiv.org/abs/2506.01796",
      "pdf_url": "https://arxiv.org/pdf/2506.01796",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22661",
      "title": "GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning",
      "authors": [
        "Qingchen Yu",
        "Zifan Zheng",
        "Ding Chen",
        "Simin Niu",
        "Bo Tang",
        "Feiyu Xiong",
        "Zhiyu Li"
      ],
      "abstract": "The evaluation of large language models (LLMs) has traditionally relied on static benchmarks, a paradigm that poses two major limitations: (1) predefined test sets lack adaptability to diverse application domains, and (2) standardized evaluation protocols often fail to capture fine-grained assessments of domain-specific knowledge and contextual reasoning abilities. To overcome these challenges, we propose GuessArena, an adaptive evaluation framework grounded in adversarial game-based interactions. Inspired by the interactive structure of the Guess Who I Am? game, our framework seamlessly integrates dynamic domain knowledge modeling with progressive reasoning assessment to improve evaluation fidelity. Empirical studies across five vertical domains-finance, healthcare, manufacturing, information technology, and education-demonstrate that GuessArena effectively distinguishes LLMs in terms of domain knowledge coverage and reasoning chain completeness. Compared to conventional benchmarks, our method provides substantial advantages in interpretability, scalability, and scenario adaptability.",
      "arxiv_url": "https://arxiv.org/abs/2505.22661",
      "pdf_url": "https://arxiv.org/pdf/2505.22661",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24754",
      "title": "Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation",
      "authors": [
        "Yingchaojie Feng",
        "Yiqun Sun",
        "Yandong Sun",
        "Minfeng Zhu",
        "Qiang Huang",
        "Anthony K. H. Tung",
        "Wei Chen"
      ],
      "abstract": "In this work, we investigate an important task named instruction-following text embedding, which generates dynamic text embeddings that adapt to user instructions, highlighting specific attributes of text. Despite recent advancements, existing approaches suffer from significant computational overhead, as they require re-encoding the entire corpus for each new instruction. To address this challenge, we propose GSTransform, a novel instruction-following text embedding framework based on Guided Space Transformation. Our key observation is that instruction-relevant information is inherently encoded in generic embeddings but remains underutilized. Instead of repeatedly encoding the corpus for each instruction, GSTransform is a lightweight transformation mechanism that adapts pre-computed embeddings in real time to align with user instructions, guided by a small amount of text data with instruction-focused label annotation. We conduct extensive experiments on three instruction-awareness downstream tasks across nine real-world datasets, demonstrating that GSTransform improves instruction-following text embedding quality over state-of-the-art methods while achieving dramatic speedups of 6~300x in real-time processing on large-scale datasets. The source code is available at https://github.com/YingchaojieFeng/GSTransform.",
      "arxiv_url": "https://arxiv.org/abs/2505.24754",
      "pdf_url": "https://arxiv.org/pdf/2505.24754",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20277",
      "title": "OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction",
      "authors": [
        "Haonan Zhang",
        "Run Luo",
        "Xiong Liu",
        "Yuchuan Wu",
        "Ting-En Lin",
        "Pengpeng Zeng",
        "Qiang Qu",
        "Feiteng Fang",
        "Min Yang",
        "Lianli Gao",
        "Jingkuan Song",
        "Fei Huang",
        "Yongbin Li"
      ],
      "abstract": "Role-Playing Agents (RPAs), benefiting from large language models, is an emerging interactive AI system that simulates roles or characters with diverse personalities. However, existing methods primarily focus on mimicking dialogues among roles in textual form, neglecting the role's voice traits (e.g., voice style and emotions) as playing a crucial effect in interaction, which tends to be more immersive experiences in realistic scenarios. Towards this goal, we propose OmniCharacter, a first seamless speech-language personality interaction model to achieve immersive RPAs with low latency. Specifically, OmniCharacter enables agents to consistently exhibit role-specific personality traits and vocal traits throughout the interaction, enabling a mixture of speech and language responses. To align the model with speech-language scenarios, we construct a dataset named OmniCharacter-10K, which involves more distinctive characters (20), richly contextualized multi-round dialogue (10K), and dynamic speech response (135K). Experimental results showcase that our method yields better responses in terms of both content and style compared to existing RPAs and mainstream speech-language models, with a response latency as low as 289ms. Code and dataset are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter.",
      "arxiv_url": "https://arxiv.org/abs/2505.20277",
      "pdf_url": "https://arxiv.org/pdf/2505.20277",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "b9e17ca3cfef65884b4d75d42a9c036878d47b87",
      "title": "None of the Above, Less of the Right Parallel Patterns in Human and LLM Performance on Multi-Choice Questions Answering",
      "authors": [
        "Zhi Rui Tam",
        "Cheng-Kuang Wu",
        "Chieh-Yen Lin",
        "Yun-Nung Chen"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/b9e17ca3cfef65884b4d75d42a9c036878d47b87",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14280",
      "title": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts",
      "authors": [
        "Subhajit Chaudhury",
        "Payel Das",
        "Sarathkrishna Swaminathan",
        "Georgios Kollias",
        "Elliot Nelson",
        "Khushbu Pahwa",
        "Tejaswini Pedapati",
        "Igor Melnyk",
        "Matthew Riemer"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have yielded impressive successes on many language tasks. However, efficient processing of long contexts using LLMs remains a significant challenge. We introduce \\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic memory} module while \\textit{holistically attending to} semantically relevant context chunks. The output of \\textit{episodic attention} is then used to reweigh the decoder's self-attention to the stored KV cache of the context during training and generation. When an LLM decoder is trained using \\textbf{EpMAN}, its performance on multiple challenging single-hop long-context recall and question-answering benchmarks is found to be stronger and more robust across the range from 16k to 256k tokens than baseline decoders trained with self-attention, and popular retrieval-augmented generation frameworks.",
      "arxiv_url": "https://arxiv.org/abs/2502.14280",
      "pdf_url": "https://arxiv.org/pdf/2502.14280",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15680",
      "title": "Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training",
      "authors": [
        "Jaydeep Borkar",
        "Matthew Jagielski",
        "Katherine Lee",
        "Niloofar Mireshghallah",
        "David A. Smith",
        "Christopher A. Choquette-Choo"
      ],
      "abstract": "Due to the sensitive nature of personally identifiable information (PII), its owners may have the authority to control its inclusion or request its removal from large-language model (LLM) training. Beyond this, PII may be added or removed from training datasets due to evolving dataset curation techniques, because they were newly scraped for retraining, or because they were included in a new downstream fine-tuning stage. We find that the amount and ease of PII memorization is a dynamic property of a model that evolves throughout training pipelines and depends on commonly altered design choices. We characterize three such novel phenomena: (1) similar-appearing PII seen later in training can elicit memorization of earlier-seen sequences in what we call assisted memorization, and this is a significant factor (in our settings, up to 1/3); (2) adding PII can increase memorization of other PII significantly (in our settings, as much as $\\approx\\!7.5\\times$); and (3) removing PII can lead to other PII being memorized. Model creators should consider these first- and second-order privacy risks when training models to avoid the risk of new PII regurgitation.",
      "arxiv_url": "https://arxiv.org/abs/2502.15680",
      "pdf_url": "https://arxiv.org/pdf/2502.15680",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12829",
      "title": "KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge of Kazakhstan",
      "authors": [
        "Mukhammed Togmanov",
        "Nurdaulet Mukhituly",
        "Diana Turmakhan",
        "Jonibek Mansurov",
        "Maiya Goloburda",
        "Akhmed Sakip",
        "Zhuohan Xie",
        "Yuxia Wang",
        "Bekassyl Syzdykov",
        "Nurkhan Laiyk",
        "Alham Fikri Aji",
        "Ekaterina Kochmar",
        "Preslav Nakov",
        "Fajri Koto"
      ],
      "abstract": "Despite having a population of twenty million, Kazakhstan's culture and language remain underrepresented in the field of natural language processing. Although large language models (LLMs) continue to advance worldwide, progress in Kazakh language has been limited, as seen in the scarcity of dedicated models and benchmark evaluations. To address this gap, we introduce KazMMLU, the first MMLU-style dataset specifically designed for Kazakh language. KazMMLU comprises 23,000 questions that cover various educational levels, including STEM, humanities, and social sciences, sourced from authentic educational materials and manually validated by native speakers and educators. The dataset includes 10,969 Kazakh questions and 12,031 Russian questions, reflecting Kazakhstan's bilingual education system and rich local context. Our evaluation of several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4, and DeepSeek V3) demonstrates substantial room for improvement, as even the best-performing models struggle to achieve competitive performance in Kazakh and Russian. These findings underscore significant performance gaps compared to high-resource languages. We hope that our dataset will enable further research and development of Kazakh-centric LLMs. Data and code will be made available upon acceptance.",
      "arxiv_url": "https://arxiv.org/abs/2502.12829",
      "pdf_url": "https://arxiv.org/pdf/2502.12829",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19956",
      "title": "DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph",
      "authors": [
        "Jihyung Lee",
        "Jin-Seop Lee",
        "Jaehoon Lee",
        "YunSeok Choi",
        "Jee-Hyong Lee"
      ],
      "abstract": "Text-to-SQL, which translates a natural language question into an SQL query, has advanced with in-context learning of Large Language Models (LLMs). However, existing methods show little improvement in performance compared to randomly chosen demonstrations, and significant performance drops when smaller LLMs (e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely on the intrinsic capabilities of hyper-scaled LLMs, rather than effectively retrieving useful demonstrations. In this paper, we propose a novel approach for effectively retrieving demonstrations and generating SQL queries. We construct a Deep Contextual Schema Link Graph, which contains key information and semantic relationship between a question and its database schema items. This graph-based structure enables effective representation of Text-to-SQL samples and retrieval of useful demonstrations for in-context learning. Experimental results on the Spider benchmark demonstrate the effectiveness of our approach, showing consistent improvements in SQL generation performance and efficiency across both hyper-scaled LLMs and small LLMs. The code is available at https://github.com/jjklle/DCG-SQL}{https://github.com/jjklle/DCG-SQL.",
      "arxiv_url": "https://arxiv.org/abs/2505.19956",
      "pdf_url": "https://arxiv.org/pdf/2505.19956",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.07345",
      "title": "QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines",
      "authors": [
        "Ohjoon Kwon",
        "Changsu Lee",
        "Jihye Back",
        "Sun Suk Lim",
        "Inho Kang",
        "Donghyeon Jeon"
      ],
      "abstract": "Large language models (LLMs) have been widely used for relevance assessment in information retrieval. However, our study demonstrates that combining two distinct small language models (SLMs) with different architectures can outperform LLMs in this task. Our approach -- QUPID -- integrates a generative SLM with an embedding-based SLM, achieving higher relevance judgment accuracy while reducing computational costs compared to state-of-the-art LLM solutions. This computational efficiency makes QUPID highly scalable for real-world search systems processing millions of queries daily. In experiments across diverse document types, our method demonstrated consistent performance improvements (Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x faster inference times. Furthermore, when integrated into production search pipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how architectural diversity in model combinations can significantly enhance both search relevance and operational efficiency in information retrieval systems.",
      "arxiv_url": "https://arxiv.org/abs/2505.07345",
      "pdf_url": "https://arxiv.org/pdf/2505.07345",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ba5620df3df36fd41e7c1ced7d5724d405f3deea",
      "title": "EMGLLM: Data-to-Text Alignment for Electromyogram Diagnosis Generation with Medical Numerical Data Encoding",
      "authors": [
        "Zefei Long",
        "Zhenbiao Cao",
        "Wei Chen",
        "Zhongyu Wei"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/ba5620df3df36fd41e7c1ced7d5724d405f3deea",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.08057",
      "title": "Odysseus Navigates the Sirens' Song: Dynamic Focus Decoding for Factual and Diverse Open-Ended Text Generation",
      "authors": [
        "Wen Luo",
        "Feifan Song",
        "Wei Li",
        "Guangyue Peng",
        "Shaohang Wei",
        "Houfeng Wang"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly required to generate text that is both factually accurate and diverse across various open-ended applications. However, current stochastic decoding methods struggle to balance such objectives. We introduce Dynamic Focus Decoding (DFD), a novel plug-and-play stochastic approach that resolves this trade-off without requiring additional data, knowledge, or models. DFD adaptively adjusts the decoding focus based on distributional differences across layers, leveraging the modular and hierarchical nature of factual knowledge within LLMs. This dynamic adjustment improves factuality in knowledge-intensive decoding steps and promotes diversity in less knowledge-reliant steps. DFD can be easily integrated with existing decoding methods, enhancing both factuality and diversity with minimal computational overhead. Extensive experiments across seven datasets demonstrate that DFD significantly improves performance, providing a scalable and efficient solution for open-ended text generation.",
      "arxiv_url": "https://arxiv.org/abs/2503.08057",
      "pdf_url": "https://arxiv.org/pdf/2503.08057",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ba74f09a77b601977fcd5f295dc8e1e9623b6aea",
      "title": "Improving Efficiency in Large Language Models via Extendable Block Floating Point Representation",
      "authors": [
        "Dongyang Li",
        "Zeyang Li",
        "Bosheng Liu",
        "Jigang Wu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/ba74f09a77b601977fcd5f295dc8e1e9623b6aea",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12961",
      "title": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger",
      "authors": [
        "Wenjun Li",
        "Dexun Li",
        "Kuicai Dong",
        "Cong Zhang",
        "Hao Zhang",
        "Weiwen Liu",
        "Yasheng Wang",
        "Ruiming Tang",
        "Yong Liu"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable emergent capabilities, transforming the execution of functional tasks by leveraging external tools for complex problems that require specialized processing or up-to-date data. While existing research expands LLMs access to diverse tools (e.g., program interpreters, search engines, calculators), the necessity of using these tools is often overlooked, leading to indiscriminate tool invocation. This naive approach raises two key issues: increased latency due to unnecessary tool calls, and potential errors resulting from faulty interactions with external tools. In this paper, we introduce meta-cognition as a proxy for LLMs self-assessment of their capabilities, reflecting the model's awareness of its own limitations. Based on this, we propose MeCo, an adaptive decision-making strategy for external tool use. MeCo quantifies metacognitive scores by capturing high-level cognitive signals in the representation space, guiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs minimal cost. Experiments across multiple backbone models and benchmarks show that MeCo reliably detects LLMs'internal cognitive signals and significantly improves tool-use decision-making.",
      "arxiv_url": "https://arxiv.org/abs/2502.12961",
      "pdf_url": "https://arxiv.org/pdf/2502.12961",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "bae83b96e77db094e71648095e8669dc8d64968d",
      "title": "Sparse Latents Steer Retrieval-Augmented Generation",
      "authors": [
        "Chunlei Xin",
        "Shuheng Zhou",
        "Huijia Zhu",
        "Weiqiang Wang",
        "Xuanang Chen",
        "Xinyan Guan",
        "Yaojie Lu",
        "Hongyu Lin",
        "Xianpei Han",
        "Le Sun"
      ],
      "abstract": "Understanding the mechanisms underlying Large Language Model (LLM) behavior in Retrieval-Augmented Generation (RAG) systems is critical for enhancing reliability. In this paper, we leverage Sparse Autoencoders (SAEs) within the LLaMA Scope to uncover sparse, interpretable latents that govern RAG behaviors. Through systematic analysis of SAE activations, we identify specific latents associated with two fundamental RAG decisions: (1) context versus memory prioritization, and (2) response generation versus query rejection. Intervention experiments demonstrate that these latents enable precise control over model behavior and maintain generalizability across various experimental settings. Mechanistic analysis reveals that manipulating these latents influences model behavior by reconfiguring attention patterns of retrieval heads. Our findings establish SAEs as a principled tool for understanding and controlling RAG behaviors, demonstrating capabilities in precise behavior steering without architectural modifications.",
      "arxiv_url": "https://www.semanticscholar.org/paper/bae83b96e77db094e71648095e8669dc8d64968d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.16488",
      "title": "ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination Detection in LLMs",
      "authors": [
        "Zhenliang Zhang",
        "Xinyu Hu",
        "Huixuan Zhang",
        "Junzhe Zhang",
        "Xiaojun Wan"
      ],
      "abstract": "Large language models (LLMs) excel at various natural language processing tasks, but their tendency to generate hallucinations undermines their reliability. Existing hallucination detection methods leveraging hidden states predominantly focus on static and isolated representations, overlooking their dynamic evolution across layers, which limits efficacy. To address this limitation, we shift the focus to the hidden state update process and introduce a novel metric, the ICR Score (Information Contribution to Residual Stream), which quantifies the contribution of modules to the hidden states'update. We empirically validate that the ICR Score is effective and reliable in distinguishing hallucinations. Building on these insights, we propose a hallucination detection method, the ICR Probe, which captures the cross-layer evolution of hidden states. Experimental results show that the ICR Probe achieves superior performance with significantly fewer parameters. Furthermore, ablation studies and case analyses offer deeper insights into the underlying mechanism of this method, improving its interpretability.",
      "arxiv_url": "https://arxiv.org/abs/2507.16488",
      "pdf_url": "https://arxiv.org/pdf/2507.16488",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01918",
      "title": "Spatial Coordinates as a Cell Language: A Multi-Sentence Framework for Imaging Mass Cytometry Analysis",
      "authors": [
        "Chi-Jane Chen",
        "Yuhang Chen",
        "Sukwon Yun",
        "Natalie Stanley",
        "Tianlong Chen"
      ],
      "abstract": "Image mass cytometry (IMC) enables high-dimensional spatial profiling by combining mass cytometry's analytical power with spatial distributions of cell phenotypes. Recent studies leverage large language models (LLMs) to extract cell states by translating gene or protein expression into biological context. However, existing single-cell LLMs face two major challenges: (1) Integration of spatial information: they struggle to generalize spatial coordinates and effectively encode spatial context as text, and (2) Treating each cell independently: they overlook cell-cell interactions, limiting their ability to capture biological relationships. To address these limitations, we propose Spatial2Sentence, a novel framework that integrates single-cell expression and spatial information into natural language using a multi-sentence approach. Spatial2Sentence constructs expression similarity and distance matrices, pairing spatially adjacent and expressionally similar cells as positive pairs while using distant and dissimilar cells as negatives. These multi-sentence representations enable LLMs to learn cellular interactions in both expression and spatial contexts. Equipped with multi-task learning, Spatial2Sentence outperforms existing single-cell LLMs on preprocessed IMC datasets, improving cell-type classification by 5.98% and clinical status prediction by 4.18% on the diabetes dataset while enhancing interpretability. The source code can be found here: https://github.com/UNITES-Lab/Spatial2Sentence.",
      "arxiv_url": "https://arxiv.org/abs/2506.01918",
      "pdf_url": "https://arxiv.org/pdf/2506.01918",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00751",
      "title": "RAPID: Efficient Retrieval-Augmented Long Text Generation with Writing Planning and Information Discovery",
      "authors": [
        "Hongchao Gu",
        "Dexun Li",
        "Kuicai Dong",
        "Hao Zhang",
        "Hang Lv",
        "Hao Wang",
        "Defu Lian",
        "Yong Liu",
        "Enhong Chen"
      ],
      "abstract": "Generating knowledge-intensive and comprehensive long texts, such as encyclopedia articles, remains significant challenges for Large Language Models. It requires not only the precise integration of facts but also the maintenance of thematic coherence throughout the article. Existing methods, such as direct generation and multi-agent discussion, often struggle with issues like hallucinations, topic incoherence, and significant latency. To address these challenges, we propose RAPID, an efficient retrieval-augmented long text generation framework. RAPID consists of three main modules: (1) Retrieval-augmented preliminary outline generation to reduce hallucinations, (2) Attribute-constrained search for efficient information discovery, (3) Plan-guided article generation for enhanced coherence. Extensive experiments on our newly compiled benchmark dataset, FreshWiki-2024, demonstrate that RAPID significantly outperforms state-of-the-art methods across a wide range of evaluation metrics (e.g. long-text generation, outline quality, latency, etc). Our work provides a robust and efficient solution to the challenges of automated long-text generation.",
      "arxiv_url": "https://arxiv.org/abs/2503.00751",
      "pdf_url": "https://arxiv.org/pdf/2503.00751",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "bba3c30cb3b15cebee1d4c1901e695864d3bde39",
      "title": "Towards A Better Initial Policy Model For Scalable Long-CoT Reinforcement Learning",
      "authors": [
        "Bofei Gao",
        "Yejie Wang",
        "Yibo Miao",
        "Ruoyu Wu",
        "Feifan Song",
        "Long Yu",
        "Tianyu Liu",
        "Baobao Chang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/bba3c30cb3b15cebee1d4c1901e695864d3bde39",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.13366",
      "title": "Enhancing Goal-oriented Proactive Dialogue Systems via Consistency Reflection and Correction",
      "authors": [
        "Didi Zhang",
        "Yaxin Fan",
        "Peifeng Li",
        "Qiaoming Zhu"
      ],
      "abstract": "Goal-oriented proactive dialogue systems are designed to guide user conversations seamlessly towards specific objectives by planning a goal-oriented path. However, previous research has focused predominantly on optimizing these paths while neglecting the inconsistencies that may arise between generated responses and dialogue contexts, including user profiles, dialogue history, domain knowledge, and subgoals. To address this issue, we introduce a model-agnostic two-stage Consistency Reflection and Correction (CRC) framework. Specifically, in the consistency reflection stage, the model is prompted to reflect on the discrepancies between generated responses and dialogue contexts, identifying inconsistencies and suggesting possible corrections. In the consistency correction stage, the model generates responses that are more consistent with the dialogue context based on these reflection results. We conducted experiments on various model architectures with different parameter sizes, including encoder-decoder models (BART, T5) and decoder-only models (GPT-2, DialoGPT, Phi3, Mistral and LLaMA3), and the experimental results on three datasets demonstrate that our CRC framework significantly improves the consistency between generated responses and dialogue contexts.",
      "arxiv_url": "https://arxiv.org/abs/2506.13366",
      "pdf_url": "https://arxiv.org/pdf/2506.13366",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16414",
      "title": "TabGen-ICL: Residual-Aware In-Context Example Selection for Tabular Data Generation",
      "authors": [
        "Liancheng Fang",
        "Aiwei Liu",
        "Hengrui Zhang",
        "Henry Peng Zou",
        "Weizhi Zhang",
        "Philip S. Yu"
      ],
      "abstract": "Large Language models (LLMs) have achieved encouraging results in tabular data generation. However, existing approaches require fine-tuning, which is computationally expensive. This paper explores an alternative: prompting a fixed LLM with in-context examples. We observe that using randomly selected in-context examples hampers the LLM's performance, resulting in sub-optimal generation quality. To address this, we propose a novel in-context learning framework: TabGen-ICL, to enhance the in-context learning ability of LLMs for tabular data generation. TabGen-ICL operates iteratively, retrieving a subset of real samples that represent the residual between currently generated samples and true data distributions. This approach serves two purposes: locally, it provides more effective in-context learning examples for the LLM in each iteration; globally, it progressively narrows the gap between generated and real data. Extensive experiments on five real-world tabular datasets demonstrate that TabGen-ICL significantly outperforms the random selection strategy. Specifically, it reduces the error rate by a margin of $3.5\\%-42.2\\%$ on fidelity metrics. We demonstrate for the first time that prompting a fixed LLM can yield high-quality synthetic tabular data. The code is provided in the \\href{https://github.com/fangliancheng/TabGEN-ICL}{link}.",
      "arxiv_url": "https://arxiv.org/abs/2502.16414",
      "pdf_url": "https://arxiv.org/pdf/2502.16414",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2508.01625",
      "title": "EAC-MoE: Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models",
      "authors": [
        "Yuanteng Chen",
        "Yuantian Shao",
        "Peisong Wang",
        "Jian Cheng"
      ],
      "abstract": "Mixture-of-Experts (MoE) has demonstrated promising potential in scaling LLMs. However, it is hindered by two critical challenges: (1) substantial GPU memory consumption to load all experts; (2) low activated parameters cannot be equivalently translated into inference acceleration effects. In this work, we propose EAC-MoE, an Expert-Selection Aware Compressor for MoE-LLMs, which deeply aligns with the characteristics of MoE from the perspectives of quantization and pruning, and introduces two modules to address these two challenges respectively: (1) The expert selection bias caused by low-bit quantization is a major factor contributing to the performance degradation in MoE-LLMs. Based on this, we propose Quantization with Expert-Selection Calibration (QESC), which mitigates the expert selection bias by calibrating the routers within the MoE; (2) There are always certain experts that are not crucial for the corresponding tasks, yet causing inference latency. Therefore, we propose Pruning based on Expert-Selection Frequency (PESF), which significantly improves inference speed by pruning less frequently used experts for current task. Extensive experiments demonstrate that our approach significantly reduces memory usage and improves inference speed with minimal performance degradation.",
      "arxiv_url": "https://arxiv.org/abs/2508.01625",
      "pdf_url": "https://arxiv.org/pdf/2508.01625",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-08-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.04070",
      "title": "More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives",
      "authors": [
        "Xiaoqing Zhang",
        "Ang Lv",
        "Yuhan Liu",
        "Flood Sung",
        "Wei Liu",
        "Shuo Shang",
        "Xiuying Chen",
        "Rui Yan"
      ],
      "abstract": "Large language models (LLMs) excel at few-shot in-context learning (ICL) without requiring parameter updates. However, as ICL demonstrations increase from a few to many, performance tends to plateau and eventually decline. We identify two primary causes for this trend: the suboptimal negative log-likelihood (NLL) optimization objective and the incremental data noise. To address these issues, we introduce \\textit{DrICL}, a novel optimization method that enhances model performance through \\textit{Differentiated} and \\textit{Reweighting} objectives. Globally, DrICL utilizes differentiated learning to optimize the NLL objective, ensuring that many-shot performance surpasses zero-shot levels. Locally, it dynamically adjusts the weighting of many-shot demonstrations by leveraging cumulative advantages inspired by reinforcement learning, thereby mitigating the impact of noisy data. Recognizing the lack of multi-task datasets with diverse many-shot distributions, we develop the \\textit{Many-Shot ICL Benchmark} (ICL-50)-a large-scale benchmark of 50 tasks that cover shot numbers from 1 to 350 within sequences of up to 8,000 tokens-for both fine-tuning and evaluation purposes. Experimental results demonstrate that LLMs enhanced with DrICL achieve significant improvements in many-shot setups across various tasks, including both in-domain and out-of-domain scenarios. We release the code and dataset hoping to facilitate further research in many-shot ICL\\footnote{https://github.com/xiaoqzhwhu/DrICL}.",
      "arxiv_url": "https://arxiv.org/abs/2501.04070",
      "pdf_url": "https://arxiv.org/pdf/2501.04070",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00382",
      "title": "Spectral Insights into Data-Oblivious Critical Layers in Large Language Models",
      "authors": [
        "Xuyuan Liu",
        "Lei Hsiung",
        "Yaoqing Yang",
        "Yujun Yan"
      ],
      "abstract": "Understanding how feature representations evolve across layers in large language models (LLMs) is key to improving their interpretability and robustness. While recent studies have identified critical layers linked to specific functions or behaviors, these efforts typically rely on data-dependent analyses of fine-tuned models, limiting their use to post-hoc settings. In contrast, we introduce a data-oblivious approach to identify intrinsic critical layers in pre-fine-tuned LLMs by analyzing representation dynamics via Centered Kernel Alignment(CKA). We show that layers with significant shifts in representation space are also those most affected during fine-tuning--a pattern that holds consistently across tasks for a given model. Our spectral analysis further reveals that these shifts are driven by changes in the top principal components, which encode semantic transitions from rationales to conclusions. We further apply these findings to two practical scenarios: efficient domain adaptation, where fine-tuning critical layers leads to greater loss reduction compared to non-critical layers; and backdoor defense, where freezing them reduces attack success rates by up to 40%.",
      "arxiv_url": "https://arxiv.org/abs/2506.00382",
      "pdf_url": "https://arxiv.org/pdf/2506.00382",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.07879",
      "title": "OMGM: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval",
      "authors": [
        "Wei Yang",
        "Jingjing Fu",
        "Rui Wang",
        "Jinyu Wang",
        "Lei Song",
        "Jiang Bian"
      ],
      "abstract": "Vision-language retrieval-augmented generation (RAG) has become an effective approach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which requires external knowledge beyond the visual content presented in images. The effectiveness of Vision-language RAG systems hinges on multimodal retrieval, which is inherently challenging due to the diverse modalities and knowledge granularities in both queries and knowledge bases. Existing methods have not fully tapped into the potential interplay between these elements. We propose a multimodal RAG system featuring a coarse-to-fine, multi-step retrieval that harmonizes multiple granularities and modalities to enhance efficacy. Our system begins with a broad initial search aligning knowledge granularity for cross-modal retrieval, followed by a multimodal fusion reranking to capture the nuanced multimodal information for top entity selection. A text reranker then filters out the most relevant fine-grained section for augmented generation. Extensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our method achieves state-of-the-art retrieval performance and highly competitive answering results, underscoring its effectiveness in advancing KB-VQA systems.",
      "arxiv_url": "https://arxiv.org/abs/2505.07879",
      "pdf_url": "https://arxiv.org/pdf/2505.07879",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Multi-Modal RAG",
        "RAG"
      ],
      "published_date": "2025-05-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "bd30fc07bc04041a246f6325f71c9f3bf9a6429d",
      "title": "Generating Q&A Benchmarks for RAG Evaluation in Enterprise Settings",
      "authors": [
        "Simone Filice",
        "Guy Horowitz",
        "David Carmel",
        "Zohar S. Karnin",
        "L. Lewin-Eytan",
        "Y. Maarek"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/bd30fc07bc04041a246f6325f71c9f3bf9a6429d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.15630",
      "title": "Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement",
      "authors": [
        "Xiaowei Yuan",
        "Zhao Yang",
        "Ziyang Huang",
        "Yequan Wang",
        "Siqi Fan",
        "Yiming Ju",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks, yet they often struggle with context-faithfulness generations that properly reflect contextual knowledge. While existing approaches focus on enhancing the decoding strategies, they ignore the fundamental mechanism of how contextual information is processed within LLMs' internal states. As a result, LLMs remain limited in their ability to fully leverage contextual knowledge. In this paper, we propose Context-aware Layer Enhancement (CaLE), a novel intervention method that enhances the utilization of contextual knowledge within LLMs' internal representations. By employing V-usable information analysis, CaLE strategically amplifies the growth of contextual information at an optimal layer, thereby enriching representations in the final layer. Our experiments demonstrate that CaLE effectively improves context-faithful generation in Question-Answering tasks, particularly in scenarios involving unknown or conflicting contextual knowledge.",
      "arxiv_url": "https://arxiv.org/abs/2504.15630",
      "pdf_url": "https://arxiv.org/pdf/2504.15630",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "bda47dae82583acf4808b6624b6894ab9cb9a9aa",
      "title": "Towards Building Large Scale Datasets and State-of-the-Art Automatic Speech Translation Systems for 14 Indian Languages",
      "authors": [
        "Ashwin Sankar",
        "Sparsh Jain",
        "Nikhil Narasimhan",
        "Devilal Choudhary",
        "Dhairya Suman",
        "Mohammed Safi Ur Rahman Khan",
        "Anoop Kunchukuttan",
        "Mitesh M. Khapra",
        "Raj Dabre"
      ],
      "abstract": "Speech translation for Indian languages remains a challenging task due to the scarcity of large-scale, publicly available datasets that capture the linguistic diversity and domain coverage essential for real-world applications. Existing datasets cover a fraction of Indian languages and lack the breadth needed to train robust models that generalize beyond curated benchmarks. To bridge this gap, we introduce B HASA A NUVAAD , the largest speech translation dataset for Indian languages, spanning over 44 thousand hours of audio and 17 million aligned text segments across 14 Indian languages and English. Our dataset is built through a threefold methodology: (a) aggregating high-quality existing sources, (b) large-scale web crawling to ensure linguistic and domain diversity, and (c) creating synthetic data to model real-world speech disfluencies. Leveraging B HASA A NUVAAD , we train I NDIC - S EAMLESS , a state-of-the-art speech translation model for Indian languages that performs better than existing models. Our experiments demonstrate improvements in the translation quality, setting a new standard for Indian language speech translation. We will release all the code, data and model weights in the open-source, with permissive licenses to promote accessibility and collaboration.",
      "arxiv_url": "https://www.semanticscholar.org/paper/bda47dae82583acf4808b6624b6894ab9cb9a9aa",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14445",
      "title": "PredictaBoard: Benchmarking LLM Score Predictability",
      "authors": [
        "Lorenzo Pacchiardi",
        "Konstantinos Voudouris",
        "Ben Slater",
        "Fernando Mart'inez-Plumed",
        "Jos'e Hern'andez-Orallo",
        "Lexin Zhou",
        "Wout Schellaert"
      ],
      "abstract": "Despite possessing impressive skills, Large Language Models (LLMs) often fail unpredictably, demonstrating inconsistent success in even basic common sense reasoning tasks. This unpredictability poses a significant challenge to ensuring their safe deployment, as identifying and operating within a reliable\"safe zone\"is essential for mitigating risks. To address this, we present PredictaBoard, a novel collaborative benchmarking framework designed to evaluate the ability of score predictors (referred to as assessors) to anticipate LLM errors on specific task instances (i.e., prompts) from existing datasets. PredictaBoard evaluates pairs of LLMs and assessors by considering the rejection rate at different tolerance errors. As such, PredictaBoard stimulates research into developing better assessors and making LLMs more predictable, not only with a higher average performance. We conduct illustrative experiments using baseline assessors and state-of-the-art LLMs. PredictaBoard highlights the critical need to evaluate predictability alongside performance, paving the way for safer AI systems where errors are not only minimised but also anticipated and effectively mitigated. Code for our benchmark can be found at https://github.com/Kinds-of-Intelligence-CFI/PredictaBoard",
      "arxiv_url": "https://arxiv.org/abs/2502.14445",
      "pdf_url": "https://arxiv.org/pdf/2502.14445",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.17860",
      "title": "Enhancing Retrieval Systems with Inference-Time Logical Reasoning",
      "authors": [
        "Felix Faltings",
        "Wei Wei",
        "Yujia Bao"
      ],
      "abstract": "Traditional retrieval methods rely on transforming user queries into vector representations and retrieving documents based on cosine similarity within an embedding space. While efficient and scalable, this approach often fails to handle complex queries involving logical constructs such as negations, conjunctions, and disjunctions. In this paper, we propose a novel inference-time logical reasoning framework that explicitly incorporates logical reasoning into the retrieval process. Our method extracts logical reasoning structures from natural language queries and then composes the individual cosine similarity scores to formulate the final document scores. This approach enables the retrieval process to handle complex logical reasoning without compromising computational efficiency. Our results on both synthetic and real-world benchmarks demonstrate that the proposed method consistently outperforms traditional retrieval methods across different models and datasets, significantly improving retrieval performance for complex queries.",
      "arxiv_url": "https://arxiv.org/abs/2503.17860",
      "pdf_url": "https://arxiv.org/pdf/2503.17860",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "be7383874d3c049bbd897ea74604718b21b84a73",
      "title": "ARC 'Challenge' Is Not That Challenging",
      "authors": [
        "Lukasz Borchmann"
      ],
      "abstract": "ARC Challenge 1 appears more difficult than ARC Easy for modern LLMs primarily due to an evaluation setup that prevents direct comparison of answer choices rather than inherent complexity. Although some researchers have quietly shifted to a more appropriate evaluation scheme over the last year, the implications of this change have yet to be widely acknowledged. We highlight this overlooked shift, show how similar evaluation practices falsely imply reasoning deficits in other benchmarks, and demonstrate that fairer methods dramatically reduce performance gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing so, we reveal how evaluation shapes perceived difficulty and offer guidelines to ensure that multiple-choice evaluations accurately reflect actual model capabilities.",
      "arxiv_url": "https://www.semanticscholar.org/paper/be7383874d3c049bbd897ea74604718b21b84a73",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11268",
      "title": "Improved Unbiased Watermark for Large Language Models",
      "authors": [
        "Ruibo Chen",
        "Yihan Wu",
        "Junfeng Guo",
        "Heng Huang"
      ],
      "abstract": "As artificial intelligence surpasses human capabilities in text generation, the necessity to authenticate the origins of AI-generated content has become paramount. Unbiased watermarks offer a powerful solution by embedding statistical signals into language model-generated text without distorting the quality. In this paper, we introduce MCmark, a family of unbiased, Multi-Channel-based watermarks. MCmark works by partitioning the model's vocabulary into segments and promoting token probabilities within a selected segment based on a watermark key. We demonstrate that MCmark not only preserves the original distribution of the language model but also offers significant improvements in detectability and robustness over existing unbiased watermarks. Our experiments with widely-used language models demonstrate an improvement in detectability of over 10% using MCmark, compared to existing state-of-the-art unbiased watermarks. This advancement underscores MCmark's potential in enhancing the practical application of watermarking in AI-generated texts.",
      "arxiv_url": "https://arxiv.org/abs/2502.11268",
      "pdf_url": "https://arxiv.org/pdf/2502.11268",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.04795",
      "title": "Developmentally-plausible Working Memory Shapes a Critical Period for Language Acquisition",
      "authors": [
        "Masato Mita",
        "Ryosuke Yoshida",
        "Yohei Oseki"
      ],
      "abstract": "Large language models possess general linguistic abilities but acquire language less efficiently than humans. This study proposes a method for integrating the developmental characteristics of working memory during the critical period, a stage when human language acquisition is particularly efficient, into the training process of language models. The proposed method introduces a mechanism that initially constrains working memory during the early stages of training and gradually relaxes this constraint in an exponential manner as learning progresses. Targeted syntactic evaluation shows that the proposed method outperforms conventional methods without memory constraints or with static memory constraints. These findings not only provide new directions for designing data-efficient language models but also offer indirect evidence supporting the role of the developmental characteristics of working memory as the underlying mechanism of the critical period in language acquisition.",
      "arxiv_url": "https://arxiv.org/abs/2502.04795",
      "pdf_url": "https://arxiv.org/pdf/2502.04795",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "beab5938e2e0eebddf8be05bf717a8cf3c088326",
      "title": "Model-Dependent Moderation: Inconsistencies in Hate Speech Detection Across LLM-based Systems",
      "authors": [
        "Neil Fasching",
        "Yphtach Lelkes"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/beab5938e2e0eebddf8be05bf717a8cf3c088326",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15950",
      "title": "Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models",
      "authors": [
        "Lior Belenki",
        "Alekh Agarwal",
        "Tianze Shi",
        "Kristina Toutanova"
      ],
      "abstract": "We propose a method to optimize language model pre-training data mixtures through efficient approximation of the cross-entropy loss corresponding to each candidate mixture via a Mixture of Data Experts (MDE). We use this approximation as a source of additional features in a regression model, trained from observations of model loss for a small number of mixtures. Experiments with Transformer decoder-only language models in the range of 70M to 1B parameters on the SlimPajama dataset show that our method achieves significantly better performance than approaches that train regression models using only the mixture rates as input features. Combining this improved optimization method with an objective that takes into account cross-entropy on end task data leads to superior performance on few-shot downstream evaluations. We also provide theoretical insights on why aggregation of data expert predictions can provide good approximations to model losses for data mixtures.",
      "arxiv_url": "https://arxiv.org/abs/2502.15950",
      "pdf_url": "https://arxiv.org/pdf/2502.15950",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13917",
      "title": "TESS 2: A Large-Scale Generalist Diffusion Language Model",
      "authors": [
        "Jaesung Tae",
        "Hamish Ivison",
        "Sachin Kumar",
        "Arman Cohan"
      ],
      "abstract": "We introduce TESS 2, a general instruction-following diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first adapting a strong AR model via continued pretraining with the usual cross-entropy as diffusion loss, and then performing further instruction tuning. We find that adaptation training as well as the choice of the base model is crucial for training good instruction-following diffusion models. We further propose reward guidance, a novel and modular inference-time guidance procedure to align model outputs without needing to train the underlying model. Finally, we show that TESS 2 further improves with increased inference-time compute, highlighting the utility of diffusion LMs in having fine-grained controllability over the amount of compute used at inference time. Code and models are available at https://github.com/hamishivi/tess-2.",
      "arxiv_url": "https://arxiv.org/abs/2502.13917",
      "pdf_url": "https://arxiv.org/pdf/2502.13917",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.18971",
      "title": "LLMs as Planning Formalizers: A Survey for Leveraging Large Language Models to Construct Automated Planning Models",
      "authors": [
        "Marcus Tantakoun",
        "Christian Muise",
        "Xiao-Dan Zhu"
      ],
      "abstract": "Large Language Models (LLMs) excel in various natural language tasks but often struggle with long-horizon planning problems requiring structured reasoning. This limitation has drawn interest in integrating neuro-symbolic approaches within the Automated Planning (AP) and Natural Language Processing (NLP) communities. However, identifying optimal AP deployment frameworks can be daunting and introduces new challenges. This paper aims to provide a timely survey of the current research with an in-depth analysis, positioning LLMs as tools for formalizing and refining planning specifications to support reliable off-the-shelf AP planners. By systematically reviewing the current state of research, we highlight methodologies, and identify critical challenges and future directions, hoping to contribute to the joint research on NLP and Automated Planning.",
      "arxiv_url": "https://arxiv.org/abs/2503.18971",
      "pdf_url": "https://arxiv.org/pdf/2503.18971",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.05122",
      "title": "Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model",
      "authors": [
        "Gregor Geigle",
        "Florian Schneider",
        "Carolin Holtermann",
        "Chris Biemann",
        "Radu Timofte",
        "Anne Lauscher",
        "Goran Glavavs"
      ],
      "abstract": "Most Large Vision-Language Models (LVLMs) to date are trained predominantly on English data, which makes them struggle to understand non-English input and fail to generate output in the desired target language. Existing efforts mitigate these issues by adding multilingual training data, but do so in a largely ad-hoc manner, lacking insight into how different training mixes tip the scale for different groups of languages. In this work, we present a comprehensive investigation into the training strategies for massively multilingual LVLMs. First, we conduct a series of multi-stage experiments spanning 13 downstream vision-language tasks and 43 languages, systematically examining: (1) the number of training languages that can be included without degrading English performance and (2) optimal language distributions of pre-training as well as (3) instruction-tuning data. Further, we (4) investigate how to improve multilingual text-in-image understanding, and introduce a new benchmark for the task. Surprisingly, our analysis reveals that one can (i) include as many as 100 training languages simultaneously (ii) with as little as 25-50\\% of non-English data, to greatly improve multilingual performance while retaining strong English performance. We further find that (iii) including non-English OCR data in pre-training and instruction-tuning is paramount for improving multilingual text-in-image understanding. Finally, we put all our findings together and train Centurio, a 100-language LVLM, offering state-of-the-art performance in an evaluation covering 14 tasks and 56 languages.",
      "arxiv_url": "https://arxiv.org/abs/2501.05122",
      "pdf_url": "https://arxiv.org/pdf/2501.05122",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.21299",
      "title": "BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models",
      "authors": [
        "Zhiting Fan",
        "Ruizhe Chen",
        "Zuozhu Liu"
      ],
      "abstract": "Identifying bias in LLM-generated content is a crucial prerequisite for ensuring fairness in LLMs. Existing methods, such as fairness classifiers and LLM-based judges, face limitations related to difficulties in understanding underlying intentions and the lack of criteria for fairness judgment. In this paper, we introduce BiasGuard, a novel bias detection tool that explicitly analyzes inputs and reasons through fairness specifications to provide accurate judgments. BiasGuard is implemented through a two-stage approach: the first stage initializes the model to explicitly reason based on fairness specifications, while the second stage leverages reinforcement learning to enhance its reasoning and judgment capabilities. Our experiments, conducted across five datasets, demonstrate that BiasGuard outperforms existing tools, improving accuracy and reducing over-fairness misjudgments. We also highlight the importance of reasoning-enhanced decision-making and provide evidence for the effectiveness of our two-stage optimization pipeline.",
      "arxiv_url": "https://arxiv.org/abs/2504.21299",
      "pdf_url": "https://arxiv.org/pdf/2504.21299",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11903",
      "title": "MMRC: A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation",
      "authors": [
        "Haochen Xue",
        "Feilong Tang",
        "Ming Hu",
        "Yexin Liu",
        "Qidong Huang",
        "Yulong Li",
        "Chengzhi Liu",
        "Zhongxing Xu",
        "Chong Zhang",
        "Chunmei Feng",
        "Yutong Xie",
        "Imran Razzak",
        "Zongyuan Ge",
        "Jionglong Su",
        "Junjun He",
        "Yu Qiao"
      ],
      "abstract": "Recent multimodal large language models (MLLMs) have demonstrated significant potential in open-ended conversation, generating more accurate and personalized responses. However, their abilities to memorize, recall, and reason in sustained interactions within real-world scenarios remain underexplored. This paper introduces MMRC, a Multi-Modal Real-world Conversation benchmark for evaluating six core open-ended abilities of MLLMs: information extraction, multi-turn reasoning, information update, image management, memory recall, and answer refusal. With data collected from real-world scenarios, MMRC comprises 5,120 conversations and 28,720 corresponding manually labeled questions, posing a significant challenge to existing MLLMs. Evaluations on 20 MLLMs in MMRC indicate an accuracy drop during open-ended interactions. We identify four common failure patterns: long-term memory degradation, inadequacies in updating factual knowledge, accumulated assumption of error propagation, and reluctance to say no. To mitigate these issues, we propose a simple yet effective NOTE-TAKING strategy, which can record key information from the conversation and remind the model during its responses, enhancing conversational capabilities. Experiments across six MLLMs demonstrate significant performance improvements.",
      "arxiv_url": "https://arxiv.org/abs/2502.11903",
      "pdf_url": "https://arxiv.org/pdf/2502.11903",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.10201",
      "title": "Prediction hubs are context-informed frequent tokens in LLMs",
      "authors": [
        "Beatrix M. G. Nielsen",
        "Iuri Macocco",
        "Marco Baroni"
      ],
      "abstract": "Hubness, the tendency for a few points to be among the nearest neighbours of a disproportionate number of other points, commonly arises when applying standard distance measures to high-dimensional data, often negatively impacting distance-based analysis. As autoregressive large language models (LLMs) operate on high-dimensional representations, we ask whether they are also affected by hubness. We first prove that the only large-scale representation comparison operation performed by LLMs, namely that between context and unembedding vectors to determine continuation probabilities, is not characterized by the concentration of distances phenomenon that typically causes the appearance of nuisance hubness. We then empirically show that this comparison still leads to a high degree of hubness, but the hubs in this case do not constitute a disturbance. They are rather the result of context-modulated frequent tokens often appearing in the pool of likely candidates for next token prediction. However, when other distances are used to compare LLM representations, we do not have the same theoretical guarantees, and, indeed, we see nuisance hubs appear. There are two main takeaways. First, hubness, while omnipresent in high-dimensional spaces, is not a negative property that needs to be mitigated when LLMs are being used for next token prediction. Second, when comparing representations from LLMs using Euclidean or cosine distance, there is a high risk of nuisance hubs and practitioners should use mitigation techniques if relevant.",
      "arxiv_url": "https://arxiv.org/abs/2502.10201",
      "pdf_url": "https://arxiv.org/pdf/2502.10201",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "bfa575cde37a5925855e41990b47f9205ff7656d",
      "title": "Evaluating Lexical Proficiency in Neural Language Models",
      "authors": [
        "Cristiano Ciaccio",
        "Alessio Miaschi",
        "F. Dell’Orletta"
      ],
      "abstract": "We present a novel evaluation framework designed to assess the lexical proficiency and linguistic creativity of Transformer-based Language Models (LMs). We validate the framework by analyzing the performance of a set of LMs of different sizes, in both mono-and multilingual configuration, across tasks involving the generation, definition, and contextual usage of lexicalized words, neologisms, and nonce words. To support these evaluations, we developed a novel dataset of lexical entries for the Italian language, including curated definitions and usage examples sourced from various on-line platforms. The results highlight the robustness and effectiveness of our framework in evaluating multiple dimensions of LMs’ linguistic understanding and offer an insight, through the assessment of their linguistic creativity, on the lexical generalization abilities of LMs 1 .",
      "arxiv_url": "https://www.semanticscholar.org/paper/bfa575cde37a5925855e41990b47f9205ff7656d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23808",
      "title": "DenseLoRA: Dense Low-Rank Adaptation of Large Language Models",
      "authors": [
        "Lin Mu",
        "Xiaoyu Wang",
        "Li Ni",
        "Yang Li",
        "Zhize Wu",
        "Peiquan Jin",
        "Yiwen Zhang"
      ],
      "abstract": "Low-rank adaptation (LoRA) has been developed as an efficient approach for adapting large language models (LLMs) by fine-tuning two low-rank matrices, thereby reducing the number of trainable parameters. However, prior research indicates that many of the weights in these matrices are redundant, leading to inefficiencies in parameter utilization. To address this limitation, we introduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances parameter efficiency while achieving superior performance compared to LoRA. DenseLoRA builds upon the concept of representation fine-tuning, incorporating a single Encoder-Decoder to refine and compress hidden representations across all adaptation layers before applying adaptation. Instead of relying on two redundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense low-rank matrix, improving parameter utilization and adaptation efficiency. We evaluate DenseLoRA on various benchmarks, showing that it achieves 83.8% accuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8% accuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we conduct extensive experiments to systematically assess the impact of DenseLoRA's components on overall model performance. Code is available at https://github.com/mulin-ahu/DenseLoRA.",
      "arxiv_url": "https://arxiv.org/abs/2505.23808",
      "pdf_url": "https://arxiv.org/pdf/2505.23808",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16086",
      "title": "Stealing Training Data from Large Language Models in Decentralized Training through Activation Inversion Attack",
      "authors": [
        "Chenxi Dai",
        "Lin Lu",
        "Pan Zhou"
      ],
      "abstract": "Decentralized training has become a resource-efficient framework to democratize the training of large language models (LLMs). However, the privacy risks associated with this framework, particularly due to the potential inclusion of sensitive data in training datasets, remain unexplored. This paper identifies a novel and realistic attack surface: the privacy leakage from training data in decentralized training, and proposes \\textit{activation inversion attack} (AIA) for the first time. AIA first constructs a shadow dataset comprising text labels and corresponding activations using public datasets. Leveraging this dataset, an attack model can be trained to reconstruct the training data from activations in victim decentralized training. We conduct extensive experiments on various LLMs and publicly available datasets to demonstrate the susceptibility of decentralized training to AIA. These findings highlight the urgent need to enhance security measures in decentralized training to mitigate privacy risks in training LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2502.16086",
      "pdf_url": "https://arxiv.org/pdf/2502.16086",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.15480",
      "title": "KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance",
      "authors": [
        "Qihuang Zhong",
        "Liang Ding",
        "Xiantao Cai",
        "Juhua Liu",
        "Bo Du",
        "D. Tao"
      ],
      "abstract": "Supervised fine-tuning (SFT) is a common approach to improve the domain-specific question-answering (QA) performance of large language models (LLMs). However, recent literature reveals that due to the conflicts between LLMs' internal knowledge and the context knowledge of training data, vanilla SFT using the full QA training set is usually suboptimal. In this paper, we first design a query diversification strategy for robust conflict detection and then conduct a series of experiments to analyze the impact of knowledge conflict. We find that 1) training samples with varied conflicts contribute differently, where SFT on the data with large conflicts leads to catastrophic performance drops; 2) compared to directly filtering out the conflict data, appropriately applying the conflict data would be more beneficial. Motivated by this, we propose a simple-yet-effective Knowledge-aware Fine-tuning (namely KaFT) approach to effectively boost LLMs' performance. The core of KaFT is to adapt the training weight by assigning different rewards for different training samples according to conflict level. Extensive experiments show that KaFT brings consistent and significant improvements across four LLMs. More analyses prove that KaFT effectively improves the model generalization and alleviates the hallucination.",
      "arxiv_url": "https://arxiv.org/abs/2505.15480",
      "pdf_url": "https://arxiv.org/pdf/2505.15480",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.00828",
      "title": "ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering",
      "authors": [
        "Alexander Miserlis Hoyle",
        "Lorena Calvo-Bartolomé",
        "Jordan L. Boyd-Graber",
        "Philip Resnik"
      ],
      "abstract": "Topic model and document-clustering evaluations either use automated metrics that align poorly with human preferences or require expert labels that are intractable to scale. We design a scalable human evaluation protocol and a corresponding automated approximation that reflect practitioners'real-world usage of models. Annotators -- or an LLM-based proxy -- review text items assigned to a topic or cluster, infer a category for the group, then apply that category to other documents. Using this protocol, we collect extensive crowdworker annotations of outputs from a diverse set of topic models on two datasets. We then use these annotations to validate automated proxies, finding that the best LLM proxies are statistically indistinguishable from a human annotator and can therefore serve as a reasonable substitute in automated evaluations. Package, web interface, and data are at https://github.com/ahoho/proxann",
      "arxiv_url": "https://arxiv.org/abs/2507.00828",
      "pdf_url": "https://arxiv.org/pdf/2507.00828",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "c02cb596b1921d3c008b8f81907415018dd1bea0",
      "title": "TDCSA: LLM-Guided Top-Down Approach for Robust Citation Sentiment Analysis",
      "authors": [
        "Fan Gao",
        "Jieyang Peng",
        "Xiaoming Tao",
        "Youzheng Wang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/c02cb596b1921d3c008b8f81907415018dd1bea0",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18968",
      "title": "Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles",
      "authors": [
        "Kuang Wang",
        "Xianfei Li",
        "Shenghao Yang",
        "Li Zhou",
        "Feng Jiang",
        "Haizhou Li"
      ],
      "abstract": "User simulators are crucial for replicating human interactions with dialogue systems, supporting both collaborative training and automatic evaluation, especially for large language models (LLMs). However, current role-playing methods face challenges such as a lack of utterance-level authenticity and user-level diversity, often hindered by role confusion and dependence on predefined profiles of well-known figures. In contrast, direct simulation focuses solely on text, neglecting implicit user traits like personality and conversation-level consistency. To address these issues, we introduce the User Simulator with Implicit Profiles (USP), a framework that infers implicit user profiles from human-machine interactions to simulate personalized and realistic dialogues. We first develop an LLM-driven extractor with a comprehensive profile schema, then refine the simulation using conditional supervised fine-tuning and reinforcement learning with cycle consistency, optimizing at both the utterance and conversation levels. Finally, a diverse profile sampler captures the distribution of real-world user profiles. Experimental results show that USP outperforms strong baselines in terms of authenticity and diversity while maintaining comparable consistency. Additionally, using USP to evaluate LLM on dynamic multi-turn aligns well with mainstream benchmarks, demonstrating its effectiveness in real-world applications.",
      "arxiv_url": "https://arxiv.org/abs/2502.18968",
      "pdf_url": "https://arxiv.org/pdf/2502.18968",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00985",
      "title": "Enhancing Text Editing for Grammatical Error Correction: Arabic as a Case Study",
      "authors": [
        "Bashar Alhafni",
        "Nizar Habash"
      ],
      "abstract": "Text editing frames grammatical error correction (GEC) as a sequence tagging problem, where edit tags are assigned to input tokens, and applying these edits results in the corrected text. This approach has gained attention for its efficiency and interpretability. However, while extensively explored for English, text editing remains largely underexplored for morphologically rich languages like Arabic. In this paper, we introduce a text editing approach that derives edit tags directly from data, eliminating the need for language-specific edits. We demonstrate its effectiveness on Arabic, a diglossic and morphologically rich language, and investigate the impact of different edit representations on model performance. Our approach achieves SOTA results on two Arabic GEC benchmarks and performs on par with SOTA on two others. Additionally, our models are over six times faster than existing Arabic GEC systems, making our approach more practical for real-world applications. Finally, we explore ensemble models, demonstrating how combining different models leads to further performance improvements. We make our code, data, and pretrained models publicly available.",
      "arxiv_url": "https://arxiv.org/abs/2503.00985",
      "pdf_url": "https://arxiv.org/pdf/2503.00985",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23291",
      "title": "ScEdit: Script-based Assessment of Knowledge Editing",
      "authors": [
        "Xinye Li",
        "Zunwen Zheng",
        "Qian Zhang",
        "Dekai Zhuang",
        "Jiabao Kang",
        "Liyan Xu",
        "Qingbin Liu",
        "Xi Chen",
        "Zhiying Tu",
        "Dianhui Chu",
        "Dianbo Sui"
      ],
      "abstract": "Knowledge Editing (KE) has gained increasing attention, yet current KE tasks remain relatively simple. Under current evaluation frameworks, many editing methods achieve exceptionally high scores, sometimes nearing perfection. However, few studies integrate KE into real-world application scenarios (e.g., recent interest in LLM-as-agent). To support our analysis, we introduce a novel script-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) -- which encompasses both counterfactual and temporal edits. We integrate token-level and text-level evaluation methods, comprehensively analyzing existing KE techniques. The benchmark extends traditional fact-based (\"What\"-type question) evaluation to action-based (\"How\"-type question) evaluation. We observe that all KE methods exhibit a drop in performance on established metrics and face challenges on text-level metrics, indicating a challenging task. Our benchmark is available at https://github.com/asdfo123/ScEdit.",
      "arxiv_url": "https://arxiv.org/abs/2505.23291",
      "pdf_url": "https://arxiv.org/pdf/2505.23291",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "c06edd085eae1bfa694d880d6cb215a8c159f145",
      "title": "M³GQA: A Multi-Entity Multi-Hop Multi-Setting Graph Question Answering Benchmark",
      "authors": [
        "Boci Peng",
        "Yongchao Liu",
        "Xiaohe Bo",
        "Jiaxin Guo",
        "Yun Zhu",
        "Xuanbo Fan",
        "Chuntao Hong",
        "Yan Zhang"
      ],
      "abstract": "Recently, GraphRAG systems have achieved remarkable progress in enhancing the performance and reliability of large language models (LLMs). However, most previous benchmarks are template-based and primarily focus on few-entity queries, which are mono-typic and simplistic, failing to offer comprehensive and robust assessments. Besides, the lack of ground-truth reasoning paths also hinders the assessments of different components in GraphRAG systems. To address these limitations, we propose M 3 GQA, a complex, diverse, and high-quality GraphRAG benchmark focusing on multi-entity queries, with six distinct settings for comprehensive evaluation. In order to construct diverse data with semantically correct ground-truth reasoning paths, we introduce a novel reasoning-driven four-step data construction method, including tree sampling, reasoning path backtracking, query creation, and multi-stage refinement and filtering. Extensive experiments demonstrate that M 3 GQA effectively reflects the capabilities of GraphRAG methods, offering valuable insights into the model performance and reliability. By",
      "arxiv_url": "https://www.semanticscholar.org/paper/c06edd085eae1bfa694d880d6cb215a8c159f145",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "c0a73a9556433dd1d72935b3d750869f04f12f38",
      "title": "LIST: Linearly Incremental SQL Translator for Single-Hop Reasoning, Generation and Verification",
      "authors": [
        "Kaiyuan Guan",
        "Ruoxin Li",
        "Xudong Guo",
        "Zhenning Huang",
        "Xudong Weng",
        "Hehuan Liu",
        "Zheng Wei",
        "Zang Li"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/c0a73a9556433dd1d72935b3d750869f04f12f38",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14677",
      "title": "Data-Constrained Synthesis of Training Data for De-Identification",
      "authors": [
        "Thomas Vakili",
        "Aron Henriksson",
        "H. Dalianis"
      ],
      "abstract": "Many sensitive domains -- such as the clinical domain -- lack widely available datasets due to privacy risks. The increasing generative capabilities of large language models (LLMs) have made synthetic datasets a viable path forward. In this study, we domain-adapt LLMs to the clinical domain and generate synthetic clinical texts that are machine-annotated with tags for personally identifiable information using capable encoder-based NER models. The synthetic corpora are then used to train synthetic NER models. The results show that training NER models using synthetic corpora incurs only a small drop in predictive performance. The limits of this process are investigated in a systematic ablation study -- using both Swedish and Spanish data. Our analysis shows that smaller datasets can be sufficient for domain-adapting LLMs for data synthesis. Instead, the effectiveness of this process is almost entirely contingent on the performance of the machine-annotating NER models trained using the original data.",
      "arxiv_url": "https://arxiv.org/abs/2502.14677",
      "pdf_url": "https://arxiv.org/pdf/2502.14677",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11735",
      "title": "MT-RAIG: Novel Benchmark and Evaluation Framework for Retrieval-Augmented Insight Generation over Multiple Tables",
      "authors": [
        "Kwangwook Seo",
        "Donguk Kwon",
        "Dongha Lee"
      ],
      "abstract": "Recent advancements in table-based reasoning have expanded beyond factoid-level QA to address insight-level tasks, where systems should synthesize implicit knowledge in the table to provide explainable analyses. Although effective, existing studies remain confined to scenarios where a single gold table is given alongside the user query, failing to address cases where users seek comprehensive insights from multiple unknown tables. To bridge these gaps, we propose MT-RAIG Bench, design to evaluate systems on Retrieval-Augmented Insight Generation over Mulitple-Tables. Additionally, to tackle the suboptimality of existing automatic evaluation methods in the table domain, we further introduce a fine-grained evaluation framework MT-RAIG Eval, which achieves better alignment with human quality judgments on the generated insights. We conduct extensive experiments and reveal that even frontier LLMs still struggle with complex multi-table reasoning, establishing our MT-RAIG Bench as a challenging testbed for future research.",
      "arxiv_url": "https://arxiv.org/abs/2502.11735",
      "pdf_url": "https://arxiv.org/pdf/2502.11735",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.08319",
      "title": "Enhancing Automated Interpretability with Output-Centric Feature Descriptions",
      "authors": [
        "Yoav Gur-Arieh",
        "Roy Mayan",
        "Chen Agassy",
        "Atticus Geiger",
        "Mor Geva"
      ],
      "abstract": "Automated interpretability pipelines generate natural language descriptions for the concepts represented by features in large language models (LLMs), such as plants or the first word in a sentence. These descriptions are derived using inputs that activate the feature, which may be a dimension or a direction in the model's representation space. However, identifying activating inputs is costly, and the mechanistic role of a feature in model behavior is determined both by how inputs cause a feature to activate and by how feature activation affects outputs. Using steering evaluations, we reveal that current pipelines provide descriptions that fail to capture the causal effect of the feature on outputs. To fix this, we propose efficient, output-centric methods for automatically generating feature descriptions. These methods use the tokens weighted higher after feature stimulation or the highest weight tokens after applying the vocabulary\"unembedding\"head directly to the feature. Our output-centric descriptions better capture the causal effect of a feature on model outputs than input-centric descriptions, but combining the two leads to the best performance on both input and output evaluations. Lastly, we show that output-centric descriptions can be used to find inputs that activate features previously thought to be\"dead\".",
      "arxiv_url": "https://arxiv.org/abs/2501.08319",
      "pdf_url": "https://arxiv.org/pdf/2501.08319",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16667",
      "title": "ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive Programming",
      "authors": [
        "Xinwei Yang",
        "Zhaofeng Liu",
        "Chen Huang",
        "Jiashuai Zhang",
        "Tong Zhang",
        "Yifan Zhang",
        "Wenqiang Lei"
      ],
      "abstract": "While recent research increasingly emphasizes the value of human-LLM collaboration in competitive programming and proposes numerous empirical methods, a comprehensive understanding remains elusive due to the fragmented nature of existing studies and their use of diverse, application-specific human feedback. Thus, our work serves a three-fold purpose: First, we present the first taxonomy of human feedback consolidating the entire programming process, which promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a novel programming dataset specifically designed for human-LLM collaboration, meticulously annotated to enable large-scale simulated human feedback and facilitate costeffective real human interaction studies. Third, we introduce ELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM competitive programming. With ELABORATION, we pinpoint strengthes and weaknesses of existing methods, thereby setting the foundation for future improvement. Our code and dataset are available at https://github.com/SCUNLP/ELABORATION",
      "arxiv_url": "https://arxiv.org/abs/2505.16667",
      "pdf_url": "https://arxiv.org/pdf/2505.16667",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20704",
      "title": "Fuzzy Speculative Decoding for a Tunable Accuracy-Runtime Tradeoff",
      "authors": [
        "Maximilian Holsman",
        "Yukun Huang",
        "Bhuwan Dhingra"
      ],
      "abstract": "Speculative Decoding (SD) enforces strict distributional equivalence to the target model when accepting candidate tokens. While it maintains the target model's generation quality, this strict equivalence limits the speedup achievable by SD and prevents users from trading deviations from the target distribution in exchange for further inference speed gains. To address these limitations, we introduce Fuzzy Speculative Decoding (FSD) - a decoding algorithm that generalizes SD by accepting candidate tokens based on the divergences between the target and draft model distributions. By allowing for controlled divergence from the target model, FSD enables users to flexibly trade generation quality for inference speed. Across several benchmarks, our method is able to achieve significant runtime improvements of over 5 tokens per second faster than SD at only an approximate 2% absolute reduction in benchmark accuracy. In many cases, FSD is even able to match SD benchmark accuracy at over 2 tokens per second faster, demonstrating that distributional equivalence is not necessary to maintain target model performance. Furthermore, FSD can be seamlessly integrated into existing SD extensions; we demonstrate this by applying FSD to EAGLE-2, greatly enhancing this existing extension's efficiency while allowing it to leverage FSD's tunable quality-speed trade-off.",
      "arxiv_url": "https://arxiv.org/abs/2502.20704",
      "pdf_url": "https://arxiv.org/pdf/2502.20704",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.10993",
      "title": "RoseRAG: Robust Retrieval-augmented Generation with Small-scale LLMs via Margin-aware Preference Optimization",
      "authors": [
        "Tianci Liu",
        "Haoxiang Jiang",
        "Tianze Wang",
        "Ran Xu",
        "Yue Yu",
        "Linjun Zhang",
        "Tuo Zhao",
        "Haoyu Wang"
      ],
      "abstract": "Large language models (LLMs) have achieved impressive performance but face high computational costs and latency, limiting their deployment in resource-constrained settings. In contrast, small-scale LLMs (SLMs) are more efficient yet struggle to capture evolving real-world knowledge. Retrieval-augmented generation (RAG) helps by integrating external knowledge, but imperfect retrieval can introduce distracting noise that misleads SLMs. We propose RoseRAG, a robust RAG framework for SLMs via Margin-aware Preference Optimization. RoseRAG employs multi-turn prompting for detailed reasoning, rejection sampling for high-quality explanations, and contrastive preference selection to refine responses by maximizing the likelihood gap between preferred and non-preferred outputs. By integrating these components into a margin-aware optimization process, RoseRAG robustly enhances the accuracy and reliability of SLMs for RAG applications. Extensive experiments on three open-domain question answering benchmarks indicate that our innovative RoseRAG surpasses state-of-the-art baselines significantly.",
      "arxiv_url": "https://arxiv.org/abs/2502.10993",
      "pdf_url": "https://arxiv.org/pdf/2502.10993",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19100",
      "title": "ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning",
      "authors": [
        "Yeyuan Wang",
        "Dehong Gao",
        "Rujiao Long",
        "Lei Yi",
        "Linbo Jin",
        "Libin Yang",
        "Xiaoyan Cai"
      ],
      "abstract": "Direct Preference Optimization (DPO) has gained significant attention for its simplicity and computational efficiency in aligning large language models (LLMs). Recent advancements have extended DPO to multimodal scenarios, achieving strong performance. However, traditional DPO relies on binary preference optimization, rewarding or penalizing entire responses without considering fine-grained segment correctness, leading to suboptimal solutions. The root of this issue lies in the absence of fine-grained supervision during the optimization process. To address this, we propose Adaptive Sentence-level Preference Optimization (ASPO), which evaluates individual sentences for more precise preference optimization. By dynamically calculating adaptive rewards at the sentence level based on model predictions, ASPO enhances response content assessment without additional models or parameters. This significantly improves the alignment of multimodal features. Extensive experiments show that ASPO substantially enhances the overall performance of multimodal models.",
      "arxiv_url": "https://arxiv.org/abs/2505.19100",
      "pdf_url": "https://arxiv.org/pdf/2505.19100",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19621",
      "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models",
      "authors": [
        "George Kour",
        "Itay Nakash",
        "Ateret Anaby-Tavor",
        "Michal Shmueli-Scheuer"
      ],
      "abstract": "As Large Language Models (LLMs) become deeply integrated into human life and increasingly influence decision-making, it's crucial to evaluate whether and to what extent they exhibit subjective preferences, opinions, and beliefs. These tendencies may stem from biases within the models, which may shape their behavior, influence the advice and recommendations they offer to users, and potentially reinforce certain viewpoints. This paper presents the Preference, Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs' subjective inclinations across societal, cultural, ethical, and personal domains. We applied our benchmark to evaluate leading open- and closed-source LLMs, measuring desired properties such as reliability, neutrality, and consistency. In addition, we investigated the effect of increasing the test-time compute, through reasoning and self-reflection mechanisms, on those metrics. While effective in other tasks, our results show that these mechanisms offer only limited gains in our domain. Furthermore, we reveal that newer model versions are becoming less consistent and more biased toward specific viewpoints, highlighting a blind spot and a concerning trend. POBS: https://ibm.github.io/POBS",
      "arxiv_url": "https://arxiv.org/abs/2505.19621",
      "pdf_url": "https://arxiv.org/pdf/2505.19621",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13946",
      "title": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region",
      "authors": [
        "Chak Tou Leong",
        "Qingyu Yin",
        "Jian Wang",
        "Wenjie Li"
      ],
      "abstract": "The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region.",
      "arxiv_url": "https://arxiv.org/abs/2502.13946",
      "pdf_url": "https://arxiv.org/pdf/2502.13946",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.11314",
      "title": "Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large Language Models via Representation Engineering",
      "authors": [
        "Xinyu Tang",
        "Xiaolei Wang",
        "Zhihao Lv",
        "Yingqian Min",
        "Wayne Xin Zhao",
        "Binbin Hu",
        "Ziqi Liu",
        "Zhiqiang Zhang"
      ],
      "abstract": "Recent advancements in long chain-of-thoughts(long CoTs) have significantly improved the reasoning capabilities of large language models(LLMs). Existing work finds that the capability of long CoT reasoning can be efficiently elicited by tuning on only a few examples and can easily transfer to other tasks. This motivates us to investigate whether long CoT reasoning is a general capability for LLMs. In this work, we conduct an empirical analysis for this question from the perspective of representation. We find that LLMs do encode long CoT reasoning as a general capability, with a clear distinction from vanilla CoTs. Furthermore, domain-specific representations are also required for the effective transfer of long CoT reasoning. Inspired by these findings, we propose GLoRE, a novel representation engineering method to unleash the general long CoT reasoning capabilities of LLMs. Extensive experiments demonstrate the effectiveness and efficiency of GLoRE in both in-domain and cross-domain scenarios.",
      "arxiv_url": "https://arxiv.org/abs/2503.11314",
      "pdf_url": "https://arxiv.org/pdf/2503.11314",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19175",
      "title": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis",
      "authors": [
        "Daniel Rose",
        "Chia-Chien Hung",
        "Marco Lepri",
        "Israa Alqassem",
        "Kiril Gashteovski",
        "Carolin Lawrence"
      ],
      "abstract": "Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical decision-making, in which physicians iteratively refine a ranked list of possible diseases based on symptoms, antecedents, and medical knowledge. While recent advances in large language models (LLMs) have shown promise in supporting DDx, existing approaches face key limitations, including single-dataset evaluations, isolated optimization of components, unrealistic assumptions about complete patient profiles, and single-attempt diagnosis. We introduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for interactive DDx, where diagnostic reasoning evolves through iterative learning, rather than assuming a complete patient profile is accessible. MEDDxAgent integrates three modular components: (1) an orchestrator (DDxDriver), (2) a history taking simulator, and (3) two specialized agents for knowledge retrieval and diagnosis strategy. To ensure robust evaluation, we introduce a comprehensive DDx benchmark covering respiratory, skin, and rare diseases. We analyze single-turn diagnostic approaches and demonstrate the importance of iterative refinement when patient profiles are not available at the outset. Our broad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy improvements in interactive DDx across both large and small LLMs, while offering critical explainability into its diagnostic reasoning process.",
      "arxiv_url": "https://arxiv.org/abs/2502.19175",
      "pdf_url": "https://arxiv.org/pdf/2502.19175",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.02772",
      "title": "GeAR: Generation Augmented Retrieval",
      "authors": [
        "Haoyu Liu",
        "Shaohan Huang",
        "Jianfeng Liu",
        "Yuefeng Zhan",
        "Hao Sun",
        "Weiwei Deng",
        "Feng Sun",
        "Furu Wei",
        "Qi Zhang"
      ],
      "abstract": "Document retrieval techniques are essential for developing large-scale information systems. The common approach involves using a bi-encoder to compute the semantic similarity between a query and documents. However, the scalar similarity often fail to reflect enough information, hindering the interpretation of retrieval results. In addition, this process primarily focuses on global semantics, overlooking the finer-grained semantic relationships between the query and the document's content. In this paper, we introduce a novel method, $\\textbf{Ge}$neration $\\textbf{A}$ugmented $\\textbf{R}$etrieval ($\\textbf{GeAR}$), which not only improves the global document-query similarity through contrastive learning, but also integrates well-designed fusion and decoding modules. This enables GeAR to generate relevant context within the documents based on a given query, facilitating learning to retrieve local fine-grained information. Furthermore, when used as a retriever, GeAR does not incur any additional computational cost over bi-encoders. GeAR exhibits competitive retrieval performance across diverse scenarios and tasks. Moreover, qualitative analysis and the results generated by GeAR provide novel insights into the interpretation of retrieval results. The code, data, and models will be released at \\href{https://github.com/microsoft/LMOps}{https://github.com/microsoft/LMOps}.",
      "arxiv_url": "https://arxiv.org/abs/2501.02772",
      "pdf_url": "https://arxiv.org/pdf/2501.02772",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18683",
      "title": "TULUN: Transparent and Adaptable Low-resource Machine Translation",
      "authors": [
        "Raphael Merx",
        "Hanna Suominen",
        "Lois Hong",
        "Nick Thieberger",
        "Trevor Cohn",
        "Ekaterina Vylomova"
      ],
      "abstract": "Machine translation (MT) systems that support low-resource languages often struggle on specialized domains. While researchers have proposed various techniques for domain adaptation, these approaches typically require model fine-tuning, making them impractical for non-technical users and small organizations. To address this gap, we propose Tulun, a versatile solution for terminology-aware translation, combining neural MT with large language model (LLM)-based post-editing guided by existing glossaries and translation memories. Our open-source web-based platform enables users to easily create, edit, and leverage terminology resources, fostering a collaborative human-machine translation process that respects and incorporates domain expertise while increasing MT accuracy. Evaluations show effectiveness in both real-world and benchmark scenarios: on medical and disaster relief translation tasks for Tetun and Bislama, our system achieves improvements of 16.90-22.41 ChrF++ points over baseline MT systems. Across six low-resource languages on the FLORES dataset, Tulun outperforms both standalone MT and LLM approaches, achieving an average improvement of 2.8 ChrF points over NLLB-54B.",
      "arxiv_url": "https://arxiv.org/abs/2505.18683",
      "pdf_url": "https://arxiv.org/pdf/2505.18683",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.11316",
      "title": "Internal Value Alignment in Large Language Models through Controlled Value Vector Activation",
      "authors": [
        "Haoran Jin",
        "Meng Li",
        "Xiting Wang",
        "Zhihao Xu",
        "Minlie Huang",
        "Yantao Jia",
        "Defu Lian"
      ],
      "abstract": "Aligning Large Language Models (LLMs) with human values has attracted increasing attention since it provides clarity, transparency, and the ability to adapt to evolving scenarios. In this paper, we introduce a Controlled Value Vector Activation (ConVA) method that directly aligns the internal values of LLMs by interpreting how a value is encoded in their latent representations and modifies relevant activations to ensure consistent values in LLMs. To ensure an accurate and unbiased interpretation, we propose a context-controlled value vector identification method. To consistently control values without sacrificing model performance, we introduce a gated value vector activation method for effective and minimum degree of value control. Experiments show that our method achieves the highest control success rate across 10 basic values without hurting LLM performance and fluency, and ensures target values even with opposite and potentially malicious input prompts. Source code and data are available at~ https://github.com/hr-jin/ConVA.",
      "arxiv_url": "https://arxiv.org/abs/2507.11316",
      "pdf_url": "https://arxiv.org/pdf/2507.11316",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.21222",
      "title": "Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval",
      "authors": [
        "YongChan Chun",
        "Minhyuk Kim",
        "Dongjun Kim",
        "Chanjun Park",
        "Heu-Jeoung Lim"
      ],
      "abstract": "Automatic Term Extraction (ATE) identifies domain-specific expressions that are crucial for downstream tasks such as machine translation and information retrieval. Although large language models (LLMs) have significantly advanced various NLP tasks, their potential for ATE has scarcely been examined. We propose a retrieval-based prompting strategy that, in the few-shot setting, selects demonstrations according to \\emph{syntactic} rather than semantic similarity. This syntactic retrieval method is domain-agnostic and provides more reliable guidance for capturing term boundaries. We evaluate the approach in both in-domain and cross-domain settings, analyzing how lexical overlap between the query sentence and its retrieved examples affects performance. Experiments on three specialized ATE benchmarks show that syntactic retrieval improves F1-score. These findings highlight the importance of syntactic cues when adapting LLMs to terminology-extraction tasks.",
      "arxiv_url": "https://arxiv.org/abs/2506.21222",
      "pdf_url": "https://arxiv.org/pdf/2506.21222",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.06207",
      "title": "Is LLM an Overconfident Judge? Unveiling the Capabilities of LLMs in Detecting Offensive Language with Annotation Disagreement",
      "authors": [
        "Junyu Lu",
        "Kai Ma",
        "Kaichun Wang",
        "Kelaiti Xiao",
        "Roy Ka-Wei Lee",
        "Bo Xu",
        "Liang Yang",
        "Hongfei Lin"
      ],
      "abstract": "Large Language Models (LLMs) have become essential for offensive language detection, yet their ability to handle annotation disagreement remains underexplored. Disagreement samples, which arise from subjective interpretations, pose a unique challenge due to their ambiguous nature. Understanding how LLMs process these cases, particularly their confidence levels, can offer insight into their alignment with human annotators. This study systematically evaluates the performance of multiple LLMs in detecting offensive language at varying levels of annotation agreement. We analyze binary classification accuracy, examine the relationship between model confidence and human disagreement, and explore how disagreement samples influence model decision-making during few-shot learning and instruction fine-tuning. Our findings reveal that LLMs struggle with low-agreement samples, often exhibiting overconfidence in these ambiguous cases. However, utilizing disagreement samples in training improves both detection accuracy and model alignment with human judgment. These insights provide a foundation for enhancing LLM-based offensive language detection in real-world moderation tasks.",
      "arxiv_url": "https://arxiv.org/abs/2502.06207",
      "pdf_url": "https://arxiv.org/pdf/2502.06207",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15132",
      "title": "CoT-ICL Lab: A Synthetic Framework for Studying Chain-of-Thought Learning from In-Context Demonstrations",
      "authors": [
        "Vignesh Kothapalli",
        "Hamed Firooz",
        "Maziar Sanjabi"
      ],
      "abstract": "We introduce CoT-ICL Lab, a framework and methodology to generate synthetic tokenized datasets and systematically study chain-of-thought (CoT) in-context learning (ICL) in language models. CoT-ICL Lab allows fine grained control over the complexity of in-context examples by decoupling (1) the causal structure involved in chain token generation from (2) the underlying token processing functions. We train decoder-only transformers (up to 700M parameters) on these datasets and show that CoT accelerates the accuracy transition to higher values across model sizes. In particular, we find that model depth is crucial for leveraging CoT with limited in-context examples, while more examples help shallow models match deeper model performance. Additionally, limiting the diversity of token processing functions throughout training improves causal structure learning via ICL. We also interpret these transitions by analyzing transformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a simple yet powerful testbed for theoretical and empirical insights into ICL and CoT in language models.",
      "arxiv_url": "https://arxiv.org/abs/2502.15132",
      "pdf_url": "https://arxiv.org/pdf/2502.15132",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21937",
      "title": "Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages",
      "authors": [
        "P. Singh",
        "Kritarth Prasad",
        "Mohammadi Zaki",
        "Pankaj Wasnik"
      ],
      "abstract": "Translating multi-word expressions (MWEs) and idioms requires a deep understanding of the cultural nuances of both the source and target languages. This challenge is further amplified by the one-to-many nature of idiomatic translations, where a single source idiom can have multiple target-language equivalents depending on cultural references and contextual variations. Traditional static knowledge graphs (KGs) and prompt-based approaches struggle to capture these complex relationships, often leading to suboptimal translations. To address this, we propose IdiomCE, an adaptive graph neural network (GNN) based methodology that learns intricate mappings between idiomatic expressions, effectively generalizing to both seen and unseen nodes during training. Our proposed method enhances translation quality even in resource-constrained settings, facilitating improved idiomatic translation in smaller models. We evaluate our approach on multiple idiomatic translation datasets using reference-less metrics, demonstrating significant improvements in translating idioms from English to various Indian languages.",
      "arxiv_url": "https://arxiv.org/abs/2505.21937",
      "pdf_url": "https://arxiv.org/pdf/2505.21937",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00875",
      "title": "CC-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint Multilingual Supervised Fine-Tuning",
      "authors": [
        "Yangfan Ye",
        "Xiaocheng Feng",
        "Zekun Yuan",
        "Xiachong Feng",
        "Libo Qin",
        "Lei Huang",
        "Weitao Ma",
        "Yi-Chong Huang",
        "Zhirui Zhang",
        "Yunfei Lu",
        "Xiaohui Yan",
        "Duyu Tang",
        "Dandan Tu",
        "Bing Qin"
      ],
      "abstract": "Current large language models (LLMs) often exhibit imbalanced multilingual capabilities due to their English-centric training corpora. To address this, existing fine-tuning approaches operating at the data-level (e.g., through data augmentation or distillation) typically introduce implicit cross-lingual alignment, overlooking the potential for more profound, latent-level cross-lingual interactions. In this work, we propose CC-Tuning, a novel multilingual fine-tuning paradigm that explicitly establishes a cross-lingual connection mechanism at the latent level. During training, CC-Tuning fuses the feed forward activations from both English and non-English inputs, enabling the model to benefit from both linguistic resources. This process is facilitated with a trainable Decision Maker that identifies beneficial activations. Furthermore, during inference, a Transform Matrix is utilized to simulate the cross-lingual connection under monolingual setting through representation transformation. Our experiments on six benchmarks covering 22 languages show that CC-Tuning outperforms vanilla SFT and offers a strong latent-level alternative to data-level augmentation methods. Further analysis also highlights the practicality of CC-Tuning and the potential of latent-level cross-lingual interactions in advancing the multilingual performance of LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.00875",
      "pdf_url": "https://arxiv.org/pdf/2506.00875",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.14492",
      "title": "FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering",
      "authors": [
        "Yichen Li",
        "Zhiting Fan",
        "Ruizhe Chen",
        "Xiaotang Gai",
        "Luqi Gong",
        "Yan Zhang",
        "Zuozhu Liu"
      ],
      "abstract": "Large language models (LLMs) are prone to capturing biases from training corpus, leading to potential negative social impacts. Existing prompt-based debiasing methods exhibit instability due to their sensitivity to prompt changes, while fine-tuning-based techniques incur substantial computational overhead and catastrophic forgetting. In this paper, we propose FairSteer, a novel inference-time debiasing framework without requiring customized prompt design or model retraining. Motivated by the linear representation hypothesis, our preliminary investigation demonstrates that fairness-related features can be encoded into separable directions in the hidden activation space. FairSteer operates in three steps: biased activation detection, debiasing steering vector (DSV) computation, and dynamic activation steering. Specifically, it first trains a lightweight linear classifier to detect bias signatures in activations, and then computes DSVs as intervention directions derived from small contrastive prompt pairs. Subsequently, it performs debiasing by adjusting activations with DSVs in the inference stage. Comprehensive evaluation with six LLMs demonstrates the superiority of FairSteer across question-answering, counterfactual input evaluation and open-ended text generation tasks. Code will be released.",
      "arxiv_url": "https://arxiv.org/abs/2504.14492",
      "pdf_url": "https://arxiv.org/pdf/2504.14492",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.12158",
      "title": "The AI Gap: How Socioeconomic Status Affects Language Technology Interactions",
      "authors": [
        "Elisa Bassignana",
        "A. C. Curry",
        "Dirk Hovy"
      ],
      "abstract": "Socioeconomic status (SES) fundamentally influences how people interact with each other and more recently, with digital technologies like Large Language Models (LLMs). While previous research has highlighted the interaction between SES and language technology, it was limited by reliance on proxy metrics and synthetic data. We survey 1,000 individuals from diverse socioeconomic backgrounds about their use of language technologies and generative AI, and collect 6,482 prompts from their previous interactions with LLMs. We find systematic differences across SES groups in language technology usage (i.e., frequency, performed tasks), interaction styles, and topics. Higher SES entails a higher level of abstraction, convey requests more concisely, and topics like 'inclusivity' and 'travel'. Lower SES correlates with higher anthropomorphization of LLMs (using ''hello'' and ''thank you'') and more concrete language. Our findings suggest that while generative language technologies are becoming more accessible to everyone, socioeconomic linguistic differences still stratify their use to exacerbate the digital divide. These differences underscore the importance of considering SES in developing language technologies to accommodate varying linguistic needs rooted in socioeconomic factors and limit the AI Gap across SES groups.",
      "arxiv_url": "https://arxiv.org/abs/2505.12158",
      "pdf_url": "https://arxiv.org/pdf/2505.12158",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11671",
      "title": "Diversity-Oriented Data Augmentation with Large Language Models",
      "authors": [
        "Zaitian Wang",
        "Jinghan Zhang",
        "XinHao Zhang",
        "Kunpeng Liu",
        "Pengfei Wang",
        "Yuanchun Zhou"
      ],
      "abstract": "Data augmentation is an essential technique in natural language processing (NLP) for enriching training datasets by generating diverse samples. This process is crucial for improving the robustness and generalization capabilities of NLP models. However, a significant challenge remains: \\textit{Insufficient Attention to Sample Distribution Diversity}. Most existing methods focus on increasing the sample numbers while neglecting the sample distribution diversity, which can lead to model overfitting. In response, we explore data augmentation's impact on dataset diversity and propose a \\textbf{\\underline{D}}iversity-\\textbf{\\underline{o}}riented data \\textbf{\\underline{Aug}}mentation framework (\\textbf{DoAug}). % \\(\\mathscr{DoAug}\\) Specifically, we utilize a diversity-oriented fine-tuning approach to train an LLM as a diverse paraphraser, which is capable of augmenting textual datasets by generating diversified paraphrases. Then, we apply the LLM paraphraser to a selected coreset of highly informative samples and integrate the paraphrases with the original data to create a more diverse augmented dataset. Finally, we conduct extensive experiments on 12 real-world textual datasets. The results show that our fine-tuned LLM augmenter improves diversity while preserving label consistency, thereby enhancing the robustness and performance of downstream tasks. Specifically, it achieves an average performance gain of \\(10.52\\%\\), surpassing the runner-up baseline with more than three percentage points.",
      "arxiv_url": "https://arxiv.org/abs/2502.11671",
      "pdf_url": "https://arxiv.org/pdf/2502.11671",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.15333",
      "title": "Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation",
      "authors": [
        "Yuhao Zhang",
        "Xiangnan Ma",
        "Kaiqi Kou",
        "Peizhuo Liu",
        "Weiqiao Shan",
        "Benyou Wang",
        "Tong Xiao",
        "Yuxin Huang",
        "Zheng Yu",
        "Jingbo Zhu"
      ],
      "abstract": "The success of building textless speech-to-speech translation (S2ST) models has attracted much attention. However, S2ST still faces two main challenges: 1) extracting linguistic features for various speech signals, called cross-modal (CM), and 2) learning alignment of difference languages in long sequences, called cross-lingual (CL). We propose the unit language to overcome the two modeling challenges. The unit language can be considered a text-like representation format, constructed using $n$-gram language modeling. We implement multi-task learning to utilize the unit language in guiding the speech modeling process. Our initial results reveal a conflict when applying source and target unit languages simultaneously. We propose task prompt modeling to mitigate this conflict. We conduct experiments on four languages of the Voxpupil dataset. Our method demonstrates significant improvements over a strong baseline and achieves performance comparable to models trained with text.",
      "arxiv_url": "https://arxiv.org/abs/2505.15333",
      "pdf_url": "https://arxiv.org/pdf/2505.15333",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00449",
      "title": "Rehearse With User: Personalized Opinion Summarization via Role-Playing based on Large Language Models",
      "authors": [
        "Yanyue Zhang",
        "Yulan He",
        "Deyu Zhou"
      ],
      "abstract": "Personalized opinion summarization is crucial as it considers individual user interests while generating product summaries. Recent studies show that although large language models demonstrate powerful text summarization and evaluation capabilities without the need for training data, they face difficulties in personalized tasks involving long texts. To address this, \\textbf{Rehearsal}, a personalized opinion summarization framework via LLMs-based role-playing is proposed. Having the model act as the user, the model can better understand the user's personalized needs. Additionally, a role-playing supervisor and practice process are introduced to improve the role-playing ability of the LLMs, leading to a better expression of user needs. Furthermore, through suggestions from virtual users, the summary generation is intervened, ensuring that the generated summary includes information of interest to the user, thus achieving personalized summary generation. Experiment results demonstrate that our method can effectively improve the level of personalization in large model-generated summaries.",
      "arxiv_url": "https://arxiv.org/abs/2503.00449",
      "pdf_url": "https://arxiv.org/pdf/2503.00449",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-03-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.08504",
      "title": "CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations",
      "authors": [
        "Divyaksh Shukla",
        "Ritesh Baviskar",
        "Dwijesh Gohil",
        "Aniket Tiwari",
        "Atul Shree",
        "Ashutosh Modi"
      ],
      "abstract": "Discourse parsing is an important task useful for NLU applications such as summarization, machine comprehension, and emotion recognition. The current discourse parsing datasets based on conversations consists of written English dialogues restricted to a single domain. In this resource paper, we introduce CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations. The corpus (code-mixed in Hindi and English) has both audio and transcribed text and is annotated with nine discourse relations. We experiment with various SoTA baseline models; the poor performance of SoTA models highlights the challenges of multi-domain code-mixed corpus, pointing towards the need for developing better models for such realistic settings.",
      "arxiv_url": "https://arxiv.org/abs/2506.08504",
      "pdf_url": "https://arxiv.org/pdf/2506.08504",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19765",
      "title": "EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models",
      "authors": [
        "Che Hyun Lee",
        "Heeseung Kim",
        "Ji-Ran Yeom",
        "Sungroh Yoon"
      ],
      "abstract": "We propose EdiText, a controllable text editing method that modifies the reference text to desired attributes at various scales. We integrate an SDEdit-based editing technique that allows for broad adjustments in the degree of text editing. Additionally, we introduce a novel fine-level editing method based on self-conditioning, which allows subtle control of reference text. While being capable of editing on its own, this fine-grained method, integrated with the SDEdit approach, enables EdiText to make precise adjustments within the desired range. EdiText demonstrates its controllability to robustly adjust reference text at a broad range of levels across various tasks, including toxicity control and sentiment control.",
      "arxiv_url": "https://arxiv.org/abs/2502.19765",
      "pdf_url": "https://arxiv.org/pdf/2502.19765",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.06186",
      "title": "LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs",
      "authors": [
        "Omkar Thawakar",
        "Dinura Dissanayake",
        "Ketan More",
        "Ritesh Thawkar",
        "Ahmed Heakl",
        "Noor Ahsan",
        "Yuhao Li",
        "Mohammed Zumri",
        "Jean Lahoud",
        "R. Anwer",
        "Hisham Cholakkal",
        "Ivan Laptev",
        "Mubarak Shah",
        "F. Khan",
        "Salman H. Khan"
      ],
      "abstract": "Reasoning is a fundamental capability for solving complex multi-step problems, particularly in visual contexts where sequential step-wise understanding is essential. Existing approaches lack a comprehensive framework for evaluating visual reasoning and do not emphasize step-wise problem-solving. To this end, we propose a comprehensive framework for advancing step-by-step visual reasoning in large language models (LMMs) through three key contributions. First, we introduce a visual reasoning benchmark specifically designed to evaluate multi-step reasoning tasks. The benchmark presents a diverse set of challenges with eight different categories ranging from complex visual perception to scientific reasoning with over 4k reasoning steps in total, enabling robust evaluation of LLMs' abilities to perform accurate and interpretable visual reasoning across multiple steps. Second, we propose a novel metric that assesses visual reasoning quality at the granularity of individual steps, emphasizing both correctness and logical coherence. The proposed metric offers deeper insights into reasoning performance compared to traditional end-task accuracy metrics. Third, we present a new multimodal visual reasoning model, named LlamaV-o1, trained using a multi-step curriculum learning approach, where tasks are progressively organized to facilitate incremental skill acquisition and problem-solving. The proposed LlamaV-o1 is designed for multi-step reasoning and learns step-by-step through a structured training paradigm. Extensive experiments show that our LlamaV-o1 outperforms existing open-source models and performs favorably against close-source proprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an average score of 67.3 with an absolute gain of 3.8\\% across six benchmarks while being 5 times faster during inference scaling. Our benchmark, model, and code are publicly available.",
      "arxiv_url": "https://arxiv.org/abs/2501.06186",
      "pdf_url": "https://arxiv.org/pdf/2501.06186",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.11273",
      "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding",
      "authors": [
        "Luohe Shi",
        "Z. Li",
        "Lefei Zhang",
        "Baoyuan Qi",
        "Guoming Liu",
        "Hai Zhao"
      ],
      "abstract": "Large language models (LLMs) based on Transformer Decoders have become the preferred choice for conversational generative AI. Despite the overall superiority of the Decoder architecture, the gradually increasing Key-Value (KV) cache during inference has emerged as a primary efficiency bottleneck, both in aspects of memory consumption and data transfer bandwidth limitations. To address these challenges, we propose a paradigm called KV-Latent. By down-sampling the Key-Value vector dimensions into a latent space, we can significantly reduce the KV Cache footprint and improve inference speed, only with a small amount of extra training, less than 1\\% of pre-training takes. Besides, we enhanced the stability of Rotary Positional Embedding applied on lower-dimensional vectors by modifying its frequency sampling mechanism, avoiding noise introduced by higher frequencies while retaining position attenuation. Our experiments, including both models with Grouped Query Attention and those without, have yielded satisfactory results. Finally, we conducted comparative experiments to study the impact of separately reducing Key and Value components on model's performance. Our approach allows for the construction of more efficient language model systems, and opens the new possibility on KV Cache saving and efficient LLMs. Our code is available at https://github.com/ShiLuohe/KV-Latent.",
      "arxiv_url": "https://arxiv.org/abs/2507.11273",
      "pdf_url": "https://arxiv.org/pdf/2507.11273",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "c4a8c6b0fdb0ac0fc9cfa0b6148649728a10b3bb",
      "title": "Boosting Policy and Process Reward Models with Monte Carlo Tree Search in Open-Domain QA",
      "authors": [
        "Chi-Min Chan",
        "Chunpu Xu",
        "Junqi Zhu",
        "Jiaming Ji",
        "Donghai Hong",
        "Pengcheng Wen",
        "Chunyang Jiang",
        "Zhen Ye",
        "Yaodong Yang",
        "Wei Xue",
        "Sirui Han",
        "Yike Guo"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/c4a8c6b0fdb0ac0fc9cfa0b6148649728a10b3bb",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01670",
      "title": "Evaluating LLMs' Assessment of Mixed-Context Hallucination Through the Lens of Summarization",
      "authors": [
        "Siya Qi",
        "Rui Cao",
        "Yulan He",
        "Zheng Yuan"
      ],
      "abstract": "With the rapid development of large language models (LLMs), LLM-as-a-judge has emerged as a widely adopted approach for text quality evaluation, including hallucination evaluation. While previous studies have focused exclusively on single-context evaluation (e.g., discourse faithfulness or world factuality), real-world hallucinations typically involve mixed contexts, which remains inadequately evaluated. In this study, we use summarization as a representative task to comprehensively evaluate LLMs' capability in detecting mixed-context hallucinations, specifically distinguishing between factual and non-factual hallucinations. Through extensive experiments across direct generation and retrieval-based models of varying scales, our main observations are: (1) LLMs' intrinsic knowledge introduces inherent biases in hallucination evaluation; (2) These biases particularly impact the detection of factual hallucinations, yielding a significant performance bottleneck; (3) The fundamental challenge lies in effective knowledge utilization, balancing between LLMs' intrinsic knowledge and external context for accurate mixed-context hallucination evaluation.",
      "arxiv_url": "https://arxiv.org/abs/2503.01670",
      "pdf_url": "https://arxiv.org/pdf/2503.01670",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04277",
      "title": "RSVP: Reasoning Segmentation via Visual Prompting and Multi-modal Chain-of-Thought",
      "authors": [
        "Yi Lu",
        "Jiawang Cao",
        "Yongliang Wu",
        "Bozheng Li",
        "Licheng Tang",
        "Yangguang Ji",
        "Chong Wu",
        "Jay Wu",
        "Wenbo Zhu"
      ],
      "abstract": "Multi-modal Large Language Models (MLLMs) have demonstrated remarkable reasoning capability while lack explicit mechanisms for visual grounding and segmentation, creating a gap between cognitive reasoning and visual perception. To bridge this gap, we introduce Reasoning Segmentation via Visual Prompting (RSVP), a novel framework that unifies multi-step multimodal reasoning with grounded visual understanding. RSVP is a two-stage structuralized framework that integrates reasoning-driven localization with segmentation refinement. In the reasoning stage, RSVP employs multimodal chain-of-thought visual prompts to help MLLMs understand queries and infer targets, generating interpretable region proposals that enhance visual grounding. In segmentation stage, RSVP refines these proposals with a Vision-Language Segmentation Module (VLSM), seamlessly integrates textual and visual cues to produce precise segmentation masks. By explicitly modelling the interaction between multimodal reasoning and segmentation, RSVP introduces a new paradigm for interpretable reasoning segmentation. It exploits MLLMs' inherent localization capabilities, enabling the models to not only reason about objects but also generate structured visual representations. Our extensive experiments demonstrate that RSVP achieves state-of-the-art performance, surpasses state-of-the-art methods by up to +6.5 gIoU and +9.2 cIoU on ReasonSeg, and achieves 49.7 mAP on SegInW under zero-shot settings. These results validate RSVP as an effective and scalable framework for integrating cognitive reasoning with structured visual understanding.",
      "arxiv_url": "https://arxiv.org/abs/2506.04277",
      "pdf_url": "https://arxiv.org/pdf/2506.04277",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19428",
      "title": "Frictional Agent Alignment Framework: Slow Down and Don't Break Things",
      "authors": [
        "Abhijnan Nath",
        "Carine Graff",
        "Andrei Bachinin",
        "Nikhil Krishnaswamy"
      ],
      "abstract": "AI support of collaborative interactions entails mediating potential misalignment between interlocutor beliefs. Common preference alignment methods like DPO excel in static settings, but struggle in dynamic collaborative tasks where the explicit signals of interlocutor beliefs are sparse and skewed. We propose the Frictional Agent Alignment Framework (FAAF), to generate precise, context-aware\"friction\"that prompts for deliberation and re-examination of existing evidence. FAAF's two-player objective decouples from data skew: a frictive-state policy identifies belief misalignments, while an intervention policy crafts collaborator-preferred responses. We derive an analytical solution to this objective, enabling training a single policy via a simple supervised loss. Experiments on three benchmarks show FAAF outperforms competitors in producing concise, interpretable friction and in OOD generalization. By aligning LLMs to act as adaptive\"thought partners\"-- not passive responders -- FAAF advances scalable, dynamic human-AI collaboration. Our code and data can be found at https://github.com/csu-signal/FAAF_ACL.",
      "arxiv_url": "https://arxiv.org/abs/2505.19428",
      "pdf_url": "https://arxiv.org/pdf/2505.19428",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.08161",
      "title": "OASIS: Order-Augmented Strategy for Improved Code Search",
      "authors": [
        "Zuchen Gao",
        "Zizheng Zhan",
        "Xianming Li",
        "Erxin Yu",
        "Haotian Zhang",
        "Bin Chen",
        "Yuqun Zhang",
        "Jing Li"
      ],
      "abstract": "Code embeddings capture the semantic representations of code and are crucial for various code-related large language model (LLM) applications, such as code search. Previous training primarily relies on optimizing the InfoNCE loss by comparing positive natural language (NL)-code pairs with in-batch negatives. However, due to the sparse nature of code contexts, training solely by comparing the major differences between positive and negative pairs may fail to capture deeper semantic nuances. To address this issue, we propose a novel order-augmented strategy for improved code search (OASIS). It leverages order-based similarity labels to train models to capture subtle differences in similarity among negative pairs. Extensive benchmark evaluations demonstrate that our OASIS model significantly outperforms previous state-of-the-art models focusing solely on major positive-negative differences. It underscores the value of exploiting subtle differences among negative pairs with order labels for effective code embedding training.",
      "arxiv_url": "https://arxiv.org/abs/2503.08161",
      "pdf_url": "https://arxiv.org/pdf/2503.08161",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13996",
      "title": "Beyond Single-Value Metrics: Evaluating and Enhancing LLM Unlearning with Cognitive Diagnosis",
      "authors": [
        "Yicheng Lang",
        "Kehan Guo",
        "Yue Huang",
        "Yujun Zhou",
        "Haomin Zhuang",
        "Tianyu Yang",
        "Yao Su",
        "Xiangliang Zhang"
      ],
      "abstract": "Due to the widespread use of LLMs and the rising critical ethical and safety concerns, LLM unlearning methods have been developed to remove harmful knowledge and undesirable capabilities. In this context, evaluations are mostly based on single-value metrics such as QA accuracy. However, these metrics often fail to capture the nuanced retention of harmful knowledge components, making it difficult to assess the true effectiveness of unlearning. To address this issue, we propose UNCD (UNlearning evaluation via Cognitive Diagnosis), a novel framework that leverages Cognitive Diagnosis Modeling for fine-grained evaluation of LLM unlearning. Our dedicated benchmark, UNCD-Cyber, provides a detailed assessment of the removal of dangerous capabilities. Moreover, we introduce UNCD-Agent, which refines unlearning by diagnosing knowledge remnants and generating targeted unlearning data. Extensive experiments across eight unlearning methods and two base models demonstrate that UNCD not only enhances evaluation but also effectively facilitates the removal of harmful LLM abilities.",
      "arxiv_url": "https://arxiv.org/abs/2502.13996",
      "pdf_url": "https://arxiv.org/pdf/2502.13996",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.18263",
      "title": "Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models",
      "authors": [
        "Suhang Wu",
        "Jialong Tang",
        "Chengyi Yang",
        "Pei Zhang",
        "Baosong Yang",
        "Junhui Li",
        "Junfeng Yao",
        "Min Zhang",
        "Jinsong Su"
      ],
      "abstract": "Direct speech translation (ST) has garnered increasing attention nowadays, yet the accurate translation of terminology within utterances remains a great challenge. In this regard, current studies mainly concentrate on leveraging various translation knowledge into ST models. However, these methods often struggle with interference from irrelevant noise and can not fully utilize the translation knowledge. To address these issues, in this paper, we propose a novel Locate-and-Focus method for terminology translation. It first effectively locates the speech clips containing terminologies within the utterance to construct translation knowledge, minimizing irrelevant information for the ST model. Subsequently, it associates the translation knowledge with the utterance and hypothesis from both audio and textual modalities, allowing the ST model to better focus on translation knowledge during translation. Experimental results across various datasets demonstrate that our method effectively locates terminologies within utterances and enhances the success rate of terminology translation, while maintaining robust general translation performance.",
      "arxiv_url": "https://arxiv.org/abs/2507.18263",
      "pdf_url": "https://arxiv.org/pdf/2507.18263",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.11902",
      "title": "TreeRL: LLM Reinforcement Learning with On-Policy Tree Search",
      "authors": [
        "Zhenyu Hou",
        "Ziniu Hu",
        "Yujiang Li",
        "Rui Lu",
        "Jie Tang",
        "Yuxiao Dong"
      ],
      "abstract": "Reinforcement learning (RL) with tree search has demonstrated superior performance in traditional reasoning tasks. Compared to conventional independent chain sampling strategies with outcome supervision, tree search enables better exploration of the reasoning space and provides dense, on-policy process rewards during RL training but remains under-explored in On-Policy LLM RL. We propose TreeRL, a reinforcement learning framework that directly incorporates on-policy tree search for RL training. Our approach includes intermediate supervision and eliminates the need for a separate reward model training. Existing approaches typically train a separate process reward model, which can suffer from distribution mismatch and reward hacking. We also introduce a cost-effective tree search approach that achieves higher search efficiency under the same generation token budget by strategically branching from high-uncertainty intermediate steps rather than using random branching. Experiments on challenging math and code reasoning benchmarks demonstrate that TreeRL achieves superior performance compared to traditional ChainRL, highlighting the potential of tree search for LLM. TreeRL is open-sourced at https://github.com/THUDM/TreeRL.",
      "arxiv_url": "https://arxiv.org/abs/2506.11902",
      "pdf_url": "https://arxiv.org/pdf/2506.11902",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20871",
      "title": "Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG",
      "authors": [
        "Xin Sun",
        "Jianan Xie",
        "Zhongqi Chen",
        "Q. Liu",
        "Shu Wu",
        "Yuehe Chen",
        "Bowen Song",
        "Weiqiang Wang",
        "Zilei Wang",
        "Liang Wang"
      ],
      "abstract": "Large language models (LLMs) augmented with retrieval systems have significantly advanced natural language processing tasks by integrating external knowledge sources, enabling more accurate and contextually rich responses. To improve the robustness of such systems against noisy retrievals, Retrieval-Augmented Fine-Tuning (RAFT) has emerged as a widely adopted method. However, RAFT conditions models to generate answers even in the absence of reliable knowledge. This behavior undermines their reliability in high-stakes domains, where acknowledging uncertainty is critical. To address this issue, we propose Divide-Then-Align (DTA), a post-training approach designed to endow RAG systems with the ability to respond with\"I don't know\"when the query is out of the knowledge boundary of both the retrieved passages and the model's internal knowledge. DTA divides data samples into four knowledge quadrants and constructs tailored preference data for each quadrant, resulting in a curated dataset for Direct Preference Optimization (DPO). Experimental results on three benchmark datasets demonstrate that DTA effectively balances accuracy with appropriate abstention, enhancing the reliability and trustworthiness of retrieval-augmented systems.",
      "arxiv_url": "https://arxiv.org/abs/2505.20871",
      "pdf_url": "https://arxiv.org/pdf/2505.20871",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10736",
      "title": "Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization",
      "authors": [
        "Ximing Dong",
        "Shaowei Wang",
        "Dayi Lin",
        "Ahmed E. Hassan"
      ],
      "abstract": "Optimizing Large Language Model (LLM) performance requires well-crafted prompts, but manual prompt engineering is labor-intensive and often ineffective. Automated prompt optimization techniques address this challenge but the majority of them rely on randomly selected evaluation subsets, which fail to represent the full dataset, leading to unreliable evaluations and suboptimal prompts. Existing coreset selection methods, designed for LLM benchmarking, are unsuitable for prompt optimization due to challenges in clustering similar samples, high data collection costs, and the unavailability of performance data for new or private datasets. To overcome these issues, we propose IPOMP, an Iterative evaluation data selection for effective Prompt Optimization using real-time Model Performance. IPOMP is a two-stage approach that selects representative and diverse samples using semantic clustering and boundary analysis, followed by iterative refinement with real-time model performance data to replace redundant samples. Evaluations on the BIG-bench dataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by at least 57% compared with SOTA baselines, with minimal computational overhead below 1%. Furthermore, the results demonstrate that our real-time performance-guided refinement approach can be universally applied to enhance existing coreset selection methods.",
      "arxiv_url": "https://arxiv.org/abs/2505.10736",
      "pdf_url": "https://arxiv.org/pdf/2505.10736",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06539",
      "title": "Beyond Facts: Evaluating Intent Hallucination in Large Language Models",
      "authors": [
        "Yijie Hao",
        "Haofei Yu",
        "Jiaxuan You"
      ],
      "abstract": "When exposed to complex queries containing multiple conditions, today's large language models (LLMs) tend to produce responses that only partially satisfy the query while neglecting certain conditions. We therefore introduce the concept of Intent Hallucination. In this phenomenon, LLMs either omit (neglecting to address certain parts) or misinterpret (responding to invented query parts) elements of the given query, leading to intent hallucinated generation. To systematically evaluate intent hallucination, we introduce FAITHQA, a novel benchmark for intent hallucination that contains 20,068 problems, covering both query-only and retrieval-augmented generation (RAG) setups with varying topics and difficulty. FAITHQA is the first hallucination benchmark that goes beyond factual verification, tailored to identify the fundamental cause of intent hallucination. By evaluating various LLMs on FAITHQA, we find that (1) intent hallucination is a common issue even for state-of-the-art models, and (2) the phenomenon stems from omission or misinterpretation of LLMs. To facilitate future research, we introduce an automatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting intent hallucination. Human evaluation results demonstrate that CONSTRAINT SCORE is closer to human performance for intent hallucination compared to baselines.",
      "arxiv_url": "https://arxiv.org/abs/2506.06539",
      "pdf_url": "https://arxiv.org/pdf/2506.06539",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-06-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16860",
      "title": "LongAttn: Selecting Long-context Training Data via Token-level Attention",
      "authors": [
        "Longyun Wu",
        "Dawei Zhu",
        "Guangxiang Zhao",
        "Zhuocheng Yu",
        "Junfeng Ran",
        "Xiangyu Wong",
        "Lin Sun",
        "Sujian Li"
      ],
      "abstract": "With the development of large language models (LLMs), there has been an increasing need for significant advancements in handling long contexts. To enhance long-context capabilities, constructing high-quality training data with long-range dependencies is crucial. Existing methods to select long-context data often rely on sentence-level analysis, which can be greatly optimized in both performance and efficiency. In this paper, we propose a novel token-level framework, LongAttn, which leverages the self-attention mechanism of LLMs to measure the long-range dependencies for the data. By calculating token-level dependency strength and distribution uniformity of token scores, LongAttn effectively quantifies long-range dependencies, enabling more accurate and efficient data selection. We filter LongABC-32K from open-source long-context datasets (ArXiv, Book, and Code). Through our comprehensive experiments, LongAttn has demonstrated its excellent effectiveness, scalability, and efficiency. To facilitate future research in long-context data, we released our code and the high-quality long-context training data LongABC-32K.",
      "arxiv_url": "https://arxiv.org/abs/2502.16860",
      "pdf_url": "https://arxiv.org/pdf/2502.16860",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11196",
      "title": "How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training",
      "authors": [
        "Yixin Ou",
        "Yunzhi Yao",
        "Ningyu Zhang",
        "Hui Jin",
        "Jiacheng Sun",
        "Shumin Deng",
        "Zhenguo Li",
        "Huajun Chen"
      ],
      "abstract": "Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at https://github.com/zjunlp/DynamicKnowledgeCircuits.",
      "arxiv_url": "https://arxiv.org/abs/2502.11196",
      "pdf_url": "https://arxiv.org/pdf/2502.11196",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10717",
      "title": "A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment",
      "authors": [
        "Jean-Philippe Corbeil",
        "Amin Dada",
        "Jean-Michel Attendu",
        "Asma Ben Abacha",
        "Alessandro Sordoni",
        "Lucas Caccia",
        "Franccois Beaulieu",
        "Thomas Lin",
        "J. Kleesiek",
        "Paul Vozila"
      ],
      "abstract": "High computation costs and latency of large language models such as GPT-4 have limited their deployment in clinical settings. Small language models (SLMs) offer a cost-effective alternative, but their limited capacity requires biomedical domain adaptation, which remains challenging. An additional bottleneck is the unavailability and high sensitivity of clinical data. To address these challenges, we propose a novel framework for adapting SLMs into high-performing clinical models. We introduce the MediPhi collection of 3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning of experts on relevant medical and clinical corpora (PMC, Medical Guideline, MedWiki, etc.), model merging, and clinical-tasks alignment. To cover most clinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our expert models deliver relative improvements on this benchmark over the base model without any task-specific fine-tuning: 64.3% on medical entities, 49.5% on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by 14%). We unify the expert models into MediPhi via model merging, preserving gains across benchmarks. Furthermore, we built the MediFlow collection, a synthetic dataset of 2.5 million high-quality instructions on 14 medical NLP tasks, 98 fine-grained document types, and JSON format support. Alignment of MediPhi using supervised fine-tuning and direct preference optimization achieves further gains of 18.9% on average.",
      "arxiv_url": "https://arxiv.org/abs/2505.10717",
      "pdf_url": "https://arxiv.org/pdf/2505.10717",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14577",
      "title": "TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring",
      "authors": [
        "Sohaila Eltanbouly",
        "Salam Albatarni",
        "Tamer Elsayed"
      ],
      "abstract": "Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there is a notable lack of attention for assessing essays according to individual traits. In this work, we propose TRATES, a novel trait-specific and rubric-based cross-prompt AES framework that is generic yet specific to the underlying trait. The framework leverages a Large Language Model (LLM) that utilizes the trait grading rubrics to generate trait-specific features (represented by assessment questions), then assesses those features given an essay. The trait-specific features are eventually combined with generic writing-quality and prompt-specific features to train a simple classical regression model that predicts trait scores of essays from an unseen prompt. Experiments show that TRATES achieves a new state-of-the-art performance across all traits on a widely-used dataset, with the generated LLM-based features being the most significant.",
      "arxiv_url": "https://arxiv.org/abs/2505.14577",
      "pdf_url": "https://arxiv.org/pdf/2505.14577",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01312",
      "title": "Growing Through Experience: Scaling Episodic Grounding in Language Models",
      "authors": [
        "Chunhui Zhang",
        "Sirui Wang",
        "Z. Ouyang",
        "Xiangchi Yuan",
        "Soroush Vosoughi"
      ],
      "abstract": "Language models (LMs) require robust episodic grounding-the capacity to learn from and apply past experiences-to excel at physical planning tasks. Current episodic grounding approaches struggle with scalability and integration, limiting their effectiveness, especially for medium-sized LMs (7B parameters). While larger LMs (70-405B parameters) possess superior hierarchical representations and extensive pre-trained knowledge, they encounter a fundamental scale paradox: despite their advanced abstraction capabilities, they lack efficient mechanisms to leverage experience streams. We propose a scalable weak-to-strong episodic learning framework that effectively transfers episodic behaviors from smaller to larger LMs. This framework integrates Monte Carlo tree search for structured experience collection with a novel distillation method, preserving the inherent LM capabilities while embedding episodic memory. Experiments demonstrate our method surpasses state-of-the-art proprietary LMs by 3.45% across diverse planning and question-answering tasks. Layer-wise probing further indicates significant improvements in task alignment, especially within deeper LM layers, highlighting stable generalization even for previously unseen scenarios with increased planning complexity-conditions where baseline methods degrade markedly.",
      "arxiv_url": "https://arxiv.org/abs/2506.01312",
      "pdf_url": "https://arxiv.org/pdf/2506.01312",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.17110",
      "title": "Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling",
      "authors": [
        "Junlin Li",
        "Guodong Du",
        "Jing Li",
        "Sim Kuan Goh",
        "Wenya Wang",
        "Yequan Wang",
        "Fangming Liu",
        "Ho-Kin Tang",
        "Saleh Alharbi",
        "Daojing He",
        "Min Zhang"
      ],
      "abstract": "Fine-tuning Large Language Models (LLMs) with multimodal encoders on modality-specific data expands the modalities that LLMs can handle, leading to the formation of Multimodal LLMs (MLLMs). However, this paradigm heavily relies on resource-intensive and inflexible fine-tuning from scratch with new multimodal data. In this paper, we propose MMER (Multi-modality Expansion and Retention), a training-free approach that integrates existing MLLMs for effective multimodal expansion while retaining their original performance. Specifically, MMER reuses MLLMs' multimodal encoders while merging their LLM parameters. By comparing original and merged LLM parameters, MMER generates binary masks to approximately separate LLM parameters for each modality. These decoupled parameters can independently process modality-specific inputs, reducing parameter conflicts and preserving original MLLMs' fidelity. MMER can also mitigate catastrophic forgetting by applying a similar process to MLLMs fine-tuned on new tasks. Extensive experiments show significant improvements over baselines, proving that MMER effectively expands LLMs' multimodal capabilities while retaining 99% of the original performance, and also markedly mitigates catastrophic forgetting.",
      "arxiv_url": "https://arxiv.org/abs/2505.17110",
      "pdf_url": "https://arxiv.org/pdf/2505.17110",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "c6c9fa5137a01006d7fdfcd2405c66a311473140",
      "title": "Initializing and Retrofitting Key-Value Adaptors for Traceable Model Editing",
      "authors": [
        "Hanlun Zhu",
        "Yunshi Lan",
        "Xiang Li",
        "Weining Qian"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/c6c9fa5137a01006d7fdfcd2405c66a311473140",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.12370",
      "title": "Understanding Common Ground Misalignment in Goal-Oriented Dialog: A Case-Study with Ubuntu Chat Logs",
      "authors": [
        "Rupak Sarkar",
        "Neha Srikanth",
        "Taylor Hudson",
        "Rachel Rudinger",
        "C. Bonial",
        "Philip Resnik"
      ],
      "abstract": "While it is commonly accepted that maintaining common ground plays a role in conversational success, little prior research exists connecting conversational grounding to success in task-oriented conversations. We study failures of grounding in the Ubuntu IRC dataset, where participants use text-only communication to resolve technical issues. We find that disruptions in conversational flow often stem from a misalignment in common ground, driven by a divergence in beliefs and assumptions held by participants. These disruptions, which we call conversational friction, significantly correlate with task success. We find that although LLMs can identify overt cases of conversational friction, they struggle with subtler and more context-dependent instances requiring pragmatic or domain-specific reasoning.",
      "arxiv_url": "https://arxiv.org/abs/2503.12370",
      "pdf_url": "https://arxiv.org/pdf/2503.12370",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20490",
      "title": "EgoNormia: Benchmarking Physical Social Norm Understanding",
      "authors": [
        "MohammadHossein Rezaei",
        "Yicheng Fu",
        "Phil Cuvin",
        "Caleb Ziems",
        "Yanzhe Zhang",
        "Hao Zhu",
        "Diyi Yang"
      ],
      "abstract": "Human activity is moderated by norms; however, supervision for normative reasoning is sparse, particularly where norms are physically- or socially-grounded. We thus present EGONORMIA $\\|\\epsilon\\|$, comprising 1,853 (200 for EGONORMIA-verified) multiple choice questions (MCQs) grounded within egocentric videos of human interactions, enabling the evaluation and improvement of normative reasoning in vision-language models (VLMs). EGONORMIA spans seven norm categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline to generate grounded MCQs from raw egocentric video. Our work demonstrates that current state-of-the-art VLMs lack robust grounded norm understanding, scoring a maximum of 54% on EGONORMIA and 65% on EGONORMIA-verified, with performance across norm categories indicating significant risks of safety and privacy when VLMs are used in real-world agents. We additionally explore methods for improving normative understanding, demonstrating that a naive retrieval-based generation (RAG) method using EGONORMIA can enhance normative reasoning in VLMs.",
      "arxiv_url": "https://arxiv.org/abs/2502.20490",
      "pdf_url": "https://arxiv.org/pdf/2502.20490",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11433",
      "title": "FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading",
      "authors": [
        "Guojun Xiong",
        "Zhiyang Deng",
        "Keyi Wang",
        "Yupeng Cao",
        "Haohang Li",
        "Yangyang Yu",
        "Xueqing Peng",
        "Mingquan Lin",
        "Kaleb E Smith",
        "Xiao-Yang Liu",
        "Jimin Huang",
        "Sophia Ananiadou",
        "Qianqian Xie"
      ],
      "abstract": "Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose \\textsc{FLAG-Trader}, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements.",
      "arxiv_url": "https://arxiv.org/abs/2502.11433",
      "pdf_url": "https://arxiv.org/pdf/2502.11433",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.15504",
      "title": "Enhancing Hyperbole and Metaphor Detection with Their Bidirectional Dynamic Interaction and Emotion Knowledge",
      "authors": [
        "Limin Zheng",
        "Sihang Wang",
        "Hao Fei",
        "Zuquan Peng",
        "Fei Li",
        "Jianming Fu",
        "Chong Teng",
        "Donghong Ji"
      ],
      "abstract": "Text-based hyperbole and metaphor detection are of great significance for natural language processing (NLP) tasks. However, due to their semantic obscurity and expressive diversity, it is rather challenging to identify them. Existing methods mostly focus on superficial text features, ignoring the associations of hyperbole and metaphor as well as the effect of implicit emotion on perceiving these rhetorical devices. To implement these hypotheses, we propose an emotion-guided hyperbole and metaphor detection framework based on bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis module deeply mines the emotion connotations behind hyperbole and metaphor. Next, the emotion-based domain mapping module identifies the target and source domains to gain a deeper understanding of the implicit meanings of hyperbole and metaphor. Finally, the bidirectional dynamic interaction module enables the mutual promotion between hyperbole and metaphor. Meanwhile, a verification mechanism is designed to ensure detection accuracy and reliability. Experiments show that EmoBi outperforms all baseline methods on four datasets. Specifically, compared to the current SoTA, the F1 score increased by 28.1% for hyperbole detection on the TroFi dataset and 23.1% for metaphor detection on the HYPO-L dataset. These results, underpinned by in-depth analyses, underscore the effectiveness and potential of our approach for advancing hyperbole and metaphor detection.",
      "arxiv_url": "https://arxiv.org/abs/2506.15504",
      "pdf_url": "https://arxiv.org/pdf/2506.15504",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12379",
      "title": "Training-free LLM Merging for Multi-task Learning",
      "authors": [
        "Zichuan Fu",
        "Xian Wu",
        "Yejing Wang",
        "Wanyu Wang",
        "Shanshan Ye",
        "Hongzhi Yin",
        "Yi Chang",
        "Yefeng Zheng",
        "Xiangyu Zhao"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse natural language processing (NLP) tasks. The release of open-source LLMs like LLaMA and Qwen has triggered the development of numerous fine-tuned models tailored for various tasks and languages. In this paper, we explore an important question: is it possible to combine these specialized models to create a unified model with multi-task capabilities. We introduces Hierarchical Iterative Merging (Hi-Merging), a training-free method for unifying different specialized LLMs into a single model. Specifically, Hi-Merging employs model-wise and layer-wise pruning and scaling, guided by contribution analysis, to mitigate parameter conflicts. Extensive experiments on multiple-choice and question-answering tasks in both Chinese and English validate Hi-Merging's ability for multi-task learning. The results demonstrate that Hi-Merging consistently outperforms existing merging techniques and surpasses the performance of models fine-tuned on combined datasets in most scenarios. Code is available at: https://github.com/Applied-Machine-Learning-Lab/Hi-Merging.",
      "arxiv_url": "https://arxiv.org/abs/2506.12379",
      "pdf_url": "https://arxiv.org/pdf/2506.12379",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "c767b30c7d3e26a5b8de5a50ea0a7a1234ad4765",
      "title": "Systematic Evaluation of Auto-Encoding and Large Language Model Representations for Capturing Author States and Traits",
      "authors": [
        "Khushboo Singh",
        "Vasudha Varadarajan",
        "Adithya V Ganesan",
        "A. Nilsson",
        "Nikita Soni",
        "Syeda Mahwish",
        "P. Chitale",
        "Ryan L. Boyd",
        "L. Ungar",
        "Richard N. Rosenthal",
        "H. A. Schwartz"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used in human-centered applications, yet their ability to model diverse psychological constructs is not well understood. In this study, we systematically evaluate a range of Transformer-LMs to predict psychological variables across five major dimensions: affect, substance use, mental health, sociodemographics, and personality. Analyses span three temporal levels— short daily text responses about current affect, text aggregated over two-weeks, and user-level text collected over two years—allowing us to examine how each model’s strengths align with the underlying stability of different constructs. The findings show that mental health signals emerge as the most accurately predicted dimensions ( r ≈ 0 . 6 ) across all temporal scales. At the daily scale, smaller models like DeBERTa and HaRT often performed better, whereas, at longer scales or with greater context, larger model like Llama3-8B performed the best. Also, aggregating text over the entire study period yielded stronger correlations for outcomes, such as age and income. Overall, these results suggest the importance of selecting appropriate model architectures and temporal aggregation techniques based on the stability and nature of the target variable.",
      "arxiv_url": "https://www.semanticscholar.org/paper/c767b30c7d3e26a5b8de5a50ea0a7a1234ad4765",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.03601",
      "title": "Feature-Level Insights into Artificial Text Detection with Sparse Autoencoders",
      "authors": [
        "Kristian Kuznetsov",
        "Laida Kushnareva",
        "Polina Druzhinina",
        "Anton Razzhigaev",
        "Anastasia Voznyuk",
        "Irina Piontkovskaya",
        "Evgeny Burnaev",
        "Serguei Barannikov"
      ],
      "abstract": "Artificial Text Detection (ATD) is becoming increasingly important with the rise of advanced Large Language Models (LLMs). Despite numerous efforts, no single algorithm performs consistently well across different types of unseen text or guarantees effective generalization to new LLMs. Interpretability plays a crucial role in achieving this goal. In this study, we enhance ATD interpretability by using Sparse Autoencoders (SAE) to extract features from Gemma-2-2b residual stream. We identify both interpretable and efficient features, analyzing their semantics and relevance through domain- and model-specific statistics, a steering approach, and manual or LLM-based interpretation. Our methods offer valuable insights into how texts from various models differ from human-written content. We show that modern LLMs have a distinct writing style, especially in information-dense domains, even though they can produce human-like outputs with personalized prompts.",
      "arxiv_url": "https://arxiv.org/abs/2503.03601",
      "pdf_url": "https://arxiv.org/pdf/2503.03601",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-03-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03489",
      "title": "EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding",
      "authors": [
        "Mingxu Tao",
        "Jie Hu",
        "Mingchuan Yang",
        "Yunhuai Liu",
        "Dongyan Zhao",
        "Yansong Feng"
      ],
      "abstract": "The remarkable performance of Large language models (LLMs) relies heavily on the availability of abundant high-quality training data. However, the high cost of acquiring annotated data often prevents models from obtaining capabilities to tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe that boosts model performance in data-scarcity scenarios without extra training. We first employ model extrapolation to enhance a finetuned model with its inferior version, and then adopt contrastive decoding to further reduce predicted errors, by comparing the logit scores given by the extrapolated and the vanilla finetuned model. Experiments across three tasks over four different LLMs show that EpiCoDe consistently outperforms existing methods with significant and robust improvement. We also propose a new theoretical framework to reveal the mechanism behind contrastive decoding in data-scarcity scenarios, which further helps us better understand the effectiveness of EpiCoDe.",
      "arxiv_url": "https://arxiv.org/abs/2506.03489",
      "pdf_url": "https://arxiv.org/pdf/2506.03489",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "c7a796c6e72da07a6da52ef081da0a1435afdcd9",
      "title": "The Open Argument Mining Framework",
      "authors": [
        "Debela Tesfaye Gemechu",
        "Ramon Ruiz-Dolz",
        "Kamila Górska",
        "Somaye Moslemnejad",
        "Eimear Maguire",
        "Dimitra Zografistou",
        "Yohan Jo",
        "John Lawrence",
        "Chris Reed"
      ],
      "abstract": "Despite extensive research in Argument Mining (AM), the field faces significant challenges in limited reproducibility, difficulty in comparing systems due to varying task combinations, and a lack of interoperability caused by the heterogeneous nature of argumentation theory. These challenges are further exacerbated by the absence of dedicated tools, with most advancements remaining isolated research outputs rather than reusable systems. The oAMF (Open Argument Mining Framework) addresses these issues by providing an open-source, modular, and scalable platform that unifies diverse AM methods. Initially released with seventeen integrated modules, the oAMF serves as a starting point for researchers and developers to build, experiment with, and deploy AM pipelines while ensuring interoperability and allowing multiple theories of argumentation to co-exist within the same framework. Its flexible design supports integration via Python APIs, drag-and-drop tools, and web interfaces, streamlining AM development for research and industry setup, facilitating method comparison, and reproducibility.",
      "arxiv_url": "https://www.semanticscholar.org/paper/c7a796c6e72da07a6da52ef081da0a1435afdcd9",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18867",
      "title": "Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing",
      "authors": [
        "Ming Cheng",
        "Jiaying Gong",
        "Hoda Eldardiry"
      ],
      "abstract": "Lay paraphrasing aims to make scientific information accessible to audiences without technical backgrounds. However, most existing studies focus on a single domain, such as biomedicine. With the rise of interdisciplinary research, it is increasingly necessary to comprehend knowledge spanning multiple technical fields. To address this, we propose Sci-LoRA, a model that leverages a mixture of LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA dynamically generates and applies weights for each LoRA, enabling it to adjust the impact of different domains based on the input text, without requiring explicit domain labels. To balance domain-specific knowledge and generalization across various domains, Sci-LoRA integrates information at both the data and model levels. This dynamic fusion enhances the adaptability and performance across various domains. Experimental results across twelve domains on five public datasets show that Sci-LoRA significantly outperforms state-of-the-art large language models and demonstrates flexible generalization and adaptability in cross-domain lay paraphrasing.",
      "arxiv_url": "https://arxiv.org/abs/2505.18867",
      "pdf_url": "https://arxiv.org/pdf/2505.18867",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.16207",
      "title": "From Informal to Formal - Incorporating and Evaluating LLMs on Natural Language Requirements to Verifiable Formal Proofs",
      "authors": [
        "Jialun Cao",
        "Yaojie Lu",
        "Meiziniu Li",
        "Haoyang Ma",
        "Haokun Li",
        "Mengda He",
        "Cheng Wen",
        "Le Sun",
        "Hongyu Zhang",
        "Shengchao Qin",
        "S. Cheung",
        "Cong Tian"
      ],
      "abstract": "The research in AI-based formal mathematical reasoning has shown an unstoppable growth trend. These studies have excelled in mathematical competitions like IMO and have made significant progress. This paper focuses on formal verification, an immediate application scenario of formal reasoning, and breaks it down into sub-tasks. We constructed 18k high-quality instruction-response pairs across five formal specification languages (Coq, Lean4, Dafny, ACSL, and TLA+) by distilling gpt-4o and evaluated against ten open-sourced LLMs, including recent popular DeepSeek-R1. We also fine-tuned several 7~8B small models to achieve comparable performance with Deepseek-R1-671B. Interestingly, we observed that fine-tuning with formal data also enhances mathematics, reasoning, and coding capabilities. Fine-tuned models are released at https: //huggingface.co/fm-universe.",
      "arxiv_url": "https://arxiv.org/abs/2501.16207",
      "pdf_url": "https://arxiv.org/pdf/2501.16207",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "c8376ed461eca46d4f3eee233d454c1121a95a2d",
      "title": "MISP-Meeting: A Real-World Dataset with Multimodal Cues for Long-form Meeting Transcription and Summarization",
      "authors": [
        "HangChen HangChen",
        "Chao-Han Huck Yang",
        "Jia-Chen Gu",
        "Sabato Marco Siniscalchi",
        "Jun Du"
      ],
      "abstract": "We introduce MISP-Meeting, a new real-world, multimodal dataset that covers subject-oriented long-form content. MISP-Meeting integrates information from speech, vision, and text modalities to facilitate automatic meeting transcription and summarization (AMTS). Chal-lenging conditions in human meetings, including far-field speech recognition, audio-visual understanding, and long-term summarization, have been carefully evaluated. We benchmark state-of-the-art automatic speech recognition (ASR) and large language models (LLMs) on this dataset, enhanced with multimodal cues. Experiments demonstrate that incorporating multimodal cues, such as lip movements and visual focus of attention, significantly enhances transcription accuracy, reducing the character error rate (CER) from 36 . 60% to 20 . 27% via guided source separation (GSS), fine-tuning, and audio-visual fusion. Furthermore, our summarization analysis reveals a direct correlation between ASR quality and summary coherence, underscoring the importance of robust multi-modal modeling. Our dataset and codebase have been released as open source. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/c8376ed461eca46d4f3eee233d454c1121a95a2d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.10762",
      "title": "Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation",
      "authors": [
        "Guofu Xie",
        "Xiao Zhang",
        "Ting Yao",
        "Yun-zhen Shi"
      ],
      "abstract": "User information needs are often highly diverse and varied. A key challenge in current research is how to achieve controllable multi-objective generation while enabling rapid adaptation to accommodate diverse user demands during test time. Existing solutions, such as Rewarded Soup, focus on merging language models individually tuned on single objectives. While easy to implement and widely used, these approaches face limitations in achieving optimal performance due to their disregard for the impacts of competing objectives on model tuning. To address this issue, we propose Bone Soup, a novel model merging approach that first seeks a series of backbone models by considering the impacts of multiple objectives and then makes the soup (i.e., merge the backbone models). Specifically, Bone Soup begins by training multiple backbone models for different objectives using multi-objective reinforcement learning. Each backbone model is guided by a combination of backbone reward signals. To ensure that these models are optimal for the Pareto front, the backbone rewards are crafted by combining standard reward functions into basis vectors, which can then be modified through a rule-based construction method. Bone Soup leverages a symmetric circulant matrix mapping to generate the merging coefficients, which are used to merge the backbone models according to user preferences. Extensive experimental results demonstrate that Bone Soup exhibits strong controllability and Pareto optimality in controllable multi-objective generation, providing a more effective and efficient approach to addressing diverse user needs at test time.",
      "arxiv_url": "https://arxiv.org/abs/2502.10762",
      "pdf_url": "https://arxiv.org/pdf/2502.10762",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.13699",
      "title": "DI-BENCH: Benchmarking Large Language Models on Dependency Inference with Testable Repositories at Scale",
      "authors": [
        "Linghao Zhang",
        "Junhao Wang",
        "Shilin He",
        "Chaoyun Zhang",
        "Yu Kang",
        "Bowen Li",
        "Jiaheng Wen",
        "Chengxing Xie",
        "Maoquan Wang",
        "Yufan Huang",
        "Elsie Nallipogu",
        "Qingwei Lin",
        "Yingnong Dang",
        "Saravan Rajmohan",
        "Dongmei Zhang",
        "Qi Zhang"
      ],
      "abstract": "Large Language Models have advanced automated software development, however, it remains a challenge to correctly infer dependencies, namely, identifying the internal components and external packages required for a repository to successfully run. Existing studies highlight that dependency-related issues cause over 40\\% of observed runtime errors on the generated repository. To address this, we introduce DI-BENCH, a large-scale benchmark and evaluation framework specifically designed to assess LLMs' capability on dependency inference. The benchmark features 581 repositories with testing environments across Python, C#, Rust, and JavaScript. Extensive experiments with textual and execution-based metrics reveal that the current best-performing model achieves only a 42.9% execution pass rate, indicating significant room for improvement. DI-BENCH establishes a new viewpoint for evaluating LLM performance on repositories, paving the way for more robust end-to-end software synthesis.",
      "arxiv_url": "https://arxiv.org/abs/2501.13699",
      "pdf_url": "https://arxiv.org/pdf/2501.13699",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.20194",
      "title": "GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization",
      "authors": [
        "Zhouhong Gu",
        "Xingzhou Chen",
        "Xiaoran Shi",
        "Tao Wang",
        "Suhang Zheng",
        "Tianyu Li",
        "Hongwei Feng",
        "Yanghua Xiao"
      ],
      "abstract": "Recent advances in large language models have highlighted the critical need for precise control over model outputs through predefined constraints. While existing methods attempt to achieve this through either direct instruction-response synthesis or preferential response optimization, they often struggle with constraint understanding and adaptation. This limitation becomes particularly evident when handling fine-grained constraints, leading to either hallucination or brittle performance. We introduce Generative Adversarial Policy Optimization (GAPO), a novel framework that combines GAN-based training dynamics with an encoder-only reward model to progressively learn and adapt to increasingly complex constraints. GAPO leverages adversarial training to automatically generate training samples of varying difficulty while utilizing the encoder-only architecture to better capture prompt-response relationships. Extensive experiments demonstrate GAPO's superior performance across multiple benchmarks, particularly in scenarios requiring fine-grained constraint handling, where it significantly outperforms existing methods like PPO, DPO, and KTO. Our results suggest that GAPO's unique approach to preferential prompt learning offers a more robust and effective solution for controlling LLM outputs. Code is avaliable in https://github.com/MikeGu721/GAPO.",
      "arxiv_url": "https://arxiv.org/abs/2503.20194",
      "pdf_url": "https://arxiv.org/pdf/2503.20194",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15916",
      "title": "The Esethu Framework: Reimagining Sustainable Dataset Governance and Curation for Low-Resource Languages",
      "authors": [
        "Jenalea Rajab",
        "Anuoluwapo Aremu",
        "E. Chimoto",
        "Dale Dunbar",
        "Graham Morrissey",
        "Fadel Thior",
        "Luandrie Potgieter",
        "Jessico Ojo",
        "A. Tonja",
        "Maushami Chetty",
        "Onyothi Nekoto",
        "Pelonomi Moiloa",
        "Jade Abbott",
        "V. Marivate",
        "Benjamin Rosman"
      ],
      "abstract": "This paper presents the Esethu Framework, a sustainable data curation framework specifically designed to empower local communities and ensure equitable benefit-sharing from their linguistic resource. This framework is supported by the Esethu license, a novel community-centric data license. As a proof of concept, we introduce the Vuk'uzenzele isiXhosa Speech Dataset (ViXSD), an open-source corpus developed under the Esethu Framework and License. The dataset, containing read speech from native isiXhosa speakers enriched with demographic and linguistic metadata, demonstrates how community-driven licensing and curation principles can bridge resource gaps in automatic speech recognition (ASR) for African languages while safeguarding the interests of data creators. We describe the framework guiding dataset development, outline the Esethu license provisions, present the methodology for ViXSD, and present ASR experiments validating ViXSD's usability in building and refining voice-driven applications for isiXhosa.",
      "arxiv_url": "https://arxiv.org/abs/2502.15916",
      "pdf_url": "https://arxiv.org/pdf/2502.15916",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23908",
      "title": "Transforming Podcast Preview Generation: From Expert Models to LLM-Based Systems",
      "authors": [
        "Winstead Zhu",
        "Ann Clifton",
        "Azin Ghazimatin",
        "Edgar Tanaka",
        "Ward Ronan"
      ],
      "abstract": "Discovering and evaluating long-form talk content such as videos and podcasts poses a significant challenge for users, as it requires a considerable time investment. Previews offer a practical solution by providing concise snippets that showcase key moments of the content, enabling users to make more informed and confident choices. We propose an LLM-based approach for generating podcast episode previews and deploy the solution at scale, serving hundreds of thousands of podcast previews in a real-world application. Comprehensive offline evaluations and online A/B testing demonstrate that LLM-generated previews consistently outperform a strong baseline built on top of various ML expert models, showcasing a significant reduction in the need for meticulous feature engineering. The offline results indicate notable enhancements in understandability, contextual clarity, and interest level, and the online A/B test shows a 4.6% increase in user engagement with preview content, along with a 5x boost in processing efficiency, offering a more streamlined and performant solution compared to the strong baseline of feature-engineered expert models.",
      "arxiv_url": "https://arxiv.org/abs/2505.23908",
      "pdf_url": "https://arxiv.org/pdf/2505.23908",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24331",
      "title": "Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective Role-Playing Agents",
      "authors": [
        "Fanhang Man",
        "Huandong Wang",
        "Jianjie Fang",
        "Zhaoyi Deng",
        "Baining Zhao",
        "Xinlei Chen",
        "Yong Li"
      ],
      "abstract": "User sentiment on social media reveals the underlying social trends, crises, and needs. Researchers have analyzed users'past messages to trace the evolution of sentiments and reconstruct sentiment dynamics. However, predicting the imminent sentiment of an ongoing event is rarely studied. In this paper, we address the problem of \\textbf{sentiment forecasting} on social media to predict the user's future sentiment in response to the development of the event. We extract sentiment-related features to enhance the modeling skill and propose a multi-perspective role-playing framework to simulate the process of human response. Our preliminary results show significant improvement in sentiment forecasting on both microscopic and macroscopic levels.",
      "arxiv_url": "https://arxiv.org/abs/2505.24331",
      "pdf_url": "https://arxiv.org/pdf/2505.24331",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "c92e6a486ab4101ff4199aaa730f2a331ac72f33",
      "title": "CoT-VTM: Visual-to-Music Generation with Chain-of-Thought Reasoning",
      "authors": [
        "Xikang Guan",
        "Zheng Gu",
        "Jing Huo",
        "Tianyu Ding",
        "Yang Gao"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/c92e6a486ab4101ff4199aaa730f2a331ac72f33",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.01578",
      "title": "Grounding Task Assistance with Multimodal Cues from a Single Demonstration",
      "authors": [
        "Gabriel Sarch",
        "B. T. Kumaravel",
        "Sahithya Ravi",
        "Vibhav Vineet",
        "Andrew D. Wilson"
      ],
      "abstract": "A person's demonstration often serves as a key reference for others learning the same task. However, RGB video, the dominant medium for representing these demonstrations, often fails to capture fine-grained contextual cues such as intent, safety-critical environmental factors, and subtle preferences embedded in human behavior. This sensory gap fundamentally limits the ability of Vision Language Models (VLMs) to reason about why actions occur and how they should adapt to individual users. To address this, we introduce MICA (Multimodal Interactive Contextualized Assistance), a framework that improves conversational agents for task assistance by integrating eye gaze and speech cues. MICA segments demonstrations into meaningful sub-tasks and extracts keyframes and captions that capture fine-grained intent and user-specific cues, enabling richer contextual grounding for visual question answering. Evaluations on questions derived from real-time chat-assisted task replication show that multimodal cues significantly improve response quality over frame-based retrieval. Notably, gaze cues alone achieves 93% of speech performance, and their combination yields the highest accuracy. Task type determines the effectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the need for adaptable multimodal models. These results highlight the limitations of frame-based context and demonstrate the value of multimodal signals for real-world AI task assistance.",
      "arxiv_url": "https://arxiv.org/abs/2505.01578",
      "pdf_url": "https://arxiv.org/pdf/2505.01578",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.02969",
      "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large Language Model",
      "authors": [
        "Siqi Ouyang",
        "Xi Xu",
        "Lei Li"
      ],
      "abstract": "Simultaneous translation of unbounded streaming speech remains a challenging problem due to the need for effectively processing the history speech context and past translations so that quality and latency, including computation overhead, can be balanced. Most prior works assume pre-segmented speech, limiting their real-world applicability. In this paper, we propose InfiniSST, a novel approach that formulates SST as a multi-turn dialogue task, enabling seamless translation of unbounded speech. We construct translation trajectories and robust segments from MuST-C with multi-latency augmentation during training and develop a key-value (KV) cache management strategy to facilitate efficient inference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that InfiniSST reduces computation-aware latency by 0.5 to 1 second while maintaining the same translation quality compared to baselines. Ablation studies further validate the contributions of our data construction and cache management strategy. We release the code and demo at https://github.com/LeiLiLab/InfiniSST",
      "arxiv_url": "https://arxiv.org/abs/2503.02969",
      "pdf_url": "https://arxiv.org/pdf/2503.02969",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "c97f6f41b4d1f23f2cfd0abc390580a72070d1a5",
      "title": "CROSSAGENTIE: Cross-Type and Cross-Task Multi-Agent LLM Collaboration for Zero-Shot Information Extraction",
      "authors": [
        "Meng Lu",
        "Yuzhang Xie",
        "Zhenyu Bi",
        "Shuxiang Cao",
        "Xuan Wang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/c97f6f41b4d1f23f2cfd0abc390580a72070d1a5",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12936",
      "title": "CliniDial: A Naturally Occurring Multimodal Dialogue Dataset for Team Reflection in Action During Clinical Operation",
      "authors": [
        "Naihao Deng",
        "K. Das",
        "Rada Mihalcea",
        "Vitaliy Popov",
        "M. Abouelenien"
      ],
      "abstract": "In clinical operations, teamwork can be the crucial factor that determines the final outcome. Prior studies have shown that sufficient collaboration is the key factor that determines the outcome of an operation. To understand how the team practices teamwork during the operation, we collected CliniDial from simulations of medical operations. CliniDial includes the audio data and its transcriptions, the simulated physiology signals of the patient manikins, and how the team operates from two camera angles. We annotate behavior codes following an existing framework to understand the teamwork process for CliniDial. We pinpoint three main characteristics of our dataset, including its label imbalances, rich and natural interactions, and multiple modalities, and conduct experiments to test existing LLMs' capabilities on handling data with these characteristics. Experimental results show that CliniDial poses significant challenges to the existing models, inviting future effort on developing methods that can deal with real-world clinical data. We open-source the codebase at https://github.com/MichiganNLP/CliniDial",
      "arxiv_url": "https://arxiv.org/abs/2506.12936",
      "pdf_url": "https://arxiv.org/pdf/2506.12936",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.16172",
      "title": "SGIC: A Self-Guided Iterative Calibration Framework for RAG",
      "authors": [
        "Guanhua Chen",
        "Yutong Yao",
        "Lidia S. Chao",
        "Xuebo Liu",
        "Derek F. Wong"
      ],
      "abstract": "Recent research in retrieval-augmented generation (RAG) has concentrated on retrieving useful information from candidate documents. However, numerous methodologies frequently neglect the calibration capabilities of large language models (LLMs), which capitalize on their robust in-context reasoning prowess. This work illustrates that providing LLMs with specific cues substantially improves their calibration efficacy, especially in multi-round calibrations. We present a new SGIC: Self-Guided Iterative Calibration Framework that employs uncertainty scores as a tool. Initially, this framework calculates uncertainty scores to determine both the relevance of each document to the query and the confidence level in the responses produced by the LLMs. Subsequently, it reevaluates these scores iteratively, amalgamating them with prior responses to refine calibration. Furthermore, we introduce an innovative approach for constructing an iterative self-calibration training set, which optimizes LLMs to efficiently harness uncertainty scores for capturing critical information and enhancing response accuracy. Our proposed framework significantly improves performance on both closed-source and open-weight LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.16172",
      "pdf_url": "https://arxiv.org/pdf/2506.16172",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-06-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22934",
      "title": "Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging",
      "authors": [
        "Haobo Zhang",
        "Jiayu Zhou"
      ],
      "abstract": "Fine-tuning large language models (LMs) for individual tasks yields strong performance but is expensive for deployment and storage. Recent works explore model merging to combine multiple task-specific models into a single multi-task model without additional training. However, existing merging methods often fail for models fine-tuned with low-rank adaptation (LoRA), due to significant performance degradation. In this paper, we show that this issue arises from a previously overlooked interplay between model parameters and data distributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM) to constrain the LoRA subspace prior to fine-tuning, ensuring that updates relevant to one task do not adversely shift outputs for others. Our approach can seamlessly integrate with most existing merging algorithms, reducing the unintended interference among tasks. Extensive experiments on eight datasets, tested with three widely used LMs and two large LMs, demonstrate that our method not only boosts merging performance but also preserves single-task accuracy. Furthermore, our approach exhibits greater robustness to the hyperparameters of merging. These results highlight the importance of data-parameter interaction in model merging and offer a plug-and-play solution for merging LoRA models.",
      "arxiv_url": "https://arxiv.org/abs/2505.22934",
      "pdf_url": "https://arxiv.org/pdf/2505.22934",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02321",
      "title": "Quantifying Misattribution Unfairness in Authorship Attribution",
      "authors": [
        "Pegah Alipoormolabashi",
        "Ajay Patel",
        "Niranjan Balasubramanian"
      ],
      "abstract": "Authorship misattribution can have profound consequences in real life. In forensic settings simply being considered as one of the potential authors of an evidential piece of text or communication can result in undesirable scrutiny. This raises a fairness question: Is every author in the candidate pool at equal risk of misattribution? Standard evaluation measures for authorship attribution systems do not explicitly account for this notion of fairness. We introduce a simple measure, Misattribution Unfairness Index (MAUIk), which is based on how often authors are ranked in the top k for texts they did not write. Using this measure we quantify the unfairness of five models on two different datasets. All models exhibit high levels of unfairness with increased risks for some authors. Furthermore, we find that this unfairness relates to how the models embed the authors as vectors in the latent search space. In particular, we observe that the risk of misattribution is higher for authors closer to the centroid (or center) of the embedded authors in the haystack. These results indicate the potential for harm and the need for communicating with and calibrating end users on misattribution risk when building and providing such models for downstream use.",
      "arxiv_url": "https://arxiv.org/abs/2506.02321",
      "pdf_url": "https://arxiv.org/pdf/2506.02321",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11436",
      "title": "ADO: Automatic Data Optimization for Inputs in LLM Prompts",
      "authors": [
        "Sam Lin",
        "Wenyue Hua",
        "Lingyao Li",
        "Zhenting Wang",
        "Yongfeng Zhang"
      ],
      "abstract": "This study explores a novel approach to enhance the performance of Large Language Models (LLMs) through the optimization of input data within prompts. While previous research has primarily focused on refining instruction components and augmenting input data with in-context examples, our work investigates the potential benefits of optimizing the input data itself. We introduce a two-pronged strategy for input data optimization: content engineering and structural reformulation. Content engineering involves imputing missing values, removing irrelevant attributes, and enriching profiles by generating additional information inferred from existing attributes. Subsequent to content engineering, structural reformulation is applied to optimize the presentation of the modified content to LLMs, given their sensitivity to input format. Our findings suggest that these optimizations can significantly improve the performance of LLMs in various tasks, offering a promising avenue for future research in prompt engineering. The source code is available at https://anonymous.4open.science/r/ADO-6BC5/",
      "arxiv_url": "https://arxiv.org/abs/2502.11436",
      "pdf_url": "https://arxiv.org/pdf/2502.11436",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "c9df551e7ac32eb21b4ac72f9190d30d9aff1fa6",
      "title": "Parameter-Aware Contrastive Knowledge Editing: Tracing and Rectifying based on Critical Transmission Paths",
      "authors": [
        "Songlin Zhai",
        "Yuan Meng",
        "Yu-xin Zhang",
        "Guilin Qi"
      ],
      "abstract": "Large language models (LLMs) have encoded vast amounts of knowledge in their parameters, but the acquired knowledge can sometimes be incorrect or outdated over time, necessitating rectification after pre-training. Traditional localized methods in knowledge-based model editing (KME) typically assume that knowledge is stored in particular intermediate layers. However, recent research suggests that these methods do not identify the optimal locations for parameter editing, as knowledge gradually accumulates across all layers in LLMs during the forward pass rather than being stored in specific layers. This paper, for the first time, introduces the concept of critical transmission paths into KME for parameter updating. Specifically, these paths capture the key information flows that significantly influence the model predictions for the editing process. To facilitate this process, we also design a parameter-aware contrastive rectifying algorithm that considers less important paths as contrastive examples. Experiments on two prominent datasets and three widely used LLMs demonstrate the superiority of our method in editing performance.",
      "arxiv_url": "https://www.semanticscholar.org/paper/c9df551e7ac32eb21b4ac72f9190d30d9aff1fa6",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "c9f89777af5775509657e5553254af344e3491bb",
      "title": "Battling against Tough Resister: Strategy Planning with Adversarial Game for Non-collaborative Dialogues",
      "authors": [
        "Haiyang Wang",
        "Zhiliang Tian",
        "Yuchen Pan",
        "Xin Song",
        "Xin Niu",
        "Minlie Huang",
        "Bin Zhou"
      ],
      "abstract": "Non-collaborative dialogue involves two participants with conflicting interests engaging in a multi-round dialogue to achieve their own goals. Strategy planning is the key to guiding both participants towards a consensus. Most LLMs-based methods use stimulus prompts or external strategy planners for strategy planning. However, stimulus prompts fail to teach LLMs to plan dialogue strategies explicitly. Moreover, training external strategy planners doesn’t fully account for adversarial interactions, thereby limiting their effectiveness against tough resisters. In this paper, to mitigate the above issues, we propose GAIA , a G ame-based A dversarial self-play I nter A ctive training paradigm, which constructs an adversarial two-player (a persuader and a resister) zero-sum game and guides the game to approximate Nash Equilibrium (NE) via reinforcement learning (RL) for the non-collaborative dialogues. First, we design a Chain-of-Mind prompt to reason the resister’s dialogue act step-by-step to plan the persuasive strategies. Secondly, to adversarially improve the persuader, we construct diverse resistant planners and theoretically improve the persuader’s optimal lower bound. Finally, we iteratively optimise their policies via adversarial self-play interactive RL and design an ϵ -NE verification algorithm to approximate the game’s NE. Experiments on three datasets show that our model obtains state-of-the-art performance.",
      "arxiv_url": "https://www.semanticscholar.org/paper/c9f89777af5775509657e5553254af344e3491bb",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01334",
      "title": "Enhancing Interpretable Image Classification Through LLM Agents and Conditional Concept Bottleneck Models",
      "authors": [
        "Yiwen Jiang",
        "Deval Mehta",
        "Wei Feng",
        "Zongyuan Ge"
      ],
      "abstract": "Concept Bottleneck Models (CBMs) decompose image classification into a process governed by interpretable, human-readable concepts. Recent advances in CBMs have used Large Language Models (LLMs) to generate candidate concepts. However, a critical question remains: What is the optimal number of concepts to use? Current concept banks suffer from redundancy or insufficient coverage. To address this issue, we introduce a dynamic, agent-based approach that adjusts the concept bank in response to environmental feedback, optimizing the number of concepts for sufficiency yet concise coverage. Moreover, we propose Conditional Concept Bottleneck Models (CoCoBMs) to overcome the limitations in traditional CBMs' concept scoring mechanisms. It enhances the accuracy of assessing each concept's contribution to classification tasks and feature an editable matrix that allows LLMs to correct concept scores that conflict with their internal knowledge. Our evaluations across 6 datasets show that our method not only improves classification accuracy by 6% but also enhances interpretability assessments by 30%.",
      "arxiv_url": "https://arxiv.org/abs/2506.01334",
      "pdf_url": "https://arxiv.org/pdf/2506.01334",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19249",
      "title": "Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases",
      "authors": [
        "Michael Y Hu",
        "Jackson Petty",
        "Chuan Shi",
        "William Merrill",
        "Tal Linzen"
      ],
      "abstract": "Pretraining language models on formal language can improve their acquisition of natural language. Which features of the formal language impart an inductive bias that leads to effective transfer? Drawing on insights from linguistics and complexity theory, we hypothesize that effective transfer occurs when two conditions are met: the formal language should capture the dependency structures present in natural language, and it should remain within the computational limitations of the model architecture. We experiment with pre-pretraining (training on formal language before natural languages) on transformers and find that formal languages capturing hierarchical dependencies indeed enable language models to achieve lower loss on natural language and better linguistic generalization compared to other formal languages. We also find modest support for the hypothesis that the formal language should fall within the computational limitations of the architecture. Strikingly, pre-pretraining reduces loss more efficiently than training on a matched amount of natural language. For a 1B-parameter language model trained on roughly 1.6B tokens of natural language, pre-pretraining achieves the same loss and better linguistic generalization with a 33% smaller token budget. Finally, we also give mechanistic evidence of transfer from formal to natural language: attention heads acquired during pre-pretraining remain crucial for the model's performance on syntactic evaluations.",
      "arxiv_url": "https://arxiv.org/abs/2502.19249",
      "pdf_url": "https://arxiv.org/pdf/2502.19249",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18509",
      "title": "Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents",
      "authors": [
        "Ivoline C. Ngong",
        "S. Kadhe",
        "Hao Wang",
        "K. Murugesan",
        "Justin D. Weisz",
        "Amit Dhurandhar",
        "K. Ramamurthy"
      ],
      "abstract": "Conversational agents are increasingly woven into individuals'personal lives, yet users often underestimate the privacy risks associated with them. The moment users share information with these agents-such as large language models (LLMs)-their private information becomes vulnerable to exposure. In this paper, we characterize the notion of contextual privacy for user interactions with LLM-based Conversational Agents (LCAs). It aims to minimize privacy risks by ensuring that users (sender) disclose only information that is both relevant and necessary for achieving their intended goals when interacting with LCAs (untrusted receivers). Through a formative design user study, we observe how even\"privacy-conscious\"users inadvertently reveal sensitive information through indirect disclosures. Based on insights from this study, we propose a locally deployable framework that operates between users and LCAs, identifying and reformulating out-of-context information in user prompts. Our evaluation using examples from ShareGPT shows that lightweight models can effectively implement this framework, achieving strong gains in contextual privacy while preserving the user's intended interaction goals. Notably, about 76% of participants in our human evaluation preferred the reformulated prompts over the original ones, validating the usability and effectiveness of contextual privacy in our proposed framework. We opensource the code at https://github.com/IBM/contextual-privacy-LLM.",
      "arxiv_url": "https://arxiv.org/abs/2502.18509",
      "pdf_url": "https://arxiv.org/pdf/2502.18509",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.11065",
      "title": "Smotrom tvoja pa ander drogoj verden! Resurrecting Dead Pidgin with Generative Models: Russenorsk Case Study",
      "authors": [
        "Alexey Tikhonov",
        "Sergei Shteiner",
        "Anna Bykova",
        "Ivan P. Yamshchikov"
      ],
      "abstract": "Russenorsk, a pidgin language historically used in trade interactions between Russian and Norwegian speakers, represents a unique linguistic phenomenon. In this paper, we attempt to analyze its lexicon using modern large language models (LLMs), based on surviving literary sources. We construct a structured dictionary of the language, grouped by synonyms and word origins. Subsequently, we use this dictionary to formulate hypotheses about the core principles of word formation and grammatical structure in Russenorsk and show which hypotheses generated by large language models correspond to the hypotheses previously proposed ones in the academic literature. We also develop a\"reconstruction\"translation agent that generates hypothetical Russenorsk renderings of contemporary Russian and Norwegian texts.",
      "arxiv_url": "https://arxiv.org/abs/2506.11065",
      "pdf_url": "https://arxiv.org/pdf/2506.11065",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20127",
      "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
      "authors": [
        "Zexiong Ma",
        "Chao Peng",
        "Pengfei Gao",
        "Xiangxin Meng",
        "Yanzhen Zou",
        "Bing Xie"
      ],
      "abstract": "Mainstream issue-resolving frameworks predominantly rely on commercial models, leading to high costs and privacy concerns. Existing training approaches for issue resolving struggle with poor generalization and fail to fully leverage open-source development resources. We propose Subtask-oriented Reinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue resolving capability of LLMs. We decomposes issue resolving into structured subtasks: file localization, function localization, line localization, and code edit generation. SoRFT consists of two training stages: (1) rejection-sampled supervised fine-tuning, Chain of Thought (CoT) data is filtered using ground-truth before fine-tuning the LLM, and (2) rule-based reinforcement learning, which leverages PPO with ground-truth based rewards. We evaluate the SoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving state-of-the-art (SOTA) performance among open-source models (e.g., resolve 21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental results demonstrate that SoRFT significantly enhances issue-resolving performance, improves model generalization, and provides a cost-efficient alternative to commercial models.",
      "arxiv_url": "https://arxiv.org/abs/2502.20127",
      "pdf_url": "https://arxiv.org/pdf/2502.20127",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01319",
      "title": "Learning Sparsity for Effective and Efficient Music Performance Question Answering",
      "authors": [
        "Xingjian Diao",
        "Tianzhen Yang",
        "Chunhui Zhang",
        "Weiyi Wu",
        "Ming Cheng",
        "Jiang Gui"
      ],
      "abstract": "Music performances, characterized by dense and continuous audio as well as seamless audio-visual integration, present unique challenges for multimodal scene understanding and reasoning. Recent Music Performance Audio-Visual Question Answering (Music AVQA) datasets have been proposed to reflect these challenges, highlighting the continued need for more effective integration of audio-visual representations in complex question answering. However, existing Music AVQA methods often rely on dense and unoptimized representations, leading to inefficiencies in the isolation of key information, the reduction of redundancy, and the prioritization of critical samples. To address these challenges, we introduce Sparsify, a sparse learning framework specifically designed for Music AVQA. It integrates three sparsification strategies into an end-to-end pipeline and achieves state-of-the-art performance on the Music AVQA datasets. In addition, it reduces training time by 28.32% compared to its fully trained dense counterpart while maintaining accuracy, demonstrating clear efficiency gains. To further improve data efficiency, we propose a key-subset selection algorithm that selects and uses approximately 25% of MUSIC-AVQA v2.0 training data and retains 70-80% of full-data performance across models.",
      "arxiv_url": "https://arxiv.org/abs/2506.01319",
      "pdf_url": "https://arxiv.org/pdf/2506.01319",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17924",
      "title": "FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models",
      "authors": [
        "Hongzhan Lin",
        "Yang Deng",
        "Yuxuan Gu",
        "Wenxuan Zhang",
        "Jing Ma",
        "See-Kiong Ng",
        "Tat-Seng Chua"
      ],
      "abstract": "Large Language Models (LLMs) have significantly advanced the fact-checking studies. However, existing automated fact-checking evaluation methods rely on static datasets and classification metrics, which fail to automatically evaluate the justification production and uncover the nuanced limitations of LLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven framework that adaptively and dynamically assesses LLMs' fact-checking capabilities. Leveraging importance sampling principles and multi-agent collaboration, FACT-AUDIT generates adaptive and scalable datasets, performs iterative model-centric evaluations, and updates assessments based on model-specific responses. By incorporating justification production alongside verdict prediction, this framework provides a comprehensive and evolving audit of LLMs' factual reasoning capabilities, to investigate their trustworthiness. Extensive experiments demonstrate that FACT-AUDIT effectively differentiates among state-of-the-art LLMs, providing valuable insights into model strengths and limitations in model-centric fact-checking analysis.",
      "arxiv_url": "https://arxiv.org/abs/2502.17924",
      "pdf_url": "https://arxiv.org/pdf/2502.17924",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "cb93b9c33c2de4f1f678ab4ae8632036a9f7663d",
      "title": "AQuAECHR: Attributed Question Answering for European Court of Human Rights",
      "authors": [
        "Korbinian Q. Weidinger",
        "Santosh T.Y.S.S",
        "O. Ichim",
        "Matthias Grabmair"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/cb93b9c33c2de4f1f678ab4ae8632036a9f7663d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.21288",
      "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness",
      "authors": [
        "Istabrak Abbes",
        "Gabriele Prato",
        "Quentin Fournier",
        "Fernando Rodriguez",
        "Alaa Boukhary",
        "Adam Elwood",
        "Sarath Chandar"
      ],
      "abstract": "Augmenting large language models (LLMs) with external context significantly improves their performance in natural language processing (NLP) tasks. However, LLMs struggle to answer queries reliably when the provided context lacks information, often resorting to ungrounded speculation or internal knowledge. Groundedness - generating responses strictly supported by the context - is essential for ensuring factual consistency and trustworthiness. This study focuses on detecting whether a given query is grounded in a document provided in context before the costly answer generation by LLMs. Such a detection mechanism can significantly reduce both inference time and resource consumption. We show that lightweight, task specific encoder models such as RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in groundedness detection while reducing inference latency by orders of magnitude. The code is available at : https://github.com/chandarlab/Hallucinate-less",
      "arxiv_url": "https://arxiv.org/abs/2506.21288",
      "pdf_url": "https://arxiv.org/pdf/2506.21288",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "cbbdd7d79d157c7e503121cf26db8241c1ca9088",
      "title": "Tracking Life's Ups and Downs: Mining Life Events from Social Media Posts for Mental Health Analysis",
      "authors": [
        "Minghao Lv",
        "Siyuan Chen",
        "Haoan Jin",
        "Minghao Yuan",
        "Qianqian Ju",
        "Yujia Peng",
        "Ke Zhu",
        "Mengyue Wu"
      ],
      "abstract": "Social media platforms possess considerable potential in the realm of exploring mental health. Previous research has indicated that major life events can greatly impact individuals’ mental health. However, due to the complexity and ambiguity nature of life events, shedding its light on social media data is quite challenging. In this paper, we are dedicated to uncovering life events mentioned in posts on social media. We hereby provide a carefully-annotated social media event dataset, PsyEvent , which encompasses 12 major life event categories that are likely to occur in everyday life. This dataset is human-annotated under iterative procedure and boasts a high level of quality. Furthermore, by applying the life events extracted from posts to downstream tasks such as early risk detection of depression and suicide risk prediction, we have observed a considerable improvement in performance. This suggests that extracting life events from social media can be beneficial for the analysis of individuals’ mental health.",
      "arxiv_url": "https://www.semanticscholar.org/paper/cbbdd7d79d157c7e503121cf26db8241c1ca9088",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24523",
      "title": "Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors",
      "authors": [
        "Andrea Pedrotti",
        "Michele Papucci",
        "Cristiano Ciaccio",
        "Alessio Miaschi",
        "Giovanni Puccetti",
        "F. Dell’Orletta",
        "Andrea Esuli"
      ],
      "abstract": "Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we present a pipeline to test the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. To challenge the detectors, we fine-tune language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT). This exploits the detectors' reliance on stylistic clues, making new generations more challenging to detect. Additionally, we analyze the linguistic shifts induced by the alignment and which features are used by detectors to detect MGT texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detection performance. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts.",
      "arxiv_url": "https://arxiv.org/abs/2505.24523",
      "pdf_url": "https://arxiv.org/pdf/2505.24523",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01695",
      "title": "Generate, Discriminate, Evolve: Enhancing Context Faithfulness via Fine-Grained Sentence-Level Self-Evolution",
      "authors": [
        "Kun Li",
        "Tianhua Zhang",
        "Yunxiang Li",
        "Hongyin Luo",
        "Abdalla Moustafa",
        "Xixin Wu",
        "James R. Glass",
        "Helen M. Meng"
      ],
      "abstract": "Improving context faithfulness in large language models is essential for developing trustworthy retrieval augmented generation systems and mitigating hallucinations, especially in long-form question answering (LFQA) tasks or scenarios involving knowledge conflicts. Existing methods either intervene LLMs only at inference without addressing their inherent limitations or overlook the potential for self-improvement. In this paper, we introduce GenDiE (Generate, Discriminate, Evolve), a novel self-evolving framework that enhances context faithfulness through fine-grained sentence-level optimization. GenDiE combines both generative and discriminative training, equipping LLMs with self-generation and self-scoring capabilities to facilitate iterative self-evolution. This supports both data construction for model alignment and score-guided search during inference. Furthermore, by treating each sentence in a response as an independent optimization unit, GenDiE effectively addresses the limitations of previous approaches that optimize at the holistic answer level, which may miss unfaithful details. Experiments on ASQA (in-domain LFQA) and ConFiQA (out-of-domain counterfactual QA) datasets demonstrate that GenDiE surpasses various baselines in both faithfulness and correctness, and exhibits robust performance for domain adaptation.",
      "arxiv_url": "https://arxiv.org/abs/2503.01695",
      "pdf_url": "https://arxiv.org/pdf/2503.01695",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "cc1f459343e8e93b7d41e0627f29bf9945fcaa9a",
      "title": "A Framework for Flexible Extraction of Clinical Event Contextual Properties from Electronic Health Records",
      "authors": [
        "Shubham Agarwal",
        "Thomas Searle",
        "M. Ratas",
        "Anthony Shek",
        "James T. Teo",
        "Richard Dobson"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/cc1f459343e8e93b7d41e0627f29bf9945fcaa9a",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.08907",
      "title": "Dialect Normalization using Large Language Models and Morphological Rules",
      "authors": [
        "Antonios Dimakis",
        "John Pavlopoulos",
        "Antonios Anastasopoulos",
        "Α. Archimedes",
        "Aaron Grattafiori",
        "Abhimanyu Dubey",
        "Abhinav Jauhri",
        "Abhinav Pandey",
        "Abhishek Kadian",
        "Ahmad Al-Dahle",
        "Aiesha Letman",
        "Akhil Mathur",
        "Alan Schel-ten",
        "Alex Vaughan",
        "Amy Yang",
        "Angela Fan",
        "Anirudh Goyal",
        "Anthony S. Hartshorn",
        "Aobo Yang",
        "Archi Mitra",
        "A. Sravankumar",
        "A. Korenev",
        "Arthur Hinsvark",
        "Arun Rao",
        "Aston Zhang",
        "Aurelien Ro-driguez",
        "Austen Gregerson",
        "Ava Spataru",
        "Baptiste Rozière",
        "Bethany Biron",
        "Binh Tang",
        "Bob-bie Chern",
        "Charlotte Caucheteux",
        "Chaya Nayak",
        "Chloe Bi",
        "Chris Marra",
        "Chris McConnell",
        "Christian Keller",
        "C. Touret",
        "Chunyang Wu",
        "Corinne Wong",
        "Cristian Canton Ferrer",
        "Cyrus Nikolaidis",
        "Damien Al-lonsius",
        "Daniel Song",
        "Danielle Pintz",
        "Danny Livshits",
        "Danny Wyatt",
        "David Esiobu",
        "Dhruv Choudhary",
        "Dhruv Mahajan",
        "Diego Garcia-Olano",
        "Diego Perino",
        "Dieuwke Hupkes",
        "Egor Lakomkin",
        "Ehab A. AlBadawy",
        "E. Lobanova",
        "Emily Dinan",
        "Eric Michael Smith",
        "Filip Radenovic",
        "Francisco Guzmán",
        "Frank Zhang",
        "Gabriele Synnaeve",
        "Gabrielle Lee",
        "Georgia Lewis",
        "Govind Thattai",
        "Graeme Nail",
        "Grégoire Mi-alon",
        "Guan Pang",
        "Guillem Cucurell",
        "Hailey Nguyen",
        "Han-nah Korevaar",
        "Hu Xu",
        "Hugo Touvron",
        "Imanol Iliyan Zarov",
        "Arrieta Ibarra",
        "Is-abel Kloumann",
        "Ishan Misra",
        "Ivan Evtimov",
        "Jack Zhang",
        "Jade Copet",
        "Jaewon Lee",
        "Jan Geffert",
        "Jana Vranes",
        "Jason Park",
        "Jay Mahadeokar",
        "Jeet Shah",
        "J. V. D. Linde",
        "Jennifer Billock",
        "Jenny Hong",
        "Jenya Lee",
        "J. Fu",
        "Jianfeng Chi",
        "Jianyu Huang",
        "Jiawen Liu",
        "Jie Wang",
        "Jiecao Yu",
        "Joanna Bitton",
        "Joe Spisak",
        "Jongsoo Park",
        "Joseph Rocca",
        "J. Johnstun",
        "Joshua Saxe",
        "Junteng Jia",
        "Kalyan Vasuden Alwala",
        "Karthik Prasad",
        "K. Upasani",
        "Kate Plawiak",
        "Keqian Li",
        "Kenneth Heafield",
        "Kevin R. Stone",
        "Khalid El-Arini",
        "Krithika Iyer",
        "Kshitiz Malik",
        "Kuen-ley Chiu",
        "Kunal Bhalla",
        "Kushal Lakhotia",
        "Lauren Rantala-Yeary",
        "Laurens van der Maaten",
        "Lawrence Chen",
        "Liang Tan",
        "Liz Jenkins",
        "Louis Martin",
        "Lovish Madaan",
        "Lubo Malo",
        "Lukas Blecher",
        "Lukas Landzaat",
        "Luke de Oliveira",
        "M. Muzzi",
        "Ma-hesh Pasupuleti",
        "Mannat Singh",
        "Manohar Paluri",
        "Marcin Kardas",
        "M. Tsimpoukelli",
        "Mathew Oldham",
        "Mathieu Rita",
        "Maya Pavlova",
        "Melanie Kam-badur",
        "Mike Lewis",
        "Mitesh Min Si",
        "Kumar Singh",
        "Mona Hassan",
        "Naman Goyal",
        "Narjes Torabi",
        "Niko-lay Bashlykov",
        "Nikolay Bogoychev",
        "Niladri S. Chatterji",
        "Ning Zhang",
        "Olivier Duchenne",
        "Onur Çelebi",
        "Patrick Alrassy",
        "Pengchuan Zhang",
        "Petar Pengwei Li",
        "Peter Weng",
        "Prajjwal Bhargava",
        "P. Dubal",
        "Punit Praveen Krishnan",
        "S. Koura",
        "Puxin Xu",
        "Qing He",
        "Qingxiao Dong",
        "Ragavan Srinivasan",
        "Raj Ganapathy",
        "Ramon Calderer",
        "Ricardo Silveira Cabral",
        "Robert Stojnic",
        "R. Raileanu",
        "R. Maheswari",
        "Rohit Girdhar",
        "Rohit Patel",
        "Romain Sauvestre",
        "Ron-nie Polidoro",
        "Roshan Sumbaly",
        "Ross Taylor",
        "Ruan Silva",
        "R. Hou",
        "Rui Wang",
        "S. Hosseini",
        "Sa-hana Chennabasappa",
        "Sanjay Singh",
        "Sean Bell",
        "Seo-hyun Sonia Kim",
        "Sergey Edunov",
        "Shaoliang Nie",
        "Sharath Narang",
        "S. Raparthy",
        "Sheng Shen",
        "Shengye Wan",
        "Shruti Bhosale",
        "Shun Zhang",
        "Simon Van-denhende",
        "Soumya Batra",
        "Spencer Whitman",
        "Sten Sootla",
        "S. Collot",
        "Suchin Gururangan",
        "S. Borodinsky",
        "Tamar Herman",
        "T. Fowler",
        "Tarek Sheasha",
        "Thomas Georgiou",
        "Thomas Scialom",
        "Tobias Speckbacher",
        "Todor Mihaylov",
        "Tong Xiao",
        "Ujjwal Karn",
        "Vedanuj Goswami",
        "Vibhor Gupta",
        "Vignesh Ramanathan",
        "Viktor Kerkez",
        "Vincent Gonguet",
        "Vir-ginie Do",
        "Vish Vogeti",
        "Vítor Albiero",
        "Vladan Petro-vic",
        "Weiwei Chu",
        "Wenhan Xiong",
        "Wenyin Fu",
        "Yasmine Gaur",
        "Yi Babaei",
        "Wen Yiwen",
        "Yuchen Song",
        "Yue Zhang",
        "Yuning Li",
        "Mao Zacharie Delpierre",
        "Zheng Coudert",
        "Zheng Yan",
        "Zoe Chen",
        "Aaditya Papakipos",
        "Aayushi Singh",
        "Abha Sri-vastava",
        "Adam Jain",
        "Adam Kelsey",
        "S. Adithya",
        "Adolfo Gangidi",
        "Ahuva Victoria",
        "G. Ajay",
        "A. Menon",
        "Alex Sharma",
        "Alex Boesenberg",
        "Allie Baevski",
        "Amanda Feinstein",
        "Amit Kallet",
        "Amos San-gani",
        "Anam Teo",
        "Andrei Yunus",
        "A. Lupu",
        "Andrew Alvarado",
        "A. Caples",
        "Andrew Gu",
        "Andrew Ho",
        "Andrew Poulton",
        "A. Ryan",
        "Annie Dong",
        "Annie Franco",
        "Anuj Goyal",
        "Apara-jita Saraf",
        "Arkabandhu Chowdhury",
        "Ashley Gabriel",
        "Ashwin Bharambe",
        "Assaf Eisenman",
        "Azadeh Yaz-dan",
        "Beau James",
        "Ben Maurer",
        "B. Leonhardi",
        "Bernie Huang",
        "Beth Loyd",
        "Beto de Paola",
        "Bhargavi Paranjape",
        "Bing Liu",
        "Bo Wu",
        "B. Ni",
        "Braden Han-cock",
        "Bram Wasti",
        "Brandon Spence",
        "B. Stojkovic",
        "Brian Gamido",
        "Britt Montalvo",
        "Carl Parker",
        "Carly Burton",
        "Catalina Mejia",
        "Ce Liu",
        "Changhan Wang",
        "Changkyu Kim",
        "Chao Zhou",
        "Chester Hu",
        "Ching-Hsiang Chu",
        "Chris Cai",
        "Chris Tindal",
        "Christoph Fe-ichtenhofer",
        "Cynthia Gao",
        "Damon Civin",
        "Dana Beaty",
        "Daniel Kreymer",
        "Daniel Li",
        "David Adkins",
        "David Xu",
        "Davide Testuggine",
        "Delia David",
        "Devi Parikh",
        "Elaine Montgomery",
        "Eleonora Presani",
        "Emily Hahn",
        "Emily Wood",
        "Eric-Tuan Le",
        "Erik Brinkman",
        "E. Arcaute",
        "Evan Dunbar",
        "Evan Smothers",
        "Fei Sun",
        "F. Kreuk",
        "Feng Tian",
        "Filippos Kokkinos",
        "Firat Ozgenel",
        "Francesco Caggioni",
        "Frank J. Kanayet",
        "Frank Seide",
        "Gabriela Medina Florez",
        "Gabriella Schwarz",
        "Gada Badeer",
        "Georgia Swee",
        "Gil Halpern",
        "Grant Herman",
        "Grigory Sizov",
        "Guangyi",
        "Guna Zhang",
        "Hakan Lakshminarayanan",
        "Hamid Inan",
        "Han Shojanaz-eri",
        "Han Zou",
        "Hanwen Wang",
        "Haroun Zha",
        "Harrison Habeeb",
        "Helen Rudolph",
        "Henry Suk",
        "Hunter As-pegren",
        "Hongyuan Goldman",
        "Ibrahim Zhan",
        "Igor Damlaj",
        "Igor Molybog",
        "Ilias Tufanov",
        "Leontiadis",
        "Jennifer Chan",
        "Jenny Zhen",
        "J. Reizenstein",
        "J. Teboul",
        "Jessica Zhong",
        "Jian Jin",
        "Jingyi Yang",
        "Joe Cummings",
        "Jon Carvill",
        "Jon Shepard",
        "Jonathan Mc-Phie",
        "Jonathan Torres",
        "Josh Ginsburg",
        "Junjie Wang",
        "Kai Wu",
        "Kam Hou",
        "Karan Saxena",
        "Kartikay Khan-delwal",
        "Katayoun Zand",
        "Kathy Matosich",
        "K. Veeraraghavan",
        "Kelly Michelena",
        "K. Jagadeesh",
        "Kun Huang",
        "Kunal Chawla",
        "Kyle Huang",
        "Lailin Chen",
        "Lavender A Lakshya Garg",
        "Leandro Silva",
        "Lee Bell",
        "Lei Zhang",
        "Liangpeng Guo",
        "Licheng Yu",
        "Liron Moshkovich",
        "Luca Wehrst-edt",
        "Madian Khabsa",
        "Manav Avalani",
        "M. Bhatt",
        "Martynas Mankus",
        "Matan Hasson",
        "M. Lennie",
        "Matthias Reso",
        "M. Groshev",
        "Maxim Naumov",
        "Maya Lathi",
        "Meghan Keneally",
        "Miao Liu",
        "Michal Seltzer",
        "M. Valko",
        "Mihir Michelle Restrepo",
        "Mik Vyatskov",
        "Mikayel Samvelyan",
        "Mike Clark",
        "M. Macey",
        "Mike Wang",
        "Miquel Jubert Hermoso",
        "Mo Metanat",
        "Mohammad Rastegari",
        "Mun-ish Bansal",
        "N. Santhanam",
        "Natascha Parks",
        "Natasha White",
        "Navy-ata Bawa",
        "Nayan Singhal",
        "Nick Egebo",
        "Nicolas Usunier",
        "Nikolay Nikhil Mehta",
        "Pavlovich Laptev",
        "Ning Dong",
        "Norman Cheng",
        "Oleg Chernoguz",
        "O. Hart",
        "Omkar Salpekar",
        "Ozlem Kalinli",
        "Parkin Kent",
        "Parth Parekh",
        "Paul Saab",
        "Pavan Balaji",
        "P. Rittner",
        "Philip Bontrager",
        "Pierre Roux",
        "Piotr Dollár",
        "Polina Zvyagina",
        "Prashant Ratanchandani",
        "P. Yuvraj",
        "Qian Liang",
        "Rachad Alao",
        "Rachel Rodriguez",
        "Rafi Ayub",
        "Raghotham Murthy",
        "Raghu Nayani",
        "Rahul Mitra",
        "Rangaprabhu Parthasarathy",
        "Raymond Li",
        "Rebekkah Hogan",
        "Robin Battey",
        "Rocky Wang",
        "Russ Howes",
        "Ruty Rinott",
        "Sachin Mehta",
        "Sachin Siby",
        "Sai Jayesh",
        "Samyak Bondu",
        "Sara Datta",
        "Sara Chugh",
        "Sargun Hunt",
        "Sasha Dhillon",
        "S. Satadru",
        "Saurabh Pan",
        "Saurabh Mahajan",
        "Verma Seiji",
        "Sharadh Yamamoto",
        "Shaun Ramaswamy",
        "Shaun Lind-say",
        "Sheng Lindsay",
        "Sheng Feng",
        "Lin Shengxin Cindy",
        "Shishir Zha",
        "Shiva Patil",
        "Shankar Shuqiang",
        "Shuqiang Zhang",
        "Sinong Zhang",
        "Wang Sneha",
        "Soji Agarwal",
        "S. Sajuyigbe",
        "Chintala Stephanie",
        "Stephen Max",
        "Steve Chen",
        "Steve Kehoe",
        "Sudarshan Satterfield",
        "S. Govindaprasad",
        "Gupta Summer",
        "Sungmin Deng",
        "Sunny Cho",
        "Suraj Virk",
        "Sy Subramanian",
        "Sy Choudhury",
        "Tal Goldman",
        "T. Remez",
        "Tamara Glaser",
        "Thilo Best",
        "Koehler Thomas",
        "Tianhe Robinson",
        "Tianjun Li",
        "Tim Zhang",
        "Tim Matthews",
        "Tzook Chou",
        "Varun Shaked",
        "Victoria Vontimitta",
        "Victoria Ajayi",
        "Vijai Montanez",
        "V. Mohan",
        "Vishal Kumar",
        "Vlad Mangla",
        "Vlad Ionescu",
        "V. Poenaru",
        "Mihailescu Vladimir",
        "Wei Ivanov"
      ],
      "abstract": "Natural language understanding systems struggle with low-resource languages, including many dialects of high-resource ones. Dialect-to-standard normalization attempts to tackle this issue by transforming dialectal text so that it can be used by standard-language tools downstream. In this study, we tackle this task by introducing a new normalization method that combines rule-based linguistically informed transformations and large language models (LLMs) with targeted few-shot prompting, without requiring any parallel data. We implement our method for Greek dialects and apply it on a dataset of regional proverbs, evaluating the outputs using human annotators. We then use this dataset to conduct downstream experiments, finding that previous results regarding these proverbs relied solely on superficial linguistic information, including orthographic artifacts, while new observations can still be made through the remaining semantics.",
      "arxiv_url": "https://arxiv.org/abs/2506.08907",
      "pdf_url": "https://arxiv.org/pdf/2506.08907",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "cc4a10ab3c06d56a68ced5352429cfba0642c38f",
      "title": "Causal Denoising Prototypical Network for Few-Shot Multi-label Aspect Category Detection",
      "authors": [
        "Jin Cui",
        "Xinfeng Wang",
        "Yoshimi Suzuki",
        "Fumiyo Fukumoto"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/cc4a10ab3c06d56a68ced5352429cfba0642c38f",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "cc5097cfcf42778caebfa80141e4d58350afb799",
      "title": "A Tale of Evaluating Factual Consistency: Case Study on Long Document Summarization Evaluation",
      "authors": [
        "Yang Zhong",
        "Diane J. Litman"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/cc5097cfcf42778caebfa80141e4d58350afb799",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.15815",
      "title": "What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns",
      "authors": [
        "Michael A. Hedderich",
        "Anyi Wang",
        "Raoyuan Zhao",
        "Florian Eichin",
        "Barbara Plank"
      ],
      "abstract": "Prompt engineering for large language models is challenging, as even small prompt perturbations or model changes can significantly impact the generated output texts. Existing evaluation methods of LLM outputs, either automated metrics or human evaluation, have limitations, such as providing limited insights or being labor-intensive. We propose Spotlight, a new approach that combines both automation and human analysis. Based on data mining techniques, we automatically distinguish between random (decoding) variations and systematic differences in language model outputs. This process provides token patterns that describe the systematic differences and guide the user in manually analyzing the effects of their prompts and changes in models efficiently. We create three benchmarks to quantitatively test the reliability of token pattern extraction methods and demonstrate that our approach provides new insights into established prompt data. From a human-centric perspective, through demonstration studies and a user study, we show that our token pattern approach helps users understand the systematic differences of language model outputs. We are further able to discover relevant differences caused by prompt and model changes (e.g. related to gender or culture), thus supporting the prompt engineering process and human-centric model behavior research.",
      "arxiv_url": "https://arxiv.org/abs/2504.15815",
      "pdf_url": "https://arxiv.org/pdf/2504.15815",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "cc8bee67e97f683ff66dc99685dd5204caf6dbb8",
      "title": "Massively Multilingual Instruction-Following Information Extraction",
      "authors": [
        "Thang Le",
        "Huy Huu Nguyen",
        "A. Luu",
        "Thien Huu Nguyen"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/cc8bee67e97f683ff66dc99685dd5204caf6dbb8",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.04350",
      "title": "HatePRISM: Policies, Platforms, and Research Integration. Advancing NLP for Hate Speech Proactive Mitigation",
      "authors": [
        "Naquee Rizwan",
        "Seid Muhie Yimam",
        "Daryna Dementieva",
        "Florian Skupin",
        "Tim Fischer",
        "Daniil Moskovskiy",
        "Aarushi Ajay Borkar",
        "Robert Geislinger",
        "Punyajoy Saha",
        "Sarthak Roy",
        "Martin Semmann",
        "Alexander Panchenko",
        "Christian Biemann",
        "Animesh Mukherjee"
      ],
      "abstract": "Despite regulations imposed by nations and social media platforms, e.g. (Government of India, 2021; European Parliament and Council of the European Union, 2022), inter alia, hateful content persists as a significant challenge. Existing approaches primarily rely on reactive measures such as blocking or suspending offensive messages, with emerging strategies focusing on proactive measurements like detoxification and counterspeech. In our work, which we call HatePRISM, we conduct a comprehensive examination of hate speech regulations and strategies from three perspectives: country regulations, social platform policies, and NLP research datasets. Our findings reveal significant inconsistencies in hate speech definitions and moderation practices across jurisdictions and platforms, alongside a lack of alignment with research efforts. Based on these insights, we suggest ideas and research direction for further exploration of a unified framework for automated hate speech moderation incorporating diverse strategies.",
      "arxiv_url": "https://arxiv.org/abs/2507.04350",
      "pdf_url": "https://arxiv.org/pdf/2507.04350",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03902",
      "title": "The Harmonic Structure of Information Contours",
      "authors": [
        "Eleftheria Tsipidi",
        "Samuel Kiegeland",
        "Franz Nowak",
        "Tianyang Xu",
        "E. Wilcox",
        "A. Warstadt",
        "Ryan Cotterell",
        "Mario Giulianelli"
      ],
      "abstract": "The uniform information density (UID) hypothesis proposes that speakers aim to distribute information evenly throughout a text, balancing production effort and listener comprehension difficulty. However, language typically does not maintain a strictly uniform information rate; instead, it fluctuates around a global average. These fluctuations are often explained by factors such as syntactic constraints, stylistic choices, or audience design. In this work, we explore an alternative perspective: that these fluctuations may be influenced by an implicit linguistic pressure towards periodicity, where the information rate oscillates at regular intervals, potentially across multiple frequencies simultaneously. We apply harmonic regression and introduce a novel extension called time scaling to detect and test for such periodicity in information contours. Analyzing texts in English, Spanish, German, Dutch, Basque, and Brazilian Portuguese, we find consistent evidence of periodic patterns in information rate. Many dominant frequencies align with discourse structure, suggesting these oscillations reflect meaningful linguistic organization. Beyond highlighting the connection between information rate and discourse structure, our approach offers a general framework for uncovering structural pressures at various levels of linguistic granularity.",
      "arxiv_url": "https://arxiv.org/abs/2506.03902",
      "pdf_url": "https://arxiv.org/pdf/2506.03902",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.02450",
      "title": "Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing LLM Personalization",
      "authors": [
        "Yilun Qiu",
        "Xiaoyan Zhao",
        "Yang Zhang",
        "Yimeng Bai",
        "Wenjie Wang",
        "Hong Cheng",
        "Fuli Feng",
        "Tat-Seng Chua"
      ],
      "abstract": "Personalizing Large Language Models (LLMs) has become a critical step in facilitating their widespread application to enhance individual life experiences. In pursuit of personalization, distilling key preference information from an individual's historical data as instructional preference context to customize LLM generation has emerged as a promising direction. However, these methods face a fundamental limitation by overlooking the inter-user comparative analysis, which is essential for identifying the inter-user differences that truly shape preferences. To address this limitation, we propose Difference-aware Personalization Learning (DPL), a novel approach that emphasizes extracting inter-user differences to enhance LLM personalization. DPL strategically selects representative users for comparison and establishes a structured standard to extract meaningful, task-relevant differences for customizing LLM generation. Extensive experiments on real-world datasets demonstrate that DPL significantly enhances LLM personalization. We release our code at https://github.com/SnowCharmQ/DPL.",
      "arxiv_url": "https://arxiv.org/abs/2503.02450",
      "pdf_url": "https://arxiv.org/pdf/2503.02450",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-03-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.21605",
      "title": "MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents",
      "authors": [
        "Haoran Tan",
        "Zeyu Zhang",
        "Chen Ma",
        "Xu Chen",
        "Quanyu Dai",
        "Zhenhua Dong"
      ],
      "abstract": "Recent works have highlighted the significance of memory mechanisms in LLM-based agents, which enable them to store observed information and adapt to dynamic environments. However, evaluating their memory capabilities still remains challenges. Previous evaluations are commonly limited by the diversity of memory levels and interactive scenarios. They also lack comprehensive metrics to reflect the memory capabilities from multiple aspects. To address these problems, in this paper, we construct a more comprehensive dataset and benchmark to evaluate the memory capability of LLM-based agents. Our dataset incorporates factual memory and reflective memory as different levels, and proposes participation and observation as various interactive scenarios. Based on our dataset, we present a benchmark, named MemBench, to evaluate the memory capability of LLM-based agents from multiple aspects, including their effectiveness, efficiency, and capacity. To benefit the research community, we release our dataset and project at https://github.com/import-myself/Membench.",
      "arxiv_url": "https://arxiv.org/abs/2506.21605",
      "pdf_url": "https://arxiv.org/pdf/2506.21605",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.05527",
      "title": "Mitigating Shortcut Learning with InterpoLated Learning",
      "authors": [
        "Michalis Korakakis",
        "Andreas Vlachos",
        "Adrian Weller"
      ],
      "abstract": "Empirical risk minimization (ERM) incentivizes models to exploit shortcuts, i.e., spurious correlations between input attributes and labels that are prevalent in the majority of the training data but unrelated to the task at hand. This reliance hinders generalization on minority examples, where such correlations do not hold. Existing shortcut mitigation approaches are model-specific, difficult to tune, computationally expensive, and fail to improve learned representations. To address these issues, we propose InterpoLated Learning (InterpoLL) which interpolates the representations of majority examples to include features from intra-class minority examples with shortcut-mitigating patterns. This weakens shortcut influence, enabling models to acquire features predictive across both minority and majority examples. Experimental results on multiple natural language understanding tasks demonstrate that InterpoLL improves minority generalization over both ERM and state-of-the-art shortcut mitigation methods, without compromising accuracy on majority examples. Notably, these gains persist across encoder, encoder-decoder, and decoder-only architectures, demonstrating the method's broad applicability.",
      "arxiv_url": "https://arxiv.org/abs/2507.05527",
      "pdf_url": "https://arxiv.org/pdf/2507.05527",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24538",
      "title": "Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections",
      "authors": [
        "O. M. Mastromichalakis",
        "Jason Liartis",
        "Kristina Rose",
        "Antoine Isaac",
        "G. Stamou"
      ],
      "abstract": "Cultural Heritage (CH) data hold invaluable knowledge, reflecting the history, traditions, and identities of societies, and shaping our understanding of the past and present. However, many CH collections contain outdated or offensive descriptions that reflect historical biases. CH Institutions (CHIs) face significant challenges in curating these data due to the vast scale and complexity of the task. To address this, we develop an AI-powered tool that detects offensive terms in CH metadata and provides contextual insights into their historical background and contemporary perception. We leverage a multilingual vocabulary co-created with marginalized communities, researchers, and CH professionals, along with traditional NLP techniques and Large Language Models (LLMs). Available as a standalone web app and integrated with major CH platforms, the tool has processed over 7.9 million records, contextualizing the contentious terms detected in their metadata. Rather than erasing these terms, our approach seeks to inform, making biases visible and providing actionable insights for creating more inclusive and accessible CH collections.",
      "arxiv_url": "https://arxiv.org/abs/2505.24538",
      "pdf_url": "https://arxiv.org/pdf/2505.24538",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "cd5a74e29c8776d06a028267bc0e424d949607ab",
      "title": "A Mutual Information Perspective on Knowledge Graph Embedding",
      "authors": [
        "Jiang Li",
        "Xiangdong Su",
        "Zehua Duo",
        "Tian Lan",
        "Xiaotao Guo",
        "Guanglai Gao"
      ],
      "abstract": "Knowledge graph embedding techniques have emerged as a critical approach for addressing the issue of missing relations in knowledge graphs. However, existing methods often suffer from limitations, including high intra-group similarity, loss of semantic information, and insufficient inference capability, particularly in complex relation patterns such as 1-N and N-1 relations. To address these challenges, we introduce a novel KGE framework that leverages mutual information maximization to improve the semantic representation of entities and relations. By maximizing the mutual information between different components of triples, such as ( h, r ) and t , or ( r, t ) and h , the proposed method improves the model’s ability to preserve semantic dependencies while maintaining the relational structure of the knowledge graph. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach, with consistent performance improvements across various baseline models. Additionally, visualization analyses and case studies demonstrate the improved ability of the MI framework to capture complex relation patterns.",
      "arxiv_url": "https://www.semanticscholar.org/paper/cd5a74e29c8776d06a028267bc0e424d949607ab",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15538",
      "title": "SOTOPIA-Ω: Dynamic Strategy Injection Learning and Social Instruction Following Evaluation for Social Agents",
      "authors": [
        "Wenyuan Zhang",
        "Tianyun Liu",
        "Mengxiao Song",
        "Xiaodong Li",
        "Tingwen Liu"
      ],
      "abstract": "Despite the abundance of prior social strategies possessed by humans, there remains a paucity of research dedicated to their transfer and integration into social agents. Our proposed SOTOPIA-$\\Omega$ framework aims to address and bridge this gap, with a particular focus on enhancing the social capabilities of language agents. This framework dynamically injects multi-step reasoning strategies inspired by negotiation theory and two simple direct strategies into expert agents, thereby automating the construction of a high-quality social dialogue training corpus. Additionally, we introduce the concept of Social Instruction Following (S-IF) and propose two new S-IF evaluation metrics that complement social capability. We demonstrate that several 7B models trained on high-quality corpus not only significantly surpass the expert agent (GPT-4) in achieving social goals but also enhance S-IF performance. Analysis and variant experiments validate the advantages of dynamic construction, which can especially break the agent's prolonged deadlock.",
      "arxiv_url": "https://arxiv.org/abs/2502.15538",
      "pdf_url": "https://arxiv.org/pdf/2502.15538",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "cdb03535dc27b57ee677f70c54f6a4dd74c6e208",
      "title": "Are Dialects Better Prompters? A Case Study on Arabic Subjective Text Classification",
      "authors": [
        "Leila Moudjari",
        "Farah Benamara"
      ],
      "abstract": "This paper investigates the effect of dialectal prompting, variations in prompting script and model fine-tuning on subjective classification in Arabic dialects. To this end, we evaluate the performances of 12 widely used open source LLMs across four tasks and eight benchmark datasets. Our results reveal that specialized fine-tuned models with Arabic and Arabizi scripts dialectal prompts achieve the best re-sults, which provides new findings on the fine-tuning of LLMs for low-resource languages",
      "arxiv_url": "https://www.semanticscholar.org/paper/cdb03535dc27b57ee677f70c54f6a4dd74c6e208",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00245",
      "title": "Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity",
      "authors": [
        "Dang Nguyen",
        "Ali Payani",
        "Baharan Mirzasoleiman"
      ],
      "abstract": "Hallucination in large language models (LLMs) can be detected by assessing the uncertainty of model outputs, typically measured using entropy. Semantic entropy (SE) enhances traditional entropy estimation by quantifying uncertainty at the semantic cluster level. However, as modern LLMs generate longer one-sentence responses, SE becomes less effective because it overlooks two crucial factors: intra-cluster similarity (the spread within a cluster) and inter-cluster similarity (the distance between clusters). To address these limitations, we propose a simple black-box uncertainty quantification method inspired by nearest neighbor estimates of entropy. Our approach can also be easily extended to white-box settings by incorporating token probabilities. Additionally, we provide theoretical results showing that our method generalizes semantic entropy. Extensive empirical results demonstrate its effectiveness compared to semantic entropy across two recent LLMs (Phi3 and Llama3) and three common text generation tasks: question answering, text summarization, and machine translation. Our code is available at https://github.com/BigML-CS-UCLA/SNNE.",
      "arxiv_url": "https://arxiv.org/abs/2506.00245",
      "pdf_url": "https://arxiv.org/pdf/2506.00245",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.07845",
      "title": "Reasoning with Graphs: Structuring Implicit Knowledge to Enhance LLMs Reasoning",
      "authors": [
        "Haoyu Han",
        "Yaochen Xie",
        "Hui Liu",
        "Xianfeng Tang",
        "Sreyashi Nag",
        "William Headden",
        "Yang Li",
        "Chen Luo",
        "Shuiwang Ji",
        "Qi He",
        "Jiliang Tang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable success across a wide range of tasks; however, they still encounter challenges in reasoning tasks that require understanding and inferring relationships between distinct pieces of information within text sequences. This challenge is particularly pronounced in tasks involving multi-step processes, such as logical reasoning and multi-hop question answering, where understanding implicit relationships between entities and leveraging multi-hop connections in the given context are crucial. Graphs, as fundamental data structures, explicitly represent pairwise relationships between entities, thereby offering the potential to enhance LLMs' reasoning capabilities. External graphs have proven effective in supporting LLMs across multiple tasks. However, in many reasoning tasks, no pre-existing graph structure is provided. Can we structure implicit knowledge derived from context into graphs to assist LLMs in reasoning? In this paper, we propose Reasoning with Graphs (RwG) by first constructing explicit graphs from the context and then leveraging these graphs to enhance LLM reasoning performance on reasoning tasks. Extensive experiments demonstrate the effectiveness of the proposed method in improving both logical reasoning and multi-hop question answering tasks.",
      "arxiv_url": "https://arxiv.org/abs/2501.07845",
      "pdf_url": "https://arxiv.org/pdf/2501.07845",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14634",
      "title": "CER: Confidence Enhanced Reasoning in LLMs",
      "authors": [
        "Ali Razghandi",
        "Seyed Mohammad Hadi Hosseini",
        "M. Baghshah"
      ],
      "abstract": "Ensuring the reliability of Large Language Models (LLMs) in complex reasoning tasks remains a formidable challenge, particularly in scenarios that demand precise mathematical calculations and knowledge-intensive open-domain generation. In this work, we introduce an uncertainty-aware framework designed to enhance the accuracy of LLM responses by systematically incorporating model confidence at critical decision points. We propose an approach that encourages multi-step reasoning in LLMs and quantify the confidence of intermediate answers such as numerical results in mathematical reasoning and proper nouns in open-domain generation. Then, the overall confidence of each reasoning chain is evaluated based on confidence of these critical intermediate steps. Finally, we aggregate the answer of generated response paths in a way that reflects the reliability of each generated content (as opposed to self-consistency in which each generated chain contributes equally to majority voting). We conducted extensive experiments in five datasets, three mathematical datasets and two open-domain datasets, using four LLMs. The results consistently validate the effectiveness of our novel confidence aggregation method, leading to an accuracy improvement of up to 7.4% and 5.8% over baseline approaches in math and open-domain generation tasks, respectively. Code is publicly available at https://github.com/ Aquasar11/CER.",
      "arxiv_url": "https://arxiv.org/abs/2502.14634",
      "pdf_url": "https://arxiv.org/pdf/2502.14634",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.14433",
      "title": "Splintering Nonconcatenative Languages for Better Tokenization",
      "authors": [
        "Bar Gazit",
        "Shaltiel Shmidman",
        "Avi Shmidman",
        "Yu. Lyubarsky Ben-Gurion University of the Negev",
        "Dicta"
      ],
      "abstract": "Common subword tokenization algorithms like BPE and UnigramLM assume that text can be split into meaningful units by concatenative measures alone. This is not true for languages such as Hebrew and Arabic, where morphology is encoded in root-template patterns, or Malay and Georgian, where split affixes are common. We present SPLINTER, a pre-processing step which rearranges text into a linear form that better represents such nonconcatenative morphologies, enabling meaningful contiguous segments to be found by the tokenizer. We demonstrate SPLINTER's merit using both intrinsic measures evaluating token vocabularies in Hebrew, Arabic, and Malay; as well as on downstream tasks using BERT-architecture models trained for Hebrew.",
      "arxiv_url": "https://arxiv.org/abs/2503.14433",
      "pdf_url": "https://arxiv.org/pdf/2503.14433",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ce25d65cd74b2aed3f8be64094157f86f7e64eee",
      "title": "R-Fairness: Assessing Fairness of Ranking in Subjective Data",
      "authors": [
        "Lorenzo Balzotti",
        "Donatella Firmani",
        "J. G. Mathew",
        "Riccardo Torlone",
        "S. Amer-Yahia"
      ],
      "abstract": "Subjective data, reflecting individual opinions, permeate collaborative rating platforms like Yelp and Amazon, influencing everyday decisions. Despite the prevalence of such platforms, little attention has been given to fairness in their context, where groups of reviewers writing best-ranked reviews for best-ranked items have more influence on users’ behavior. In this paper, we design and evaluate a new framework for the assessment of fairness of rankings for different reviewer groups in collaborative rating platforms. The key contributions are evaluating group exposure for different queries and platforms and comparing how various fairness definitions behave in different settings. Experiments on real datasets reveal insights into the impact of item ranking on fairness computation and the varying robustness of these measures.",
      "arxiv_url": "https://www.semanticscholar.org/paper/ce25d65cd74b2aed3f8be64094157f86f7e64eee",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18817",
      "title": "Judge as A Judge: Improving the Evaluation of Retrieval-Augmented Generation through the Judge-Consistency of Large Language Models",
      "authors": [
        "Shuliang Liu",
        "Xinze Li",
        "Zhenghao Liu",
        "Yukun Yan",
        "Cheng Yang",
        "Zheni Zeng",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Ge Yu"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in alleviating hallucinations for Large Language Models (LLMs). However, existing automated evaluation metrics cannot fairly evaluate the outputs generated by RAG models during training and evaluation. LLM-based judgment models provide the potential to produce high-quality judgments, but they are highly sensitive to evaluation prompts, leading to inconsistencies when judging the output of RAG models. This paper introduces the Judge-Consistency (ConsJudge) method, which aims to enhance LLMs to generate more accurate evaluations for RAG models. Specifically, ConsJudge prompts LLMs to generate different judgments based on various combinations of judgment dimensions, utilize the judge-consistency to evaluate these judgments and select the accepted and rejected judgments for DPO training. Our experiments show that ConsJudge can effectively provide more accurate judgments for optimizing RAG models across various RAG models and datasets. Further analysis reveals that judgments generated by ConsJudge have a high agreement with the superior LLM. All codes are available at https://github.com/OpenBMB/ConsJudge.",
      "arxiv_url": "https://arxiv.org/abs/2502.18817",
      "pdf_url": "https://arxiv.org/pdf/2502.18817",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14419",
      "title": "SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation",
      "authors": [
        "Huimin Xu",
        "Xin Mao",
        "Fengming Li",
        "Xiaobao Wu",
        "Wang Chen",
        "Wei Zhang",
        "A. Luu"
      ],
      "abstract": "Process Reward Models (PRMs) have demonstrated promising results in mathematical reasoning, but existing process annotation approaches, whether through human annotations or Monte Carlo simulations, remain computationally expensive. In this paper, we introduce Step COmpression for Process Estimation (SCOPE), a novel compression-based approach that significantly reduces annotation costs. We first translate natural language reasoning steps into code and normalize them through Abstract Syntax Tree, then merge equivalent steps to construct a prefix tree. Unlike simulation-based methods that waste numerous samples on estimation, SCOPE leverages a compression-based prefix tree where each root-to-leaf path serves as a training sample, reducing the complexity from $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K samples with only 5% of the computational resources required by previous methods. Empirical results demonstrate that PRMs trained on our dataset consistently outperform existing automated annotation approaches on both Best-of-N strategy and ProcessBench.",
      "arxiv_url": "https://arxiv.org/abs/2505.14419",
      "pdf_url": "https://arxiv.org/pdf/2505.14419",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14079",
      "title": "BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks",
      "authors": [
        "Weihong Du",
        "Wenrui Liao",
        "Binyu Yan",
        "Hongru Liang",
        "Anthony G. Cohn",
        "Wenqiang Lei"
      ],
      "abstract": "Large language model (LLM) based agents have shown great potential in following human instructions and automatically completing various tasks. To complete a task, the agent needs to decompose it into easily executed steps by planning. Existing studies mainly conduct the planning by inferring what steps should be executed next starting from the agent's initial state. However, this forward reasoning paradigm doesn't work well for complex tasks. We propose to study this issue in Minecraft, a virtual environment that simulates complex tasks based on real-world scenarios. We believe that the failure of forward reasoning is caused by the big perception gap between the agent's initial state and task goal. To this end, we leverage backward reasoning and make the planning starting from the terminal state, which can directly achieve the task goal in one step. Specifically, we design a BAckward Reasoning based agent (BAR). It is equipped with a recursive goal decomposition module, a state consistency maintaining module and a stage memory module to make robust, consistent, and efficient planning starting from the terminal state. Experimental results demonstrate the superiority of BAR over existing methods and the effectiveness of proposed modules.",
      "arxiv_url": "https://arxiv.org/abs/2505.14079",
      "pdf_url": "https://arxiv.org/pdf/2505.14079",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14083",
      "title": "Are Rules Meant to be Broken? Understanding Multilingual Moral Reasoning as a Computational Pipeline with UniMoral",
      "authors": [
        "Shivani Kumar",
        "David Jurgens"
      ],
      "abstract": "Moral reasoning is a complex cognitive process shaped by individual experiences and cultural contexts and presents unique challenges for computational analysis. While natural language processing (NLP) offers promising tools for studying this phenomenon, current research lacks cohesion, employing discordant datasets and tasks that examine isolated aspects of moral reasoning. We bridge this gap with UniMoral, a unified dataset integrating psychologically grounded and social-media-derived moral dilemmas annotated with labels for action choices, ethical principles, contributing factors, and consequences, alongside annotators' moral and cultural profiles. Recognizing the cultural relativity of moral reasoning, UniMoral spans six languages, Arabic, Chinese, English, Hindi, Russian, and Spanish, capturing diverse socio-cultural contexts. We demonstrate UniMoral's utility through a benchmark evaluations of three large language models (LLMs) across four tasks: action prediction, moral typology classification, factor attribution analysis, and consequence generation. Key findings reveal that while implicitly embedded moral contexts enhance the moral reasoning capability of LLMs, there remains a critical need for increasingly specialized approaches to further advance moral reasoning in these models.",
      "arxiv_url": "https://arxiv.org/abs/2502.14083",
      "pdf_url": "https://arxiv.org/pdf/2502.14083",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01875",
      "title": "Time-MQA: Time Series Multi-Task Question Answering with Context Enhancement",
      "authors": [
        "Yaxuan Kong",
        "Yiyuan Yang",
        "Yoontae Hwang",
        "Wenjie Du",
        "Stefan Zohren",
        "Zhangyang Wang",
        "Ming Jin",
        "Qingsong Wen"
      ],
      "abstract": "Time series data are foundational in finance, healthcare, and energy domains. However, most existing methods and datasets remain focused on a narrow spectrum of tasks, such as forecasting or anomaly detection. To bridge this gap, we introduce Time Series Multi-Task Question Answering (Time-MQA), a unified framework that enables natural language queries across multiple time series tasks - numerical analytical tasks and open-ended question answering with reasoning. Central to Time-MQA is the TSQA dataset, a large-scale dataset containing $\\sim$200k question-answer pairs derived from diverse time series spanning environment, traffic, etc. This comprehensive resource covers various time series lengths and promotes robust model development. We further demonstrate how continually pre-training large language models (Mistral 7B, Llama-3 8B, and Qwen-2.5 7B) on the TSQA dataset enhanced time series reasoning capabilities, moving beyond mere numeric tasks and enabling more advanced and intuitive interactions with temporal data. The complete TSQA dataset, models, user study questionnaires for evaluation, and other related materials have been open-sourced.",
      "arxiv_url": "https://arxiv.org/abs/2503.01875",
      "pdf_url": "https://arxiv.org/pdf/2503.01875",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20813",
      "title": "RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph",
      "authors": [
        "Junsik Kim",
        "Jinwook Park",
        "Kangil Kim"
      ],
      "abstract": "In knowledge graph embedding, leveraging relation specific entity transformation has markedly enhanced performance. However, the consistency of embedding differences before and after transformation remains unaddressed, risking the loss of valuable inductive bias inherent in the embeddings. This inconsistency stems from two problems. First, transformation representations are specified for relations in a disconnected manner, allowing dissimilar transformations and corresponding entity embeddings for similar relations. Second, a generalized plug-in approach as a SFBR (Semantic Filter Based on Relations) disrupts this consistency through excessive concentration of entity embeddings under entity-based regularization, generating indistinguishable score distributions among relations. In this paper, we introduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF). Its entity transformation has three features for enhancing semantic consistency: 1) shared affine transformation of relation embeddings across all relations, 2) rooted entity transformation that adds an entity embedding to its change represented by the transformed vector, and 3) normalization of the change to prevent scale reduction. To amplify the advantages of consistency that preserve semantics on embeddings, RSCF adds relation transformation and prediction modules for enhancing the semantics. In knowledge graph completion tasks with distance-based and tensor decomposition models, RSCF significantly outperforms state-of-the-art KGE methods, showing robustness across all relations and their frequencies.",
      "arxiv_url": "https://arxiv.org/abs/2505.20813",
      "pdf_url": "https://arxiv.org/pdf/2505.20813",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.23899",
      "title": "Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the CUBE dataset",
      "authors": [
        "Diana Galván-Sosa",
        "Gabrielle Gaudeau",
        "Pride Kavumba",
        "Yunmeng Li",
        "Hongyi Gu",
        "Zheng Yuan",
        "Keisuke Sakaguchi",
        "Paula Buttery"
      ],
      "abstract": "The performance and usability of Large-Language Models (LLMs) are driving their use in explanation generation tasks. However, despite their widespread adoption, LLM explanations have been found to be unreliable, making it difficult for users to distinguish good from bad explanations. To address this issue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of 26k explanations, written and later quality-annotated using the rubric by both humans and six open- and closed-source LLMs. The CUBE dataset focuses on two reasoning and two language tasks, providing the necessary diversity for us to effectively test our proposed rubric. Using Rubrik, we find that explanations are influenced by both task and perceived difficulty. Low quality stems primarily from a lack of conciseness in LLM-generated explanations, rather than cohesion and word choice. The full dataset, rubric, and code are available at https://github.com/RubriksCube/rubriks_cube.",
      "arxiv_url": "https://arxiv.org/abs/2503.23899",
      "pdf_url": "https://arxiv.org/pdf/2503.23899",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "cf2bd75a979bc064f39ba19fc2dca8b11f192dbd",
      "title": "Reinforced IR: A Self-Boosting Framework For Domain-Adapted Information Retrieval",
      "authors": [
        "Chaofan Li",
        "Jianlyu Chen",
        "Yingxia Shao",
        "Chaozhuo Li",
        "Quanqing Xu",
        "Defu Lian",
        "Zheng Liu"
      ],
      "abstract": "While retrieval techniques are widely used in practice, they still face significant challenges in cross-domain scenarios. Recently, generation-augmented methods have emerged as a promising solution to this problem. These methods enhance raw queries by incorporating additional information from an LLM-based generator, facilitating more direct retrieval of relevant documents. However, existing methods struggle with highly specialized situations that require extensive domain expertise. To address this problem, we present Reinforced-IR , a novel approach that jointly adapts a pre-trained re-triever and generator for precise cross-domain retrieval. A key innovation of Reinforced-IR is its Self-Boosting framework, which enables re-triever and generator to learn from each other’s feedback. Specifically, the generator is reinforced to generate query augmentations that enhance the retriever’s performance, while the retriever is trained to better discriminate the relevant documents identified by the generator. This iterative process allows the end-to-end retrieval performance to be progressively optimized using an unlabeled corpus from the target domain. In our experiment, Reinforced-IR outperforms existing domain adaptation meth-ods by a large margin, leading to substantial improvements in retrieval quality across a wide range of application scenarios. We have publicly released our code at this repo",
      "arxiv_url": "https://www.semanticscholar.org/paper/cf2bd75a979bc064f39ba19fc2dca8b11f192dbd",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "cf3ab7072b9235cb0bb61e6641d8aec35e7583e1",
      "title": "Typology-Guided Adaptation in Multilingual Models",
      "authors": [
        "N. Nakashole"
      ],
      "abstract": "Multilingual models often treat language diversity as a problem of data imbalance, over-looking structural variation. We introduce the Morphological Index (MoI), a typologically grounded metric that quantifies how strongly a language relies on surface morphology for noun classification. Building on MoI, we pro-pose MoI-MoE , a Mixture of Experts model that routes inputs based on morphological structure. Evaluated on 10 Bantu languages—a large, morphologically rich and underrepresented family—MoI-MoE outperforms strong baselines, improving Swahili accuracy by 14 points on noun class recognition while maintaining performance on morphology-rich languages like Zulu. These findings highlight typo-logical structure as a practical and interpretable signal for multilingual model adaptation",
      "arxiv_url": "https://www.semanticscholar.org/paper/cf3ab7072b9235cb0bb61e6641d8aec35e7583e1",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.11873",
      "title": "Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models",
      "authors": [
        "Zihan Qiu",
        "Zeyu Huang",
        "Bo Zheng",
        "Kaiyue Wen",
        "Zekun Wang",
        "Rui Men",
        "Ivan Titov",
        "Dayiheng Liu",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "abstract": "This paper revisits the implementation of $\\textbf{L}$oad-$\\textbf{b}$alancing $\\textbf{L}$oss (LBL) when training Mixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as $N_E \\sum_{i=1}^{N_E} f_i p_i$, where $N_E$ is the total number of experts, $f_i$ represents the frequency of expert $i$ being selected, and $p_i$ denotes the average gating score of the expert $i$. Existing MoE training frameworks usually employ the parallel training strategy so that $f_i$ and the LBL are calculated within a $\\textbf{micro-batch}$ and then averaged across parallel groups. In essence, a micro-batch for training billion-scale LLMs normally contains very few sequences. So, the micro-batch LBL is almost at the sequence level, and the router is pushed to distribute the token evenly within each sequence. Under this strict constraint, even tokens from a domain-specific sequence ($\\textit{e.g.}$, code) are uniformly routed to all experts, thereby inhibiting expert specialization. In this work, we propose calculating LBL using a $\\textbf{global-batch}$ to loose this constraint. Because a global-batch contains much more diverse sequences than a micro-batch, which will encourage load balance at the corpus level. Specifically, we introduce an extra communication step to synchronize $f_i$ across micro-batches and then use it to calculate the LBL. Through experiments on training MoEs-based LLMs (up to $\\textbf{42.8B}$ total parameters and $\\textbf{400B}$ tokens), we surprisingly find that the global-batch LBL strategy yields excellent performance gains in both pre-training perplexity and downstream tasks. Our analysis reveals that the global-batch LBL also greatly improves the domain specialization of MoE experts.",
      "arxiv_url": "https://arxiv.org/abs/2501.11873",
      "pdf_url": "https://arxiv.org/pdf/2501.11873",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "cf8671b91d5ac2a5e50b5753619402139eea133a",
      "title": "ChatMap: Mining Human Thought Processes for Customer Service Chatbots via Multi-Agent Collaboration",
      "authors": [
        "Xinyi Jiang",
        "Tianyi Hu",
        "Yuheng Qin",
        "Guoming Wang",
        "Zhou Huan",
        "Kehan Chen",
        "Gang Huang",
        "Rongxing Lu",
        "Siliang Tang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/cf8671b91d5ac2a5e50b5753619402139eea133a",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11364",
      "title": "Blessing of Multilinguality: A Systematic Analysis of Multilingual In-Context Learning",
      "authors": [
        "Yilei Tu",
        "Andrew Xue",
        "Freda Shi"
      ],
      "abstract": "While multilingual large language models generally perform adequately, and sometimes even rival English performance on high-resource languages (HRLs), they often significantly underperform on low-resource languages (LRLs). Among several prompting strategies aiming at bridging the gap, multilingual in-context learning (ICL) has been particularly effective when demonstration in target languages is unavailable. However, there lacks a systematic understanding of when and why it works well. In this work, we systematically analyze multilingual ICL, using demonstrations in HRLs to enhance cross-lingual transfer. We show that demonstrations in mixed HRLs consistently outperform English-only ones across the board, particularly for tasks written in LRLs. Surprisingly, our ablation study shows that the presence of irrelevant non-English sentences in the prompt yields measurable gains, suggesting the effectiveness of multilingual exposure itself. Our results highlight the potential of strategically leveraging multilingual resources to bridge the performance gap for underrepresented languages.",
      "arxiv_url": "https://arxiv.org/abs/2502.11364",
      "pdf_url": "https://arxiv.org/pdf/2502.11364",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01303",
      "title": "PROPER: A Progressive Learning Framework for Personalized Large Language Models with Group-Level Adaptation",
      "authors": [
        "Linhai Zhang",
        "Jialong Wu",
        "Deyu Zhou",
        "Yulan He"
      ],
      "abstract": "Personalized large language models (LLMs) aim to tailor their outputs to user preferences. Recent advances in parameter-efficient fine-tuning (PEFT) methods have highlighted the effectiveness of adapting population-level LLMs to personalized LLMs by fine-tuning user-specific parameters with user history. However, user data is typically sparse, making it challenging to adapt LLMs to specific user patterns. To address this challenge, we propose PROgressive PERsonalization (PROPER), a novel progressive learning framework inspired by meso-level theory in social science. PROPER bridges population-level and user-level models by grouping users based on preferences and adapting LLMs in stages. It combines a Mixture-of-Experts (MoE) structure with Low Ranked Adaptation (LoRA), using a user-aware router to assign users to appropriate groups automatically. Additionally, a LoRA-aware router is proposed to facilitate the integration of individual user LoRAs with group-level LoRAs. Experimental results show that PROPER significantly outperforms SOTA models across multiple tasks, demonstrating the effectiveness of our approach.",
      "arxiv_url": "https://arxiv.org/abs/2503.01303",
      "pdf_url": "https://arxiv.org/pdf/2503.01303",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "cfb79f92bb9b3b6f9844b8095fffe9f7028a7ebb",
      "title": "What is in a name? Mitigating Name Bias in Text Embedding Similarity via Anonymization",
      "authors": [
        "S. Manchanda",
        "Pannagadatta K. Shivaswamy"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/cfb79f92bb9b3b6f9844b8095fffe9f7028a7ebb",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00457",
      "title": "Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break Large Models",
      "authors": [
        "Junwoo Park",
        "Hyuck Lee",
        "Dohyun Lee",
        "Daehoon Gwak",
        "Jaegul Choo"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable performance across diverse tasks without domain-specific training, fueling interest in their potential for time-series forecasting. While LLMs have shown potential in zero-shot forecasting through prompting alone, recent studies suggest that LLMs lack inherent effectiveness in forecasting. Given these conflicting findings, a rigorous validation is essential for drawing reliable conclusions. In this paper, we evaluate the effectiveness of LLMs as zero-shot forecasters compared to state-of-the-art domain-specific models. Our experiments show that LLM-based zero-shot forecasters often struggle to achieve high accuracy due to their sensitivity to noise, underperforming even simple domain-specific models. We have explored solutions to reduce LLMs' sensitivity to noise in the zero-shot setting, but improving their robustness remains a significant challenge. Our findings suggest that rather than emphasizing zero-shot forecasting, a more promising direction would be to focus on fine-tuning LLMs to better process numerical sequences. Our experimental code is available at https://github.com/junwoopark92/revisiting-LLMs-zeroshot-forecaster.",
      "arxiv_url": "https://arxiv.org/abs/2506.00457",
      "pdf_url": "https://arxiv.org/pdf/2506.00457",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.09975",
      "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text",
      "authors": [
        "Hillary Dawkins",
        "Kathleen C. Fraser",
        "Svetlana Kiritchenko"
      ],
      "abstract": "Detecting AI-generated text is a difficult problem to begin with; detecting AI-generated text on social media is made even more difficult due to the short text length and informal, idiosyncratic language of the internet. It is nonetheless important to tackle this problem, as social media represents a significant attack vector in online influence campaigns, which may be bolstered through the use of mass-produced AI-generated posts supporting (or opposing) particular policies, decisions, or events. We approach this problem with the mindset and resources of a reasonably sophisticated threat actor, and create a dataset of 505,159 AI-generated social media posts from a combination of open-source, closed-source, and fine-tuned LLMs, covering 11 different controversial topics. We show that while the posts can be detected under typical research assumptions about knowledge of and access to the generating models, under the more realistic assumption that an attacker will not release their fine-tuned model to the public, detectability drops dramatically. This result is confirmed with a human study. Ablation experiments highlight the vulnerability of various detection algorithms to fine-tuned LLMs. This result has implications across all detection domains, since fine-tuning is a generally applicable and realistic LLM use case.",
      "arxiv_url": "https://arxiv.org/abs/2506.09975",
      "pdf_url": "https://arxiv.org/pdf/2506.09975",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02298",
      "title": "LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback",
      "authors": [
        "Thai Hoang",
        "Kung-Hsiang Huang",
        "Shirley Kokane",
        "Jianguo Zhang",
        "Zuxin Liu",
        "Ming Zhu",
        "Jake Grigsby",
        "Tian Lan",
        "Michael S. Ryoo",
        "Chien-Sheng Wu",
        "Shelby Heinecke",
        "Huan Wang",
        "Silvio Savarese",
        "Caiming Xiong",
        "Juan Carlos Niebles"
      ],
      "abstract": "Large Action Models (LAMs) for AI Agents offer incredible potential but face challenges due to the need for high-quality training data, especially for multi-steps tasks that involve planning, executing tool calls, and responding to feedback. To address these issues, we present LAM SIMULATOR, a comprehensive framework designed for online exploration of agentic tasks with high-quality feedback. Our framework features a dynamic task query generator, an extensive collection of tools, and an interactive environment where Large Language Model (LLM) Agents can call tools and receive real-time feedback. This setup enables LLM Agents to explore and solve tasks autonomously, facilitating the discovery of multiple approaches to tackle any given task. The resulting action trajectory data are then used to create high-quality training datasets for LAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena, highlight the effectiveness of LAM SIMULATOR: models trained with self-generated datasets using our framework achieve significant performance gains, up to a 49.3\\% improvement over their original baselines. LAM SIMULATOR requires minimal human input during dataset creation, highlighting LAM SIMULATOR's efficiency and effectiveness in speeding up development of AI agents.",
      "arxiv_url": "https://arxiv.org/abs/2506.02298",
      "pdf_url": "https://arxiv.org/pdf/2506.02298",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.03214",
      "title": "DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral",
      "authors": [
        "Qiang Sun",
        "Sirui Li",
        "Tingting Bi",
        "Du Huynh",
        "Mark Reynolds",
        "Yuanyi Luo",
        "Wei Liu"
      ],
      "abstract": "Acquiring structured data from domain-specific, image-based documents such as scanned reports is crucial for many downstream tasks but remains challenging due to document variability. Many of these documents exist as images rather than as machine-readable text, which requires human annotation to train automated extraction systems. We present DocSpiral, the first Human-in-the-Spiral assistive document annotation platform, designed to address the challenge of extracting structured information from domain-specific, image-based document collections. Our spiral design establishes an iterative cycle in which human annotations train models that progressively require less manual intervention. DocSpiral integrates document format normalization, comprehensive annotation interfaces, evaluation metrics dashboard, and API endpoints for the development of AI / ML models into a unified workflow. Experiments demonstrate that our framework reduces annotation time by at least 41\\% while showing consistent performance gains across three iterations during model training. By making this annotation platform freely accessible, we aim to lower barriers to AI/ML models development in document processing, facilitating the adoption of large language models in image-based, document-intensive fields such as geoscience and healthcare. The system is freely available at: https://app.ai4wa.com. The demonstration video is available: https://app.ai4wa.com/docs/docspiral/demo.",
      "arxiv_url": "https://arxiv.org/abs/2505.03214",
      "pdf_url": "https://arxiv.org/pdf/2505.03214",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d049a6f2078fb2b6eed1089fe98a93a916a9d119",
      "title": "NovelCR: A Large-Scale Bilingual Dataset Tailored for Long-Span Coreference Resolution",
      "authors": [
        "Meihan Tong",
        "Shuai Wang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/d049a6f2078fb2b6eed1089fe98a93a916a9d119",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d0d3a61c2574693bfee049a996584cd75552f4c2",
      "title": "CU-MAM: Coherence-Driven Unified Macro-Structures for Argument Mining",
      "authors": [
        "Debela Tesfaye Gemechu",
        "Chris Reed"
      ],
      "abstract": "Argument Mining (AM) involves the automatic identification of argument structure in natural language. Traditional AM methods rely on micro-structural features derived from the internal properties of individual Argumentative Discourse Units (ADUs). However, argument structure is shaped by a macro-structure capturing the functional interdependence among ADUs. This macro-structure consists of segments, where each segment contains ADUs that fulfill specific roles to maintain coherence within the segment ( local coherence ) and across segments ( global coherence ). This paper presents an approach that models macro-structure, capturing both local and global coherence to identify argument structures. Experiments on heterogeneous datasets demonstrate superior performance in both in-dataset and cross-dataset evaluations. The cross-dataset evaluation shows that macro-structure enhances transferability to unseen datasets.",
      "arxiv_url": "https://www.semanticscholar.org/paper/d0d3a61c2574693bfee049a996584cd75552f4c2",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18331",
      "title": "BottleHumor: Self-Informed Humor Explanation using the Information Bottleneck Principle",
      "authors": [
        "EunJeong Hwang",
        "Peter West",
        "Vered Shwartz"
      ],
      "abstract": "Humor is prevalent in online communications and it often relies on more than one modality (e.g., cartoons and memes). Interpreting humor in multimodal settings requires drawing on diverse types of knowledge, including metaphorical, sociocultural, and commonsense knowledge. However, identifying the most useful knowledge remains an open question. We introduce \\method{}, a method inspired by the information bottleneck principle that elicits relevant world knowledge from vision and language models which is iteratively refined for generating an explanation of the humor in an unsupervised manner. Our experiments on three datasets confirm the advantage of our method over a range of baselines. Our method can further be adapted in the future for additional tasks that can benefit from eliciting and conditioning on relevant world knowledge and open new research avenues in this direction.",
      "arxiv_url": "https://arxiv.org/abs/2502.18331",
      "pdf_url": "https://arxiv.org/pdf/2502.18331",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.09921",
      "title": "PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization",
      "authors": [
        "Yidan Wang",
        "Yanan Cao",
        "Yubing Ren",
        "Fang Fang",
        "Zheng Lin",
        "Binxing Fang"
      ],
      "abstract": "Large Language Models (LLMs) excel in various domains but pose inherent privacy risks. Existing methods to evaluate privacy leakage in LLMs often use memorized prefixes or simple instructions to extract data, both of which well-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM safety mechanisms to generate harmful content, but their role in privacy scenarios remains underexplored. In this paper, we examine the effectiveness of jailbreak attacks in extracting sensitive information, bridging privacy leakage and jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework targeting Personally Identifiable Information (PII) and addressing the limitations of current jailbreak methods. Specifically, PIG identifies PII entities and their types in privacy queries, uses in-context learning to build a privacy context, and iteratively updates it with three gradient-based strategies to elicit target PII. We evaluate PIG and existing jailbreak methods using two privacy-related datasets. Experiments on four white-box and two black-box LLMs show that PIG outperforms baseline methods and achieves state-of-the-art (SoTA) results. The results underscore significant privacy risks in LLMs, emphasizing the need for stronger safeguards. Our code is availble at https://github.com/redwyd/PrivacyJailbreak.",
      "arxiv_url": "https://arxiv.org/abs/2505.09921",
      "pdf_url": "https://arxiv.org/pdf/2505.09921",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11175",
      "title": "Investigating Language Preference of Multilingual RAG Systems",
      "authors": [
        "Jeonghyun Park",
        "Hwanhee Lee"
      ],
      "abstract": "Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language models by integrating external multilingual information to produce context-aware responses. However, mRAG systems struggle with retrieving relevant information due to linguistic variations between queries and documents, generating inconsistent responses when multilingual sources conflict. In this work, we systematically investigate language preferences in both retrieval and generation of mRAG through a series of experiments. Our analysis indicates that retrievers tend to prefer high-resource and query languages, yet this preference does not consistently improve generation performance. Moreover, we observe that generators prefer the query language or Latin scripts, leading to inconsistent outputs. To overcome these issues, we propose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective framework that fuses translated multilingual passages with complementary model knowledge. Empirical results demonstrate that DKM-RAG mitigates language preference in generation and enhances performance across diverse linguistic settings. Code is available at https://github.com/jeonghyunpark2002/LanguagePreference.git",
      "arxiv_url": "https://arxiv.org/abs/2502.11175",
      "pdf_url": "https://arxiv.org/pdf/2502.11175",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17041",
      "title": "PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance",
      "authors": [
        "Haoran Li",
        "Wenbin Hu",
        "Huihao Jing",
        "Yulin Chen",
        "Qi Hu",
        "Sirui Han",
        "Tianshu Chu",
        "Peizhao Hu",
        "Yangqiu Song"
      ],
      "abstract": "Recent advancements in generative large language models (LLMs) have enabled wider applicability, accessibility, and flexibility. However, their reliability and trustworthiness are still in doubt, especially for concerns regarding individuals' data privacy. Great efforts have been made on privacy by building various evaluation benchmarks to study LLMs' privacy awareness and robustness from their generated outputs to their hidden representations. Unfortunately, most of these works adopt a narrow formulation of privacy and only investigate personally identifiable information (PII). In this paper, we follow the merit of the Contextual Integrity (CI) theory, which posits that privacy evaluation should not only cover the transmitted attributes but also encompass the whole relevant social context through private information flows. We present PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted at legal compliance to cover well-annotated privacy and safety regulations, real court cases, privacy policies, and synthetic data built from the official toolkit to study LLMs' privacy and safety compliance. We evaluate the latest LLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our experimental results suggest that though LLMs can effectively capture key CI parameters inside a given context, they still require further advancements for privacy compliance.",
      "arxiv_url": "https://arxiv.org/abs/2502.17041",
      "pdf_url": "https://arxiv.org/pdf/2502.17041",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04592",
      "title": "Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification",
      "authors": [
        "Chengwu Liu",
        "Ye Yuan",
        "Yichun Yin",
        "Yan Xu",
        "Xin Xu",
        "Zaoyu Chen",
        "Yasheng Wang",
        "Lifeng Shang",
        "Qun Liu",
        "Ming Zhang"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting has become the de facto method to elicit reasoning capabilities from large language models (LLMs). However, to mitigate hallucinations in CoT that are notoriously difficult to detect, current methods such as process reward models (PRMs) or self-consistency operate as opaque boxes and do not provide checkable evidence for their judgments, possibly limiting their effectiveness. To address this issue, we draw inspiration from the idea that\"the gold standard for supporting a mathematical claim is to provide a proof\". We propose a retrospective, step-aware formal verification framework $Safe$. Rather than assigning arbitrary scores, we strive to articulate mathematical claims in formal mathematical language Lean 4 at each reasoning step and provide formal proofs to identify hallucinations. We evaluate our framework $Safe$ across multiple language models and various mathematical datasets, demonstrating a significant performance improvement while offering interpretable and verifiable evidence. We also propose $FormalStep$ as a benchmark for step correctness theorem proving with $30,809$ formal statements. To the best of our knowledge, our work represents the first endeavor to utilize formal mathematical language Lean 4 for verifying natural language content generated by LLMs, aligning with the reason why formal mathematical languages were created in the first place: to provide a robust foundation for hallucination-prone human-written proofs.",
      "arxiv_url": "https://arxiv.org/abs/2506.04592",
      "pdf_url": "https://arxiv.org/pdf/2506.04592",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14127",
      "title": "Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above",
      "authors": [
        "Nishant Balepur",
        "Rachel Rudinger",
        "J. Boyd-Graber"
      ],
      "abstract": "Multiple choice question answering (MCQA) is popular for LLM evaluation due to its simplicity and human-like testing, but we argue for its reform. We first reveal flaws in MCQA's format, as it struggles to: 1) test generation/subjectivity; 2) match LLM use cases; and 3) fully test knowledge. We instead advocate for generative formats based on human testing, where LLMs construct and explain answers, better capturing user needs and knowledge while remaining easy to score. We then show even when MCQA is a useful format, its datasets suffer from: leakage; unanswerability; shortcuts; and saturation. In each issue, we give fixes from education, like rubrics to guide MCQ writing; scoring methods to bridle guessing; and Item Response Theory to build harder MCQs. Lastly, we discuss LLM errors in MCQA, robustness, biases, and unfaithful explanations, showing how our prior solutions better measure or address these issues. While we do not need to desert MCQA, we encourage more efforts in refining the task based on educational testing, advancing evaluations.",
      "arxiv_url": "https://arxiv.org/abs/2502.14127",
      "pdf_url": "https://arxiv.org/pdf/2502.14127",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d2d05269ebf7bdcb5b697f29a6373715e5139fb0",
      "title": "BERT-like Models for Slavic Morpheme Segmentation",
      "authors": [
        "Dmitry A. Morozov",
        "Lizaveta Astapenka",
        "Anna Glazkova",
        "Timur Garipov",
        "O. Lyashevskaya"
      ],
      "abstract": "Automatic morpheme segmentation algorithms are applicable in various tasks, such as building tokenizers and language education. For Slavic languages, the development of such algorithms is complicated by the rich derivational capabilities of these languages. Previous research has shown that, on average, these algorithms have already reached expert-level quality. However, a key unresolved issue is the significant decline in performance when segmenting words containing roots not present in the training data. This problem can be partially addressed by us-ing pre-trained language models to better account for word semantics. In this work, we explored the possibility of fine-tuning BERT-like models for morpheme segmentation using data from Belarusian, Czech, and Russian. We found that for Czech and Russian, our models outperform all previously proposed approaches, achieving word-level accuracy of 92.5-95.1%. For Belarusian, this task was addressed for the first time. The best-performing approach for Belarusian was an ensemble of convolutional neural networks with word-level accuracy of 90.45%.",
      "arxiv_url": "https://www.semanticscholar.org/paper/d2d05269ebf7bdcb5b697f29a6373715e5139fb0",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.06262",
      "title": "Dialz: A Python Toolkit for Steering Vectors",
      "authors": [
        "Zara Siddique",
        "Liam D. Turner",
        "Luis Espinosa Anke"
      ],
      "abstract": "We introduce Dialz, a framework for advancing research on steering vectors for open-source LLMs, implemented in Python. Steering vectors allow users to modify activations at inference time to amplify or weaken a 'concept', e.g. honesty or positivity, providing a more powerful alternative to prompting or fine-tuning. Dialz supports a diverse set of tasks, including creating contrastive pair datasets, computing and applying steering vectors, and visualizations. Unlike existing libraries, Dialz emphasizes modularity and usability, enabling both rapid prototyping and in-depth analysis. We demonstrate how Dialz can be used to reduce harmful outputs such as stereotypes, while also providing insights into model behaviour across different layers. We release Dialz with full documentation, tutorials, and support for popular open-source models to encourage further research in safe and controllable language generation. Dialz enables faster research cycles and facilitates insights into model interpretability, paving the way for safer, more transparent, and more reliable AI systems.",
      "arxiv_url": "https://arxiv.org/abs/2505.06262",
      "pdf_url": "https://arxiv.org/pdf/2505.06262",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04036",
      "title": "Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge",
      "authors": [
        "Xinyue Cui",
        "Johnny Tian-Zheng Wei",
        "Swabha Swayamdipta",
        "Robin Jia"
      ],
      "abstract": "Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques primarily focus on effective memorization during pretraining, while overlooking challenges that arise in other stages of the LLM lifecycle, such as the risk of watermark filtering during data preprocessing and verification difficulties due to API-only access. To address these challenges, we propose a novel data watermarking approach that injects plausible yet fictitious knowledge into training data using generated passages describing a fictitious entity and its associated attributes. Our watermarks are designed to be memorized by the LLM through seamlessly integrating in its training data, making them harder to detect lexically during preprocessing. We demonstrate that our watermarks can be effectively memorized by LLMs, and that increasing our watermarks'density, length, and diversity of attributes strengthens their memorization. We further show that our watermarks remain effective after continual pretraining and supervised finetuning. Finally, we show that our data watermarks can be evaluated even under API-only access via question answering.",
      "arxiv_url": "https://arxiv.org/abs/2503.04036",
      "pdf_url": "https://arxiv.org/pdf/2503.04036",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2510.01652",
      "title": "Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings in LLMs with Enabled Bidirectional Attention",
      "authors": [
        "Zhaoxin Feng",
        "Jianfei Ma",
        "Emmanuele Chersoni",
        "Xiaojing Zhao",
        "Xiaoyi Bao"
      ],
      "abstract": "Autoregressive Large Language Models (LLMs) demonstrate exceptional performance in language understanding and generation. However, their application in text embedding tasks has been relatively slow, along with the analysis of their semantic representation in probing tasks, due to the constraints of the unidirectional attention mechanism. This paper aims to explore whether such constraints can be overcome by enabling bidirectional attention in LLMs. We tested different variants of the Llama architecture through additional training steps, progressively enabling bidirectional attention and unsupervised/supervised contrastive learning.",
      "arxiv_url": "https://arxiv.org/abs/2510.01652",
      "pdf_url": "https://arxiv.org/pdf/2510.01652",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-10-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.12329",
      "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era",
      "authors": [
        "Kanzhi Cheng",
        "Wenpo Song",
        "Jiaxin Fan",
        "Zheng Ma",
        "Qiushi Sun",
        "Fangzhi Xu",
        "Chenyang Yan",
        "Nuo Chen",
        "Jianbing Zhang",
        "Jiajun Chen"
      ],
      "abstract": "Image captioning has been a longstanding challenge in vision-language research. With the rise of LLMs, modern Vision-Language Models (VLMs) generate detailed and comprehensive image descriptions. However, benchmarking the quality of such captions remains unresolved. This paper addresses two key questions: (1) How well do current VLMs actually perform on image captioning, particularly compared to humans? We built CapArena, a platform with over 6000 pairwise caption battles and high-quality human preference votes. Our arena-style evaluation marks a milestone, showing that leading models like GPT-4o achieve or even surpass human performance, while most open-source models lag behind. (2) Can automated metrics reliably assess detailed caption quality? Using human annotations from CapArena, we evaluate traditional and recent captioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while some metrics (e.g., METEOR) show decent caption-level agreement with humans, their systematic biases lead to inconsistencies in model ranking. In contrast, VLM-as-a-Judge demonstrates robust discernment at both the caption and model levels. Building on these insights, we release CapArena-Auto, an accurate and efficient automated benchmark for detailed captioning, achieving 94.3% correlation with human rankings at just $4 per test. Data and resources will be open-sourced at https://caparena.github.io.",
      "arxiv_url": "https://arxiv.org/abs/2503.12329",
      "pdf_url": "https://arxiv.org/pdf/2503.12329",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14038",
      "title": "ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data",
      "authors": [
        "Xinzhe Zheng",
        "Sijie Ji",
        "Jiawei Sun",
        "Renqi Chen",
        "Wei Gao",
        "Mani Srivastava"
      ],
      "abstract": "Mental health risk is a critical global public health challenge, necessitating innovative and reliable assessment methods. With the development of large language models (LLMs), they stand out to be a promising tool for explainable mental health care applications. Nevertheless, existing approaches predominantly rely on subjective textual mental records, which can be distorted by inherent mental uncertainties, leading to inconsistent and unreliable predictions. To address these limitations, this paper introduces ProMind-LLM. We investigate an innovative approach integrating objective behavior data as complementary information alongside subjective mental records for robust mental health risk assessment. Specifically, ProMind-LLM incorporates a comprehensive pipeline that includes domain-specific pretraining to tailor the LLM for mental health contexts, a self-refine mechanism to optimize the processing of numerical behavioral data, and causal chain-of-thought reasoning to enhance the reliability and interpretability of its predictions. Evaluations of two real-world datasets, PMData and Globem, demonstrate the effectiveness of our proposed methods, achieving substantial improvements over general LLMs. We anticipate that ProMind-LLM will pave the way for more dependable, interpretable, and scalable mental health case solutions.",
      "arxiv_url": "https://arxiv.org/abs/2505.14038",
      "pdf_url": "https://arxiv.org/pdf/2505.14038",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01763",
      "title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models",
      "authors": [
        "Zhengliang Shi",
        "Yuhan Wang",
        "Lingyong Yan",
        "Pengjie Ren",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Zhaochun Ren"
      ],
      "abstract": "Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.",
      "arxiv_url": "https://arxiv.org/abs/2503.01763",
      "pdf_url": "https://arxiv.org/pdf/2503.01763",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d3b13d297c74032168c9973baf678164a5925325",
      "title": "Generative Reward Modeling via Synthetic Criteria Preference Learning",
      "authors": [
        "Xiaobo Liang",
        "Haoke Zhang",
        "Juntao Li",
        "Kehai Chen",
        "Qiaoming Zhu",
        "Min Zhang"
      ],
      "abstract": "Generative Reward Models (GenRMs) leverage synthesized Chains of Thought (CoT) to reduce the need for massive labeled data, but this approach introduces risks of overoptimization due to the inability to guarantee the correctness of the CoTs. Identifying and optimizing unexpected behaviors within these synthesized CoT remains a challenge, as it heavily depends on precise annotations of intermediate behavior, similar to process supervision. In this work, we introduce a criteria-based preference tree for reward modeling, where each path in the tree represents a reasoning trajectory based on synthesized criteria. Crucially, each reasoning trajectory can be independently optimized through RL algorithm. These fine-grained process reward signals are derived from the inference-time computations and predefined rules, eliminating the need for human supervision. In experiments, SyncPL 1 showed significant improvements over baselines on multiple human preference benchmarks. We further demonstrate that synthesized data can be learned us-ing a long CoT format, analogous to an o1-like model, further enhancing performance while keeping stability and efficiency during training.",
      "arxiv_url": "https://www.semanticscholar.org/paper/d3b13d297c74032168c9973baf678164a5925325",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d3cb49cad0175770e87edabf314a85d8f2e5a4bf",
      "title": "GEMS: Generation-Based Event Argument Extraction via Multi-perspective Prompts and Ontology Steering",
      "authors": [
        "Run Lin",
        "Yao Liu",
        "Yanglei Gan",
        "Yuxiang Cai",
        "Tian Lan",
        "Qiao Liu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/d3cb49cad0175770e87edabf314a85d8f2e5a4bf",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.13044",
      "title": "Just Go Parallel: Improving the Multilingual Capabilities of Large Language Models",
      "authors": [
        "Muhammad Reza Qorib",
        "Junyi Li",
        "Hwee Tou Ng"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive translation capabilities even without being explicitly trained on parallel data. This remarkable property has led some to believe that parallel data is no longer necessary for building multilingual language models. While some attribute this to the emergent abilities of LLMs due to scale, recent work suggests that it is actually caused by incidental bilingual signals present in the training data. Various methods have been proposed to maximize the utility of parallel data to enhance the multilingual capabilities of multilingual encoder-based and encoder-decoder language models. However, some decoder-based LLMs opt to ignore parallel data instead. In this work, we conduct a systematic study on the impact of adding parallel data on LLMs' multilingual capabilities, focusing specifically on translation and multilingual common-sense reasoning. Through controlled experiments, we demonstrate that parallel data can significantly improve LLMs' multilingual capabilities.",
      "arxiv_url": "https://arxiv.org/abs/2506.13044",
      "pdf_url": "https://arxiv.org/pdf/2506.13044",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15969",
      "title": "Forgotten Polygons: Multimodal Large Language Models are Shape-Blind",
      "authors": [
        "William Rudman",
        "Michal Golovanesky",
        "Amir Bar",
        "Vedant Palit",
        "Yann LeCun",
        "Carsten Eickhoff",
        "Ritambhara Singh"
      ],
      "abstract": "Despite strong performance on vision-language tasks, Multimodal Large Language Models (MLLMs) struggle with mathematical problem-solving, with both open-source and state-of-the-art models falling short of human performance on visual-math benchmarks. To systematically examine visual-mathematical reasoning in MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test multi-step reasoning, and (3) explore a potential solution to improve visual reasoning capabilities. Our findings reveal fundamental shortcomings in shape recognition, with top models achieving under 50% accuracy in identifying regular polygons. We analyze these failures through the lens of dual-process theory and show that MLLMs rely on System 1 (intuitive, memorized associations) rather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count the sides of both familiar and novel shapes, suggesting they have neither learned the concept of sides nor effectively process visual inputs. Finally, we propose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances multi-step mathematical reasoning by explicitly referencing visual annotations in diagrams, boosting GPT-4o's accuracy on an irregular polygon side-counting task from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMs remains an open problem, and visually-guided prompting is essential for successfully engaging visual reasoning. Code available at: https://github.com/rsinghlab/Shape-Blind.",
      "arxiv_url": "https://arxiv.org/abs/2502.15969",
      "pdf_url": "https://arxiv.org/pdf/2502.15969",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.13300",
      "title": "AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research",
      "authors": [
        "Yilun Zhao",
        "Weiyuan Chen",
        "Zhijian Xu",
        "Manasi S. Patwardhan",
        "Chengye Wang",
        "Yixin Liu",
        "L. Vig",
        "Arman Cohan"
      ],
      "abstract": "We introduce AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. Our evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, we demonstrate that current automated evaluation methods are not reliable for our task, as they show a significant discrepancy when compared to human assessment. To better investigate this, we develop AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on our task. We investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks.",
      "arxiv_url": "https://arxiv.org/abs/2507.13300",
      "pdf_url": "https://arxiv.org/pdf/2507.13300",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17163",
      "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation",
      "authors": [
        "Mar'ia Andrea Cruz Bland'on",
        "Jayasimha Talur",
        "Bruno Charron",
        "Dong Liu",
        "Saab Mansour",
        "Marcello Federico"
      ],
      "abstract": "Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience. In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs. Our dataset is available at https://github.com/amazon-science/MEMERAG",
      "arxiv_url": "https://arxiv.org/abs/2502.17163",
      "pdf_url": "https://arxiv.org/pdf/2502.17163",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21301",
      "title": "How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian",
      "authors": [
        "Andrea Pedrotti",
        "Giulia Rambelli",
        "Caterina Villani",
        "M. Bolognesi"
      ],
      "abstract": "People can categorize the same entity at multiple taxonomic levels, such as basic (bear), superordinate (animal), and subordinate (grizzly bear). While prior research has focused on basic-level categories, this study is the first attempt to examine the organization of categories by analyzing exemplars produced at the subordinate level. We present a new Italian psycholinguistic dataset of human-generated exemplars for 187 concrete words. We then use these data to evaluate whether textual and vision LLMs produce meaningful exemplars that align with human category organization across three key tasks: exemplar generation, category induction, and typicality judgment. Our findings show a low alignment between humans and LLMs, consistent with previous studies. However, their performance varies notably across different semantic domains. Ultimately, this study highlights both the promises and the constraints of using AI-generated exemplars to support psychological and linguistic research.",
      "arxiv_url": "https://arxiv.org/abs/2505.21301",
      "pdf_url": "https://arxiv.org/pdf/2505.21301",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.14307",
      "title": "How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs",
      "authors": [
        "Karin de Langis",
        "Jong Inn Park",
        "Andreas Schramm",
        "Bin Hu",
        "Khanh Chi Le",
        "Dongyeop Kang"
      ],
      "abstract": "Large language models (LLMs) exhibit increasingly sophisticated linguistic capabilities, yet the extent to which these behaviors reflect human-like cognition versus advanced pattern recognition remains an open question. In this study, we investigate how LLMs process the temporal meaning of linguistic aspect in narratives that were previously used in human studies. Using an Expert-in-the-Loop probing pipeline, we conduct a series of targeted experiments to assess whether LLMs construct semantic representations and pragmatic inferences in a human-like manner. Our findings show that LLMs over-rely on prototypicality, produce inconsistent aspectual judgments, and struggle with causal reasoning derived from aspect, raising concerns about their ability to fully comprehend narratives. These results suggest that LLMs process aspect fundamentally differently from humans and lack robust narrative understanding. Beyond these empirical findings, we develop a standardized experimental framework for the reliable assessment of LLMs'cognitive and linguistic capabilities.",
      "arxiv_url": "https://arxiv.org/abs/2507.14307",
      "pdf_url": "https://arxiv.org/pdf/2507.14307",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11647",
      "title": "DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing",
      "authors": [
        "Yi Wang",
        "Fenghua Weng",
        "Sibei Yang",
        "Zhan Qin",
        "Minlie Huang",
        "Wenjie Wang"
      ],
      "abstract": "Large Language Models (LLMs) are widely applied in decision making, but their deployment is threatened by jailbreak attacks, where adversarial users manipulate model behavior to bypass safety measures. Existing defense mechanisms, such as safety fine-tuning and model editing, either require extensive parameter modifications or lack precision, leading to performance degradation on general tasks, which is unsuitable to post-deployment safety alignment. To address these challenges, we propose DELMAN (Dynamic Editing for LLMs JAilbreak DefeNse), a novel approach leveraging direct model editing for precise, dynamic protection against jailbreak attacks. DELMAN directly updates a minimal set of relevant parameters to neutralize harmful behaviors while preserving the model's utility. To avoid triggering a safe response in benign context, we incorporate KL-divergence regularization to ensure the updated model remains consistent with the original model when processing benign queries. Experimental results demonstrate that DELMAN outperforms baseline methods in mitigating jailbreak attacks while preserving the model's utility, and adapts seamlessly to new attack instances, providing a practical and efficient solution for post-deployment model protection.",
      "arxiv_url": "https://arxiv.org/abs/2502.11647",
      "pdf_url": "https://arxiv.org/pdf/2502.11647",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d48eb09e9bbcb83a10d2794412260d3fe352e4a6",
      "title": "A Parallelized Framework for Simulating Large-Scale LLM Agents with Realistic Environments and Interactions",
      "authors": [
        "Jun Zhang",
        "Yuwei Yan",
        "Junbo Yan",
        "Zhiheng Zheng",
        "J. Piao",
        "Depeng Jin",
        "Yong Li"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/d48eb09e9bbcb83a10d2794412260d3fe352e4a6",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d49925d273083770fe794fb96219dfb4c99293d2",
      "title": "Reinforcement Learning for Adversarial Query Generation to Enhance Relevance in Cold-Start Product Search",
      "authors": [
        "Akshay Jagatap",
        "Neeraj Anand",
        "Sonali Singh",
        "P. Comar"
      ],
      "abstract": "Accurate mapping of queries to product categories is crucial for efficient retrieval and ranking of relevant products in e-commerce search. Conventionally, such query classification models rely on supervised learning using historical user interactions, but their effectiveness diminishes in cold-start scenarios, where new categories or products lack sufficient training data. This results in poor query-to-category mappings, negatively affecting retrieval and ranking. Synthetic query generation has emerged as a promising solution by augmenting training data; however, existing methods do not incorporate feedback from the query relevance model, limiting their ability to generate queries that enhance product retrieval. To address this, we propose an adversarial reinforcement learning framework that optimizes an LLM-based generator to expose weaknesses in query classification models. The generator produces synthetic queries to augment the classifier’s training set, ultimately improving its performance. Additionally, we introduce a structured reward signal to ensure stable training. Experiments on public datasets show an average PR-AUC improvement of +1.82% on benchmarks and +3.26% on a proprietary dataset, demonstrating the framework’s effectiveness in enhancing query classification and mitigating cold-start challenges.",
      "arxiv_url": "https://www.semanticscholar.org/paper/d49925d273083770fe794fb96219dfb4c99293d2",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21061",
      "title": "LPOI: Listwise Preference Optimization for Vision Language Models",
      "authors": [
        "Fatemeh Pesaran Zadeh",
        "Yoojin Oh",
        "Gunhee Kim"
      ],
      "abstract": "Aligning large VLMs with human preferences is a challenging task, as methods like RLHF and DPO often overfit to textual information or exacerbate hallucinations. Although augmenting negative image samples partially addresses these pitfalls, no prior work has employed listwise preference optimization for VLMs, due to the complexity and cost of constructing listwise image samples. In this work, we propose LPOI, the first object-aware listwise preference optimization developed for reducing hallucinations in VLMs. LPOI identifies and masks a critical object in the image, and then interpolates the masked region between the positive and negative images to form a sequence of incrementally more complete images. The model is trained to rank these images in ascending order of object visibility, effectively reducing hallucinations while retaining visual fidelity. LPOI requires no extra annotations beyond standard pairwise preference data, as it automatically constructs the ranked lists through object masking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and Object HalBench confirm that LPOI outperforms existing preference optimization methods in reducing hallucinations and enhancing VLM performance. We make the code available at https://github.com/fatemehpesaran310/lpoi.",
      "arxiv_url": "https://arxiv.org/abs/2505.21061",
      "pdf_url": "https://arxiv.org/pdf/2505.21061",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d4c05fdecbecbf74bae897f665ce9dff8f5df14f",
      "title": "Generate First, Then Sample: Enhancing Fake News Detection with LLM-Augmented Reinforced Sampling",
      "authors": [
        "Zhao Tong",
        "Yimeng Gu",
        "Huidong Liu",
        "Qiang Liu",
        "Shu Wu",
        "Haichao Shi",
        "Xiao-Yu Zhang"
      ],
      "abstract": "The spread of fake news on online platforms has long been a pressing concern. Considering this, extensive efforts have been made to develop fake news detectors. However, a major drawback of these models is their relatively low performance - lagging by more than 20% - in identifying fake news compared to real news, making them less suitable for practical deployment. This gap is likely due to an imbalance in the dataset and the model’s inadequate understanding of data distribution on the targeted platform. In this work, we focus on improving the model’s effectiveness in detecting fake news. To achieve this, we first adopt an LLM to generate fake news in three different styles which are later incorporated into the training set, to augment the representation of fake news. Then , we apply Reinforcement Learning to dynamically sample fake news, allowing the model to learn the optimal real-to-fake news ratio for training an effective fake news detector on the targeted platform. This approach allows our model to perform effectively even with a limited amount of annotated news data and consistently improve detection accuracy across different platforms. Experimental re-sults demonstrate that our approach achieves state-of-the-art performance on two benchmark datasets, improving fake news detection performance by 24.02% and 11.06% respectively.",
      "arxiv_url": "https://www.semanticscholar.org/paper/d4c05fdecbecbf74bae897f665ce9dff8f5df14f",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d4e602343e9dcc64abae9b3e99503699d31419d8",
      "title": "Curriculum Debiasing: Toward Robust Parameter-Efficient Fine-Tuning Against Dataset Biases",
      "authors": [
        "Mingyu Lee",
        "Yeachan Kim",
        "Wing-Lam Mok",
        "SangKeun Lee"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) addresses the memory footprint issue of full fine-tuning by modifying only a subset of model parameters. However, on datasets exhibiting spurious correlations, we observed that PEFT slows down the model’s convergence on unbiased examples, while the convergence on biased examples remains fast. This leads to the model’s overfitting on biased examples, causing significant performance degradation in out-of-distribution (OOD) scenarios. Traditional debiasing methods mitigate this issue by emphasizing unbiased examples during training but often come at the cost of in-distribution (ID) performance drops. To address this trade-off issue, we propose a C URRICULUM D EBIASING framework that presents examples in a biased-to-unbiased order. Our framework initially limits the model’s exposure to unbiased examples, which are more difficult to learn, allowing it to first establish a foundation on easy-to-converge biased examples. As training progresses, we gradually increase the proportion of unbiased examples in the training set, guiding the model away from reliance on spurious correlations. Compared to the original PEFT methods, our method accelerates convergence on unbiased examples by approximately twofold and improves ID and OOD performance by 1.2% and 8.0%, respectively. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/d4e602343e9dcc64abae9b3e99503699d31419d8",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14662",
      "title": "iAgent: LLM Agent as a Shield between User and Recommender Systems",
      "authors": [
        "Wujiang Xu",
        "Yunxiao Shi",
        "Zujie Liang",
        "Xuying Ning",
        "Kai Mei",
        "Kun Wang",
        "Xi Zhu",
        "Min Xu",
        "Yongfeng Zhang"
      ],
      "abstract": "Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform's recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform's benefits, which may hinder their ability to protect and capture users' true interests. Second, these models are typically optimized using data from all users, which may overlook individual user's preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure.",
      "arxiv_url": "https://arxiv.org/abs/2502.14662",
      "pdf_url": "https://arxiv.org/pdf/2502.14662",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d52ab22c6e8b6d994fb02e184736b1bd061f3e70",
      "title": "Progressive LoRA for Multimodal Continual Instruction Tuning",
      "authors": [
        "Yahan Yu",
        "Duzhen Zhang",
        "Yong Ren",
        "Xuanle Zhao",
        "Xiuyi Chen",
        "Chenhui Chu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/d52ab22c6e8b6d994fb02e184736b1bd061f3e70",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12376",
      "title": "ConsistencyChecker: Tree-based Evaluation of LLM Generalization Capabilities",
      "authors": [
        "Zhaochen Hong",
        "Haofei Yu",
        "Jiaxuan You"
      ],
      "abstract": "Evaluating consistency in large language models (LLMs) is crucial for ensuring reliability, particularly in complex, multi-step interactions between humans and LLMs. Traditional self-consistency methods often miss subtle semantic changes in natural language and functional shifts in code or equations, which can accumulate over multiple transformations. To address this, we propose ConsistencyChecker, a tree-based evaluation framework designed to measure consistency through sequences of reversible transformations, including machine translation tasks and AI-assisted programming tasks. In our framework, nodes represent distinct text states, while edges correspond to pairs of inverse operations. Dynamic and LLM-generated benchmarks ensure a fair assessment of the model's generalization ability and eliminate benchmark leakage. Consistency is quantified based on similarity across different depths of the transformation tree. Experiments on eight models from various families and sizes show that ConsistencyChecker can distinguish the performance of different models. Notably, our consistency scores-computed entirely without using WMT paired data-correlate strongly (r>0.7) with WMT 2024 auto-ranking, demonstrating the validity of our benchmark-free approach. Our implementation is available at: https://github.com/ulab-uiuc/consistencychecker.",
      "arxiv_url": "https://arxiv.org/abs/2506.12376",
      "pdf_url": "https://arxiv.org/pdf/2506.12376",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.00216",
      "title": "Towards Style Alignment in Cross-Cultural Translation",
      "authors": [
        "Shreya Havaldar",
        "Adam Stein",
        "Eric Wong",
        "L. Ungar"
      ],
      "abstract": "Successful communication depends on the speaker's intended style (i.e., what the speaker is trying to convey) aligning with the listener's interpreted style (i.e., what the listener perceives). However, cultural differences often lead to misalignment between the two; for example, politeness is often lost in translation. We characterize the ways that LLMs fail to translate style - biasing translations towards neutrality and performing worse in non-Western languages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic Alignment), a method that leverages learned stylistic concepts to encourage LLM translation to appropriately convey cultural communication norms and align style.",
      "arxiv_url": "https://arxiv.org/abs/2507.00216",
      "pdf_url": "https://arxiv.org/pdf/2507.00216",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2508.15861",
      "title": "XFinBench: Benchmarking LLMs in Complex Financial Problem Solving and Reasoning",
      "authors": [
        "Zhihan Zhang",
        "Yixin Cao",
        "Lizi Liao"
      ],
      "abstract": "Solving financial problems demands complex reasoning, multimodal data processing, and a broad technical understanding, presenting unique challenges for current large language models (LLMs). We introduce XFinBench, a novel benchmark with 4,235 examples designed to evaluate LLM's ability in solving complex, knowledge-intensive financial problems across diverse graduate-level finance topics with multi-modal context. We identify five core capabilities of LLMs using XFinBench, i.e, terminology understanding, temporal reasoning, future forecasting, scenario planning, and numerical modelling. Upon XFinBench, we conduct extensive experiments on 18 leading models. The result shows that o1 is the best-performing text-only model with an overall accuracy of 67.3%, but still lags significantly behind human experts with 12.5%, especially in temporal reasoning and scenario planning capabilities. We further construct a knowledge bank with 3,032 finance terms for knowledge augmentation analysis, and find that relevant knowledge to the question only brings consistent accuracy improvements to small open-source model. Additionally, our error analysis reveals that rounding errors during calculation and blindness to position and intersection of curves in the image are two primary issues leading to model's poor performance in calculating and visual-context questions, respectively. Code and dataset are accessible via GitHub: https://github.com/Zhihan72/XFinBench.",
      "arxiv_url": "https://arxiv.org/abs/2508.15861",
      "pdf_url": "https://arxiv.org/pdf/2508.15861",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-08-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.16656",
      "title": "P-CoT: A Pedagogically-motivated Participatory Chain-of-Thought Prompting for Phonological Reasoning in LLMs",
      "authors": [
        "Dongjun Jang",
        "Youngchae Ahn",
        "Hyopil Shin"
      ],
      "abstract": "This study explores the potential of phonological reasoning within text-based large language models (LLMs). Utilizing the PhonologyBench benchmark, we assess tasks like rhyme word generation, g2p conversion, and syllable counting. Our evaluations across 12 LLMs reveal that while few-shot learning offers inconsistent gains, the introduction of a novel Pedagogically-motivated Participatory Chain-of-Thought (P-CoT) prompt, which is anchored in educational theories like scaffolding and discovery learning, consistently enhances performance. This method leverages structured guidance to activate latent phonological abilities, achieving up to 52% improvement and even surpassing human baselines in certain tasks. Future work could aim to optimize P-CoT prompts for specific models or explore their application across different linguistic domains.",
      "arxiv_url": "https://arxiv.org/abs/2507.16656",
      "pdf_url": "https://arxiv.org/pdf/2507.16656",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d5d42f51f6199c52a83ab1486a5ba1608e842269",
      "title": "CA-GAR: Context-Aware Alignment of LLM Generation for Document Retrieval",
      "authors": [
        "Heng Yu",
        "Junfeng Kang",
        "Rui Li",
        "Qi Liu",
        "Liyang He",
        "Zhenya Huang",
        "Shuanghong Shen",
        "Junyu Lu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/d5d42f51f6199c52a83ab1486a5ba1608e842269",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21816",
      "title": "Revisiting Common Assumptions about Arabic Dialects in NLP",
      "authors": [
        "Amr Keleg",
        "Sharon Goldwater",
        "Walid Magdy"
      ],
      "abstract": "Arabic has diverse dialects, where one dialect can be substantially different from the others. In the NLP literature, some assumptions about these dialects are widely adopted (e.g., ``Arabic dialects can be grouped into distinguishable regional dialects\") and are manifested in different computational tasks such as Arabic Dialect Identification (ADI). However, these assumptions are not quantitatively verified. We identify four of these assumptions and examine them by extending and analyzing a multi-label dataset, where the validity of each sentence in 11 different country-level dialects is manually assessed by speakers of these dialects. Our analysis indicates that the four assumptions oversimplify reality, and some of them are not always accurate. This in turn might be hindering further progress in different Arabic NLP tasks.",
      "arxiv_url": "https://arxiv.org/abs/2505.21816",
      "pdf_url": "https://arxiv.org/pdf/2505.21816",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02998",
      "title": "A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems",
      "authors": [
        "DJordje Klisura",
        "Astrid R Bernaga Torres",
        "Anna Karen G'arate-Escamilla",
        "Rajesh Roshan Biswal",
        "Ke Yang",
        "Hilal Pataci",
        "Anthony Rios"
      ],
      "abstract": "Privacy policies inform users about data collection and usage, yet their complexity limits accessibility for diverse populations. Existing Privacy Policy Question Answering (QA) systems exhibit performance disparities across English dialects, disadvantaging speakers of non-standard varieties. We propose a novel multi-agent framework inspired by human-centered design principles to mitigate dialectal biases. Our approach integrates a Dialect Agent, which translates queries into Standard American English (SAE) while preserving dialectal intent, and a Privacy Policy Agent, which refines predictions using domain expertise. Unlike prior approaches, our method does not require retraining or dialect-specific fine-tuning, making it broadly applicable across models and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves GPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from 0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without additional training data. These results highlight the effectiveness of structured agent collaboration in mitigating dialect biases and underscore the importance of designing NLP systems that account for linguistic diversity to ensure equitable access to privacy information.",
      "arxiv_url": "https://arxiv.org/abs/2506.02998",
      "pdf_url": "https://arxiv.org/pdf/2506.02998",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.14956",
      "title": "ExPerT: Effective and Explainable Evaluation of Personalized Long-Form Text Generation",
      "authors": [
        "Alireza Salemi",
        "Julian Killingback",
        "Hamed Zamani"
      ],
      "abstract": "Evaluating personalized text generated by large language models (LLMs) is challenging, as only the LLM user, i.e., prompt author, can reliably assess the output, but re-engaging the same individuals across studies is infeasible. This paper addresses the challenge of evaluating personalized text generation by introducing ExPerT, an explainable reference-based evaluation framework. ExPerT leverages an LLM to extract atomic aspects and their evidence from the generated and reference texts, match the aspects, and evaluate their alignment based on content and writing style -- two key attributes in personalized text generation. Additionally, ExPerT generates detailed, fine-grained explanations for every step of the evaluation process, enhancing transparency and interpretability. Our experiments demonstrate that ExPerT achieves a 7.2% relative improvement in alignment with human judgments compared to the state-of-the-art text generation evaluation methods. Furthermore, human evaluators rated the usability of ExPerT's explanations at 4.7 out of 5, highlighting its effectiveness in making evaluation decisions more interpretable.",
      "arxiv_url": "https://arxiv.org/abs/2501.14956",
      "pdf_url": "https://arxiv.org/pdf/2501.14956",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-01-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.09307",
      "title": "When the LM misunderstood the human chuckled: Analyzing garden path effects in humans and language models",
      "authors": [
        "S. Amouyal",
        "A. Meltzer-Asscher",
        "Jonathan Berant"
      ],
      "abstract": "Modern Large Language Models (LLMs) have shown human-like abilities in many language tasks, sparking interest in comparing LLMs' and humans' language processing. In this paper, we conduct a detailed comparison of the two on a sentence comprehension task using garden-path constructions, which are notoriously challenging for humans. Based on psycholinguistic research, we formulate hypotheses on why garden-path sentences are hard, and test these hypotheses on human participants and a large suite of LLMs using comprehension questions. Our findings reveal that both LLMs and humans struggle with specific syntactic complexities, with some models showing high correlation with human comprehension. To complement our findings, we test LLM comprehension of garden-path constructions with paraphrasing and text-to-image generation tasks, and find that the results mirror the sentence comprehension question results, further validating our findings on LLM understanding of these constructions.",
      "arxiv_url": "https://arxiv.org/abs/2502.09307",
      "pdf_url": "https://arxiv.org/pdf/2502.09307",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2509.13773",
      "title": "MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation",
      "authors": [
        "Zhipeng Bian",
        "Jieming Zhu",
        "Xuyang Xie",
        "Quanyu Dai",
        "Zhou Zhao",
        "Zhenhua Dong"
      ],
      "abstract": "The rapid advancement of generative AI technologies is driving the integration of diverse AI-powered services into smartphones, transforming how users interact with their devices. To simplify access to predefined AI services, this paper introduces MIRA, a pioneering framework for task instruction recommendation that enables intuitive one-touch AI tasking on smartphones. With MIRA, users can long-press on images or text objects to receive contextually relevant instruction recommendations for executing AI tasks. Our work introduces three key innovations: 1) A multimodal large language model (MLLM)-based recommendation pipeline with structured reasoning to extract key entities, infer user intent, and generate precise instructions; 2) A template-augmented reasoning mechanism that integrates high-level reasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based constrained decoding strategy that restricts outputs to predefined instruction candidates, ensuring coherent and intent-aligned suggestions. Through evaluation using a real-world annotated datasets and a user study, MIRA has demonstrated substantial improvements in the accuracy of instruction recommendation. The encouraging results highlight MIRA's potential to revolutionize the way users engage with AI services on their smartphones, offering a more seamless and efficient experience.",
      "arxiv_url": "https://arxiv.org/abs/2509.13773",
      "pdf_url": "https://arxiv.org/pdf/2509.13773",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-09-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.17191",
      "title": "Decomposed Opinion Summarization with Verified Aspect-Aware Modules",
      "authors": [
        "Miao Li",
        "Jey Han Lau",
        "Eduard H. Hovy",
        "Mirella Lapata"
      ],
      "abstract": "Opinion summarization plays a key role in deriving meaningful insights from large-scale online reviews. To make the process more explainable and grounded, we propose a domain-agnostic modular approach guided by review aspects (e.g., cleanliness for hotel reviews) which separates the tasks of aspect identification, opinion consolidation, and meta-review synthesis to enable greater transparency and ease of inspection. We conduct extensive experiments across datasets representing scientific research, business, and product domains. Results show that our approach generates more grounded summaries compared to strong baseline models, as verified through automated and human evaluations. Additionally, our modular approach, which incorporates reasoning based on review aspects, produces more informative intermediate outputs than other knowledge-agnostic decomposition approaches. Lastly, we provide empirical results to show that these intermediate outputs can support humans in summarizing opinions from large volumes of reviews.",
      "arxiv_url": "https://arxiv.org/abs/2501.17191",
      "pdf_url": "https://arxiv.org/pdf/2501.17191",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.08686",
      "title": "Brevity is the soul of sustainability: Characterizing LLM response lengths",
      "authors": [
        "Soham Poddar",
        "Paramita Koley",
        "Janardan Misra",
        "Sanjay Podder",
        "Navveen Balani",
        "Niloy Ganguly",
        "Saptarshi Ghosh"
      ],
      "abstract": "A significant portion of the energy consumed by Large Language Models (LLMs) arises from their inference processes; hence developing energy-efficient methods for inference is crucial. While several techniques exist for inference optimization, output compression remains relatively unexplored, with only a few preliminary efforts addressing this aspect. In this work, we first benchmark 12 decoder-only LLMs across 5 datasets, revealing that these models often produce responses that are substantially longer than necessary. We then conduct a comprehensive quality assessment of LLM responses, formally defining six information categories present in LLM responses. We show that LLMs often tend to include redundant or additional information besides the minimal answer. To address this issue of long responses by LLMs, we explore several simple and intuitive prompt-engineering strategies. Empirical evaluation shows that appropriate prompts targeting length reduction and controlling information content can achieve significant energy optimization between 25-60\\% by reducing the response length while preserving the quality of LLM responses.",
      "arxiv_url": "https://arxiv.org/abs/2506.08686",
      "pdf_url": "https://arxiv.org/pdf/2506.08686",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00549",
      "title": "Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages",
      "authors": [
        "Hyangsuk Min",
        "Yuho Lee",
        "Minjeong Ban",
        "Jiaqi Deng",
        "Nicole Hee-Yeon Kim",
        "Taewon Yun",
        "Hang Su",
        "Jason Cai",
        "Hwanjun Song"
      ],
      "abstract": "Evaluation frameworks for text summarization have evolved in terms of both domain coverage and metrics. However, existing benchmarks still lack domain-specific assessment criteria, remain predominantly English-centric, and face challenges with human annotation due to the complexity of reasoning. To address these, we introduce MSumBench, which provides a multi-dimensional, multi-domain evaluation of summarization in English and Chinese. It also incorporates specialized assessment criteria for each domain and leverages a multi-agent debate system to enhance annotation quality. By evaluating eight modern summarization models, we discover distinct performance patterns across domains and languages. We further examine large language models as summary evaluators, analyzing the correlation between their evaluation and summarization capabilities, and uncovering systematic bias in their assessment of self-generated summaries. Our benchmark dataset is publicly available at https://github.com/DISL-Lab/MSumBench.",
      "arxiv_url": "https://arxiv.org/abs/2506.00549",
      "pdf_url": "https://arxiv.org/pdf/2506.00549",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00024",
      "title": "Do Emotions Really Affect Argument Convincingness? A Dynamic Approach with LLM-based Manipulation Checks",
      "authors": [
        "Yanran Chen",
        "Steffen Eger"
      ],
      "abstract": "Emotions have been shown to play a role in argument convincingness, yet this aspect is underexplored in the natural language processing (NLP) community. Unlike prior studies that use static analyses, focus on a single text domain or language, or treat emotion as just one of many factors, we introduce a dynamic framework inspired by manipulation checks commonly used in psychology and social science; leveraging LLM-based manipulation checks, this framework examines the extent to which perceived emotional intensity influences perceived convincingness. Through human evaluation of arguments across different languages, text domains, and topics, we find that in over half of cases, human judgments of convincingness remain unchanged despite variations in perceived emotional intensity; when emotions do have an impact, they more often enhance rather than weaken convincingness. We further analyze whether 11 LLMs behave like humans in the same scenario, finding that while LLMs generally mirror human patterns, they struggle to capture nuanced emotional effects in individual judgments.",
      "arxiv_url": "https://arxiv.org/abs/2503.00024",
      "pdf_url": "https://arxiv.org/pdf/2503.00024",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d70eead076dd6d658c77360aa06d452fd386b687",
      "title": "Speed Without Sacrifice: Fine-Tuning Language Models with Medusa and Knowledge Distillation in Travel Applications",
      "authors": [
        "Daniel Zagyva",
        "Emmanouil Stergiadis",
        "Laurens Van Der Maas",
        "Aleksandra Dokic",
        "Eran Fainman",
        "Ilya Gusev",
        "Moran Beladev"
      ],
      "abstract": "In high-stakes industrial NLP applications, balancing generation quality with speed and efficiency presents significant challenges. We address them by investigating two complementary optimization approaches: Medusa for speculative decoding and knowledge distillation (KD) for model compression. We demonstrate the practical application of these techniques in real-world travel domain tasks, including trip planning, smart filters, and generating accommodation descriptions. We introduce modifications to the Medusa implementation, starting with base pre-trained models rather than conversational fine-tuned ones, and developing a simplified single-stage training process for Medusa-2 that maintains performance while reducing computational requirements. Lastly, we present a novel framework that combines Medusa with KD, achieving compounded benefits in both model size and inference speed. Our experiments with TinyLlama-1.1B as the student model and Llama-3.1-70B as the teacher show that the combined approach maintains the teacher’s performance quality while reducing inference latency by 10-20x.",
      "arxiv_url": "https://www.semanticscholar.org/paper/d70eead076dd6d658c77360aa06d452fd386b687",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d71509de325df6585d596a3f8622443bce02d293",
      "title": "Using Subtext to Enhance Generative IDRR",
      "authors": [
        "Zhipang Wang",
        "Yu Hong",
        "Weihao Sun",
        "Guodong Zhou"
      ],
      "abstract": "Implicit Discourse Relation Recognition (abbr., IDRR) is a NLP task of classifying argument pairs into different types of semantic relations. Arguments contain subtexts, some of which are beneficial to the perception of semantic relations. However, subtexts are connotative. The neural IDRR model fails to be aware of them without being given pertinent prompts. In this paper, we leverage LLaMA to generate subtexts for argument pairs, and verify the effectiveness of subtext-based IDRR. We construct an IDRR baseline using the decoder-only backbone LLaMA, and enhance it with subtext-aware relation reasoning. A confidence-diagnosed dual-channel network is used for collaboration between in-subtext and out-of-subtext IDRR. We experiment on PDTB-2.0 and PDTB-3.0 for both the main-level and secondary-level relation taxonomies. The test results show that our approach yields substantial improvements compared to the baseline, and achieves higher F 1-scores on both benchmarks than the previous decoder-only IDRR models. We make the source codes and data publicly available. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/d71509de325df6585d596a3f8622443bce02d293",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2510.10208",
      "title": "Weed Out, Then Harvest: Dual Low-Rank Adaptation is an Effective Noisy Label Detector for Noise-Robust Learning",
      "authors": [
        "Bo Yuan",
        "Yulin Chen",
        "Yin Zhang"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) large language models (LLMs) have shown impressive performance in various downstream tasks. However, in many real-world scenarios, the collected training data inevitably contains noisy labels. To learn from noisy labels, most solutions select samples with small losses for model training. However, the selected samples, in turn, impact the loss computation in the next iteration. An inaccurate initial selection can create a vicious cycle, leading to suboptimal performance. To break this cycle, we propose Delora, a novel framework that decouples the sample selection from model training. For sample selection, Delora establishes a noisy label detector by introducing clean and noisy LoRA. Benefiting from the memory effect, the clean LoRA is encouraged to memorize clean data, while the noisy LoRA is constrained to memorize mislabeled data, which serves as a learnable threshold for selecting clean and noisy samples. For model training, Delora can use carefully selected samples to fine-tune language models seamlessly. Experimental results on synthetic and real-world noisy datasets demonstrate the effectiveness of Delora in noisy label detection and text classification.",
      "arxiv_url": "https://arxiv.org/abs/2510.10208",
      "pdf_url": "https://arxiv.org/pdf/2510.10208",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-10-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01489",
      "title": "Multilingual Definition Modeling",
      "authors": [
        "Edison Marrese-Taylor",
        "Erica K. Shimomoto",
        "Alfredo Solano",
        "Enrique Reid"
      ],
      "abstract": "In this paper, we propose the first multilingual study on definition modeling. We use monolingual dictionary data for four new languages (Spanish, French, Portuguese, and German) and perform an in-depth empirical study to test the performance of pre-trained multilingual language models on definition modeling of monosemic words when finetuned on this data. Furthermore, we use a zero-shot approach to test the multilingual capabilities of two popular chat-based Large Language Models (LLMs) in the task. Results show that multilingual language models can perform on-pair with English but cannot leverage potential cross-lingual synergies, with LLMs generally offering better performance overall. A comprehensive human evaluation of the LLM-generated definition highlights the zero and few-shot capabilities of these models in this new task, also showing their shortcomings. Finally, we show that performance on our task via BERTScore strongly correlates to the performance on multilingual LLM benchmarks, suggesting that our task offers a viable compute-constrained, stable and natural alternative to these.",
      "arxiv_url": "https://arxiv.org/abs/2506.01489",
      "pdf_url": "https://arxiv.org/pdf/2506.01489",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.03975",
      "title": "GREATERPROMPT: A Unified, Customizable, and High-Performing Open-Source Toolkit for Prompt Optimization",
      "authors": [
        "Wenliang Zheng",
        "Sarkar Snigdha Sarathi Das",
        "Yusen Zhang",
        "Rui Zhang"
      ],
      "abstract": "LLMs have gained immense popularity among researchers and the general public for its impressive capabilities on a variety of tasks. Notably, the efficacy of LLMs remains significantly dependent on the quality and structure of the input prompts, making prompt design a critical factor for their performance. Recent advancements in automated prompt optimization have introduced diverse techniques that automatically enhance prompts to better align model outputs with user expectations. However, these methods often suffer from the lack of standardization and compatibility across different techniques, limited flexibility in customization, inconsistent performance across model scales, and they often exclusively rely on expensive proprietary LLM APIs. To fill in this gap, we introduce GREATERPROMPT, a novel framework that democratizes prompt optimization by unifying diverse methods under a unified, customizable API while delivering highly effective prompts for different tasks. Our framework flexibly accommodates various model scales by leveraging both text feedback-based optimization for larger LLMs and internal gradient-based optimization for smaller models to achieve powerful and precise prompt improvements. Moreover, we provide a user-friendly Web UI that ensures accessibility for non-expert users, enabling broader adoption and enhanced performance across various user groups and application scenarios. GREATERPROMPT is available at https://github.com/psunlpgroup/GreaterPrompt via GitHub, PyPI, and web user interfaces.",
      "arxiv_url": "https://arxiv.org/abs/2504.03975",
      "pdf_url": "https://arxiv.org/pdf/2504.03975",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d746c74dc9e337253c45d25a191258739221591d",
      "title": "Acoustic Individual Identification of White-Faced Capuchin Monkeys Using Joint Multi-Species Embeddings",
      "authors": [
        "Álvaro Vega-Hidalgo",
        "Artem Abzaliev",
        "Thore Bergman",
        "Rada Mihalcea"
      ],
      "abstract": "Acoustic individual identification of wild animals is an essential task for understanding animal vocalizations within their social contexts, and for facilitating conservation and wildlife monitoring efforts. However, most of the work in this space relies on human efforts, as the development of methods for automatic individual identification is hindered by the lack of data. In this paper, we explore cross-species pre-training to address the task of individual classification in white-faced capuchin monkeys. Using acoustic embeddings from birds and humans, we find that they can be effectively used to identify the calls from individual monkeys. Moreover, we find that joint multi-species representations can lead to further improvements over the use of one representation at a time. Our work demonstrates the potential of cross-species data transfer and multi-species representations, as strategies to address tasks on species with very limited data.",
      "arxiv_url": "https://www.semanticscholar.org/paper/d746c74dc9e337253c45d25a191258739221591d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d74980a6030f6f2e1f72f788abbd61ce627b116f",
      "title": "Annotating the Annotators: Analysis, Insights and Modelling from an Annotation Campaign on Persuasion Techniques Detection",
      "authors": [
        "Davide Bassi",
        "D. Dimitrov",
        "Bernardo D'Auria",
        "Firoj Alam",
        "Maram Hasanain",
        "Christian Moro",
        "L. Orrù",
        "G. Turchi",
        "Preslav Nakov",
        "Giovanni Da San Martino"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/d74980a6030f6f2e1f72f788abbd61ce627b116f",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d799ac055349e2d3989ba591a9fd89f36db539cc",
      "title": "MERaLiON-AudioLLM: Advancing Speech and Language Understanding for Singapore",
      "authors": [
        "Yingxu He",
        "Zhuohan Liu",
        "Geyu Lin",
        "Shuo Sun",
        "Bin Wang",
        "Wenyu Zhang",
        "Xunlong Zou",
        "Nancy F. Chen",
        "AiTi Aw"
      ],
      "abstract": "We introduce MERaLiON-AudioLLM, the first general-purpose multitask audio-based large language model designed to understand Singlish, a colloquial and code-switched variety of English spoken in Singapore. Trained on 62 million multimodal instruction samples spanning over 260,000 hours of audio, MERaLiON-AudioLLM exhibits strong performance across diverse tasks including automatic speech recognition, spoken question answering, speech translation, and paralinguistic analysis. We benchmark MERaLiON-AudioLLM across a broad range of multilingual and multi-task scenarios, and it demonstrates competitive performance against existing open-source models. The model achieves significant gains in local speech recognition and task-specific understanding, underscoring its utility for region-specific AI applications. We develop an interactive demo interface to enable user-friendly access, supported by a back-end with custom caching and load-balancing mechanisms. The interactive demos, model weights and video are publicly available for both the first release of MERaLiON-AudioLLM 1 and the recent second release of MERaLiON-2 2 . This paper focuses exclusively on the development and evaluation of the first release.",
      "arxiv_url": "https://www.semanticscholar.org/paper/d799ac055349e2d3989ba591a9fd89f36db539cc",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16586",
      "title": "Multimodal Large Language Models for Text-rich Image Understanding: A Comprehensive Review",
      "authors": [
        "Pei Fu",
        "Tongkun Guan",
        "Zining Wang",
        "Zhentao Guo",
        "Chen Duan",
        "Hao Sun",
        "Boming Chen",
        "Jiayao Ma",
        "Qianyi Jiang",
        "Kai Zhou",
        "Junfeng Luo"
      ],
      "abstract": "The recent emergence of Multi-modal Large Language Models (MLLMs) has introduced a new dimension to the Text-rich Image Understanding (TIU) field, with models demonstrating impressive and inspiring performance. However, their rapid evolution and widespread adoption have made it increasingly challenging to keep up with the latest advancements. To address this, we present a systematic and comprehensive survey to facilitate further research on TIU MLLMs. Initially, we outline the timeline, architecture, and pipeline of nearly all TIU MLLMs. Then, we review the performance of selected models on mainstream benchmarks. Finally, we explore promising directions, challenges, and limitations within the field.",
      "arxiv_url": "https://arxiv.org/abs/2502.16586",
      "pdf_url": "https://arxiv.org/pdf/2502.16586",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.17784",
      "title": "EXECUTE: A Multilingual Benchmark for LLM Token Understanding",
      "authors": [
        "Lukas Edman",
        "Helmut Schmid",
        "Alexander Fraser"
      ],
      "abstract": "The CUTE benchmark showed that LLMs struggle with character understanding in English. We extend it to more languages with diverse scripts and writing systems, introducing EXECUTE. Our simplified framework allows easy expansion to any language. Tests across multiple LLMs reveal that challenges in other languages are not always on the character level as in English. Some languages show word-level processing issues, some show no issues at all. We also examine sub-character tasks in Chinese, Japanese, and Korean to assess LLMs' understanding of character components.",
      "arxiv_url": "https://arxiv.org/abs/2505.17784",
      "pdf_url": "https://arxiv.org/pdf/2505.17784",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03978",
      "title": "Structured Pruning for Diverse Best-of-N Reasoning Optimization",
      "authors": [
        "Hieu Trung Nguyen",
        "Bao Nguyen",
        "Viet Anh Nguyen"
      ],
      "abstract": "Model pruning in transformer-based language models, traditionally viewed as a means of achieving computational savings, can enhance the model's reasoning capabilities. In this work, we uncover a surprising phenomenon: the selective pruning of certain attention heads leads to improvements in reasoning performance, particularly on challenging tasks. Motivated by this observation, we propose SPRINT, a novel contrastive learning framework that dynamically selects the optimal head and layer to prune during inference. By aligning question embeddings with head embeddings, SPRINT identifies those pruned-head configurations that result in more accurate reasoning. Extensive experiments demonstrate that our method significantly outperforms traditional best-of-$N$ and random head selection strategies on the MATH500 and GSM8K datasets.",
      "arxiv_url": "https://arxiv.org/abs/2506.03978",
      "pdf_url": "https://arxiv.org/pdf/2506.03978",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d7dd0b0a40dfffb7668375c8bafdf13a7ff43cfa",
      "title": "The Linguistic Connectivities Within Large Language Models",
      "authors": [
        "Dan Wang",
        "Boxi Cao",
        "Ning Bian",
        "Xuanang Chen",
        "Yaojie Lu",
        "Hongyu Lin",
        "Jia Zheng",
        "Le Sun",
        "Shanshan Jiang",
        "Bin Dong",
        "Xianpei Han"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/d7dd0b0a40dfffb7668375c8bafdf13a7ff43cfa",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22176",
      "title": "TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation",
      "authors": [
        "Vihang Pancholi",
        "J. Bafna",
        "Tejas Anvekar",
        "Manish Shrivastava",
        "Vivek Gupta"
      ],
      "abstract": "Evaluating tables qualitatively and quantitatively poses a significant challenge, as standard metrics often overlook subtle structural and content-level discrepancies. To address this, we propose a rubric-based evaluation framework that integrates multi-level structural descriptors with fine-grained contextual signals, enabling more precise and consistent table comparison. Building on this, we introduce TabXEval, an eXhaustive and eXplainable two-phase evaluation framework. TabXEval first aligns reference and predicted tables structurally via TabAlign, then performs semantic and syntactic comparison using TabCompare, offering interpretable and granular feedback. We evaluate TabXEval on TabXBench, a diverse, multi-domain benchmark featuring realistic table perturbations and human annotations. A sensitivity-specificity analysis further demonstrates the robustness and explainability of TabXEval across varied table tasks. Code and data are available at https://coral-lab-asu.github.io/tabxeval/",
      "arxiv_url": "https://arxiv.org/abs/2505.22176",
      "pdf_url": "https://arxiv.org/pdf/2505.22176",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12414",
      "title": "Lost in Transcription, Found in Distribution Shift: Demystifying Hallucination in Speech Foundation Models",
      "authors": [
        "Hanin Atwany",
        "Abdul Waheed",
        "Rita Singh",
        "Monojit Choudhury",
        "Bhiksha Raj"
      ],
      "abstract": "Speech foundation models trained at a massive scale, both in terms of model and data size, result in robust systems capable of performing multiple speech tasks, including automatic speech recognition (ASR). These models transcend language and domain barriers, yet effectively measuring their performance remains a challenge. Traditional metrics like word error rate (WER) and character error rate (CER) are commonly used to evaluate ASR performance but often fail to reflect transcription quality in critical contexts, particularly when detecting fabricated outputs. This phenomenon, known as hallucination, is especially concerning in high-stakes domains such as healthcare, legal, and aviation, where errors can have severe consequences. In our work, we address this gap by investigating hallucination in ASR models. We examine how factors such as distribution shifts, model size, and model architecture influence the hallucination error rate (HER), a metric we introduce to quantify hallucinations. Our analysis of over 20 ASR models reveals \\numinsights~key insights: (1) High WERs can mask low hallucination rates, while low WERs may conceal dangerous hallucinations. (2) Synthetic noise, both adversarial and common perturbations like white noise, pitch shift, and time stretching, increase HER. (3) Distribution shift correlates strongly with HER ($\\alpha = 0.91$). Our findings highlight the importance of incorporating HER alongside traditional metrics like WER to better assess ASR model performance, particularly in high-stakes domains.",
      "arxiv_url": "https://arxiv.org/abs/2502.12414",
      "pdf_url": "https://arxiv.org/pdf/2502.12414",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11448",
      "title": "AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection",
      "authors": [
        "Weidi Luo",
        "Shenghong Dai",
        "Xiaogeng Liu",
        "Suman Banerjee",
        "Huan Sun",
        "Muhao Chen",
        "Chaowei Xiao"
      ],
      "abstract": "The rapid advancements in Large Language Models (LLMs) have enabled their deployment as autonomous agents for handling complex tasks in dynamic environments. These LLMs demonstrate strong problem-solving capabilities and adaptability to multifaceted scenarios. However, their use as agents also introduces significant risks, including task-specific risks, which are identified by the agent administrator based on the specific task requirements and constraints, and systemic risks, which stem from vulnerabilities in their design or interactions, potentially compromising confidentiality, integrity, or availability (CIA) of information and triggering security risks. Existing defense agencies fail to adaptively and effectively mitigate these risks. In this paper, we propose AGrail, a lifelong agent guardrail to enhance LLM agent safety, which features adaptive safety check generation, effective safety check optimization, and tool compatibility and flexibility. Extensive experiments demonstrate that AGrail not only achieves strong performance against task-specific and system risks but also exhibits transferability across different LLM agents' tasks.",
      "arxiv_url": "https://arxiv.org/abs/2502.11448",
      "pdf_url": "https://arxiv.org/pdf/2502.11448",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14497",
      "title": "Stories that (are) Move(d by) Markets: A Causal Exploration of Market Shocks and Semantic Shifts across Different Partisan Groups",
      "authors": [
        "Felix Drinkall",
        "Stefan Zohren",
        "Michael McMahon",
        "J. Pierrehumbert"
      ],
      "abstract": "Macroeconomic fluctuations and the narratives that shape them form a mutually reinforcing cycle: public discourse can spur behavioural changes leading to economic shifts, which then result in changes in the stories that propagate. We show that shifts in semantic embedding space can be causally linked to financial market shocks -- deviations from the expected market behaviour. Furthermore, we show how partisanship can influence the predictive power of text for market fluctuations and shape reactions to those same shocks. We also provide some evidence that text-based signals are particularly salient during unexpected events such as COVID-19, highlighting the value of language data as an exogenous variable in economic forecasting. Our findings underscore the bidirectional relationship between news outlets and market shocks, offering a novel empirical approach to studying their effect on each other.",
      "arxiv_url": "https://arxiv.org/abs/2502.14497",
      "pdf_url": "https://arxiv.org/pdf/2502.14497",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.13677",
      "title": "Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results",
      "authors": [
        "Andrea Santilli",
        "Adam Golinski",
        "Michael Kirchhof",
        "Federico Danieli",
        "Arno Blaas",
        "Miao Xiong",
        "Luca Zappella",
        "Sinead Williamson"
      ],
      "abstract": "Uncertainty Quantification (UQ) in Language Models (LMs) is key to improving their safety and reliability. Evaluations often use metrics like AUROC to assess how well UQ methods (e.g., negative sequence probabilities) correlate with task correctness functions (e.g., ROUGE-L). We show that mutual biases--when both UQ methods and correctness functions are biased by the same factors--systematically distort evaluation. First, we formally prove that any mutual bias non-randomly skews AUROC rankings, compromising benchmark integrity. Second, we confirm this happens empirically by testing 7 widely used correctness functions, from lexical-based and embedding-based metrics to LM-as-a-judge approaches, across 4 datasets x 4 models x 8 UQ methods. Our analysis shows that length biases in correctness functions distort UQ assessments by interacting with length biases in UQ methods. We identify LM-as-a-judge methods as the least length-biased, offering a promising path for a fairer UQ evaluation.",
      "arxiv_url": "https://arxiv.org/abs/2504.13677",
      "pdf_url": "https://arxiv.org/pdf/2504.13677",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d8e70363b55655b55fee8b70ade285006f4cc9c3",
      "title": "Scaling up the State Size of RNN LLMs for Long-Context Scenarios",
      "authors": [
        "Kai Liu",
        "Jianfei Gao",
        "Kai Chen"
      ],
      "abstract": "The Transformer architecture has become the standard LLM architecture due to its powerful self-attention mechanism. However, it suffers from quadratic computational complexity and linear memory complexity. RNN-based LLMs have been proposed as alternatives. Yet, RNN models struggle in long-context scenarios, making it challenging to replace self-attention with RNNs. We identify the state size as a critical bottleneck, which is significantly smaller than that of Transformers with a basic context length of 2k. However, simply increasing the state size significantly raises the number of parameters and lowers training efficiency. In this paper, we propose an efficient scaling method to scale the state size of RNN models to match the 2k context length of Transformers, with small parameters overhead. Experimental re-sults demonstrate that scaling the state size significantly enhances long-context understanding. Retrieval performance scales almost linearly with state size, with a 454M model featuring an expanded state achieving performance comparable to a 1.47B model on FDA, a recall-intensive task. These findings highlight state scaling as a promising approach for advancing RNN-based LLMs.",
      "arxiv_url": "https://www.semanticscholar.org/paper/d8e70363b55655b55fee8b70ade285006f4cc9c3",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d9008193574110313cfbfc665a99d762eb4ca699",
      "title": "Improving Parallel Sentence Mining for Low-Resource and Endangered Languages",
      "authors": [
        "Shu Okabe",
        "Katharina Hämmerl",
        "Alexander Fraser"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/d9008193574110313cfbfc665a99d762eb4ca699",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.12911",
      "title": "Benchmarking Multi-National Value Alignment for Large Language Models",
      "authors": [
        "Weijie Shi",
        "Chengyi Ju",
        "Chengzhong Liu",
        "Jiaming Ji",
        "Jipeng Zhang",
        "Ruiyuan Zhang",
        "Jia Zhu",
        "Jiajie Xu",
        "Yaodong Yang",
        "Sirui Han",
        "Yike Guo"
      ],
      "abstract": "Do Large Language Models (LLMs) hold positions that conflict with your country's values? Occasionally they do! However, existing works primarily focus on ethical reviews, failing to capture the diversity of national values, which encompass broader policy, legal, and moral considerations. Furthermore, current benchmarks that rely on spectrum tests using manually designed questionnaires are not easily scalable. To address these limitations, we introduce NaVAB, a comprehensive benchmark to evaluate the alignment of LLMs with the values of five major nations: China, the United States, the United Kingdom, France, and Germany. NaVAB implements a national value extraction pipeline to efficiently construct value assessment datasets. Specifically, we propose a modeling procedure with instruction tagging to process raw data sources, a screening process to filter value-related topics and a generation process with a Conflict Reduction mechanism to filter non-conflicting values.We conduct extensive experiments on various LLMs across countries, and the results provide insights into assisting in the identification of misaligned scenarios. Moreover, we demonstrate that NaVAB can be combined with alignment techniques to effectively reduce value concerns by aligning LLMs' values with the target country.",
      "arxiv_url": "https://arxiv.org/abs/2504.12911",
      "pdf_url": "https://arxiv.org/pdf/2504.12911",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00748",
      "title": "Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning in Large Language Model Translations",
      "authors": [
        "Pardis Sadat Zahraei",
        "Ali Emami"
      ],
      "abstract": "Addressing gender bias and maintaining logical coherence in machine translation remains challenging, particularly when translating between natural gender languages, like English, and genderless languages, such as Persian, Indonesian, and Finnish. We introduce the Translate-with-Care (TWC) dataset, comprising 3,950 challenging scenarios across six low- to mid-resource languages, to assess translation systems' performance. Our analysis of diverse technologies, including GPT-4, mBART-50, NLLB-200, and Google Translate, reveals a universal struggle in translating genderless content, resulting in gender stereotyping and reasoning errors. All models preferred masculine pronouns when gender stereotypes could influence choices. Google Translate and GPT-4 showed particularly strong bias, favoring male pronouns 4-6 times more than feminine ones in leadership and professional success contexts. Fine-tuning mBART-50 on TWC substantially resolved these biases and errors, led to strong generalization, and surpassed proprietary LLMs while remaining open-source. This work emphasizes the need for targeted approaches to gender and semantic coherence in machine translation, particularly for genderless languages, contributing to more equitable and accurate translation systems.",
      "arxiv_url": "https://arxiv.org/abs/2506.00748",
      "pdf_url": "https://arxiv.org/pdf/2506.00748",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.10415",
      "title": "Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?",
      "authors": [
        "Yingjin Song",
        "Yupei Du",
        "Denis Paperno",
        "Albert Gatt"
      ],
      "abstract": "This paper introduces the TempVS benchmark, which focuses on temporal grounding and reasoning capabilities of Multimodal Large Language Models (MLLMs) in image sequences. TempVS consists of three main tests (i.e., event relation inference, sentence ordering and image ordering), each accompanied with a basic grounding test. TempVS requires MLLMs to rely on both visual and linguistic modalities to understand the temporal order of events. We evaluate 38 state-of-the-art MLLMs, demonstrating that models struggle to solve TempVS, with a substantial performance gap compared to human capabilities. We also provide fine-grained insights that suggest promising directions for future research. Our TempVS benchmark data and code are available at https://github.com/yjsong22/TempVS.",
      "arxiv_url": "https://arxiv.org/abs/2506.10415",
      "pdf_url": "https://arxiv.org/pdf/2506.10415",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01531",
      "title": "STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework",
      "authors": [
        "Wenhao Liu",
        "Zhenyi Lu",
        "Xinyu Hu",
        "Jierui Zhang",
        "Dailin Li",
        "Jiacheng Cen",
        "Huilin Cao",
        "Haiteng Wang",
        "Yuhan Li",
        "Kun Xie",
        "Dandan Li",
        "Pei Zhang",
        "Chengbo Zhang",
        "Yuxiang Ren",
        "Xiaohong Huang",
        "Yan Ma"
      ],
      "abstract": "High-quality math datasets are crucial for advancing the reasoning abilities of large language models (LLMs). However, existing datasets often suffer from three key issues: outdated and insufficient challenging content, neglecting human-like reasoning, and limited reliability due to single-LLM generation. To address these, we introduce STORM-BORN, an ultra-challenging dataset of mathematical derivations sourced from cutting-edge academic papers, which includes dense human-like approximations and heuristic cues. To ensure the reliability and quality, we propose a novel human-in-the-loop, multi-agent data generation framework, integrating reasoning-dense filters, multi-agent collaboration, and human mathematicians' evaluations. We curated a set of 2,000 synthetic samples and deliberately selected the 100 most difficult problems. Even most advanced models like GPT-o1 solved fewer than 5% of them. Fine-tuning on STORM-BORN boosts accuracy by 7.84% (LLaMA3-8B) and 9.12% (Qwen2.5-7B). As AI approaches mathematician-level reasoning, STORM-BORN provides both a high-difficulty benchmark and a human-like reasoning training resource. Our code and dataset are publicly available at https://github.com/lwhere/STORM-BORN.",
      "arxiv_url": "https://arxiv.org/abs/2506.01531",
      "pdf_url": "https://arxiv.org/pdf/2506.01531",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.16075",
      "title": "PISCO: Pretty Simple Compression for Retrieval-Augmented Generation",
      "authors": [
        "Maxime Louis",
        "Herv'e D'ejean",
        "S. Clinchant"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) pipelines enhance Large Language Models (LLMs) by retrieving relevant documents, but they face scalability issues due to high inference costs and limited context size. Document compression is a practical solution, but current soft compression methods suffer from accuracy losses and require extensive pretraining. In this paper, we introduce PISCO, a novel method that achieves a 16x compression rate with minimal accuracy loss (0-3%) across diverse RAG-based question-answering (QA) tasks. Unlike existing approaches, PISCO requires no pretraining or annotated data, relying solely on sequence-level knowledge distillation from document-based questions. With the ability to fine-tune a 7-10B LLM in 48 hours on a single A100 GPU, PISCO offers a highly efficient and scalable solution. We present comprehensive experiments showing that PISCO outperforms existing compression models by 8% in accuracy.",
      "arxiv_url": "https://arxiv.org/abs/2501.16075",
      "pdf_url": "https://arxiv.org/pdf/2501.16075",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-01-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d9cdb3e663386b23e0e91f54d2ffce473c349f58",
      "title": "Modeling the Evolution of English Noun Compounds with Feature-Rich Diachronic Compositionality Prediction",
      "authors": [
        "Filip Miletic",
        "Sabine Schulte im Walde"
      ],
      "abstract": "We analyze the evolution of English noun compounds, which we represent as vectors of time-specific values. We implement a wide array of methods to create a rich set of features, us-ing them to classify compounds for present-day compositionality and to assess the informative-ness of the corresponding linguistic patterns. Our best results use BERT – reflecting the similarity of compounds and sentence contexts – and we further capture relevant and complementary information across approaches. Lever-aging these feature differences, we find that the development of low-compositional meanings is reflected by a parallel drop in compositionality and sustained semantic change. The same distinction is echoed in transformer processing: compositionality estimates require far less contextualization than semantic change estimates.",
      "arxiv_url": "https://www.semanticscholar.org/paper/d9cdb3e663386b23e0e91f54d2ffce473c349f58",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "d9e39f8792cf127a747d4c27359185d95874eb4c",
      "title": "Measuring What Matters: Evaluating Ensemble LLMs with Label Refinement in Inductive Coding",
      "authors": [
        "Angelina Parfenova",
        "Jürgen Pfeffer"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/d9e39f8792cf127a747d4c27359185d95874eb4c",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "da00b5fa3611bc2e33f15512264e47566f006c2a",
      "title": "A Reinforcement Learning Framework for Cross-Lingual Stance Detection Using Chain-of-Thought Alignment",
      "authors": [
        "Binghui Li",
        "Minghui Zou",
        "Xiaowang Zhang",
        "Shizhan Chen",
        "Zhiyong Feng"
      ],
      "abstract": "Cross-lingual stance detection identifies users’ attitudes toward specific targets in texts by transferring knowledge from source languages to target languages. Previous studies have typically facilitated this transfer by translating and aligning labels or targets. However, these meth-ods cannot effectively perform cross-lingual transfer of the complex reasoning processes in stance detection. To address this challenge, we propose a reinforcement learning framework using cross-lingual Chain-of-Thought (CoT) alignment, referred to as RCCA. Specifically, we adopt a cross-lingual CoT alignment strategy to obtain the high-quality CoTs generated from target language inputs. After that, we leverage reinforcement learning by sampling CoTs and assigning rewards according to pre-defined rules, aiming to enhance the model’s generalization capabilities in the target language. Experimental results on four multilingual datasets demonstrate that our approach outperforms competitive methods.",
      "arxiv_url": "https://www.semanticscholar.org/paper/da00b5fa3611bc2e33f15512264e47566f006c2a",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19582",
      "title": "Where Are We? Evaluating LLM Performance on African Languages",
      "authors": [
        "Ife Adebara",
        "Hawau Olamide Toyin",
        "Nahom Tesfu Ghebremichael",
        "AbdelRahim Elmadany",
        "M. Abdul-Mageed"
      ],
      "abstract": "Africa's rich linguistic heritage remains underrepresented in NLP, largely due to historical policies that favor foreign languages and create significant data inequities. In this paper, we integrate theoretical insights on Africa's language landscape with an empirical evaluation using Sahara - a comprehensive benchmark curated from large-scale, publicly accessible datasets capturing the continent's linguistic diversity. By systematically assessing the performance of leading large language models (LLMs) on Sahara, we demonstrate how policy-induced data variations directly impact model effectiveness across African languages. Our findings reveal that while a few languages perform reasonably well, many Indigenous languages remain marginalized due to sparse data. Leveraging these insights, we offer actionable recommendations for policy reforms and inclusive data practices. Overall, our work underscores the urgent need for a dual approach - combining theoretical understanding with empirical evaluation - to foster linguistic diversity in AI for African communities.",
      "arxiv_url": "https://arxiv.org/abs/2502.19582",
      "pdf_url": "https://arxiv.org/pdf/2502.19582",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11431",
      "title": "Any Information Is Just Worth One Single Screenshot: Unifying Search With Visualized Information Retrieval",
      "authors": [
        "Ze Liu",
        "Zhengyang Liang",
        "Junjie Zhou",
        "Zheng Liu",
        "Defu Lian"
      ],
      "abstract": "With the popularity of multimodal techniques, it receives growing interests to acquire useful information in visual forms. In this work, we formally define an emerging IR paradigm called \\textit{Visualized Information Retrieval}, or \\textbf{Vis-IR}, where multimodal information, such as texts, images, tables and charts, is jointly represented by a unified visual format called \\textbf{Screenshots}, for various retrieval applications. We further make three key contributions for Vis-IR. First, we create \\textbf{VIRA} (Vis-IR Aggregation), a large-scale dataset comprising a vast collection of screenshots from diverse sources, carefully curated into captioned and question-answer formats. Second, we develop \\textbf{UniSE} (Universal Screenshot Embeddings), a family of retrieval models that enable screenshots to query or be queried across arbitrary data modalities. Finally, we construct \\textbf{MVRB} (Massive Visualized IR Benchmark), a comprehensive benchmark covering a variety of task forms and application scenarios. Through extensive evaluations on MVRB, we highlight the deficiency from existing multimodal retrievers and the substantial improvements made by UniSE. Our work will be shared with the community, laying a solid foundation for this emerging field.",
      "arxiv_url": "https://arxiv.org/abs/2502.11431",
      "pdf_url": "https://arxiv.org/pdf/2502.11431",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "da185569de364e49f8dbbe4ed689866766f3f550",
      "title": "Evaluation of Attribution Bias in Generator-Aware Retrieval-Augmented Large Language Models",
      "authors": [
        "Amin Abolghasemi",
        "Leif Azzopardi",
        "Seyyed Hadi Hashemi",
        "M. D. Rijke",
        "Suzan Verberne"
      ],
      "abstract": ".",
      "arxiv_url": "https://www.semanticscholar.org/paper/da185569de364e49f8dbbe4ed689866766f3f550",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.17900",
      "title": "MedPlan:A Two-Stage RAG-Based System for Personalized Medical Plan Generation",
      "authors": [
        "Hsin-Ling Hsu",
        "Cong-Tinh Dao",
        "Luning Wang",
        "Zitao Shuai",
        "T. M. Phan",
        "Jun-En Ding",
        "Chun-Chieh Liao",
        "Pengfei Hu",
        "Xiaoxue Han",
        "Chih-Ho Hsu",
        "Dongsheng Luo",
        "Wen-Chih Peng",
        "Feng Liu",
        "Fang-Ming Hung",
        "Chenwei Wu"
      ],
      "abstract": "Despite recent success in applying large language models (LLMs) to electronic health records (EHR), most systems focus primarily on assessment rather than treatment planning. We identify three critical limitations in current approaches: they generate treatment plans in a single pass rather than following the sequential reasoning process used by clinicians; they rarely incorporate patient-specific historical context; and they fail to effectively distinguish between subjective and objective clinical information. Motivated by the SOAP methodology (Subjective, Objective, Assessment, Plan), we introduce \\ours{}, a novel framework that structures LLM reasoning to align with real-life clinician workflows. Our approach employs a two-stage architecture that first generates a clinical assessment based on patient symptoms and objective data, then formulates a structured treatment plan informed by this assessment and enriched with patient-specific information through retrieval-augmented generation. Comprehensive evaluation demonstrates that our method significantly outperforms baseline approaches in both assessment accuracy and treatment plan quality.",
      "arxiv_url": "https://arxiv.org/abs/2503.17900",
      "pdf_url": "https://arxiv.org/pdf/2503.17900",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization",
        "RAG"
      ],
      "published_date": "2025-03-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "da90291ffa1197fc74b596ff562d765c640293d9",
      "title": "War of Thoughts: Competition Stimulates Stronger Reasoning in Large Language Models",
      "authors": [
        "Yibin Chen",
        "Jinyi Liu",
        "Yan Zheng",
        "Yifu Yuan",
        "Jianye Hao"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/da90291ffa1197fc74b596ff562d765c640293d9",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "da926def1d138654d74f4e5d36d49e4fd15ed076",
      "title": "Structured Discourse Representation for Factual Consistency Verification",
      "authors": [
        "Kun Zhang",
        "Oana Balalau",
        "Ioana Manolescu"
      ],
      "abstract": "Analysing the differences in how events are represented across texts, or verifying whether the language model generations hallucinate, requires the ability to systematically compare their content. To support such comparison, structured representation that captures fine-grained information plays a vital role. In particular, identifying distinct atomic facts and the discourse relations connecting them enables deeper semantic comparison. Our proposed approach combines structured discourse information extraction with a classifier, FDSpotter , for factual consistency verification. We show that adversarial discourse relations pose challenges for language models, but fine-tuning on our annotated data, DiscInfer , achieves competitive performance. Our proposed approach advances factual consistency verification by grounding in linguistic structure and decomposing it into interpretable components. We demonstrate the effectiveness of our method on the evaluation of two tasks: data-to-text generation and text summarisation. Our code and dataset will be publicly available on GitHub.",
      "arxiv_url": "https://www.semanticscholar.org/paper/da926def1d138654d74f4e5d36d49e4fd15ed076",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.09447",
      "title": "Pixel-Level Reasoning Segmentation via Multi-turn Conversations",
      "authors": [
        "Dexian Cai",
        "Xiaocui Yang",
        "Yongkang Liu",
        "Daling Wang",
        "Shi Feng",
        "Yifei Zhang",
        "Soujanya Poria"
      ],
      "abstract": "Existing visual perception systems focus on region-level segmentation in single-turn dialogues, relying on complex and explicit query instructions. Such systems cannot reason at the pixel level and comprehend dynamic user intent that changes over interaction. Our work tackles this issue by introducing a novel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on multi-turn conversations, tracking evolving user intent via multi-turn interactions for fine-grained segmentation. To establish a benchmark for this novel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on Multi-Turn Conversations (PRIST), comprising 24k utterances from 8.3k multi-turn conversational scenarios with segmentation targets. Building on PRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning Segmentation framework, integrates pixel-level segmentation with robust multi-turn conversation understanding, generating pixel-grounded explanations aligned with user intent. The PRIST dataset and MIRSA framework fill the gap in pixel-level reasoning segmentation. Experimental results on the PRIST dataset demonstrate that our method outperforms current segmentation-specific baselines in terms of segmentation and LLM-based reasoning metrics. The code and data are available at: https://github.com/ccccai239/PixelRIST.",
      "arxiv_url": "https://arxiv.org/abs/2502.09447",
      "pdf_url": "https://arxiv.org/pdf/2502.09447",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11900",
      "title": "Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data",
      "authors": [
        "Philipp Christmann",
        "G. Weikum"
      ],
      "abstract": "Question answering over mixed sources, like text and tables, has been advanced by verbalizing all contents and encoding it with a language model. A prominent case of such heterogeneous data is personal information: user devices log vast amounts of data every day, such as calendar entries, workout statistics, shopping records, streaming history, and more. Information needs range from simple look-ups to queries of analytical nature. The challenge is to provide humans with convenient access with small footprint, so that all personal data stays on the user devices. We present ReQAP, a novel method that creates an executable operator tree for a given question, via recursive decomposition. Operators are designed to enable seamless integration of structured and unstructured sources, and the execution of the operator tree yields a traceable answer. We further release the PerQA benchmark, with persona-based data and questions, covering a diverse spectrum of realistic user needs.",
      "arxiv_url": "https://arxiv.org/abs/2505.11900",
      "pdf_url": "https://arxiv.org/pdf/2505.11900",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "db0c7c56fb763ec3f3a5316a6bddb443dc00f83c",
      "title": "Memory or Reasoning? Explore How LLMs Compute Mixed Arithmetic Expressions",
      "authors": [
        "Chengzhi Li",
        "Heyan Huang",
        "Ping Jian",
        "Zhen Yang",
        "Chenxu Wang",
        "Yifan Wang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/db0c7c56fb763ec3f3a5316a6bddb443dc00f83c",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.23440",
      "title": "Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation",
      "authors": [
        "Mingzhe Li",
        "Xin Lu",
        "Yanyan Zhao"
      ],
      "abstract": "Synthesizing high-quality instruction data from unsupervised text is a promising paradigm for training large language models (LLMs), yet automated methods for this task still exhibit significant limitations in the diversity and difficulty of synthesized instructions. To address these challenges, we propose Self-Foveate, an LLM-driven method for instruction synthesis. Inspired by hierarchical human visual perception, Self-Foveate introduces a\"Micro-Scatter-Macro\"multi-level foveation methodology that guides the extraction of textual information at three complementary granularities, from fine-grained details through cross-region connections to holistic patterns, thereby enhancing both the diversity and difficulty of synthesized instructions. Furthermore, a re-synthesis module is incorporated to improve the fidelity of instructions to source text and their overall quality. Comprehensive experiments across multiple unsupervised corpora and diverse model architectures demonstrate that Self-Foveate consistently outperforms existing methods. We publicly release our code at https://github.com/Mubuky/Self-Foveate",
      "arxiv_url": "https://arxiv.org/abs/2507.23440",
      "pdf_url": "https://arxiv.org/pdf/2507.23440",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.15245",
      "title": "Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework",
      "authors": [
        "Zihao Jiang",
        "Ben Liu",
        "Miao Peng",
        "Wenjie Xu",
        "Yao Xiao",
        "Zhenyan Shan",
        "Min Peng"
      ],
      "abstract": "While large language models (LLMs) show great potential in temporal reasoning, most existing work focuses heavily on enhancing performance, often neglecting the explainable reasoning processes underlying the results. To address this gap, we introduce a comprehensive benchmark covering a wide range of temporal granularities, designed to systematically evaluate LLMs' capabilities in explainable temporal reasoning. Furthermore, our findings reveal that LLMs struggle to deliver convincing explanations when relying solely on textual information. To address challenge, we propose GETER, a novel structure-aware generative framework that integrates Graph structures with text for Explainable TEmporal Reasoning. Specifically, we first leverage temporal knowledge graphs to develop a temporal encoder that captures structural information for the query. Subsequently, we introduce a structure-text prefix adapter to map graph structure features into the text embedding space. Finally, LLMs generate explanation text by seamlessly integrating the soft graph token with instruction-tuning prompt tokens. Experimental results indicate that GETER achieves state-of-the-art performance while also demonstrating its effectiveness as well as strong generalization capabilities. Our dataset and code are available at https://github.com/carryTatum/GETER.",
      "arxiv_url": "https://arxiv.org/abs/2505.15245",
      "pdf_url": "https://arxiv.org/pdf/2505.15245",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.08468",
      "title": "mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data",
      "authors": [
        "Haonan Chen",
        "Liang Wang",
        "Nan Yang",
        "Yutao Zhu",
        "Ziliang Zhao",
        "Furu Wei",
        "Zhicheng Dou"
      ],
      "abstract": "Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in https://github.com/haon-chen/mmE5.",
      "arxiv_url": "https://arxiv.org/abs/2502.08468",
      "pdf_url": "https://arxiv.org/pdf/2502.08468",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14613",
      "title": "Behavioral Analysis of Information Salience in Large Language Models",
      "authors": [
        "Jan Trienes",
        "Jörg Schlötterer",
        "Junyi Jessy Li",
        "Christin Seifert"
      ],
      "abstract": "Large Language Models (LLMs) excel at text summarization, a task that requires models to select content based on its importance. However, the exact notion of salience that LLMs have internalized remains unclear. To bridge this gap, we introduce an explainable framework to systematically derive and investigate information salience in LLMs through their summarization behavior. Using length-controlled summarization as a behavioral probe into the content selection process, and tracing the answerability of Questions Under Discussion throughout, we derive a proxy for how models prioritize information. Our experiments on 13 models across four datasets reveal that LLMs have a nuanced, hierarchical notion of salience, generally consistent across model families and sizes. While models show highly consistent behavior and hence salience patterns, this notion of salience cannot be accessed through introspection, and only weakly correlates with human perceptions of information salience.",
      "arxiv_url": "https://arxiv.org/abs/2502.14613",
      "pdf_url": "https://arxiv.org/pdf/2502.14613",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "db59e4aad2fa34cdd39af61f70d17e2501a3ca03",
      "title": "FOCUS: Evaluating Pre-trained Vision-Language Models on Underspecification Reasoning",
      "authors": [
        "Kankan Zhou",
        "Eason Lai",
        "K. Mouratidis",
        "Jing Jiang"
      ],
      "abstract": "Humans possess a remarkable ability to interpret underspecified ambiguous statements by inferring their meanings from contexts such as visual inputs. This ability, however, may not be as developed in recent pre-trained vision-language models (VLMs). In this paper, we introduce a novel probing dataset called FO-CUS to evaluate whether state-of-the-art VLMs have this ability. FOCUS consists of underspecified sentences paired with image contexts and carefully designed probing questions. Our experiments reveal that VLMs still fall short in handling underspecification even when visual inputs that can help resolve the ambiguities are available. To further support research in un-derspecification, FOCUS will be released for public use. We hope this dataset will inspire further research on the reasoning and contextual understanding capabilities of VLMs.",
      "arxiv_url": "https://www.semanticscholar.org/paper/db59e4aad2fa34cdd39af61f70d17e2501a3ca03",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.07459",
      "title": "Why Uncertainty Estimation Methods Fall Short in RAG: An Axiomatic Analysis",
      "authors": [
        "Heydar Soudani",
        "Evangelos Kanoulas",
        "Faegheh Hasibi"
      ],
      "abstract": "Large Language Models (LLMs) are valued for their strong performance across various tasks, but they also produce inaccurate or misleading outputs. Uncertainty Estimation (UE) quantifies the model's confidence and helps users assess response reliability. However, existing UE methods have not been thoroughly examined in scenarios like Retrieval-Augmented Generation (RAG), where the input prompt includes non-parametric knowledge. This paper shows that current UE methods cannot reliably assess correctness in the RAG setting. We further propose an axiomatic framework to identify deficiencies in existing methods and guide the development of improved approaches. Our framework introduces five constraints that an effective UE method should meet after incorporating retrieved documents into the LLM's prompt. Experimental results reveal that no existing UE method fully satisfies all the axioms, explaining their suboptimal performance in RAG. We further introduce a simple yet effective calibration function based on our framework, which not only satisfies more axioms than baseline methods but also improves the correlation between uncertainty estimates and correctness.",
      "arxiv_url": "https://arxiv.org/abs/2505.07459",
      "pdf_url": "https://arxiv.org/pdf/2505.07459",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-05-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11958",
      "title": "Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning",
      "authors": [
        "Aswini Kumar",
        "Anil Bandhakavi",
        "Tanmoy Chakraborty"
      ],
      "abstract": "Counterspeech has proven to be a powerful tool to combat hate speech online. Previous studies have focused on generating counterspeech conditioned only on specific intents (single attributed). However, a holistic approach considering multiple attributes simultaneously can yield more nuanced and effective responses. Here, we introduce HiPPrO, Hierarchical Prefix learning with Preference Optimization, a novel two-stage framework that utilizes the effectiveness of attribute-specific prefix embedding spaces hierarchically optimized during the counterspeech generation process in the first phase. Thereafter, we incorporate both reference and reward-free preference optimization to generate more constructive counterspeech. Furthermore, we extend IntentCONANv2 by annotating all 13,973 counterspeech instances with emotion labels by five annotators. HiPPrO leverages hierarchical prefix optimization to integrate these dual attributes effectively. An extensive evaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent conformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L, respectively, compared to several baseline models. Human evaluations further substantiate the superiority of our approach, highlighting the enhanced relevance and appropriateness of the generated counterspeech. This work underscores the potential of multi-attribute conditioning in advancing the efficacy of counterspeech generation systems. Our code is available on Github and dataset is open-sourced on Hugging-face.",
      "arxiv_url": "https://arxiv.org/abs/2505.11958",
      "pdf_url": "https://arxiv.org/pdf/2505.11958",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "dc0c56a13a194e4fac34e818f42e77fd2f592009",
      "title": "FlashBack: Efficient Retrieval-Augmented Language Modeling for Fast Inference",
      "authors": [
        "Runheng Liu",
        "Xingchen Xiao",
        "Heyan Huang",
        "Zewen Chi",
        "Zhijing Wu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/dc0c56a13a194e4fac34e818f42e77fd2f592009",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.04455",
      "title": "GradOT: Training-free Gradient-preserving Offsite-tuning for Large Language Models",
      "authors": [
        "Kai Yao",
        "Zhaorui Tan",
        "Penglei Gao",
        "Lichun Li",
        "Kaixin Wu",
        "Yinggui Wang",
        "Yuan-yuan Zhao",
        "Yixin Ji",
        "Jianke Zhu",
        "Wei Wang"
      ],
      "abstract": "The rapid growth of large language models (LLMs) with traditional centralized fine-tuning emerges as a key technique for adapting these models to domain-specific challenges, yielding privacy risks for both model and data owners. One promising solution, called offsite-tuning (OT), is proposed to address these challenges, where a weaker emulator is compressed from the original model and further fine-tuned with adapter to enhance privacy. However, the existing OT-based methods require high computational costs and lack theoretical analysis. This paper introduces a novel OT approach based on gradient-preserving compression, named GradOT. By analyzing the OT problem through the lens of optimization, we propose a method that selectively applies compression techniques such as rank compression and channel pruning, preserving the gradients of fine-tuned adapters while ensuring privacy. Extensive experiments demonstrate that our approach surpasses existing OT methods, both in terms of privacy protection and model performance. Our method provides a theoretical foundation for OT and offers a practical, training-free solution for offsite-tuning of large-scale LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2507.04455",
      "pdf_url": "https://arxiv.org/pdf/2507.04455",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06842",
      "title": "PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation",
      "authors": [
        "Arkadiusz Modzelewski",
        "Witold Sosnowski",
        "Tiziano Labruna",
        "Adam Wierzbicki",
        "Giovanni Da San Martino"
      ],
      "abstract": "Disinformation detection is a key aspect of media literacy. Psychological studies have shown that knowledge of persuasive fallacies helps individuals detect disinformation. Inspired by these findings, we experimented with large language models (LLMs) to test whether infusing persuasion knowledge enhances disinformation detection. As a result, we introduce the Persuasion-Augmented Chain of Thought (PCoT), a novel approach that leverages persuasion to improve disinformation detection in zero-shot classification. We extensively evaluate PCoT on online news and social media posts. Moreover, we publish two novel, up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets enable the evaluation of PCoT on content entirely unseen by the LLMs used in our experiments, as the content was published after the models'knowledge cutoffs. We show that, on average, PCoT outperforms competitive methods by 15% across five LLMs and five datasets. These findings highlight the value of persuasion in strengthening zero-shot disinformation detection.",
      "arxiv_url": "https://arxiv.org/abs/2506.06842",
      "pdf_url": "https://arxiv.org/pdf/2506.06842",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.16994",
      "title": "FADE: Why Bad Descriptions Happen to Good Features",
      "authors": [
        "Bruno Puri",
        "Aakriti Jain",
        "Elena Golimblevskaia",
        "Patrick Kahardipraja",
        "Thomas Wiegand",
        "Wojciech Samek",
        "S. Lapuschkin"
      ],
      "abstract": "Recent advances in mechanistic interpretability have highlighted the potential of automating interpretability pipelines in analyzing the latent representations within LLMs. While this may enhance our understanding of internal mechanisms, the field lacks standardized evaluation methods for assessing the validity of discovered features. We attempt to bridge this gap by introducing FADE: Feature Alignment to Description Evaluation, a scalable model-agnostic framework for automatically evaluating feature-to-description alignment. FADE evaluates alignment across four key metrics - Clarity, Responsiveness, Purity, and Faithfulness - and systematically quantifies the causes of the misalignment between features and their descriptions. We apply FADE to analyze existing open-source feature descriptions and assess key components of automated interpretability pipelines, aiming to enhance the quality of descriptions. Our findings highlight fundamental challenges in generating feature descriptions, particularly for SAEs compared to MLP neurons, providing insights into the limitations and future directions of automated interpretability. We release FADE as an open-source package at: https://github.com/brunibrun/FADE",
      "arxiv_url": "https://arxiv.org/abs/2502.16994",
      "pdf_url": "https://arxiv.org/pdf/2502.16994",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14020",
      "title": "Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning",
      "authors": [
        "Hao Dong",
        "Ziyue Qiao",
        "Zhiyuan Ning",
        "Qi Hao",
        "Yi Du",
        "Pengyang Wang",
        "Yuanchun Zhou"
      ],
      "abstract": "Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs (KGs), incorporate the temporal feature to express the transience of knowledge by describing when facts occur. TKG extrapolation aims to infer possible future facts based on known history, which has garnered significant attention in recent years. Some existing methods treat TKG as a sequence of independent subgraphs to model temporal evolution patterns, demonstrating impressive reasoning performance. However, they still have limitations: 1) In modeling subgraph semantic evolution, they usually neglect the internal structural interactions between subgraphs, which are actually crucial for encoding TKGs. 2) They overlook the potential smooth features that do not lead to semantic changes, which should be distinguished from the semantic evolution process. Therefore, we propose a novel Disentangled Multi-span Evolutionary Network (DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution strategy that captures local neighbor features while perceiving historical neighbor semantic information, thus enabling internal interactions between subgraphs during the evolution process. To maximize the capture of semantic change patterns, we design a disentangle component that adaptively separates nodes' active and stable features, used to dynamically control the influence of historical semantics on future evolution. Extensive experiments conducted on four real-world TKG datasets show that DiMNet demonstrates substantial performance in TKG reasoning, and outperforms the state-of-the-art up to 22.7% in MRR.",
      "arxiv_url": "https://arxiv.org/abs/2505.14020",
      "pdf_url": "https://arxiv.org/pdf/2505.14020",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.01821",
      "title": "SDPO: Segment-Level Direct Preference Optimization for Social Agents",
      "authors": [
        "Aobo Kong",
        "Wen-Cheng Ma",
        "Shiwan Zhao",
        "Yongbin Li",
        "Yuchuan Wu",
        "Ke Wang",
        "Xiaoqian Liu",
        "Qicheng Li",
        "Yong Qin",
        "Fei Huang"
      ],
      "abstract": "Social agents powered by large language models (LLMs) can simulate human social behaviors but fall short in handling complex social dialogues. Direct Preference Optimization (DPO) has proven effective in aligning LLM behavior with human preferences across various agent tasks. However, standard DPO focuses solely on individual turns, which limits its effectiveness in multi-turn social interactions. Several DPO-based multi-turn alignment methods with session-level data have shown potential in addressing this problem.While these methods consider multiple turns across entire sessions, they are often overly coarse-grained, introducing training noise, and lack robust theoretical support. To resolve these limitations, we propose Segment-Level Direct Preference Optimization (SDPO), which dynamically select key segments within interactions to optimize multi-turn agent behavior. SDPO minimizes training noise and is grounded in a rigorous theoretical framework. Evaluations on the SOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform both existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring SDPO's potential to advance the social intelligence of LLM-based agents. We release our code and data at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.",
      "arxiv_url": "https://arxiv.org/abs/2501.01821",
      "pdf_url": "https://arxiv.org/pdf/2501.01821",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "dce265e15f6a393d8c48e472c59ff96b85535ef1",
      "title": "HyperFM: Fact-Centric Multimodal Fusion for Link Prediction over Hyper-Relational Knowledge Graphs",
      "authors": [
        "Yuhuan Lu",
        "Weijian Yu",
        "Xin Jing",
        "Dingqi Yang"
      ],
      "abstract": "With the ubiquity of hyper-relational facts in modern Knowledge Graphs (KGs), existing link prediction techniques mostly focus on learning the sophisticated relationships among multiple entities and relations contained in a fact, while ignoring the multimodal information, which often provides additional clues to boost link prediction performance. Neverthe-less, traditional multimodal fusion approaches, which are mainly designed for triple facts under either entity-centric or relation-guided fusion schemes, fail to integrate the multimodal information with the rich context of the hyper-relational fact consisting of multiple entities and relations. Against this background, we propose HyperFM , a Hyper -relational F act-centric M ultimodal Fusion technique. It effectively captures the intricate interactions be-tween different data modalities while accommodating the hyper-relational structure of the KG in a fact-centric manner via a customized Hypergraph Transformer. We evaluate Hy-perFM against a sizeable collection of base-lines in link prediction tasks on two real-world KG datasets. The results show that HyperFM consistently achieves the best performance, yielding an average improvement of 6.0-6.8% over the best-performing baselines on the two datasets. Moreover, a series of ablation studies systematically validate our fact-centric fusion scheme.",
      "arxiv_url": "https://www.semanticscholar.org/paper/dce265e15f6a393d8c48e472c59ff96b85535ef1",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22280",
      "title": "Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review",
      "authors": [
        "Zihan Xu",
        "Haotian Ma",
        "Gongbo Zhang",
        "Yihao Ding",
        "Chunhua Weng",
        "Yifan Peng"
      ],
      "abstract": "Evidence-based medicine (EBM) is at the forefront of modern healthcare, emphasizing the use of the best available scientific evidence to guide clinical decisions. Due to the sheer volume and rapid growth of medical literature and the high cost of curation, there is a critical need to investigate Natural Language Processing (NLP) methods to identify, appraise, synthesize, summarize, and disseminate evidence in EBM. This survey presents an in-depth review of 129 research studies on leveraging NLP for EBM, illustrating its pivotal role in enhancing clinical decision-making processes. The paper systematically explores how NLP supports the five fundamental steps of EBM - Ask, Acquire, Appraise, Apply, and Assess. The review not only identifies current limitations within the field but also proposes directions for future research, emphasizing the potential for NLP to revolutionize EBM by refining evidence extraction, evidence synthesis, appraisal, summarization, enhancing data comprehensibility, and facilitating a more efficient clinical workflow.",
      "arxiv_url": "https://arxiv.org/abs/2505.22280",
      "pdf_url": "https://arxiv.org/pdf/2505.22280",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22801",
      "title": "Towards a More Generalized Approach in Open Relation Extraction",
      "authors": [
        "Qing Wang",
        "Yuepei Li",
        "Qiao Qiao",
        "Kang Zhou",
        "Qi Li"
      ],
      "abstract": "Open Relation Extraction (OpenRE) seeks to identify and extract novel relational facts between named entities from unlabeled data without pre-defined relation schemas. Traditional OpenRE methods typically assume that the unlabeled data consists solely of novel relations or is pre-divided into known and novel instances. However, in real-world scenarios, novel relations are arbitrarily distributed. In this paper, we propose a generalized OpenRE setting that considers unlabeled data as a mixture of both known and novel instances. To address this, we propose MixORE, a two-phase framework that integrates relation classification and clustering to jointly learn known and novel relations. Experiments on three benchmark datasets demonstrate that MixORE consistently outperforms competitive baselines in known relation classification and novel relation clustering. Our findings contribute to the advancement of generalized OpenRE research and real-world applications.",
      "arxiv_url": "https://arxiv.org/abs/2505.22801",
      "pdf_url": "https://arxiv.org/pdf/2505.22801",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "dd239aca26c4aca1d71352ae94ed0de55bbee818",
      "title": "Multilingual Text-to-Image Generation Magnifies Gender Stereotypes",
      "authors": [
        "Felix Friedrich",
        "Katharina Hämmerl",
        "P. Schramowski",
        "Manuel Brack",
        "Jindrich Libovický",
        "Alexander Fraser",
        "K. Kersting"
      ],
      "abstract": "Text-to-image (T2I) generation models have achieved great results in image quality, flexibility, and text alignment, leading to widespread use. Through improvements in multilingual abilities, a larger community can access this technology. Yet, we show that multilingual models suffer from substantial gender bias. Furthermore, the expectation that results should be similar across languages does not hold. We introduce MAGBIG , a controlled benchmark designed to study gender bias in multilingual T2I models, and use it to assess the impact of multi-lingualism on gender bias. To this end, we construct a set of multilingual prompts that offers a carefully controlled setting accounting for the complex grammatical differences influencing gender across languages. Our re-sults show strong gender biases and notable language-specific differences across models. While we explore prompt engineering strategies to mitigate these biases, we find them largely ineffective and sometimes even detrimental to text-to-image alignment. Our analysis highlights the need for research on diverse language representations and greater control over bias in T2I models. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/dd239aca26c4aca1d71352ae94ed0de55bbee818",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12552",
      "title": "Profiling News Media for Factuality and Bias Using LLMs and the Fact-Checking Methodology of Human Experts",
      "authors": [
        "Zain Muhammad Mujahid",
        "Dilshod Azizov",
        "Maha Tufail Agro",
        "Preslav Nakov"
      ],
      "abstract": "In an age characterized by the proliferation of mis- and disinformation online, it is critical to empower readers to understand the content they are reading. Important efforts in this direction rely on manual or automatic fact-checking, which can be challenging for emerging claims with limited information. Such scenarios can be handled by assessing the reliability and the political bias of the source of the claim, i.e., characterizing entire news outlets rather than individual claims or articles. This is an important but understudied research direction. While prior work has looked into linguistic and social contexts, we do not analyze individual articles or information in social media. Instead, we propose a novel methodology that emulates the criteria that professional fact-checkers use to assess the factuality and political bias of an entire outlet. Specifically, we design a variety of prompts based on these criteria and elicit responses from large language models (LLMs), which we aggregate to make predictions. In addition to demonstrating sizable improvements over strong baselines via extensive experiments with multiple LLMs, we provide an in-depth error analysis of the effect of media popularity and region on model performance. Further, we conduct an ablation study to highlight the key components of our dataset that contribute to these improvements. To facilitate future research, we released our dataset and code at https://github.com/mbzuai-nlp/llm-media-profiling.",
      "arxiv_url": "https://arxiv.org/abs/2506.12552",
      "pdf_url": "https://arxiv.org/pdf/2506.12552",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20658",
      "title": "Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge",
      "authors": [
        "Yue Fang",
        "Zhi Jin",
        "Jie An",
        "Hongshen Chen",
        "Xiaohong Chen",
        "Naijun Zhan"
      ],
      "abstract": "Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise formal specification, making it widely used in cyber-physical systems such as autonomous driving and robotics. Automatically transforming NL into STL is an attractive approach to overcome the limitations of manual transformation, which is time-consuming and error-prone. However, due to the lack of datasets, automatic transformation currently faces significant challenges and has not been fully explored. In this paper, we propose an NL-STL dataset named STL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched with diverse patterns. To develop the dataset, we first manually create a small-scale seed set of NL-STL pairs. Next, representative examples are identified through clustering and used to guide large language models (LLMs) in generating additional NL-STL pairs. Finally, diversity and accuracy are ensured through rigorous rule-based filters and human validation. Furthermore, we introduce the Knowledge-Guided STL Transformation (KGST) framework, a novel approach for transforming natural language into STL, involving a generate-then-refine process based on external knowledge. Statistical analysis shows that the STL-DivEn dataset exhibits more diversity than the existing NL-STL dataset. Moreover, both metric-based and human evaluations indicate that our KGST approach outperforms baseline models in transformation accuracy on STL-DivEn and DeepSTL datasets.",
      "arxiv_url": "https://arxiv.org/abs/2505.20658",
      "pdf_url": "https://arxiv.org/pdf/2505.20658",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11533",
      "title": "Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of Stealing Privacy",
      "authors": [
        "Zhenyuan Guo",
        "Yi Shi",
        "Wenlong Meng",
        "Chen Gong",
        "Chengkun Wei",
        "Wenzhi Chen"
      ],
      "abstract": "Model merging is a widespread technology in large language models (LLMs) that integrates multiple task-specific LLMs into a unified one, enabling the merged model to inherit the specialized capabilities of these LLMs. Most task-specific LLMs are sourced from open-source communities and have not undergone rigorous auditing, potentially imposing risks in model merging. This paper highlights an overlooked privacy risk: \\textit{an unsafe model could compromise the privacy of other LLMs involved in the model merging.} Specifically, we propose PhiMM, a privacy attack approach that trains a phishing model capable of stealing privacy using a crafted privacy phishing instruction dataset. Furthermore, we introduce a novel model cloaking method that mimics a specialized capability to conceal attack intent, luring users into merging the phishing model. Once victims merge the phishing model, the attacker can extract personally identifiable information (PII) or infer membership information (MI) by querying the merged model with the phishing instruction. Experimental results show that merging a phishing model increases the risk of privacy breaches. Compared to the results before merging, PII leakage increased by 3.9\\% and MI leakage increased by 17.4\\% on average. We release the code of PhiMM through a link.",
      "arxiv_url": "https://arxiv.org/abs/2502.11533",
      "pdf_url": "https://arxiv.org/pdf/2502.11533",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "dd461768ce65e59d56778d07d6459a09bc086408",
      "title": "Visual Evidence Prompting Mitigates Hallucinations in Large Vision-Language Models",
      "authors": [
        "Wei Li",
        "Zhen Huang",
        "Houqiang Li",
        "Le Lu",
        "Yang Lu",
        "Xinmei Tian",
        "Xu Shen",
        "Jieping Ye"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/dd461768ce65e59d56778d07d6459a09bc086408",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12562",
      "title": "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings",
      "authors": [
        "Weikai Lu",
        "Hao Peng",
        "Huiping Zhuang",
        "Cen Chen",
        "Ziqian Zeng"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have serious security vulnerabilities.While safety alignment using multimodal datasets consisting of text and data of additional modalities can effectively enhance MLLM's security, it is costly to construct these datasets. Existing low-resource security alignment methods, including textual alignment, have been found to struggle with the security risks posed by additional modalities. To address this, we propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes embeddings of additional modality through gradient updates to expand textual datasets. This enables multimodal safety alignment training even when only textual data is available. Extensive experiments on image, video, and audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding on a single RTX3090 GPU within 24 seconds. SEA significantly improves the security of MLLMs when faced with threats from additional modalities. To assess the security risks introduced by video and audio, we also introduced a new benchmark called VA-SafetyBench. High attack success rates across multiple MLLMs validate its challenge. Our code and data will be available at https://github.com/ZeroNLP/SEA.",
      "arxiv_url": "https://arxiv.org/abs/2502.12562",
      "pdf_url": "https://arxiv.org/pdf/2502.12562",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11887",
      "title": "AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation",
      "authors": [
        "Xiechi Zhang",
        "Zetian Ouyang",
        "Linlin Wang",
        "Gerard de Melo",
        "Zhu Cao",
        "Xiaoling Wang",
        "Ya Zhang",
        "Yanfeng Wang",
        "Liang He"
      ],
      "abstract": "With the proliferation of large language models (LLMs) in the medical domain, there is increasing demand for improved evaluation techniques to assess their capabilities. However, traditional metrics like F1 and ROUGE, which rely on token overlaps to measure quality, significantly overlook the importance of medical terminology. While human evaluation tends to be more reliable, it can be very costly and may as well suffer from inaccuracies due to limits in human expertise and motivation. Although there are some evaluation methods based on LLMs, their usability in the medical field is limited due to their proprietary nature or lack of expertise. To tackle these challenges, we present AutoMedEval, an open-sourced automatic evaluation model with 13B parameters specifically engineered to measure the question-answering proficiency of medical LLMs. The overarching objective of AutoMedEval is to assess the quality of responses produced by diverse models, aspiring to significantly reduce the dependence on human evaluation. Specifically, we propose a hierarchical training method involving curriculum instruction tuning and an iterative knowledge introspection mechanism, enabling AutoMedEval to acquire professional medical assessment capabilities with limited instructional data. Human evaluations indicate that AutoMedEval surpasses other baselines in terms of correlation with human judgments.",
      "arxiv_url": "https://arxiv.org/abs/2505.11887",
      "pdf_url": "https://arxiv.org/pdf/2505.11887",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ddc8b3743d0100ac2a6ee9a3e66472d1720c3c80",
      "title": "Efficient Domain Continual pretraining by Mitigating the Stability Gap",
      "authors": [
        "Yiduo Guo",
        "Jie Fu",
        "Huishuai Zhang",
        "Dongyan Zhao"
      ],
      "abstract": "Continual pretraining enables Large Language Models (LLMs) to adapt to specialized domains like medicine and law. However, we observe a consistent phenomenon across different model sizes and domains: a temporary performance drop at the start of the continual pretraining process, followed by a performance recovery phase. To gain a deeper understanding of this issue, we use the stability gap— a concept adapted from the visual domain—which explains this initial drop arises from instability in the model’s general abilities. We validate this hypothesis through a series of experiments. To address this initial instability and enhance LLM performance within a fixed compute bud-get, we propose a training strategy that mitigates instability by increasing the number of epochs, alongside two data sampling strategies targeting data domain relevance and corpus distribution. We conduct experiments on Llama-family models to validate the effectiveness of our strategies for continual pretraining and instruction tuning in medical and legal domains. Our strategies improve the average medical task performance of the OpenLlama-3B model from 36.2% to 40.7% using only 40% of the original training budget, while also enhancing general task performance without causing forgetting. Furthermore, we apply our strategies to continually pre-train and instruction-tune the Llama-3-8B model. The resulting model, Llama-3-Physician 1 , achieves the best medical performance among open-source models and rivals GPT-4 on specific tasks.",
      "arxiv_url": "https://www.semanticscholar.org/paper/ddc8b3743d0100ac2a6ee9a3e66472d1720c3c80",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13458",
      "title": "ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails",
      "authors": [
        "Xiaofei Wen",
        "Wenxuan Zhou",
        "W. Mo",
        "Muhao Chen"
      ],
      "abstract": "Ensuring the safety of large language models (LLMs) is critical as they are deployed in real-world applications. Existing guardrails rely on rule-based filtering or single-pass classification, limiting their ability to handle nuanced safety violations. To address this, we propose ThinkGuard, a critique-augmented guardrail model that distills knowledge from high-capacity LLMs by generating structured critiques alongside safety labels. Fine-tuned on critique-augmented data, the captured deliberative thinking ability drastically enhances the guardrail's cautiousness and interpretability. Evaluated on multiple safety benchmarks, ThinkGuard achieves the highest average F1 and AUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard improves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses label-only fine-tuned models, confirming that structured critiques enhance both classification precision and nuanced safety reasoning while maintaining computational efficiency.",
      "arxiv_url": "https://arxiv.org/abs/2502.13458",
      "pdf_url": "https://arxiv.org/pdf/2502.13458",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.19011",
      "title": "PKAG-DDI: Pairwise Knowledge-Augmented Language Model for Drug-Drug Interaction Event Text Generation",
      "authors": [
        "Ziyan Wang",
        "Zhankun Xiong",
        "Feng Huang",
        "Wen Zhang"
      ],
      "abstract": "Drug-drug interactions (DDIs) arise when multiple drugs are administered concurrently. Accurately predicting the specific mechanisms underlying DDIs (named DDI events or DDIEs) is critical for the safe clinical use of drugs. DDIEs are typically represented as textual descriptions. However, most computational methods focus more on predicting the DDIE class label over generating human-readable natural language increasing clinicians'interpretation costs. Furthermore, current methods overlook the fact that each drug assumes distinct biological functions in a DDI, which, when used as input context, can enhance the understanding of the DDIE process and benefit DDIE generation by the language model (LM). In this work, we propose a novel pairwise knowledge-augmented generative method (termed PKAG-DDI) for DDIE text generation. It consists of a pairwise knowledge selector efficiently injecting structural information between drugs bidirectionally and simultaneously to select pairwise biological functions from the knowledge set, and a pairwise knowledge integration strategy that matches and integrates the selected biological functions into the LM. Experiments on two professional datasets show that PKAG-DDI outperforms existing methods in DDIE text generation, especially in challenging inductive scenarios, indicating its practicality and generalization.",
      "arxiv_url": "https://arxiv.org/abs/2507.19011",
      "pdf_url": "https://arxiv.org/pdf/2507.19011",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.15225",
      "title": "SEAL: Scaling to Emphasize Attention for Long-Context Retrieval",
      "authors": [
        "Changhun Lee",
        "Jun-gyu Jin",
        "Younghyun Cho",
        "Eunhyeok Park"
      ],
      "abstract": "While many advanced LLMs are designed to handle long sequence data, we can still observe notable quality degradation even within the sequence limit. In this work, we introduce a novel approach called Scaling to Emphasize Attention for Long-context retrieval (SEAL), which enhances the retrieval performance of large language models (LLMs) over long contexts. We observe that specific attention heads are closely tied to long-context retrieval, showing positive or negative correlation with retrieval scores, and adjusting the strength of these heads boosts the quality of LLMs in long context by a large margin. Built on this insight, we propose a learning-based mechanism that leverages generated data to emphasize these heads. By applying SEAL, we achieve significant improvements in long-context retrieval performance across various tasks and models. Additionally, when combined with existing training-free context extension techniques, SEAL extends the contextual limits of LLMs while maintaining highly reliable outputs.",
      "arxiv_url": "https://arxiv.org/abs/2501.15225",
      "pdf_url": "https://arxiv.org/pdf/2501.15225",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025-01-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.11380",
      "title": "Enhance Multimodal Consistency and Coherence for Text-Image Plan Generation",
      "authors": [
        "Xiaoxin Lu",
        "Ranran Haoran Zhang",
        "Yusen Zhang",
        "Rui Zhang"
      ],
      "abstract": "People get informed of a daily task plan through diverse media involving both texts and images. However, most prior research only focuses on LLM's capability of textual plan generation. The potential of large-scale models in providing text-image plans remains understudied. Generating high-quality text-image plans faces two main challenges: ensuring consistent alignment between two modalities and keeping coherence among visual steps. To address these challenges, we propose a novel framework that generates and refines text-image plans step-by-step. At each iteration, our framework (1) drafts the next textual step based on the prediction history; (2) edits the last visual step to obtain the next one; (3) extracts PDDL-like visual information; and (4) refines the draft with the extracted visual information. The textual and visual step produced in stage (4) and (2) will then serve as inputs for the next iteration. Our approach offers a plug-and-play improvement to various backbone models, such as Mistral-7B, Gemini-1.5, and GPT-4o. To evaluate the effectiveness of our approach, we collect a new benchmark consisting of 1,100 tasks and their text-image pair solutions covering 11 daily topics. We also design and validate a new set of metrics to evaluate the multimodal consistency and coherence in text-image plans. Extensive experiment results show the effectiveness of our approach on a range of backbone models against competitive baselines. Our code and data are available at https://github.com/psunlpgroup/MPlanner.",
      "arxiv_url": "https://arxiv.org/abs/2506.11380",
      "pdf_url": "https://arxiv.org/pdf/2506.11380",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00519",
      "title": "CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention",
      "authors": [
        "Yuxi Sun",
        "Aoqi Zuo",
        "Wei Gao",
        "Jing Ma"
      ],
      "abstract": "Large Language Models (LLMs) often exhibit knowledge disparities across languages. Encouraging LLMs to \\textit{abstain} when faced with knowledge gaps is a promising strategy to reduce hallucinations in multilingual settings. Current abstention strategies for multilingual scenarios primarily rely on generating feedback in various languages using LLMs and performing self-reflection. However, these methods can be adversely impacted by inaccuracies and biases in the generated feedback. To address this, from a causal perspective, we introduce \\textit{CausalAbstain}, a method that helps LLMs determine whether to utilize multiple generated feedback responses and how to identify the most useful ones. Extensive experiments demonstrate that \\textit{CausalAbstain} effectively selects helpful feedback and enhances abstention decisions with interpretability in both native language (\\textsc{Casual-native}) and multilingual (\\textsc{Causal-multi}) settings, outperforming strong baselines on two benchmark datasets covering encyclopedic and commonsense knowledge QA tasks. Our code and data are open-sourced at https://github.com/peachch/CausalAbstain.",
      "arxiv_url": "https://arxiv.org/abs/2506.00519",
      "pdf_url": "https://arxiv.org/pdf/2506.00519",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.19988",
      "title": "ExCoT: Optimizing Reasoning for Text-to-SQL with Execution Feedback",
      "authors": [
        "Bohan Zhai",
        "Canwen Xu",
        "Yuxiong He",
        "Zhewei Yao"
      ],
      "abstract": "Text-to-SQL demands precise reasoning to convert natural language questions into structured queries. While large language models (LLMs) excel in many reasoning tasks, their ability to leverage Chain-of-Thought (CoT) reasoning for text-to-SQL remains underexplored. We identify critical limitations: zero-shot CoT offers minimal gains, and Direct Preference Optimization (DPO) applied without CoT yields marginal improvements. We propose ExCoT, a novel framework that iteratively optimizes open-source LLMs by combining CoT reasoning with off-policy and on-policy DPO, relying solely on execution accuracy as feedback. This approach eliminates the need for reward models or human-annotated preferences. Our experimental results demonstrate significant performance gains: ExCoT improves execution accuracy on BIRD dev set from 57.37% to 68.51% and on Spider test set from 78.81% to 86.59% for LLaMA-3 70B, with Qwen-2.5-Coder demonstrating similar improvements. Our best model achieves state-of-the-art performance in the single-model setting on both BIRD and Spider datasets, notably achieving 68.53% on the BIRD test set.",
      "arxiv_url": "https://arxiv.org/abs/2503.19988",
      "pdf_url": "https://arxiv.org/pdf/2503.19988",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19328",
      "title": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems",
      "authors": [
        "Hao Peng",
        "Yunjia Qi",
        "Xiaozhi Wang",
        "Zijun Yao",
        "Bin Xu",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects to provide reliable rewards. We empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards. We conduct comprehensive experiments on existing reward model benchmarks and inference time best-of-n searches on real-world downstream tasks. RewardAgent significantly outperforms vanilla reward models, demonstrating its effectiveness. We further construct training preference pairs using RewardAgent and train an LLM with the DPO objective, achieving superior performance on various NLP benchmarks compared to conventional reward models. Our codes are publicly released to facilitate further research (https://github.com/THU-KEG/Agentic-Reward-Modeling).",
      "arxiv_url": "https://arxiv.org/abs/2502.19328",
      "pdf_url": "https://arxiv.org/pdf/2502.19328",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11476",
      "title": "FastMCTS: A Simple Sampling Strategy for Data Synthesis",
      "authors": [
        "Peiji Li",
        "Kai Lv",
        "Yunfan Shao",
        "Yichuan Ma",
        "Linyang Li",
        "Xiaoqing Zheng",
        "Xipeng Qiu",
        "Qipeng Guo"
      ],
      "abstract": "Synthetic high-quality multi-step reasoning data can significantly enhance the performance of large language models on various tasks. However, most existing methods rely on rejection sampling, which generates trajectories independently and suffers from inefficiency and imbalanced sampling across problems of varying difficulty. In this work, we introduce FastMCTS, an innovative data synthesis strategy inspired by Monte Carlo Tree Search. FastMCTS provides a more efficient sampling method for multi-step reasoning data, offering step-level evaluation signals and promoting balanced sampling across problems of different difficulty levels. Experiments on both English and Chinese reasoning datasets demonstrate that FastMCTS generates over 30\\% more correct reasoning paths compared to rejection sampling as the number of generated tokens scales up. Furthermore, under comparable synthetic data budgets, models trained on FastMCTS-generated data outperform those trained on rejection sampling data by 3.9\\% across multiple benchmarks. As a lightweight sampling strategy, FastMCTS offers a practical and efficient alternative for synthesizing high-quality reasoning data. Our code will be released soon.",
      "arxiv_url": "https://arxiv.org/abs/2502.11476",
      "pdf_url": "https://arxiv.org/pdf/2502.11476",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00514",
      "title": "Evaluating the Evaluation of Diversity in Commonsense Generation",
      "authors": [
        "Tianhui Zhang",
        "Bei Peng",
        "D. Bollegala"
      ],
      "abstract": "In commonsense generation, given a set of input concepts, a model must generate a response that is not only commonsense bearing, but also capturing multiple diverse viewpoints. Numerous evaluation metrics based on form- and content-level overlap have been proposed in prior work for evaluating the diversity of a commonsense generation model. However, it remains unclear as to which metrics are best suited for evaluating the diversity in commonsense generation. To address this gap, we conduct a systematic meta-evaluation of diversity metrics for commonsense generation. We find that form-based diversity metrics tend to consistently overestimate the diversity in sentence sets, where even randomly generated sentences are assigned overly high diversity scores. We then use an Large Language Model (LLM) to create a novel dataset annotated for the diversity of sentences generated for a commonsense generation task, and use it to conduct a meta-evaluation of the existing diversity evaluation metrics. Our experimental results show that content-based diversity evaluation metrics consistently outperform the form-based counterparts, showing high correlations with the LLM-based ratings. We recommend that future work on commonsense generation should use content-based metrics for evaluating the diversity of their outputs.",
      "arxiv_url": "https://arxiv.org/abs/2506.00514",
      "pdf_url": "https://arxiv.org/pdf/2506.00514",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.02174",
      "title": "Adversarial Tokenization",
      "authors": [
        "Renato Lui Geh",
        "Zilei Shao",
        "Guy Van den Broeck"
      ],
      "abstract": "Current LLM pipelines account for only one possible tokenization for a given string, ignoring exponentially many alternative tokenizations during training and inference. For example, the standard Llama3 tokenization of penguin is [p,enguin], yet [peng,uin] is another perfectly valid alternative. In this paper, we show that despite LLMs being trained solely on one tokenization, they still retain semantic understanding of other tokenizations, raising questions about their implications in LLM safety. Put succinctly, we answer the following question: can we adversarially tokenize an obviously malicious string to evade safety and alignment restrictions? We show that not only is adversarial tokenization an effective yet previously neglected axis of attack, but it is also competitive against existing state-of-the-art adversarial approaches without changing the text of the harmful request. We empirically validate this exploit across three state-of-the-art LLMs and adversarial datasets, revealing a previously unknown vulnerability in subword models.",
      "arxiv_url": "https://arxiv.org/abs/2503.02174",
      "pdf_url": "https://arxiv.org/pdf/2503.02174",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.02444",
      "title": "Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models",
      "authors": [
        "Haoran Ye",
        "Tianze Zhang",
        "Yuhang Xie",
        "Liyuan Zhang",
        "Yuanyi Ren",
        "Xin Zhang",
        "Guojie Song"
      ],
      "abstract": "Values are core drivers of individual and collective perception, cognition, and behavior. Value systems, such as Schwartz's Theory of Basic Human Values, delineate the hierarchy and interplay among these values, enabling cross-disciplinary investigations into decision-making and societal dynamics. Recently, the rise of Large Language Models (LLMs) has raised concerns regarding their elusive intrinsic values. Despite growing efforts in evaluating, understanding, and aligning LLM values, a psychologically grounded LLM value system remains underexplored. This study addresses the gap by introducing the Generative Psycho-Lexical Approach (GPLA), a scalable, adaptable, and theoretically informed method for constructing value systems. Leveraging GPLA, we propose a psychologically grounded five-factor value system tailored for LLMs. For systematic validation, we present three benchmarking tasks that integrate psychological principles with cutting-edge AI priorities. Our results reveal that the proposed value system meets standard psychological criteria, better captures LLM values, improves LLM safety prediction, and enhances LLM alignment, when compared to the canonical Schwartz's values.",
      "arxiv_url": "https://arxiv.org/abs/2502.02444",
      "pdf_url": "https://arxiv.org/pdf/2502.02444",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14629",
      "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models",
      "authors": [
        "Fnu Mohbat",
        "Mohammed J. Zaki"
      ],
      "abstract": "Recent advances in large language models (LLMs) and the abundance of food data have resulted in studies to improve food understanding using LLMs. Despite several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there has been limited research on integrating food related KGs with LLMs. We introduce KERL, a unified system that leverages food KGs and LLMs to provide personalized food recommendations and generates recipes with associated micro-nutritional information. Given a natural language question, KERL extracts entities, retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints. Next, our system generates the cooking steps and nutritional information for each recipe. To evaluate our approach, we also develop a benchmark dataset by curating recipe related questions, combined with constraints and personal preferences. Through extensive experiments, we show that our proposed KG-augmented LLM significantly outperforms existing approaches, offering a complete and coherent solution for food recommendation, recipe generation, and nutritional analysis. Our code and benchmark datasets are publicly available at https://github.com/mohbattharani/KERL.",
      "arxiv_url": "https://arxiv.org/abs/2505.14629",
      "pdf_url": "https://arxiv.org/pdf/2505.14629",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "dfaa3345baa73612061ab8ae1704111da30a6590",
      "title": "PwnGPT: Automatic Exploit Generation Based on Large Language Models",
      "authors": [
        "Wanzong Peng",
        "Lin Ye",
        "Xuetao Du",
        "Hongli Zhang",
        "D. Zhan",
        "Yunting Zhang",
        "Yicheng Guo",
        "Chen Zhang"
      ],
      "abstract": ".",
      "arxiv_url": "https://www.semanticscholar.org/paper/dfaa3345baa73612061ab8ae1704111da30a6590",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "dfc3ae8c9f7ef2418500b27a7c0011a4d648c2d7",
      "title": "Multi-matrix Factorization Attention",
      "authors": [
        "Jingcheng Hu",
        "Houyi Li",
        "Yinmin Zhang",
        "Zili Wang",
        "Shuigeng Zhou",
        "Xiangyu Zhang",
        "Harry Shum"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/dfc3ae8c9f7ef2418500b27a7c0011a4d648c2d7",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "dfc4f1de92c414797c7b0157c3d5ef59472db451",
      "title": "TicTac: Time-aware Supervised Fine-tuning for Automatic Text Dating",
      "authors": [
        "Han Ren",
        "Minna Peng"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/dfc4f1de92c414797c7b0157c3d5ef59472db451",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "dfefb3a3b4aec70600ca451c193d736cd48967f8",
      "title": "A Classifier of Word-Level Variants in Witnesses of Biblical Hebrew Manuscripts",
      "authors": [
        "Iglika Nikolova-Stoupak",
        "Maxime Amblard",
        "Sophie Robert-Hayek",
        "Frédérique Rey"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/dfefb3a3b4aec70600ca451c193d736cd48967f8",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e01240e83dbd00fbe9debe1a821eaac803f86bb5",
      "title": "An Adaptive Multi-Threshold Loss and a General Framework for Collaborating Losses in Document-Level Relation Extraction",
      "authors": [
        "Huangming Xu",
        "Fu Zhang",
        "Jingwei Cheng"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/e01240e83dbd00fbe9debe1a821eaac803f86bb5",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13031",
      "title": "HPSS: Heuristic Prompting Strategy Search for LLM Evaluators",
      "authors": [
        "Bosi Wen",
        "Pei Ke",
        "Yufei Sun",
        "Cunxiang Wang",
        "Xiaotao Gu",
        "Jinfeng Zhou",
        "Jie Tang",
        "Hongning Wang",
        "Minlie Huang"
      ],
      "abstract": "Since the adoption of large language models (LLMs) for text evaluation has become increasingly prevalent in the field of natural language processing (NLP), a series of existing works attempt to optimize the prompts for LLM evaluators to improve their alignment with human judgment. However, their efforts are limited to optimizing individual factors of evaluation prompts, such as evaluation criteria or output formats, neglecting the combinatorial impact of multiple factors, which leads to insufficient optimization of the evaluation pipeline. Nevertheless, identifying well-behaved prompting strategies for adjusting multiple factors requires extensive enumeration. To this end, we comprehensively integrate 8 key factors for evaluation prompts and propose a novel automatic prompting strategy optimization method called Heuristic Prompting Strategy Search (HPSS). Inspired by the genetic algorithm, HPSS conducts an iterative search to find well-behaved prompting strategies for LLM evaluators. A heuristic function is employed to guide the search process, enhancing the performance of our algorithm. Extensive experiments across four evaluation tasks demonstrate the effectiveness of HPSS, consistently outperforming both human-designed evaluation prompts and existing automatic prompt optimization methods. Our code is available at https://github.com/thu-coai/HPSS.",
      "arxiv_url": "https://arxiv.org/abs/2502.13031",
      "pdf_url": "https://arxiv.org/pdf/2502.13031",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24731",
      "title": "Circuit Stability Characterizes Language Model Generalization",
      "authors": [
        "Alan Sun"
      ],
      "abstract": "Extensively evaluating the capabilities of (large) language models is difficult. Rapid development of state-of-the-art models induce benchmark saturation, while creating more challenging datasets is labor-intensive. Inspired by the recent developments in mechanistic interpretability, we introduce circuit stability as a new way to assess model performance. Circuit stability refers to a model's ability to apply a consistent reasoning process-its circuit-across various inputs. We mathematically formalize circuit stability and circuit equivalence. Then, through three case studies, we empirically show that circuit stability and the lack thereof can characterize and predict different aspects of generalization. Our proposed methods offer a step towards rigorously relating the generality of models to their interpretability.",
      "arxiv_url": "https://arxiv.org/abs/2505.24731",
      "pdf_url": "https://arxiv.org/pdf/2505.24731",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03616",
      "title": "Learning to Insert [PAUSE] Tokens for Better Reasoning",
      "authors": [
        "Eunki Kim",
        "Sangryul Kim",
        "James Thorne"
      ],
      "abstract": "To enhance reasoning capabilities, previous works have explored incorporating special-purpose tokens into the training process. These strategies strengthen the learning mechanism of transformer-based large language models (LLMs). Building on prior research, in which inserting dummy tokens consecutively just before reasoning steps can enhance effectiveness, we introduce a novel approach termed Dynamic Inserting Tokens Training (DIT). Our method identifies positions within sequences where model confidence is lowest according to token log-likelihood. Strategically inserting [PAUSE] tokens on these positions bolsters the model's predictive capabilities for subsequent tokens. Experimental results across diverse datasets and models, from the 2.7B model to the 8B model, demonstrate that DIT consistently outperforms traditional fine-tuning and previous token insertion methods. With this simple yet effective method, we achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on AQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets. Our work shows a model-based, dynamic approach rather than a heuristic one, thereby broadening the scope of research in reasoning.",
      "arxiv_url": "https://arxiv.org/abs/2506.03616",
      "pdf_url": "https://arxiv.org/pdf/2506.03616",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00585",
      "title": "Entriever: Energy-based Retriever for Knowledge-Grounded Dialog Systems",
      "authors": [
        "Yucheng Cai",
        "Ke Li",
        "Yi Huang",
        "Junlan Feng",
        "Zhijian Ou"
      ],
      "abstract": "A retriever, which retrieves relevant knowledge pieces from a knowledge base given a context, is an important component in many natural language processing (NLP) tasks. Retrievers have been introduced in knowledge-grounded dialog systems to improve knowledge acquisition. In knowledge-grounded dialog systems, when conditioning on a given context, there may be multiple relevant and correlated knowledge pieces. However, knowledge pieces are usually assumed to be conditionally independent in current retriever models. To address this issue, we propose Entriever, an energy-based retriever. Entriever directly models the candidate retrieval results as a whole instead of modeling the knowledge pieces separately, with the relevance score defined by an energy function. We explore various architectures of energy functions and different training methods for Entriever, and show that Entriever substantially outperforms the strong cross-encoder baseline in knowledge retrieval tasks. Furthermore, we show that in semi-supervised training of knowledge-grounded dialog systems, Entriever enables effective scoring of retrieved knowledge pieces and significantly improves end-to-end performance of dialog systems.",
      "arxiv_url": "https://arxiv.org/abs/2506.00585",
      "pdf_url": "https://arxiv.org/pdf/2506.00585",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.16644",
      "title": "Semantic Outlier Removal with Embedding Models and LLMs",
      "authors": [
        "Eren Akbiyik",
        "Joao Almeida",
        "Rik Melis",
        "Ritu Sriram",
        "Viviana Petrescu",
        "Vilhj'almur Vilhj'almsson"
      ],
      "abstract": "Modern text processing pipelines demand robust methods to remove extraneous content while preserving a document's core message. Traditional approaches such as HTML boilerplate extraction or keyword filters often fail in multilingual settings and struggle with context-sensitive nuances, whereas Large Language Models (LLMs) offer improved quality at high computational cost. We introduce SORE (Semantic Outlier Removal), a cost-effective, transparent method that leverages multilingual sentence embeddings and approximate nearest-neighbor search to identify and excise unwanted text segments. By first identifying core content via metadata embedding and then flagging segments that either closely match predefined outlier groups or deviate significantly from the core, SORE achieves near-LLM extraction precision at a fraction of the cost. Experiments on HTML datasets demonstrate that SORE outperforms structural methods and yield high precision in diverse scenarios. Our system is currently deployed in production, processing millions of documents daily across multiple languages while maintaining both efficiency and accuracy. To facilitate reproducibility and further research, we release our implementation and evaluation datasets.",
      "arxiv_url": "https://arxiv.org/abs/2506.16644",
      "pdf_url": "https://arxiv.org/pdf/2506.16644",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.06027",
      "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation",
      "authors": [
        "S. Vasilev",
        "Christian Herold",
        "Baohao Liao",
        "Seyyed Hadi Hashemi",
        "Shahram Khadivi",
        "C. Monz"
      ],
      "abstract": "This paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a critical task in compliance with data privacy regulations like GDPR. Unlike prior methods that rely on static hyperparameters or starting model outputs, Unilogit dynamically adjusts target logits to achieve a uniform probability for the target token, leveraging the current model's outputs for more accurate self-distillation targets. This approach not only eliminates the need for additional hyperparameters but also enhances the model's ability to approximate the golden targets. Extensive experiments on public benchmarks and an in-house e-commerce dataset demonstrate Unilogit's superior performance in balancing forget and retain objectives, outperforming state-of-the-art methods such as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness across various scenarios, highlighting its practical applicability and effectiveness in achieving efficacious machine unlearning.",
      "arxiv_url": "https://arxiv.org/abs/2505.06027",
      "pdf_url": "https://arxiv.org/pdf/2505.06027",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11435",
      "title": "SMART: Self-Aware Agent for Tool Overuse Mitigation",
      "authors": [
        "Cheng Qian",
        "Emre Can Acikgoz",
        "Hongru Wang",
        "Xiusi Chen",
        "Avirup Sil",
        "Dilek Hakkani-Tur",
        "Gokhan Tur",
        "Heng Ji"
      ],
      "abstract": "Current Large Language Model (LLM) agents demonstrate strong reasoning and tool use capabilities, but often lack self-awareness, failing to balance these approaches effectively. This imbalance leads to Tool Overuse, where models unnecessarily rely on external tools for tasks solvable with parametric knowledge, increasing computational overhead. Inspired by human metacognition, we introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm that enhances an agent's self-awareness to optimize task handling and reduce tool overuse. To support this paradigm, we introduce SMART-ER, a dataset spanning three domains, where reasoning alternates between parametric knowledge and tool-dependent steps, with each step enriched by rationales explaining when tools are necessary. Through supervised training, we develop SMARTAgent, a family of models that dynamically balance parametric knowledge and tool use. Evaluations show that SMARTAgent reduces tool use by 24% while improving performance by over 37%, enabling 7B-scale models to match its 70B counterpart and GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test data like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool calls. These highlight the potential of strategic tool use to enhance reasoning, mitigate overuse, and bridge the gap between model size and performance, advancing intelligent and resource-efficient agent designs.",
      "arxiv_url": "https://arxiv.org/abs/2502.11435",
      "pdf_url": "https://arxiv.org/pdf/2502.11435",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e0c5b38951f916a07a4ec49badde824af2e9be2a",
      "title": "FORG3D: Flexible Object Rendering for Generating Vision-Language Spatial Reasoning Data from 3D Scenes",
      "authors": [
        "Oscar Pang",
        "Freda Shi"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/e0c5b38951f916a07a4ec49badde824af2e9be2a",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15823",
      "title": "InductionBench: LLMs Fail in the Simplest Complexity Class",
      "authors": [
        "Wenyue Hua",
        "Tyler Wong",
        "Sun Fei",
        "Liangming Pan",
        "Adam Jardine",
        "W. Wang"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities. Coda and data are available https://github.com/Wenyueh/inductive_reasoning_benchmark.",
      "arxiv_url": "https://arxiv.org/abs/2502.15823",
      "pdf_url": "https://arxiv.org/pdf/2502.15823",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13487",
      "title": "Transferring Textual Preferences to Vision-Language Understanding through Model Merging",
      "authors": [
        "Chen-An Li",
        "Tzu-Han Lin",
        "Yun-Nung Chen",
        "Hung-yi Lee"
      ],
      "abstract": "Large vision-language models (LVLMs) perform outstandingly across various multimodal tasks. However, their ability to evaluate generated content remains limited, and training vision-language reward models (VLRMs) with preference data is computationally expensive. This paper explores a training-free alternative by merging text-based reward models (RMs) with LVLMs to create VLRMs. Our approach shows that integrating these models leads to improved performance over LVLMs' scoring and text-based RMs, offering an efficient method for incorporating textual preferences into LVLMs.",
      "arxiv_url": "https://arxiv.org/abs/2502.13487",
      "pdf_url": "https://arxiv.org/pdf/2502.13487",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12483",
      "title": "The Knowledge Microscope: Features as Better Analytical Lenses than Neurons",
      "authors": [
        "Yuheng Chen",
        "Pengfei Cao",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "Previous studies primarily utilize MLP neurons as units of analysis for understanding the mechanisms of factual knowledge in Language Models (LMs); however, neurons suffer from polysemanticity, leading to limited knowledge expression and poor interpretability. In this paper, we first conduct preliminary experiments to validate that Sparse Autoencoders (SAE) can effectively decompose neurons into features, which serve as alternative analytical units. With this established, our core findings reveal three key advantages of features over neurons: (1) Features exhibit stronger influence on knowledge expression and superior interpretability. (2) Features demonstrate enhanced monosemanticity, showing distinct activation patterns between related and unrelated facts. (3) Features achieve better privacy protection than neurons, demonstrated through our proposed FeatureEdit method, which significantly outperforms existing neuron-based approaches in erasing privacy-sensitive information from LMs.Code and dataset will be available.",
      "arxiv_url": "https://arxiv.org/abs/2502.12483",
      "pdf_url": "https://arxiv.org/pdf/2502.12483",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12551",
      "title": "MEraser: An Effective Fingerprint Erasure Approach for Large Language Models",
      "authors": [
        "Jingxuan Zhang",
        "Zhenhua Xu",
        "Rui Hu",
        "Wenpeng Xing",
        "Xuhong Zhang",
        "Meng Han"
      ],
      "abstract": "Large Language Models (LLMs) have become increasingly prevalent across various sectors, raising critical concerns about model ownership and intellectual property protection. Although backdoor-based fingerprinting has emerged as a promising solution for model authentication, effective attacks for removing these fingerprints remain largely unexplored. Therefore, we present Mismatched Eraser (MEraser), a novel method for effectively removing backdoor-based fingerprints from LLMs while maintaining model performance. Our approach leverages a two-phase fine-tuning strategy utilizing carefully constructed mismatched and clean datasets. Through extensive evaluation across multiple LLM architectures and fingerprinting methods, we demonstrate that MEraser achieves complete fingerprinting removal while maintaining model performance with minimal training data of fewer than 1,000 samples. Furthermore, we introduce a transferable erasure mechanism that enables effective fingerprinting removal across different models without repeated training. In conclusion, our approach provides a practical solution for fingerprinting removal in LLMs, reveals critical vulnerabilities in current fingerprinting techniques, and establishes comprehensive evaluation benchmarks for developing more resilient model protection methods in the future.",
      "arxiv_url": "https://arxiv.org/abs/2506.12551",
      "pdf_url": "https://arxiv.org/pdf/2506.12551",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03673",
      "title": "Reason from Future: Reverse Thought Chain Enhances LLM Reasoning",
      "authors": [
        "Yinlong Xu",
        "Yanzhao Zheng",
        "Shuoshuo Sun",
        "Shuaihan Huang",
        "Baohua Dong",
        "Hangcheng Zhu",
        "Ruohui Huang",
        "Gang Yu",
        "Hongxia Xu",
        "Jian Wu"
      ],
      "abstract": "It has been demonstrated that carefully designed reasoning paradigms, like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), can enhance the reasoning capabilities of small language models by detailed thinking and extensive thought searching, unbounded branching factors in the searching space create prohibitive reasoning consumption. However these methods fall into the trap of local optimum reasoning, which means the model lacks a global perspective while solving problems. We propose a novel reasoning paradigm called Reason from Future (RFF), which generates reasoning paths by bidirectional reasoning that combines top-down planning with bottom-up reasoning accumulation. The essence of RFF lies in its reverse reasoning mechanism, which prioritizes core logical relationships and imposes goal-oriented constraints on intermediate steps, thereby reducing the searching space and mitigating error accumulation inherent in sequential forward reasoning. Empirical evaluations across diverse experiments demonstrate that RFF outperforms conventional paradigms with higher accuracy and less searching space to solve complex tasks.",
      "arxiv_url": "https://arxiv.org/abs/2506.03673",
      "pdf_url": "https://arxiv.org/pdf/2506.03673",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e18d328b931443d18d1a91c92aee32f15ee050b8",
      "title": "That doesn't sound right: Evaluating speech transcription quality in field linguistics corpora",
      "authors": [
        "Éric Le Ferrand",
        "Bo Jiang",
        "Joshua Hartshorne",
        "Emily Prud'hommeaux"
      ],
      "abstract": "Incorporating automatic speech recognition (ASR) into field linguistics workflows for language documentation has become increasingly common. While ASR performance has seen improvements in low-resource settings, obstacles remain when training models on data collected by documentary linguists. One notable challenge lies in the way that this data is curated. ASR datasets built from spontaneous speech are typically recorded in consistent settings and transcribed by native speakers following a set of well designed guidelines. In contrast, field linguists collect data in whatever format it is delivered by their language consultants and transcribe it as best they can given their language skills and the quality of the recording. This approach to data curation, while valuable for linguistic research, does not always align with the standards required for training robust ASR models. In this paper, we explore methods for identifying speech transcriptions in fieldwork data that may be unsuitable for training ASR models. We focus on two complimentary automated measures of transcription quality that can be used to identify transcripts with characteristics that are common in field data but could be detrimental to ASR training. We show that one of the metrics is highly effective at retrieving these types of transcriptions. Additionally, we find that filtering datasets using this metric of transcription quality reduces WER both in controlled experiments using simulated field-work with artificially corrupted data and in real fieldwork corpora.",
      "arxiv_url": "https://www.semanticscholar.org/paper/e18d328b931443d18d1a91c92aee32f15ee050b8",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.11673",
      "title": "Improving Causal Interventions in Amnesic Probing with Mean Projection or LEACE",
      "authors": [
        "Alicja Dobrzeniecka",
        "Antske Fokkens",
        "Pia Sommerauer"
      ],
      "abstract": "Amnesic probing is a technique used to examine the influence of specific linguistic information on the behaviour of a model. This involves identifying and removing the relevant information and then assessing whether the model's performance on the main task changes. If the removed information is relevant, the model's performance should decline. The difficulty with this approach lies in removing only the target information while leaving other information unchanged. It has been shown that Iterative Nullspace Projection (INLP), a widely used removal technique, introduces random modifications to representations when eliminating target information. We demonstrate that Mean Projection (MP) and LEACE, two proposed alternatives, remove information in a more targeted manner, thereby enhancing the potential for obtaining behavioural explanations through Amnesic Probing.",
      "arxiv_url": "https://arxiv.org/abs/2506.11673",
      "pdf_url": "https://arxiv.org/pdf/2506.11673",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.06950",
      "title": "What Makes a Good Natural Language Prompt?",
      "authors": [
        "Do Xuan Long",
        "Duy Dinh",
        "Ngoc-Hai Nguyen",
        "Kenji Kawaguchi",
        "Nancy F. Chen",
        "Shafiq Joty",
        "Min-Yen Kan"
      ],
      "abstract": "As large language models (LLMs) have progressed towards more human-like and human--AI communications have become prevalent, prompting has emerged as a decisive component. However, there is limited conceptual consensus on what exactly quantifies natural language prompts. We attempt to address this question by conducting a meta-analysis surveying more than 150 prompting-related papers from leading NLP and AI conferences from 2022 to 2025 and blogs. We propose a property- and human-centric framework for evaluating prompt quality, encompassing 21 properties categorized into six dimensions. We then examine how existing studies assess their impact on LLMs, revealing their imbalanced support across models and tasks, and substantial research gaps. Further, we analyze correlations among properties in high-quality natural language prompts, deriving prompting recommendations. We then empirically explore multi-property prompt enhancements in reasoning tasks, observing that single-property enhancements often have the greatest impact. Finally, we discover that instruction-tuning on property-enhanced prompts can result in better reasoning models. Our findings establish a foundation for property-centric prompt evaluation and optimization, bridging the gaps between human--AI communication and opening new prompting research directions.",
      "arxiv_url": "https://arxiv.org/abs/2506.06950",
      "pdf_url": "https://arxiv.org/pdf/2506.06950",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e1eafa4da589db7da8988d3b922367a2819e7a84",
      "title": "Filter-And-Refine: A MLLM Based Cascade System for Industrial-Scale Video Content Moderation",
      "authors": [
        "Zixuan Wang",
        "Jinghao Shi",
        "Hanzhong Liang",
        "Xiang Shen",
        "Vera Wen",
        "Zhiqian Chen",
        "Yifan Wu",
        "Zhixin Zhang",
        "Hongyu Xiong"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/e1eafa4da589db7da8988d3b922367a2819e7a84",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19599",
      "title": "Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar",
      "authors": [
        "Andrew Gambardella",
        "Takeshi Kojima",
        "Yusuke Iwasawa",
        "Yutaka Matsuo"
      ],
      "abstract": "Typical methods for evaluating the performance of language models evaluate their ability to answer questions accurately. These evaluation metrics are acceptable for determining the extent to which language models can understand and reason about text in a general sense, but fail to capture nuanced capabilities, such as the ability of language models to recognize and obey rare grammar points, particularly in languages other than English. We measure the perplexity of language models when confronted with the\"first person psych predicate restriction\"grammar point in Japanese. Weblab is the only tested open source model in the 7-10B parameter range which consistently assigns higher perplexity to ungrammatical psych predicate sentences than grammatical ones. We give evidence that Weblab's uniformly bad tokenization is a possible root cause for its good performance, and show that Llama 3's perplexity on grammatical psych predicate sentences can be reduced by orders of magnitude (28x difference) by restricting test sentences to those with uniformly well-behaved tokenizations. We show in further experiments on machine translation tasks that language models will use alternative grammar patterns in order to produce grammatical sentences when tokenization issues prevent the most natural sentence from being output.",
      "arxiv_url": "https://arxiv.org/abs/2505.19599",
      "pdf_url": "https://arxiv.org/pdf/2505.19599",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21277",
      "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space",
      "authors": [
        "Yao Huang",
        "Yitong Sun",
        "Shouwei Ruan",
        "Yichi Zhang",
        "Yinpeng Dong",
        "Xingxing Wei"
      ],
      "abstract": "Large Language Models (LLMs), despite advanced general capabilities, still suffer from numerous safety risks, especially jailbreak attacks that bypass safety protocols. Understanding these vulnerabilities through black-box jailbreak attacks, which better reflect real-world scenarios, offers critical insights into model robustness. While existing methods have shown improvements through various prompt engineering techniques, their success remains limited against safety-aligned models, overlooking a more fundamental problem: the effectiveness is inherently bounded by the predefined strategy spaces. However, expanding this space presents significant challenges in both systematically capturing essential attack patterns and efficiently navigating the increased complexity. To better explore the potential of expanding the strategy space, we address these challenges through a novel framework that decomposes jailbreak strategies into essential components based on the Elaboration Likelihood Model (ELM) theory and develops genetic-based optimization with intention evaluation mechanisms. To be striking, our experiments reveal unprecedented jailbreak capabilities by expanding the strategy space: we achieve over 90% success rate on Claude-3.5 where prior methods completely fail, while demonstrating strong cross-model transferability and surpassing specialized safeguard models in evaluation accuracy. The code is open-sourced at: https://github.com/Aries-iai/CL-GSO.",
      "arxiv_url": "https://arxiv.org/abs/2505.21277",
      "pdf_url": "https://arxiv.org/pdf/2505.21277",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.05266",
      "title": "User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs",
      "authors": [
        "Sougata Saha",
        "Monojit Choudhury"
      ],
      "abstract": "Measuring the generalization ability of Large Language Models (LLMs) is challenging due to data contamination. As models grow and computation becomes cheaper, ensuring tasks and test cases are unseen during training phases will become nearly impossible. We argue that knowledge-retrieval and reasoning tasks are not ideal for measuring generalization, as LLMs are not trained for specific tasks. Instead, we propose user behavior prediction, also a key aspect of personalization, as a theoretically sound, scalable, and robust alternative. We introduce a novel framework for this approach and test it on movie and music recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct. Results align with our framework's predictions, showing GPT-4o outperforms GPT-4o-mini and Llama, though all models have much room for improvement, especially Llama.",
      "arxiv_url": "https://arxiv.org/abs/2507.05266",
      "pdf_url": "https://arxiv.org/pdf/2507.05266",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-06-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14507",
      "title": "Can LLMs Simulate L2-English Dialogue? An Information-Theoretic Analysis of L1-Dependent Biases",
      "authors": [
        "Rena Gao",
        "Xuetong Wu",
        "Tatsuki Kuribayashi",
        "Mingrui Ye",
        "Siya Qi",
        "Carsten Roever",
        "Yuanxing Liu",
        "Zheng Yuan",
        "Jey Han Lau"
      ],
      "abstract": "This study evaluates Large Language Models' (LLMs) ability to simulate non-native-like English use observed in human second language (L2) learners interfered with by their native first language (L1). In dialogue-based interviews, we prompt LLMs to mimic L2 English learners with specific L1s (e.g., Japanese, Thai, Urdu) across seven languages, comparing their outputs to real L2 learner data. Our analysis examines L1-driven linguistic biases, such as reference word usage and avoidance behaviors, using information-theoretic and distributional density measures. Results show that modern LLMs (e.g., Qwen2.5, LLAMA3.3, DeepseekV3, GPT-4o) replicate L1-dependent patterns observed in human L2 data, with distinct influences from various languages (e.g., Japanese, Korean, and Mandarin significantly affect tense agreement, and Urdu influences noun-verb collocations). Our results reveal the potential of LLMs for L2 dialogue generation and evaluation for future educational applications.",
      "arxiv_url": "https://arxiv.org/abs/2502.14507",
      "pdf_url": "https://arxiv.org/pdf/2502.14507",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00421",
      "title": "Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions",
      "authors": [
        "Jihyoung Jang",
        "Minwook Bae",
        "Minji Kim",
        "Dilek Hakkani-Tur",
        "Hyounghun Kim"
      ],
      "abstract": "As chatbots continue to evolve toward human-like, real-world, interactions, multimodality remains an active area of research and exploration. So far, efforts to integrate multimodality into chatbots have primarily focused on image-centric tasks, such as visual dialogue and image-based instructions, placing emphasis on the\"eyes\"of human perception while neglecting the\"ears\", namely auditory aspects. Moreover, these studies often center around static interactions that focus on discussing the modality rather than naturally incorporating it into the conversation, which limits the richness of simultaneous, dynamic engagement. Furthermore, while multimodality has been explored in multi-party and multi-session conversations, task-specific constraints have hindered its seamless integration into dynamic, natural conversations. To address these challenges, this study aims to equip chatbots with\"eyes and ears\"capable of more immersive interactions with humans. As part of this effort, we introduce a new multimodal conversation dataset, Multimodal Multi-Session Multi-Party Conversation ($M^3C$), and propose a novel multimodal conversation model featuring multimodal memory retrieval. Our model, trained on the $M^3C$, demonstrates the ability to seamlessly engage in long-term conversations with multiple speakers in complex, real-world-like settings, effectively processing visual and auditory inputs to understand and respond appropriately. Human evaluations highlight the model's strong performance in maintaining coherent and dynamic interactions, demonstrating its potential for advanced multimodal conversational agents.",
      "arxiv_url": "https://arxiv.org/abs/2506.00421",
      "pdf_url": "https://arxiv.org/pdf/2506.00421",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10557",
      "title": "MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning",
      "authors": [
        "Ke Wang",
        "Junting Pan",
        "Linda Wei",
        "Aojun Zhou",
        "Weikang Shi",
        "Zimu Lu",
        "Han Xiao",
        "Yunqiao Yang",
        "Houxing Ren",
        "Mingjie Zhan",
        "Hongsheng Li"
      ],
      "abstract": "Natural language image-caption datasets, widely used for training Large Multimodal Models, mainly focus on natural scenarios and overlook the intricate details of mathematical figures that are critical for problem-solving, hindering the advancement of current LMMs in multimodal mathematical reasoning. To this end, we propose leveraging code as supervision for cross-modal alignment, since code inherently encodes all information needed to generate corresponding figures, establishing a precise connection between the two modalities. Specifically, we co-develop our image-to-code model and dataset with model-in-the-loop approach, resulting in an image-to-code model, FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date. Furthermore, we utilize FigCodifier to synthesize novel mathematical figures and then construct MM-MathInstruct-3M, a high-quality multimodal math instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista, achieving improvements of 8.9% and 9.2%. The dataset and models will be released at https://github.com/mathllm/MathCoder.",
      "arxiv_url": "https://arxiv.org/abs/2505.10557",
      "pdf_url": "https://arxiv.org/pdf/2505.10557",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01372",
      "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark",
      "authors": [
        "Joel Niklaus",
        "Jakob Merane",
        "Luka Nenadic",
        "Sina Ahmadi",
        "Yingqiang Gao",
        "Cyrill A. H. Chevalley",
        "Claude Humbel",
        "Christophe Gosken",
        "Lorenzo Tanzi",
        "Thomas Luthi",
        "Stefan Palombo",
        "Spencer Poff",
        "Boling Yang",
        "Nan Wu",
        "Matthew Guillod",
        "Robin Mami'e",
        "Daniel Brunner",
        "Julio Pereyra",
        "Niko A. Grupen"
      ],
      "abstract": "In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.",
      "arxiv_url": "https://arxiv.org/abs/2503.01372",
      "pdf_url": "https://arxiv.org/pdf/2503.01372",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21940",
      "title": "RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering",
      "authors": [
        "Bolei He",
        "Xinran He",
        "Mengke Chen",
        "Xianwei Xue",
        "Ying Zhu",
        "Zhenhua Ling"
      ],
      "abstract": "Large Language Models (LLMs) excel in many areas but continue to face challenges with complex reasoning tasks, such as Multi-Hop Question Answering (MHQA). MHQA requires integrating evidence from diverse sources while managing intricate logical dependencies, often leads to errors in reasoning. Retrieval-Augmented Generation (RAG), widely employed in MHQA tasks, faces challenges in effectively filtering noisy data and retrieving all necessary evidence, thereby limiting its effectiveness in addressing MHQA challenges. To address these challenges, we propose RISE:Reasoning Enhancement via Iterative Self-Exploration, a novel framework designed to enhance models' reasoning capability through iterative self-exploration. Specifically, RISE involves three key steps in addressing MHQA tasks: question decomposition, retrieve-then-read, and self-critique. By leveraging continuous self-exploration, RISE identifies accurate reasoning paths, iteratively self-improving the model's capability to integrate evidence, maintain logical consistency, and enhance performance in MHQA tasks. Extensive experiments on multiple MHQA benchmarks demonstrate that RISE significantly improves reasoning accuracy and task performance.",
      "arxiv_url": "https://arxiv.org/abs/2505.21940",
      "pdf_url": "https://arxiv.org/pdf/2505.21940",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.08309",
      "title": "Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency",
      "authors": [
        "Yupu Liang",
        "Yaping Zhang",
        "Zhiyang Zhang",
        "Zhiyuan Chen",
        "Yang Zhao",
        "Lu Xiang",
        "Chengqing Zong",
        "Yu Zhou"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have shown strong performance in document image tasks, especially Optical Character Recognition (OCR). However, they struggle with Document Image Machine Translation (DIMT), which requires handling both cross-modal and cross-lingual challenges. Previous efforts to enhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT dataset often result in the forgetting of the model's existing monolingual abilities, such as OCR. To address these challenges, we introduce a novel fine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR proficiency, inspired by the concept\"Bilingual Cognitive Advantage\". Specifically, SSR prompts the model to generate OCR text before producing translation text, which allows the model to leverage its strong monolingual OCR ability while learning to translate text across languages. Comprehensive experiments demonstrate the proposed SSR learning helps mitigate catastrophic forgetting, improving the generalization ability of MLLMs on both OCR and DIMT tasks.",
      "arxiv_url": "https://arxiv.org/abs/2507.08309",
      "pdf_url": "https://arxiv.org/pdf/2507.08309",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.13535",
      "title": "Information Extraction from Visually Rich Documents using LLM-based Organization of Documents into Independent Textual Segments",
      "authors": [
        "Aniket Bhattacharyya",
        "Anurag Tripathi",
        "Ujjal Das",
        "Archan Karmakar",
        "Amit Pathak",
        "Maneesh Gupta"
      ],
      "abstract": "Information extraction (IE) from Visually Rich Documents (VRDs) containing layout features along with text is a critical and well-studied task. Specialized non-LLM NLP-based solutions typically involve training models using both textual and geometric information to label sequences/tokens as named entities or answers to specific questions. However, these approaches lack reasoning, are not able to infer values not explicitly present in documents, and do not generalize well to new formats. Generative LLM-based approaches proposed recently are capable of reasoning, but struggle to comprehend clues from document layout especially in previously unseen document formats, and do not show competitive performance in heterogeneous VRD benchmark datasets. In this paper, we propose BLOCKIE, a novel LLM-based approach that organizes VRDs into localized, reusable semantic textual segments called $\\textit{semantic blocks}$, which are processed independently. Through focused and more generalizable reasoning,our approach outperforms the state-of-the-art on public VRD benchmarks by 1-3% in F1 scores, is resilient to document formats previously not encountered and shows abilities to correctly extract information not explicitly present in documents.",
      "arxiv_url": "https://arxiv.org/abs/2505.13535",
      "pdf_url": "https://arxiv.org/pdf/2505.13535",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.13262",
      "title": "TablePilot: Recommending Human-Preferred Tabular Data Analysis with Large Language Models",
      "authors": [
        "Deyin Yi",
        "Yihao Liu",
        "Lang Cao",
        "Mengyu Zhou",
        "Haoyu Dong",
        "Shi Han",
        "Dongmei Zhang"
      ],
      "abstract": "Tabular data analysis is crucial in many scenarios, yet efficiently identifying the most relevant data analysis queries and results for a new table remains a significant challenge. The complexity of tabular data, diverse analytical operations, and the demand for high-quality analysis make the process tedious. To address these challenges, we aim to recommend query-code-result triplets tailored for new tables in tabular data analysis workflows. In this paper, we present TablePilot, a pioneering tabular data analysis framework leveraging large language models to autonomously generate comprehensive and superior analytical results without relying on user profiles or prior interactions. The framework incorporates key designs in analysis preparation and analysis optimization to enhance accuracy. Additionally, we propose Rec-Align, a novel method to further improve recommendation quality and better align with human preferences. Experiments on DART, a dataset specifically designed for comprehensive tabular data analysis recommendation, demonstrate the effectiveness of our framework. Based on GPT-4o, the tuned TablePilot achieves 77.0% top-5 recommendation recall. Human evaluations further highlight its effectiveness in optimizing tabular data analysis workflows.",
      "arxiv_url": "https://arxiv.org/abs/2503.13262",
      "pdf_url": "https://arxiv.org/pdf/2503.13262",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e3234ac3ce8e7f47de30e760be2684668122693b",
      "title": "Adversarial Alignment with Anchor Dragging Drift (A³D²): Multimodal Domain Adaptation with Partially Shifted Modalities",
      "authors": [
        "Jun Sun",
        "Xinxin Zhang",
        "Simin Hong",
        "Jian Zhu",
        "Lingfang Zeng"
      ],
      "abstract": "Multimodal learning has celebrated remarkable success across diverse areas, yet faces the challenge of prohibitively expensive data collection and annotation when adapting models to new environments. In this context, domain adaptation has gained growing popularity as a technique for knowledge transfer, which, however, remains underexplored in multimodal settings compared with unimodal ones. This paper investigates multimodal domain adaptation, focusing on a practical partially shifting scenario where some modalities (referred to as anchors) remain domain-stable, while others (referred to as drifts) undergo a domain shift. We propose a bi-alignment scheme to simultaneously perform drift-drift and anchor-drift matching. The former is achieved through adversarial learning, aligning the representations of the drifts across source and target domains; the latter corresponds to an \"anchor dragging drift\" strategy, which matches the distributions of the drifts and anchors within the target domain using the optimal transport (OT) method. The overall design principle features A dversarial A lignment with A nchor D ragging D rift, abbreviated as A 3 D 2 , for multimodal domain adaptation with partially shifted modalities. Comprehensive empirical results verify the effectiveness of the proposed approach, and demonstrate that A 3 D 2 achieves superior performance compared with state-of-the-art approaches. The code is available at: https: //github",
      "arxiv_url": "https://www.semanticscholar.org/paper/e3234ac3ce8e7f47de30e760be2684668122693b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01859",
      "title": "CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions",
      "authors": [
        "Tamer Alkhouli",
        "Katerina Margatina",
        "James Gung",
        "Raphael Shu",
        "Claudia Zaghi",
        "Monica Sunkara",
        "Yi Zhang"
      ],
      "abstract": "We introduce Conversational Function-Calling Evaluation Through Turn-Level Interactions (CONFETTI), a conversational benchmark1 designed to evaluate the function-calling capabilities and response quality of large language models (LLMs). Current benchmarks lack comprehensive assessment of LLMs in complex conversational scenarios. CONFETTI addresses this gap through 109 human-simulated conversations, comprising 313 user turns and covering 86 APIs. These conversations explicitly target various conversational complexities, such as follow-ups, goal correction and switching, ambiguous and implicit goals. We perform off-policy turn-level evaluation using this benchmark targeting function-calling. Our benchmark also incorporates dialog act annotations to assess agent responses. We evaluate a series of state-of-the-art LLMs and analyze their performance with respect to the number of available APIs, conversation lengths, and chained function calling. Our results reveal that while some models are able to handle long conversations, and leverage more than 20+ APIs successfully, other models struggle with longer context or when increasing the number of APIs. We also report that the performance on chained function-calls is severely limited across the models. Overall, the top performing models on CONFETTI are Nova Pro (40.01%), Claude Sonnet v3.5 (35.46%) and Llama 3.1 405B (33.19%) followed by command-r-plus (31.18%) and Mistral-Large-2407 (30.07%).",
      "arxiv_url": "https://arxiv.org/abs/2506.01859",
      "pdf_url": "https://arxiv.org/pdf/2506.01859",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20429",
      "title": "PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy",
      "authors": [
        "Shuhao Guan",
        "Moule Lin",
        "Cheng Xu",
        "Xinyi Liu",
        "Jinman Zhao",
        "Jiexin Fan",
        "Qi Xu",
        "Derek Greene"
      ],
      "abstract": "This paper introduces PreP-OCR, a two-stage pipeline that combines document image restoration with semantic-aware post-OCR correction to enhance both visual clarity and textual consistency, thereby improving text extraction from degraded historical documents. First, we synthesize document-image pairs from plaintext, rendering them with diverse fonts and layouts and then applying a randomly ordered set of degradation operations. An image restoration model is trained on this synthetic data, using multi-directional patch extraction and fusion to process large images. Second, a ByT5 post-OCR model, fine-tuned on synthetic historical text pairs, addresses remaining OCR errors. Detailed experiments on 13,831 pages of real historical documents in English, French, and Spanish show that the PreP-OCR pipeline reduces character error rates by 63.9-70.3% compared to OCR on raw images. Our pipeline demonstrates the potential of integrating image restoration with linguistic error correction for digitizing historical archives.",
      "arxiv_url": "https://arxiv.org/abs/2505.20429",
      "pdf_url": "https://arxiv.org/pdf/2505.20429",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e360e74e1a9641af2733f04793d6f15119f94a14",
      "title": "Acquisition and Application of Novel Knowledge in Large Language Models",
      "authors": [
        "Ziyu Shang",
        "Jianghan Liu",
        "Zhizhao Luo",
        "Peng Wang",
        "Wenjun Ke",
        "Jiajun Liu",
        "Zijie Xu",
        "Guozheng Li"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have demonstrated their impressive generative capabilities, primarily due to their extensive parameterization, which enables them to encode vast knowledge. However, effectively integrating new knowledge into LLMs remains a major challenge. Current research typically first constructs novel knowledge datasets and then injects this knowledge into LLMs through various techniques. However, existing methods for constructing new datasets either rely on timestamps, which lack rigor, or use simple templates for synthesis, which are simplistic and do not accurately reflect the real world. To address this issue, we propose a novel knowledge dataset construction approach that simulates biological evolution using knowledge graphs to generate synthetic entities with diverse attributes, resulting in a dataset NovelHuman . We then evaluate existing training strategies and knowledge augmentation methods on NovelHuman. Systematic analysis on NovelHuman reveals that the intra-sentence position of knowledge significantly affects the acquisition of knowledge. Therefore, we introduce an intra-sentence permutation to enhance knowledge acquisition. Furthermore, given that potential conflicts exist between autoregressive (AR) training objectives and permutation-based learning, we pro-pose PermAR , a permutation-based language modeling framework for AR models. PermAR seamlessly integrates with mainstream AR architectures, endowing them with bidirectional knowledge acquisition capabilities. Extensive experiments demonstrate the superiority of Per-mAR 1 , outperforming knowledge augmentation methods by 3.3%-38%.",
      "arxiv_url": "https://www.semanticscholar.org/paper/e360e74e1a9641af2733f04793d6f15119f94a14",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e3626313dfbca5a8ca7739f5411bd999414a953a",
      "title": "Revisiting Scaling Laws for Language Models: The Role of Data Quality and Training Strategies",
      "authors": [
        "Zhengyu Chen",
        "Siqi Wang",
        "Teng Xiao",
        "Yudong Wang",
        "Shiqi Chen",
        "Xunliang Cai",
        "Junxian He",
        "Jingang Wang"
      ],
      "abstract": "Traditional scaling laws in natural language processing suggest that increasing model size and training data enhances performance. However, recent studies reveal deviations, particularly in large language models, where performance improvements decelerate—a phenomenon known as sub-scaling. This paper revisits these scaling laws by examining the impact of data quality and training strategies on model performance. Through extensive empirical analysis of over 400 models, we identify high data density and non-optimal resource allocation as key factors contributing to sub-scaling. High data density leads to diminishing returns due to redundant information, while optimal resource allocation is crucial for sustained performance improvements. We propose a sub-optimal scaling law that better predicts performance in sub-scaling regimes, highlighting the importance of data quality and diversity.",
      "arxiv_url": "https://www.semanticscholar.org/paper/e3626313dfbca5a8ca7739f5411bd999414a953a",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13135",
      "title": "Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions",
      "authors": [
        "Taedong Yun",
        "Eric Yang",
        "Mustafa Safdari",
        "Jong Ha Lee",
        "Vaishnavi Vinod Kumar",
        "S. Mahdavi",
        "Jonathan Amar",
        "Derek Peyton",
        "Reut Aharony",
        "Andreas Michaelides",
        "Logan Schneider",
        "Isaac R. Galatzer-Levy",
        "Yugang Jia",
        "John Canny",
        "Arthur Gretton",
        "Maja Matari´c",
        "Google DeepMind",
        "Verily Life Sciences",
        "Google",
        "Kristine Arges",
        "T. Assimes",
        "Vikram Bajaj",
        "Suresh Balu",
        "Mustafa R. Bashir",
        "Laura Beskow",
        "Ros-alia Blanco",
        "R. Califf",
        "Paul Campbell",
        "Larry Carin",
        "Victoria Christian",
        "Scott Cousins",
        "Millie Das",
        "M. Dockery",
        "Pamela S. Douglas",
        "Ashley Dunham",
        "Julie Eckstrand",
        "Dominik Fleischmann",
        "Emily Ford",
        "Elizabeth S. Fraulo",
        "John French",
        "S. Sanjiv",
        "Ge-offrey S Gambhir",
        "Robert C Ginsburg",
        "Francois Green",
        "Haddad Adrian",
        "John Hernandez",
        "Erich S Hernandez",
        "Huang Glenn",
        "Daniel Jaffe",
        "Lynne H King",
        "Curtis Koweek",
        "Yaping J Langlotz",
        "Kenneth W Liao",
        "K. Mahaffey",
        "William J Marcom",
        "J. D. Marks",
        "Reid Maron",
        "Shannon McCabe",
        "Rebecca McCall",
        "Jessica McCue",
        "David Mega",
        "Lawrence H Miller",
        "Rajan Muhlbaier",
        "Munshi",
        "Kristin Newby",
        "Bray Ezra Pak-Harvey",
        "Michael Pencina",
        "Eric D. Peterson",
        "Fa-tima Rodriguez",
        "Scarlet Shore",
        "Svati H Shah",
        "Steven Shipes",
        "G. Sledge",
        "Susie Spielman",
        "Ryan Spitler",
        "T. Schaack",
        "Geeta Swamy",
        "M. Willemink",
        "Charlene A Wong. 2020",
        "Yuntao Bai",
        "Saurav Kadavath",
        "Sandipan Kundu",
        "Amanda Askell",
        "John Kernion",
        "Andy Jones",
        "Anna Chen",
        "Anna Goldie",
        "Azalia Mirhoseini",
        "C. McKinnon",
        "Carol Chen",
        "Catherine Olsson",
        "Chris Olah",
        "Danny Hernandez",
        "Dawn Drain",
        "Deep Ganguli",
        "Dustin Li",
        "Eli Tran-Johnson",
        "Ethan Perez",
        "Jamie Kerr",
        "J. Mueller",
        "Jeffrey Ladish",
        "Joshua Landau",
        "Kamal Ndousse",
        "Kamile Lukosuite",
        "Liane Lovitt",
        "M. Sellitto",
        "Nelson Elhage",
        "Nicholas Schiefer",
        "Noem'i Mercado",
        "Nova Dassarma",
        "R. Lasenby",
        "Robin Larson",
        "Sam Ringer",
        "Scott John-ston",
        "Sheer Shauna Kravec",
        "El Showk",
        "Stanislav Fort",
        "Tamera Lanham",
        "Timothy Telleen-Lawton",
        "T. Henighan",
        "Tristan Hume",
        "Samuel R. Bow-man",
        "Zac Hatfield-Dodds",
        "Benjamin Mann",
        "Dario Amodei"
      ],
      "abstract": "We present an end-to-end framework for generating synthetic users for evaluating interactive agents designed to encourage positive behavior changes, such as in health and lifestyle coaching. The synthetic users are grounded in health and lifestyle conditions, specifically sleep and diabetes management in this study, to ensure realistic interactions with the health coaching agent. Synthetic users are created in two stages: first, structured data are generated grounded in real-world health and lifestyle factors in addition to basic demographics and behavioral attributes; second, full profiles of the synthetic users are developed conditioned on the structured data. Interactions between synthetic users and the coaching agent are simulated using generative agent-based models such as Concordia, or directly by prompting a language model. Using two independently-developed agents for sleep and diabetes coaching as case studies, the validity of this framework is demonstrated by analyzing the coaching agent's understanding of the synthetic users'needs and challenges. Finally, through multiple blinded evaluations of user-coach interactions by human experts, we demonstrate that our synthetic users with health and behavioral attributes more accurately portray real human users with the same attributes, compared to generic synthetic users not grounded in such attributes. The proposed framework lays the foundation for efficient development of conversational agents through extensive, realistic, and grounded simulated interactions.",
      "arxiv_url": "https://arxiv.org/abs/2502.13135",
      "pdf_url": "https://arxiv.org/pdf/2502.13135",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20864",
      "title": "Do Language Models Understand Honorific Systems in Javanese?",
      "authors": [
        "MohammadRifqi Farhansyah",
        "Iwan Darmawan",
        "Adryan Kusumawardhana",
        "Genta Indra Winata",
        "Alham Fikri Aji",
        "Derry Wijaya"
      ],
      "abstract": "The Javanese language features a complex system of honorifics that vary according to the social status of the speaker, listener, and referent. Despite its cultural and linguistic significance, there has been limited progress in developing a comprehensive corpus to capture these variations for natural language processing (NLP) tasks. In this paper, we present Unggah-Ungguh, a carefully curated dataset designed to encapsulate the nuances of Unggah-Ungguh Basa, the Javanese speech etiquette framework that dictates the choice of words and phrases based on social hierarchy and context. Using Unggah-Ungguh, we assess the ability of language models (LMs) to process various levels of Javanese honorifics through classification and machine translation tasks. To further evaluate cross-lingual LMs, we conduct machine translation experiments between Javanese (at specific honorific levels) and Indonesian. Additionally, we explore whether LMs can generate contextually appropriate Javanese honorifics in conversation tasks, where the honorific usage should align with the social role and contextual cues. Our findings indicate that current LMs struggle with most honorific levels, exhibitinga bias toward certain honorific tiers.",
      "arxiv_url": "https://arxiv.org/abs/2502.20864",
      "pdf_url": "https://arxiv.org/pdf/2502.20864",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.15981",
      "title": "Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion",
      "authors": [
        "Markus Frohmann",
        "Gabriel Meseguer-Brocal",
        "Markus Schedl",
        "Elena V. Epure"
      ],
      "abstract": "The rapid advancement of AI-based music generation tools is revolutionizing the music industry but also posing challenges to artists, copyright holders, and providers alike. This necessitates reliable methods for detecting such AI-generated content. However, existing detectors, relying on either audio or lyrics, face key practical limitations: audio-based detectors fail to generalize to new or unseen generators and are vulnerable to audio perturbations; lyrics-based methods require cleanly formatted and accurate lyrics, unavailable in practice. To overcome these limitations, we propose a novel, practically grounded approach: a multimodal, modular late-fusion pipeline that combines automatically transcribed sung lyrics and speech features capturing lyrics-related information within the audio. By relying on lyrical aspects directly from audio, our method enhances robustness, mitigates susceptibility to low-level artifacts, and enables practical applicability. Experiments show that our method, DE-detect, outperforms existing lyrics-based detectors while also being more robust to audio perturbations. Thus, it offers an effective, robust solution for detecting AI-generated music in real-world scenarios. Our code is available at https://github.com/deezer/robust-AI-lyrics-detection.",
      "arxiv_url": "https://arxiv.org/abs/2506.15981",
      "pdf_url": "https://arxiv.org/pdf/2506.15981",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12486",
      "title": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning",
      "authors": [
        "Xiaoqian Liu",
        "Ke Wang",
        "Yongbin Li",
        "Yuchuan Wu",
        "Wen-Cheng Ma",
        "Aobo Kong",
        "Fei Huang",
        "Jianbin Jiao",
        "Junge Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL),utilizing process rewards and iterative self-play. Experiments across social and physical domains demonstrate EPO's ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications. Code and data are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO.",
      "arxiv_url": "https://arxiv.org/abs/2502.12486",
      "pdf_url": "https://arxiv.org/pdf/2502.12486",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23108",
      "title": "Generating Diverse Training Samples for Relation Extraction with Large Language Models",
      "authors": [
        "Zexuan Li",
        "Hongliang Dai",
        "Piji Li"
      ],
      "abstract": "Using Large Language Models (LLMs) to generate training data can potentially be a preferable way to improve zero or few-shot NLP tasks. However, many problems remain to be investigated for this direction. For the task of Relation Extraction (RE), we find that samples generated by directly prompting LLMs may easily have high structural similarities with each other. They tend to use a limited variety of phrasing while expressing the relation between a pair of entities. Therefore, in this paper, we study how to effectively improve the diversity of the training samples generated with LLMs for RE, while also maintaining their correctness. We first try to make the LLMs produce dissimilar samples by directly giving instructions in In-Context Learning (ICL) prompts. Then, we propose an approach to fine-tune LLMs for diversity training sample generation through Direct Preference Optimization (DPO). Our experiments on commonly used RE datasets show that both attempts can improve the quality of the generated training data. We also find that comparing with directly performing RE with an LLM, training a non-LLM RE model with its generated samples may lead to better performance.",
      "arxiv_url": "https://arxiv.org/abs/2505.23108",
      "pdf_url": "https://arxiv.org/pdf/2505.23108",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24525",
      "title": "Limited-Resource Adapters Are Regularizers, Not Linguists",
      "authors": [
        "Marcell Richard Fekete",
        "Nathaniel Robinson",
        "Ernests Lavrinovics",
        "E. D. Jean-Baptiste",
        "Raj Dabre",
        "Johannes Bjerva",
        "Heather Lent"
      ],
      "abstract": "Cross-lingual transfer from related high-resource languages is a well-established strategy to enhance low-resource language technologies. Prior work has shown that adapters show promise for, e.g., improving low-resource machine translation (MT). In this work, we investigate an adapter souping method combined with cross-attention fine-tuning of a pre-trained MT model to leverage language transfer for three low-resource Creole languages, which exhibit relatedness to different language groups across distinct linguistic dimensions. Our approach improves performance substantially over baselines. However, we find that linguistic relatedness -- or even a lack thereof -- does not covary meaningfully with adapter performance. Surprisingly, our cross-attention fine-tuning approach appears equally effective with randomly initialized adapters, implying that the benefit of adapters in this setting lies in parameter regularization, and not in meaningful information transfer. We provide analysis supporting this regularization hypothesis. Our findings underscore the reality that neural language processing involves many success factors, and that not all neural methods leverage linguistic knowledge in intuitive ways.",
      "arxiv_url": "https://arxiv.org/abs/2505.24525",
      "pdf_url": "https://arxiv.org/pdf/2505.24525",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23315",
      "title": "Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments",
      "authors": [
        "Abhirup Chakravarty",
        "Mark Brenchley",
        "Trevor Breakspear",
        "Ian Lewin",
        "Yan Huang"
      ],
      "abstract": "A key ethical challenge in Automated Essay Scoring (AES) is ensuring that scores are only released when they meet high reliability standards. Confidence modelling addresses this by assigning a reliability estimate measure, in the form of a confidence score, to each automated score. In this study, we frame confidence estimation as a classification task: predicting whether an AES-generated score correctly places a candidate in the appropriate CEFR level. While this is a binary decision, we leverage the inherent granularity of the scoring domain in two ways. First, we reformulate the task as an n-ary classification problem using score binning. Second, we introduce a set of novel Kernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that incorporate the ordinal structure of CEFR labels. Our best-performing model achieves an F1 score of 0.97, and enables the system to release 47% of scores with 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to approximately 92% (approx.) CEFR agreement from the standalone AES model where we release all AM predicted scores.",
      "arxiv_url": "https://arxiv.org/abs/2505.23315",
      "pdf_url": "https://arxiv.org/pdf/2505.23315",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12148",
      "title": "Hatevolution: What Static Benchmarks Don't Tell Us",
      "authors": [
        "C. D. Bonaventura",
        "Barbara McGillivray",
        "Yulan He",
        "Albert Meroño-Peñuela"
      ],
      "abstract": "Language changes over time, including in the hate speech domain, which evolves quickly following social dynamics and cultural shifts. While NLP research has investigated the impact of language evolution on model training and has proposed several solutions for it, its impact on model benchmarking remains under-explored. Yet, hate speech benchmarks play a crucial role to ensure model safety. In this paper, we empirically evaluate the robustness of 20 language models across two evolving hate speech experiments, and we show the temporal misalignment between static and time-sensitive evaluations. Our findings call for time-sensitive linguistic benchmarks in order to correctly and reliably evaluate language models in the hate speech domain.",
      "arxiv_url": "https://arxiv.org/abs/2506.12148",
      "pdf_url": "https://arxiv.org/pdf/2506.12148",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00417",
      "title": "A Multi-Labeled Dataset for Indonesian Discourse: Examining Toxicity, Polarization, and Demographics Information",
      "authors": [
        "Lucky Susanto",
        "M. Wijanarko",
        "P. Pratama",
        "Zilu Tang",
        "Fariz Akyas",
        "Traci Hong",
        "Ika Idris",
        "Alham Fikri Aji",
        "Derry Wijaya"
      ],
      "abstract": "Polarization is defined as divisive opinions held by two or more groups on substantive issues. As the world's third-largest democracy, Indonesia faces growing concerns about the interplay between political polarization and online toxicity, which is often directed at vulnerable minority groups. Despite the importance of this issue, previous NLP research has not fully explored the relationship between toxicity and polarization. To bridge this gap, we present a novel multi-label Indonesian dataset that incorporates toxicity, polarization, and annotator demographic information. Benchmarking this dataset using BERT-base models and large language models (LLMs) shows that polarization information enhances toxicity classification, and vice versa. Furthermore, providing demographic information significantly improves the performance of polarization classification.",
      "arxiv_url": "https://arxiv.org/abs/2503.00417",
      "pdf_url": "https://arxiv.org/pdf/2503.00417",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10691",
      "title": "Reasoning is All You Need for Video Generalization: A Counterfactual Benchmark with Sub-question Evaluation",
      "authors": [
        "Qiji Zhou",
        "Yifan Gong",
        "Guangsheng Bao",
        "Hongjie Qiu",
        "Jinqiang Li",
        "Xiangrong Zhu",
        "Huajian Zhang",
        "Yue Zhang"
      ],
      "abstract": "Counterfactual reasoning is crucial for robust video understanding but remains underexplored in existing multimodal benchmarks. In this paper, we introduce \\textbf{COVER} (\\textbf{\\underline{CO}}unterfactual \\textbf{\\underline{V}}id\\textbf{\\underline{E}}o \\textbf{\\underline{R}}easoning), a multidimensional multimodal benchmark that systematically evaluates MLLMs across the abstract-concrete and perception-cognition dimensions. Beyond prior multimodal benchmarks, COVER decomposes complex queries into structured sub-questions, enabling fine-grained reasoning analysis. Experiments on commercial and open-source models reveal a strong correlation between sub-question accuracy and counterfactual reasoning performance, highlighting the role of structured inference in video understanding. Furthermore, our results suggest a key insight: enhancing the reasoning capability of models is essential for improving the robustness of video understanding. COVER establishes a new standard for assessing MLLMs' logical reasoning abilities in dynamic environments. Our work is available at https://github.com/gongyifan-hash/COVER-Benchmark.",
      "arxiv_url": "https://arxiv.org/abs/2503.10691",
      "pdf_url": "https://arxiv.org/pdf/2503.10691",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-12",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.20059",
      "title": "DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction",
      "authors": [
        "Weijieying Ren",
        "Tianxiang Zhao",
        "Lei Wang",
        "Tianchun Wang",
        "Vasant Honavar"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have led to remarkable progresses in medical consultation. However, existing medical LLMs overlook the essential role of Electronic Health Records (EHR) and focus primarily on diagnosis recommendation, limiting their clinical applicability. We propose DiaLLM, the first medical LLM that integrates heterogeneous EHR data into clinically grounded dialogues, enabling clinical test recommendation, result interpretation, and diagnosis prediction to better align with real-world medical practice. To construct clinically grounded dialogues from EHR, we design a Clinical Test Reference (CTR) strategy that maps each clinical code to its corresponding description and classifies test results as\"normal\"or\"abnormal\". Additionally, DiaLLM employs a reinforcement learning framework for evidence acquisition and automated diagnosis. To handle the large action space, we introduce a reject sampling strategy to reduce redundancy and improve exploration efficiency. Furthermore, a confirmation reward and a class-sensitive diagnosis reward are designed to guide accurate diagnosis prediction. Extensive experimental results demonstrate that DiaLLM outperforms baselines in clinical test recommendation and diagnosis prediction.",
      "arxiv_url": "https://arxiv.org/abs/2506.20059",
      "pdf_url": "https://arxiv.org/pdf/2506.20059",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e5f363da5719235dd2859035027444c1c185aa21",
      "title": "MMDEND: Dendrite-Inspired Multi-Branch Multi-Compartment Parallel Spiking Neuron for Sequence Modeling",
      "authors": [
        "Kexin Wang",
        "Yuhong Chou",
        "Richard D. Shang",
        "Shijie Mei",
        "Jiahong Zhang",
        "Yanbin Huang",
        "Man Yao",
        "Boxing Xu",
        "Guoqi Li"
      ],
      "abstract": "Vanilla spiking neurons are simplified from complex biological neurons with dendrites, soma, and synapses, into single somatic compartments. Due to limitations in performance and training efficiency, vanilla spiking neurons face significant challenges in modeling long sequences. In terms of performance, the over-simplified dynamics of spiking neurons omit long-term temporal dependencies. Additionally, the long-tail membrane potential distribution and binary activation discretization errors further limit their capacity to model long sequences. In terms of efficiency, the serial mechanism of spiking neurons leads to excessively long training times for long sequences. Though parallel spiking neurons are an efficient solu-tion, their number of parameters is often tied to the hidden dimension or sequence length, which makes current parallel neurons unsuitable for large architectures. To address these issues, we propose MMDEND 1 : a M ulti-Branch M ulti-Compartment Parallel Spiking Dend ritic Neuron. Its proportion-adjustable multi-branch, multi-compartment structure enables long-term temporal dependencies. Additionally, we introduce a S caling-S hifting Integer F iring ( SSF ) mechanism that fits the long-tail",
      "arxiv_url": "https://www.semanticscholar.org/paper/e5f363da5719235dd2859035027444c1c185aa21",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23191",
      "title": "ExpeTrans: LLMs Are Experiential Transfer Learners",
      "authors": [
        "Jin-Fang Gao",
        "Xiao Ding",
        "Lingxiao Zou",
        "Bibo Cai",
        "Bing Qin",
        "Ting Liu"
      ],
      "abstract": "Recent studies provide large language models (LLMs) with textual task-solving experiences via prompts to improve their performance. However, previous methods rely on substantial human labor or time to gather such experiences for each task, which is impractical given the growing variety of task types in user queries to LLMs. To address this issue, we design an autonomous experience transfer framework to explore whether LLMs can mimic human cognitive intelligence to autonomously transfer experience from existing source tasks to newly encountered target tasks. This not only allows the acquisition of experience without extensive costs of previous methods, but also offers a novel path for the generalization of LLMs. Experimental results on 13 datasets demonstrate that our framework effectively improves the performance of LLMs. Furthermore, we provide a detailed analysis of each module in the framework.",
      "arxiv_url": "https://arxiv.org/abs/2505.23191",
      "pdf_url": "https://arxiv.org/pdf/2505.23191",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05010",
      "title": "ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow Development",
      "authors": [
        "Zhenran Xu",
        "Xue Yang",
        "Yiyu Wang",
        "Qingli Hu",
        "Zijiao Wu",
        "Longyue Wang",
        "Weihua Luo",
        "Kaifu Zhang",
        "Baotian Hu",
        "Min Zhang"
      ],
      "abstract": "We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficiency of ComfyUI, an open-source platform for AI-driven art creation. Despite its flexibility and user-friendly interface, ComfyUI can present challenges to newcomers, including limited documentation, model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot addresses these challenges by offering intelligent node and model recommendations, along with automated one-click workflow construction. At its core, the system employs a hierarchical multi-agent framework comprising a central assistant agent for task delegation and specialized worker agents for different usages, supported by our curated ComfyUI knowledge bases to streamline debugging and deployment. We validate the effectiveness of ComfyUI-Copilot through both offline quantitative evaluations and online user feedback, showing that it accurately recommends nodes and accelerates workflow development. Additionally, use cases illustrate that ComfyUI-Copilot lowers entry barriers for beginners and enhances workflow efficiency for experienced users. The ComfyUI-Copilot installation package and a demo video are available at https://github.com/AIDC-AI/ComfyUI-Copilot.",
      "arxiv_url": "https://arxiv.org/abs/2506.05010",
      "pdf_url": "https://arxiv.org/pdf/2506.05010",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01315",
      "title": "Explainable Depression Detection in Clinical Interviews with Personalized Retrieval-Augmented Generation",
      "authors": [
        "Linhai Zhang",
        "Ziyang Gao",
        "Deyu Zhou",
        "Yulan He"
      ],
      "abstract": "Depression is a widespread mental health disorder, and clinical interviews are the gold standard for assessment. However, their reliance on scarce professionals highlights the need for automated detection. Current systems mainly employ black-box neural networks, which lack interpretability, which is crucial in mental health contexts. Some attempts to improve interpretability use post-hoc LLM generation but suffer from hallucination. To address these limitations, we propose RED, a Retrieval-augmented generation framework for Explainable depression Detection. RED retrieves evidence from clinical interview transcripts, providing explanations for predictions. Traditional query-based retrieval systems use a one-size-fits-all approach, which may not be optimal for depression detection, as user backgrounds and situations vary. We introduce a personalized query generation module that combines standard queries with user-specific background inferred by LLMs, tailoring retrieval to individual contexts. Additionally, to enhance LLM performance in social intelligence, we augment LLMs by retrieving relevant knowledge from a social intelligence datastore using an event-centric retriever. Experimental results on the real-world benchmark demonstrate RED's effectiveness compared to neural networks and LLM-based baselines.",
      "arxiv_url": "https://arxiv.org/abs/2503.01315",
      "pdf_url": "https://arxiv.org/pdf/2503.01315",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization",
        "RAG"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07165",
      "title": "AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models",
      "authors": [
        "Qi Liu",
        "Jingqing Ruan",
        "Hao Li",
        "Haodong Zhao",
        "Desheng Wang",
        "Jiansong Chen",
        "Guanglu Wan",
        "Xunliang Cai",
        "Zhi Zheng",
        "Tong Xu"
      ],
      "abstract": "Existing multi-objective preference alignment methods for large language models (LLMs) face limitations: (1) the inability to effectively balance various preference dimensions, and (2) reliance on auxiliary reward/reference models introduces computational complexity. To address these challenges, we propose Adaptive Multi-objective Preference Optimization (AMoPO), a novel framework that achieves dynamic balance across preference dimensions. By introducing the multi-objective optimization paradigm to use the dimension-aware generation metrics as implicit rewards, AMoPO aligns LLMs with diverse preferences without additional reward models or reference models. We introduce an adaptive weight assignment mechanism that models the generation space as a Gaussian distribution, allowing dynamic prioritization of preference dimensions. Empirical results demonstrate that AMoPO outperforms state-of-the-art baselines by 28.5%, and the experiments on 7B, 14B, and 32B models reveal the scaling ability of AMoPO. Moreover, additional analysis of multiple dimensions verifies its adaptability and effectiveness. These findings validate AMoPO's capability to achieve dimension-aware preference alignment, highlighting its superiority. Our codes and datasets are available at https://github.com/Javkonline/AMoPO.",
      "arxiv_url": "https://arxiv.org/abs/2506.07165",
      "pdf_url": "https://arxiv.org/pdf/2506.07165",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24341",
      "title": "Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings",
      "authors": [
        "Shujian Yang",
        "Shiyao Cui",
        "Chuanrui Hu",
        "Haicheng Wang",
        "Tianwei Zhang",
        "Minlie Huang",
        "Jialiang Lu",
        "Han Qiu"
      ],
      "abstract": "Detecting toxic content using language models is important but challenging. While large language models (LLMs) have demonstrated strong performance in understanding Chinese, recent studies show that simple character substitutions in toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In this paper, we highlight the multimodal nature of Chinese language as a key challenge for deploying LLMs in toxic Chinese detection. First, we propose a taxonomy of 3 perturbation strategies and 8 specific approaches in toxic Chinese content. Then, we curate a dataset based on this taxonomy, and benchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect perturbed toxic Chinese text. Additionally, we explore cost-effective enhancement solutions like in-context learning (ICL) and supervised fine-tuning (SFT). Our results reveal two important findings. (1) LLMs are less capable of detecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a small number of perturbed examples may cause the LLMs\"overcorrect'': misidentify many normal Chinese contents as toxic.",
      "arxiv_url": "https://arxiv.org/abs/2505.24341",
      "pdf_url": "https://arxiv.org/pdf/2505.24341",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e68c15f023fb1c59d425b48eac8393f208e6ea04",
      "title": "(RSA)2: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding",
      "authors": [
        "C. Piano",
        "David Austin",
        "Pablo Piantanida",
        "Jackie CK Cheung"
      ],
      "abstract": "Figurative language (e.g., irony, hyperbole, un-derstatement) is ubiquitous in human communication, resulting in utterances where the literal and the intended meanings do not match. The Rational Speech Act (RSA) framework, which explicitly models speaker intentions, is the most widespread theory of probabilistic prag-matics, but existing implementations are either unable to account for figurative expressions or require modeling the implicit motivations for using figurative language (e.g., to express joy or annoyance) in a setting-specific way. In this paper, we introduce the R hetorical-S trategy-A ware RSA (RSA) 2 framework which models figurative language use by considering a speaker’s employed rhetorical strategy. We show that (RSA) 2 enables human-compatible interpretations of non-literal utterances without modeling a speaker’s motivations for being non-literal. Combined with LLMs, it achieves state-of-the-art performance on the ironic split of PragMega+, a new irony interpretation dataset introduced in this study. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/e68c15f023fb1c59d425b48eac8393f208e6ea04",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05629",
      "title": "Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs",
      "authors": [
        "Ananth Muppidi",
        "Abhilash Nandy",
        "Sambaran Bandyopadhyay"
      ],
      "abstract": "The performance of large language models in domain-specific tasks necessitates fine-tuning, which is computationally expensive and technically challenging. This paper focuses on parameter-efficient fine-tuning using soft prompting, a promising approach that adapts pre-trained models to downstream tasks by learning a small set of parameters. We propose a novel Input Dependent Soft Prompting technique with a self-Attention Mechanism (ID-SPAM) that generates soft prompts based on the input tokens and attends different tokens with varying importance. Our method is simple and efficient, keeping the number of trainable parameters small. We show the merits of the proposed approach compared to state-of-the-art techniques on various tasks and show the improved zero shot domain transfer capability.",
      "arxiv_url": "https://arxiv.org/abs/2506.05629",
      "pdf_url": "https://arxiv.org/pdf/2506.05629",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01535",
      "title": "Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for Low-Resource Languages Using Bilingual Dictionaries",
      "authors": [
        "Haruki Sakajo",
        "Yusuke Ide",
        "Justin Vasselli",
        "Yusuke Sakai",
        "Yingtao Tian",
        "Hidetaka Kamigaito",
        "Taro Watanabe"
      ],
      "abstract": "Cross-lingual vocabulary transfer plays a promising role in adapting pre-trained language models to new languages, including low-resource languages. Existing approaches that utilize monolingual or parallel corpora face challenges when applied to languages with limited resources. In this work, we propose a simple yet effective vocabulary transfer method that utilizes bilingual dictionaries, which are available for many languages, thanks to descriptive linguists. Our proposed method leverages a property of BPE tokenizers where removing a subword from the vocabulary causes a fallback to shorter subwords. The embeddings of target subwords are estimated iteratively by progressively removing them from the tokenizer. The experimental results show that our approach outperforms existing methods for low-resource languages, demonstrating the effectiveness of a dictionary-based approach for cross-lingual vocabulary transfer.",
      "arxiv_url": "https://arxiv.org/abs/2506.01535",
      "pdf_url": "https://arxiv.org/pdf/2506.01535",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e69ef3c49619962f9dec43e7b2d9fcc82d892338",
      "title": "All That Glitters is Not Gold: Improving Robust Retrieval-Augmented Language Models with Fact-Centric Preference Alignment",
      "authors": [
        "Jia Hao",
        "Chunhong Zhang",
        "Jiarun Liu",
        "Haiyu Zhao",
        "Zhiqiang Zhan",
        "Zheng Hu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/e69ef3c49619962f9dec43e7b2d9fcc82d892338",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e6bc2108cab045879aacdc93e67d5a8732a18631",
      "title": "Masks Can be Learned as an Alternative to Experts",
      "authors": [
        "Peiyu Liu",
        "Tianwen Wei",
        "Bo Zhu",
        "Xin Zhao",
        "Shuicheng Yan"
      ],
      "abstract": "In this work, we investigate how to sparsify a pre-trained dense large language model into a mixture-of-experts (MoE) architecture for faster inference. Our approach applies mask matrix to the activations for each expert, constrained by L 0 regularization to minimize the number of activated parameters. To ensure minimal performance loss under this constraint, we initialize the model with all parameters active and progressively sparsify it during training. This approach proves more efficient than one-shot sparsification techniques, which typically require significant resources for performance recovery. Moreover, our approach automatically identifies shared, token-specific, and inactive experts, allowing for more efficient allocation of computational resources. Through extensive experiments, we achieve up to 97% performance retention on downstream tasks with only 50% of the feed-forward parameters activated in dense models. Beyond improving inference efficiency, this strategy of sharing computational units among experts provides a principled foundation for building more scalable and generalizable MoE architectures, paving the way for future expert-based model designs. Our code is available at https:// github.com/lpyhdzx/Mixture-of-Masks .",
      "arxiv_url": "https://www.semanticscholar.org/paper/e6bc2108cab045879aacdc93e67d5a8732a18631",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00479",
      "title": "EffiVLM-BENCH: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Vision-Language Models",
      "authors": [
        "Zekun Wang",
        "Minghua Ma",
        "Zexin Wang",
        "Rongchuan Mu",
        "Liping Shan",
        "Ming Liu",
        "Bing Qin"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have achieved remarkable success, yet their significant computational demands hinder practical deployment. While efforts to improve LVLM efficiency are growing, existing methods lack comprehensive evaluation across diverse backbones, benchmarks, and metrics. In this work, we systematically evaluate mainstream acceleration techniques for LVLMs, categorized into token and parameter compression. We introduce EffiVLM-Bench, a unified framework for assessing not only absolute performance but also generalization and loyalty, while exploring Pareto-optimal trade-offs. Our extensive experiments and in-depth analyses offer insights into optimal strategies for accelerating LVLMs. We open-source code and recipes for EffiVLM-Bench to foster future research.",
      "arxiv_url": "https://arxiv.org/abs/2506.00479",
      "pdf_url": "https://arxiv.org/pdf/2506.00479",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.17951",
      "title": "A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment",
      "authors": [
        "Quanwei Tang",
        "S. Lee",
        "Junshuang Wu",
        "Dong Zhang",
        "Shoushan Li",
        "Erik Cambria",
        "Guodong Zhou"
      ],
      "abstract": "Recent advancements in retrieval-augmented generation (RAG) have enhanced large language models in question answering by integrating external knowledge. However, challenges persist in achieving global understanding and aligning responses with human ethical and quality preferences. To address these issues, we propose GraphMPA, a comprehensive graph-based framework with mode-seeking preference alignment. Our approach constructs a hierarchical document graph using a general similarity measurement, mimicking human cognitive processes for information understanding and synthesis. Additionally, we introduce mode-seeking preference optimization to better align model outputs with human preferences through probability-matching constraints. Extensive experiments on six datasets demonstrate the effectiveness of our \\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.",
      "arxiv_url": "https://arxiv.org/abs/2506.17951",
      "pdf_url": "https://arxiv.org/pdf/2506.17951",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-06-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.14615",
      "title": "Unique Hard Attention: A Tale of Two Sides",
      "authors": [
        "Selim Jerad",
        "Anej Svete",
        "Jiaoda Li",
        "Ryan Cotterell"
      ],
      "abstract": "Understanding the expressive power of transformers has recently attracted attention, as it offers insights into their abilities and limitations. Many studies analyze unique hard attention transformers, where attention selects a single position that maximizes the attention scores. When multiple positions achieve the maximum score, either the rightmost or the leftmost of those is chosen. In this paper, we highlight the importance of this seeming triviality. Recently, finite-precision transformers with both leftmost- and rightmost-hard attention were shown to be equivalent to Linear Temporal Logic (LTL). We show that this no longer holds with only leftmost-hard attention -- in that case, they correspond to a \\emph{strictly weaker} fragment of LTL. Furthermore, we show that models with leftmost-hard attention are equivalent to \\emph{soft} attention, suggesting they may better approximate real-world transformers than right-attention models. These findings refine the landscape of transformer expressivity and underscore the role of attention directionality.",
      "arxiv_url": "https://arxiv.org/abs/2503.14615",
      "pdf_url": "https://arxiv.org/pdf/2503.14615",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.02088",
      "title": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models",
      "authors": [
        "Tian Lan",
        "Xiangdong Su",
        "Xu Liu",
        "Ruirui Wang",
        "Ke Chang",
        "Jiang Li",
        "Guanglai Gao"
      ],
      "abstract": "As large language models (LLMs) are increasingly applied to various NLP tasks, their inherent biases are gradually disclosed. Therefore, measuring biases in LLMs is crucial to mitigate its ethical risks. However, most existing bias evaluation datasets focus on English and North American culture, and their bias categories are not fully applicable to other cultures. The datasets grounded in the Chinese language and culture are scarce. More importantly, these datasets usually only support single evaluation tasks and cannot evaluate the bias from multiple aspects in LLMs. To address these issues, we present a Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias evaluation instances, covering 12 single bias categories, 82 subcategories and introducing 5 evaluation tasks, providing extensive category coverage, content diversity, and measuring comprehensiveness. Additionally, we evaluate several popular LLMs from different series and with parameter sizes. In general, all these LLMs demonstrated varying degrees of bias. We conduct an in-depth analysis of results, offering novel insights into bias in LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2507.02088",
      "pdf_url": "https://arxiv.org/pdf/2507.02088",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11387",
      "title": "RoleMRC: A Fine-Grained Composite Benchmark for Role-Playing and Instruction-Following",
      "authors": [
        "Junru Lu",
        "Jiazheng Li",
        "Guodong Shen",
        "Lin Gui",
        "Siyu An",
        "Yulan He",
        "Di Yin",
        "Xing Sun"
      ],
      "abstract": "Role-playing is important for Large Language Models (LLMs) to follow diverse instructions while maintaining role identity and the role's pre-defined ability limits. Existing role-playing datasets mostly contribute to controlling role style and knowledge boundaries, but overlook role-playing in instruction-following scenarios. We introduce a fine-grained role-playing and instruction-following composite benchmark, named RoleMRC, including: (1) Multi-turn dialogues between ideal roles and humans, including free chats or discussions upon given passages; (2) Role-playing machine reading comprehension, involving response, refusal, and attempts according to passage answerability and role ability; (3) More complex scenarios with nested, multi-turn and prioritized instructions. The final RoleMRC features a 10.2k role profile meta-pool, 37.9k well-synthesized role-playing instructions, and 1.4k testing samples. We develop a pipeline to quantitatively evaluate the fine-grained role-playing and instruction-following capabilities of several mainstream LLMs, as well as models that are fine-tuned on our data. Moreover, cross-evaluation on external role-playing datasets confirms that models fine-tuned on RoleMRC enhances instruction-following without compromising general role-playing and reasoning capabilities. We also probe the neural-level activation maps of different capabilities over post-tuned LLMs. Access to our RoleMRC, RoleMRC-mix and Codes: https://github.com/LuJunru/RoleMRC.",
      "arxiv_url": "https://arxiv.org/abs/2502.11387",
      "pdf_url": "https://arxiv.org/pdf/2502.11387",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.20946",
      "title": "Trace-of-Thought Prompting: Investigating Prompt-Based Knowledge Distillation Through Question Decomposition",
      "authors": [
        "Tyler McDonald",
        "Ali Emami"
      ],
      "abstract": "Knowledge distillation allows smaller neural networks to emulate the performance of larger, teacher models with reduced computational demands. Traditional methods for Large Language Models (LLMs) often necessitate extensive fine-tuning, which limits their accessibility. To address this, we introduce Trace-of-Thought Prompting, a novel framework designed to distill critical reasoning capabilities from high-resource teacher models (over 8 billion parameters) to low-resource student models (up to 8 billion parameters). This approach leverages problem decomposition to enhance interpretability and facilitate human-in-the-loop interventions. Empirical evaluations on the GSM8K and MATH datasets show that student models achieve accuracy gains of up to 113% on GSM8K and 21% on MATH, with significant improvements particularly notable in smaller models like Llama 2 and Zephyr. Our results suggest a promising pathway for open-source, low-resource models to eventually serve both as both students and teachers, potentially reducing our reliance on high-resource, proprietary models.",
      "arxiv_url": "https://arxiv.org/abs/2504.20946",
      "pdf_url": "https://arxiv.org/pdf/2504.20946",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15765",
      "title": "Generalized Attention Flow: Feature Attribution for Transformer Models via Maximum Flow",
      "authors": [
        "B. Azarkhalili",
        "Maxwell Libbrecht"
      ],
      "abstract": "This paper introduces Generalized Attention Flow (GAF), a novel feature attribution method for Transformer-based models to address the limitations of current approaches. By extending Attention Flow and replacing attention weights with the generalized Information Tensor, GAF integrates attention weights, their gradients, the maximum flow problem, and the barrier method to enhance the performance of feature attributions. The proposed method exhibits key theoretical properties and mitigates the shortcomings of prior techniques that rely solely on simple aggregation of attention weights. Our comprehensive benchmarking on sequence classification tasks demonstrates that a specific variant of GAF consistently outperforms state-of-the-art feature attribution methods in most evaluation settings, providing a more reliable interpretation of Transformer model outputs.",
      "arxiv_url": "https://arxiv.org/abs/2502.15765",
      "pdf_url": "https://arxiv.org/pdf/2502.15765",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e7b060fa888d7928b2c9e6dc13e5b57bb8364ee5",
      "title": "Let The Jury Decide: Fair Demonstration Selection for In-Context Learning through Incremental Greedy Evaluation",
      "authors": [
        "Sadaf Md. Halim",
        "Chen Zhao",
        "Xintao Wu",
        "Latifur Khan",
        "Christan Grant",
        "Fariha Ishrat Rahman",
        "Feng Chen"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/e7b060fa888d7928b2c9e6dc13e5b57bb8364ee5",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.22157",
      "title": "Training Language Model to Critique for Better Refinement",
      "authors": [
        "Tianshu Yu",
        "Chao Xiang",
        "Mingchuan Yang",
        "Pei Ke",
        "Bosi Wen",
        "Cunxiang Wang",
        "Jiale Cheng",
        "Li Zhang",
        "Xinyu Mu",
        "Chuxiong Sun",
        "Minlie Huang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable evaluation and critique capabilities, providing insightful feedback and identifying flaws in various tasks. However, limited research has explored which types of critiques are most effective for improving model responses or how to generate such critiques. To address this gap, we introduce \\textbf{R}efinement-oriented \\textbf{C}ritique \\textbf{O}ptimization (RCO), a novel framework designed to train critic models using refinement signals. RCO uses a feedback loop where critiques, generated by the critic model, guide the actor model in refining its responses. The critique utility (CU) quantifies the effectiveness of these refinements, serving as the reward signal for training the critic model. By focusing on critiques that lead to better refinements, RCO eliminates the need for direct critique preference assessment, ensuring that critiques driving meaningful improvements are rewarded. We evaluate RCO across five tasks, i.e., dialog generation, summarization, question answering, mathematical reasoning, and code generation, and show that it significantly outperforms traditional methods and open-source models in terms of critique quality and refinement outcomes. Our contributions include the introduction of RCO, a novel supervision scheme based on refined response preferences, and comprehensive experimental results that highlight the method's effectiveness in enhancing LLM critique-refinement loops.",
      "arxiv_url": "https://arxiv.org/abs/2506.22157",
      "pdf_url": "https://arxiv.org/pdf/2506.22157",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02519",
      "title": "Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning",
      "authors": [
        "Sohan Patnaik",
        "Milan Aggarwal",
        "Sumita Bhatia",
        "Balaji Krishnamurthy"
      ],
      "abstract": "LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions by generating step-by-step rationales. Prior works have utilized this capability to improve smaller and cheaper LMs (say, with 7B parameters). However, various practical constraints, such as copyright and legal issues, owing to lack of transparency in the pre-training data of large (often closed) models, prevent their use in commercial settings. Little focus has been given to improving the innate reasoning ability of smaller models without distilling information from larger LLMs. To address this, we propose COLLATE, a trainable framework that tunes a (small) LLM to generate those outputs from a pool of diverse rationales that selectively improves the downstream task. COLLATE enforces multiple instances of the same LLM to exhibit distinct behavior and employs them to generate rationales to obtain diverse outputs. The LLM is then tuned via preference optimization to choose the candidate rationale which maximizes the likelihood of ground-truth answer. COLLATE outperforms several trainable and prompting baselines on 5 datasets across 3 domains: maths problem solving, natural language inference, and commonsense reasoning. We show the eff icacy of COLLATE on LLMs from different model families across varying parameter scales (1B to 8B) and demonstrate the benefit of multiple rationale providers guided by the end task through ablations. Code is released here (https://github.com/Sohanpatnaik106/collate).",
      "arxiv_url": "https://arxiv.org/abs/2506.02519",
      "pdf_url": "https://arxiv.org/pdf/2506.02519",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e8b26a7a45d3651e0ad3e7f198bd58e10c7c5fb6",
      "title": "Improving Occupational ISCO Classification of Multilingual Swiss Job Postings with LLM-Refined Training Data",
      "authors": [
        "Ann-Sophie Gnehm",
        "Simon Clematide"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/e8b26a7a45d3651e0ad3e7f198bd58e10c7c5fb6",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.20238",
      "title": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving",
      "authors": [
        "Guizhen Chen",
        "Weiwen Xu",
        "Hao Zhang",
        "Hou Pong Chan",
        "Chaoqun Liu",
        "Li Bing",
        "Deli Zhao",
        "A. Luu",
        "Yu Rong"
      ],
      "abstract": "Many challenging reasoning tasks require not just rapid, intuitive responses, but a more deliberate, multi-step approach. Recent progress in large language models (LLMs) highlights an important shift from the\"System 1\"way of quick reactions to the\"System 2\"style of reflection-and-correction problem solving. However, current benchmarks heavily rely on the final-answer accuracy, leaving much of a model's intermediate reasoning steps unexamined. This fails to assess the model's ability to reflect and rectify mistakes within the reasoning process. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark for fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be decomposed into atomic steps, making it ideal for rigorous validation of intermediate correctness. Building on this, we introduce two tasks: state checking, and state transition, for a comprehensive evaluation of how models assess the current situation and plan the next move. To support broader research, we also provide a puzzle training set aimed at enhancing performance on general mathematical tasks. We show that models trained on our state checking and transition data demonstrate gains in math reasoning by up to 5.1% on GSM8K.",
      "arxiv_url": "https://arxiv.org/abs/2502.20238",
      "pdf_url": "https://arxiv.org/pdf/2502.20238",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11368",
      "title": "GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents",
      "authors": [
        "Lingxiao Diao",
        "Xinyue Xu",
        "Wanxuan Sun",
        "Cheng Yang",
        "Zhuosheng Zhang"
      ],
      "abstract": "Large language models (LLMs) have been widely deployed as autonomous agents capable of following user instructions and making decisions in real-world applications. Previous studies have made notable progress in benchmarking the instruction following capabilities of LLMs in general domains, with a primary focus on their inherent commonsense knowledge. Recently, LLMs have been increasingly deployed as domain-oriented agents, which rely on domain-oriented guidelines that may conflict with their commonsense knowledge. These guidelines exhibit two key characteristics: they consist of a wide range of domain-oriented rules and are subject to frequent updates. Despite these challenges, the absence of comprehensive benchmarks for evaluating the domain-oriented guideline following capabilities of LLMs presents a significant obstacle to their effective assessment and further development. In this paper, we introduce GuideBench, a comprehensive benchmark designed to evaluate guideline following performance of LLMs. GuideBench evaluates LLMs on three critical aspects: (i) adherence to diverse rules, (ii) robustness to rule updates, and (iii) alignment with human preferences. Experimental results on a range of LLMs indicate substantial opportunities for improving their ability to follow domain-oriented guidelines.",
      "arxiv_url": "https://arxiv.org/abs/2505.11368",
      "pdf_url": "https://arxiv.org/pdf/2505.11368",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22946",
      "title": "NegVQA: Can Vision Language Models Understand Negation?",
      "authors": [
        "Yuhui Zhang",
        "Yuchang Su",
        "Yiming Liu",
        "S. Yeung-Levy"
      ],
      "abstract": "Negation is a fundamental linguistic phenomenon that can entirely reverse the meaning of a sentence. As vision language models (VLMs) continue to advance and are deployed in high-stakes applications, assessing their ability to comprehend negation becomes essential. To address this, we introduce NegVQA, a visual question answering (VQA) benchmark consisting of 7,379 two-choice questions covering diverse negation scenarios and image-question distributions. We construct NegVQA by leveraging large language models to generate negated versions of questions from existing VQA datasets. Evaluating 20 state-of-the-art VLMs across seven model families, we find that these models struggle significantly with negation, exhibiting a substantial performance drop compared to their responses to the original questions. Furthermore, we uncover a U-shaped scaling trend, where increasing model size initially degrades performance on NegVQA before leading to improvements. Our benchmark reveals critical gaps in VLMs' negation understanding and offers insights into future VLM development. Project page available at https://yuhui-zh15.github.io/NegVQA/.",
      "arxiv_url": "https://arxiv.org/abs/2505.22946",
      "pdf_url": "https://arxiv.org/pdf/2505.22946",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11718",
      "title": "See the World, Discover Knowledge: A Chinese Factuality Evaluation for Large Vision Language Models",
      "authors": [
        "Jihao Gu",
        "Yingyao Wang",
        "Pi Bu",
        "Chen Wang",
        "Ziming Wang",
        "Tengtao Song",
        "Donglai Wei",
        "Jiale Yuan",
        "Yingxiu Zhao",
        "Yancheng He",
        "Shilong Li",
        "Jiaheng Liu",
        "Meng Cao",
        "Jun-chao Song",
        "Yingshui Tan",
        "Xiang Li",
        "Wenbo Su",
        "Xiaoyong Zhu",
        "Bo Zheng"
      ],
      "abstract": "The evaluation of factual accuracy in large vision language models (LVLMs) has lagged behind their rapid development, making it challenging to fully reflect these models' knowledge capacity and reliability. In this paper, we introduce the first factuality-based visual question-answering benchmark in Chinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of LVLMs across 8 major topics and 56 subtopics. The key features of this benchmark include a focus on the Chinese language, diverse knowledge types, a multi-hop question construction, high-quality data, static consistency, and easy-to-evaluate through short answers. Moreover, we contribute a rigorous data construction pipeline and decouple the visual factuality into two parts: seeing the world (i.e., object recognition) and discovering knowledge. This decoupling allows us to analyze the capability boundaries and execution mechanisms of LVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source models, revealing critical performance gaps within this field. Our evaluation-friendly code and data have already been open-sourced.",
      "arxiv_url": "https://arxiv.org/abs/2502.11718",
      "pdf_url": "https://arxiv.org/pdf/2502.11718",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e9261c8dd5cedff1d8ee3693963f6e349ed9de51",
      "title": "C2KD: Cross-layer and Cross-head Knowledge Distillation for Small Language Model-based Recommendation",
      "authors": [
        "Xiao Chen",
        "Changyi Ma",
        "Wenqi Fan",
        "Zhaoxiang Zhang",
        "Qing Li"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/e9261c8dd5cedff1d8ee3693963f6e349ed9de51",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.05507",
      "title": "Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?",
      "authors": [
        "Qing-Lin Liang",
        "Zhao Zhang",
        "Zeyu Sun",
        "Zheng Lin",
        "Qi Luo",
        "Yueyi Xiao",
        "Yizhou Chen",
        "Yuqun Zhang",
        "Haotian Zhang",
        "Lu Zhang",
        "Bin Chen",
        "Yingfei Xiong"
      ],
      "abstract": "Grammar serves as a cornerstone in programming languages and software engineering, providing frameworks to define the syntactic space and program structure. Existing research demonstrates the effectiveness of grammar-based code representations in small-scale models, showing their ability to reduce syntax errors and enhance performance. However, as language models scale to the billion level or beyond, syntax-level errors become rare, making it unclear whether grammar information still provides performance benefits. To explore this, we develop a series of billion-scale GrammarCoder models, incorporating grammar rules in the code generation process. Experiments on HumanEval (+) and MBPP (+) demonstrate a notable improvement in code generation accuracy. Further analysis shows that grammar-based representations enhance LLMs'ability to discern subtle code differences, reducing semantic errors caused by minor variations. These findings suggest that grammar-based code representations remain valuable even in billion-scale models, not only by maintaining syntax correctness but also by improving semantic differentiation.",
      "arxiv_url": "https://arxiv.org/abs/2503.05507",
      "pdf_url": "https://arxiv.org/pdf/2503.05507",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13550",
      "title": "STaR-SQL: Self-Taught Reasoner for Text-to-SQL",
      "authors": [
        "Mingqian He",
        "Yongliang Shen",
        "Wenqi Zhang",
        "Qiuying Peng",
        "Jun Wang",
        "Weiming Lu"
      ],
      "abstract": "Generating step-by-step\"chain-of-thought\"rationales has proven effective for improving the performance of large language models on complex reasoning tasks. However, applying such techniques to structured tasks, such as text-to-SQL, remains largely unexplored. In this paper, we introduce Self-Taught Reasoner for text-to-SQL (STaR-SQL), a novel approach that reframes SQL query generation as a reasoning-driven process. Our method prompts the LLM to produce detailed reasoning steps for SQL queries and fine-tunes it on rationales that lead to correct outcomes. Unlike traditional methods, STaR-SQL dedicates additional test-time computation to reasoning, thereby positioning LLMs as spontaneous reasoners rather than mere prompt-based agents. To further scale the inference process, we incorporate an outcome-supervised reward model (ORM) as a verifier, which enhances SQL query accuracy. Experimental results on the challenging Spider benchmark demonstrate that STaR-SQL significantly improves text-to-SQL performance, achieving an execution accuracy of 86.6%. This surpasses a few-shot baseline by 31.6% and a baseline fine-tuned to predict answers directly by 18.0%. Additionally, STaR-SQL outperforms agent-like prompting methods that leverage more powerful yet closed-source models such as GPT-4. These findings underscore the potential of reasoning-augmented training for structured tasks and open the door to extending self-improving reasoning models to text-to-SQL generation and beyond.",
      "arxiv_url": "https://arxiv.org/abs/2502.13550",
      "pdf_url": "https://arxiv.org/pdf/2502.13550",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00288",
      "title": "Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation",
      "authors": [
        "Ahmed Elhady",
        "Eneko Agirre",
        "Mikel Artetxe"
      ],
      "abstract": "Continued pretraining (CPT) is a popular approach to adapt existing large language models (LLMs) to new languages. When doing so, it is common practice to include a portion of English data in the mixture, but its role has not been carefully studied to date. In this work, we show that including English does not impact validation perplexity, yet it is critical for the emergence of downstream capabilities in the target language. We introduce a language-agnostic benchmark for in-context learning (ICL), which reveals catastrophic forgetting early on CPT when English is not included. This in turn damages the ability of the model to generalize to downstream prompts in the target language as measured by perplexity, even if it does not manifest in terms of accuracy until later in training, and can be tied to a big shift in the model parameters. Based on these insights, we introduce curriculum learning and exponential moving average (EMA) of weights as effective alternatives to mitigate the need for English. All in all, our work sheds light into the dynamics by which emergent abilities arise when doing CPT for language adaptation, and can serve as a foundation to design more effective methods in the future.",
      "arxiv_url": "https://arxiv.org/abs/2506.00288",
      "pdf_url": "https://arxiv.org/pdf/2506.00288",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12492",
      "title": "Boost, Disentangle, and Customize: A Robust System2-to-System1 Pipeline for Code Generation",
      "authors": [
        "Kounianhua Du",
        "Hanjing Wang",
        "Jianxing Liu",
        "Jizheng Chen",
        "Xinyi Dai",
        "Yasheng Wang",
        "Ruiming Tang",
        "Yong Yu",
        "Jun Wang",
        "Weinan Zhang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in various domains, particularly in system 1 tasks, yet the intricacies of their problem-solving mechanisms in system 2 tasks are not sufficiently explored. Recent research on System2-to-System1 methods surge, exploring the System 2 reasoning knowledge via inference-time computation and compressing the explored knowledge into System 1 process. In this paper, we focus on code generation, which is a representative System 2 task, and identify two primary challenges: (1) the complex hidden reasoning processes and (2) the heterogeneous data distributions that complicate the exploration and training of robust LLM solvers. To tackle these issues, we propose a novel BDC framework that explores insightful System 2 knowledge of LLMs using a MC-Tree-Of-Agents algorithm with mutual \\textbf{B}oosting, \\textbf{D}isentangles the heterogeneous training data for composable LoRA-experts, and obtain \\textbf{C}ustomized problem solver for each data instance with an input-aware hypernetwork to weight over the LoRA-experts, offering effectiveness, flexibility, and robustness. This framework leverages multiple LLMs through mutual verification and boosting, integrated into a Monte-Carlo Tree Search process enhanced by reflection-based pruning and refinement. Additionally, we introduce the DisenLora algorithm, which clusters heterogeneous data to fine-tune LLMs into composable Lora experts, enabling the adaptive generation of customized problem solvers through an input-aware hypernetwork. This work lays the groundwork for advancing LLM capabilities in complex reasoning tasks, offering a novel System2-to-System1 solution.",
      "arxiv_url": "https://arxiv.org/abs/2502.12492",
      "pdf_url": "https://arxiv.org/pdf/2502.12492",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03580",
      "title": "Automatically Suggesting Diverse Example Sentences for L2 Japanese Learners Using Pre-Trained Language Models",
      "authors": [
        "Enrico Benedetti",
        "Akiko Aizawa",
        "Florian Boudin"
      ],
      "abstract": "Providing example sentences that are diverse and aligned with learners' proficiency levels is essential for fostering effective language acquisition. This study examines the use of Pre-trained Language Models (PLMs) to produce example sentences targeting L2 Japanese learners. We utilize PLMs in two ways: as quality scoring components in a retrieval system that draws from a newly curated corpus of Japanese sentences, and as direct sentence generators using zero-shot learning. We evaluate the quality of sentences by considering multiple aspects such as difficulty, diversity, and naturalness, with a panel of raters consisting of learners of Japanese, native speakers -- and GPT-4. Our findings suggest that there is inherent disagreement among participants on the ratings of sentence qualities, except for difficulty. Despite that, the retrieval approach was preferred by all evaluators, especially for beginner and advanced target proficiency, while the generative approaches received lower scores on average. Even so, our experiments highlight the potential for using PLMs to enhance the adaptability of sentence suggestion systems and therefore improve the language learning journey.",
      "arxiv_url": "https://arxiv.org/abs/2506.03580",
      "pdf_url": "https://arxiv.org/pdf/2506.03580",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.09387",
      "title": "Truth Knows No Language: Evaluating Truthfulness Beyond English",
      "authors": [
        "B. Figueras",
        "Eneko Sagarzazu",
        "Julen Etxaniz",
        "Jeremy Barnes",
        "Pablo Gamallo",
        "Iria de-Dios-Flores",
        "R. Agerri"
      ],
      "abstract": "We introduce a professionally translated extension of the TruthfulQA benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and Spanish. Truthfulness evaluations of large language models (LLMs) have primarily been conducted in English. However, the ability of LLMs to maintain truthfulness across languages remains under-explored. Our study evaluates 12 state-of-the-art open LLMs, comparing base and instruction-tuned models using human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our findings reveal that, while LLMs perform best in English and worst in Basque (the lowest-resourced language), overall truthfulness discrepancies across languages are smaller than anticipated. Furthermore, we show that LLM-as-a-Judge correlates more closely with human judgments than multiple-choice metrics, and that informativeness plays a critical role in truthfulness assessment. Our results also indicate that machine translation provides a viable approach for extending truthfulness benchmarks to additional languages, offering a scalable alternative to professional translation. Finally, we observe that universal knowledge questions are better handled across languages than context- and time-dependent ones, highlighting the need for truthfulness evaluations that account for cultural and temporal variability. Dataset and code are publicly available under open licenses.",
      "arxiv_url": "https://arxiv.org/abs/2502.09387",
      "pdf_url": "https://arxiv.org/pdf/2502.09387",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11874",
      "title": "VAQUUM: Are Vague Quantifiers Grounded in Visual Data?",
      "authors": [
        "Hugh Mee Wong",
        "R. Nouwen",
        "Albert Gatt"
      ],
      "abstract": "Vague quantifiers such as\"a few\"and\"many\"are influenced by various contextual factors, including the number of objects present in a given context. In this work, we evaluate the extent to which vision-and-language models (VLMs) are compatible with humans when producing or judging the appropriateness of vague quantifiers in visual contexts. We release a novel dataset, VAQUUM, containing 20,300 human ratings on quantified statements across a total of 1089 images. Using this dataset, we compare human judgments and VLM predictions using three different evaluation methods. Our findings show that VLMs, like humans, are influenced by object counts in vague quantifier use. However, we find significant inconsistencies across models in different evaluation settings, suggesting that judging and producing vague quantifiers rely on two different processes.",
      "arxiv_url": "https://arxiv.org/abs/2502.11874",
      "pdf_url": "https://arxiv.org/pdf/2502.11874",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.05863",
      "title": "Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education",
      "authors": [
        "Yanhao Jia",
        "Xinyi Wu",
        "Hao Li",
        "Qinglin Zhang",
        "Yuxiao Hu",
        "Shuai Zhao",
        "Wenqi Fan"
      ],
      "abstract": "In AI-facilitated teaching, leveraging various query styles to interpret abstract text descriptions is crucial for ensuring high-quality teaching. However, current retrieval models primarily focus on natural text-image retrieval, making them insufficiently tailored to educational scenarios due to the ambiguities in the retrieval process. In this paper, we propose a diverse expression retrieval task tailored to educational scenarios, supporting retrieval based on multiple query styles and expressions. We introduce the STEM Education Retrieval Dataset (SER), which contains over 24,000 query pairs of different styles, and the Uni-Retrieval, an efficient and style-diversified retrieval vision-language model based on prompt tuning. Uni-Retrieval extracts query style features as prototypes and builds a continuously updated Prompt Bank containing prompt tokens for diverse queries. This bank can updated during test time to represent domain-specific knowledge for different subject retrieval scenarios. Our framework demonstrates scalability and robustness by dynamically retrieving prompt tokens based on prototype similarity, effectively facilitating learning for unknown queries. Experimental results indicate that Uni-Retrieval outperforms existing retrieval models in most retrieval tasks. This advancement provides a scalable and precise solution for diverse educational needs.",
      "arxiv_url": "https://arxiv.org/abs/2502.05863",
      "pdf_url": "https://arxiv.org/pdf/2502.05863",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20976",
      "title": "Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing",
      "authors": [
        "Peiming Guo",
        "Meishan Zhang",
        "Jianling Li",
        "Min Zhang",
        "Yue Zhang"
      ],
      "abstract": "Cross-domain constituency parsing is still an unsolved challenge in computational linguistics since the available multi-domain constituency treebank is limited. We investigate automatic treebank generation by large language models (LLMs) in this paper. The performance of LLMs on constituency parsing is poor, therefore we propose a novel treebank generation method, LLM back generation, which is similar to the reverse process of constituency parsing. LLM back generation takes the incomplete cross-domain constituency tree with only domain keyword leaf nodes as input and fills the missing words to generate the cross-domain constituency treebank. Besides, we also introduce a span-level contrastive learning pre-training strategy to make full use of the LLM back generation treebank for cross-domain constituency parsing. We verify the effectiveness of our LLM back generation treebank coupled with contrastive learning pre-training on five target domains of MCTB. Experimental results show that our approach achieves state-of-the-art performance on average results compared with various baselines.",
      "arxiv_url": "https://arxiv.org/abs/2505.20976",
      "pdf_url": "https://arxiv.org/pdf/2505.20976",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e9b33d78487815fee1e2db10bd874a2e05462c50",
      "title": "Decoding by Contrasting Knowledge: Enhancing Large Language Model Confidence on Edited Facts",
      "authors": [
        "Baolong Bi",
        "Shenghua Liu",
        "Lingrui Mei",
        "Yiwei Wang",
        "Junfeng Fang",
        "Pengliang Ji",
        "Xueqi Cheng"
      ],
      "abstract": "The knowledge within large language models (LLMs) may become outdated quickly. While in-context editing (ICE) is currently the most effective method for knowledge editing (KE), it is constrained by the black-box modeling of LLMs and thus lacks interpretability. Our work aims to elucidate the superior performance of ICE in KE by analyzing the impacts of in-context new knowledge on token-wise distributions. We observe that despite a significant boost in logits of the new knowledge, the performance of ICE is still hindered by stub-born knowledge. We propose a novel approach termed De coding by C ontrasting K nowledge (DeCK). DeCK derives the distribution of the next token by contrasting the logits obtained from the newly edited knowledge guided by ICE with those from the unedited parametric knowledge. Our experiments demonstrate that DeCK enhances the confidence of LLMs in edited facts. For instance, it improves the performance of LL A MA3-8B-INSTRUCT on MQ U AKE by up to 219%, demonstrating its capability to strengthen ICE. DeCK can be easily integrated into any ICE method as a decoding component to enhance editing capabilities. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/e9b33d78487815fee1e2db10bd874a2e05462c50",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02378",
      "title": "Exploring Explanations Improves the Robustness of In-Context Learning",
      "authors": [
        "Ukyo Honda",
        "Tatsushi Oka"
      ],
      "abstract": "In-context learning (ICL) has emerged as a successful paradigm for leveraging large language models (LLMs). However, it often struggles to generalize beyond the distribution of the provided demonstrations. A recent advancement in enhancing robustness is ICL with explanations (X-ICL), which improves prediction reliability by guiding LLMs to understand and articulate the reasoning behind correct labels. Building on this approach, we introduce an advanced framework that extends X-ICL by systematically exploring explanations for all possible labels (X$^2$-ICL), thereby enabling more comprehensive and robust decision-making. Experimental results on multiple natural language understanding datasets validate the effectiveness of X$^2$-ICL, demonstrating significantly improved robustness to out-of-distribution data compared to the existing ICL approaches.",
      "arxiv_url": "https://arxiv.org/abs/2506.02378",
      "pdf_url": "https://arxiv.org/pdf/2506.02378",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.13220",
      "title": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science",
      "authors": [
        "Jie Ying",
        "Zihong Chen",
        "Zhefan Wang",
        "Wanli Jiang",
        "Chenyang Wang",
        "Zhonghang Yuan",
        "Haoyang Su",
        "Huanjun Kong",
        "Fan Yang",
        "Nanqing Dong"
      ],
      "abstract": "Seed science is essential for modern agriculture, directly influencing crop yields and global food security. However, challenges such as interdisciplinary complexity and high costs with limited returns hinder progress, leading to a shortage of experts and insufficient technological support. While large language models (LLMs) have shown promise across various fields, their application in seed science remains limited due to the scarcity of digital resources, complex gene-trait relationships, and the lack of standardized benchmarks. To address this gap, we introduce SeedBench -- the first multi-task benchmark specifically designed for seed science. Developed in collaboration with domain experts, SeedBench focuses on seed breeding and simulates key aspects of modern breeding processes. We conduct a comprehensive evaluation of 26 leading LLMs, encompassing proprietary, open-source, and domain-specific fine-tuned models. Our findings not only highlight the substantial gaps between the power of LLMs and the real-world seed science problems, but also make a foundational step for research on LLMs for seed design.",
      "arxiv_url": "https://arxiv.org/abs/2505.13220",
      "pdf_url": "https://arxiv.org/pdf/2505.13220",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00900",
      "title": "SocialEval: Evaluating Social Intelligence of Large Language Models",
      "authors": [
        "Jinfeng Zhou",
        "Yuxuan Chen",
        "Yihan Shi",
        "Xuanming Zhang",
        "Leqi Lei",
        "Yi Feng",
        "Zexuan Xiong",
        "Miao Yan",
        "Xunzhi Wang",
        "Yaru Cao",
        "Jianing Yin",
        "Shuai Wang",
        "Quanyu Dai",
        "Zhenhua Dong",
        "Hongning Wang",
        "Minlie Huang"
      ],
      "abstract": "LLMs exhibit promising Social Intelligence (SI) in modeling human behavior, raising the need to evaluate LLMs' SI and their discrepancy with humans. SI equips humans with interpersonal abilities to behave wisely in navigating social interactions to achieve social goals. This presents an operational evaluation paradigm: outcome-oriented goal achievement evaluation and process-oriented interpersonal ability evaluation, which existing work fails to address. To this end, we propose SocialEval, a script-based bilingual SI benchmark, integrating outcome- and process-oriented evaluation by manually crafting narrative scripts. Each script is structured as a world tree that contains plot lines driven by interpersonal ability, providing a comprehensive view of how LLMs navigate social interactions. Experiments show that LLMs fall behind humans on both SI evaluations, exhibit prosociality, and prefer more positive social behaviors, even if they lead to goal failure. Analysis of LLMs' formed representation space and neuronal activations reveals that LLMs have developed ability-specific functional partitions akin to the human brain.",
      "arxiv_url": "https://arxiv.org/abs/2506.00900",
      "pdf_url": "https://arxiv.org/pdf/2506.00900",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18795",
      "title": "Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning in LMs",
      "authors": [
        "Xiulin Yang",
        "Tatsuya Aoyama",
        "Yuekun Yao",
        "E. Wilcox"
      ],
      "abstract": "Do language models (LMs) offer insights into human language learning? A common argument against this idea is that because their architecture and training paradigm are so vastly different from humans, LMs can learn arbitrary inputs as easily as natural languages. We test this claim by training LMs to model impossible and typologically unattested languages. Unlike previous work, which has focused exclusively on English, we conduct experiments on 12 languages from 4 language families with two newly constructed parallel corpora. Our results show that while GPT-2 small can largely distinguish attested languages from their impossible counterparts, it does not achieve perfect separation between all the attested languages and all the impossible ones. We further test whether GPT-2 small distinguishes typologically attested from unattested languages with different NP orders by manipulating word order based on Greenberg's Universal 20. We find that the model's perplexity scores do not distinguish attested vs. unattested word orders, while its performance on the generalization test does. These findings suggest that LMs exhibit some human-like inductive biases, though these biases are weaker than those found in human learners.",
      "arxiv_url": "https://arxiv.org/abs/2502.18795",
      "pdf_url": "https://arxiv.org/pdf/2502.18795",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "e9f5513be9537c62ec79b2e6ccfdfe85e805e78a",
      "title": "HyperCRS: Hypergraph-Aware Multi-Grained Preference Learning to Burst Filter Bubbles in Conversational Recommendation System",
      "authors": [
        "Yongsen Zheng",
        "Mingjie Qian",
        "Guohua Wang",
        "Yang Liu",
        "Ziliang Chen",
        "Mingzhi Mao",
        "Liang Lin",
        "Kwok-Yan Lam"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/e9f5513be9537c62ec79b2e6ccfdfe85e805e78a",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Personalization"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.08938",
      "title": "FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation",
      "authors": [
        "Qinggang Zhang",
        "Zhishang Xiang",
        "Yilin Xiao",
        "Le Wang",
        "Junhui Li",
        "Xinrun Wang",
        "Jinsong Su"
      ],
      "abstract": "Large language models (LLMs) augmented with retrieval systems have demonstrated significant potential in handling knowledge-intensive tasks. However, these models often struggle with unfaithfulness issues, generating outputs that either ignore the retrieved context or inconsistently blend it with the LLM`s parametric knowledge. This issue is particularly severe in cases of knowledge conflict, where the retrieved context conflicts with the model`s parametric knowledge. While existing faithful RAG approaches enforce strict context adherence through well-designed prompts or modified decoding strategies, our analysis reveals a critical limitation: they achieve faithfulness by forcibly suppressing the model`s parametric knowledge, which undermines the model`s internal knowledge structure and increases the risk of misinterpreting the context. To this end, this paper proposes FaithfulRAG, a novel framework that resolves knowledge conflicts by explicitly modeling discrepancies between the model`s parametric knowledge and retrieved context. Specifically, FaithfulRAG identifies conflicting knowledge at the fact level and designs a self-thinking process, allowing LLMs to reason about and integrate conflicting facts before generating responses. Extensive experiments demonstrate that our method outperforms state-of-the-art methods. The code is available at https://github.com/DeepLearnXMU/Faithful-RAG",
      "arxiv_url": "https://arxiv.org/abs/2506.08938",
      "pdf_url": "https://arxiv.org/pdf/2506.08938",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-06-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19941",
      "title": "Alleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimation",
      "authors": [
        "Xiang Geng",
        "Zhejian Lai",
        "Jiajun Chen",
        "Hao Yang",
        "Shujian Huang"
      ],
      "abstract": "Quality Estimation (QE) models evaluate the quality of machine translations without reference translations, serving as the reward models for the translation task. Due to the data scarcity, synthetic data generation has emerged as a promising solution. However, synthetic QE data often suffers from distribution shift, which can manifest as discrepancies between pseudo and real translations, or in pseudo labels that do not align with human preferences. To tackle this issue, we introduce DCSQE, a novel framework for alleviating distribution shift in synthetic QE data. To reduce the difference between pseudo and real translations, we employ the constrained beam search algorithm and enhance translation diversity through the use of distinct generation models. DCSQE uses references, i.e., translation supervision signals, to guide both the generation and annotation processes, enhancing the quality of token-level labels. DCSQE further identifies the shortest phrase covering consecutive error tokens, mimicking human annotation behavior, to assign the final phrase-level labels. Specially, we underscore that the translation model can not annotate translations of itself accurately. Extensive experiments demonstrate that DCSQE outperforms SOTA baselines like CometKiwi in both supervised and unsupervised settings. Further analysis offers insights into synthetic data generation that could benefit reward models for other tasks. The code is available at https://github.com/NJUNLP/njuqe.",
      "arxiv_url": "https://arxiv.org/abs/2502.19941",
      "pdf_url": "https://arxiv.org/pdf/2502.19941",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.03303",
      "title": "SEOE: A Scalable and Reliable Semantic Evaluation Framework for Open Domain Event Detection",
      "authors": [
        "Yi-Fan Lu",
        "Xian-Ling Mao",
        "Tian Lan",
        "Tong Zhang",
        "Yu-Shi Zhu",
        "Heyan Huang"
      ],
      "abstract": "Automatic evaluation for Open Domain Event Detection (ODED) is a highly challenging task, because ODED is characterized by a vast diversity of un-constrained output labels from various domains. Nearly all existing evaluation methods for ODED usually first construct evaluation benchmarks with limited labels and domain coverage, and then evaluate ODED methods using metrics based on token-level label matching rules. However, this kind of evaluation framework faces two issues: (1) The limited evaluation benchmarks lack representatives of the real world, making it difficult to accurately reflect the performance of various ODED methods in real-world scenarios; (2) Evaluation metrics based on token-level matching rules fail to capture semantic similarity between predictions and golden labels. To address these two problems above, we propose a scalable and reliable Semantic-level Evaluation framework for Open domain Event detection (SEOE) by constructing a more representative evaluation benchmark and introducing a semantic evaluation metric. Specifically, our proposed framework first constructs a scalable evaluation benchmark that currently includes 564 event types covering 7 major domains, with a cost-effective supplementary annotation strategy to ensure the benchmark's representativeness. The strategy also allows for the supplement of new event types and domains in the future. Then, the proposed SEOE leverages large language models (LLMs) as automatic evaluation agents to compute a semantic F1-score, incorporating fine-grained definitions of semantically similar labels to enhance the reliability of the evaluation. Extensive experiments validate the representatives of the benchmark and the reliability of the semantic evaluation metric. Existing ODED methods are thoroughly evaluated, and the error patterns of predictions are analyzed, revealing several insightful findings.",
      "arxiv_url": "https://arxiv.org/abs/2503.03303",
      "pdf_url": "https://arxiv.org/pdf/2503.03303",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.11080",
      "title": "MANBench: Is Your Multimodal Model Smarter than Human?",
      "authors": [
        "Han Zhou",
        "Qitong Xu",
        "Yiheng Dong",
        "Xin Yang"
      ],
      "abstract": "The rapid advancement of Multimodal Large Language Models (MLLMs) has ignited discussions regarding their potential to surpass human performance in multimodal tasks. In response, we introduce MANBench (Multimodal Ability Norms Benchmark), a bilingual benchmark (English and Chinese) comprising 1,314 questions across nine tasks, spanning knowledge-based and non-knowledge-based domains. MANBench emphasizes intuitive reasoning, seamless cross-modal integration, and real-world complexity, providing a rigorous evaluation framework. Through extensive human experiments involving diverse participants, we compared human performance against state-of-the-art MLLMs. The results indicate that while MLLMs excel in tasks like Knowledge and Text-Image Understanding, they struggle with deeper cross-modal reasoning tasks such as Transmorphic Understanding, Image Consistency, and Multi-image Understanding. Moreover, both humans and MLLMs face challenges in highly complex tasks like Puzzles and Spatial Imagination. MANBench highlights the strengths and limitations of MLLMs, revealing that even advanced models fall short of achieving human-level performance across many domains. We hope MANBench will inspire efforts to bridge the gap between MLLMs and human multimodal capabilities. The code and dataset are available at https://github.com/micdz/MANBench.",
      "arxiv_url": "https://arxiv.org/abs/2506.11080",
      "pdf_url": "https://arxiv.org/pdf/2506.11080",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ea63c5554ec6443e8eaa37a078f447cfe5394e8b",
      "title": "Amplifying Trans and Nonbinary Voices: A Community-Centred Harm Taxonomy for LLMs",
      "authors": [
        "Eddie L. Ungless",
        "Sunipa Dev",
        "Cynthia L. Bennett",
        "Rebecca Gulotta",
        "Jasmijn Bastings",
        "Remi Denton"
      ],
      "abstract": "Warning: some of the example prompts given in this paper are offensive including slurs. These are intended to illustrate potential harms. We explore large language model (LLM) responses that may negatively impact the transgender and nonbinary (TGNB) community and introduce the Transing Transform-ers Toolkit, T 3 , which provides resources for identifying such harmful response behaviors. The heart of T 3 is a community-centred taxonomy of harms, developed in collaboration with the TGNB community, which we complement with, amongst other guidance, suggested heuristics for evaluation. To develop the taxonomy, we adopted a multi-method approach that included surveys and focus groups with community experts. The contribution highlights the importance of community-centred approaches in mitigating harm, and outlines pathways for LLM developers to improve how their models handle TGNB-related topics.",
      "arxiv_url": "https://www.semanticscholar.org/paper/ea63c5554ec6443e8eaa37a078f447cfe5394e8b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.06670",
      "title": "STARS: A Unified Framework for Singing Transcription, Alignment, and Refined Style Annotation",
      "authors": [
        "Wenxiang Guo",
        "Yu Zhang",
        "Changhao Pan",
        "Zhiyuan Zhu",
        "Ruiqi Li",
        "Zhetao Chen",
        "Wenhao Xu",
        "Fei Wu",
        "Zhou Zhao"
      ],
      "abstract": "Recent breakthroughs in singing voice synthesis (SVS) have heightened the demand for high-quality annotated datasets, yet manual annotation remains prohibitively labor-intensive and resource-intensive. Existing automatic singing annotation (ASA) methods, however, primarily tackle isolated aspects of the annotation pipeline. To address this fundamental challenge, we present STARS, which is, to our knowledge, the first unified framework that simultaneously addresses singing transcription, alignment, and refined style annotation. Our framework delivers comprehensive multi-level annotations encompassing: (1) precise phoneme-audio alignment, (2) robust note transcription and temporal localization, (3) expressive vocal technique identification, and (4) global stylistic characterization including emotion and pace. The proposed architecture employs hierarchical acoustic feature processing across frame, word, phoneme, note, and sentence levels. The novel non-autoregressive local acoustic encoders enable structured hierarchical representation learning. Experimental validation confirms the framework's superior performance across multiple evaluation dimensions compared to existing annotation approaches. Furthermore, applications in SVS training demonstrate that models utilizing STARS-annotated data achieve significantly enhanced perceptual naturalness and precise style control. This work not only overcomes critical scalability challenges in the creation of singing datasets but also pioneers new methodologies for controllable singing voice synthesis. Audio samples are available at https://gwx314.github.io/stars-demo/.",
      "arxiv_url": "https://arxiv.org/abs/2507.06670",
      "pdf_url": "https://arxiv.org/pdf/2507.06670",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.07572",
      "title": "WebWalker: Benchmarking LLMs in Web Traversal",
      "authors": [
        "Jialong Wu",
        "Wenbiao Yin",
        "Yong Jiang",
        "Zhenglin Wang",
        "Zekun Xi",
        "Runnan Fang",
        "Deyu Zhou",
        "Pengjun Xie",
        "Fei Huang"
      ],
      "abstract": "Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address it, we introduce WebWalkerQA, a benchmark designed to assess the ability of LLMs to perform web traversal. It evaluates the capacity of LLMs to traverse a website's subpages to extract high-quality data systematically. We propose WebWalker, which is a multi-agent framework that mimics human-like web navigation through an explore-critic paradigm. Extensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through the horizontal and vertical integration in real-world scenarios.",
      "arxiv_url": "https://arxiv.org/abs/2501.07572",
      "pdf_url": "https://arxiv.org/pdf/2501.07572",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM",
        "RAG"
      ],
      "published_date": "2025-01-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11799",
      "title": "Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning",
      "authors": [
        "Peiying Yu",
        "Guoxin Chen",
        "Jingjing Wang"
      ],
      "abstract": "Despite the remarkable capabilities of large language models (LLMs) in various reasoning tasks, they still struggle with table reasoning tasks, particularly in maintaining consistency throughout multi-step reasoning processes. While existing approaches have explored various decomposition strategies, they often lack effective mechanisms to identify and correct errors in intermediate reasoning steps, leading to cascading error propagation. To address these issues, we propose Table-Critic, a novel multi-agent framework that facilitates collaborative criticism and iterative refinement of the reasoning process until convergence to correct solutions. Our framework consists of four specialized agents: a Judge for error identification, a Critic for comprehensive critiques, a Refiner for process improvement, and a Curator for pattern distillation. To effectively deal with diverse and unpredictable error types, we introduce a self-evolving template tree that systematically accumulates critique knowledge through experience-driven learning and guides future reflections. Extensive experiments have demonstrated that Table-Critic achieves substantial improvements over existing methods, achieving superior accuracy and error correction rates while maintaining computational efficiency and lower solution degradation rate.",
      "arxiv_url": "https://arxiv.org/abs/2502.11799",
      "pdf_url": "https://arxiv.org/pdf/2502.11799",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.05352",
      "title": "Achieving binary weight and activation for LLMs using Post-Training Quantization",
      "authors": [
        "Siqing Song",
        "Chuang Wang",
        "Ruiqi Wang",
        "Yi Yang",
        "Xuyao Zhang"
      ],
      "abstract": "Quantizing large language models (LLMs) to 1-bit precision significantly reduces computational costs, but existing quantization techniques suffer from noticeable performance degradation when using weight and activation precisions below 4 bits (W4A4). In this paper, we propose a post-training quantization framework with W(1+1)A(1*4) configuration, where weights are quantized to 1 bit with an additional 1 bit for fine-grain grouping and activations are quantized to 1 bit with a 4-fold increase in the number of channels. For weight quantization, we propose utilizing Hessian-aware fine-grained grouping along with an EM-based quantization scheme. For activation quantization, we decompose INT4-quantized activations into a 4 * INT1 format equivalently and simultaneously smooth the scaling factors based on quantization errors, which further reduces the quantization errors in activations. Our method surpasses state-of-the-art (SOTA) LLM quantization baselines on W2A4 across multiple tasks, pushing the boundaries of existing LLM quantization methods toward fully binarized models. Code is available at https://github.com/JimmyCrave/LLM-PTQ-binarization.",
      "arxiv_url": "https://arxiv.org/abs/2504.05352",
      "pdf_url": "https://arxiv.org/pdf/2504.05352",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04647",
      "title": "Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment",
      "authors": [
        "Wen Yang",
        "Junhong Wu",
        "Chen Wang",
        "Chengqing Zong",
        "Jiajun Zhang"
      ],
      "abstract": "Direct Preference Optimization (DPO) has become a prominent method for aligning Large Language Models (LLMs) with human preferences. While DPO has enabled significant progress in aligning English LLMs, multilingual preference alignment is hampered by data scarcity. To address this, we propose a novel approach that $\\textit{captures}$ learned preferences from well-aligned English models by implicit rewards and $\\textit{transfers}$ them to other languages through iterative training. Specifically, we derive an implicit reward model from the logits of an English DPO-aligned model and its corresponding reference model. This reward model is then leveraged to annotate preference relations in cross-lingual instruction-following pairs, using English instructions to evaluate multilingual responses. The annotated data is subsequently used for multilingual DPO fine-tuning, facilitating preference knowledge transfer from English to other languages. Fine-tuning Llama3 for two iterations resulted in a 12.72% average improvement in Win Rate and a 5.97% increase in Length Control Win Rate across all training languages on the X-AlpacaEval leaderboard. Our findings demonstrate that leveraging existing English-aligned models can enable efficient and effective multilingual preference alignment, significantly reducing the need for extensive multilingual preference data. The code is available at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding",
      "arxiv_url": "https://arxiv.org/abs/2503.04647",
      "pdf_url": "https://arxiv.org/pdf/2503.04647",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17956",
      "title": "Towards Better Understanding of Program-of-Thought Reasoning in Cross-Lingual and Multilingual Environments",
      "authors": [
        "Patomporn Payoungkhamdee",
        "Pume Tuchinda",
        "Jinheon Baek",
        "Samuel Cahyawijaya",
        "Can Udomcharoenchaikit",
        "Potsawee Manakul",
        "Peerat Limkonchotiwat",
        "Ekapol Chuangsuwanich",
        "Sarana Nutanong"
      ],
      "abstract": "Multi-step reasoning is essential for large language models (LLMs), yet multilingual performance remains challenging. While Chain-of-Thought (CoT) prompting improves reasoning, it struggles with non-English languages due to the entanglement of reasoning and execution. Program-of-Thought (PoT) prompting separates reasoning from execution, offering a promising alternative but shifting the challenge to generating programs from non-English questions. We propose a framework to evaluate PoT by separating multilingual reasoning from code execution to examine (i) the impact of fine-tuning on question-reasoning alignment and (ii) how reasoning quality affects answer correctness. Our findings demonstrate that PoT fine-tuning substantially enhances multilingual reasoning, outperforming CoT fine-tuned models. We further demonstrate a strong correlation between reasoning quality (measured through code quality) and answer accuracy, highlighting its potential as a test-time performance improvement heuristic.",
      "arxiv_url": "https://arxiv.org/abs/2502.17956",
      "pdf_url": "https://arxiv.org/pdf/2502.17956",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11683",
      "title": "Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation",
      "authors": [
        "Susanna Rücker",
        "A. Akbik"
      ],
      "abstract": "Entity disambiguation (ED) is the task of linking mentions in text to corresponding entries in a knowledge base. Dual Encoders address this by embedding mentions and label candidates in a shared embedding space and applying a similarity metric to predict the correct label. In this work, we focus on evaluating key design decisions for Dual Encoder-based ED, such as its loss function, similarity metric, label verbalization format, and negative sampling strategy. We present the resulting model VerbalizED, a document-level Dual Encoder model that includes contextual label verbalizations and efficient hard negative sampling. Additionally, we explore an iterative prediction variant that aims to improve the disambiguation of challenging data points. Comprehensive experiments on AIDA-Yago validate the effectiveness of our approach, offering insights into impactful design choices that result in a new State-of-the-Art system on the ZELDA benchmark.",
      "arxiv_url": "https://arxiv.org/abs/2505.11683",
      "pdf_url": "https://arxiv.org/pdf/2505.11683",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13928",
      "title": "Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images",
      "authors": [
        "Shengguang Wu",
        "Fan-Yun Sun",
        "Kaiyue Wen",
        "Nick Haber"
      ],
      "abstract": "Recent studies have shown that Large Vision-Language Models (VLMs) tend to neglect image content and over-rely on language-model priors, resulting in errors in visually grounded tasks and hallucinations. We hypothesize that this issue arises because existing VLMs are not explicitly trained to generate texts that are accurately grounded in fine-grained image details. To enhance visual feedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive Optimization), a novel finetuning objective that steers the model toward capturing important visual details and aligning them with corresponding text tokens. To further facilitate this detailed alignment, we introduce MVC, a paired image-text dataset built by automatically filtering and augmenting visual counterfactual data to challenge the model with hard contrastive cases involving Minimal Visual Contrasts. Experiments show that our method consistently improves VLM performance across diverse benchmarks covering various abilities and domains, achieving up to a 22% reduction in hallucinations, and significant gains in vision-centric and general tasks. Notably, these improvements become increasingly pronounced in benchmarks with higher visual dependency. In short, S-VCO offers a significant enhancement of VLM's visually-dependent task performance while retaining or even improving the model's general abilities. We opensource our code at https://s-vco.github.io/",
      "arxiv_url": "https://arxiv.org/abs/2502.13928",
      "pdf_url": "https://arxiv.org/pdf/2502.13928",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "eb9747aa52900da8b3ab8f5c3271f2416e634195",
      "title": "Large Language Models Struggle to Describe the Haystack without Human Help: A Social Science-Inspired Evaluation of Topic Models",
      "authors": [
        "Zongxia Li",
        "Lorena Calvo-Bartolomé",
        "A. Hoyle",
        "Paiheng Xu",
        "D. Stephens",
        "J. Fung",
        "Alden Dima",
        "J. Boyd-Graber"
      ],
      "abstract": "A common use of NLP by social scientists is to understand large document collections. Recent data exploration and content analysis have shifted from probabilistic topic models to Large Language Models ( LLM s). Yet their effectiveness in helping users understand content in real-world applications remains under explored. This study compares the knowledge users gain from unsupervised LLM s, supervised LLM s, and traditional topic models across two datasets. While unsupervised LLM s generate more human-readable topics, their topics are overly generic for domain-specific datasets and do not help users learn much about the documents. Adding human supervision to LLM generation improves data exploration by mitigating hallucination and over-genericity but requires greater human effort. Traditional topic models, such as Latent Dirichlet Allocation ( LDA ), remain effective for exploration but are less user-friendly. LLM s struggle to describe the haystack of large corpora without human help, particularly domain-specific data, and face scaling and hallucination limitations due to context length constraints. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/eb9747aa52900da8b3ab8f5c3271f2416e634195",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.11953",
      "title": "Proverbs Run in Pairs: Evaluating Proverb Translation Capability of Large Language Model",
      "authors": [
        "Minghan Wang",
        "Viet Pham",
        "Farhad Moghimifar",
        "Thuy-Trang Vu"
      ],
      "abstract": "Despite achieving remarkable performance, machine translation (MT) research remains underexplored in terms of translating cultural elements in languages, such as idioms, proverbs, and colloquial expressions. This paper investigates the capability of state-of-the-art neural machine translation (NMT) and large language models (LLMs) in translating proverbs, which are deeply rooted in cultural contexts. We construct a translation dataset of standalone proverbs and proverbs in conversation for four language pairs. Our experiments show that the studied models can achieve good translation between languages with similar cultural backgrounds, and LLMs generally outperform NMT models in proverb translation. Furthermore, we find that current automatic evaluation metrics such as BLEU, CHRF++ and COMET are inadequate for reliably assessing the quality of proverb translation, highlighting the need for more culturally aware evaluation metrics.",
      "arxiv_url": "https://arxiv.org/abs/2501.11953",
      "pdf_url": "https://arxiv.org/pdf/2501.11953",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04083",
      "title": "A Generative Adaptive Replay Continual Learning Model for Temporal Knowledge Graph Reasoning",
      "authors": [
        "Zhiyu Zhang",
        "Wei Chen",
        "Youfang Lin",
        "Huaiyu Wan"
      ],
      "abstract": "Recent Continual Learning (CL)-based Temporal Knowledge Graph Reasoning (TKGR) methods focus on significantly reducing computational cost and mitigating catastrophic forgetting caused by fine-tuning models with new data. However, existing CL-based TKGR methods still face two key limitations: (1) They usually one-sidedly reorganize individual historical facts, while overlooking the historical context essential for accurately understanding the historical semantics of these facts; (2) They preserve historical knowledge by simply replaying historical facts, while ignoring the potential conflicts between historical and emerging facts. In this paper, we propose a Deep Generative Adaptive Replay (DGAR) method, which can generate and adaptively replay historical entity distribution representations from the whole historical context. To address the first challenge, historical context prompts as sampling units are built to preserve the whole historical context information. To overcome the second challenge, a pre-trained diffusion model is adopted to generate the historical distribution. During the generation process, the common features between the historical and current distributions are enhanced under the guidance of the TKGR model. In addition, a layer-by-layer adaptive replay mechanism is designed to effectively integrate historical and current distributions. Experimental results demonstrate that DGAR significantly outperforms baselines in reasoning and mitigating forgetting.",
      "arxiv_url": "https://arxiv.org/abs/2506.04083",
      "pdf_url": "https://arxiv.org/pdf/2506.04083",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ebeac400b88e99ad532e1a0efcfe9b00989c336b",
      "title": "VISIAR: Empower MLLM for Visual Story Ideation",
      "authors": [
        "Zhaoyang Xia",
        "Somdeb Sarkhel",
        "Md Mehrab Tanjim",
        "Stefano Petrangeli",
        "Ishita Dasgupta",
        "Yuxiao Chen",
        "Jinxuan Xu",
        "Di Liu",
        "Saayan Mitra",
        "Dimitris N. Metaxas"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/ebeac400b88e99ad532e1a0efcfe9b00989c336b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ebf1e9cc6a3841471bc597d0044bc8807bdc6202",
      "title": "\"What do you call a dog that is incontrovertibly true? Dogma\": Testing LLM Generalization through Humor",
      "authors": [
        "Alessio Cocchieri",
        "Luca Ragazzi",
        "Paolo Italiani",
        "Giuseppe Tagliavini",
        "Gianluca Moro"
      ],
      "abstract": "Humor, requiring creativity and contextual understanding, is a hallmark of human intelligence, showcasing adaptability across linguistic scenarios. While recent advances in large language models (LLMs) demonstrate strong reasoning on various benchmarks, it remains unclear whether they truly adapt to new tasks like humans (i.e., generalize) or merely replicate memorized content. To explore this, we introduce P HUNNY , a new humor-based question-answering benchmark designed to assess LLMs’ reasoning through carefully crafted puns. Our dataset is manually curated to ensure novelty and minimize data contamination, providing a robust evaluation of LLMs’ linguistic comprehension. Experiments on pun comprehension, resolution, and generation reveal that most LLMs struggle with generalization, even on simple tasks, consistently under-performing the human baseline. Additionally, our detailed error analysis provides valuable insights to guide future research. 1",
      "arxiv_url": "https://www.semanticscholar.org/paper/ebf1e9cc6a3841471bc597d0044bc8807bdc6202",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ec11a89a36c83f3d9dbf1347f7c51e0e738cc15b",
      "title": "Tracing and Dissecting How LLMs Recall Factual Knowledge for Real World Questions",
      "authors": [
        "Yiqun Wang",
        "Chaoqun Wan",
        "Sile Hu",
        "Yonggang Zhang",
        "Xiang Tian",
        "Yaowu Chen",
        "Xu Shen",
        "Jieping Ye"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have shown promising ability to perform commonsense reasoning, bringing machines closer to human-like understanding. However, deciphering the internal reasoning processes of LLMs remains challenging due to the complex interdependencies among generated tokens, especially in practical question-answering. In this study, we introduce a two-dimensional analysis framework—comprising token back-tracing and individual token decod-ing—to uncover how LLMs conduct factual knowledge recall. Through explanatory analysis of three typical reasoning datasets, we identify a consistent three-phase pattern: Subject Augmentation and Broadcasting, Object Retrieval and Reranking, and Conclusion Fusion and Generation. Our findings reveal that LLMs do not lack relevant knowledge but struggle to select the most accurate information based on context during the retrieval and rerank phase. Leveraging these findings, we apply representation engineering and selective fine-tuning to target specific modules responsible for retrieval and rerank errors. Experimental results show large improvements in response accuracy for both in-domain and out-of-domain settings, validating the rationality of the interpreting result.",
      "arxiv_url": "https://www.semanticscholar.org/paper/ec11a89a36c83f3d9dbf1347f7c51e0e738cc15b",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03101",
      "title": "Beyond Text Compression: Evaluating Tokenizers Across Scales",
      "authors": [
        "Jonas F. Lotz",
        "Ant'onio V. Lopes",
        "Stephan Peitz",
        "Hendra Setiawan",
        "Leonardo Emili"
      ],
      "abstract": "The choice of tokenizer can profoundly impact language model performance, yet accessible and reliable evaluations of tokenizer quality remain an open challenge. Inspired by scaling consistency, we show that smaller models can accurately predict significant differences in tokenizer impact on larger models at a fraction of the compute cost. By systematically evaluating both English-centric and multilingual tokenizers, we find that tokenizer choice has negligible effects on tasks in English but results in consistent performance differences in multilingual settings. We propose new intrinsic tokenizer metrics inspired by Zipf's law that correlate more strongly with downstream performance than text compression when modeling unseen languages. By combining several metrics to capture multiple aspects of tokenizer behavior, we develop a reliable framework for intrinsic tokenizer evaluations. Our work offers a more efficient path to informed tokenizer selection in future language model development.",
      "arxiv_url": "https://arxiv.org/abs/2506.03101",
      "pdf_url": "https://arxiv.org/pdf/2506.03101",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.11953",
      "title": "IAM: Efficient Inference through Attention Mapping between Different-scale LLMs",
      "authors": [
        "Yi Zhao",
        "Z. Li",
        "Hai Zhao"
      ],
      "abstract": "LLMs encounter significant challenges in resource consumption nowadays, especially with long contexts. Despite extensive efforts dedicate to enhancing inference efficiency, these methods primarily exploit internal sparsity within the models, without leveraging external information for optimization. We identify the high similarity of attention matrices across different-scale LLMs, which offers a novel perspective for optimization. We first conduct a comprehensive analysis of how to measure similarity, how to select mapping Layers and whether mapping is consistency. Based on these insights, we introduce the IAM framework, which achieves dual benefits of accelerated attention computation and reduced KV cache usage by performing attention mapping between small and large LLMs. Our experimental results demonstrate that IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without appreciably sacrificing performance. Experiments on different series of models show the generalizability of IAM. Importantly, it is also orthogonal to many existing KV cache optimization methods, making it a versatile addition to the current toolkit for enhancing LLM efficiency.",
      "arxiv_url": "https://arxiv.org/abs/2507.11953",
      "pdf_url": "https://arxiv.org/pdf/2507.11953",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11862",
      "title": "Understanding In-Context Machine Translation for Low-Resource Languages: A Case Study on Manchu",
      "authors": [
        "Renhao Pei",
        "Yihong Liu",
        "Peiqin Lin",
        "Franccois Yvon",
        "Hinrich Schütze"
      ],
      "abstract": "In-context machine translation (MT) with large language models (LLMs) is a promising approach for low-resource MT, as it can readily take advantage of linguistic resources such as grammar books and dictionaries. Such resources are usually selectively integrated into the prompt so that LLMs can directly perform translation without any specific training, via their in-context learning capability (ICL). However, the relative importance of each type of resource, e.g., dictionary, grammar book, and retrieved parallel examples, is not entirely clear. To address this gap, this study systematically investigates how each resource and its quality affect the translation performance, with the Manchu language as our case study. To remove any prior knowledge of Manchu encoded in the LLM parameters and single out the effect of ICL, we also experiment with an enciphered version of Manchu texts. Our results indicate that high-quality dictionaries and good parallel examples are very helpful, while grammars hardly help. In a follow-up study, we showcase a promising application of in-context MT: parallel data augmentation as a way to bootstrap a conventional MT model. When monolingual data abound, generating synthetic parallel data through in-context MT offers a pathway to mitigate data scarcity and build effective and efficient low-resource neural MT systems.",
      "arxiv_url": "https://arxiv.org/abs/2502.11862",
      "pdf_url": "https://arxiv.org/pdf/2502.11862",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10945",
      "title": "Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer",
      "authors": [
        "Seungyoon Lee",
        "Seongtae Hong",
        "Hyeonseok Moon",
        "Heu-Jeoung Lim"
      ],
      "abstract": "Large Language Models (LLMs) increasingly incorporate multilingual capabilities, fueling the demand to transfer them into target language-specific models. However, most approaches, which blend the source model's embedding by replacing the source vocabulary with the target language-specific vocabulary, may constrain expressive capacity in the target language since the source model is predominantly trained on English data. In this paper, we propose Semantic Aware Linear Transfer (SALT), a novel cross-lingual transfer technique that recycles embeddings from target language Pre-trained Language Models (PLMs) to transmit the deep representational strengths of PLM-derived embedding to LLMs. SALT derives unique regression lines based on the similarity in the overlap of the source and target vocabularies, to handle each non-overlapping token's embedding space. Our extensive experiments show that SALT significantly outperforms other transfer methods and achieves lower loss with accelerating faster convergence during language adaptation. Notably, SALT obtains remarkable performance in cross-lingual understanding setups compared to other methods. Furthermore, we highlight the scalable use of PLMs to enhance the functionality of contemporary LLMs by conducting experiments with varying architectures.",
      "arxiv_url": "https://arxiv.org/abs/2505.10945",
      "pdf_url": "https://arxiv.org/pdf/2505.10945",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01714",
      "title": "Word Form Matters: LLMs' Semantic Reconstruction under Typoglycemia",
      "authors": [
        "Chenxi Wang",
        "Tianle Gu",
        "Zhongyu Wei",
        "Lang Gao",
        "Zirui Song",
        "Xiuying Chen"
      ],
      "abstract": "Human readers can efficiently comprehend scrambled words, a phenomenon known as Typoglycemia, primarily by relying on word form; if word form alone is insufficient, they further utilize contextual cues for interpretation. While advanced large language models (LLMs) exhibit similar abilities, the underlying mechanisms remain unclear. To investigate this, we conduct controlled experiments to analyze the roles of word form and contextual information in semantic reconstruction and examine LLM attention patterns. Specifically, we first propose SemRecScore, a reliable metric to quantify the degree of semantic reconstruction, and validate its effectiveness. Using this metric, we study how word form and contextual information influence LLMs' semantic reconstruction ability, identifying word form as the core factor in this process. Furthermore, we analyze how LLMs utilize word form and find that they rely on specialized attention heads to extract and process word form information, with this mechanism remaining stable across varying levels of word scrambling. This distinction between LLMs' fixed attention patterns primarily focused on word form and human readers' adaptive strategy in balancing word form and contextual information provides insights into enhancing LLM performance by incorporating human-like, context-aware mechanisms.",
      "arxiv_url": "https://arxiv.org/abs/2503.01714",
      "pdf_url": "https://arxiv.org/pdf/2503.01714",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.21049",
      "title": "A Semi-supervised Scalable Unified Framework for E-commerce Query Classification",
      "authors": [
        "Chunyuan Yuan",
        "Chong Zhang",
        "Zheng Fang",
        "Ming Pang",
        "Xue Jiang",
        "Changping Peng",
        "Zhangang Lin",
        "Ching Law"
      ],
      "abstract": "Query classification, including multiple subtasks such as intent and category prediction, is vital to e-commerce applications. E-commerce queries are usually short and lack context, and the information between labels cannot be used, resulting in insufficient prior information for modeling. Most existing industrial query classification methods rely on users'posterior click behavior to construct training samples, resulting in a Matthew vicious cycle. Furthermore, the subtasks of query classification lack a unified framework, leading to low efficiency for algorithm optimization. In this paper, we propose a novel Semi-supervised Scalable Unified Framework (SSUF), containing multiple enhanced modules to unify the query classification tasks. The knowledge-enhanced module uses world knowledge to enhance query representations and solve the problem of insufficient query information. The label-enhanced module uses label semantics and semi-supervised signals to reduce the dependence on posterior labels. The structure-enhanced module enhances the label representation based on the complex label relations. Each module is highly pluggable, and input features can be added or removed as needed according to each subtask. We conduct extensive offline and online A/B experiments, and the results show that SSUF significantly outperforms the state-of-the-art models.",
      "arxiv_url": "https://arxiv.org/abs/2506.21049",
      "pdf_url": "https://arxiv.org/pdf/2506.21049",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21863",
      "title": "GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning",
      "authors": [
        "Shikhhar Siingh",
        "Abhinav Rawat",
        "Chitta Baral",
        "Vivek Gupta"
      ],
      "abstract": "Publicly significant images from events hold valuable contextual information, crucial for journalism and education. However, existing methods often struggle to extract this relevance accurately. To address this, we introduce GETReason (Geospatial Event Temporal Reasoning), a framework that moves beyond surface-level image descriptions to infer deeper contextual meaning. We propose that extracting global event, temporal, and geospatial information enhances understanding of an image's significance. Additionally, we introduce GREAT (Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric for evaluating reasoning-based image understanding. Our layered multi-agent approach, assessed using a reasoning-weighted metric, demonstrates that meaningful insights can be inferred, effectively linking images to their broader event context.",
      "arxiv_url": "https://arxiv.org/abs/2505.21863",
      "pdf_url": "https://arxiv.org/pdf/2505.21863",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19754",
      "title": "NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering",
      "authors": [
        "Ruisheng Cao",
        "Hanchong Zhang",
        "Tiancheng Huang",
        "Zhangyi Kang",
        "Yuxin Zhang",
        "Liangtai Sun",
        "Hanqi Li",
        "Yuxun Miao",
        "Shuai Fan",
        "Lu Chen",
        "Kai Yu"
      ],
      "abstract": "The increasing number of academic papers poses significant challenges for researchers to efficiently acquire key details. While retrieval augmented generation (RAG) shows great promise in large language model (LLM) based automated question answering, previous works often isolate neural and symbolic retrieval despite their complementary strengths. Moreover, conventional single-view chunking neglects the rich structure and layout of PDFs, e.g., sections and tables. In this work, we propose NeuSym-RAG, a hybrid neural symbolic retrieval framework which combines both paradigms in an interactive process. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG organizes semi-structured PDF content into both the relational database and vectorstore, enabling LLM agents to iteratively gather context until sufficient to generate answers. Experiments on three full PDF-based QA datasets, including a self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the vector-based RAG and various structured baselines, highlighting its capacity to unify both retrieval schemes and utilize multiple views. Code and data are publicly available at https://github.com/X-LANCE/NeuSym-RAG.",
      "arxiv_url": "https://arxiv.org/abs/2505.19754",
      "pdf_url": "https://arxiv.org/pdf/2505.19754",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ed5a169847178113978d8aaac7f02162afe123aa",
      "title": "GRI-QA: a Comprehensive Benchmark for Table Question Answering over Environmental Data",
      "authors": [
        "M. Contalbo",
        "Sara Pederzoli",
        "Francesco Del Buono",
        "Venturelli Valeria",
        "Francesco Guerra",
        "Matteo Paganelli"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/ed5a169847178113978d8aaac7f02162afe123aa",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ed7200fd60660be8450222768f66f14b2a4228fd",
      "title": "Re³Syn: A Dependency-Based Data Synthesis Framework for Long-Context Post-training",
      "authors": [
        "Zhiyang Zhang",
        "Ziqiang Liu",
        "Huiming Wang",
        "Renke Shan",
        "Li Kuang",
        "Lu Wang",
        "De Wen Soh"
      ],
      "abstract": "An important trend in the realm of large language models (LLMs) is the development of longer context windows. However, training LLMs with long context windows to acquire the capability of effectively modeling lengthy inputs is often hindered by the scarcity of naturally long-context data. Existing methods for constructing long-context data by concatenating short documents have overlooked a crucial characteristic of long-context data quality, namely semantic dependency. In this paper, we propose a novel framework called Re trieval, Dependency Re cognition, and Re order for data syn thesis ( R E 3 S YN 1 ), which leverages semantic similarity to retrieve relevant documents and form several batches. Within each batch, the framework comprehensively recognizes dependency and utilizes them, along with a reorder algorithm, to organize the short documents into coherent long-context data. Comprehensive experiments on multiple benchmarks indicate that the data generated by the R E 3 S YN has longer dependencies and significantly enhances the model’s long-context capabilities.",
      "arxiv_url": "https://www.semanticscholar.org/paper/ed7200fd60660be8450222768f66f14b2a4228fd",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.11104",
      "title": "DAM: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration",
      "authors": [
        "Hanzhi Zhang",
        "Heng Fan",
        "Kewei Sha",
        "Yan Huang",
        "Yunhe Feng"
      ],
      "abstract": "Long-context understanding is crucial for many NLP applications, yet transformers struggle with efficiency due to the quadratic complexity of self-attention. Sparse attention methods alleviate this cost but often impose static, predefined masks, failing to capture heterogeneous attention patterns. This results in suboptimal token interactions, limiting adaptability and retrieval accuracy in long-sequence tasks. This work introduces a dynamic sparse attention mechanism that assigns adaptive masks at the attention-map level, preserving heterogeneous patterns across layers and heads. Unlike existing approaches, our method eliminates the need for fine-tuning and predefined mask structures while maintaining computational efficiency. By learning context-aware attention structures, it achieves high alignment with full-attention models, ensuring minimal performance degradation while reducing memory and compute overhead. This approach provides a scalable alternative to full attention, enabling the practical deployment of large-scale Large Language Models (LLMs) without sacrificing retrieval performance. DAM is available at: https://github.com/HanzhiZhang-Ulrica/DAM.",
      "arxiv_url": "https://arxiv.org/abs/2506.11104",
      "pdf_url": "https://arxiv.org/pdf/2506.11104",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15771",
      "title": "Learning to Reason from Feedback at Test-Time",
      "authors": [
        "Yanyang Li",
        "Michael R. Lyu",
        "Liwei Wang"
      ],
      "abstract": "Solving complex tasks in a single attempt is challenging for large language models (LLMs). Iterative interaction with the environment and feedback is often required to achieve success, making effective feedback utilization a critical topic. Existing approaches either struggle with length generalization or rely on naive retries without leveraging prior information. In this paper, we introduce FTTT, a novel paradigm that formulates feedback utilization as an optimization problem at test time. Additionally, we propose a learnable test-time optimizer, OpTune, to effectively exploit feedback. Experiments on two LLMs across four reasoning datasets demonstrate that FTTT and OpTune achieve superior scalability and performance.",
      "arxiv_url": "https://arxiv.org/abs/2502.15771",
      "pdf_url": "https://arxiv.org/pdf/2502.15771",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11020",
      "title": "TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages",
      "authors": [
        "Jafar Isbarov",
        "Arofat Akhundjanova",
        "Mammad Hajili",
        "K. Huseynova",
        "Dmitry Gaynullin",
        "Anar Rzayev",
        "Osman Tursun",
        "Ilshat Saetov",
        "Rinat Kharisov",
        "Saule Belginova",
        "Ariana Kenbayeva",
        "Amina Alisheva",
        "Aizirek Turdubaeva",
        "Abdullatif Köksal",
        "S. Rustamov",
        "Duygu Ataman"
      ],
      "abstract": "Being able to thoroughly assess massive multi-task language understanding (MMLU) capabilities is essential for advancing the applicability of multilingual language models. However, preparing such benchmarks in high quality native language is often costly and therefore limits the representativeness of evaluation datasets. While recent efforts focused on building more inclusive MMLU benchmarks, these are conventionally built using machine translation from high-resource languages, which may introduce errors and fail to account for the linguistic and cultural intricacies of the target languages. In this paper, we address the lack of native language MMLU benchmark especially in the under-represented Turkic language family with distinct morphosyntactic and cultural characteristics. We propose two benchmarks for Turkic language MMLU: TUMLU is a comprehensive, multilingual, and natively developed language understanding benchmark specifically designed for Turkic languages. It consists of middle- and high-school level questions spanning 11 academic subjects in Azerbaijani, Crimean Tatar, Karakalpak, Kazakh, Tatar, Turkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise, balanced, and manually verified subset of the dataset. Using this dataset, we systematically evaluate a diverse range of open and proprietary multilingual large language models (LLMs), including Claude, Gemini, GPT, and LLaMA, offering an in-depth analysis of their performance across different languages, subjects, and alphabets. To promote further research and development in multilingual language understanding, we release TUMLU-mini and all corresponding evaluation scripts.",
      "arxiv_url": "https://arxiv.org/abs/2502.11020",
      "pdf_url": "https://arxiv.org/pdf/2502.11020",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18488",
      "title": "Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications",
      "authors": [
        "Yanxiang Zhang",
        "Zheng Xu",
        "Shanshan Wu",
        "Yuanbo Zhang",
        "Daniel Ramage"
      ],
      "abstract": "Error correction is an important capability when applying large language models (LLMs) to facilitate user typing on mobile devices. In this paper, we use LLMs to synthesize a high-quality dataset of error correction pairs to evaluate and improve LLMs for mobile applications. We first prompt LLMs with error correction domain knowledge to build a scalable and reliable addition to the existing data synthesis pipeline. We then adapt the synthetic data distribution to match the mobile application domain by reweighting the samples. The reweighting model is learnt by predicting (a handful of) live A/B test metrics when deploying LLMs in production, given the LLM performance on offline evaluation data and scores from a small privacy-preserving on-device language model. Finally, we present best practices for mixing our synthetic data with other data sources to improve model performance on error correction in both offline evaluation and production live A/B testing.",
      "arxiv_url": "https://arxiv.org/abs/2505.18488",
      "pdf_url": "https://arxiv.org/pdf/2505.18488",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "edef75f7b631527d353c6ef3c01d34aa3bbd4c38",
      "title": "No Questions are Stupid, but some are Poorly Posed: Understanding Poorly-Posed Information-Seeking Questions",
      "authors": [
        "Neha Srikanth",
        "Rachel Rudinger",
        "J. Boyd-Graber"
      ],
      "abstract": "Questions help unlock information to satisfy users’ information needs. However, when the question is poorly posed, answerers (whether human or computer) may struggle to answer the question in a way that satisfies the asker, despite possibly knowing everything necessary to address the asker’s latent information need. Using Reddit question-answer interactions from r/NoStupidQuestions , we develop a computational framework grounded in linguistic theory to study poorly-posedness of questions by generating spaces of potential interpretations of questions and computing distributions over these spaces based on interpretations chosen by both human answerers in the Reddit question thread, as well as by a suite of large language models. Both humans and models struggle to converge on dominant interpretations when faced with poorly posed questions, but employ different strategies: humans focus on specific interpretations through question negotiation, while models attempt comprehensive coverage by addressing many interpretations simultaneously.",
      "arxiv_url": "https://www.semanticscholar.org/paper/edef75f7b631527d353c6ef3c01d34aa3bbd4c38",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00038",
      "title": "from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors",
      "authors": [
        "Yu Yan",
        "Sheng Sun",
        "Zenghao Duan",
        "Teli Liu",
        "Min Liu",
        "Zhiyi Yin",
        "Qi Li",
        "Jiangyu Lei"
      ],
      "abstract": "Current studies have exposed the risk of Large Language Models (LLMs) generating harmful content by jailbreak attacks. However, they overlook that the direct generation of harmful content from scratch is more difficult than inducing LLM to calibrate benign content into harmful forms. In our study, we introduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR) to induce the LLM to calibrate malicious metaphors for jailbreaking. Specifically, to answer harmful queries, AVATAR adaptively identifies a set of benign but logically related metaphors as the initial seed. Then, driven by these metaphors, the target LLM is induced to reason and calibrate about the metaphorical content, thus jailbroken by either directly outputting harmful responses or calibrating residuals between metaphorical and professional harmful content. Experimental results demonstrate that AVATAR can effectively and transferable jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2503.00038",
      "pdf_url": "https://arxiv.org/pdf/2503.00038",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19400",
      "title": "TheoremExplainAgent: Towards Video-based Multimodal Explanations for LLM Theorem Understanding",
      "authors": [
        "Max W.F. Ku",
        "Thomas Chong",
        "Jonathan Leung",
        "Krish Shah",
        "Alvin Yu",
        "Wenhu Chen"
      ],
      "abstract": "Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations.",
      "arxiv_url": "https://arxiv.org/abs/2502.19400",
      "pdf_url": "https://arxiv.org/pdf/2502.19400",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-02-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.01977",
      "title": "AutoGUI: Scaling GUI Grounding with Automatic Functionality Annotations from LLMs",
      "authors": [
        "Hongxin Li",
        "Jingfan Chen",
        "Jingran Su",
        "Yuntao Chen",
        "Qing Li",
        "Zhaoxiang Zhang"
      ],
      "abstract": "User interface understanding with vision-language models (VLMs) has received much attention due to its potential for enhancing software automation. However, existing datasets used to build UI-VLMs either only contain large-scale context-free element annotations or contextualized functional descriptions for elements at a small scale. In this work, we propose the \\textbf{AutoGUI} pipeline for automatically annotating UI elements with detailed functionality descriptions at scale. Specifically, we leverage large language models (LLMs) to infer element functionality by comparing UI state changes before and after simulated interactions. To improve annotation quality, we propose LLM-aided rejection and verification, eliminating invalid annotations without human labor. We construct a high-quality AutoGUI-704k dataset using the proposed pipeline, featuring diverse and detailed functionality annotations that are hardly provided by previous datasets. Human evaluation shows that we achieve annotation correctness comparable to a trained human annotator. Extensive experiments show that our dataset remarkably enhances VLM's UI grounding capabilities and exhibits significant scaling effects. We also show the interesting potential use of our dataset in UI agent tasks. Please view our project at https://autogui-project.github.io/.",
      "arxiv_url": "https://arxiv.org/abs/2502.01977",
      "pdf_url": "https://arxiv.org/pdf/2502.01977",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ee552989a03693a441863af4c29dc594bfcd1ab5",
      "title": "AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration",
      "authors": [
        "Zhexuan Wang",
        "Yutong Wang",
        "Xuebo Liu",
        "Liang Ding",
        "Miao Zhang",
        "Jie Liu",
        "Min Zhang"
      ],
      "abstract": "Multi-agent systems (MAS) based on large language models (LLMs) have demonstrated significant potential in collaborative problem-solving. However, they still face substantial challenges of low communication efficiency and suboptimal task performance, making the careful design of the agents’ communication topologies particularly important. Inspired by the management theory that roles in an efficient team are often dynamically adjusted, we propose AgentDropout , which identifies redundant agents and communication across different communication rounds by optimizing the adjacency matrices of the communication graphs and eliminates them to enhance both token efficiency and task performance. Compared to state-of-the-art methods, AgentDropout achieves an average reduction of 21.6% in prompt token consumption and 18.4% in completion token consumption, along with a performance improvement of 1.14 on the tasks. Furthermore, the extended experiments demonstrate that AgentDropout achieves notable domain transferability and structure robustness, revealing its reliability and effectiveness. We release our code at https://github.",
      "arxiv_url": "https://www.semanticscholar.org/paper/ee552989a03693a441863af4c29dc594bfcd1ab5",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ee77d6a751b228c7111b1f97b74172fd0280738e",
      "title": "Towards Multi-System Log Anomaly Detection",
      "authors": [
        "Boyang Wang",
        "Runqiang Zang",
        "Hongcheng Guo",
        "Shun Zhang",
        "Shaoshen Cao",
        "Donglin Di",
        "Zhoujun Li"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/ee77d6a751b228c7111b1f97b74172fd0280738e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02302",
      "title": "Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments",
      "authors": [
        "Russell Scheinberg",
        "Ameeta Agrawal",
        "Amber Shore",
        "So Young Lee"
      ],
      "abstract": "Large language models (LLMs) can explain grammatical rules, yet they often fail to apply those rules when judging sentence acceptability. We present\"grammar prompting\", an explain-then-process paradigm: a large LLM first produces a concise explanation of the relevant syntactic phenomenon, then that explanation is fed back as additional context to the target model -- either an LLM or a smaller language model (SLM) -- before deciding which sentence of a minimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian RuBLiMP benchmarks, this simple prompt design yields substantial improvements over strong baselines across many syntactic phenomena. Feeding an LLM's metalinguistic explanation back to the target model bridges the gap between knowing a rule and using it. On SLMs, grammar prompting alone trims the average LLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by 56% (13.0 pp ->5.8 pp), all at negligible cost. The lightweight, language-agnostic cue lets low-cost SLMs approach frontier-LLM performance in multilingual settings.",
      "arxiv_url": "https://arxiv.org/abs/2506.02302",
      "pdf_url": "https://arxiv.org/pdf/2506.02302",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "eea27eb1dc9130d4b0e325d72fd815c11eedaa42",
      "title": "Tunable LLM-based Proactive Recommendation Agent",
      "authors": [
        "Mingze Wang",
        "Chongming Gao",
        "Wenjie Wang",
        "Yangyang Li",
        "Fuli Feng"
      ],
      "abstract": "Recommender systems are indispensable on various digital platforms. However, traditional methods often reinforce existing user interests, which leads to echo chambers and limits diversity. Proactive Recommendation Systems (PRS) aim to address this issue by cultivating users’ latent interests through multi-step recommendations. Despite advancements, challenges persist particularly in optimizing long-term rewards and adapting to real-time user feed-back. In this study, we propose an LLM-based Actor-Critic Agent framework to enhance PRS. This framework utilizes the LLM-based agent to adjust recommendations in real time based on feedback and employs agent-tuning meth-ods to optimize long-term rewards using three proposed reward functions. Extensive experiments validate the significant superiority of this framework over existing methods by optimizing long-term rewards and dynamically evolving with user feedback. Our codes are available at https://github.com/gnaWeinrE/T-PRA .",
      "arxiv_url": "https://www.semanticscholar.org/paper/eea27eb1dc9130d4b0e325d72fd815c11eedaa42",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02701",
      "title": "On Entity Identification in Language Models",
      "authors": [
        "Masaki Sakata",
        "Benjamin Heinzerling",
        "Sho Yokoi",
        "Takumi Ito",
        "Kentaro Inui"
      ],
      "abstract": "We analyze the extent to which internal representations of language models (LMs) identify and distinguish mentions of named entities, focusing on the many-to-many correspondence between entities and their mentions. We first formulate two problems of entity mentions -- ambiguity and variability -- and propose a framework analogous to clustering quality metrics. Specifically, we quantify through cluster analysis of LM internal representations the extent to which mentions of the same entity cluster together and mentions of different entities remain separated. Our experiments examine five Transformer-based autoregressive models, showing that they effectively identify and distinguish entities with metrics analogous to precision and recall ranging from 0.66 to 0.9. Further analysis reveals that entity-related information is compactly represented in a low-dimensional linear subspace at early LM layers. Additionally, we clarify how the characteristics of entity representations influence word prediction performance. These findings are interpreted through the lens of isomorphism between LM representations and entity-centric knowledge structures in the real world, providing insights into how LMs internally organize and use entity information.",
      "arxiv_url": "https://arxiv.org/abs/2506.02701",
      "pdf_url": "https://arxiv.org/pdf/2506.02701",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23693",
      "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos",
      "authors": [
        "Tingyu Song",
        "Tongyan Hu",
        "Guo Gan",
        "Yilun Zhao"
      ],
      "abstract": "MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks. This highlights the challenging nature of our benchmark. Additionally, to investigate the practical applications of VF-Eval in improving video generation, we conduct an experiment, RePrompt, demonstrating that aligning MLLMs more closely with human feedback can benefit video generation.",
      "arxiv_url": "https://arxiv.org/abs/2505.23693",
      "pdf_url": "https://arxiv.org/pdf/2505.23693",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.13173",
      "title": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs",
      "authors": [
        "V.S.D.S.Mahesh Akavarapu",
        "Hrishikesh Terdalkar",
        "Pramit Bhattacharyya",
        "Shubhangi Agarwal",
        "Vishakha Deulgaonkar",
        "Pralay Manna",
        "Chaitali Dangarikar",
        "Arnab Bhattacharya"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across diverse tasks and languages. In this study, we focus on natural language understanding in three classical languages -- Sanskrit, Ancient Greek and Latin -- to investigate the factors affecting cross-lingual zero-shot generalization. First, we explore named entity recognition and machine translation into English. While LLMs perform equal to or better than fine-tuned baselines on out-of-domain data, smaller models often struggle, especially with niche or abstract entity types. In addition, we concentrate on Sanskrit by presenting a factoid question-answering (QA) dataset and show that incorporating context via retrieval-augmented generation approach significantly boosts performance. In contrast, we observe pronounced performance drops for smaller LLMs across these QA tasks. These results suggest model scale as an important factor influencing cross-lingual generalization. Assuming that models used such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical languages, our findings provide insights into how LLMs may generalize on these languages and their consequent utility in classical studies.",
      "arxiv_url": "https://arxiv.org/abs/2505.13173",
      "pdf_url": "https://arxiv.org/pdf/2505.13173",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-05-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "eeffa01514a2f76bede179a4d29e8758cc5809be",
      "title": "Should I Believe in What Medical AI Says? A Chinese Benchmark for Medication Based on Knowledge and Reasoning",
      "authors": [
        "Yue Wu",
        "Yangmin Huang",
        "Qianyun Du",
        "Lixian Lai",
        "Zhiyang He",
        "Jiaxue Hu",
        "Xiaodong Tao"
      ],
      "abstract": "Large language models (LLMs) show potential in healthcare but often generate hallucinations, especially when handling unfamiliar information. In medication, a systematic benchmark to evaluate model capabilities is lacking, which is critical given the high-risk nature of medical information. This paper introduces a Chinese benchmark aimed at assessing models in medication tasks, focusing on knowledge and reasoning across six datasets: indication, dosage and administration, contraindicated population, mechanisms of action, drug recommendation, and drug interaction. We evaluate eight closed-source and five open-source models to identify knowledge boundaries, providing the first systematic analysis of limitations and risks in proprietary medical models.",
      "arxiv_url": "https://www.semanticscholar.org/paper/eeffa01514a2f76bede179a4d29e8758cc5809be",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.10627",
      "title": "SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems",
      "authors": [
        "Ziyu Guo",
        "Ray Zhang",
        "Hao Chen",
        "Jialin Gao",
        "Dongzhi Jiang",
        "Jiaze Wang",
        "Pheng-Ann Heng"
      ],
      "abstract": "The rapid advancement of Large Multi-modal Models (LMMs) has enabled their application in scientific problem-solving, yet their fine-grained capabilities remain under-explored. In this paper, we introduce SciVerse, a multi-modal scientific evaluation benchmark to thoroughly assess LMMs across 5,735 test instances in five distinct versions. We aim to investigate three key dimensions of LMMs: scientific knowledge comprehension, multi-modal content interpretation, and Chain-of-Thought (CoT) reasoning. To unveil whether LMMs possess sufficient scientific expertise, we first transform each problem into three versions containing different levels of knowledge required for solving, i.e., Knowledge-free, -lite, and -rich. Then, to explore how LMMs interpret multi-modal scientific content, we annotate another two versions, i.e., Vision-rich and -only, marking more question information from texts to diagrams. Comparing the results of different versions, SciVerse systematically examines the professional knowledge stock and visual perception skills of LMMs in scientific domains. In addition, to rigorously assess CoT reasoning, we propose a new scientific CoT evaluation strategy, conducting a step-wise assessment on knowledge and logical errors in model outputs. Our extensive evaluation of different LMMs on SciVerse reveals critical limitations in their scientific proficiency and provides new insights into future developments. Project page: https://sciverse-cuhk.github.io",
      "arxiv_url": "https://arxiv.org/abs/2503.10627",
      "pdf_url": "https://arxiv.org/pdf/2503.10627",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ef2bca795a95966ebe26e08e361140351addfe9d",
      "title": "AGD: Adversarial Game Defense Against Jailbreak Attacks in Large Language Models",
      "authors": [
        "Shilong Pan",
        "Zhiliang Tian",
        "Zhen Huang",
        "Wanlong Yu",
        "Zhihua Wen",
        "Xinwang Liu",
        "Kai Lu",
        "Minlie Huang",
        "Dongsheng Li"
      ],
      "abstract": "LLMs demonstrate remarkable utility but remain vulnerable to jailbreak attacks that aim to elicit harmful responses. Existing defenses, including post-training alignment and prompt engineering, rely on training on safety-annotated datasets and safe prompt templates, struggling with adaptability to out-of-distribution (OOD) attacks. Steering internal representations of LLMs provides real-time adjustments to defend against OOD attacks. However, it struggles with maintaining model utility, since modifying the representation disrupts the forward pass of inference. It barely considers the competitive objectives of helpfulness and harmlessness in LLMs. We argue that adversarial game-based approaches promise a solution for conflicts be-tween the two objectives. In this paper, we propose A dversarial G ame D efense (AGD), an adversarial game-based defense method that dynamically adjusts LLMs’ internal representations to achieve a balanced trade-off between helpfulness and harmlessness. AGD first proposes an interquartile range (IQR) method to detect abnormal attention weights and correct the abnormal weights via adversarial training. AGD adopts a bi-level optimization to play a two-player variable-sum game to approach Nash Equilibrium (NE), where the two players adversarially refine head activations for help-fulness and harmlessness respectively. Furthermore, AGD applies an expert model to next-token sampling to generate safer responses. Experiments show that AGD significantly improves LLMs’ safety over all baselines.",
      "arxiv_url": "https://www.semanticscholar.org/paper/ef2bca795a95966ebe26e08e361140351addfe9d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ef4381d06d9cd06b4ea093ea883fbd1abb8f1ecc",
      "title": "MLAS-LoRA: Language-Aware Parameters Detection and LoRA-Based Knowledge Transfer for Multilingual Machine Translation",
      "authors": [
        "Tianyu Dong",
        "Bo Li",
        "Jinsong Liu",
        "Shaolin Zhu",
        "Deyi Xiong"
      ],
      "abstract": ".",
      "arxiv_url": "https://www.semanticscholar.org/paper/ef4381d06d9cd06b4ea093ea883fbd1abb8f1ecc",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.05084",
      "title": "Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction",
      "authors": [
        "Xiaowei Zhu",
        "Yubing Ren",
        "Yanan Cao",
        "Xixun Lin",
        "Fang Fang",
        "Yangxi Li"
      ],
      "abstract": "The rapid advancement of large language models has raised significant concerns regarding their potential misuse by malicious actors. As a result, developing effective detectors to mitigate these risks has become a critical priority. However, most existing detection methods focus excessively on detection accuracy, often neglecting the societal risks posed by high false positive rates (FPRs). This paper addresses this issue by leveraging Conformal Prediction (CP), which effectively constrains the upper bound of FPRs. While directly applying CP constrains FPRs, it also leads to a significant reduction in detection performance. To overcome this trade-off, this paper proposes a Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction (MCP), which both enforces the FPR constraint and improves detection performance. This paper also introduces RealDet, a high-quality dataset that spans a wide range of domains, ensuring realistic calibration and enabling superior detection performance when combined with MCP. Empirical evaluations demonstrate that MCP effectively constrains FPRs, significantly enhances detection performance, and increases robustness against adversarial attacks across multiple detectors and datasets.",
      "arxiv_url": "https://arxiv.org/abs/2505.05084",
      "pdf_url": "https://arxiv.org/pdf/2505.05084",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04020",
      "title": "QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering",
      "authors": [
        "A. Tang",
        "Xiuzhen Zhang",
        "M. Dinh",
        "Zhuang Li"
      ],
      "abstract": "Review-based Product Question Answering (PQA) allows e-commerce platforms to automatically address customer queries by leveraging insights from user reviews. However, existing PQA systems generate answers with only a single perspective, failing to capture the diversity of customer opinions. In this paper we introduce a novel task Quantitative Query-Focused Summarization (QQSUM), which aims to summarize diverse customer opinions into representative Key Points (KPs) and quantify their prevalence to effectively answer user queries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its generated answers still fall short of capturing the full diversity of viewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG, employs few-shot learning to jointly train a KP-oriented retriever and a KP summary generator, enabling KP-based summaries that capture diverse and representative opinions. Experimental results demonstrate that QQSUM-RAG achieves superior performance compared to state-of-the-art RAG baselines in both textual quality and quantification accuracy of opinions. Our source code is available at: https://github.com/antangrocket1312/QQSUMM",
      "arxiv_url": "https://arxiv.org/abs/2506.04020",
      "pdf_url": "https://arxiv.org/pdf/2506.04020",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02565",
      "title": "Towards Generating Controllable and Solvable Geometry Problem by Leveraging Symbolic Deduction Engine",
      "authors": [
        "Zhuoxu Jiang",
        "Tianyang Zhang",
        "Peiyan Peng",
        "Jing Chen",
        "Yinong Xun",
        "Haotian Zhang",
        "Lichi Li",
        "Yong Li",
        "Shaohua Zhang"
      ],
      "abstract": "Generating high-quality geometry problems is both an important and challenging task in education. Compared to math word problems, geometry problems further emphasize multi-modal formats and the translation between informal and formal languages. In this paper, we introduce a novel task for geometry problem generation and propose a new pipeline method: the Symbolic Deduction Engine-based Geometry Problem Generation framework (SDE-GPG). The framework leverages a symbolic deduction engine and contains four main steps: (1) searching a predefined mapping table from knowledge points to extended definitions, (2) sampling extended definitions and performing symbolic deduction, (3) filtering out unqualified problems, and (4) generating textual problems and diagrams. Specifically, our method supports to avoid inherent biases in translating natural language into formal language by designing the mapping table, and guarantees to control the generated problems in terms of knowledge points and difficulties by an elaborate checking function. With obtained formal problems, they are translated to natural language and the accompanying diagrams are automatically drew by rule-based methods. We conduct experiments using real-world combinations of knowledge points from two public datasets. The results demonstrate that the SDE-GPG can effectively generate readable, solvable and controllable geometry problems.",
      "arxiv_url": "https://arxiv.org/abs/2506.02565",
      "pdf_url": "https://arxiv.org/pdf/2506.02565",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14752",
      "title": "TritonBench: Benchmarking Large Language Model Capabilities for Generating Triton Operators",
      "authors": [
        "Jianling Li",
        "Sha Li",
        "Zhenye Gao",
        "Qi Shi",
        "Yuxuan Li",
        "Zefan Wang",
        "Jiacheng Huang",
        "Haojie Wang",
        "Jianrong Wang",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Triton, a high-level Python-like language designed for building efficient GPU kernels, is widely adopted in deep learning frameworks due to its portability, flexibility, and accessibility. However, programming and parallel optimization still require considerable trial and error from Triton developers. Despite advances in large language models (LLMs) for conventional code generation, these models struggle to generate accurate, performance-optimized Triton code, as they lack awareness of its specifications and the complexities of GPU programming. More critically, there is an urgent need for systematic evaluations tailored to Triton. In this work, we introduce TritonBench, the first comprehensive benchmark for Triton operator generation. TritonBench features two evaluation channels: a curated set of 184 real-world operators from GitHub and a collection of operators aligned with PyTorch interfaces. Unlike conventional code benchmarks prioritizing functional correctness, TritonBench also profiles efficiency performance on widely deployed GPUs aligned with industry applications. Our study reveals that current state-of-the-art code LLMs struggle to generate efficient Triton operators, highlighting a significant gap in high-performance code generation. TritonBench will be available at https://github.com/thunlp/TritonBench.",
      "arxiv_url": "https://arxiv.org/abs/2502.14752",
      "pdf_url": "https://arxiv.org/pdf/2502.14752",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13656",
      "title": "Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models",
      "authors": [
        "Liyang He",
        "Chenglong Liu",
        "Rui Li",
        "Zhenya Huang",
        "Shulan Ruan",
        "Jun Zhou",
        "Enhong Chen"
      ],
      "abstract": "Sentence embedding is essential for many NLP tasks, with contrastive learning methods achieving strong performance using annotated datasets like NLI. Yet, the reliance on manual labels limits scalability. Recent studies leverage large language models (LLMs) to generate sentence pairs, reducing annotation dependency. However, they overlook ranking information crucial for fine-grained semantic distinctions. To tackle this challenge, we propose a method for controlling the generation direction of LLMs in the latent space. Unlike unconstrained generation, the controlled approach ensures meaningful semantic divergence. Then, we refine exist sentence embedding model by integrating ranking information and semantic information. Experiments on multiple benchmarks demonstrate that our method achieves new SOTA performance with a modest cost in ranking sentence synthesis.",
      "arxiv_url": "https://arxiv.org/abs/2502.13656",
      "pdf_url": "https://arxiv.org/pdf/2502.13656",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.10024",
      "title": "Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models",
      "authors": [
        "Elena Sofia Ruzzetti",
        "Giancarlo A. Xompero",
        "Davide Venditti",
        "F. Zanzotto"
      ],
      "abstract": "Large Language Models (LLMs) memorize, and thus, among huge amounts of uncontrolled data, may memorize Personally Identifiable Information (PII), which should not be stored and, consequently, not leaked. In this paper, we introduce Private Memorization Editing (PME), an approach for preventing private data leakage that turns an apparent limitation, that is, the LLMs'memorization ability, into a powerful privacy defense strategy. While attacks against LLMs have been performed exploiting previous knowledge regarding their training data, our approach aims to exploit the same kind of knowledge in order to make a model more robust. We detect a memorized PII and then mitigate the memorization of PII by editing a model knowledge of its training data. We verify that our procedure does not affect the underlying language model while making it more robust against privacy Training Data Extraction attacks. We demonstrate that PME can effectively reduce the number of leaked PII in a number of configurations, in some cases even reducing the accuracy of the privacy attacks to zero.",
      "arxiv_url": "https://arxiv.org/abs/2506.10024",
      "pdf_url": "https://arxiv.org/pdf/2506.10024",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04807",
      "title": "Call for Rigor in Reporting Quality of Instruction Tuning Data",
      "authors": [
        "Hyeonseok Moon",
        "Jaehyung Seo",
        "Heu-Jeoung Lim"
      ],
      "abstract": "Instruction tuning is crucial for adapting large language models (LLMs) to align with user intentions. Numerous studies emphasize the significance of the quality of instruction tuning (IT) data, revealing a strong correlation between IT data quality and the alignment performance of LLMs. In these studies, the quality of IT data is typically assessed by evaluating the performance of LLMs trained with that data. However, we identified a prevalent issue in such practice: hyperparameters for training models are often selected arbitrarily without adequate justification. We observed significant variations in hyperparameters applied across different studies, even when training the same model with the same data. In this study, we demonstrate the potential problems arising from this practice and emphasize the need for careful consideration in verifying data quality. Through our experiments on the quality of LIMA data and a selected set of 1,000 Alpaca data points, we demonstrate that arbitrary hyperparameter decisions can make any arbitrary conclusion.",
      "arxiv_url": "https://arxiv.org/abs/2503.04807",
      "pdf_url": "https://arxiv.org/pdf/2503.04807",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "efbc1b3dd1d45b8676c39c678342b9811f8281f6",
      "title": "Towards A \"Novel\" Benchmark: Evaluating Literary Fiction with Large Language Models",
      "authors": [
        "Wenqing Wang",
        "Mingqi Gao",
        "Xinyu Hu",
        "Xiaojun Wan"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/efbc1b3dd1d45b8676c39c678342b9811f8281f6",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.03553",
      "title": "Agentic Knowledgeable Self-awareness",
      "authors": [
        "Shuofei Qiao",
        "Zhisong Qiu",
        "Baochang Ren",
        "Xiaobin Wang",
        "Xiangyuan Ru",
        "Ningyu Zhang",
        "Xiang Chen",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Huajun Chen"
      ],
      "abstract": "Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a\"flood irrigation\"methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.",
      "arxiv_url": "https://arxiv.org/abs/2504.03553",
      "pdf_url": "https://arxiv.org/pdf/2504.03553",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-04-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f00e98001c2a7259a0b5a0fd3add98d1651c4f74",
      "title": "SubLIME: Subset Selection via Rank Correlation Prediction for Data-Efficient LLM Evaluation",
      "authors": [
        "Gayathri Saranathan",
        "Cong Xu",
        "M. Alam",
        "Tarun Kumar",
        "Martin Foltin",
        "Soon Yee Wong",
        "Suparna Bhattacharya"
      ],
      "abstract": "The rapid expansion of Large Language Models (LLMs) and natural language processing datasets has made exhaustive benchmark evaluations computationally prohibitive. Inspired by high-stakes competitions like the International Mathematical Olympiad—where a few well-chosen problems suffice to differentiate top performers—we present SubLIME , which reduces evaluation costs by 80% to 99% while preserving ranking fidelity. It trains a Rank Correlation Prediction (RCP) model that combines limited performance data from only 5–20 anchor LLMs with dataset intrinsic metrics— Difficulty , Quality , and Distributional Dispersion —to predict how closely a candidate subset reflects full-benchmark rankings. Guided by these predictions, SubLIME selects a “winning” subset (1–20% of full set data) for evaluating new LLMs, preserving global rankings significant better than other data-efficient methods across ten diverse benchmarks.",
      "arxiv_url": "https://www.semanticscholar.org/paper/f00e98001c2a7259a0b5a0fd3add98d1651c4f74",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01865",
      "title": "Guiding not Forcing: Enhancing the Transferability of Jailbreaking Attacks on LLMs via Removing Superfluous Constraints",
      "authors": [
        "Junxiao Yang",
        "Zhexin Zhang",
        "Shiyao Cui",
        "Hongning Wang",
        "Minlie Huang"
      ],
      "abstract": "Jailbreaking attacks can effectively induce unsafe behaviors in Large Language Models (LLMs); however, the transferability of these attacks across different models remains limited. This study aims to understand and enhance the transferability of gradient-based jailbreaking methods, which are among the standard approaches for attacking white-box models. Through a detailed analysis of the optimization process, we introduce a novel conceptual framework to elucidate transferability and identify superfluous constraints-specifically, the response pattern constraint and the token tail constraint-as significant barriers to improved transferability. Removing these unnecessary constraints substantially enhances the transferability and controllability of gradient-based attacks. Evaluated on Llama-3-8B-Instruct as the source model, our method increases the overall Transfer Attack Success Rate (T-ASR) across a set of target models with varying safety levels from 18.4% to 50.3%, while also improving the stability and controllability of jailbreak behaviors on both source and target models.",
      "arxiv_url": "https://arxiv.org/abs/2503.01865",
      "pdf_url": "https://arxiv.org/pdf/2503.01865",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18341",
      "title": "Moderation Matters:Measuring Conversational Moderation Impact in English as a Second Language Group Discussion",
      "authors": [
        "Rena Gao",
        "Ming-Bin Chen",
        "Lea Frermann",
        "Jey Han Lau"
      ],
      "abstract": "English as a Second Language (ESL) speakers often struggle to engage in group discussions due to language barriers. While moderators can facilitate participation, few studies assess conversational engagement and evaluate moderation effectiveness. To address this gap, we develop a dataset comprising 17 sessions from an online ESL conversation club, which includes both moderated and non-moderated discussions. We then introduce an approach that integrates automatic ESL dialogue assessment and a framework that categorizes moderation strategies. Our findings indicate that moderators help improve the flow of topics and start/end a conversation. Interestingly, we find active acknowledgement and encouragement to be the most effective moderation strategy, while excessive information and opinion sharing by moderators has a negative impact. Ultimately, our study paves the way for analyzing ESL group discussions and the role of moderators in non-native conversation settings.",
      "arxiv_url": "https://arxiv.org/abs/2502.18341",
      "pdf_url": "https://arxiv.org/pdf/2502.18341",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f02c7702024f24e3f1a00b3f583d7c2af66503fd",
      "title": "QuASAR: A Question-Driven Structure-Aware Approach for Table-to-Text Generation",
      "authors": [
        "WeiJie Liu",
        "Yibin Zheng",
        "Fang Kong"
      ],
      "abstract": "Table-to-text generation aims to automatically produce natural language descriptions from structured or semi-structured tabular data. Unlike traditional text generation tasks, it requires models to accurately understand and represent table structures. Existing approaches typically process tables by linearizing them or converting them into graph structures. However, these methods either fail to adequately capture the table structure or rely on complex attention mechanisms, limiting their applicability. To tackle these challenges, we propose QuASAR, a question-driven self-supervised approach designed to enhance the model’s structural perception and representation capabilities. Specifically, QuASAR formulates a set of structure-related queries for self-supervised training, explicitly guiding the model to capture both local and global table structures. Additionally, we introduce two auxiliary pre-training tasks: a word-to-sentence reconstruction task and a numerical summarization task, which further enhance the fluency and factuality of the generated text. Experimental results on the ToTTo and HiTab datasets demonstrate that our approach produces higher-quality text compared to existing methods. All of our source code and data are publicly available at https:// github.com/weijieliu-cs/QuASAR.",
      "arxiv_url": "https://www.semanticscholar.org/paper/f02c7702024f24e3f1a00b3f583d7c2af66503fd",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18397",
      "title": "KiRAG: Knowledge-Driven Iterative Retriever for Enhancing Retrieval-Augmented Generation",
      "authors": [
        "Jinyuan Fang",
        "Zaiqiao Meng",
        "Craig Macdonald"
      ],
      "abstract": "Iterative retrieval-augmented generation (iRAG) models offer an effective approach for multi-hop question answering (QA). However, their retrieval process faces two key challenges: (1) it can be disrupted by irrelevant documents or factually inaccurate chain-of-thoughts; (2) their retrievers are not designed to dynamically adapt to the evolving information needs in multi-step reasoning, making it difficult to identify and retrieve the missing information required at each iterative step. Therefore, we propose KiRAG, which uses a knowledge-driven iterative retriever model to enhance the retrieval process of iRAG. Specifically, KiRAG decomposes documents into knowledge triples and performs iterative retrieval with these triples to enable a factually reliable retrieval process. Moreover, KiRAG integrates reasoning into the retrieval process to dynamically identify and retrieve knowledge that bridges information gaps, effectively adapting to the evolving information needs. Empirical results show that KiRAG significantly outperforms existing iRAG models, with an average improvement of 9.40% in R@3 and 5.14% in F1 on multi-hop QA.",
      "arxiv_url": "https://arxiv.org/abs/2502.18397",
      "pdf_url": "https://arxiv.org/pdf/2502.18397",
      "primary_category": "",
      "categories": [],
      "tags": [
        "RAG"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00172",
      "title": "A Survey of Uncertainty Estimation Methods on Large Language Models",
      "authors": [
        "Zhiqiu Xia",
        "Jinxuan Xu",
        "Yuqian Zhang",
        "Hang Liu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across various tasks. However, these models could offer biased, hallucinated, or non-factual responses camouflaged by their fluency and realistic appearance. Uncertainty estimation is the key method to address this challenge. While research efforts in uncertainty estimation are ramping up, there is a lack of comprehensive and dedicated surveys on LLM uncertainty estimation. This survey presents four major avenues of LLM uncertainty estimation. Furthermore, we perform extensive experimental evaluations across multiple methods and datasets. At last, we provide critical and promising future directions for LLM uncertainty estimation.",
      "arxiv_url": "https://arxiv.org/abs/2503.00172",
      "pdf_url": "https://arxiv.org/pdf/2503.00172",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.06987",
      "title": "Convert Language Model into a Value-based Strategic Planner",
      "authors": [
        "Xiaoyu Wang",
        "Yue Zhao",
        "Qingqing Gu",
        "Zhonglin Jiang",
        "Xiaokai Chen",
        "Yong Chen",
        "Luo Ji"
      ],
      "abstract": "Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Q-learning on LLMs, and propose a framework called straQ*. Our framework allows a plug-and-play LLM to bootstrap the planning during ESC, determine the optimal strategy based on long-term returns, and finally guide the LLM to response. Substantial experiments on ESC datasets suggest that straQ* outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and finite state machines.",
      "arxiv_url": "https://arxiv.org/abs/2505.06987",
      "pdf_url": "https://arxiv.org/pdf/2505.06987",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.08644",
      "title": "Exploiting Instruction-Following Retrievers for Malicious Information Retrieval",
      "authors": [
        "Parishad BehnamGhader",
        "Nicholas Meade",
        "Siva Reddy"
      ],
      "abstract": "Instruction-following retrievers have been widely adopted alongside LLMs in real-world applications, but little work has investigated the safety risks surrounding their increasing search capabilities. We empirically study the ability of retrievers to satisfy malicious queries, both when used directly and when used in a retrieval augmented generation-based setup. Concretely, we investigate six leading retrievers, including NV-Embed and LLM2Vec, and find that given malicious requests, most retrievers can (for>50% of queries) select relevant harmful passages. For example, LLM2Vec correctly selects passages for 61.35% of our malicious queries. We further uncover an emerging risk with instruction-following retrievers, where highly relevant harmful information can be surfaced by exploiting their instruction-following capabilities. Finally, we show that even safety-aligned LLMs, such as Llama3, can satisfy malicious requests when provided with harmful retrieved passages in-context. In summary, our findings underscore the malicious misuse risks associated with increasing retriever capability.",
      "arxiv_url": "https://arxiv.org/abs/2503.08644",
      "pdf_url": "https://arxiv.org/pdf/2503.08644",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-03-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f0b0ed15b994430f2dbd970186958ccadb1d05a4",
      "title": "It's Not Bragging If You Can Back It Up: Can LLMs Understand Braggings?",
      "authors": [
        "Jingjie Zeng",
        "Huayang Li",
        "Liang Yang",
        "Yuanyuan Sun",
        "Hongfei Lin"
      ],
      "abstract": "Bragging, as a pervasive social-linguistic phenomenon, reflects complex human interaction patterns. However, the understanding and generation of appropriate bragging behavior in large language models (LLMs) remains under-explored. In this paper, we propose a comprehensive study that combines analytical and controllable approaches to examine bragging in LLMs. We design three tasks, bragging recognition, bragging explanation, and brag-ging generation , along with novel evaluation metrics to assess the models’ ability to identify bragging intent, social appropriateness, and account for context sensitivity. Our analysis reveals the challenges of bragging in the social context, such as recognizing bragging and responding appropriately with bragging in conversation. This work provides new insights into how LLMs process bragging and highlights the need for more research on generating contextually appropriate behavior in LLMs 1 .",
      "arxiv_url": "https://www.semanticscholar.org/paper/f0b0ed15b994430f2dbd970186958ccadb1d05a4",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f0d305134b1dafcf7294fd4af339a4cfeadb0f88",
      "title": "World Knowledge Resolves Some Aspectual Ambiguity",
      "authors": [
        "Katarzyna Prus",
        "Mark Steedman",
        "Adam Lopez"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/f0d305134b1dafcf7294fd4af339a4cfeadb0f88",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.13559",
      "title": "Understand the Implication: Learning to Think for Pragmatic Understanding",
      "authors": [
        "S. Sravanthi",
        "Kishan Maharaj",
        "Sravani Gunnu",
        "Abhijit Mishra",
        "Pushpak Bhattacharyya"
      ],
      "abstract": "Pragmatics, the ability to infer meaning beyond literal interpretation, is crucial for social cognition and communication. While LLMs have been benchmarked for their pragmatic understanding, improving their performance remains underexplored. Existing methods rely on annotated labels but overlook the reasoning process humans naturally use to interpret implicit meaning. To bridge this gap, we introduce a novel pragmatic dataset, ImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both correct and incorrect interpretations. Through preference-tuning and supervised fine-tuning, we demonstrate that thought-based learning significantly enhances LLMs' pragmatic understanding, improving accuracy by 11.12% across model families. We further discuss a transfer-learning study where we evaluate the performance of thought-based training for the other tasks of pragmatics (presupposition, deixis) that are not seen during the training time and observe an improvement of 16.10% compared to label-trained models.",
      "arxiv_url": "https://arxiv.org/abs/2506.13559",
      "pdf_url": "https://arxiv.org/pdf/2506.13559",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11492",
      "title": "Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding",
      "authors": [
        "Kung-Hsiang Huang",
        "Can Qin",
        "Haoyi Qiu",
        "Philippe Laban",
        "Shafiq Joty",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "abstract": "Vision Language Models (VLMs) have achieved remarkable progress in multimodal tasks, yet they often struggle with visual arithmetic, seemingly simple capabilities like object counting or length comparison, which are essential for relevant complex tasks like chart understanding and geometric reasoning. In this work, we first investigate the root causes of this deficiency through a suite of probing tasks focusing on basic visual arithmetic. Our analysis reveals that while pre-trained vision encoders typically capture sufficient information, the text decoder often fails to decode it correctly for arithmetic reasoning. To address this, we propose CogAlign, a novel post-training strategy inspired by Piaget's theory of cognitive development. CogAlign trains VLMs to recognize invariant properties under visual transformations. We demonstrate that this approach significantly improves the performance of three diverse VLMs on our proposed probing tasks. Furthermore, CogAlign enhances performance by an average of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching supervised fine-tuning methods while requiring only 60% less training data. These results highlight the effectiveness and generalizability of CogAlign in improving fundamental visual arithmetic capabilities and their transfer to downstream tasks.",
      "arxiv_url": "https://arxiv.org/abs/2502.11492",
      "pdf_url": "https://arxiv.org/pdf/2502.11492",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f12dc084874a67f37cdae9ef5288f90ed1123916",
      "title": "LECTURE4ALL: A Lightweight Approach to Precise Timestamp Detection in Online Lecture Videos",
      "authors": [
        "Torben Hannemann",
        "Frank Hammerschmidt",
        "Simon Kazemi",
        "Gregor Stange",
        "Viktoria Wrobel",
        "Robert Geislinger",
        "Seid Muhie Yimam"
      ],
      "abstract": "This paper presents L ECTURE 4A LL 1 , a web application developed to improve the search experience of educational video platforms. Lecture2Go provides a vast collection of recorded lectures, but locating specific content within videos can be time-consuming. L EC - TURE 4A LL addresses this issue by leveraging a vector database and a streamlined user interface to enable direct retrieval of precise video timestamps. By enhancing search accuracy and efficiency, L ECTURE 4A LL significantly improves the accessibility and usability of educational video platforms.",
      "arxiv_url": "https://www.semanticscholar.org/paper/f12dc084874a67f37cdae9ef5288f90ed1123916",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20293",
      "title": "Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery",
      "authors": [
        "Yifan Sun",
        "Danding Wang",
        "Qiang Sheng",
        "Juan Cao",
        "Jintao Li"
      ],
      "abstract": "Concept-based explainable approaches have emerged as a promising method in explainable AI because they can interpret models in a way that aligns with human reasoning. However, their adaption in the text domain remains limited. Most existing methods rely on predefined concept annotations and cannot discover unseen concepts, while other methods that extract concepts without supervision often produce explanations that are not intuitively comprehensible to humans, potentially diminishing user trust. These methods fall short of discovering comprehensible concepts automatically. To address this issue, we propose \\textbf{ECO-Concept}, an intrinsically interpretable framework to discover comprehensible concepts with no concept annotations. ECO-Concept first utilizes an object-centric architecture to extract semantic concepts automatically. Then the comprehensibility of the extracted concepts is evaluated by large language models. Finally, the evaluation result guides the subsequent model fine-tuning to obtain more understandable explanations. Experiments show that our method achieves superior performance across diverse tasks. Further concept evaluations validate that the concepts learned by ECO-Concept surpassed current counterparts in comprehensibility.",
      "arxiv_url": "https://arxiv.org/abs/2505.20293",
      "pdf_url": "https://arxiv.org/pdf/2505.20293",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00812",
      "title": "BOSE: A Systematic Evaluation Method Optimized for Base Models",
      "authors": [
        "Hongzhi Luan",
        "Changxin Tian",
        "Zhaoxin Huan",
        "Xiaolu Zhang",
        "Kunlong Chen",
        "Zhiqiang Zhang",
        "Jun Zhou"
      ],
      "abstract": "This paper poses two critical issues in evaluating base models (without post-training): (1) Unstable evaluation during training: in the early stages of pre-training, the models lack the capability to answer questions as required, leading to unstable evaluation results. This instability makes it difficult to provide solid conclusions to guide the training, especially for key experiments such as data ablation and scaling law. (2) Inconsistency between base and instruct models: base models generally exhibit poorer evaluation performance compared to corresponding instruct models. This gap poses a challenge for assessing whether a base model with better evaluation can truly lead to a better instruct model. To address these issues, we propose Base model Oriented Systematic Evaluation (BOSE), a method specifically designed to optimize the evaluation of base models. Specifically, BOSE introduces two key innovations: In-Context Light-instruction Prompt (ICLiP) for open-ended tasks and Blank-ppl for multi-choice tasks with candidate options, which transforms the standard perplexity (ppl) metric into a fill-in-the-blank format to mitigate early-stage evaluation fluctuations. Furthermore, we are the first to propose Kendall's rank correlation to quantitatively measure the evaluation stability and consistency. Experimental results demonstrate that BOSE significantly enhances both the stability of evaluations during pre-training and the consistency between base and instruct models, thereby providing more reliable guidance for the LLMs' training.",
      "arxiv_url": "https://arxiv.org/abs/2503.00812",
      "pdf_url": "https://arxiv.org/pdf/2503.00812",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.12615",
      "title": "Konooz: Multi-domain Multi-dialect Corpus for Named Entity Recognition",
      "authors": [
        "Nagham Hamad",
        "Mohammed Khalilia",
        "Mustafa Jarrar"
      ],
      "abstract": "We introduce Konooz, a novel multi-dimensional corpus covering 16 Arabic dialects across 10 domains, resulting in 160 distinct corpora. The corpus comprises about 777k tokens, carefully collected and manually annotated with 21 entity types using both nested and flat annotation schemes - using the Wojood guidelines. While Konooz is useful for various NLP tasks like domain adaptation and transfer learning, this paper primarily focuses on benchmarking existing Arabic Named Entity Recognition (NER) models, especially cross-domain and cross-dialect model performance. Our benchmarking of four Arabic NER models using Konooz reveals a significant drop in performance of up to 38% when compared to the in-distribution data. Furthermore, we present an in-depth analysis of domain and dialect divergence and the impact of resource scarcity. We also measured the overlap between domains and dialects using the Maximum Mean Discrepancy (MMD) metric, and illustrated why certain NER models perform better on specific dialects and domains. Konooz is open-source and publicly available at https://sina.birzeit.edu/wojood/#download",
      "arxiv_url": "https://arxiv.org/abs/2506.12615",
      "pdf_url": "https://arxiv.org/pdf/2506.12615",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.19883",
      "title": "Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models",
      "authors": [
        "Sibo Yi",
        "Tianshuo Cong",
        "Xinlei He",
        "Qi Li",
        "Jiaxing Song"
      ],
      "abstract": "Small language models (SLMs) have become increasingly prominent in the deployment on edge devices due to their high efficiency and low computational cost. While researchers continue to advance the capabilities of SLMs through innovative training strategies and model compression techniques, the security risks of SLMs have received considerably less attention compared to large language models (LLMs).To fill this gap, we provide a comprehensive empirical study to evaluate the security performance of 13 state-of-the-art SLMs under various jailbreak attacks. Our experiments demonstrate that most SLMs are quite susceptible to existing jailbreak attacks, while some of them are even vulnerable to direct harmful prompts.To address the safety concerns, we evaluate several representative defense methods and demonstrate their effectiveness in enhancing the security of SLMs. We further analyze the potential security degradation caused by different SLM techniques including architecture compression, quantization, knowledge distillation, and so on. We expect that our research can highlight the security challenges of SLMs and provide valuable insights to future work in developing more robust and secure SLMs.",
      "arxiv_url": "https://arxiv.org/abs/2502.19883",
      "pdf_url": "https://arxiv.org/pdf/2502.19883",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-27",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.07971",
      "title": "SPHERE: An Evaluation Card for Human-AI Systems",
      "authors": [
        "Qianou Ma",
        "Dora Zhao",
        "Xinran Zhao",
        "Chenglei Si",
        "Chenyang Yang",
        "Ryan Louie",
        "Ehud Reiter",
        "Diyi Yang",
        "Tongshuang Wu"
      ],
      "abstract": "In the era of Large Language Models (LLMs), establishing effective evaluation methods and standards for diverse human-AI interaction systems is increasingly challenging. To encourage more transparent documentation and facilitate discussion on human-AI system evaluation design options, we present an evaluation card SPHERE, which encompasses five key dimensions: 1) What is being evaluated?; 2) How is the evaluation conducted?; 3) Who is participating in the evaluation?; 4) When is evaluation conducted?; 5) How is evaluation validated? We conduct a review of 39 human-AI systems using SPHERE, outlining current evaluation practices and areas for improvement. We provide three recommendations for improving the validity and rigor of evaluation practices.",
      "arxiv_url": "https://arxiv.org/abs/2504.07971",
      "pdf_url": "https://arxiv.org/pdf/2504.07971",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00396",
      "title": "Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively",
      "authors": [
        "Jiawei Gu",
        "Shangsong Liang"
      ],
      "abstract": "Effective decision-making in Large Language Models (LLMs) is essential for handling intricate tasks. However, existing approaches prioritize performance but often overlook the balance between effectiveness and computational cost. To address this, we first introduce the 3E Criteria to systematically assess the cost-effectiveness of search strategies, revealing that existing methods often trade significant efficiency for marginal performance gains. To improve LLM decision-making while maintaining efficiency, we propose the Speculative Reward Model (SRM), a plug-and-play framework that seamlessly integrates with existing search strategies. Specifically, SRM employs an external reward assigner to predict optimal actions, reducing reliance on LLMs' internal self-evaluation. And a speculative verification mechanism is used to prune suboptimal choices and guide the search toward more promising steps. We evaluate SRM on several complex decision-making tasks including mathematical reasoning, planning and numerical reasoning in specialized domains. Experimental results show that SRM reduces costs to 1/10 of the original search framework on average while maintaining effectiveness.",
      "arxiv_url": "https://arxiv.org/abs/2506.00396",
      "pdf_url": "https://arxiv.org/pdf/2506.00396",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-31",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f1d27fcd67a0d32af208b973283b5222ee192005",
      "title": "ReasonGraph: Visualization of Reasoning Methods and Extended Inference Paths",
      "authors": [
        "Zongqian Li",
        "Ehsan Shareghi",
        "Nigel Collier"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/f1d27fcd67a0d32af208b973283b5222ee192005",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11693",
      "title": "Hierarchical Bracketing Encodings for Dependency Parsing as Tagging",
      "authors": [
        "Ana Ezquerro",
        "David Vilares",
        "Anssi Yli-Jyrä",
        "Carlos G'omez-Rodr'iguez"
      ],
      "abstract": "We present a family of encodings for sequence labeling dependency parsing, based on the concept of hierarchical bracketing. We prove that the existing 4-bit projective encoding belongs to this family, but it is suboptimal in the number of labels used to encode a tree. We derive an optimal hierarchical bracketing, which minimizes the number of symbols used and encodes projective trees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also extend optimal hierarchical bracketing to support arbitrary non-projectivity in a more compact way than previous encodings. Our new encodings yield competitive accuracy on a diverse set of treebanks.",
      "arxiv_url": "https://arxiv.org/abs/2505.11693",
      "pdf_url": "https://arxiv.org/pdf/2505.11693",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f2108cffb560449902273f5a8ba36d0137d87df4",
      "title": "Multimodal Machine Translation with Text-Image In-depth Questioning",
      "authors": [
        "Yue Gao",
        "Jing Zhao",
        "Shiliang Sun",
        "Xiaosong Qiao",
        "Tengfei Song",
        "Hao Yang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/f2108cffb560449902273f5a8ba36d0137d87df4",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2508.05657",
      "title": "Beyond Single Labels: Improving Conversational Recommendation through LLM-Powered Data Augmentation",
      "authors": [
        "Haozhe Xu",
        "Xiaohua Wang",
        "Changze Lv",
        "Xiaoqing Zheng"
      ],
      "abstract": "Conversational recommender systems (CRSs) enhance recommendation quality by engaging users in multi-turn dialogues, capturing nuanced preferences through natural language interactions. However, these systems often face the false negative issue, where items that a user might like are incorrectly labeled as negative during training, leading to suboptimal recommendations.Expanding the label set through data augmentation presents an intuitive solution but faces the challenge of balancing two key aspects: ensuring semantic relevance and preserving the collaborative information inherent in CRS datasets. To address these issues, we propose a novel data augmentation framework that first leverages an LLM-based semantic retriever to identify diverse and semantically relevant items, which are then filtered by a relevance scorer to remove noisy candidates. Building on this, we introduce a two-stage training strategy balancing semantic relevance and collaborative information. Extensive experiments on two benchmark datasets and user simulators demonstrate significant and consistent performance improvements across various recommenders, highlighting the effectiveness of our approach in advancing CRS performance.",
      "arxiv_url": "https://arxiv.org/abs/2508.05657",
      "pdf_url": "https://arxiv.org/pdf/2508.05657",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15168",
      "title": "mStyleDistance: Multilingual Style Embeddings and their Evaluation",
      "authors": [
        "Justin Qiu",
        "Jiacheng Zhu",
        "Ajay Patel",
        "Marianna Apidianaki",
        "Christopher Callison-Burch"
      ],
      "abstract": "Style embeddings are useful for stylistic analysis and style transfer; however, only English style embeddings have been made available. We introduce Multilingual StyleDistance (mStyleDistance), a multilingual style embedding model trained using synthetic data and contrastive learning. We train the model on data from nine languages and create a multilingual STEL-or-Content benchmark (Wegmann et al., 2022) that serves to assess the embeddings' quality. We also employ our embeddings in an authorship verification task involving different languages. Our results show that mStyleDistance embeddings outperform existing models on these multilingual style benchmarks and generalize well to unseen features and languages. We make our model publicly available at https://huggingface.co/StyleDistance/mstyledistance .",
      "arxiv_url": "https://arxiv.org/abs/2502.15168",
      "pdf_url": "https://arxiv.org/pdf/2502.15168",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.10022",
      "title": "LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges",
      "authors": [
        "Haoyang Li",
        "Huan Gao",
        "Zhiyuan Zhao",
        "Zhiyu Lin",
        "Junyu Gao",
        "Xuelong Li"
      ],
      "abstract": "The widespread adoption of Large Language Models (LLMs) has heightened concerns about their security, particularly their vulnerability to jailbreak attacks that leverage crafted prompts to generate malicious outputs. While prior research has been conducted on general security capabilities of LLMs, their specific susceptibility to jailbreak attacks in code generation remains largely unexplored. To fill this gap, we propose MalwareBench, a benchmark dataset containing 3,520 jailbreaking prompts for malicious code-generation, designed to evaluate LLM robustness against such threats. MalwareBench is based on 320 manually crafted malicious code generation requirements, covering 11 jailbreak methods and 29 code functionality categories. Experiments show that mainstream LLMs exhibit limited ability to reject malicious code-generation requirements, and the combination of multiple jailbreak methods further reduces the model's security capabilities: specifically, the average rejection rate for malicious content is 60.93%, dropping to 39.92% when combined with jailbreak attack algorithms. Our work highlights that the code security capabilities of LLMs still pose significant challenges.",
      "arxiv_url": "https://arxiv.org/abs/2506.10022",
      "pdf_url": "https://arxiv.org/pdf/2506.10022",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.11257",
      "title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis",
      "authors": [
        "Xinyi Liu",
        "Xiaoyi Zhang",
        "Ziyun Zhang",
        "Yan Lu"
      ],
      "abstract": "Recent advancements in Large Vision-Language Models are accelerating the development of Graphical User Interface (GUI) agents that utilize human-like vision perception capabilities to enhance productivity on digital devices. Compared to approaches predicated on GUI metadata, which are platform-dependent and vulnerable to implementation variations, vision-based approaches offer broader applicability. In this vision-based paradigm, the GUI instruction grounding, which maps user instruction to the location of corresponding element on the given screenshot, remains a critical challenge, particularly due to limited public training dataset and resource-intensive manual instruction data annotation. In this paper, we delve into unexplored challenges in this task including element-to-screen ratio, unbalanced element type, and implicit instruction. To address these challenges, we introduce a large-scale data synthesis pipeline UI-E2I-Synth for generating varying complex instruction datasets using GPT-4o instead of human annotators. Furthermore, we propose a new GUI instruction grounding benchmark UI-I2E-Bench, which is designed to address the limitations of existing benchmarks by incorporating diverse annotation aspects. Our model, trained on the synthesized data, achieves superior performance in GUI instruction grounding, demonstrating the advancements of proposed data synthesis pipeline. The proposed benchmark, accompanied by extensive analyses, provides practical insights for future research in GUI grounding. We will release corresponding artifacts at https://microsoft.github.io/FIVE-UI-Evol/ .",
      "arxiv_url": "https://arxiv.org/abs/2504.11257",
      "pdf_url": "https://arxiv.org/pdf/2504.11257",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.19361",
      "title": "SpeechIQ: Speech-Agentic Intelligence Quotient Across Cognitive Levels in Voice Understanding by Large Language Models",
      "authors": [
        "Zhen Wan",
        "Chao-Han Huck Yang",
        "Yahan Yu",
        "Jinchuan Tian",
        "Sheng Li",
        "Ke Hu",
        "Zhehuai Chen",
        "Shinji Watanabe",
        "Fei Cheng",
        "Chenhui Chu",
        "S. Kurohashi"
      ],
      "abstract": "We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human cognition-inspired evaluation pipeline for voice understanding large language models, LLM Voice, designed to assess their voice understanding ability. Moving beyond popular voice understanding metrics such as word error rate (WER), SIQ examines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy: (1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e., similarity of LLM's interpretations); and (3) Application (i.e., QA accuracy for simulating downstream tasks). We demonstrate that SIQ not only quantifies voice understanding abilities but also provides unified comparisons between cascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation errors in existing benchmarks, and detects hallucinations in LLM Voice. Our framework represents a first-of-its-kind intelligence examination that bridges cognitive principles with voice-oriented benchmarks, while exposing overlooked challenges in multi-modal training. Our code and data will be open source to encourage future studies.",
      "arxiv_url": "https://arxiv.org/abs/2507.19361",
      "pdf_url": "https://arxiv.org/pdf/2507.19361",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2025-07-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.23084",
      "title": "The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction",
      "authors": [
        "Yihuai Hong",
        "Dian Zhou",
        "Meng Cao",
        "Lei Yu",
        "Zhijing Jin"
      ],
      "abstract": "Large language models (LLMs) excel on a variety of reasoning benchmarks, but previous studies suggest they sometimes struggle to generalize to unseen questions, potentially due to over-reliance on memorized training examples. However, the precise conditions under which LLMs switch between reasoning and memorization during text generation remain unclear. In this work, we provide a mechanistic understanding of LLMs' reasoning-memorization dynamics by identifying a set of linear features in the model's residual stream that govern the balance between genuine reasoning and memory recall. These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks. Additionally, we show that intervening in these reasoning features helps the model more accurately activate the most relevant problem-solving capabilities during answer generation. Our findings offer new insights into the underlying mechanisms of reasoning and memory in LLMs and pave the way for the development of more robust and interpretable generative AI systems.",
      "arxiv_url": "https://arxiv.org/abs/2503.23084",
      "pdf_url": "https://arxiv.org/pdf/2503.23084",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f2a1e56162f8e6c0d42d2b6cd6527277ce2f6393",
      "title": "mRAKL: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced Languages",
      "authors": [
        "H. Nigatu",
        "Min Li",
        "Maartje ter Hoeve",
        "Saloni Potdar",
        "Sarah E. Chasins"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/f2a1e56162f8e6c0d42d2b6cd6527277ce2f6393",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f2d25849c3f1667116302e7a0480b7f9d7146a42",
      "title": "CausalLink: An Interactive Evaluation Framework for Causal Reasoning",
      "authors": [
        "Jinyue Feng",
        "Frank Rudzicz"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/f2d25849c3f1667116302e7a0480b7f9d7146a42",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f2ef06ece0209cfb016b10d381b0e99595d08509",
      "title": "PIC: Unlocking Long-Form Text Generation Capabilities of Large Language Models via Position ID Compression",
      "authors": [
        "Haoran Que",
        "Wenge Rong"
      ],
      "abstract": "Long-context understanding is crucial for large language models (LLMs) and has become a fundamental capability for most LLMs. However, beyond the focus on “input-long”, the ability to “output-long” is equally significant, yet it remains underexplored. To address this limitation, we propose a simple, efficient, and plug-in approach, Position ID Compression (PIC) , to unlock the long-form text generation potential of LLMs. The idea is straightforward: by compressing the position ids of the context, we provoke and guide LLMs to generate coherent and longer output. Specifically, we find that directly reducing the position ids by a fixed ratio significantly impacts the generation quality. To mitigate this, we propose two variants of PIC: NTK-aware PIC and Dynamic PIC . Without additional training, both methods enable LLMs to extend their generation length by approximately 1.5 times without compromising generation quality. Furthermore, by integrating supervised fine-tuning ( SFT ) with PIC, we propose PIC-SFT , which further improves LLMs’ long-form text generation capabilities, achieving top performance on HelloBench and LongBench-Write. Extensive experiments demonstrate the effectiveness of our approach.",
      "arxiv_url": "https://www.semanticscholar.org/paper/f2ef06ece0209cfb016b10d381b0e99595d08509",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12085",
      "title": "APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs",
      "authors": [
        "Yuxiang Huang",
        "Mingye Li",
        "Xu Han",
        "Chaojun Xiao",
        "Weilin Zhao",
        "Sun Ao",
        "Hao Zhou",
        "Jie Zhou",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "While long-context inference is crucial for advancing large language model (LLM) applications, its prefill speed remains a significant bottleneck. Current approaches, including sequence parallelism strategies and compute reduction through approximate attention mechanisms, still fall short of delivering optimal inference efficiency. This hinders scaling the inputs to longer sequences and processing long-context queries in a timely manner. To address this, we introduce APB, an efficient long-context inference framework that leverages multi-host approximate attention to enhance prefill speed by reducing compute and enhancing parallelism simultaneously. APB introduces a communication mechanism for essential key-value pairs within a sequence parallelism framework, enabling a faster inference speed while maintaining task performance. We implement APB by incorporating a tailored FlashAttn kernel alongside optimized distribution strategies, supporting diverse models and parallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x compared with FlashAttn, RingAttn, and StarAttn, respectively, without any observable task performance degradation. We provide the implementation and experiment code of APB in https://github.com/thunlp/APB.",
      "arxiv_url": "https://arxiv.org/abs/2502.12085",
      "pdf_url": "https://arxiv.org/pdf/2502.12085",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01840",
      "title": "Minimal Pair-Based Evaluation of Code-Switching",
      "authors": [
        "Igor Sterner",
        "Simone Teufel"
      ],
      "abstract": "There is a lack of an evaluation methodology that estimates the extent to which large language models (LLMs) use code-switching (CS) in the same way as bilinguals. Existing methods do not have wide language coverage, fail to account for the diverse range of CS phenomena, or do not scale. We propose an intervention based on minimal pairs of CS. Each minimal pair contains one naturally occurring CS sentence and one minimally manipulated variant. We collect up to 1,000 such pairs each for 11 language pairs. Our human experiments show that, for every language pair, bilinguals consistently prefer the naturally occurring CS sentence. Meanwhile our experiments with current LLMs show that the larger the model, the more consistently it assigns higher probability to the naturally occurring CS sentence than to the variant. In accordance with theoretical claims, the largest probability differences arise in those pairs where the manipulated material consisted of closed-class words.",
      "arxiv_url": "https://arxiv.org/abs/2506.01840",
      "pdf_url": "https://arxiv.org/pdf/2506.01840",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.18203",
      "title": "Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation",
      "authors": [
        "Kyubeen Han",
        "Junseo Jang",
        "Hongjin Kim",
        "Geunyeong Jeong",
        "Harksoo Kim"
      ],
      "abstract": "Instruction-tuning enhances the ability of large language models (LLMs) to follow user instructions more accurately, improving usability while reducing harmful outputs. However, this process may increase the model's dependence on user input, potentially leading to the unfiltered acceptance of misinformation and the generation of hallucinations. Existing studies primarily highlight that LLMs are receptive to external information that contradict their parametric knowledge, but little research has been conducted on the direct impact of instruction-tuning on this phenomenon. In our study, we investigate the impact of instruction-tuning on LLM's susceptibility to misinformation. Our analysis reveals that instruction-tuned LLMs are significantly more likely to accept misinformation when it is presented by the user. A comparison with base models shows that instruction-tuning increases reliance on user-provided information, shifting susceptibility from the assistant role to the user role. Furthermore, we explore additional factors influencing misinformation susceptibility, such as the role of the user in prompt structure, misinformation length, and the presence of warnings in the system prompt. Our findings underscore the need for systematic approaches to mitigate unintended consequences of instruction-tuning and enhance the reliability of LLMs in real-world applications.",
      "arxiv_url": "https://arxiv.org/abs/2507.18203",
      "pdf_url": "https://arxiv.org/pdf/2507.18203",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19356",
      "title": "Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval",
      "authors": [
        "Kidist Amde Mekonnen",
        "Yosef Alemneh",
        "M. D. Rijke"
      ],
      "abstract": "Neural retrieval methods using transformer-based pre-trained language models have advanced multilingual and cross-lingual retrieval. However, their effectiveness for low-resource, morphologically rich languages such as Amharic remains underexplored due to data scarcity and suboptimal tokenization. We address this gap by introducing Amharic-specific dense retrieval models based on pre-trained Amharic BERT and RoBERTa backbones. Our proposed RoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative improvement in MRR@10 and a 9.86% gain in Recall@10 over the strongest multilingual baseline, Arctic Embed 2.0 (568M parameters). More compact variants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while being over 13x smaller. Additionally, we train a ColBERT-based late interaction retrieval model that achieves the highest MRR@10 score (0.843) among all evaluated models. We benchmark our proposed models against both sparse and dense retrieval baselines to systematically assess retrieval effectiveness in Amharic. Our analysis highlights key challenges in low-resource settings and underscores the importance of language-specific adaptation. To foster future research in low-resource IR, we publicly release our dataset, codebase, and trained models at https://github.com/kidist-amde/amharic-ir-benchmarks.",
      "arxiv_url": "https://arxiv.org/abs/2505.19356",
      "pdf_url": "https://arxiv.org/pdf/2505.19356",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00304",
      "title": "Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion with LLMs",
      "authors": [
        "Payal Mohapatra",
        "Akash Pandey",
        "Xiaoyuan Zhang",
        "Qi Zhu"
      ],
      "abstract": "Unvoiced electromyography (EMG) is an effective communication tool for individuals unable to produce vocal speech. However, most prior methods rely on paired voiced and unvoiced EMG signals, along with speech data, for EMG-to-text conversion, which is not practical for such individuals. Given the rise of large language models (LLMs) in speech recognition, we explore their potential to understand unvoiced speech. To this end, we address the challenge of learning from unvoiced EMG alone and propose a novel EMG adaptor module that maps EMG features into an LLM's input space, achieving an average word error rate (WER) of 0.49 on a closed-vocabulary unvoiced EMG-to-text task. Even with a conservative data availability of just six minutes, our approach improves performance over specialized models by nearly 20%. While LLMs have been shown to be extendable to new language modalities -- such as audio -- understanding articulatory biosignals like unvoiced EMG remains more challenging. This work takes a crucial first step toward enabling LLMs to comprehend unvoiced speech using surface EMG.",
      "arxiv_url": "https://arxiv.org/abs/2506.00304",
      "pdf_url": "https://arxiv.org/pdf/2506.00304",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11061",
      "title": "Déjà Vu? Decoding Repeated Reading from Eye Movements",
      "authors": [
        "Yoav Meiri",
        "Omer Shubi",
        "Cfir Avraham Hadar",
        "Ariel Kreisberg Nitzav",
        "Yevgeni Berzak"
      ],
      "abstract": "Be it your favorite novel, a newswire article, a cooking recipe or an academic paper -- in many daily situations we read the same text more than once. In this work, we ask whether it is possible to automatically determine whether the reader has previously encountered a text based on their eye movement patterns. We introduce two variants of this task and address them with considerable success using both feature-based and neural models. We further introduce a general strategy for enhancing these models with machine generated simulations of eye movements from a cognitive model. Finally, we present an analysis of model performance which on the one hand yields insights on the information used by the models, and on the other hand leverages predictive modeling as an analytic tool for better characterization of the role of memory in repeated reading. Our work advances the understanding of the extent and manner in which eye movements in reading capture memory effects from prior text exposure, and paves the way for future applications that involve predictive modeling of repeated reading.",
      "arxiv_url": "https://arxiv.org/abs/2502.11061",
      "pdf_url": "https://arxiv.org/pdf/2502.11061",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f3b5178db3b22b4b971ab3bc5c492e23f8f108b2",
      "title": "English-based acoustic models perform well in the forced alignment of two English-based Pacific Creoles",
      "authors": [
        "Sam Passmore",
        "L. S. Roque",
        "Kirsty Gillespie",
        "Saurabh Nath",
        "Kira Davey",
        "Keira Mullan",
        "Tim Cawley",
        "Jennifer Biggs",
        "Rosey Billington",
        "Bethwyn Evans",
        "Nick Thieberger",
        "Danielle Barth"
      ],
      "abstract": "Expanding the breadth of languages used to study sociophonetic variation and change is an important step in the theoretical development of sociophonetics. As data archives grow, forced alignment can accelerate the study of sociophonetic variation in minority languages. This paper examines the application of English and custom-made acoustic models on the alignment of vowels in two Pacific Creoles, Tok Pisin (59 hours) and Bislama (38.5 hours). We find that English models perform acceptably well in both languages, and as well as humans in vowel environments described as ‘Highly Reliable’. Custom models performed better in Bislama than Tok Pisin. We end the paper with recommendations on the use of cross-linguistic acoustic models in the case of English-Based Creoles.",
      "arxiv_url": "https://www.semanticscholar.org/paper/f3b5178db3b22b4b971ab3bc5c492e23f8f108b2",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17848",
      "title": "LR2Bench: Evaluating Long-chain Reflective Reasoning Capabilities of Large Language Models via Constraint Satisfaction Problems",
      "authors": [
        "Jianghao Chen",
        "Zhenlin Wei",
        "Zhenjiang Ren",
        "Ziyong Li",
        "Jiajun Zhang"
      ],
      "abstract": "Recent progress in Large Reasoning Models (LRMs) has significantly enhanced the reasoning abilities of Large Language Models (LLMs), empowering them to tackle increasingly complex tasks through reflection capabilities, such as making assumptions, backtracking, and self-refinement. However, effectively evaluating such reflection capabilities remains challenging due to the lack of appropriate benchmarks. To bridge this gap, we introduce LR$^2$Bench, a novel benchmark designed to evaluate the Long-chain Reflective Reasoning capabilities of LLMs. LR$^2$Bench comprises 850 samples across six Constraint Satisfaction Problems (CSPs) where reflective reasoning is crucial for deriving solutions that meet all given constraints. Each type of task focuses on distinct constraint patterns, such as knowledge-based, logical, and spatial constraints, providing a comprehensive evaluation of diverse problem-solving scenarios. Our extensive evaluation on both conventional LLMs and LRMs reveals that even the most advanced LRMs, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in LR$^2$Bench, achieving an average Exact Match score of only 20.0% and 23.6%, respectively. These findings underscore the significant room for improvement in the reflective reasoning capabilities of current LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2502.17848",
      "pdf_url": "https://arxiv.org/pdf/2502.17848",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.05021",
      "title": "Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety",
      "authors": [
        "Yuyou Zhang",
        "Miao Li",
        "William Jongwon Han",
        "Yi-Fan Yao",
        "Zhepeng Cen",
        "Ding Zhao"
      ],
      "abstract": "Large Language Models (LLMs) are vulnerable to jailbreak attacks that exploit weaknesses in traditional safety alignment, which often relies on rigid refusal heuristics or representation engineering to block harmful outputs. While they are effective for direct adversarial attacks, they fall short of broader safety challenges requiring nuanced, context-aware decision-making. To address this, we propose Reasoning-enhanced Finetuning for interpretable LLM Safety (Rational), a novel framework that trains models to engage in explicit safe reasoning before response. Fine-tuned models leverage the extensive pretraining knowledge in self-generated reasoning to bootstrap their own safety through structured reasoning, internalizing context-sensitive decision-making. Our findings suggest that safety extends beyond refusal, requiring context awareness for more robust, interpretable, and adaptive responses. Reasoning is not only a core capability of LLMs but also a fundamental mechanism for LLM safety. Rational employs reasoning-enhanced fine-tuning, allowing it to reject harmful prompts while providing meaningful and context-aware responses in complex scenarios.",
      "arxiv_url": "https://arxiv.org/abs/2503.05021",
      "pdf_url": "https://arxiv.org/pdf/2503.05021",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.01668",
      "title": "CoT-based Synthesizer: Enhancing LLM Performance through Answer Synthesis",
      "authors": [
        "Bohan Zhang",
        "Xiaokang Zhang",
        "Jing Zhang",
        "Jifan Yu",
        "Sijia Luo",
        "Jie Tang"
      ],
      "abstract": "Current inference scaling methods, such as Self-consistency and Best-of-N, have proven effective in improving the accuracy of LLMs on complex reasoning tasks. However, these methods rely heavily on the quality of candidate responses and are unable to produce correct answers when all candidates are incorrect. In this paper, we propose a novel inference scaling strategy, CoT-based Synthesizer, which leverages CoT reasoning to synthesize superior answers by analyzing complementary information from multiple candidate responses, even when all candidate responses are flawed. To enable a lightweight and cost-effective implementation, we introduce an automated data generation pipeline that creates diverse training data. This allows smaller LLMs trained on this data to improve the inference accuracy of larger models, including API-based LLMs. Experimental results across four benchmark datasets with seven policy models demonstrate that our method significantly enhances performance, with gains of 11.8% for Llama3-8B and 10.3% for GPT-4o on the MATH dataset. The corresponding training data and code are publicly available on https://github.com/RUCKBReasoning/CoT-based-Synthesizer.",
      "arxiv_url": "https://arxiv.org/abs/2501.01668",
      "pdf_url": "https://arxiv.org/pdf/2501.01668",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20154",
      "title": "UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models",
      "authors": [
        "Xueyan Zhang",
        "Jinman Zhao",
        "Zhifei Yang",
        "Yibo Zhong",
        "Shuhao Guan",
        "Linbo Cao",
        "Yining Wang"
      ],
      "abstract": "This paper introduces Uniform Orthogonal Reinitialization Adaptation (UORA), a novel parameter-efficient fine-tuning (PEFT) approach for Large Language Models (LLMs). UORA achieves state-of-the-art performance and parameter efficiency by leveraging a low-rank approximation method to reduce the number of trainable parameters. Unlike existing methods such as LoRA and VeRA, UORA employs an interpolation-based reparametrization mechanism that selectively reinitializes rows and columns in frozen projection matrices, guided by the vector magnitude heuristic. This results in substantially fewer trainable parameters compared to LoRA and outperforms VeRA in computation and storage efficiency. Comprehensive experiments across various benchmarks demonstrate UORA's superiority in achieving competitive fine-tuning performance with negligible computational overhead. We demonstrate its performance on GLUE and E2E benchmarks and its effectiveness in instruction-tuning large language models and image classification models. Our contributions establish a new paradigm for scalable and resource-efficient fine-tuning of LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2505.20154",
      "pdf_url": "https://arxiv.org/pdf/2505.20154",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.11315",
      "title": "MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens",
      "authors": [
        "Jeong Hun Yeo",
        "Hyeongseop Rha",
        "Se Jin Park",
        "Y. Ro"
      ],
      "abstract": "Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early AV-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.72% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%.",
      "arxiv_url": "https://arxiv.org/abs/2503.11315",
      "pdf_url": "https://arxiv.org/pdf/2503.11315",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-14",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12782",
      "title": "VidCapBench: A Comprehensive Benchmark of Video Captioning for Controllable Text-to-Video Generation",
      "authors": [
        "Xinlong Chen",
        "Yuanxing Zhang",
        "Chongling Rao",
        "Yushuo Guan",
        "Jiaheng Liu",
        "Fuzheng Zhang",
        "Chengru Song",
        "Qiang Liu",
        "Di Zhang",
        "Tieniu Tan"
      ],
      "abstract": "The training of controllable text-to-video (T2V) models relies heavily on the alignment between videos and captions, yet little existing research connects video caption evaluation with T2V generation assessment. This paper introduces VidCapBench, a video caption evaluation scheme specifically designed for T2V generation, agnostic to any particular caption format. VidCapBench employs a data annotation pipeline, combining expert model labeling and human refinement, to associate each collected video with key information spanning video aesthetics, content, motion, and physical laws. VidCapBench then partitions these key information attributes into automatically assessable and manually assessable subsets, catering to both the rapid evaluation needs of agile development and the accuracy requirements of thorough validation. By evaluating numerous state-of-the-art captioning models, we demonstrate the superior stability and comprehensiveness of VidCapBench compared to existing video captioning evaluation approaches. Verification with off-the-shelf T2V models reveals a significant positive correlation between scores on VidCapBench and the T2V quality evaluation metrics, indicating that VidCapBench can provide valuable guidance for training T2V models. The project is available at https://github.com/VidCapBench/VidCapBench.",
      "arxiv_url": "https://arxiv.org/abs/2502.12782",
      "pdf_url": "https://arxiv.org/pdf/2502.12782",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.15848",
      "title": "Entropy-based Exploration Conduction for Multi-step Reasoning",
      "authors": [
        "Jinghan Zhang",
        "Xiting Wang",
        "Fengran Mo",
        "Y Zhou",
        "Wanfu Gao",
        "Kunpeng Liu"
      ],
      "abstract": "Multi-step processes via large language models (LLMs) have proven effective for solving complex reasoning tasks. However, the depth of exploration of the reasoning procedure can significantly affect the task performance. Existing methods to automatically decide the depth often lead to high cost and a lack of flexibility. To address these issues, we propose Entropy-based Exploration Depth Conduction (Entro-duction), a novel method that dynamically adjusts the exploration depth during multi-step reasoning by monitoring LLM's output entropy and variance entropy. We employ these two features to capture the model's uncertainty of the current step and the fluctuation of uncertainty across consecutive reasoning steps. Based on the observed entropy changes, the LLM selects whether to deepen, expand, or stop exploration according to the probability, which facilitates the trade-off between the reasoning accuracy and exploration effectiveness. Experimental results across four benchmark datasets demonstrate the efficacy of Entro-duction.",
      "arxiv_url": "https://arxiv.org/abs/2503.15848",
      "pdf_url": "https://arxiv.org/pdf/2503.15848",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.19362",
      "title": "LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences",
      "authors": [
        "Yusuke Hirota",
        "Boyi Li",
        "Ryo Hachiuma",
        "Yueh-Hua Wu",
        "B. Ivanovic",
        "Yuta Nakashima",
        "Marco Pavone",
        "Yejin Choi",
        "Yu-Chiang Frank Wang",
        "Chao-Han Huck Yang"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have transformed image captioning, shifting from concise captions to detailed descriptions. We introduce LOTUS, a leaderboard for evaluating detailed captions, addressing three main gaps in existing evaluations: lack of standardized criteria, bias-aware assessments, and user preference considerations. LOTUS comprehensively evaluates various aspects, including caption quality (e.g., alignment, descriptiveness), risks (\\eg, hallucination), and societal biases (e.g., gender bias) while enabling preference-oriented evaluations by tailoring criteria to diverse user preferences. Our analysis of recent LVLMs reveals no single model excels across all criteria, while correlations emerge between caption detail and bias risks. Preference-oriented evaluations demonstrate that optimal model selection depends on user priorities.",
      "arxiv_url": "https://arxiv.org/abs/2507.19362",
      "pdf_url": "https://arxiv.org/pdf/2507.19362",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Personalization"
      ],
      "published_date": "2025-07-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.18228",
      "title": "Debt Collection Negotiations with Large Language Models: An Evaluation System and Optimizing Decision Making with Multi-Agent",
      "authors": [
        "Xiaofeng Wang",
        "Zhixin Zhang",
        "Jinguang Zheng",
        "Yiming Ai",
        "Rui Wang"
      ],
      "abstract": "Debt collection negotiations (DCN) are vital for managing non-performing loans (NPLs) and reducing creditor losses. Traditional methods are labor-intensive, while large language models (LLMs) offer promising automation potential. However, prior systems lacked dynamic negotiation and real-time decision-making capabilities. This paper explores LLMs in automating DCN and proposes a novel evaluation framework with 13 metrics across 4 aspects. Our experiments reveal that LLMs tend to over-concede compared to human negotiators. To address this, we propose the Multi-Agent Debt Negotiation (MADeN) framework, incorporating planning and judging modules to improve decision rationality. We also apply post-training techniques, including DPO with rejection sampling, to optimize performance. Our studies provide valuable insights for practitioners and researchers seeking to enhance efficiency and outcomes in this domain.",
      "arxiv_url": "https://arxiv.org/abs/2502.18228",
      "pdf_url": "https://arxiv.org/pdf/2502.18228",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.20252",
      "title": "LogicQA: Logical Anomaly Detection with Vision Language Model Generated Questions",
      "authors": [
        "Yejin Kwon",
        "Daeun Moon",
        "Youngje Oh",
        "Hyunsoo Yoon"
      ],
      "abstract": "Anomaly Detection (AD) focuses on detecting samples that differ from the standard pattern, making it a vital tool in process control. Logical anomalies may appear visually normal yet violate predefined constraints on object presence, arrangement, or quantity, depending on reasoning and explainability. We introduce LogicQA, a framework that enhances AD by providing industrial operators with explanations for logical anomalies. LogicQA compiles automatically generated questions into a checklist and collects responses to identify violations of logical constraints. LogicQA is training-free, annotation-free, and operates in a few-shot setting. We achieve state-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO AD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the explanations of anomalies. Also, our approach has shown outstanding performance on semiconductor SEM corporate data, further validating its effectiveness in industrial applications.",
      "arxiv_url": "https://arxiv.org/abs/2503.20252",
      "pdf_url": "https://arxiv.org/pdf/2503.20252",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.18171",
      "title": "Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models",
      "authors": [
        "Kexin Chen",
        "Dongxia Wang",
        "Yi Liu",
        "Haonan Zhang",
        "Wenhai Wang"
      ],
      "abstract": "Despite the widespread use of Transformer-based text embedding models in NLP tasks, surprising'sticky tokens'can undermine the reliability of embeddings. These tokens, when repeatedly inserted into sentences, pull sentence similarity toward a certain value, disrupting the normal distribution of embedding distances and degrading downstream performance. In this paper, we systematically investigate such anomalous tokens, formally defining them and introducing an efficient detection method, Sticky Token Detector (STD), based on sentence and token filtering. Applying STD to 40 checkpoints across 14 model families, we discover a total of 868 sticky tokens. Our analysis reveals that these tokens often originate from special or unused entries in the vocabulary, as well as fragmented subwords from multilingual corpora. Notably, their presence does not strictly correlate with model size or vocabulary size. We further evaluate how sticky tokens affect downstream tasks like clustering and retrieval, observing significant performance drops of up to 50%. Through attention-layer analysis, we show that sticky tokens disproportionately dominate the model's internal representations, raising concerns about tokenization robustness. Our findings show the need for better tokenization strategies and model design to mitigate the impact of sticky tokens in future text embedding applications.",
      "arxiv_url": "https://arxiv.org/abs/2507.18171",
      "pdf_url": "https://arxiv.org/pdf/2507.18171",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.20046",
      "title": "Infogen: Generating Complex Statistical Infographics from Documents",
      "authors": [
        "Akash Ghosh",
        "Aparna Garimella",
        "Pritika Ramu",
        "Sambaran Bandyopadhyay",
        "Sriparna Saha"
      ],
      "abstract": "Statistical infographics are powerful tools that simplify complex data into visually engaging and easy-to-understand formats. Despite advancements in AI, particularly with LLMs, existing efforts have been limited to generating simple charts, with no prior work addressing the creation of complex infographics from text-heavy documents that demand a deep understanding of the content. We address this gap by introducing the task of generating statistical infographics composed of multiple sub-charts (e.g., line, bar, pie) that are contextually accurate, insightful, and visually aligned. To achieve this, we define infographic metadata that includes its title and textual insights, along with sub-chart-specific details such as their corresponding data and alignment. We also present Infodat, the first benchmark dataset for text-to-infographic metadata generation, where each sample links a document to its metadata. We propose Infogen, a two-stage framework where fine-tuned LLMs first generate metadata, which is then converted into infographic code. Extensive evaluations on Infodat demonstrate that Infogen achieves state-of-the-art performance, outperforming both closed and open-source LLMs in text-to-statistical infographic generation.",
      "arxiv_url": "https://arxiv.org/abs/2507.20046",
      "pdf_url": "https://arxiv.org/pdf/2507.20046",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12204",
      "title": "Predicting Depression in Screening Interviews from Interactive Multi-Theme Collaboration",
      "authors": [
        "Xianbing Zhao",
        "Yiqing Lyu",
        "Di Wang",
        "Buzhou Tang"
      ],
      "abstract": "Automatic depression detection provides cues for early clinical intervention by clinicians. Clinical interviews for depression detection involve dialogues centered around multiple themes. Existing studies primarily design end-to-end neural network models to capture the hierarchical structure of clinical interview dialogues. However, these methods exhibit defects in modeling the thematic content of clinical interviews: 1) they fail to capture intra-theme and inter-theme correlation explicitly, and 2) they do not allow clinicians to intervene and focus on themes of interest. To address these issues, this paper introduces an interactive depression detection framework. This framework leverages in-context learning techniques to identify themes in clinical interviews and then models both intra-theme and inter-theme correlation. Additionally, it employs AI-driven feedback to simulate the interests of clinicians, enabling interactive adjustment of theme importance. PDIMC achieves absolute improvements of 35\\% and 12\\% compared to the state-of-the-art on the depression detection dataset DAIC-WOZ, which demonstrates the effectiveness of modeling theme correlation and incorporating interactive external feedback.",
      "arxiv_url": "https://arxiv.org/abs/2502.12204",
      "pdf_url": "https://arxiv.org/pdf/2502.12204",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.08691",
      "title": "VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism",
      "authors": [
        "Congzhi Zhang",
        "Jiawei Peng",
        "Zhenglin Wang",
        "Yilong Lai",
        "Haowen Sun",
        "Heng Chang",
        "Fei Ma",
        "Weijiang Yu"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have shown exceptional performance in multimodal tasks, but their effectiveness in complex visual reasoning is still constrained, especially when employing Chain-of-Thought prompting techniques. In this paper, we propose VReST, a novel training-free approach that enhances Reasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms. VReST meticulously traverses the reasoning landscape by establishing a search tree, where each node encapsulates a reasoning step, and each path delineates a comprehensive reasoning sequence. Our innovative multimodal Self-Reward mechanism assesses the quality of reasoning steps by integrating the utility of sub-questions, answer correctness, and the relevance of vision-language clues, all without the need for additional models. VReST surpasses current prompting methods and secures state-of-the-art performance across three multimodal mathematical reasoning benchmarks. Furthermore, it substantiates the efficacy of test-time scaling laws in multimodal tasks, offering a promising direction for future research.",
      "arxiv_url": "https://arxiv.org/abs/2506.08691",
      "pdf_url": "https://arxiv.org/pdf/2506.08691",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.17338",
      "title": "Inferring from Logits: Exploring Best Practices for Decoding-Free Generative Candidate Selection",
      "authors": [
        "Mingyu Derek Ma",
        "Yanna Ding",
        "Zijie Huang",
        "Jianxi Gao",
        "Yizhou Sun",
        "Wei Wang"
      ],
      "abstract": "Generative Language Models rely on autoregressive decoding to produce the output sequence token by token. Many tasks such as preference optimization, require the model to produce task-level output consisting of multiple tokens directly by selecting candidates from a pool as predictions. Determining a task-level prediction from candidates using the ordinary token-level decoding mechanism is constrained by time-consuming decoding and interrupted gradients by discrete token selection. Existing works have been using decoding-free candidate selection methods to obtain candidate probability from initial output logits over vocabulary. Though these estimation methods are widely used, they are not systematically evaluated, especially on end tasks. We introduce an evaluation of a comprehensive collection of decoding-free candidate selection approaches on a comprehensive set of tasks, including five multiple-choice QA tasks with a small candidate pool and four clinical decision tasks with a massive amount of candidates, some with 10k+ options. We evaluate the estimation methods paired with a wide spectrum of foundation LMs covering different architectures, sizes and training paradigms. The results and insights from our analysis inform the future model design.",
      "arxiv_url": "https://arxiv.org/abs/2501.17338",
      "pdf_url": "https://arxiv.org/pdf/2501.17338",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-01-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11880",
      "title": "Bitnet.cpp: Efficient Edge Inference for Ternary LLMs",
      "authors": [
        "Jinheng Wang",
        "Hansong Zhou",
        "Ting Song",
        "Shijie Cao",
        "Yan Xia",
        "Ting Cao",
        "Jianyu Wei",
        "Shuming Ma",
        "Hongyu Wang",
        "Furu Wei"
      ],
      "abstract": "The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has spurred interest in ternary LLMs. Despite this, research and practical applications focusing on efficient edge inference for ternary LLMs remain scarce. To bridge this gap, we introduce Bitnet.cpp, an inference system optimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix multiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs, Bitnet.cpp incorporates a novel mpGEMM library to facilitate sub-2-bits-per-weight, efficient and lossless inference. The library features two core solutions: Ternary Lookup Table (TL), which addresses spatial inefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S), which ensures lossless edge inference, both enabling high-speed inference. Our experiments show that Bitnet.cpp achieves up to a 6.25x increase in speed over full-precision baselines and up to 2.32x over low-bit baselines, setting new benchmarks in the field. Additionally, we expand TL to element-wise lookup table (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and empirical evidence of its considerable potential. Bitnet.cpp is publicly available at https://github.com/microsoft/BitNet/tree/paper , offering a sophisticated solution for the efficient and practical deployment of edge LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2502.11880",
      "pdf_url": "https://arxiv.org/pdf/2502.11880",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11703",
      "title": "CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models in Medical Quality Control Indicator Calculation",
      "authors": [
        "Guangya Yu",
        "Yanhao Li",
        "Zongying Jiang",
        "Yuxiong Jin",
        "Li Dai",
        "Yupian Lin",
        "Ruihui Hou",
        "Weiyan Zhang",
        "Yongqi Fan",
        "Qi Ye",
        "Jingping Liu",
        "Tong Ruan"
      ],
      "abstract": "Medical quality control indicators are essential to assess the qualifications of healthcare institutions for medical services. With the impressive performance of large language models (LLMs) like GPT-4 in the medical field, leveraging these technologies for the Medical Quality Control Indicator Calculation (MQCIC) presents a promising approach. In this work, (1) we introduce a real-world task MQCIC and propose an open-source Chinese electronic medical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances and 76 indicators. (2) We propose a semi-automatic method to enhance the rule representation. Then we propose the Clinical Facts-based Inferential Rule (CF-IR) method that disentangles the clinical fact verification and inferential rule reasoning actions. (3) We conduct comprehensive experiments on 20 representative LLMs, covering general and medical models. Our findings reveal that CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct an error analysis and investigate the capabilities of clinical fact verification and inferential rule reasoning, providing insights to improve performance in the MQCIC further. The dataset and code is available in this repository https://github.com/YuY-2001/C-MQCIC.",
      "arxiv_url": "https://arxiv.org/abs/2502.11703",
      "pdf_url": "https://arxiv.org/pdf/2502.11703",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24561",
      "title": "Improving Language and Modality Transfer in Translation by Character-level Modeling",
      "authors": [
        "Yiannis (Ioannis) Tsiamas",
        "David Dale",
        "M. Costa-jussà"
      ],
      "abstract": "Current translation systems, despite being highly multilingual, cover only 5% of the world's languages. Expanding language coverage to the long-tail of low-resource languages requires data-efficient methods that rely on cross-lingual and cross-modal knowledge transfer. To this end, we propose a character-based approach to improve adaptability to new languages and modalities. Our method leverages SONAR, a multilingual fixed-size embedding space with different modules for encoding and decoding. We use a teacher-student approach with parallel translation data to obtain a character-level encoder. Then, using ASR data, we train a lightweight adapter to connect a massively multilingual CTC ASR model (MMS), to the character-level encoder, potentially enabling speech translation from 1,000+ languages. Experimental results in text translation for 75 languages on FLORES+ demonstrate that our character-based approach can achieve better language transfer than traditional subword-based models, especially outperforming them in low-resource settings, and demonstrating better zero-shot generalizability to unseen languages. Our speech adaptation, maximizing knowledge transfer from the text modality, achieves state-of-the-art results in speech-to-text translation on the FLEURS benchmark on 33 languages, surpassing previous supervised and cascade models, albeit being a zero-shot model with minimal supervision from ASR data.",
      "arxiv_url": "https://arxiv.org/abs/2505.24561",
      "pdf_url": "https://arxiv.org/pdf/2505.24561",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16128",
      "title": "Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning",
      "authors": [
        "Yue Zhou",
        "Barbara Di Eugenio"
      ],
      "abstract": "Despite LLMs' explicit alignment against demographic stereotypes, they have been shown to exhibit biases under various social contexts. In this work, we find that LLMs exhibit concerning biases in how they associate solution veracity with demographics. Through experiments across five human value-aligned LLMs on mathematics, coding, commonsense, and writing problems, we reveal two forms of such veracity biases: Attribution Bias, where models disproportionately attribute correct solutions to certain demographic groups, and Evaluation Bias, where models' assessment of identical solutions varies based on perceived demographic authorship. Our results show pervasive biases: LLMs consistently attribute fewer correct solutions and more incorrect ones to African-American groups in math and coding, while Asian authorships are least preferred in writing evaluation. In additional studies, we show LLMs automatically assign racially stereotypical colors to demographic groups in visualization code, suggesting these biases are deeply embedded in models' reasoning processes. Our findings indicate that demographic bias extends beyond surface-level stereotypes and social context provocations, raising concerns about LLMs' deployment in educational and evaluation settings.",
      "arxiv_url": "https://arxiv.org/abs/2505.16128",
      "pdf_url": "https://arxiv.org/pdf/2505.16128",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.05506",
      "title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question Answering",
      "authors": [
        "Ahmed Masry",
        "Mohammed Saidul Islam",
        "Mahir Ahmed",
        "Aayush Bajaj",
        "Firoz Kabir",
        "Aaryaman Kartha",
        "Md Tahmid Rahman Laskar",
        "Mizanur Rahman",
        "Shadikur Rahman",
        "Mehrad Shahmohammadi",
        "Megh Thakkar",
        "Md. Rizwan Parvez",
        "Enamul Hoque",
        "Shafiq R. Joty"
      ],
      "abstract": "Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart types, including infographics and dashboards, and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.",
      "arxiv_url": "https://arxiv.org/abs/2504.05506",
      "pdf_url": "https://arxiv.org/pdf/2504.05506",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-04-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02204",
      "title": "BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models",
      "authors": [
        "Lindia Tjuatja",
        "Graham Neubig"
      ],
      "abstract": "Language model evaluation is a daunting task: prompts are brittle, corpus-level perplexities are vague, and the choice of benchmarks are endless. Finding examples that show meaningful, generalizable differences between two LMs is crucial to understanding where one model succeeds and another fails. Can this process be done automatically? In this work, we propose methodology for automated comparison of language models that uses performance-aware contextual embeddings to find fine-grained features of text where one LM outperforms another. Our method, which we name BehaviorBox, extracts coherent features that demonstrate differences with respect to the ease of generation between two LMs. Specifically, BehaviorBox finds features that describe groups of words in fine-grained contexts, such as\"conditional 'were' in the phrase 'if you were'\"and\"exclamation marks after emotional statements\", where one model outperforms another within a particular datatset. We apply BehaviorBox to compare models that vary in size, model family, and post-training, and enumerate insights into specific contexts that illustrate meaningful differences in performance which cannot be found by measures such as corpus-level perplexity alone.",
      "arxiv_url": "https://arxiv.org/abs/2506.02204",
      "pdf_url": "https://arxiv.org/pdf/2506.02204",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.14523",
      "title": "Exploring Graph Representations of Logical Forms for Language Modeling",
      "authors": [
        "Michael Sullivan"
      ],
      "abstract": "We make the case for language models over logical forms (LFLMs), arguing that such models are more data-efficient than their textual counterparts. To that end, we introduce the Graph-based Formal-Logical Distributional Semantics (GFoLDS) prototype, a pretrained LM over graph representations of logical forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong experimental evidence that LFLMs can leverage the built-in, basic linguistic knowledge inherent in such models to immediately begin learning more complex patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual, transformer LMs (BERT) pretrained on the same data, indicating that LFLMs can learn with substantially less data than models over plain text. Furthermore, we show that the performance of this model is likely to scale with additional parameters and pretraining data, suggesting the viability of LFLMs in real-world applications.",
      "arxiv_url": "https://arxiv.org/abs/2505.14523",
      "pdf_url": "https://arxiv.org/pdf/2505.14523",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.00977",
      "title": "Context-Aware Hierarchical Merging for Long Document Summarization",
      "authors": [
        "Litu Ou",
        "Mirella Lapata"
      ],
      "abstract": "Hierarchical Merging is a technique commonly used to summarize very long texts ($>$100K tokens) by breaking down the input into smaller sections, summarizing those sections individually, and then merging or combining those summaries into a final coherent summary. Although it helps address the limitations of large language models (LLMs) with fixed input length constraints, the recursive merging process can amplify LLM hallucinations, increasing the risk of factual inaccuracies. In this paper, we seek to mitigate hallucinations by enriching hierarchical merging with context from the source document. Specifically, we propose different approaches to contextual augmentation ranging from \\emph{replacing} intermediate summaries with relevant input context, to \\emph{refining} them while using the context as supporting evidence, and \\emph{aligning} them implicitly (via citations) to the input. Experimental results on datasets representing legal and narrative domains show that contextual augmentation consistently outperforms zero-shot and hierarchical merging baselines for the Llama 3.1 model family. Our analysis further reveals that refinement methods tend to perform best when paired with extractive summarization for identifying relevant input.",
      "arxiv_url": "https://arxiv.org/abs/2502.00977",
      "pdf_url": "https://arxiv.org/pdf/2502.00977",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.07322",
      "title": "MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject Batch Editing for LLMs",
      "authors": [
        "Zilu Dong",
        "Xiangqing Shen",
        "Rui Xia"
      ],
      "abstract": "As large language models continue to scale up, knowledge editing techniques that modify models'internal knowledge without full retraining have gained significant attention. MEMIT, a prominent batch editing algorithm, stands out for its capability to perform mass knowledge modifications. However, we uncover that MEMIT's editing efficacy significantly deteriorates when processing batches containing multiple edits sharing the same subject. Our analysis reveals this stems from MEMIT's key value modeling framework: identical keys (derived from the shared subject) are forced to represent different values (corresponding to different knowledge), resulting in update conflicts during editing. Addressing this issue, we propose MEMIT-Merge, an enhanced approach that merges value computation processes for facts sharing the same subject, effectively resolving the performance degradation in samesubject batch editing scenarios. Experimental results demonstrate that when MEMIT's edit success rate drops to around 50% at larger batch sizes, MEMIT-Merge maintains a success rate exceeding 90%, showcasing remarkable robustness to subject entity collisions. The code is available at https://github.com/NUSTM/ MEMIT-Merge.",
      "arxiv_url": "https://arxiv.org/abs/2502.07322",
      "pdf_url": "https://arxiv.org/pdf/2502.07322",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00134",
      "title": "Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models",
      "authors": [
        "Fardin Ahsan Sakib",
        "Ziwei Zhu",
        "Karen Trister Grace",
        "Meliha Yetisgen",
        "Ozlem Uzuner"
      ],
      "abstract": "Social determinants of health (SDOH) extraction from clinical text is critical for downstream healthcare analytics. Although large language models (LLMs) have shown promise, they may rely on superficial cues leading to spurious predictions. Using the MIMIC portion of the SHAC (Social History Annotation Corpus) dataset and focusing on drug status extraction as a case study, we demonstrate that mentions of alcohol or smoking can falsely induce models to predict current/past drug use where none is present, while also uncovering concerning gender disparities in model performance. We further evaluate mitigation strategies - such as prompt engineering and chain-of-thought reasoning - to reduce these false positives, providing insights into enhancing LLM reliability in health domains.",
      "arxiv_url": "https://arxiv.org/abs/2506.00134",
      "pdf_url": "https://arxiv.org/pdf/2506.00134",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.23523",
      "title": "Question-Aware Knowledge Graph Prompting for Enhancing Large Language Models",
      "authors": [
        "Haochen Liu",
        "Song Wang",
        "Chen Chen",
        "Jundong Li"
      ],
      "abstract": "Large Language Models (LLMs) often struggle with tasks requiring external knowledge, such as knowledge-intensive Multiple Choice Question Answering (MCQA). Integrating Knowledge Graphs (KGs) can enhance reasoning; however, existing methods typically demand costly fine-tuning or retrieve noisy KG information. Recent approaches leverage Graph Neural Networks (GNNs) to generate KG-based input embedding prefixes as soft prompts for LLMs but fail to account for question relevance, resulting in noisy prompts. Moreover, in MCQA tasks, the absence of relevant KG knowledge for certain answer options remains a significant challenge. To address these issues, we propose Question-Aware Knowledge Graph Prompting (QAP), which incorporates question embeddings into GNN aggregation to dynamically assess KG relevance. QAP employs global attention to capture inter-option relationships, enriching soft prompts with inferred knowledge. Experimental results demonstrate that QAP outperforms state-of-the-art methods across multiple datasets, highlighting its effectiveness.",
      "arxiv_url": "https://arxiv.org/abs/2503.23523",
      "pdf_url": "https://arxiv.org/pdf/2503.23523",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.20249",
      "title": "WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models",
      "authors": [
        "Yongan Yu",
        "Qingchen Hu",
        "Xianda Du",
        "Jiayin Wang",
        "Fengran Mo",
        "Renee Sieber"
      ],
      "abstract": "Climate change adaptation requires the understanding of disruptive weather impacts on society, where large language models (LLMs) might be applicable. However, their effectiveness is under-explored due to the difficulty of high-quality corpus collection and the lack of available benchmarks. The climate-related events stored in regional newspapers record how communities adapted and recovered from disasters. However, the processing of the original corpus is non-trivial. In this study, we first develop a disruptive weather impact dataset with a four-stage well-crafted construction pipeline. Then, we propose WXImpactBench, the first benchmark for evaluating the capacity of LLMs on disruptive weather impacts. The benchmark involves two evaluation tasks, multi-label classification and ranking-based question answering. Extensive experiments on evaluating a set of LLMs provide first-hand analysis of the challenges in developing disruptive weather impact understanding and climate change adaptation systems. The constructed dataset and the code for the evaluation framework are available to help society protect against vulnerabilities from disasters.",
      "arxiv_url": "https://arxiv.org/abs/2505.20249",
      "pdf_url": "https://arxiv.org/pdf/2505.20249",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-26",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.03441",
      "title": "Finding A Voice: Exploring the Potential of African American Dialect and Voice Generation for Chatbots",
      "authors": [
        "Sarah E. Finch",
        "Ellie S. Paek",
        "Ikseon Choi",
        "Jinho D. Choi"
      ],
      "abstract": "As chatbots become integral to daily life, personalizing systems is key for fostering trust, engagement, and inclusivity. This study examines how linguistic similarity affects chatbot performance, focusing on integrating African American English (AAE) into virtual agents to better serve the African American community. We develop text-based and spoken chatbots using large language models and text-to-speech technology, then evaluate them with AAE speakers against standard English chatbots. Our results show that while text-based AAE chatbots often underperform, spoken chatbots benefit from an African American voice and AAE elements, improving performance and preference. These findings underscore the complexities of linguistic personalization and the dynamics between text and speech modalities, highlighting technological limitations that affect chatbots'AA speech generation and pointing to promising future research directions.",
      "arxiv_url": "https://arxiv.org/abs/2501.03441",
      "pdf_url": "https://arxiv.org/pdf/2501.03441",
      "primary_category": "",
      "categories": [],
      "tags": [
        "Personalization"
      ],
      "published_date": "2025-01-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01187",
      "title": "LAQuer: Localized Attribution Queries in Content-grounded Generation",
      "authors": [
        "Eran Hirsch",
        "Aviv Slobodkin",
        "David Wan",
        "Elias Stengel-Eskin",
        "Mohit Bansal",
        "Ido Dagan"
      ],
      "abstract": "Grounded text generation models often produce content that deviates from their source material, requiring user verification to ensure accuracy. Existing attribution methods associate entire sentences with source documents, which can be overwhelming for users seeking to fact-check specific claims. In contrast, existing sub-sentence attribution methods may be more precise but fail to align with users' interests. In light of these limitations, we introduce Localized Attribution Queries (LAQuer), a new task that localizes selected spans of generated output to their corresponding source spans, allowing fine-grained and user-directed attribution. We compare two approaches for the LAQuer task, including prompting large language models (LLMs) and leveraging LLM internal representations. We then explore a modeling framework that extends existing attributed text generation methods to LAQuer. We evaluate this framework across two grounded text generation tasks: Multi-document Summarization (MDS) and Long-form Question Answering (LFQA). Our findings show that LAQuer methods significantly reduce the length of the attributed text. Our contributions include: (1) proposing the LAQuer task to enhance attribution usability, (2) suggesting a modeling framework and benchmarking multiple baselines, and (3) proposing a new evaluation setting to promote future research on localized attribution in content-grounded generation.",
      "arxiv_url": "https://arxiv.org/abs/2506.01187",
      "pdf_url": "https://arxiv.org/pdf/2506.01187",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-01",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f7f784e60010db0f10cecbc03152495d61c4e36d",
      "title": "LegoMT2: Selective Asynchronous Sharded Data Parallel Training for Massive Neural Machine Translation",
      "authors": [
        "Fei Yuan",
        "Yinquan Lu",
        "Lei Li",
        "Jingjing Xu"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/f7f784e60010db0f10cecbc03152495d61c4e36d",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11062",
      "title": "Beyond Similarity: A Gradient-based Graph Method for Instruction Tuning Data Selection",
      "authors": [
        "Yang Zhao",
        "Li Du",
        "Xiao Ding",
        "Yangou Ouyang",
        "Hepeng Wang",
        "Kai Xiong",
        "Jin-Fang Gao",
        "Zhouhao Sun",
        "Dongliang Xu",
        "Yang Qing",
        "Dongcheng Li",
        "Bing Qin",
        "Ting Liu"
      ],
      "abstract": "Large language models (LLMs) have shown great potential across various industries due to their remarkable ability to generalize through instruction tuning. However, the limited availability of domain-specific data significantly hampers their performance on specialized tasks. While existing methods primarily focus on selecting training data from general datasets that are similar to the target domain, they often fail to consider the joint distribution of instructions, resulting in inefficient learning and suboptimal knowledge transfer. To address these challenges, we introduce G2IS (Gradient-based Graph Instruction Selection), a novel method that constructs a mixed gradient-based instruction graph to capture the joint distribution and interdependencies between instructions. By accounting for the relationships between instructions, G2IS improves domain adaptation efficiency. Additionally, we propose a gradient walk algorithm to refine the data selection process, enhancing both training effectiveness and efficiency. Our experiments demonstrate that G2IS outperforms traditional methods across various domain adaptation tasks, yielding significant performance gains, particularly in complex, data-scarce scenarios. These results underscore the potential of G2IS in advancing the development of large, domain-specific models.",
      "arxiv_url": "https://arxiv.org/abs/2502.11062",
      "pdf_url": "https://arxiv.org/pdf/2502.11062",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04735",
      "title": "How Personality Traits Shape LLM Risk-Taking Behaviour",
      "authors": [
        "John Hartley",
        "Conor Hamill",
        "Devesh Batra",
        "Dale Seddon",
        "Ramin Okhrati",
        "Raad Khraishi"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed as autonomous agents, necessitating a deeper understanding of their decision-making behaviour under risk. This study investigates the relationship between LLMs' personality traits and risk propensity, employing cumulative prospect theory (CPT) and the Big Five personality framework. We focus on GPT-4o, comparing its behaviour to human baselines and earlier models. Our findings reveal that GPT-4o exhibits higher Conscientiousness and Agreeableness traits compared to human averages, while functioning as a risk-neutral rational agent in prospect selection. Interventions on GPT-4o's Big Five traits, particularly Openness, significantly influence its risk propensity, mirroring patterns observed in human studies. Notably, Openness emerges as the most influential factor in GPT-4o's risk propensity, aligning with human findings. In contrast, legacy models like GPT-4-Turbo demonstrate inconsistent generalization of the personality-risk relationship. This research advances our understanding of LLM behaviour under risk and elucidates the potential and limitations of personality-based interventions in shaping LLM decision-making. Our findings have implications for the development of more robust and predictable AI systems such as financial modelling.",
      "arxiv_url": "https://arxiv.org/abs/2503.04735",
      "pdf_url": "https://arxiv.org/pdf/2503.04735",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.22694",
      "title": "MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning",
      "authors": [
        "Dacao Zhang",
        "Kun Zhang",
        "Shimao Chu",
        "Le Wu",
        "Xin Li",
        "Si Wei"
      ],
      "abstract": "With the rapid development of Large Language Models (LLMs), Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant attention, which aims to achieve efficient fine-tuning of LLMs with fewer parameters. As a representative PEFT method, Low-Rank Adaptation (LoRA) introduces low-rank matrices to approximate the incremental tuning parameters and achieves impressive performance over multiple scenarios. After that, plenty of improvements have been proposed for further improvement. However, these methods either focus on single-task scenarios or separately train multiple LoRA modules for multi-task scenarios, limiting the efficiency and effectiveness of LoRA in multi-task scenarios. To better adapt to multi-task fine-tuning, in this paper, we propose a novel Mixture of Low-Rank Experts (MoRE) for multi-task PEFT. Specifically, instead of using an individual LoRA for each task, we align different ranks of LoRA module with different tasks, which we named low-rank experts. Moreover, we design a novel adaptive rank selector to select the appropriate expert for each task. By jointly training low-rank experts, MoRE can enhance the adaptability and efficiency of LoRA in multi-task scenarios. Finally, we conduct extensive experiments over multiple multi-task benchmarks along with different LLMs to verify model performance. Experimental results demonstrate that compared to traditional LoRA and its variants, MoRE significantly improves the performance of LLMs in multi-task scenarios and incurs no additional inference cost. We also release the model and code to facilitate the community.",
      "arxiv_url": "https://arxiv.org/abs/2505.22694",
      "pdf_url": "https://arxiv.org/pdf/2505.22694",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14911",
      "title": "Batayan: A Filipino NLP benchmark for evaluating Large Language Models",
      "authors": [
        "J. Montalan",
        "J. Layacan",
        "David Demitri Africa",
        "Richell Isaiah Flores",
        "Michael T. Lopez",
        "Theresa Denise Magsajo",
        "Anjanette Cayabyab",
        "William-Chandra Tjhi"
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated remarkable capabilities on widely benchmarked high-resource languages. However, linguistic nuances of under-resourced languages remain unexplored. We introduce Batayan, a holistic Filipino benchmark that systematically evaluates LLMs across three key natural language processing (NLP) competencies: understanding, reasoning, and generation. Batayan consolidates eight tasks, three of which have not existed prior for Filipino corpora, covering both Tagalog and code-switched Taglish utterances. Our rigorous, native-speaker-driven adaptation and validation processes ensures fluency and authenticity to the complex morphological and syntactic structures of Filipino, alleviating the pervasive translationese bias in existing Filipino corpora. We report empirical results on a variety of open-source and commercial LLMs, highlighting significant performance gaps that signal the under-representation of Filipino in pre-training corpora, the unique hurdles in modeling Filipino's rich morphology and construction, and the importance of explicit Filipino language support. Moreover, we discuss the practical challenges encountered in dataset construction and propose principled solutions for building culturally and linguistically-faithful resources in under-represented languages. We also provide a public evaluation suite as a clear foundation for iterative, community-driven progress in Filipino NLP.",
      "arxiv_url": "https://arxiv.org/abs/2502.14911",
      "pdf_url": "https://arxiv.org/pdf/2502.14911",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24643",
      "title": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based Pairwise Ranking with Batching and Caching",
      "authors": [
        "Juan Wisznia",
        "Cecilia Bolanos",
        "Juan Tollo",
        "Giovanni Marraffini",
        "Agust'in Gianolini",
        "Noe Hsueh",
        "Luciano Del Corro"
      ],
      "abstract": "We introduce a novel framework for analyzing sorting algorithms in pairwise ranking prompting (PRP), re-centering the cost model around LLM inferences rather than traditional pairwise comparisons. While classical metrics based on comparison counts have traditionally been used to gauge efficiency, our analysis reveals that expensive LLM inferences overturn these predictions; accordingly, our framework encourages strategies such as batching and caching to mitigate inference costs. We show that algorithms optimal in the classical setting can lose efficiency when LLM inferences dominate the cost under certain optimizations.",
      "arxiv_url": "https://arxiv.org/abs/2505.24643",
      "pdf_url": "https://arxiv.org/pdf/2505.24643",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05388",
      "title": "taz2024full: Analysing German Newspapers for Gender Bias and Discrimination across Decades",
      "authors": [
        "Stefanie Urchs",
        "Veronika Thurner",
        "M. Aßenmacher",
        "Christian Heumann",
        "Stephanie Thiemichen"
      ],
      "abstract": "Open-access corpora are essential for advancing natural language processing (NLP) and computational social science (CSS). However, large-scale resources for German remain limited, restricting research on linguistic trends and societal issues such as gender bias. We present taz2024full, the largest publicly available corpus of German newspaper articles to date, comprising over 1.8 million texts from taz, spanning 1980 to 2024. As a demonstration of the corpus's utility for bias and discrimination research, we analyse gender representation across four decades of reporting. We find a consistent overrepresentation of men, but also a gradual shift toward more balanced coverage in recent years. Using a scalable, structured analysis pipeline, we provide a foundation for studying actor mentions, sentiment, and linguistic framing in German journalistic texts. The corpus supports a wide range of applications, from diachronic language analysis to critical media studies, and is freely available to foster inclusive and reproducible research in German-language NLP.",
      "arxiv_url": "https://arxiv.org/abs/2506.05388",
      "pdf_url": "https://arxiv.org/pdf/2506.05388",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.19360",
      "title": "ChartLens: Fine-grained Visual Attribution in Charts",
      "authors": [
        "Manan Suri",
        "Puneet Mathur",
        "Nedim Lipka",
        "Franck Dernoncourt",
        "Ryan A. Rossi",
        "Dinesh Manocha"
      ],
      "abstract": "The growing capabilities of multimodal large language models (MLLMs) have advanced tasks like chart understanding. However, these models often suffer from hallucinations, where generated text sequences conflict with the provided visual data. To address this, we introduce Post-Hoc Visual Attribution for Charts, which identifies fine-grained chart elements that validate a given chart-associated response. We propose ChartLens, a novel chart attribution algorithm that uses segmentation-based techniques to identify chart objects and employs set-of-marks prompting with MLLMs for fine-grained visual attribution. Additionally, we present ChartVA-Eval, a benchmark with synthetic and real-world charts from diverse domains like finance, policy, and economics, featuring fine-grained attribution annotations. Our evaluations show that ChartLens improves fine-grained attributions by 26-66%.",
      "arxiv_url": "https://arxiv.org/abs/2505.19360",
      "pdf_url": "https://arxiv.org/pdf/2505.19360",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.22610",
      "title": "Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users",
      "authors": [
        "Antonia Karamolegkou",
        "Malvina Nikandrou",
        "Georgios Pantazopoulos",
        "Danae Sanchez Villegas",
        "Phillip Rust",
        "Ruchira Dhar",
        "Daniel Hershcovich",
        "Anders Søgaard"
      ],
      "abstract": "This paper explores the effectiveness of Multimodal Large Language models (MLLMs) as assistive technologies for visually impaired individuals. We conduct a user survey to identify adoption patterns and key challenges users face with such technologies. Despite a high adoption rate of these models, our findings highlight concerns related to contextual understanding, cultural sensitivity, and complex scene understanding, particularly for individuals who may rely solely on them for visual interpretation. Informed by these results, we collate five user-centred tasks with image and video inputs, including a novel task on Optical Braille Recognition. Our systematic evaluation of twelve MLLMs reveals that further advancements are necessary to overcome limitations related to cultural context, multilingual support, Braille reading comprehension, assistive object recognition, and hallucinations. This work provides critical insights into the future direction of multimodal AI for accessibility, underscoring the need for more inclusive, robust, and trustworthy visual assistance technologies.",
      "arxiv_url": "https://arxiv.org/abs/2503.22610",
      "pdf_url": "https://arxiv.org/pdf/2503.22610",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05947",
      "title": "IntentionESC: An Intention-Centered Framework for Enhancing Emotional Support in Dialogue Systems",
      "authors": [
        "Xinjie Zhang",
        "Wenxuan Wang",
        "Qin Jin"
      ],
      "abstract": "In emotional support conversations, unclear intentions can lead supporters to employ inappropriate strategies, inadvertently imposing their expectations or solutions on the seeker. Clearly defined intentions are essential for guiding both the supporter's motivations and the overall emotional support process. In this paper, we propose the Intention-centered Emotional Support Conversation (IntentionESC) framework, which defines the possible intentions of supporters in emotional support conversations, identifies key emotional state aspects for inferring these intentions, and maps them to appropriate support strategies. While Large Language Models (LLMs) excel in text generating, they fundamentally operate as probabilistic models trained on extensive datasets, lacking a true understanding of human thought processes and intentions. To address this limitation, we introduce the Intention Centric Chain-of-Thought (ICECoT) mechanism. ICECoT enables LLMs to mimic human reasoning by analyzing emotional states, inferring intentions, and selecting suitable support strategies, thereby generating more effective emotional support responses. To train the model with ICECoT and integrate expert knowledge, we design an automated annotation pipeline that produces high-quality training data. Furthermore, we develop a comprehensive evaluation scheme to assess emotional support efficacy and conduct extensive experiments to validate our framework. Our data and code are available at https://github.com/43zxj/IntentionESC_ICECoT.",
      "arxiv_url": "https://arxiv.org/abs/2506.05947",
      "pdf_url": "https://arxiv.org/pdf/2506.05947",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.13237",
      "title": "ImPart: Importance-Aware Delta-Sparsification for Improved Model Compression and Merging in LLMs",
      "authors": [
        "Yan Yang",
        "Yixia Li",
        "Hongru Wang",
        "Xuetao Wei",
        "Jianqiao Yu",
        "Yun Chen",
        "Guanhua Chen"
      ],
      "abstract": "With the proliferation of task-specific large language models, delta compression has emerged as a method to mitigate the resource challenges of deploying numerous such models by effectively compressing the delta model parameters. Previous delta-sparsification methods either remove parameters randomly or truncate singular vectors directly after singular value decomposition (SVD). However, these methods either disregard parameter importance entirely or evaluate it with too coarse a granularity. In this work, we introduce ImPart, a novel importance-aware delta sparsification approach. Leveraging SVD, it dynamically adjusts sparsity ratios of different singular vectors based on their importance, effectively retaining crucial task-specific knowledge even at high sparsity ratios. Experiments show that ImPart achieves state-of-the-art delta sparsification performance, demonstrating $2\\times$ higher compression ratio than baselines at the same performance level. When integrated with existing methods, ImPart sets a new state-of-the-art on delta quantization and model merging.",
      "arxiv_url": "https://arxiv.org/abs/2504.13237",
      "pdf_url": "https://arxiv.org/pdf/2504.13237",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.21930",
      "title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "authors": [
        "Dongyue Li",
        "Ziniu Zhang",
        "Lu Wang",
        "Hongyan Zhang"
      ],
      "abstract": "This paper develops an ensemble method for fine-tuning a language model to multiple datasets. Existing methods, such as quantized LoRA (QLoRA), are efficient when adapting to a single dataset. When training on multiple datasets of different tasks, a common setup in practice, it remains unclear how to design an efficient adaptation for fine-tuning language models. We propose to use an ensemble of multiple smaller adapters instead of a single adapter per task. We design an efficient algorithm that partitions $n$ datasets into $m$ groups, where $m$ is typically much smaller than $n$ in practice, and train one adapter for each group before taking a weighted combination to form the ensemble. The algorithm leverages a first-order approximation property of low-rank adaptation to quickly obtain the fine-tuning performances of dataset combinations since methods like LoRA stay close to the base model. Hence, we use the gradients of the base model to estimate its behavior during fine-tuning. Empirically, this approximation holds with less than $1\\%$ error on models with up to $34$ billion parameters, leading to an estimation of true fine-tuning performances under $5\\%$ error while speeding up computation compared to base fine-tuning by $105$ times. When applied to fine-tune Llama and GPT models on ten text classification tasks, our approach provides up to $10\\%$ higher average test accuracy over QLoRA, with only $9\\%$ more FLOPs. On a Llama model with $34$ billion parameters, an ensemble of QLoRA increases test accuracy by $3\\%$ compared to QLoRA, with only $8\\%$ more FLOPs.",
      "arxiv_url": "https://arxiv.org/abs/2505.21930",
      "pdf_url": "https://arxiv.org/pdf/2505.21930",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12442",
      "title": "HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation",
      "authors": [
        "Hao Liu",
        "Zhengren Wang",
        "Xi Chen",
        "Zhiyu Li",
        "Feiyu Xiong",
        "Qinhan Yu",
        "Wentao Zhang"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) systems often struggle with imperfect retrieval, as traditional retrievers focus on lexical or semantic similarity rather than logical relevance. To address this, we propose \\textbf{HopRAG}, a novel RAG framework that augments retrieval with logical reasoning through graph-structured knowledge exploration. During indexing, HopRAG constructs a passage graph, with text chunks as vertices and logical connections established via LLM-generated pseudo-queries as edges. During retrieval, it employs a \\textit{retrieve-reason-prune} mechanism: starting with lexically or semantically similar passages, the system explores multi-hop neighbors guided by pseudo-queries and LLM reasoning to identify truly relevant ones. Experiments on multiple multi-hop benchmarks demonstrate that HopRAG's \\textit{retrieve-reason-prune} mechanism can expand the retrieval scope based on logical connections and improve final answer quality.",
      "arxiv_url": "https://arxiv.org/abs/2502.12442",
      "pdf_url": "https://arxiv.org/pdf/2502.12442",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.01702",
      "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness",
      "authors": [
        "Zixin Chen",
        "Hongzhan Lin",
        "Kaixin Li",
        "Ziyang Luo",
        "Zhen Ye",
        "Guang Chen",
        "Zhiyong Huang",
        "Jing Ma"
      ],
      "abstract": "The proliferation of multimodal memes in the social media era demands that multimodal Large Language Models (mLLMs) effectively understand meme harmfulness. Existing benchmarks for assessing mLLMs on harmful meme understanding rely on accuracy-based, model-agnostic evaluations using static datasets. These benchmarks are limited in their ability to provide up-to-date and thorough assessments, as online memes evolve dynamically. To address this, we propose AdamMeme, a flexible, agent-based evaluation framework that adaptively probes the reasoning capabilities of mLLMs in deciphering meme harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive evaluations by iteratively updating the meme data with challenging samples, thereby exposing specific limitations in how mLLMs interpret harmfulness. Extensive experiments show that our framework systematically reveals the varying performance of different target mLLMs, offering in-depth, fine-grained analyses of model-specific weaknesses. Our code is available at https://github.com/Lbotirx/AdamMeme.",
      "arxiv_url": "https://arxiv.org/abs/2507.01702",
      "pdf_url": "https://arxiv.org/pdf/2507.01702",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f9571833d20d86717efc1d0b514c3ccadcda0b89",
      "title": "GenGO Ultra: an LLM-powered ACL Paper Explorer",
      "authors": [
        "Sotaro Takeshita",
        "Tornike Tsereteli",
        "S. Ponzetto"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/f9571833d20d86717efc1d0b514c3ccadcda0b89",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.02502",
      "title": "LADM: Long-context Training Data Selection with Attention-based Dependency Measurement for LLMs",
      "authors": [
        "Jianghao Chen",
        "Junhong Wu",
        "Yangyifan Xu",
        "Jiajun Zhang"
      ],
      "abstract": "Long-context modeling has drawn more and more attention in the area of Large Language Models (LLMs). Continual training with long-context data becomes the de-facto method to equip LLMs with the ability to process long inputs. However, it still remains an open challenge to measure the quality of long-context training data. To address this issue, we propose a Long-context data selection framework with Attention-based Dependency Measurement (LADM), which can efficiently identify high-quality long-context data from a large-scale, multi-domain pre-training corpus. LADM leverages the retrieval capabilities of the attention mechanism to capture contextual dependencies, ensuring a comprehensive quality measurement of long-context data. Experimental results show that our LADM framework significantly boosts the performance of LLMs on multiple long-context tasks with only 1B tokens for continual training.",
      "arxiv_url": "https://arxiv.org/abs/2503.02502",
      "pdf_url": "https://arxiv.org/pdf/2503.02502",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04722",
      "title": "Enough Coin Flips Can Make LLMs Act Bayesian",
      "authors": [
        "Ritwik Gupta",
        "Rodolfo Corona",
        "Jiaxin Ge",
        "Eric Wang",
        "Dan Klein",
        "Trevor Darrell",
        "David M. Chan"
      ],
      "abstract": "Large language models (LLMs) exhibit the ability to generalize given few-shot examples in their input prompt, an emergent capability known as in-context learning (ICL). We investigate whether LLMs use ICL to perform structured reasoning in ways that are consistent with a Bayesian framework or rely on pattern matching. Using a controlled setting of biased coin flips, we find that: (1) LLMs often possess biased priors, causing initial divergence in zero-shot settings, (2) in-context evidence outweighs explicit bias instructions, (3) LLMs broadly follow Bayesian posterior updates, with deviations primarily due to miscalibrated priors rather than flawed updates, and (4) attention magnitude has negligible effect on Bayesian inference. With sufficient demonstrations of biased coin flips via ICL, LLMs update their priors in a Bayesian manner.",
      "arxiv_url": "https://arxiv.org/abs/2503.04722",
      "pdf_url": "https://arxiv.org/pdf/2503.04722",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f9a29909c691679a389ed3d33e0b778b905e7227",
      "title": "ATLANTIS: Weak-to-Strong Learning via Importance Sampling",
      "authors": [
        "Yi Liu",
        "Guoyin Wang",
        "Shicheng Li",
        "Feifan Song",
        "Xu Sun"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/f9a29909c691679a389ed3d33e0b778b905e7227",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f9cbeb9ddf69d3d0e5f6748cffec3daccf1a24b0",
      "title": "Learning from Litigation: Graphs for Retrieval and Reasoning in eDiscovery",
      "authors": [
        "Sounak Lahiri",
        "Sumit Pai",
        "Tim Weninger",
        "Sanmitra Bhattacharya"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/f9cbeb9ddf69d3d0e5f6748cffec3daccf1a24b0",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12611",
      "title": "Who Writes What: Unveiling the Impact of Author Roles on AI-generated Text Detection",
      "authors": [
        "Jiatao Li",
        "Xiaojun Wan"
      ],
      "abstract": "The rise of Large Language Models (LLMs) necessitates accurate AI-generated text detection. However, current approaches largely overlook the influence of author characteristics. We investigate how sociolinguistic attributes-gender, CEFR proficiency, academic field, and language environment-impact state-of-the-art AI text detectors. Using the ICNALE corpus of human-authored texts and parallel AI-generated texts from diverse LLMs, we conduct a rigorous evaluation employing multi-factor ANOVA and weighted least squares (WLS). Our results reveal significant biases: CEFR proficiency and language environment consistently affected detector accuracy, while gender and academic field showed detector-dependent effects. These findings highlight the crucial need for socially aware AI text detection to avoid unfairly penalizing specific demographic groups. We offer novel empirical evidence, a robust statistical framework, and actionable insights for developing more equitable and reliable detection systems in real-world, out-of-domain contexts. This work paves the way for future research on bias mitigation, inclusive evaluation benchmarks, and socially responsible LLM detectors.",
      "arxiv_url": "https://arxiv.org/abs/2502.12611",
      "pdf_url": "https://arxiv.org/pdf/2502.12611",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15639",
      "title": "Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models",
      "authors": [
        "Anirudh Sundar",
        "Sinead Williamson",
        "Katherine Metcalf",
        "B. Theobald",
        "Skyler Seto",
        "Masha Fedzechkina"
      ],
      "abstract": "Aligned representations across languages is a desired property in multilingual large language models (mLLMs), as alignment can improve performance in cross-lingual tasks. Typically alignment requires fine-tuning a model, which is computationally expensive, and sizable language data, which often may not be available. A data-efficient alternative to fine-tuning is model interventions -- a method for manipulating model activations to steer generation into the desired direction. We analyze the effect of a popular intervention (finding experts) on the alignment of cross-lingual representations in mLLMs. We identify the neurons to manipulate for a given language and introspect the embedding space of mLLMs pre- and post-manipulation. We show that modifying the mLLM's activations changes its embedding space such that cross-lingual alignment is enhanced. Further, we show that the changes to the embedding space translate into improved downstream performance on retrieval tasks, with up to 2x improvements in top-1 accuracy on cross-lingual retrieval.",
      "arxiv_url": "https://arxiv.org/abs/2502.15639",
      "pdf_url": "https://arxiv.org/pdf/2502.15639",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.11631",
      "title": "SceneGram: Conceptualizing and Describing Tangrams in Scene Context",
      "authors": [
        "Simeon Junker",
        "Sina Zarrieß"
      ],
      "abstract": "Research on reference and naming suggests that humans can come up with very different ways of conceptualizing and referring to the same object, e.g. the same abstract tangram shape can be a\"crab\",\"sink\"or\"space ship\". Another common assumption in cognitive science is that scene context fundamentally shapes our visual perception of objects and conceptual expectations. This paper contributes SceneGram, a dataset of human references to tangram shapes placed in different scene contexts, allowing for systematic analyses of the effect of scene context on conceptualization. Based on this data, we analyze references to tangram shapes generated by multimodal LLMs, showing that these models do not account for the richness and variability of conceptualizations found in human references.",
      "arxiv_url": "https://arxiv.org/abs/2506.11631",
      "pdf_url": "https://arxiv.org/pdf/2506.11631",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-13",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.00771",
      "title": "Evaluating Personalized Tool-Augmented LLMs from the Perspectives of Personalization and Proactivity",
      "authors": [
        "Yupu Hao",
        "Pengfei Cao",
        "Zhuoran Jin",
        "Huanxuan Liao",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "Personalized tool utilization is essential for aligning large language models (LLMs) with user preference in interaction scenarios with various tools. However, most of the current benchmarks primarily focus on either personalization of text generation or direct tool-utilizing, without considering both. In this work, we introduce a novel benchmark ETAPP for evaluating personalized tool invocation, establishing a sandbox environment, and a comprehensive dataset of 800 testing cases covering diverse user profiles. To improve the accuracy of our evaluation, we propose a key-point-based LLM evaluation method, mitigating biases in the LLM-as-a-judge system by manually annotating key points for each test case and providing them to LLM as the reference. Additionally, we evaluate the excellent LLMs and provide an in-depth analysis. Furthermore, we investigate the impact of different tool-invoking strategies on LLMs' personalization performance and the effects of fine-tuning in our task. The effectiveness of our preference-setting and key-point-based evaluation method is also validated. Our findings offer insights into improving personalized LLM agents. Our Code is available at https://github.com/hypasd-art/ETAPP.",
      "arxiv_url": "https://arxiv.org/abs/2503.00771",
      "pdf_url": "https://arxiv.org/pdf/2503.00771",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-03-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "f9fa04a4a6edb4e5eb952dbb0df08d78154653f6",
      "title": "CxGGEC: Construction-Guided Grammatical Error Correction",
      "authors": [
        "Yayu Cao",
        "Tianxiang Wang",
        "Lvxiaowei Xu",
        "Zhenyao Wang",
        "Ming Cai"
      ],
      "abstract": "The grammatical error correction (GEC) task aims to detect and correct grammatical errors in text to enhance its accuracy and readability. Current GEC methods primarily rely on grammatical labels for syntactic information, often overlooking the inherent usage patterns of language. In this work, we explore the potential of construction grammar (CxG) to improve GEC by leveraging constructions to capture underlying language patterns and guide corrections. We first establish a comprehensive construction inventory from corpora. Next, we introduce a construction prediction model that identifies potential constructions in ungrammat-ical sentences using a noise-tolerant language model. Finally, we train a CxGGEC model on construction-masked parallel data, which performs GEC by decoding construction to-kens into their original forms and correcting erroneous tokens. Extensive experiments on English and Chinese GEC benchmarks demonstrate the effectiveness of our approach.",
      "arxiv_url": "https://www.semanticscholar.org/paper/f9fa04a4a6edb4e5eb952dbb0df08d78154653f6",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "fa1beef03145a2eba02fdfed3b47af080c4b72c6",
      "title": "Uncertainty-Aware Contrastive Decoding",
      "authors": [
        "Hakyung Lee",
        "Subeen Park",
        "Joowang Kim",
        "Sungjun Lim",
        "Kyungwoo Song"
      ],
      "abstract": "Large language models excel in a wide range of natural language processing tasks, but generating factually accurate and consistent outputs remains a challenge. To improve text reliability, Contrastive Decoding (CD) refines token selection by leveraging differences between an expert and base model, penalizing low-quality token choices. However, CD employs static weighting between models, making it sensitive to variations in model architecture and input characteristics, often resulting in suboptimal token selection and error propagation throughout generation. We propose Uncertainty-Aware Contrastive Decoding (UCD), a method that dynamically adjusts model contributions at each decoding step based on uncertainty. We introduce a cumulative energy function, where uncertainty is quantified as the negative log-sum-exp over logits, and decomposed into entropy and expected logit components. This energy serves as a dynamic confidence signal, guiding adaptive model weighting during generation. We demonstrate through extensive experiments that UCD significantly improves factual accuracy and reliability over existing decoding methods. Finally, we provide a theoretical analysis showing that our energy function serves as a well-defined uncertainty metric capturing model confidence. Our code is available at https://github.com/MLAI-Yonsei/UCD.",
      "arxiv_url": "https://www.semanticscholar.org/paper/fa1beef03145a2eba02fdfed3b47af080c4b72c6",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.15993",
      "title": "Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku",
      "authors": [
        "Anirudh Maiya",
        "Razan Alghamdi",
        "M. Pacheco",
        "Ashutosh Trivedi",
        "Fabio Somenzi"
      ],
      "abstract": "The success of Large Language Models (LLMs) in human-AI collaborative decision-making hinges on their ability to provide trustworthy, gradual, and tailored explanations. Solving complex puzzles, such as Sudoku, offers a canonical example of this collaboration, where clear and customized explanations often hold greater importance than the final solution. In this study, we evaluate the performance of five LLMs in solving and explaining \\sixsix{} Sudoku puzzles. While one LLM demonstrates limited success in solving puzzles, none can explain the solution process in a manner that reflects strategic reasoning or intuitive problem-solving. These findings underscore significant challenges that must be addressed before LLMs can become effective partners in human-AI collaborative decision-making.",
      "arxiv_url": "https://arxiv.org/abs/2505.15993",
      "pdf_url": "https://arxiv.org/pdf/2505.15993",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-21",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01675",
      "title": "Cross-Lingual Transfer of Cultural Knowledge: An Asymmetric Phenomenon",
      "authors": [
        "Chen Zhang",
        "Zhiyuan Liao",
        "Yansong Feng"
      ],
      "abstract": "Despite substantial research efforts evaluating how well large language models~(LLMs) handle global cultural diversity, the mechanisms behind their cultural knowledge acquisition, particularly in multilingual settings, remain unclear. We study this question by investigating how cultural knowledge transfers across languages during language adaptation of LLMs. We introduce an interpretable framework for studying this transfer, ensuring training data transparency and controlling transfer effects. Through a study of four non-Anglophonic cultures, we observe bidirectional cultural transfer between English and other high-resource languages, while low-resource languages primarily transfer knowledge to English with limited reverse flow. To explain this asymmetric phenomenon, we propose a frequency-based hypothesis: cultural knowledge appearing more frequently in the pretraining data transfers more easily, which is supported by empirical analysis of the training corpora.",
      "arxiv_url": "https://arxiv.org/abs/2506.01675",
      "pdf_url": "https://arxiv.org/pdf/2506.01675",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04992",
      "title": "Wanda++: Pruning Large Language Models via Regional Gradients",
      "authors": [
        "Yifan Yang",
        "Kai Zhen",
        "Bhavana Ganesh",
        "A. Galstyan",
        "Goeric Huybrechts",
        "Markus Muller",
        "Jonas M. Kubler",
        "R. Swaminathan",
        "Athanasios Mouchtaris",
        "S. Bodapati",
        "Nathan Susanj",
        "Zheng Zhang",
        "Jack FitzGerald",
        "Abhishek Kumar"
      ],
      "abstract": "Large Language Models (LLMs) pruning seeks to remove unimportant weights for inference speedup with minimal accuracy impact. However, existing methods often suffer from accuracy degradation without full-model sparsity-aware fine-tuning. This paper presents Wanda++, a novel pruning framework that outperforms the state-of-the-art methods by utilizing decoder-block-level \\textbf{regional} gradients. Specifically, Wanda++ improves the pruning score with regional gradients for the first time and proposes an efficient regional optimization method to minimize pruning-induced output discrepancies between the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up to 32\\% over Wanda in the language modeling task and generalizes effectively to downstream tasks. Moreover, despite updating weights with regional optimization, Wanda++ remains orthogonal to sparsity-aware fine-tuning, further reducing perplexity with LoRA in great extend. Our approach is lightweight, pruning a 7B LLaMA model in under 10 minutes on a single H100 GPU.",
      "arxiv_url": "https://arxiv.org/abs/2503.04992",
      "pdf_url": "https://arxiv.org/pdf/2503.04992",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.04142",
      "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis",
      "authors": [
        "Kejian Zhu",
        "Shangqing Tu",
        "Zhuoran Jin",
        "Lei Hou",
        "Juanzi Li",
        "Jun Zhao"
      ],
      "abstract": "The development of large language models (LLMs) depends on trustworthy evaluation. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical. In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through comparative and causal analysis. Building on this, we introduce an evaluation method called shortcut neuron patching to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient ($\\rho$) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. Code: https://github.com/GaryStack/Trustworthy-Evaluation",
      "arxiv_url": "https://arxiv.org/abs/2506.04142",
      "pdf_url": "https://arxiv.org/pdf/2506.04142",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.24223",
      "title": "Automated Structured Radiology Report Generation",
      "authors": [
        "Jean-Benoit Delbrouck",
        "Justin Xu",
        "Johannes Moll",
        "Alois Thomas",
        "Zhihong Chen",
        "Sophie Ostmeier",
        "Asfandyar Azhar",
        "Kelvin Zhenghao Li",
        "Andrew Johnston",
        "Christian Bluethgen",
        "E. Reis",
        "Mohamed S. Muneer",
        "Maya Varma",
        "Curtis P. Langlotz"
      ],
      "abstract": "Automated radiology report generation from chest X-ray (CXR) images has the potential to improve clinical efficiency and reduce radiologists' workload. However, most datasets, including the publicly available MIMIC-CXR and CheXpert Plus, consist entirely of free-form reports, which are inherently variable and unstructured. This variability poses challenges for both generation and evaluation: existing models struggle to produce consistent, clinically meaningful reports, and standard evaluation metrics fail to capture the nuances of radiological interpretation. To address this, we introduce Structured Radiology Report Generation (SRRG), a new task that reformulates free-text radiology reports into a standardized format, ensuring clarity, consistency, and structured clinical reporting. We create a novel dataset by restructuring reports using large language models (LLMs) following strict structured reporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained disease classification model trained on 55 labels, enabling more precise and clinically informed evaluation of structured reports. To assess report quality, we propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease taxonomy to bridge the gap between free-text variability and structured clinical reporting. We validate our dataset through a reader study conducted by five board-certified radiologists and extensive benchmarking experiments.",
      "arxiv_url": "https://arxiv.org/abs/2505.24223",
      "pdf_url": "https://arxiv.org/pdf/2505.24223",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-30",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.23349",
      "title": "Towards Reward Fairness in RLHF: From a Resource Allocation Perspective",
      "authors": [
        "Ouyang Sheng",
        "Yulan Hu",
        "Ge Chen",
        "Qingyang Li",
        "Fuzheng Zhang",
        "Yong Liu"
      ],
      "abstract": "Rewards serve as proxies for human preferences and play a crucial role in Reinforcement Learning from Human Feedback (RLHF). However, if these rewards are inherently imperfect, exhibiting various biases, they can adversely affect the alignment of large language models (LLMs). In this paper, we collectively define the various biases present in rewards as the problem of reward unfairness. We propose a bias-agnostic method to address the issue of reward fairness from a resource allocation perspective, without specifically designing for each type of bias, yet effectively mitigating them. Specifically, we model preference learning as a resource allocation problem, treating rewards as resources to be allocated while considering the trade-off between utility and fairness in their distribution. We propose two methods, Fairness Regularization and Fairness Coefficient, to achieve fairness in rewards. We apply our methods in both verification and reinforcement learning scenarios to obtain a fairness reward model and a policy model, respectively. Experiments conducted in these scenarios demonstrate that our approach aligns LLMs with human preferences in a more fair manner.",
      "arxiv_url": "https://arxiv.org/abs/2505.23349",
      "pdf_url": "https://arxiv.org/pdf/2505.23349",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-29",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "fae1f5d52bbaf4f4ee1e11532602c87e35c6ae57",
      "title": "MobiLoRA: Accelerating LoRA-based LLM Inference on Mobile Devices via Context-aware KV Cache Optimization",
      "authors": [
        "Borui Li",
        "Yitao Wang",
        "Haoran Ma",
        "Ligeng Chen",
        "Jun Xiao",
        "Shuai Wang"
      ],
      "abstract": "Deploying large language models (LLMs) with low-rank adaptation (LoRA) on mobile devices is promising due to their capability to complete diverse domain-specific tasks while ensuring privacy and accessibility. In this paper, we introduce MobiLoRA to accelerate LoRA-based LLM inference on mobile devices. MobiLoRA focuses on optimizing the key-value (KV) caches due to the limited computing and memory resources of mobile devices. The key insight of MobiLoRA lies in the utilization of two contexts for on-device LoRA serving: semantic-level contexts, such as prompts with shared prefixes, and system-level contexts, such as the application status (e.g., foreground or killed) of LLM requests. Specifically, for semantic-level contexts, Mo-biLoRA proposes similarity-aware delta encoding, which leverages token-wise similarity in KV caches across LoRA adapters for efficient storage and reuse. Furthermore, Mo-biLoRA advocates context-aware KV cache management to optimize cache eviction considering the system-level contexts. We implement MobiLoRA and compare it with state-of-the-art LLM serving frameworks using real-world mobile device traces. Results show that Mo-biLoRA accelerates LoRA-based LLM inference by 18.1%~80.5% on mobile devices.",
      "arxiv_url": "https://www.semanticscholar.org/paper/fae1f5d52bbaf4f4ee1e11532602c87e35c6ae57",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.03763",
      "title": "ClozeMath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations",
      "authors": [
        "Quang Hieu Pham",
        "Thuy Duong Nguyen",
        "Tung Pham",
        "A. Luu",
        "Dat Quoc Nguyen"
      ],
      "abstract": "The capabilities of large language models (LLMs) have been enhanced by training on data that reflects human thought processes, such as the Chain-of-Thought format. However, evidence suggests that the conventional scheme of next-word prediction may not fully capture how humans learn to think. Inspired by how humans generalize mathematical reasoning, we propose a new approach named ClozeMath to fine-tune LLMs for mathematical reasoning. Our ClozeMath involves a text-infilling task that predicts masked equations from a given solution, analogous to cloze exercises used in human learning. Experiments on GSM8K, MATH, and GSM-Symbolic show that ClozeMath surpasses the strong baseline Masked Thought in performance and robustness, with two test-time scaling decoding algorithms, Beam Search and Chain-of-Thought decoding. Additionally, we conduct an ablation study to analyze the effects of various architectural and implementation choices on our approach.",
      "arxiv_url": "https://arxiv.org/abs/2506.03763",
      "pdf_url": "https://arxiv.org/pdf/2506.03763",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-04",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.03038",
      "title": "Cautious Next Token Prediction",
      "authors": [
        "Yizhou Wang",
        "Lingzhi Zhang",
        "Yue Bai",
        "M. Chiu",
        "Zhengmian Hu",
        "Mingyuan Zhang",
        "Qihua Dong",
        "Yu Yin",
        "Sohrab Amirghodsi",
        "Yun Fu"
      ],
      "abstract": "Next token prediction paradigm has been prevailing for autoregressive models in the era of LLMs. The current default sampling choice for popular LLMs is temperature scaling together with nucleus sampling to balance diversity and coherence. Nevertheless, such approach leads to inferior performance in various NLP tasks when the model is not certain about testing questions. To this end, we propose a brand new training-free decoding strategy, dubbed as Cautious Next Token Prediction (CNTP). In the decoding process, if the model has comparatively high prediction entropy at a certain step, we sample multiple trials starting from the step independently and stop when encountering any punctuation. Then we select the trial with the lowest perplexity score viewed as the most probable and reliable trial path given the model's capacity. The trial number is negatively correlated with the prediction confidence, i.e., the less confident the model is, the more trials it should sample. This is consistent with human beings'behaviour: when feeling uncertain or unconfident, one tends to think more creatively, exploring multiple thinking paths, to cautiously select the path one feels most confident about. Extensive experiments on both LLMs and MLLMs show that our proposed CNTP approach outperforms existing standard decoding strategies consistently by a clear margin. Moreover, the integration of CNTP with self consistency can further improve over vanilla self consistency. We believe our proposed CNTP has the potential to become one of the default choices for LLM decoding. Code is available at https://github.com/wyzjack/CNTP.",
      "arxiv_url": "https://arxiv.org/abs/2507.03038",
      "pdf_url": "https://arxiv.org/pdf/2507.03038",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.01461",
      "title": "Marco-o1 v2: Towards Widening The Distillation Bottleneck for Reasoning Models",
      "authors": [
        "Huifeng Yin",
        "Yu Zhao",
        "Minghao Wu",
        "Xuanfan Ni",
        "Bo Zeng",
        "Hao Wang",
        "Tianqi Shi",
        "Liangying Shao",
        "Chenyang Lyu",
        "Longyue Wang",
        "Weihua Luo",
        "Kaifu Zhang"
      ],
      "abstract": "Large Reasoning Models(LRMs) such as OpenAI o1 and DeepSeek-R1 have shown remarkable reasoning capabilities by scaling test-time compute and generating long Chain-of-Thought(CoT). Distillation--post-training on LRMs-generated data--is a straightforward yet effective method to enhance the reasoning abilities of smaller models, but faces a critical bottleneck: we found that distilled long CoT data poses learning difficulty for small models and leads to the inheritance of biases (i.e. over-thinking) when using Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) methods. To alleviate this bottleneck, we propose constructing tree-based CoT data from scratch via Monte Carlo Tree Search(MCTS). We then exploit a set of CoT-aware approaches, including Thoughts Length Balance, Fine-grained DPO, and Joint Post-training Objective, to enhance SFT and RL on the constructed data. We conduct evaluation on various benchmarks such as math (GSM8K, MATH, AIME). instruction-following (Multi-IF) and planning (Blocksworld), results demonstrate our approaches substantially improve the reasoning performance of distilled models compared to standard distilled models via reducing the hallucinations in long-time thinking. The project homepage is https://github.com/AIDC-AI/Marco-o1.",
      "arxiv_url": "https://arxiv.org/abs/2503.01461",
      "pdf_url": "https://arxiv.org/pdf/2503.01461",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-03-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.05746",
      "title": "LLM-Symbolic Integration for Robust Temporal Tabular Reasoning",
      "authors": [
        "Atharv Kulkarni",
        "Kushagra Dixit",
        "Vivek Srikumar",
        "Dan Roth",
        "Vivek Gupta"
      ],
      "abstract": "Temporal tabular question answering presents a significant challenge for Large Language Models (LLMs), requiring robust reasoning over structured data, which is a task where traditional prompting methods often fall short. These methods face challenges such as memorization, sensitivity to table size, and reduced performance on complex queries. To overcome these limitations, we introduce TempTabQA-C, a synthetic dataset designed for systematic and controlled evaluations, alongside a symbolic intermediate representation that transforms tables into database schemas. This structured approach allows LLMs to generate and execute SQL queries, enhancing generalization and mitigating biases. By incorporating adaptive few-shot prompting with contextually tailored examples, our method achieves superior robustness, scalability, and performance. Experimental results consistently highlight improvements across key challenges, setting a new benchmark for robust temporal reasoning with LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2506.05746",
      "pdf_url": "https://arxiv.org/pdf/2506.05746",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "fb5e23ec9168edfe95f2c52ad229b65804a777e1",
      "title": "Generating Questions, Answers, and Distractors for Videos: Exploring Semantic Uncertainty of Object Motions",
      "authors": [
        "Wenjian Ding",
        "Yao Zhang",
        "Jun Wang",
        "Adam Jatowt",
        "Zhenglu Yang"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/fb5e23ec9168edfe95f2c52ad229b65804a777e1",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.17144",
      "title": "MDIT-Bench: Evaluating the Dual-Implicit Toxicity in Large Multimodal Models",
      "authors": [
        "Bohan Jin",
        "Shuhan Qi",
        "Kehai Chen",
        "Xinyi Guo",
        "Xuan Wang"
      ],
      "abstract": "The widespread use of Large Multimodal Models (LMMs) has raised concerns about model toxicity. However, current research mainly focuses on explicit toxicity, with less attention to some more implicit toxicity regarding prejudice and discrimination. To address this limitation, we introduce a subtler type of toxicity named dual-implicit toxicity and a novel toxicity benchmark termed MDIT-Bench: Multimodal Dual-Implicit Toxicity Benchmark. Specifically, we first create the MDIT-Dataset with dual-implicit toxicity using the proposed Multi-stage Human-in-loop In-context Generation method. Based on this dataset, we construct the MDIT-Bench, a benchmark for evaluating the sensitivity of models to dual-implicit toxicity, with 317,638 questions covering 12 categories, 23 subcategories, and 780 topics. MDIT-Bench includes three difficulty levels, and we propose a metric to measure the toxicity gap exhibited by the model across them. In the experiment, we conducted MDIT-Bench on 13 prominent LMMs, and the results show that these LMMs cannot handle dual-implicit toxicity effectively. The model's performance drops significantly in hard level, revealing that these LMMs still contain a significant amount of hidden but activatable toxicity. Data are available at https://github.com/nuo1nuo/MDIT-Bench.",
      "arxiv_url": "https://arxiv.org/abs/2505.17144",
      "pdf_url": "https://arxiv.org/pdf/2505.17144",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.04724",
      "title": "LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM",
      "authors": [
        "Sambal Shikhar",
        "Mohammed Irfan Kurpath",
        "Sahal Shaji Mullappilly",
        "Jean Lahoud",
        "F. Khan",
        "R. Anwer",
        "Salman H. Khan",
        "Hisham Cholakkal"
      ],
      "abstract": "Recent advancements in speech-to-speech dialogue systems leverage LLMs for multimodal interactions, yet they remain hindered by fine-tuning requirements, high computational overhead, and text-speech misalignment. Existing speech-enabled LLMs often degrade conversational quality by modifying the LLM, thereby compromising its linguistic capabilities. In contrast, we propose LLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS system that generates high-quality speech with low latency, while fully preserving the capabilities of the base LLM. Our approach achieves a significantly lower Word Error Rate compared to speech-enabled LLMs, while operating at comparable latency and UTMOS score. By decoupling speech synthesis from LLM processing via a multi-queue token streaming system, LLMVoX supports seamless, infinite-length dialogues. Its plug-and-play design also facilitates extension to various tasks with different backbones. Furthermore, LLMVoX generalizes to new languages with only dataset adaptation, attaining a low Character Error Rate on an Arabic speech task. Additionally, we have integrated LLMVoX with a Vision-Language Model to create an omni-model with speech, text, and vision capabilities, without requiring additional multimodal training. Our code base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .",
      "arxiv_url": "https://arxiv.org/abs/2503.04724",
      "pdf_url": "https://arxiv.org/pdf/2503.04724",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.10923",
      "title": "Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization",
      "authors": [
        "Yuhao Wang",
        "Keyan Ding",
        "Kehua Feng",
        "Zeyuan Wang",
        "Ming Qin",
        "Xiaotong Li",
        "Qiang Zhang",
        "Huajun Chen"
      ],
      "abstract": "Protein language models have emerged as powerful tools for sequence generation, offering substantial advantages in functional optimization and denovo design. However, these models also present significant risks of generating harmful protein sequences, such as those that enhance viral transmissibility or evade immune responses. These concerns underscore critical biosafety and ethical challenges. To address these issues, we propose a Knowledge-guided Preference Optimization (KPO) framework that integrates prior knowledge via a Protein Safety Knowledge Graph. This framework utilizes an efficient graph pruning strategy to identify preferred sequences and employs reinforcement learning to minimize the risk of generating harmful proteins. Experimental results demonstrate that KPO effectively reduces the likelihood of producing hazardous sequences while maintaining high functionality, offering a robust safety assurance framework for applying generative models in biotechnology.",
      "arxiv_url": "https://arxiv.org/abs/2507.10923",
      "pdf_url": "https://arxiv.org/pdf/2507.10923",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-07-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "fbc91a0a0826e673625c29c02c87ba186719e952",
      "title": "From Recall to Creation: Generating Follow-Up Questions Using Bloom's Taxonomy and Grice's Maxims",
      "authors": [
        "Archana Yadav",
        "Harshvivek Kashid",
        "†. PushpakBhattacharyya",
        "Medchalimi Sruthi",
        "Chintalapalli Raja",
        "Mandala Kullayappa ⋄",
        "Jagadeesh Reddy",
        "Mihail Eric",
        "Christopher D Manning. 2017",
        "Yubin Ge",
        "Ziang Xiao",
        "Jana Diesner",
        "Heng Ji",
        "Karrie Karahalios",
        "Hari Sundaram. 2023",
        "Bilal Ghanem",
        "Lauren Lutz",
        "Julia Rivard Dexter",
        "Spencer McIntosh von der Ohe",
        "Liangtai Sun",
        "Yang Han",
        "Zihan Zhao",
        "Da Ma",
        "Zhennan Shen",
        "Baocai Chen",
        "Lu Chen",
        "Lianmin Zheng",
        "Wei-Lin Chiang",
        "Ying Sheng",
        "Siyuan Zhuang",
        "Zhanghao Wu",
        "Yonghao Zhuang",
        "Zi Lin",
        "Zhuohan Li",
        "Dacheng Li",
        "Eric P. Xing",
        "Hao Zhang",
        "Joseph E. Gonzalez"
      ],
      "abstract": "In-car AI assistants enhance driving by enabling hands-free interactions, yet they often struggle with multi-turn conversations and fail to handle cognitively complex follow-up questions. This limits their effectiveness in real-world deployment. To address this limitation, we propose a framework that leverages Bloom’s Taxonomy to systematically generate follow-up questions with increasing cognitive complexity and a Gricean-inspired evaluation framework to assess their Logical Consistency, Informativeness, Relevance, and Clarity. We introduce a dataset comprising 750 human-annotated seed questions and 3750 follow-up questions, with human evaluation confirming that 96.68% of the generated questions adhere to the intended Bloom’s Taxonomy levels. Our approach, validated through both LLM-based and human assessments, also identifies the specific cognitive complexity level at which in-car AI assistants begin to falter information that can help developers measure and optimize key cognitive aspects of conversational performance.",
      "arxiv_url": "https://www.semanticscholar.org/paper/fbc91a0a0826e673625c29c02c87ba186719e952",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.13927",
      "title": "CRPO: Confidence-Reward Driven Preference Optimization for Machine Translation",
      "authors": [
        "Guofeng Cui",
        "Pichao Wang",
        "Yang Liu",
        "Zemian Ke",
        "Zhu Liu",
        "Vimal Bhat"
      ],
      "abstract": "Large language models (LLMs) have shown great potential in natural language processing tasks, but their application to machine translation (MT) remains challenging due to pretraining on English-centric data and the complexity of reinforcement learning from human feedback (RLHF). Direct Preference Optimization (DPO) has emerged as a simpler and more efficient alternative, but its performance depends heavily on the quality of preference data. To address this, we propose Confidence-Reward driven Preference Optimization (CRPO), a novel method that combines reward scores with model confidence to improve data selection for fine-tuning. CRPO selects challenging sentence pairs where the model is uncertain or underperforms, leading to more effective learning. While primarily designed for LLMs, CRPO also generalizes to encoder-decoder models like NLLB, demonstrating its versatility. Empirical results show that CRPO outperforms existing methods such as RS-DPO, RSO and MBR score in both translation accuracy and data efficiency.",
      "arxiv_url": "https://arxiv.org/abs/2501.13927",
      "pdf_url": "https://arxiv.org/pdf/2501.13927",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "fbf31b44d56031fa904d564c1f5429639857e7d2",
      "title": "A Triple-View Framework for Fine-Grained Emotion Classification with Clustering-Guided Contrastive Learning",
      "authors": [
        "Junqing Gong",
        "Binhan Yang",
        "Wei Shen"
      ],
      "abstract": "Fine-grained emotion classification (FEC) aims to analyze speakers’ utterances and distinguish dozens of emotions with subtle differences, allowing for a more nuanced understanding of human emotional states. However, compared to traditional coarse-grained emotion classification, two difficulties arise as the granularity of emotions becomes finer, i.e., the presence of closely confusable emotions which are hard to distinguish, and the biased performance caused by long-tailed emotions. Although addressing both difficulties is vital to FEC, previous studies have predominantly focused on dealing with only one of them. In this paper, we pro-pose TACO, a novel triple-view framework that treats FEC as an instance-label (i.e., utterance-emotion) joint embedding learning problem to tackle both difficulties concurrently by considering three complementary views. Specifically, we design a clustering-guided contrastive loss, which incorporates clustering techniques to guide the contrastive learning process and facilitate more discriminative instance embed-dings. Additionally, we introduce the emotion label description as a helpful resource to refine label embeddings and mitigate the poor performance towards under-represented (i.e., long-tailed) emotions. Extensive experiments on two widely-used benchmark datasets demonstrate that our proposed TACO achieves substantial and consistent improvements compared to other competitive baseline methods.",
      "arxiv_url": "https://www.semanticscholar.org/paper/fbf31b44d56031fa904d564c1f5429639857e7d2",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13647",
      "title": "Instruction Tuning on Public Government and Cultural Data for Low-Resource Language: a Case Study in Kazakh",
      "authors": [
        "Nurkhan Laiyk",
        "Daniil Orel",
        "Rituraj Joshi",
        "Maiya Goloburda",
        "Yuxia Wang",
        "Preslav Nakov",
        "Fajri Koto"
      ],
      "abstract": "Instruction tuning in low-resource languages remains underexplored due to limited text data, particularly in government and cultural domains. To address this, we introduce and open-source a large-scale (10,600 samples) instruction-following (IFT) dataset, covering key institutional and cultural knowledge relevant to Kazakhstan. Our dataset enhances LLMs'understanding of procedural, legal, and structural governance topics. We employ LLM-assisted data generation, comparing open-weight and closed-weight models for dataset construction, and select GPT-4o as the backbone. Each entity of our dataset undergoes full manual verification to ensure high quality. We also show that fine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent performance improvements in both multiple-choice and generative tasks, demonstrating the potential of LLM-assisted instruction tuning for low-resource languages.",
      "arxiv_url": "https://arxiv.org/abs/2502.13647",
      "pdf_url": "https://arxiv.org/pdf/2502.13647",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.05949",
      "title": "NeoQA: Evidence-based Question Answering with Generated News Events",
      "authors": [
        "Max Glockner",
        "Xiang Jiang",
        "Leonardo F. R. Ribeiro",
        "Iryna Gurevych",
        "Markus Dreyer"
      ],
      "abstract": "Evaluating Retrieval-Augmented Generation (RAG) in large language models (LLMs) is challenging because benchmarks can quickly become stale. Questions initially requiring retrieval may become answerable from pretraining knowledge as newer models incorporate more recent information during pretraining, making it difficult to distinguish evidence-based reasoning from recall. We introduce NeoQA (News Events for Out-of-training Question Answering), a benchmark designed to address this issue. To construct NeoQA, we generated timelines and knowledge bases of fictional news events and entities along with news articles and Q\\&A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring that no prior evidence exists in their training data. We propose our dataset as a new platform for evaluating evidence-based question answering, as it requires LLMs to generate responses exclusively from retrieved evidence and only when sufficient evidence is available. NeoQA enables controlled evaluation across various evidence scenarios, including cases with missing or misleading details. Our findings indicate that LLMs struggle to distinguish subtle mismatches between questions and evidence, and suffer from short-cut reasoning when key information required to answer a question is missing from the evidence, underscoring key limitations in evidence-based reasoning.",
      "arxiv_url": "https://arxiv.org/abs/2505.05949",
      "pdf_url": "https://arxiv.org/pdf/2505.05949",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-05-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.08026",
      "title": "In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents",
      "authors": [
        "Zhen Tan",
        "Jun Yan",
        "I-Hung Hsu",
        "Rujun Han",
        "Zifeng Wang",
        "Long T. Le",
        "Yiwen Song",
        "Yanfei Chen",
        "Hamid Palangi",
        "George Lee",
        "Anand Iyer",
        "Tianlong Chen",
        "Huan Liu",
        "Chen-Yu Lee",
        "Tomas Pfister"
      ],
      "abstract": "Large Language Models (LLMs) have made significant progress in open-ended dialogue, yet their inability to retain and retrieve relevant information from long-term interactions limits their effectiveness in applications requiring sustained personalization. External memory mechanisms have been proposed to address this limitation, enabling LLMs to maintain conversational continuity. However, existing approaches struggle with two key challenges. First, rigid memory granularity fails to capture the natural semantic structure of conversations, leading to fragmented and incomplete representations. Second, fixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user interaction patterns. In this work, we propose Reflective Memory Management (RMM), a novel mechanism for long-term dialogue agents, integrating forward- and backward-looking reflections: (1) Prospective Reflection, which dynamically summarizes interactions across granularities-utterances, turns, and sessions-into a personalized memory bank for effective future retrieval, and (2) Retrospective Reflection, which iteratively refines the retrieval in an online reinforcement learning (RL) manner based on LLMs'cited evidence. Experiments show that RMM demonstrates consistent improvement across various metrics and benchmarks. For example, RMM shows more than 10% accuracy improvement over the baseline without memory management on the LongMemEval dataset.",
      "arxiv_url": "https://arxiv.org/abs/2503.08026",
      "pdf_url": "https://arxiv.org/pdf/2503.08026",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-03-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.15081",
      "title": "Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification",
      "authors": [
        "Yaxin Fan",
        "Peifeng Li",
        "Qiaoming Zhu"
      ],
      "abstract": "Dialogue discourse parsing aims to identify and analyze discourse relations between the utterances within dialogues. However, linguistic features in dialogues, such as omission and idiom, frequently introduce ambiguities that obscure the intended discourse relations, posing significant challenges for parsers. To address this issue, we propose a Discourse-aware Clarification Module (DCM) to enhance the performance of the dialogue discourse parser. DCM employs two distinct reasoning processes: clarification type reasoning and discourse goal reasoning. The former analyzes linguistic features, while the latter distinguishes the intended relation from the ambiguous one. Furthermore, we introduce Contribution-aware Preference Optimization (CPO) to mitigate the risk of erroneous clarifications, thereby reducing cascading errors. CPO enables the parser to assess the contributions of the clarifications from DCM and provide feedback to optimize the DCM, enhancing its adaptability and alignment with the parser's requirements. Extensive experiments on the STAC and Molweni datasets demonstrate that our approach effectively resolves ambiguities and significantly outperforms the state-of-the-art (SOTA) baselines.",
      "arxiv_url": "https://arxiv.org/abs/2506.15081",
      "pdf_url": "https://arxiv.org/pdf/2506.15081",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.13726",
      "title": "RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented Generation",
      "authors": [
        "Shi-Qi Yan",
        "Zhen-Hua Ling"
      ],
      "abstract": "While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing external knowledge, its generation process heavily depends on the quality and accuracy of the retrieved context. Large language models (LLMs) struggle to evaluate the correctness of non-parametric knowledge retrieved externally when it differs from internal memorization, leading to knowledge conflicts during response generation. To this end, we introduce the Retrieval Preference Optimization (RPO), a lightweight and effective alignment method to adaptively leverage multi-source knowledge based on retrieval relevance. An implicit representation of retrieval relevance is derived and incorporated into the reward model to integrate retrieval evaluation and response generation into a single model, solving the problem that previous methods necessitate the additional procedure to assess the retrieval quality. Notably, RPO is the only RAG-dedicated alignment approach that quantifies the awareness of retrieval relevance in training, overcoming mathematical obstacles. Experiments on four datasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any extra component, exhibiting its robust generalization.",
      "arxiv_url": "https://arxiv.org/abs/2501.13726",
      "pdf_url": "https://arxiv.org/pdf/2501.13726",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-01-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "fc92da62bb6d9343f76a5495372b0466bd25df5e",
      "title": "TeRDy: Temporal Relation Dynamics through Frequency Decomposition for Temporal Knowledge Graph Completion",
      "authors": [
        "Ziyang Liu",
        "Chaokun Wang"
      ],
      "abstract": "Temporal knowledge graph completion aims to predict missing facts in a knowledge graph by leveraging temporal information. Existing methods often struggle to capture both the long-term changes and short-term variability of relations, which are crucial for accurate prediction. In this paper, we propose a novel method called TeRDy for temporal knowledge graph completion. TeRDy captures temporal relational dynamics by utilizing time-invariant embeddings, along with long-term temporally dynamic embeddings (e.g., enduring political alliances) and short-term temporally dynamic embeddings (e.g., transient political events). These two types of embeddings are derived from low-and high-frequency components via frequency decomposition. Also, we design temporal smoothing and temporal gradient to seamlessly incorporate timestamp embeddings into relation embeddings. Extensive experiments on benchmark datasets demonstrate that TeRDy outperforms state-of-the-art temporal knowledge graph embedding methods.",
      "arxiv_url": "https://www.semanticscholar.org/paper/fc92da62bb6d9343f76a5495372b0466bd25df5e",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.07135",
      "title": "SINCon: Mitigate LLM-Generated Malicious Message Injection Attack for Rumor Detection",
      "authors": [
        "Mingqing Zhang",
        "Q. Liu",
        "Xiang Tao",
        "Shu Wu",
        "Liang Wang"
      ],
      "abstract": "In the era of rapidly evolving large language models (LLMs), state-of-the-art rumor detection systems, particularly those based on Message Propagation Trees (MPTs), which represent a conversation tree with the post as its root and the replies as its descendants, are facing increasing threats from adversarial attacks that leverage LLMs to generate and inject malicious messages. Existing methods are based on the assumption that different nodes exhibit varying degrees of influence on predictions. They define nodes with high predictive influence as important nodes and target them for attacks. If the model treats nodes' predictive influence more uniformly, attackers will find it harder to target high predictive influence nodes. In this paper, we propose Similarizing the predictive Influence of Nodes with Contrastive Learning (SINCon), a defense mechanism that encourages the model to learn graph representations where nodes with varying importance have a more uniform influence on predictions. Extensive experiments on the Twitter and Weibo datasets demonstrate that SINCon not only preserves high classification accuracy on clean data but also significantly enhances resistance against LLM-driven message injection attacks.",
      "arxiv_url": "https://arxiv.org/abs/2504.07135",
      "pdf_url": "https://arxiv.org/pdf/2504.07135",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "fce46598ba992d5f78a986d4aa08aa923e41c8ef",
      "title": "Automated main concept generation for narrative discourse assessment in aphasia",
      "authors": [
        "Ankita Gupta",
        "Marisa Hudspeth",
        "Polly Stokes",
        "J. Kurland",
        "Brendan T. O'Connor"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/fce46598ba992d5f78a986d4aa08aa923e41c8ef",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12420",
      "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
      "authors": [
        "Shuqi Liu",
        "Han Wu",
        "Bowei He",
        "Xiongwei Han",
        "Mingxuan Yuan",
        "Linqi Song"
      ],
      "abstract": "Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks. We present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients. Extensive experiments on Mistral 7B and LLaMA2-7B/13B models demonstrate that Sens-Merging significantly improves performance across general knowledge, mathematical reasoning, and code generation tasks. Notably, when combined with existing merging techniques, our method enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. Our findings reveal important trade-offs between task-specific and cross-task scalings, providing insights for future model merging strategies.",
      "arxiv_url": "https://arxiv.org/abs/2502.12420",
      "pdf_url": "https://arxiv.org/pdf/2502.12420",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "fced39c567cd65fc2f23db23472a43ed22dfcf5f",
      "title": "DaNet: Dual-Aware Enhanced Alignment Network for Multimodal Aspect-Based Sentiment Analysis",
      "authors": [
        "Aoqiang Zhu",
        "Min Hu",
        "Xiaohua Wang",
        "Jiaoyun Yang",
        "Yiming Tang",
        "Ning An"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/fced39c567cd65fc2f23db23472a43ed22dfcf5f",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14132",
      "title": "Can Community Notes Replace Professional Fact-Checkers?",
      "authors": [
        "Nadav Borenstein",
        "Greta Warren",
        "Desmond Elliott",
        "Isabelle Augenstein"
      ],
      "abstract": "Two commonly employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite fact-checking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to reference fact-checking sources compared to other sources. Our results show that successful community moderation relies on professional fact-checking and highlight how citizen and professional fact-checking are deeply intertwined.",
      "arxiv_url": "https://arxiv.org/abs/2502.14132",
      "pdf_url": "https://arxiv.org/pdf/2502.14132",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.17888",
      "title": "RankCoT: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts",
      "authors": [
        "Mingyan Wu",
        "Zhenghao Liu",
        "Yukun Yan",
        "Xinze Li",
        "Shi Yu",
        "Zheni Zeng",
        "Yu Gu",
        "Ge Yu"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) enhances the performance of Large Language Models (LLMs) by incorporating external knowledge. However, LLMs still encounter challenges in effectively utilizing the knowledge from retrieved documents, often being misled by irrelevant or noisy information. To address this issue, we introduce RankCoT, a knowledge refinement method that incorporates reranking signals in generating CoT-based summarization for knowledge refinement based on given query and all retrieval documents. During training, RankCoT prompts the LLM to generate Chain-of-Thought (CoT) candidates based on the query and individual documents. It then fine-tunes the LLM to directly reproduce the best CoT from these candidate outputs based on all retrieved documents, which requires LLM to filter out irrelevant documents during generating CoT-style summarization. Additionally, RankCoT incorporates a self-reflection mechanism that further refines the CoT outputs, resulting in higher-quality training data. Our experiments demonstrate the effectiveness of RankCoT, showing its superior performance over other knowledge refinement models. Further analysis reveals that RankCoT can provide shorter but effective refinement results, enabling the generator to produce more accurate answers. All code and data are available at https://github.com/NEUIR/RankCoT.",
      "arxiv_url": "https://arxiv.org/abs/2502.17888",
      "pdf_url": "https://arxiv.org/pdf/2502.17888",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2025-02-25",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2504.17674",
      "title": "Energy Considerations of Large Language Model Inference and Efficiency Optimizations",
      "authors": [
        "Jared Fernandez",
        "Clara Na",
        "Vashisth Tiwari",
        "Yonatan Bisk",
        "Sasha Luccioni",
        "Emma Strubell"
      ],
      "abstract": "As large language models (LLMs) scale in size and adoption, their computational and environmental costs continue to rise. Prior benchmarking efforts have primarily focused on latency reduction in idealized settings, often overlooking the diverse real-world inference workloads that shape energy use. In this work, we systematically analyze the energy implications of common inference efficiency optimizations across diverse Natural Language Processing (NLP) and generative Artificial Intelligence (AI) workloads, including conversational AI and code generation. We introduce a modeling approach that approximates real-world LLM workflows through a binning strategy for input-output token distributions and batch size variations. Our empirical analysis spans software frameworks, decoding strategies, GPU architectures, online and offline serving settings, and model parallelism configurations. We show that the effectiveness of inference optimizations is highly sensitive to workload geometry, software stack, and hardware accelerators, demonstrating that naive energy estimates based on FLOPs or theoretical GPU utilization significantly underestimate real-world energy consumption. Our findings reveal that the proper application of relevant inference efficiency optimizations can reduce total energy use by up to 73% from unoptimized baselines. These insights provide a foundation for sustainable LLM deployment and inform energy-efficient design strategies for future AI infrastructure.",
      "arxiv_url": "https://arxiv.org/abs/2504.17674",
      "pdf_url": "https://arxiv.org/pdf/2504.17674",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-04-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.12446",
      "title": "Multi-Attribute Steering of Language Models via Targeted Intervention",
      "authors": [
        "Duy Nguyen",
        "Archiki Prasad",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Inference-time intervention (ITI) has emerged as a promising method for steering large language model (LLM) behavior in a particular direction (e.g., improving helpfulness) by intervening on token representations without costly updates to the LLM's parameters. However, existing ITI approaches fail to scale to multi-attribute settings with conflicts, such as enhancing helpfulness while also reducing toxicity. To address this, we introduce Multi-Attribute Targeted Steering (MAT-Steer), a novel steering framework designed for selective token-level intervention across multiple attributes. MAT-Steer learns steering vectors using an alignment objective that shifts the model's internal representations of undesirable outputs closer to those of desirable ones while enforcing sparsity and orthogonality among vectors for different attributes, thereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two distinct settings: (i) on question answering (QA) tasks where we balance attributes like truthfulness, bias, and toxicity; (ii) on generative tasks where we simultaneously improve attributes like helpfulness, correctness, and coherence. MAT-Steer outperforms existing ITI and parameter-efficient fine-tuning approaches across both task types (e.g., 3% average accuracy gain across QA tasks and 55.82% win rate against the best ITI baseline).",
      "arxiv_url": "https://arxiv.org/abs/2502.12446",
      "pdf_url": "https://arxiv.org/pdf/2502.12446",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.11368",
      "title": "LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing",
      "authors": [
        "Zhengxiang Wang",
        "Veronika Makarova",
        "Zhi Li",
        "Jordan Kodner",
        "Owen Rambow"
      ],
      "abstract": "The paper explores the performance of LLMs in the context of multi-dimensional analytic writing assessments, i.e. their ability to provide both scores and comments based on multiple assessment criteria. Using a corpus of literature reviews written by L2 graduate students and assessed by human experts against 9 analytic criteria, we prompt several popular LLMs to perform the same task under various conditions. To evaluate the quality of feedback comments, we apply a novel feedback comment quality evaluation framework. This framework is interpretable, cost-efficient, scalable, and reproducible, compared to existing methods that rely on manual judgments. We find that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. We release our corpus and code for reproducibility.",
      "arxiv_url": "https://arxiv.org/abs/2502.11368",
      "pdf_url": "https://arxiv.org/pdf/2502.11368",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.19352",
      "title": "Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation",
      "authors": [
        "Jisu Shin",
        "Juhyun Oh",
        "Eunsu Kim",
        "Hoyun Song",
        "Alice Oh"
      ],
      "abstract": "Ensuring persona fidelity in large language models (LLMs) is essential for maintaining coherent and engaging human-AI interactions. However, LLMs often exhibit Out-of-Character (OOC) behavior, where generated responses deviate from an assigned persona, leading to inconsistencies that affect model reliability. Existing evaluation methods typically assign single scores to entire responses, struggling to capture subtle persona misalignment, particularly in long-form text generation. To address this limitation, we propose an atomic-level evaluation framework that quantifies persona fidelity at a finer granularity. Our three key metrics measure the degree of persona alignment and consistency within and across generations. Our approach enables a more precise and realistic assessment of persona fidelity by identifying subtle deviations that real users would encounter. Through our experiments, we demonstrate that our framework effectively detects persona inconsistencies that prior methods overlook. By analyzing persona fidelity across diverse tasks and personality types, we reveal how task structure and persona desirability influence model adaptability, highlighting challenges in maintaining consistent persona expression.",
      "arxiv_url": "https://arxiv.org/abs/2506.19352",
      "pdf_url": "https://arxiv.org/pdf/2506.19352",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-23",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "fdc59d6ac3c2a6bfbab105a7708a72d427198505",
      "title": "Lemmas Matter, But Not Like That: Predictors of Lemma-Based Generalization in Morphological Inflection",
      "authors": [
        "Sarah Payne",
        "Jordan Kodner"
      ],
      "abstract": "Recent work has shown that overlap – whether a given lemma or feature set is attested independently in train – drives model performance on morphological inflection tasks. The impact of lemma overlap, however, is debated, with accuracy drops from 0% to 30% reported between seen and unseen test lemmas. In this paper, we introduce a novel splitting algorithm designed to investigate predictors of accuracy on seen and unseen lemmas. We find only an 11% average drop from seen to unseen test lemmas but show that the number of lemmas in train has a much stronger effect on accuracy on unseen than seen lemmas. We also show that the previously reported 30% drop is inflated due to the introduction of a near-30% drop in the number of training lemmas from the original splits to the novel splits. These results help us better understand the factors affecting morphological generalization by neural models.",
      "arxiv_url": "https://www.semanticscholar.org/paper/fdc59d6ac3c2a6bfbab105a7708a72d427198505",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.07010",
      "title": "ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation",
      "authors": [
        "Kaiyuan Liu",
        "Youcheng Pan",
        "Jing Li",
        "Daojing He",
        "Yang Xiang",
        "Yexing Du",
        "Tianrun Gao"
      ],
      "abstract": "Recently, LLM agents have made rapid progress in improving their programming capabilities. However, existing benchmarks lack the ability to automatically evaluate from users' perspective, and also lack the explainability of the results of LLM agents' code generation capabilities. Thus, we introduce ProjectEval, a new benchmark for LLM agents project-level code generation's automated evaluation by simulating user interaction. ProjectEval is constructed by LLM with human reviewing. It has three different level inputs of natural languages or code skeletons. ProjectEval can evaluate the generated projects by user interaction simulation for execution, and by code similarity through existing objective indicators. Through ProjectEval, we find that systematic engineering project code, overall understanding of the project and comprehensive analysis capability are the keys for LLM agents to achieve practical projects. Our findings and benchmark provide valuable insights for developing more effective programming agents that can be deployed in future real-world production.",
      "arxiv_url": "https://arxiv.org/abs/2503.07010",
      "pdf_url": "https://arxiv.org/pdf/2503.07010",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.00042",
      "title": "Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists",
      "authors": [
        "Yue Cui",
        "Liuyi Yao",
        "Shuchang Tao",
        "Weijie Shi",
        "Yaliang Li",
        "Bolin Ding",
        "Xiaofang Zhou"
      ],
      "abstract": "Large language models (LLMs) have significantly advanced natural language processing, particularly through the integration of external tools and APIs. However, their effectiveness is frequently hampered by parameter mis-filling during tool calling. In this paper, we propose the Hierarchical Tool Error Checklist (HiTEC) framework to systematically diagnose and mitigate tool-calling errors without relying on extensive real-world interactions. HiTEC introduces a two-tiered approach: a global error checklist that identifies common, cross-tool issues, and a local error checklist that targets tool-specific and contextual failures. Building on this structure, we propose two deployments: HiTEC-In Context Learning (HiTEC-ICL) and HiTEC-Kahneman-Tversky Optimization (HiTEC-KTO). HiTEC-ICL embeds the global checklist in the initial prompts and leverages a two-round conversational interaction to dynamically refine parameter handling, while HiTEC-KTO generates high-quality negative examples to drive fine-tuning via preference-based optimization. Extensive experiments across five public datasets demonstrate that our framework significantly improves parameter-filling accuracy and tool-calling success rates compared to baseline methods.",
      "arxiv_url": "https://arxiv.org/abs/2506.00042",
      "pdf_url": "https://arxiv.org/pdf/2506.00042",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-05-28",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.06827",
      "title": "Sandcastles in the Storm: Revisiting the (Im)possibility of Strong Watermarking",
      "authors": [
        "Fabrice Harel-Canada",
        "Boran Erol",
        "Connor Choi",
        "Jason Liu",
        "Gary Jiarui Song",
        "Nanyun Peng",
        "Amit Sahai"
      ],
      "abstract": "Watermarking AI-generated text is critical for combating misuse. Yet recent theoretical work argues that any watermark can be erased via random walk attacks that perturb text while preserving quality. However, such attacks rely on two key assumptions: (1) rapid mixing (watermarks dissolve quickly under perturbations) and (2) reliable quality preservation (automated quality oracles perfectly guide edits). Through large-scale experiments and human-validated assessments, we find mixing is slow: 100% of perturbed texts retain traces of their origin after hundreds of edits, defying rapid mixing. Oracles falter, as state-of-the-art quality detectors misjudge edits (77% accuracy), compounding errors during attacks. Ultimately, attacks underperform: automated walks remove watermarks just 26% of the time -- dropping to 10% under human quality review. These findings challenge the inevitability of watermark removal. Instead, practical barriers -- slow mixing and imperfect quality control -- reveal watermarking to be far more robust than theoretical models suggest. The gap between idealized attacks and real-world feasibility underscores the need for stronger watermarking methods and more realistic attack models.",
      "arxiv_url": "https://arxiv.org/abs/2505.06827",
      "pdf_url": "https://arxiv.org/pdf/2505.06827",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-11",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.04014",
      "title": "Nunchi-Bench: Benchmarking Language Models on Cultural Reasoning with a Focus on Korean Superstition",
      "authors": [
        "Kyuhee Kim",
        "Sangah Lee"
      ],
      "abstract": "As large language models (LLMs) become key advisors in various domains, their cultural sensitivity and reasoning skills are crucial in multicultural environments. We introduce Nunchi-Bench, a benchmark designed to evaluate LLMs'cultural understanding, with a focus on Korean superstitions. The benchmark consists of 247 questions spanning 31 topics, assessing factual knowledge, culturally appropriate advice, and situational interpretation. We evaluate multilingual LLMs in both Korean and English to analyze their ability to reason about Korean cultural contexts and how language variations affect performance. To systematically assess cultural reasoning, we propose a novel evaluation strategy with customized scoring metrics that capture the extent to which models recognize cultural nuances and respond appropriately. Our findings highlight significant challenges in LLMs'cultural reasoning. While models generally recognize factual information, they struggle to apply it in practical scenarios. Furthermore, explicit cultural framing enhances performance more effectively than relying solely on the language of the prompt. To support further research, we publicly release Nunchi-Bench alongside a leaderboard.",
      "arxiv_url": "https://arxiv.org/abs/2507.04014",
      "pdf_url": "https://arxiv.org/pdf/2507.04014",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "fe780288d4a790fc3c4bccd8396723b73324345a",
      "title": "Code-SPA: Style Preference Alignment to Large Language Models for Effective and Robust Code Debugging",
      "authors": [
        "Tengfei Wen",
        "Xuanang Chen",
        "Ben He",
        "Le Sun"
      ],
      "abstract": ",",
      "arxiv_url": "https://www.semanticscholar.org/paper/fe780288d4a790fc3c4bccd8396723b73324345a",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.10719",
      "title": "Tracr-Injection: Distilling Algorithms into Pre-trained Language Models",
      "authors": [
        "Tomás Vergara Browne",
        "Álvaro Soto"
      ],
      "abstract": "Motivated by the surge of large language models, there has been a push to formally characterize the symbolic abilities intrinsic to the transformer architecture. A programming language, called RASP, has been proposed, which can be directly compiled into transformer weights to implement these algorithms. However, the tasks that can be implemented in RASP are often uncommon to learn from natural unsupervised data, showing a mismatch between theoretical capabilities of the transformer architecture, and the practical learnability of these capabilities from unsupervised data. We propose tracr-injection, a method that allows us to distill algorithms written in RASP directly into a pre-trained language model. We showcase our method by injecting 3 different algorithms into a language model. We show how our method creates an interpretable subspace within the model's residual stream, which can be decoded into the variables present in the code of the RASP algorithm. Additionally, we found that the proposed method can improve out-of-distribution performance compared to our baseline, indicating that indeed a more symbolic mechanism is taking place in the inner workings of the model. We release the code used to run our experiments.",
      "arxiv_url": "https://arxiv.org/abs/2505.10719",
      "pdf_url": "https://arxiv.org/pdf/2505.10719",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-15",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.07390",
      "title": "Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data",
      "authors": [
        "Xinjie Wen",
        "Yijun Yang",
        "Cuiyun Gao",
        "Yang Xiao",
        "Deheng Ye"
      ],
      "abstract": "Large language models (LLMs) demonstrate considerable proficiency in numerous coding-related tasks; however, their capabilities in detecting software vulnerabilities remain limited. This limitation primarily stems from two factors: (1) the absence of reasoning data related to vulnerabilities, which hinders the models' ability to capture underlying vulnerability patterns; and (2) their focus on learning semantic representations rather than the reason behind them, thus failing to recognize semantically similar vulnerability samples. Furthermore, the development of LLMs specialized in vulnerability detection is challenging, particularly in environments characterized by the scarcity of high-quality datasets. In this paper, we propose a novel framework ReVD that excels at mining vulnerability patterns through reasoning data synthesizing and vulnerability-specific preference optimization. Specifically, we construct forward and backward reasoning processes for vulnerability and corresponding fixed code, ensuring the synthesis of high-quality reasoning data. Moreover, we design the triplet supervised fine-tuning followed by curriculum online preference optimization for enabling ReVD to better understand vulnerability patterns. The extensive experiments conducted on PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for LLM-based software vulnerability detection, e.g., 12.24\\%-22.77\\% improvement in the accuracy. The source code and data are available at https://github.com/Xin-Cheng-Wen/PO4Vul.",
      "arxiv_url": "https://arxiv.org/abs/2506.07390",
      "pdf_url": "https://arxiv.org/pdf/2506.07390",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-09",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.15814",
      "title": "Slamming: Training a Speech Language Model on One GPU in a Day",
      "authors": [
        "Gallil Maimon",
        "Avishai Elmakies",
        "Yossi Adi"
      ],
      "abstract": "We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples at - https://pages.cs.huji.ac.il/adiyoss-lab/slamming .",
      "arxiv_url": "https://arxiv.org/abs/2502.15814",
      "pdf_url": "https://arxiv.org/pdf/2502.15814",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-02-19",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.13001",
      "title": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with a Multi-Agent Conversations",
      "authors": [
        "Frederic Kirstein",
        "Muneeb Khan",
        "Jan Philip Wahle",
        "Terry Ruas",
        "Bela Gipp"
      ],
      "abstract": "Meeting summarization suffers from limited high-quality data, mainly due to privacy restrictions and expensive collection processes. We address this gap with FAME, a dataset of 500 meetings in English and 300 in German produced by MIMIC, our new multi-agent meeting synthesis framework that generates meeting transcripts on a given knowledge source by defining psychologically grounded participant profiles, outlining the conversation, and orchestrating a large language model (LLM) debate. A modular post-processing step refines these outputs, mitigating potential repetitiveness and overly formal tones, ensuring coherent, credible dialogues at scale. We also propose a psychologically grounded evaluation framework assessing naturalness, social behavior authenticity, and transcript difficulties. Human assessments show that FAME approximates real-meeting spontaneity (4.5/5 in naturalness), preserves speaker-centric challenges (3/5 in spoken language), and introduces richer information-oriented difficulty (4/5 in difficulty). These findings highlight that FAME is a good and scalable proxy for real-world meeting conditions. It enables new test scenarios for meeting summarization research and other conversation-centric applications in tasks requiring conversation data or simulating social scenarios under behavioral constraints.",
      "arxiv_url": "https://arxiv.org/abs/2502.13001",
      "pdf_url": "https://arxiv.org/pdf/2502.13001",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-18",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.02979",
      "title": "Registering Source Tokens to Target Language Spaces in Multilingual Neural Machine Translation",
      "authors": [
        "Zhi Qu",
        "Yiran Wang",
        "Jiannan Mao",
        "Chenchen Ding",
        "Hideki Tanaka",
        "Masao Utiyama",
        "Taro Watanabe"
      ],
      "abstract": "The multilingual neural machine translation (MNMT) aims for arbitrary translations across multiple languages. Although MNMT-specific models trained on parallel data offer low costs in training and deployment, their performance consistently lags behind that of large language models (LLMs). In this work, we introduce registering, a novel method that enables a small MNMT-specific model to compete with LLMs. Specifically, we insert a set of artificial tokens specifying the target language, called registers, into the input sequence between the source and target tokens. By modifying the attention mask, the target token generation only pays attention to the activation of registers, representing the source tokens in the target language space. Experiments on EC-40, a large-scale benchmark, show that our method advances the state-of-the-art of MNMT. We further pre-train two models, namely MITRE (multilingual translation with registers), by 9.3 billion sentence pairs across 24 languages collected from public corpora. One of them, MITRE-913M, outperforms NLLB-3.3B, achieves comparable performance with commercial LLMs, and shows strong adaptability in fine-tuning. Finally, we open-source our models to facilitate further research and development in MNMT: https://github.com/zhiqu22/mitre.",
      "arxiv_url": "https://arxiv.org/abs/2501.02979",
      "pdf_url": "https://arxiv.org/pdf/2501.02979",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-06",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2501.03545",
      "title": "Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual Information in Long-form Text Generation",
      "authors": [
        "Chris Samarinas",
        "Alexander Krubner",
        "Alireza Salemi",
        "Youngwoo Kim",
        "Hamed Zamani"
      ],
      "abstract": "This paper presents ICAT, an evaluation framework for measuring coverage of diverse factual information in long-form text generation. ICAT breaks down a long output text into a list of atomic claims and not only verifies each claim through retrieval from a (reliable) knowledge source, but also computes the alignment between the atomic factual claims and various aspects expected to be presented in the output. We study three implementations of the ICAT framework, each with a different assumption on the availability of aspects and alignment method. By adopting data from the diversification task in the TREC Web Track and the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong correlation with human judgments and provide comprehensive evaluation across multiple state-of-the-art LLMs. Our framework further offers interpretable and fine-grained analysis of diversity and coverage. Its modular design allows for easy adaptation to different domains and datasets, making it a valuable tool for evaluating the qualitative aspects of long-form responses produced by LLMs.",
      "arxiv_url": "https://arxiv.org/abs/2501.03545",
      "pdf_url": "https://arxiv.org/pdf/2501.03545",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-01-07",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "ff0bb8eab5591013deeec2ae22fcfd0bbfeb94bc",
      "title": "Personal Travel Solver: A Preference-Driven LLM-Solver System for Travel Planning",
      "authors": [
        "Zijian Shao",
        "Jiancan Wu",
        "Weijian Chen",
        "Xiang Wang"
      ],
      "abstract": "Personal travel planning is a challenging task that aims to find a feasible plan that not only satisfies diverse constraints but also meets the demands of the user’s explicit and implicit preferences. In this paper, we study how to integrate the user’s implicit preference into the progress of travel planning. We introduce Re-alTravel, an augmented version of the Trav-elPlanner by incorporating real user reviews and point-of-interest metadata from Google Local. Based on RealTravel, we propose Personal Travel Solver (PTS), an integrated system that combines LLMs with numerical solvers to generate travel plans that satisfy both explicit constraints and implicit user preferences. PTS employs a novel architecture that seamlessly connects explicit constraint validation with implicit preference modeling through five specialized modules. The experimental results demonstrate the system’s effectiveness, achieving better performance than baseline methods, and improvement in the level of personalization. Our data and code are available at PersonalTravelSolver.",
      "arxiv_url": "https://www.semanticscholar.org/paper/ff0bb8eab5591013deeec2ae22fcfd0bbfeb94bc",
      "pdf_url": "",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.13405",
      "title": "RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for Evaluating LLM-Based Table Analysis",
      "authors": [
        "Peng Wu",
        "Yuhang Yang",
        "Guangcheng Zhu",
        "Chaonan Ye",
        "Hong Gu",
        "Xu Lu",
        "Rui Xiao",
        "Bowen Bao",
        "Yijing He",
        "Liangyu Zha",
        "Wen-song Ye",
        "Junbo Zhao",
        "Haobo Wang"
      ],
      "abstract": "With the rapid advancement of Large Language Models (LLMs), there is an increasing need for challenging benchmarks to evaluate their capabilities in handling complex tabular data. However, existing benchmarks are either based on outdated data setups or focus solely on simple, flat table structures. In this paper, we introduce RealHiTBench, a comprehensive benchmark designed to evaluate the performance of both LLMs and Multimodal LLMs (MLLMs) across a variety of input formats for complex tabular data, including LaTeX, HTML, and PNG. RealHiTBench also includes a diverse collection of tables with intricate structures, spanning a wide range of task types. Our experimental results, using 25 state-of-the-art LLMs, demonstrate that RealHiTBench is indeed a challenging benchmark. Moreover, we also develop TreeThinker, a tree-based pipeline that organizes hierarchical headers into a tree structure for enhanced tabular reasoning, validating the importance of improving LLMs'perception of table hierarchies. We hope that our work will inspire further research on tabular data reasoning and the development of more robust models. The code and data are available at https://github.com/cspzyy/RealHiTBench.",
      "arxiv_url": "https://arxiv.org/abs/2506.13405",
      "pdf_url": "https://arxiv.org/pdf/2506.13405",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-16",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.18708",
      "title": "A General Knowledge Injection Framework for ICD Coding",
      "authors": [
        "Xu Zhang",
        "Kun Zhang",
        "Wenxin Ma",
        "Rongsheng Wang",
        "Chenxu Wu",
        "Yingtai Li",
        "S. K. Zhou"
      ],
      "abstract": "ICD Coding aims to assign a wide range of medical codes to a medical text document, which is a popular and challenging task in the healthcare domain. To alleviate the problems of long-tail distribution and the lack of annotations of code-specific evidence, many previous works have proposed incorporating code knowledge to improve coding performance. However, existing methods often focus on a single type of knowledge and design specialized modules that are complex and incompatible with each other, thereby limiting their scalability and effectiveness. To address this issue, we propose GKI-ICD, a novel, general knowledge injection framework that integrates three key types of knowledge, namely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized design of additional modules. The comprehensive utilization of the above knowledge, which exhibits both differences and complementarity, can effectively enhance the ICD coding performance. Extensive experiments on existing popular ICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves the state-of-the-art performance on most evaluation metrics. Code is available at https://github.com/xuzhang0112/GKI-ICD.",
      "arxiv_url": "https://arxiv.org/abs/2505.18708",
      "pdf_url": "https://arxiv.org/pdf/2505.18708",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-24",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2507.06043",
      "title": "CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations",
      "authors": [
        "Xiaohu Li",
        "Yunfeng Ning",
        "Zepeng Bao",
        "Mayi Xu",
        "Jianhao Chen",
        "Tieyun Qian"
      ],
      "abstract": "Security alignment enables the Large Language Model (LLM) to gain the protection against malicious queries, but various jailbreak attack methods reveal the vulnerability of this security mechanism. Previous studies have isolated LLM jailbreak attacks and defenses. We analyze the security protection mechanism of the LLM, and propose a framework that combines attack and defense. Our method is based on the linearly separable property of LLM intermediate layer embedding, as well as the essence of jailbreak attack, which aims to embed harmful problems and transfer them to the safe area. We utilize generative adversarial network (GAN) to learn the security judgment boundary inside the LLM to achieve efficient jailbreak attack and defense. The experimental results indicate that our method achieves an average jailbreak success rate of 88.85\\% across three popular LLMs, while the defense success rate on the state-of-the-art jailbreak dataset reaches an average of 84.17\\%. This not only validates the effectiveness of our approach but also sheds light on the internal security mechanisms of LLMs, offering new insights for enhancing model security The code and data are available at https://github.com/NLPGM/CAVGAN.",
      "arxiv_url": "https://arxiv.org/abs/2507.06043",
      "pdf_url": "https://arxiv.org/pdf/2507.06043",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-07-08",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.08952",
      "title": "Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions",
      "authors": [
        "Clara Lachenmaier",
        "Judith Sieker",
        "Sina Zarrieß"
      ],
      "abstract": "Communication among humans relies on conversational grounding, allowing interlocutors to reach mutual understanding even when they do not have perfect knowledge and must resolve discrepancies in each other's beliefs. This paper investigates how large language models (LLMs) manage common ground in cases where they (don't) possess knowledge, focusing on facts in the political domain where the risk of misinformation and grounding failure is high. We examine the ability of LLMs to answer direct knowledge questions and loaded questions that presuppose misinformation. We evaluate whether loaded questions lead LLMs to engage in active grounding and correct false user beliefs, in connection to their level of knowledge and their political bias. Our findings highlight significant challenges in LLMs' ability to engage in grounding and reject false user beliefs, raising concerns about their role in mitigating misinformation in political discourse.",
      "arxiv_url": "https://arxiv.org/abs/2506.08952",
      "pdf_url": "https://arxiv.org/pdf/2506.08952",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-10",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2502.14830",
      "title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs",
      "authors": [
        "Danni Liu",
        "Jan Niehues"
      ],
      "abstract": "While large language models demonstrate remarkable capabilities at task-specific applications through fine-tuning, extending these benefits across diverse languages is essential for broad accessibility. However, effective cross-lingual transfer is hindered by LLM performance gaps across languages and the scarcity of fine-tuning data in many languages. Through analysis of LLM internal representations from over 1,000+ language pairs, we discover that middle layers exhibit the strongest potential for cross-lingual alignment. Building on this finding, we propose a middle-layer alignment objective integrated into task-specific training. Our experiments on slot filling, machine translation, and structured text generation show consistent improvements in cross-lingual transfer, especially to lower-resource languages. The method is robust to the choice of alignment languages and generalizes to languages unseen during alignment. Furthermore, we show that separately trained alignment modules can be merged with existing task-specific modules, improving cross-lingual capabilities without full re-training. Our code is publicly available (https://github.com/dannigt/mid-align).",
      "arxiv_url": "https://arxiv.org/abs/2502.14830",
      "pdf_url": "https://arxiv.org/pdf/2502.14830",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-02-20",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.16526",
      "title": "EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance",
      "authors": [
        "Heejae Suh",
        "Yejin Jeon",
        "Deokhyung Kang",
        "Taehee Park",
        "Yejin Min",
        "G. Lee"
      ],
      "abstract": "Small large language models (sLLMs) offer the advantage of being lightweight and efficient, which makes them suitable for resource-constrained environments. However, sLLMs often struggle to maintain topic consistency in task-oriented dialogue systems, which is critical for scenarios such as service chatbots. Specifically, it is important to ensure that the model denies off-topic or malicious inputs and adheres to its intended functionality so as to prevent potential misuse and uphold reliability. Towards this, existing activation engineering approaches have been proposed to manipulate internal activations during inference. While these methods are effective in certain scenarios, our preliminary experiments reveal their limitations in ensuring topic adherence. Therefore, to address this, we propose a novel approach termed Entropy-scaled Steering vectors for Topic Maintenance (EnSToM). EnSToM dynamically adjusts the steering intensity based on input uncertainty, which allows the model to handle off-topic distractors effectively while preserving on-topic accuracy. Our experiments demonstrate that EnSToM achieves significant performance gain with a relatively small data size compared to fine-tuning approaches. By improving topic adherence without compromising efficiency, our approach provides a robust solution for enhancing sLLM-based dialogue systems.",
      "arxiv_url": "https://arxiv.org/abs/2505.16526",
      "pdf_url": "https://arxiv.org/pdf/2505.16526",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-22",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.01829",
      "title": "CiteEval: Principle-Driven Citation Evaluation for Source Attribution",
      "authors": [
        "Yumo Xu",
        "Peng Qi",
        "Jifan Chen",
        "Kunlun Liu",
        "Rujun Han",
        "Lan Liu",
        "Bonan Min",
        "Vittorio Castelli",
        "Arshit Gupta",
        "Zhiguo Wang"
      ],
      "abstract": "Citation quality is crucial in information-seeking systems, directly influencing trust and the effectiveness of information access. Current evaluation frameworks, both human and automatic, mainly rely on Natural Language Inference (NLI) to assess binary or ternary supportiveness from cited sources, which we argue is a suboptimal proxy for citation evaluation. In this work we introduce CiteEval, a citation evaluation framework driven by principles focusing on fine-grained citation assessment within a broad context, encompassing not only the cited sources but the full retrieval context, user query, and generated text. Guided by the proposed framework, we construct CiteBench, a multi-domain benchmark with high-quality human annotations on citation quality. To enable efficient evaluation, we further develop CiteEval-Auto, a suite of model-based metrics that exhibit strong correlation with human judgments. Experiments across diverse systems demonstrate CiteEval-Auto's superior ability to capture the multifaceted nature of citations compared to existing metrics, offering a principled and scalable approach to evaluate and improve model-generated citations.",
      "arxiv_url": "https://arxiv.org/abs/2506.01829",
      "pdf_url": "https://arxiv.org/pdf/2506.01829",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-06-02",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.03335",
      "title": "iNews: A Multimodal Dataset for Modeling Personalized Affective Responses to News",
      "authors": [
        "Tiancheng Hu",
        "Nigel Collier"
      ],
      "abstract": "Understanding how individuals perceive and react to information is fundamental for advancing social and behavioral sciences and developing human-centered AI systems. Current approaches often lack the granular data needed to model these personalized responses, relying instead on aggregated labels that obscure the rich variability driven by individual differences. We introduce iNews, a novel large-scale dataset specifically designed to facilitate the modeling of personalized affective responses to news content. Our dataset comprises annotations from 291 demographically diverse UK participants across 2,899 multimodal Facebook news posts from major UK outlets, with an average of 5.18 annotators per sample. For each post, annotators provide multifaceted labels including valence, arousal, dominance, discrete emotions, content relevance judgments, sharing likelihood, and modality importance ratings. Crucially, we collect comprehensive annotator persona information covering demographics, personality, media trust, and consumption patterns, which explain 15.2% of annotation variance - substantially higher than existing NLP datasets. Incorporating this information yields a 7% accuracy gain in zero-shot prediction and remains beneficial even with 32-shot in-context learning. iNews opens new possibilities for research in LLM personalization, subjectivity, affective computing, and human behavior simulation.",
      "arxiv_url": "https://arxiv.org/abs/2503.03335",
      "pdf_url": "https://arxiv.org/pdf/2503.03335",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2025-03-05",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2506.02591",
      "title": "On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures",
      "authors": [
        "Minh Duc Bui",
        "Kyung Eun Park",
        "Goran Glavavs",
        "Fabian David Schmidt",
        "K. Wense"
      ],
      "abstract": "Measurement systems (e.g., currencies) differ across cultures, but the conversions between them are well defined so that humans can state facts using any measurement system of their choice. Being available to users from diverse cultural backgrounds, large language models (LLMs) should also be able to provide accurate information irrespective of the measurement system at hand. Using newly compiled datasets we test if this is the case for seven open-source LLMs, addressing three key research questions: (RQ1) What is the default system used by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their accuracy vary across different measurement systems? (RQ3) Can LLMs mitigate potential challenges w.r.t. underrepresented systems via reasoning? Our findings show that LLMs default to the measurement system predominantly used in the data. Additionally, we observe considerable instability and variance in performance across different measurement systems. While this instability can in part be mitigated by employing reasoning methods such as chain-of-thought (CoT), this implies longer responses and thereby significantly increases test-time compute (and inference costs), marginalizing users from cultural backgrounds that use underrepresented measurement systems.",
      "arxiv_url": "https://arxiv.org/abs/2506.02591",
      "pdf_url": "https://arxiv.org/pdf/2506.02591",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-06-03",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2505.11983",
      "title": "Online Iterative Self-Alignment for Radiology Report Generation",
      "authors": [
        "Ting Xiao",
        "Lei Shi",
        "Yang Zhang",
        "HaoFeng Yang",
        "Zhe Wang",
        "Chenjia Bai"
      ],
      "abstract": "Radiology Report Generation (RRG) is an important research topic for relieving radiologist' heavy workload. Existing RRG models mainly rely on supervised fine-tuning (SFT) based on different model architectures using data pairs of radiological images and corresponding radiologist-annotated reports. Recent research has shifted focus to post-training improvements, aligning RRG model outputs with human preferences using reinforcement learning (RL). However, the limited data coverage of high-quality annotated data poses risks of overfitting and generalization. This paper proposes a novel Online Iterative Self-Alignment (OISA) method for RRG that consists of four stages: self-generation of diverse data, self-evaluation for multi-objective preference data,self-alignment for multi-objective optimization and self-iteration for further improvement. Our approach allows for generating varied reports tailored to specific clinical objectives, enhancing the overall performance of the RRG model iteratively. Unlike existing methods, our frame-work significantly increases data quality and optimizes performance through iterative multi-objective optimization. Experimental results demonstrate that our method surpasses previous approaches, achieving state-of-the-art performance across multiple evaluation metrics.",
      "arxiv_url": "https://arxiv.org/abs/2505.11983",
      "pdf_url": "https://arxiv.org/pdf/2505.11983",
      "primary_category": "",
      "categories": [],
      "tags": [],
      "published_date": "2025-05-17",
      "source": "conference",
      "conference": "ACL 2025"
    },
    {
      "arxiv_id": "2503.09347",
      "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
      "authors": [
        "Hongyu Chen",
        "Seraphina Goldfarb-Tarrant"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly employed as automated evaluators to assess the safety of generated content, yet their reliability in this role remains uncertain. This study evaluates a diverse set of 11 LLM judge models across critical safety domains, examining three key aspects: self-consistency in repeated judging tasks, alignment with human judgments, and susceptibility to input artifacts such as apologetic or verbose phrasing. Our findings reveal that biases in LLM judges can significantly distort the final verdict on which content source is safer, undermining the validity of comparative evaluations. Notably, apologetic language artifacts alone can skew evaluator preferences by up to 98\\%. Contrary to expectations, larger models do not consistently exhibit greater robustness, while smaller models sometimes show higher resistance to specific artifacts. To mitigate LLM evaluator robustness issues, we investigate jury-based evaluations aggregating decisions from multiple models. Although this approach both improves robustness and enhances alignment to human judgements, artifact sensitivity persists even with the best jury configurations. These results highlight the urgent need for diversified, artifact-resistant methodologies to ensure reliable safety assessments.",
      "arxiv_url": "https://arxiv.org/abs/2503.09347",
      "pdf_url": "https://arxiv.org/pdf/2503.09347",
      "primary_category": "",
      "categories": [],
      "tags": [
        "LLM"
      ],
      "published_date": "2025-03-12",
      "source": "conference",
      "conference": "ACL 2025"
    }
  ],
  "available_tags": [
    "Agentic AI",
    "Context Compression",
    "LLM",
    "Multi-Modal RAG",
    "Personalization",
    "RAG",
    "Search Agent"
  ]
}