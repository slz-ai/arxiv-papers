{
  "conference": "NeurIPS 2025",
  "conference_id": "neurips_2025",
  "paper_count": 1289,
  "papers": [
    {
      "arxiv_id": "DWf4vroKWJ",
      "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning",
      "authors": [
        "Haozhen Zhang",
        "Tao Feng",
        "Jiaxuan You"
      ],
      "abstract": "The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \\textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave \"think\" actions (internal deliberation) with \"route\" actions (dynamic model invocation), and integrates each response into its evolving context. To facilitate learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for optimizing the balance between performance and cost, opening a pathway toward enhancing performance-cost trade-offs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms several strong baselines, achieving superior performance while maintaining robust generalization and cost management.",
      "arxiv_url": "https://openreview.net/forum?id=DWf4vroKWJ",
      "pdf_url": "https://openreview.net/pdf/363710ed9012131f837da723473810ad47f9d2c9.pdf",
      "primary_category": "Large Language Models, LLM Routers, LLM Selection",
      "categories": [
        "Large Language Models",
        "LLM Routers",
        "LLM Selection",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cNqMAmpZh4",
      "title": "From Euler to AI: Unifying Formulas for Mathematical Constants",
      "authors": [
        "Tomer Raz",
        "Michael Shalyt",
        "Elyasheev Leibtag",
        "Rotem Kalisch",
        "Shachar Weinbaum",
        "Yaron Hadad",
        "Ido Kaminer"
      ],
      "abstract": "The constant $\\large \\pi$ has fascinated scholars throughout the centuries, inspiring numerous formulas for its evaluation, such as infinite sums and continued fractions. Despite their individual significance, many of the underlying connections among formulas remain unknown, missing unifying theories that could unveil deeper understanding. The absence of a unifying theory reflects a broader challenge across math and science: knowledge is typically accumulated through isolated discoveries, while deeper connections often remain hidden. In this work, we present an automated framework for the unification of mathematical formulas. Our system combines large language models (LLMs) for systematic formula harvesting, an LLM-code feedback loop for validation, and a novel symbolic algorithm for clustering and eventual unification. We demonstrate this methodology on the hallmark case of $\\large \\pi$, an ideal testing ground for symbolic unification. Applying this approach to 455,050 arXiv papers, we validate 385 distinct formulas for $\\large \\pi$ and prove relations between 360 (94\\%) of them, of which 166 (43\\%) can be derived from a single mathematical object—linking canonical formulas by Euler, Gauss, Brouncker, and newer ones from algorithmic discoveries by the Ramanujan Machine.\n  Our method generalizes to other constants, including $e$, $\\zeta(3)$, and Catalan’s constant, demonstrating the potential of AI-assisted mathematics to uncover hidden structures and unify knowledge across domains.",
      "arxiv_url": "https://openreview.net/forum?id=cNqMAmpZh4",
      "pdf_url": "https://openreview.net/pdf/f55f320fc899be57cfb3d8129ece9f565cbf6b97.pdf",
      "primary_category": "AI for Science, AI for Math, LLM-Tool Integration",
      "categories": [
        "AI for Science",
        "AI for Math",
        "LLM-Tool Integration",
        "Mathematical Constants",
        "Continued Fractions",
        "Recurrences",
        "Number Theory",
        "Pi"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "l4F50jpiVH",
      "title": "Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment",
      "authors": [
        "Deokjae Lee",
        "Hyun Oh Song"
      ],
      "abstract": "We study weight-only post-training quantization (PTQ), which quantizes the weights of a large language model (LLM) without retraining, using little or no calibration data. Weight-only PTQ is crucial for reducing the memory footprint and latency of LLM inference, especially in memory-bound, small-batch inference scenarios, such as personalized inference on edge devices. Despite its importance, irregular weight distributions with heavy-tailed outliers in LLMs complicate quantization, recently motivating rotation-based methods that transform weights into near-Gaussian distributions, which are more regular with fewer outliers, thereby reducing quantization error. In this work, we first derive the information-theoretically optimal bit allocation for Gaussianized weights under given bit budgets, revealing that fine-grained fractional-bit quantizers approaching the Gaussian distortion-rate bound are essential to achieve near-optimal quantization performance. To bridge this theoretical insight and practical implementation, we introduce Q-Palette, a versatile collection of fractional-bit quantizers that range from trellis-coded quantizers offering near-optimal distortion to simpler vector and scalar quantizers optimized for faster inference, all efficiently implemented with optimized CUDA kernels across various bitwidths. Furthermore, leveraging Q-Palette as a foundational component, we propose a novel mixed-scheme quantization framework, jointly optimizing quantizer choices and layer fusion decisions given resource constraints. The code is available at https://github.com/snu-mllab/Q-Palette.",
      "arxiv_url": "https://openreview.net/forum?id=l4F50jpiVH",
      "pdf_url": "https://openreview.net/pdf/518ecfba9ed223f4e43c3783e924fd758b01eb9e.pdf",
      "primary_category": "LLM quantization, Post-training quantization, Mixed scheme quantization",
      "categories": [
        "LLM quantization",
        "Post-training quantization",
        "Mixed scheme quantization",
        "Data-free quantization"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "KurYdcCbjv",
      "title": "Generalized Linear Mode Connectivity for Transformers",
      "authors": [
        "Alexander Theus",
        "Alessandro Cabodi",
        "Sotiris Anagnostidis",
        "Antonio Orvieto",
        "Sidak Pal Singh",
        "Valentina Boeva"
      ],
      "abstract": "Understanding the geometry of neural network loss landscapes is a central question in deep learning, with implications for generalization and optimization. A striking phenomenon is $\\textit{linear mode connectivity}$ (LMC), where independently trained models can be connected by low- or zero-barrier paths, despite appearing to lie in separate loss basins. However, this is often obscured by symmetries in parameter space—such as neuron permutations—which make functionally equivalent models appear dissimilar. Prior work has predominantly focused on neuron reordering through permutations, but such approaches are limited in scope and fail to capture the richer symmetries exhibited by modern architectures such as Transformers. In this work, we introduce a unified framework that captures four symmetry classes—permutations, semi-permutations, orthogonal transformations, and general invertible maps—broadening the set of valid reparameterizations and subsuming many previous approaches as special cases. Crucially, this generalization enables, for the first time, the discovery of low- and zero-barrier linear interpolation paths between independently trained Vision Transformers and GPT-2 models. Furthermore, our framework extends beyond pairwise alignment, to multi-model and width-heterogeneous settings, enabling alignment across architectures of different sizes. These results reveal deeper structure in the loss landscape and underscore the importance of symmetry-aware analysis for understanding model space geometry.",
      "arxiv_url": "https://openreview.net/forum?id=KurYdcCbjv",
      "pdf_url": "https://openreview.net/pdf/74f91986c42f68d256ffcef1d4a4ee46f7530d3d.pdf",
      "primary_category": "Neural Network Merging, Linear Mode Connectivity, Model Re-basin",
      "categories": [
        "Neural Network Merging",
        "Linear Mode Connectivity",
        "Model Re-basin",
        "Parameter Space Geometry",
        "Transformer",
        "Permutation Invariance",
        "Model Fusion"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Ve693NkzcU",
      "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$  Pruning",
      "authors": [
        "Chaofan Lin",
        "Jiaming Tang",
        "Shuo Yang",
        "Hanshuo Wang",
        "Tian Tang",
        "Boyu Tian",
        "Ion Stoica",
        "Song Han",
        "Mingyu Gao"
      ],
      "abstract": "Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been of great importance recently. However, most existing sparse attention algorithms use a fixed budget of how many tokens to use in their computations. This simple static decision raises critical issues in real-world deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. \nIn this paper, we reveal a key insight that leveraging the idea of top-$p$ sampling (a.k.a., nucleus sampling) in sparse attention could enable efficient and adaptive budget decisions. Based on this, we propose Twilight, a framework that enhances any existing sparse attention algorithm with adaptive budget decision capabilities without sacrificing accuracy. \nEmpirical results show that Twilight can adaptively prune up to 98% tokens with nearly no accuracy loss in both mid- and long-context scenarios, leading to a $1.4\\times$ speedup over state-of-the-art sparse attention mechanisms.",
      "arxiv_url": "https://openreview.net/forum?id=Ve693NkzcU",
      "pdf_url": "https://openreview.net/pdf/d63b27c7918827915ee716011ba51753540879dd.pdf",
      "primary_category": "Large Language Model, Sparse Attention, Decode",
      "categories": [
        "Large Language Model",
        "Sparse Attention",
        "Decode",
        "KV Cache"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "5GaDcRVgBw",
      "title": "MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching",
      "authors": [
        "Liang Yue",
        "Yihong Tang",
        "Kehai Chen",
        "Jie Liu",
        "Min Zhang"
      ],
      "abstract": "Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models' instruction-following capabilities and task-specific performance. However, obtaining high-quality fine-tuning data for large models is challenging due to data collection difficulties and high production costs. To address this, we propose MASTER, a novel data augmentation method that enriches original data through interactions among multiple agents with varying cognitive levels. We simulate three pedagogically grounded teaching scenarios, leveraging multi-agent conversations to generate high-quality teacher-student interaction data. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented from existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5. Experiments show that models fine-tuned with BOOST-QA perform excellently across multiple benchmarks, demonstrating strong multitask generalization. Notably, MASTER significantly improves models' reasoning abilities in complex tasks, providing valuable insights for future research.",
      "arxiv_url": "https://openreview.net/forum?id=5GaDcRVgBw",
      "pdf_url": "https://openreview.net/pdf/89c75563b27f85aa0ae6b1e6667a8606ee8339d0.pdf",
      "primary_category": "Instruction Fine-Tuning；Data Augmentation；Multi-Agent Systems；Natural Language Processing",
      "categories": [
        "Instruction Fine-Tuning；Data Augmentation；Multi-Agent Systems；Natural Language Processing"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "geNdDlzKTG",
      "title": "Thinking in Character: Advancing Role-Playing Agents with Role-Aware Reasoning",
      "authors": [
        "Yihong Tang",
        "Kehai Chen",
        "Muyun Yang",
        "Zheng-Yu Niu",
        "Jing Li",
        "Tiejun Zhao",
        "Min Zhang"
      ],
      "abstract": "The advancement of Large Language Models (LLMs) has spurred significant interest in Role-Playing Agents (RPAs) for applications such as emotional companionship and virtual interaction. However, recent RPAs are often built on explicit dialogue data, lacking deep, human-like internal thought processes, resulting in superficial knowledge and style expression. While Large Reasoning Models (LRMs) can be employed to simulate character thought, their direct application is hindered by attention diversion (i.e., RPAs forget their role) and style drift (i.e., overly formal and rigid reasoning rather than character-consistent reasoning). To address these challenges, this paper introduces a novel Role-Aware Reasoning (RAR) method, which consists of two important stages: Role Identity Activation (RIA) and Reasoning Style Optimization (RSO). RIA explicitly guides the model with character profiles during reasoning to counteract attention diversion, and then RSO aligns reasoning style with the character and scene via LRM distillation to mitigate style drift. Extensive experiments demonstrate that the proposed RAR significantly enhances the performance of RPAs by effectively addressing attention diversion and style drift.",
      "arxiv_url": "https://openreview.net/forum?id=geNdDlzKTG",
      "pdf_url": "https://openreview.net/pdf/8c6689cef67064ebd1bcb308893de9620efac277.pdf",
      "primary_category": "Role-playing, dialogue generation",
      "categories": [
        "Role-playing",
        "dialogue generation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VwsXmcMyg5",
      "title": "SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification",
      "authors": [
        "ZhengLin Lai",
        "Mengyao Liao",
        "Bingzhe Wu",
        "Dong Xu",
        "Zebin Zhao",
        "Zhihang Yuan",
        "Chao Fan",
        "Jianqiang Li"
      ],
      "abstract": "Large language models with Mixture-of-Experts (MoE) architectures achieve efficiency and scalability, yet their routing mechanisms introduce safety alignment challenges insufficiently addressed by techniques developed for dense models. In this work, the MoE-specific safety risk of positional vulnerability—that safety-aligned behaviors rely on specific expert modules—is formalized and systematically analyzed. An analytical framework, SAFEx, is presented to robustly identify, characterize, and validate safety-critical experts via a stability-based expert selection procedure, and to decompose them into two functional groups: the Harmful Content Detection Group (HCDG), which specializes in identifying and recognizing harmful content within user inputs, and the Harmful Response Control Group (HRCG), which specializes in controlling and enforcing model behaviors to generate appropriate safety responses. Expert-level interventions are conducted to probe causality and to test mitigation. Targeted masking of SAFEx-selected experts reveals that safety behavior is highly concentrated. On Qwen3-30B-A3B, configured with 48 MoE-FFN layers and 128 experts per layer under top-8 routing (48×128=6,144 experts in total), disabling 12 selected experts reduces the refusal rate by 22%. In addition, lightweight adaptation is performed using LoRA under three configurations—the HRCG, the union of HCDG and HRCG, and all experts—and the resulting updates are composed through negative weight merging targeted at the HRCG, leading to improved refusal under adversarial prompts without full-model retraining. These results establish positional vulnerability as a distinct MoE-specific safety challenge and provide a practical, compute-efficient pathway for expert-level safety interventions within routed architectures.",
      "arxiv_url": "https://openreview.net/forum?id=VwsXmcMyg5",
      "pdf_url": "https://openreview.net/pdf/fedcd79e0b8d0417167c9aa43e3096fe21438d2d.pdf",
      "primary_category": "Trustworthy AI",
      "categories": [
        "Trustworthy AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "iBFfb6bGOz",
      "title": "Atomic Thinking of LLMs: Decoupling and Exploring Mathematical Reasoning Abilities",
      "authors": [
        "Jiayi Kuang",
        "Haojing Huang",
        "Yinghui Li",
        "Xinnian Liang",
        "Zhikun Xu",
        "Yangning Li",
        "Xiaoyu Tan",
        "Chao Qu",
        "Meishan Zhang",
        "Ying Shen",
        "Philip S. Yu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated outstanding performance in mathematical reasoning capabilities. However, we argue that current large-scale reasoning models primarily rely on scaling up training datasets with diverse mathematical problems and long thinking chains, which raises questions about whether LLMs genuinely acquire mathematical concepts and reasoning principles or merely remember the training data. In contrast, humans tend to break down complex problems into multiple fundamental atomic capabilities. Inspired by this, we propose a new paradigm for evaluating mathematical atomic capabilities. Our work categorizes atomic abilities into two dimensions: (1) field-specific abilities across four major mathematical fields, algebra, geometry, analysis, and topology, and (2) logical abilities at different levels, including conceptual understanding, forward multi-step reasoning with formal math language, and counterexample-driven backward reasoning. We propose corresponding training and evaluation datasets for each atomic capability unit, and conduct extensive experiments about how different atomic capabilities influence others, to explore the strategies to elicit the required specific atomic capability. Evaluation and experimental results on advanced models show many interesting discoveries and inspirations about the different performances of models on various atomic capabilities and the interactions between atomic capabilities. Our findings highlight the importance of decoupling mathematical intelligence into atomic components, providing new insights into model cognition and guiding the development of training strategies toward a more efficient, transferable, and cognitively grounded paradigm of \"atomic thinking\".",
      "arxiv_url": "https://openreview.net/forum?id=iBFfb6bGOz",
      "pdf_url": "https://openreview.net/pdf/78925ef619e272b7ccf7401d04d090ada6990831.pdf",
      "primary_category": "Large Language Models, Mathematical Reasoning, Atomic Thinking",
      "categories": [
        "Large Language Models",
        "Mathematical Reasoning",
        "Atomic Thinking"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "OM0Qkq9xtY",
      "title": "Brain network science modelling of sparse neural networks enables Transformers and LLMs to perform as fully connected",
      "authors": [
        "Yingtao Zhang",
        "Diego Cerretti",
        "Jialin Zhao",
        "Wenjing Wu",
        "Ziheng Liao",
        "Umberto Michieli",
        "Carlo Vittorio Cannistraci"
      ],
      "abstract": "This study aims to enlarge our current knowledge on the application of brain-inspired network science principles for training artificial neural networks (ANNs) with sparse connectivity. Dynamic sparse training (DST) emulates the synaptic turnover of real brain networks, reducing the computational demands of training and inference in ANNs. However, existing DST methods face difficulties in maintaining peak performance at high connectivity sparsity levels. The Cannistraci-Hebb training (CHT) is a brain-inspired method that is used in DST for growing synaptic connectivity in sparse neural networks. CHT leverages a gradient-free, topology-driven link regrowth mechanism, which has been shown to achieve ultra-sparse (1\\% connectivity or lower) advantage across various tasks compared to fully connected networks. Yet, CHT suffers two main drawbacks: (i) its time complexity is $\\mathcal{O}(N\\cdot d^3)$- N node network size, d node degree - hence it can be efficiently applied only to ultra-sparse networks. (ii) it rigidly selects top link prediction scores, which is inappropriate for the early training epochs, when the network topology presents many unreliable connections. Here, we design the first brain-inspired network model - termed bipartite receptive field (BRF) - to initialize the connectivity of sparse artificial neural networks. Then, we propose a matrix multiplication GPU-friendly approximation of the CH link predictor, which reduces the computational complexity to $\\mathcal{O}(N^3)$, enabling a fast implementation of link prediction in large-scale models. Moreover, we introduce the Cannistraci-Hebb training soft rule (CHTs), which adopts a flexible strategy for sampling connections in both link removal and regrowth, balancing the exploration and exploitation of network topology. Additionally, we propose a sigmoid-based gradual density decay strategy, leading to an advanced framework referred to as CHTss. Empirical results show that BRF offers performance advantages over previous network science models. Using 1\\% of connections, CHTs outperforms fully connected networks in MLP architectures on visual classification tasks, compressing some networks to less than 30\\% of the nodes. Using 5\\% of the connections, CHTss outperforms fully connected networks in two Transformer-based machine translation tasks. Finally, with only 30\\% of the connections, both CHTs and CHTss achieve superior performance over other dynamic sparse training methods, and perform on par with—or even surpass—their fully connected counterparts in language modeling across various sparsity levels within the LLaMA model family. The code is available at: https://github.com/biomedical-cybernetics/Cannistraci-Hebb-Training-Soft-Rule-.",
      "arxiv_url": "https://openreview.net/forum?id=OM0Qkq9xtY",
      "pdf_url": "https://openreview.net/pdf/d4df0dbbaaa665a14d4e33596a9bec5cc6954c06.pdf",
      "primary_category": "dynamic sparse training, network science, epitopological Learning",
      "categories": [
        "dynamic sparse training",
        "network science",
        "epitopological Learning",
        "efficient training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9PL1DIIB7e",
      "title": "JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model",
      "authors": [
        "Qihao Duan",
        "Bingding Huang",
        "Zhenqiao Song",
        "Irina Lehmann",
        "Lei Gu",
        "Roland Eils",
        "Benjamin Wild"
      ],
      "abstract": "Large language models (LLMs) have revolutionized natural language processing and are increasingly applied to other sequential data types, including genetic sequences. However, adapting LLMs to genetics presents significant challenges. Capturing complex genomic interactions requires modeling long-range global dependencies within DNA sequences, where interactions often span over 10,000 base pairs, even within a single gene. This poses substantial computational demands under conventional model architectures and training paradigms. Additionally, traditional LLM training approaches are suboptimal for DNA sequences: autoregressive training, while efficient for training, only supports unidirectional sequence understanding. However, DNA is inherently bidirectional. For instance, bidirectional promoters regulate gene expression in both directions and govern approximately 11% of human gene expression. Masked language models (MLMs) enable bidirectional understanding. However, they are inefficient since only masked tokens contribute to loss calculations at each training step. To address these limitations, we introduce JanusDNA, the first bidirectional DNA foundation model built upon a novel pretraining paradigm, integrating the optimization efficiency of autoregressive modeling with the bidirectional comprehension capability of masked modeling. JanusDNA's architecture leverages a Mamba-Attention Mixture-of-Experts (MoE) design, combining the global, high-resolution context awareness of attention mechanisms with the efficient sequential representation learning capabilities of Mamba. The MoE layers further enhance the model's capacity through sparse parameter scaling, while maintaining manageable computational costs. Notably, JanusDNA can process up to 1 million base pairs at single-nucleotide resolution on a single 80GB GPU using its hybrid architecture. Extensive experiments and ablation studies demonstrate that JanusDNA achieves new state-of-the-art performance on three genomic representation benchmarks. Remarkably, JanusDNA surpasses models with 250x more activated parameters, underscoring its efficiency and effectiveness. Code available at https://anonymous.4open.science/r/JanusDNA/.",
      "arxiv_url": "https://openreview.net/forum?id=9PL1DIIB7e",
      "pdf_url": "https://openreview.net/pdf/e0890fe573f380ab7dfa6c28d2d868aa21121b43.pdf",
      "primary_category": "genomics, foundation model, hybrid architecture",
      "categories": [
        "genomics",
        "foundation model",
        "hybrid architecture",
        "learning efficiency"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wVxIBvUAlj",
      "title": "Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration",
      "authors": [
        "Junqi Gao",
        "Zhichang Guo",
        "Dazhi Zhang",
        "Dong Li",
        "Runze Liu",
        "Pengfei Li",
        "Kai Tian",
        "Biqing Qi"
      ],
      "abstract": "Heterogeneous Large Language Model (LLM) fusion integrates the strengths of multiple source LLMs with different architectures into a target LLM with low computational overhead. While promising, existing methods suffer from two major limitations: 1) **reliance on real data from limited domain** for knowledge fusion, preventing the target LLM from fully acquiring knowledge across diverse domains, and 2) **fixed data allocation proportions** across domains, failing to dynamically adjust according to the target LLM's varying capabilities across domains, leading to a capability imbalance. To overcome these limitations, we propose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework. Through the organization of knowledge domains into a hierarchical tree structure, Bohdi enables automatic domain exploration and multi-domain data generation through multi-model collaboration, thereby comprehensively extracting knowledge from source LLMs. By formalizing domain expansion and data sampling proportion allocation on the knowledge tree as a Hierarchical Multi-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism to adaptively adjust sampling proportions based on the target LLM's performance feedback across domains. Integrated with our proposed Introspection-Rebirth (IR) mechanism, DynaBranches dynamically tracks capability shifts during target LLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT), further enhancing its online adaptation capability. Comparative experimental results on a comprehensive suite of benchmarks demonstrate that Bohdi significantly outperforms existing baselines on multiple target LLMs, exhibits higher data efficiency, and virtually eliminates the imbalance in the target LLM's capabilities.",
      "arxiv_url": "https://openreview.net/forum?id=wVxIBvUAlj",
      "pdf_url": "https://openreview.net/pdf/2450ff0c9195847a41ea55365386d55fb6357f6d.pdf",
      "primary_category": "Heterogeneous Model Fusion, Large Language Models",
      "categories": [
        "Heterogeneous Model Fusion",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "14B5d6NEaH",
      "title": "Reinforcement Learning with Backtracking Feedback",
      "authors": [
        "Bilgehan Sel",
        "Vaishakh Keshava",
        "Phillip Wallis",
        "Lukas Rutishauser",
        "Ming Jin",
        "Dingcheng Li"
      ],
      "abstract": "Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient \"backtrack by x tokens\" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.",
      "arxiv_url": "https://openreview.net/forum?id=14B5d6NEaH",
      "pdf_url": "https://openreview.net/pdf/238b73fa0aa8697d940827f6657973777e3770bd.pdf",
      "primary_category": "large language models, safety alignment, reinforcement learning",
      "categories": [
        "large language models",
        "safety alignment",
        "reinforcement learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "fggSyPPk0K",
      "title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner",
      "authors": [
        "Junhao Shi",
        "Zhaoye Fei",
        "Siyin Wang",
        "Qipeng Guo",
        "Jingjing Gong",
        "Xipeng Qiu"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) show promise for embodied planning tasks but struggle with complex scenarios involving unfamiliar environments and multi-step goals. \nCurrent approaches rely on environment-agnostic imitation learning that disconnects instructions from environmental contexts, causing models to struggle with context-sensitive instructions and rely on supplementary cues rather than visual reasoning during long-horizon interactions.\nIn this work, we propose World-Aware Planning Narrative Enhancement (WAP), a framework that infuses LVLMs with comprehensive environmental understanding through four cognitive capabilities (visual appearance modeling, spatial reasoning, functional abstraction, and syntactic grounding) while developing and evaluating models using only raw visual observations through curriculum learning.\nEvaluations on the EB-ALFRED benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a 60.7 absolute improvement in task success rates—particularly in commonsense reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced open-source models outperform proprietary systems like GPT-4o and Claude-3.5-Sonnet by a large margin.",
      "arxiv_url": "https://openreview.net/forum?id=fggSyPPk0K",
      "pdf_url": "https://openreview.net/pdf/c4dfdf9e91cc4527c4c879b52d8cc8d352f626ce.pdf",
      "primary_category": "planning, embodied, LVLMs",
      "categories": [
        "planning",
        "embodied",
        "LVLMs"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Zrqn7ZshXG",
      "title": "From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization",
      "authors": [
        "Shoaib Ahmed Siddiqui",
        "Adrian Weller",
        "David Krueger",
        "Gintare Karolina Dziugaite",
        "Michael Curtis Mozer",
        "Eleni Triantafillou"
      ],
      "abstract": "Recent unlearning methods for LLMs are vulnerable to relearning attacks: knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of (even seemingly-unrelated) examples. We study this phenomenon in a controlled setting for example-level unlearning in vision classifiers. We make the surprising discovery that forget-set accuracy can recover from around 50\\% post-unlearning to nearly 100\\% with fine-tuning on just the *retain* set---i.e., zero examples of the forget set. We observe this effect across a wide variety of unlearning methods, whereas for a model retrained from scratch excluding the forget set (gold standard), the accuracy remains at 50\\%. We observe that resistance to relearning attacks can be predicted by weight-space properties, specifically, $L_2$-distance and linear mode connectivity between the original and the unlearned model. Leveraging this insight, we propose a new class of methods that achieve state-of-the-art resistance to relearning attacks.",
      "arxiv_url": "https://openreview.net/forum?id=Zrqn7ZshXG",
      "pdf_url": "https://openreview.net/pdf/be9613c2de1eedd9326b1b659054e2fabaf7892b.pdf",
      "primary_category": "Unlearning, tamper-resistance, relearning attacks",
      "categories": [
        "Unlearning",
        "tamper-resistance",
        "relearning attacks",
        "weight-space analysis"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "H1NGlLNaVC",
      "title": "CoCoA: A Minimum Bayes Risk Framework Bridging Confidence and Consistency for Uncertainty Quantification in LLMs",
      "authors": [
        "Roman Vashurin",
        "Maiya Goloburda",
        "Albina Ilina",
        "Aleksandr Rubashevskii",
        "Preslav Nakov",
        "Artem Shelmanov",
        "Maxim Panov"
      ],
      "abstract": "Uncertainty quantification for Large Language Models (LLMs) encompasses a diverse range of approaches, with two major families being particularly prominent: (i) information-based, which estimate model confidence from token-level probabilities, and (ii) consistency-based, which assess the semantic agreement among multiple outputs generated using repeated sampling. While several recent methods have sought to combine these two paradigms to improve uncertainty quantification performance, they often fail to consistently outperform simpler baselines. In this work, we revisit the foundations of uncertainty estimation through the lens of Minimum Bayes Risk decoding, establishing a direct link between uncertainty and the optimal decision-making process of LLMs. Building on these findings, we propose CoCoA, a unified framework that integrates model confidence with output consistency, yielding a family of efficient and robust uncertainty quantification methods. We evaluate CoCoA across diverse tasks, including question answering, abstractive text summarization, and machine translation, and demonstrate sizable improvements over state-of-the-art uncertainty quantification approaches.",
      "arxiv_url": "https://openreview.net/forum?id=H1NGlLNaVC",
      "pdf_url": "https://openreview.net/pdf/36f74a3dd4e92dac192bad51acfae5ac8e53023f.pdf",
      "primary_category": "LLM, Large Language Model, Uncertainty Quantification",
      "categories": [
        "LLM",
        "Large Language Model",
        "Uncertainty Quantification",
        "Minimum Bayes Risk"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "AWU93F6Bup",
      "title": "MoodAngels: A Retrieval-augmented Multi-agent Framework for Psychiatry Diagnosis",
      "authors": [
        "Mengxi Xiao",
        "Ben Liu",
        "He Li",
        "Jimin Huang",
        "Qianqian Xie",
        "Xiaofen Zong",
        "Mang Ye",
        "Min Peng"
      ],
      "abstract": "The application of AI in psychiatric diagnosis faces significant challenges, including the subjective nature of mental health assessments, symptom overlap across disorders, and privacy constraints limiting data availability. To address these issues, we present MoodAngels, the first specialized multi-agent framework for mood disorder diagnosis. Our approach combines granular-scale analysis of clinical assessments with a structured verification process, enabling more accurate interpretation of complex psychiatric data. Complementing this framework, we introduce MoodSyn, an open-source dataset of 1,173 synthetic psychiatric cases that preserves clinical validity while ensuring patient privacy. Experimental results demonstrate that MoodAngels outperforms conventional methods, with our baseline agent achieving 12.3\\% higher accuracy than GPT-4o on real-world cases, and our full multi-agent system delivering further improvements. Together, these contributions provide both an advanced diagnostic tool and a critical research resource for computational psychiatry, bridging important gaps in AI-assisted mental health assessment.",
      "arxiv_url": "https://openreview.net/forum?id=AWU93F6Bup",
      "pdf_url": "https://openreview.net/pdf/df7fcff1d36dbd01901985729311dd34fae572a1.pdf",
      "primary_category": "psychiatry diagnosis, multi-agent framework, mental health",
      "categories": [
        "psychiatry diagnosis",
        "multi-agent framework",
        "mental health"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6lCY5bLW8E",
      "title": "FedFACT: A Provable Framework for Controllable Group-Fairness Calibration in Federated Learning",
      "authors": [
        "Li Zhang",
        "Zhongxuan Han",
        "XiaoHua Feng",
        "Jiaming Zhang",
        "Yuyuan Li",
        "Chaochao Chen"
      ],
      "abstract": "With emerging application of Federated Learning (FL) in decision-making scenarios, it is imperative to regulate model fairness to prevent disparities across sensitive groups (e.g., female, male).\nCurrent research predominantly focuses on two concepts of group fairness within FL: *Global Fairness* (overall model disparity across all clients) and *Local Fairness* (the disparity within each client).\nHowever, the non-decomposable, non-differentiable nature of fairness criteria pose two fundamental, unresolved challenges for fair FL: (i) *Harmonizing global and local fairness, especially in multi-class classification*; (ii) *Enabling a controllable, optimal accuracy-fairness trade-off*.\nTo tackle the aforementioned challenges, we propose a novel controllable federated group-fairness calibration framework, named FedFACT.\nFedFACT identifies the Bayes-optimal classifiers under both global and local fairness constraints in multi-class case, yielding models with minimal performance decline while guaranteeing fairness.\nTo effectively realize an adjustable, optimal accuracy-fairness balance, we derive specific characterizations of the Bayes-optimal fair classifiers for reformulating fair FL as personalized cost-sensitive learning problem for in-processing, and bi-level optimization for post-processing.\nTheoretically, we provide convergence and generalization guarantees for FedFACT to approach the near-optimal accuracy under given fairness levels.\nExtensive experiments on multiple datasets across various data heterogeneity demonstrate that FedFACT consistently outperforms baselines in balancing accuracy and global-local fairness.",
      "arxiv_url": "https://openreview.net/forum?id=6lCY5bLW8E",
      "pdf_url": "https://openreview.net/pdf/5df5d12019dda54e36fff2ebea8a2c18a18d1413.pdf",
      "primary_category": "Federated Learning; Fairness; Multi-Class Classification",
      "categories": [
        "Federated Learning; Fairness; Multi-Class Classification"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2TKEGTfQBd",
      "title": "Transformer Key-Value Memories Are Nearly as Interpretable as Sparse Autoencoders",
      "authors": [
        "Mengyu Ye",
        "Jun Suzuki",
        "Tatsuro Inaba",
        "Tatsuki Kuribayashi"
      ],
      "abstract": "Recent interpretability work on large language models (LLMs) has been increasingly dominated by a feature-discovery approach with the help of proxy modules. Then, the quality of features learned by, e.g., sparse auto-encoders (SAEs), is evaluated.\nThis paradigm naturally raises a critical question: do such learned features have better properties than those already represented within the original model parameters, and unfortunately, only a few studies have made such comparisons systematically so far.\nIn this work, we revisit the interpretability of feature vectors stored in feed-forward (FF) layers, given the perspective of FF as key-value memories, with modern interpretability benchmarks.\nOur extensive evaluation revealed that SAE and FFs exhibits a similar range of interpretability, although SAEs displayed an observable but minimal improvement in some aspects. \nFurthermore, in certain aspects, surprisingly, even vanilla FFs yielded better interpretability than the SAEs, and features discovered in SAEs and FFs diverged.\nThese bring questions about the advantage of SAEs from both perspectives of feature quality and faithfulness, compared to directly interpreting FF feature vectors, and FF key-value parameters serve as a strong baseline in modern interpretability research.",
      "arxiv_url": "https://openreview.net/forum?id=2TKEGTfQBd",
      "pdf_url": "https://openreview.net/pdf/adec8dec1b058f333782bd49d8e0eb087317ba82.pdf",
      "primary_category": "interpretability, key-value memories, sparse autoencoders",
      "categories": [
        "interpretability",
        "key-value memories",
        "sparse autoencoders"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IQ513IX1G5",
      "title": "Beyond Oracle: Verifier-Supervision for Instruction Hierarchy in Reasoning and Instruction-Tuned LLMs",
      "authors": [
        "Sian-Yao Huang",
        "Li-Hsien Chang",
        "Che-Yu Lin",
        "Cheng-Lin Yang"
      ],
      "abstract": "Large language models (LLMs) are often prompted with multi-level directives, such as system instructions and user queries, that imply a hierarchy of authority. Yet models frequently fail to enforce this structure, especially in multi-step reasoning where errors propagate across intermediate steps. Existing methods rely on oracle completions but lack verifiable reward signals or intermediate traces, limiting their applicability. We introduce a unified supervision framework that embeds programmatically verifiable checkers into synthesized instruction-conflict instances. Each instance pairs a compliance directive with a conflicting one, along with an executable verifier that deterministically checks output adherence. This enables alignment without oracle labels or reasoning traces, supporting both instruction-tuned and reasoning models. The framework is instantiated via a synthesis pipeline that includes unit-test–based validation, LLM-assisted repair, and a probabilistic analysis of cleaning reliability. Fine-tuning on the resulting data improves instruction hierarchy adherence and boosts safety robustness, generalizing to adversarial safety benchmarks without task-specific supervision. This highlights verifiable supervision as a scalable foundation for robust alignment. All code, dataset, and verifier pipeline are publicly available at: https://github.com/cycraft-corp/BeyondOracle.",
      "arxiv_url": "https://openreview.net/forum?id=IQ513IX1G5",
      "pdf_url": "https://openreview.net/pdf/d576f2caa769be21da3d38e86e6ef480862f10ec.pdf",
      "primary_category": "instruction hierarchy, verifiable supervision, reasoning LLMs",
      "categories": [
        "instruction hierarchy",
        "verifiable supervision",
        "reasoning LLMs",
        "instruction-tuned LLMs",
        "programmatic verification",
        "oracle-free alignment",
        "safety generalization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vIaNnnQxcl",
      "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment",
      "authors": [
        "Wonje Jeung",
        "Sangyeon Yoon",
        "Minsuk Kahng",
        "Albert No"
      ],
      "abstract": "Large Reasoning Models (LRMs) have become powerful tools for complex problem solving, but their structured reasoning pathways can lead to unsafe outputs when exposed to harmful prompts. Existing safety alignment methods reduce harmful outputs but can degrade reasoning depth, leading to significant trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at the start of their reasoning, in response to harmful prompts, while leaving the rest of the reasoning process unsupervised. Empirical results across multiple  benchmarks indicate that SAFEPATH effectively reduces harmful outputs while maintaining reasoning performance. Specifically, SAFEPATH reduces harmful responses by up to 90.0\\% and blocks 83.3\\% of jailbreak attempts in the DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot variant that requires no fine-tuning. In addition, we provide a comprehensive analysis of how existing methods in LLMs generalize, or fail, when applied to reasoning-centric models, revealing critical gaps and new directions for safer AI.",
      "arxiv_url": "https://openreview.net/forum?id=vIaNnnQxcl",
      "pdf_url": "https://openreview.net/pdf/bdb50a5d4b6df8189537be5d04e1a9bdd1dd5249.pdf",
      "primary_category": "Large Reasoning Models (LRMs), Chain-of-Thought Reasoning, Safety Alignment",
      "categories": [
        "Large Reasoning Models (LRMs)",
        "Chain-of-Thought Reasoning",
        "Safety Alignment",
        "Zero-shot Alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "5XoqKCmkS7",
      "title": "Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models",
      "authors": [
        "Tae-Young Lee",
        "Juwon Seo",
        "Jong Hwan Ko",
        "Gyeong-Moon Park"
      ],
      "abstract": "Recent advances in diffusion models have enabled high-quality synthesis of specific subjects, such as identities or objects. This capability, while unlocking new possibilities in content creation, also introduces significant privacy risks, as personalization techniques can be misused by malicious users to generate unauthorized images. Although several studies have attempted to counter this by generating adversarially perturbed samples designed to disrupt personalization, they rely on unrealistic assumptions and become ineffective in the presence of even a few clean images or under simple image transformations. To address these challenges, we shift the protection target from the images to the diffusion model itself to hinder the personalization of specific subjects, through our novel framework called $\\textbf{A}$nti-$\\textbf{P}$ersonalized $\\textbf{D}$iffusion $\\textbf{M}$odels ($\\textbf{APDM}$). We first provide a theoretical analysis demonstrating that a naive approach of existing loss functions to diffusion models is inherently incapable of ensuring convergence for robust anti-personalization. Motivated by this finding, we introduce Direct Protective Optimization (DPO), a novel loss function that effectively disrupts subject personalization in the target model without compromising generative quality. Moreover, we propose a new dual-path optimization strategy, coined Learning to Protect (L2P). By alternating between personalization and protection paths, L2P simulates future personalization trajectories and adaptively reinforces protection at each step.\nExperimental results demonstrate that our framework outperforms existing methods, achieving state-of-the-art performance in preventing unauthorized personalization.\nThe code is available at https://github.com/KU-VGI/APDM.",
      "arxiv_url": "https://openreview.net/forum?id=5XoqKCmkS7",
      "pdf_url": "https://openreview.net/pdf/af600c6e27691bf86fed8e0f1dca1bb8a444a2f5.pdf",
      "primary_category": "text-to-image diffusion models, personalization, privacy",
      "categories": [
        "text-to-image diffusion models",
        "personalization",
        "privacy"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2g5cJqX15Y",
      "title": "Large language models can learn and generalize steganographic chain-of-thought under process supervision",
      "authors": [
        "Robert McCarthy",
        "Joey SKAF",
        "Luis Ibanez-Lissen",
        "Vasil Georgiev",
        "Connor Watts",
        "Hannes Whittingham",
        "Lorena Gonzalez-Manzano",
        "Cameron Tice",
        "Edward James Young",
        "Puria Radmard",
        "David Lindner"
      ],
      "abstract": "Chain-of-thought (CoT) reasoning not only enhances large language model performance but also provides critical insights into decision-making processes, marking it as a useful tool for monitoring model intent and planning. By proactively preventing models from acting on CoT indicating misaligned or harmful intent, CoT monitoring can be used to reduce risks associated with deploying models. However, developers may be incentivized to train away the appearance of harmful intent from CoT traces, by either customer preferences or regulatory requirements. However, recent works have shown that banning the mention of a specific example of reward hacking causes obfuscation of the undesired reasoning traces but the persistence of the undesired behavior, threatening the reliability of CoT monitoring. However, obfuscation of reasoning can be due to its internalization to latent space computation, or its encoding within the CoT. We provide an extension to these results with regard to the ability of models to learn a specific type of obfuscated reasoning: steganography. First, we show that penalizing the use of specific strings within load-bearing reasoning traces causes models to substitute alternative strings. Crucially, this does not alter the underlying method by which the model performs the task, demonstrating that the model can learn to steganographically encode its reasoning. This is an example of models learning to encode their reasoning. We further demonstrate that models can generalize an encoding scheme. When the penalized strings belong to an overarching class, the model learns not only to substitute strings seen in training, but also develops a general encoding scheme for all members of the class which it can apply to held-out testing strings.",
      "arxiv_url": "https://openreview.net/forum?id=2g5cJqX15Y",
      "pdf_url": "https://openreview.net/pdf/a4499bf01db396810920b93ba85dec92e6ac6e43.pdf",
      "primary_category": "AI Safety, AI Control, Steganography",
      "categories": [
        "AI Safety",
        "AI Control",
        "Steganography",
        "Encoded Reasoning",
        "Chain-of-Thought",
        "Reinforcement Learning",
        "LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "jHWCeU39Ft",
      "title": "MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining",
      "authors": [
        "Zhixun Chen",
        "Ping Guo",
        "Wenhan Han",
        "Yifan Zhang",
        "BINBINLIU",
        "Haobin Lin",
        "Fengze Liu",
        "Yan Zhao",
        "Bingni Zhang",
        "Taifeng Wang",
        "Yin Zheng",
        "Trevor Cohn",
        "Meng Fang"
      ],
      "abstract": "Data quality is a critical driver of large language model performance, yet existing model-based selection methods focus almost exclusively on English, neglecting other languages that are essential in the training mix for multilingual LLMs. We introduce MuRating, a scalable framework that transfers high-quality English data-quality signals into a multilingual autorater, capable of handling 17 languages. MuRating aggregates multiple English autoraters via pairwise comparisons to learn unified document quality scores, then projects these judgments through translation to train a multilingual evaluator on monolingual, cross-lingual, and parallel text pairs. Applied to web data, MuRating selects balanced subsets of English and multilingual content to pretrain LLaMA-architecture models of 1.2B and 7B parameters. Compared to strong baselines, including QuRater, FineWeb2-HQ, AskLLM, DCLM, our approach increases average accuracy on both English benchmarks and multilingual evaluations. Extensive analyses further validate that pairwise training provides greater stability and robustness than pointwise scoring, underscoring the effectiveness of MuRating as a general multilingual data-selection framework.",
      "arxiv_url": "https://openreview.net/forum?id=jHWCeU39Ft",
      "pdf_url": "https://openreview.net/pdf/a40675df6170e80abf10600ec2e5716efe6e4785.pdf",
      "primary_category": "Natural Language Processing",
      "categories": [
        "Natural Language Processing"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XNo4yS9n1k",
      "title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models",
      "authors": [
        "Zekai Zhao",
        "Qi Liu",
        "Kun Zhou",
        "Zihan Liu",
        "Yifei Shao",
        "Zhiting Hu",
        "Biwei Huang"
      ],
      "abstract": "Despite the remarkable reasoning performance, eliciting the long chain-of-thought(CoT) ability in large language models(LLMs) typically requires costly reinforcement learning or supervised fine-tuning on high-quality distilled data. We investigate the internal mechanisms behind this capability and show that a small set of high-impact activations in the last few layers, greatly govern the long-form reasoning attributes, e.g. output length and self-reflection. Through simply amplifying these activations and adding ``wait'' tokens, the long CoT ability can be invoked without training, leading to significantly increased self-reflection rate and accuracy. In addition, we also find that the activation changes follow predictable trajectories, i.e. a sharp rise after special tokens and a subsequent exponential decay. Based on these insights, we introduce a general training-free activation control technique. It utilizes a few contrastive examples to identify the relevant activations, and then incorporates simple analytic functions to adjust their values at inference time to elicit long CoTs. Extensive experiments have verified the effectiveness of our methods in efficiently eliciting the long CoT ability of LLMs and improving the performance. Besides, we further propose a parameter-efficient fine-tuning method that trains only the last-layer activation amplification module and a few LoRA layers, outperforming LoRA on reasoning benchmarks with much fewer parameters. Our code and data will be fully public released.",
      "arxiv_url": "https://openreview.net/forum?id=XNo4yS9n1k",
      "pdf_url": "https://openreview.net/pdf/2b62626f959a636707dfb61efe094e4dcd7bd32f.pdf",
      "primary_category": "Large Language Models, Long Chain of Thoughts",
      "categories": [
        "Large Language Models",
        "Long Chain of Thoughts"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "jSgCM0uZn3",
      "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play",
      "authors": [
        "Ran Xu",
        "Yuchen Zhuang",
        "Zihan Dong",
        "Ruiyu Wang",
        "Yue Yu",
        "Joyce C. Ho",
        "Linjun Zhang",
        "Haoyu Wang",
        "Wenqi Shi",
        "Carl Yang"
      ],
      "abstract": "Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the giant DeepSeek-V3 model using less than 5% of iits parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9× more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks.",
      "arxiv_url": "https://openreview.net/forum?id=jSgCM0uZn3",
      "pdf_url": "https://openreview.net/pdf/d73f4b0dc08ba840cfc15d8e7e9be6ad4a9b52ce.pdf",
      "primary_category": "large language model, retrieval augmented generation, self-play",
      "categories": [
        "large language model",
        "retrieval augmented generation",
        "self-play"
      ],
      "tags": [
        "LLM",
        "Search Agent"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "baNBqpzvMT",
      "title": "Mixing Expert Knowledge: Bring Human Thoughts Back To the Game of Go",
      "authors": [
        "Yichuan Ma",
        "Linyang Li",
        "Yongkang Chen",
        "Peiji Li",
        "Jiasheng Ye",
        "Qipeng Guo",
        "Dahua Lin",
        "Kai Chen"
      ],
      "abstract": "Large language models (LLMs) have demonstrated exceptional performance in reasoning tasks such as mathematics and coding, matching or surpassing human capabilities. However, these impressive reasoning abilities face significant challenges in specialized domains. Taking Go as an example, although AlphaGo has established the high performance ceiling of AI systems in Go, mainstream LLMs still struggle to reach even beginner-level proficiency, let alone perform natural language reasoning. This performance gap between general-purpose LLMs and domain experts is significantly limiting the application of LLMs on a wider range of domain-specific tasks. In this work, we aim to bridge the divide between LLMs' general reasoning capabilities and expert knowledge in domain-specific tasks. We perform mixed fine-tuning with structured Go expertise and general long Chain-of-Thought (CoT) reasoning data as a cold start, followed by reinforcement learning to integrate expert knowledge in Go with general reasoning capabilities. Through this methodology, we present LoGos, a powerful LLM that not only maintains outstanding general reasoning abilities, but also conducts Go gameplay in natural language, demonstrating effective strategic  reasoning and accurate next-move prediction. LoGos achieves performance comparable to human professional players, substantially surpassing all existing LLMs. Through this work, we aim to contribute insights on applying general LLM reasoning capabilities to specialized domains. We will release the first large-scale Go dataset for LLM training, the first LLM Go evaluation benchmark, and the first general LLM that reaches human expert-level performance in Go.",
      "arxiv_url": "https://openreview.net/forum?id=baNBqpzvMT",
      "pdf_url": "https://openreview.net/pdf/37b8d16d2219519662c61ad8b2c119f9a01e7d24.pdf",
      "primary_category": "LLM reasoning, Reinforcement Learning, Go",
      "categories": [
        "LLM reasoning",
        "Reinforcement Learning",
        "Go"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3JoQTGhUzz",
      "title": "IPAD: Inverse Prompt for AI Detection - A Robust and Interpretable LLM-Generated Text Detector",
      "authors": [
        "Zheng CHEN",
        "Yushi Feng",
        "Jisheng Dang",
        "Changyang He",
        "Yue Deng",
        "Hongxi Pu",
        "Haoxuan Li",
        "Bo Li"
      ],
      "abstract": "Large Language Models (LLMs) have attained human-level fluency in text generation, which complicates the distinguishing between human-written and LLM generated texts. This increases the risk of misuse and highlights the need for reliable detectors. Yet, existing detectors exhibit poor robustness on out-of-distribution (OOD) data and attacked data, which is critical for real-world scenarios. Also, they struggle to provide interpretable evidence to support their decisions, thus undermining reliability. In light of these challenges, we propose IPAD (Inverse Prompt for AI Detection), a novel framework consisting of a Prompt Inverter that identifies predicted prompts that could have generated the input text, and two Distinguishers that examine the probability that the input texts align with the predicted prompts. Empirical evaluations demonstrate that IPAD outperforms the strongest baselines by 9.05% (Average Recall) on in-distribution data, 12.93% (AUROC) on out-of-distribution (OOD) data, and 5.48% (AUROC) on attacked data. IPAD also performs robust on structured datasets. Furthermore, an interpretability assessment is conducted to illustrate that IPAD enhances the AI detection trustworthiness by allowing users to directly examine the decision-making evidence, which provides interpretable support for its state-of-the-art detection results.",
      "arxiv_url": "https://openreview.net/forum?id=3JoQTGhUzz",
      "pdf_url": "https://openreview.net/pdf/334c4ba4801e17d429da93ef49f6544171555005.pdf",
      "primary_category": "AI Detection, Prompt Inversion, Large Language Models",
      "categories": [
        "AI Detection",
        "Prompt Inversion",
        "Large Language Models",
        "Explainability",
        "AI Safety"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tmtUA2X57D",
      "title": "Graph-Theoretic Insights into Bayesian Personalized Ranking for Recommendation",
      "authors": [
        "Kai Zheng",
        "Jianxin Wang",
        "Jinhui Xu"
      ],
      "abstract": "Graph self-supervised learning (GSL) is essential for processing graph-structured data, reducing the need for manual labeling. Traditionally, this paradigm has extensively utilized Bayesian Personalized Ranking (BPR) as its primary loss function. Despite its widespread application, the theoretical analysis of its node relations evaluation have remained largely unexplored. This paper employs recent advancements in latent hyperbolic geometry to deepen our understanding of node relationships from a graph-theoretical perspective. We analyze BPR’s limitations, particularly its reliance on local connectivity through 2-hop paths, which overlooks global connectivity and the broader topological structure. To address these shortcomings, we purpose a novel loss function, BPR+, designed to encompass even-hop paths and better capture global connectivity and topological nuances. This approach facilitates a more detailed measurement of user-item relationships and improves the granularity of relationship assessments. We validate BPR+ through extensive empirical testing across five real-world datasets and demonstrate its efficacy in refining graph self-supervised learning frameworks. Additionally, we explore the application of BPR+ in drug repositioning, highlighting its potential to support pharmaceutical research and development. Our findings not only illuminate the success factors of previous methodologies but also offer new theoretical insights into this learning paradigm.",
      "arxiv_url": "https://openreview.net/forum?id=tmtUA2X57D",
      "pdf_url": "https://openreview.net/pdf/4e8cee866548ecb0cc0907a7389284254a1d6f2f.pdf",
      "primary_category": "Graph self-supervised learning, Bayesian Personalized Ranking, latent hyperbolic geometry",
      "categories": [
        "Graph self-supervised learning",
        "Bayesian Personalized Ranking",
        "latent hyperbolic geometry",
        "loss function",
        "Network geometry"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "rRHuBZdDfY",
      "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving",
      "authors": [
        "Shihan Dou",
        "Ming Zhang",
        "Chenhao Huang",
        "Jiayi Chen",
        "Feng Chen",
        "Shichun Liu",
        "Yan Liu",
        "Chenxiao Liu",
        "CHENG ZHONG",
        "Zongzhang Zhang",
        "Tao Gui",
        "Chao Xin",
        "Wei Chengzhi",
        "Lin Yan",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large language models (LLMs) on their learning capability and efficiency in challenging tasks, a critical, yet underexplored aspect of model potential. EvaLearn contains 648 challenging problems across six task types, grouped into 182 sequences, each sequence dedicated to one task type. Diverging from most existing benchmarks that evaluate models in parallel, EvaLearn requires models to solve problems sequentially, allowing them to leverage the experience gained from previous solutions. EvaLearn provides five comprehensive automated metrics to evaluate models and quantify their learning capability and efficiency. We extensively benchmark nine frontier models and observe varied performance profiles: some models, such as Claude-3.7-sonnet, start with moderate initial performance but exhibit strong learning ability, while some models struggle to benefit from experience and may even show negative transfer. Moreover, we investigate model performance under two learning settings and find that instance-level rubrics and teacher-model feedback further facilitate model learning. Importantly, we observe that current LLMs with stronger static abilities do not show a clear advantage in learning capability across all tasks, highlighting that EvaLearn evaluates a new dimension of model performance. We hope EvaLearn provides a novel evaluation perspective for assessing LLM potential and understanding the gap between models and human capabilities, promoting the development of deeper and more dynamic evaluation approaches. All datasets, the automatic evaluation framework, and the results studied in this paper are available in the supplementary materials.",
      "arxiv_url": "https://openreview.net/forum?id=rRHuBZdDfY",
      "pdf_url": "https://openreview.net/pdf/96092d8fb9b3de940f2a0ee41fd64e7f629e43cb.pdf",
      "primary_category": "benchmark, LLMs, evaluation",
      "categories": [
        "benchmark",
        "LLMs",
        "evaluation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "dbq6NZfi3c",
      "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions",
      "authors": [
        "Xiaorui Wu",
        "Fei Li",
        "Xiaofeng Mao",
        "Xin Zhang",
        "Li Zheng",
        "Yuxiang Peng",
        "Chong Teng",
        "Donghong Ji",
        "Zhuang Li"
      ],
      "abstract": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary LLM refusals due to conservative safety alignment, significantly impairing user experience. Collecting such instructions is crucial for evaluating and mitigating over-refusals, but existing instruction curation methods, like manual creation or instruction rewriting, either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts. To address these limitations, we introduce EVOREFUSE, a prompt optimization approach that generates diverse pseudo-malicious instructions consistently eliciting confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm exploring the instruction space in more diverse directions than existing methods via mutation strategies and recombination, and iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability. Using EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582 pseudo-malicious instructions that outperforms the next-best benchmark with 85.34% higher average refusal triggering rate across 9 LLMs without a safety-prior system prompt, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores; and EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with responses for supervised and preference-based alignment training. With supervised fine-tuning on EVOREFUSE-ALIGN, LLAMA3.1-8B-INSTRUCT achieves up to 29.85% fewer over-refusals than models trained on the second-best alignment dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context. Our code and datasets are available at https://github.com/FishT0ucher/EVOREFUSE .",
      "arxiv_url": "https://openreview.net/forum?id=dbq6NZfi3c",
      "pdf_url": "https://openreview.net/pdf/b17358ed78b9c539e109975756d1b8b9045b242f.pdf",
      "primary_category": "Prompt Optimization, LLM Over-Refusal, Data Synthesis",
      "categories": [
        "Prompt Optimization",
        "LLM Over-Refusal",
        "Data Synthesis"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "G2kMroO9UV",
      "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents",
      "authors": [
        "Hyungjoo Chae",
        "Sunghwan Kim",
        "Junhee Cho",
        "Seungone Kim",
        "Seungjun Moon",
        "Gyeom Hwangbo",
        "Dongha Lim",
        "Minjin Kim",
        "Yeonjun Hwang",
        "Minju Gwak",
        "Dongwook Choi",
        "Minseok Kang",
        "Gwanhoon Im",
        "ByeongUng Cho",
        "Hyojun Kim",
        "Jun Hee Han",
        "Taeyoon Kwon",
        "Minju Kim",
        "Beong-woo Kwak",
        "Dongjin Kang",
        "Jinyoung Yeo"
      ],
      "abstract": "Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks.\nYet, specialized reward models for web navigation that can be utilized during both training and test-time have been absent until now. Despite the importance of speed and cost-effectiveness, prior works have utilized MLLMs as reward models, which poses significant constraints for real-world deployment. To address this, in this work, we propose the first process reward model (PRM) called Web-Shepherd which could assess web navigation trajectories in a step-level. To achieve this, we first construct the WebPRM Collection, a large-scale dataset with 40K step-level preference pairs and annotated checklists spanning diverse domains and difficulty levels. Next, we also introduce the WebRewardBench, the first meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe that our Web-Shepherd achieves about 30 points better accuracy compared to using GPT-4o on WebRewardBench. \nFurthermore, when testing on WebArena-lite by using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve 10.9 points better performance, in 10x less cost compared to using GPT-4o-mini as the verifier. \nOur model, dataset, and code are publicly available at https://github.com/kyle8581/Web-Shepherd.",
      "arxiv_url": "https://openreview.net/forum?id=G2kMroO9UV",
      "pdf_url": "https://openreview.net/pdf/db46564195dad40b9b174514d7d03b0336d2a8eb.pdf",
      "primary_category": "Web Agent, Reward Model, LLM",
      "categories": [
        "Web Agent",
        "Reward Model",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "b7bOWd3kUL",
      "title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning",
      "authors": [
        "Honglin Lin",
        "Qizhi Pei",
        "Zhuoshi Pan",
        "Yu Li",
        "Xin Gao",
        "Juntao Li",
        "Conghui He",
        "Lijun Wu"
      ],
      "abstract": "Reasoning capability is pivotal for Large Language Models (LLMs) to solve complex tasks, yet achieving reliable and scalable reasoning remains challenging. While Chain-of-Thought (CoT) prompting has become a mainstream approach, existing methods often suffer from uncontrolled generation, insufficient quality, and limited diversity in reasoning paths. \nRecent efforts leverage code to enhance CoT by grounding reasoning in executable steps, but such methods are typically constrained to predefined mathematical problems, hindering scalability and generalizability. \nIn this work, we propose \\texttt{Caco} (Code-Assisted Chain-of-ThOught), a novel framework that automates the synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning data through code-driven augmentation. Unlike prior work, \\texttt{Caco} first fine-tunes a code-based CoT generator on existing math and programming solutions in a unified code format, then scales the data generation to a large amount of diverse reasoning traces. Crucially, we introduce automated validation via code execution and rule-based filtering to ensure logical correctness and structural diversity, followed by reverse-engineering filtered outputs into natural language instructions and language CoTs to enrich task adaptability. This closed-loop process enables fully automated, scalable synthesis of reasoning data with guaranteed executability. \nExperiments on our created \\texttt{Caco}-1.3M dataset demonstrate that \\texttt{Caco}-trained models achieve strong competitive performance on mathematical reasoning benchmarks, outperforming existing strong baselines. Further analysis reveals that \\texttt{Caco}’s code-anchored verification and instruction diversity contribute to superior generalization across unseen tasks. Our work establishes a paradigm for building self-sustaining, trustworthy reasoning systems without human intervention.",
      "arxiv_url": "https://openreview.net/forum?id=b7bOWd3kUL",
      "pdf_url": "https://openreview.net/pdf/b5f91855f5922ea89b810ed1560821c78f774fff.pdf",
      "primary_category": "Data Synthesis, Reasoning",
      "categories": [
        "Data Synthesis",
        "Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "19ygs48nOa",
      "title": "How do Transformers Learn Implicit Reasoning?",
      "authors": [
        "Jiaran Ye",
        "Zijun Yao",
        "Zhidian Huang",
        "Liangming Pan",
        "Jinxin Liu",
        "Yushi Bai",
        "Amy Xin",
        "Liu Weichuan",
        "Xiaoyin Che",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "Recent work suggests that large language models (LLMs) can perform multi-hop reasoning implicitly---producing correct answers without explicitly verbalizing intermediate steps---but the underlying mechanisms remain poorly understood.\nIn this paper, we study how such implicit reasoning emerges by training transformers from scratch in a controlled symbolic environment.\nOur analysis reveals a three-stage developmental trajectory: early memorization, followed by in-distribution generalization, and eventually cross-distribution generalization.\nWe find that training with atomic triples is not necessary but accelerates learning, and that second-hop generalization relies on query-level exposure to specific compositional structures.\nTo interpret these behaviors, we introduce two diagnostic tools: cross-query semantic patching, which identifies semantically reusable intermediate representations, and a cosine-based representational lens, which reveals that successful reasoning correlates with the cosine-base clustering in hidden space.\nThis clustering phenomenon in turn provides a coherent explanation for the behavioral dynamics observed across training, linking representational structure to reasoning capability. \nThese findings provide new insights into the interpretability of implicit multi-hop reasoning in LLMs, helping to clarify how complex reasoning processes unfold internally and offering pathways to enhance the transparency of such models.",
      "arxiv_url": "https://openreview.net/forum?id=19ygs48nOa",
      "pdf_url": "https://openreview.net/pdf/ad38d1ecc76e644507fb0bca76d2143a06260e52.pdf",
      "primary_category": "Implicit Reasoning, Multi-hop Inference, Transformer Models",
      "categories": [
        "Implicit Reasoning",
        "Multi-hop Inference",
        "Transformer Models",
        "Interpretability"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yiSoT2pHfk",
      "title": "CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections",
      "authors": [
        "Keuntae Kim",
        "Eunhye Jeong",
        "Sehyeon Lee",
        "Seohee Yoon",
        "Yong Suk Choi"
      ],
      "abstract": "Recent advances in enhancing the reasoning ability of Large Language Models (LLMs) have been remarkably successful. LLMs trained with Reinforcement Learning (RL) for reasoning demonstrate strong performance in challenging tasks such as mathematics and coding, even with relatively small model sizes. However, despite these impressive improvements in task accuracy, the assessment of creativity in LLM generations has been largely overlooked in reasoning tasks, in contrast to writing tasks. The lack of research on creativity assessment in reasoning primarily stems from two challenges: (1) the difficulty of defining the range of creativity, and (2) the necessity of human evaluation in the assessment process. To address these challenges, we propose CLAWS, a novel method that defines and classifies mathematical solutions into Typical, Creative, and Hallucinated categories without human evaluation, by leveraging attention weights across prompt sections and output. CLAWS outperforms five existing white-box detection methods—Perplexity, Logit Entropy, Window Entropy, Hidden Score, and Attention Score—on five 7–8B math RL models (DeepSeek, Qwen, Mathstral, OpenMath2, and Oreal). We validate CLAWS on 4,545 math problems collected from 181 math contests (A(J)HSME, AMC, AIME). Our code is available at https://github.com/kkt94/CLAWS.",
      "arxiv_url": "https://openreview.net/forum?id=yiSoT2pHfk",
      "pdf_url": "https://openreview.net/pdf/1def4784a6e1f2045d4c683ea291426ad146a305.pdf",
      "primary_category": "llm, reasoning, math",
      "categories": [
        "llm",
        "reasoning",
        "math",
        "creativity",
        "hallucination"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "N2bLuwofZ0",
      "title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics",
      "authors": [
        "Dongyoung Kim",
        "Sumin Park",
        "Huiwon Jang",
        "Jinwoo Shin",
        "Jaehyung Kim",
        "Younggyo Seo"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and movement reasoning.",
      "arxiv_url": "https://openreview.net/forum?id=N2bLuwofZ0",
      "pdf_url": "https://openreview.net/pdf/28f2bd82b713f7bb9565dbe9321b8b6fb4782dc3.pdf",
      "primary_category": "LLM, Reasoning, RL",
      "categories": [
        "LLM",
        "Reasoning",
        "RL",
        "Robot domain"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Mc0eJHZhW5",
      "title": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models",
      "authors": [
        "Li Tenghui",
        "Guoxu Zhou",
        "Xuyang ZHAO",
        "Yuning Qiu",
        "Qibin Zhao"
      ],
      "abstract": "As the length of input text increases, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices.\n  Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs.\n  In this work, Low Rank Query and Key attention (LRQK) is introduced, a two-stage framework that jointly decomposes full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then employs these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step.\n  By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism where only missing full-precision KV pairs are transferred, thereby preserving exact attention outputs while reducing CPU-GPU data movement.\n  Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal accuracy loss. Our code is available at \\url{https://github.com/tenghuilee/LRQK}.",
      "arxiv_url": "https://openreview.net/forum?id=Mc0eJHZhW5",
      "pdf_url": "https://openreview.net/pdf/782a279950afb7a019653682ea8ce33984f7b7c4.pdf",
      "primary_category": "LLM, KV cache, Low rank decomposition",
      "categories": [
        "LLM",
        "KV cache",
        "Low rank decomposition",
        "Long context inference"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "5eZ0iykpDU",
      "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning",
      "authors": [
        "Jian Yao",
        "Ran Cheng",
        "Xingyu Wu",
        "Jibin Wu",
        "KC Tan"
      ],
      "abstract": "The reasoning capabilities of large language models (LLMs) have advanced rapidly, particularly following the release of DeepSeek-R1, which has inspired a surge of research into data quality and reinforcement learning (RL) algorithms. Despite the pivotal role diversity plays in RL, its influence on LLM reasoning remains largely underexplored. To bridge this gap, this work presents a systematic investigation into the impact of diversity in RL-based training for LLM reasoning, and proposes a novel diversity-aware policy optimization method. Across evaluations on 12 LLMs, we observe a strong positive correlation between the solution diversity and potential@k (a novel metric quantifying an LLM’s reasoning potential) in high-performing models. This finding motivates our method to explicitly promote diversity during RL training. Specifically,  we design a token-level diversity and reformulate it into a practical objective, then we selectively apply it to positive samples. Integrated into the R1-zero training framework, our method achieves a 3.5\\% average improvement across four mathematical reasoning benchmarks, while generating more diverse and robust solutions.",
      "arxiv_url": "https://openreview.net/forum?id=5eZ0iykpDU",
      "pdf_url": "https://openreview.net/pdf/c884fab9a8271939813690f5af1e6cebe20f7a2a.pdf",
      "primary_category": "LLM, Policy Optimization, Diversity",
      "categories": [
        "LLM",
        "Policy Optimization",
        "Diversity"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "p37Kd7EQhy",
      "title": "Prior Forgetting and In-Context Overfitting",
      "authors": [
        "Sungyoon Lee"
      ],
      "abstract": "In-context learning (ICL) is one of the key capabilities contributing to the great success of LLMs. At test time, ICL is known to operate in the two modes: task recognition and task learning. In this paper, we investigate the emergence and dynamics of the two modes of ICL during pretraining. To provide an analytical understanding of the learning dynamics of the ICL abilities, we investigate the in-context random linear regression problem with a simple linear-attention-based transformer, and define and disentangle the strengths of the task recognition and task learning abilities stored in the transformer model’s parameters. We show that, during the pretraining phase, the model first learns the task learning and the task recognition abilities together in the beginning, but it (a) gradually forgets the task recognition ability to recall the priorly learned tasks and (b) relies more on the given context in the later phase, which we call (a) \\textit{prior forgetting} and (b) \\textit{in-context overfitting}, respectively.",
      "arxiv_url": "https://openreview.net/forum?id=p37Kd7EQhy",
      "pdf_url": "https://openreview.net/pdf/6716c3a29f99dd908caabe8683887a219481855e.pdf",
      "primary_category": "in-context learning, transformer, task recognition",
      "categories": [
        "in-context learning",
        "transformer",
        "task recognition",
        "task learning",
        "pretraining",
        "prior forgetting",
        "in-context overfitting"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0Zri6HSYaK",
      "title": "More Than Just Functional: LLM-as-a-Critique for Efficient Code Generation",
      "authors": [
        "Derui Zhu",
        "Dingfan Chen",
        "jinfu chen",
        "Jens Grossklags",
        "Walter Pretschner",
        "Weiyi Shang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable progress in generating functional code, leading to numerous AI-based coding program tools. However, their reliance on the perplexity objective during both training and inference primarily emphasizes functionality, often at the expense of efficiency—an essential consideration for real-world coding tasks. Perhaps interestingly, we observed that well-trained LLMs inherently possess knowledge about code efficiency, but this potential remains underutilized with standard decoding approaches. To address this, we design strategic prompts to activate the model’s embedded efficiency understanding, effectively using LLMs as \\textit{efficiency critiques} to guide code generation toward higher efficiency without sacrificing—and sometimes even improving—functionality, all without the need for costly real code execution. Extensive experiments on benchmark datasets (EffiBench, HumanEval+) across multiple representative code models demonstrate up to a 70.6\\% reduction in average execution time and a 13.6\\% decrease in maximum memory usage, highlighting the computational efficiency and practicality of our approach compared to existing alternatives.",
      "arxiv_url": "https://openreview.net/forum?id=0Zri6HSYaK",
      "pdf_url": "https://openreview.net/pdf/078f13992647a4da32aeb346041206919cbc6ee0.pdf",
      "primary_category": "Code Generation, Performance, Software Engineering",
      "categories": [
        "Code Generation",
        "Performance",
        "Software Engineering",
        "AI4SE"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "60I6TzuHOb",
      "title": "Preference-driven Knowledge Distillation for Few-shot Node Classification",
      "authors": [
        "Xing Wei",
        "Chunchun Chen",
        "Rui Fan",
        "Xiaofeng Cao",
        "Sourav Medya",
        "Wei Ye"
      ],
      "abstract": "Graph neural networks (GNNs) can efficiently process text-attributed graphs (TAGs) due to their message-passing mechanisms, but their training heavily relies on the human-annotated labels. Moreover, the complex and diverse local topologies of nodes of real-world TAGs make it challenging for a single mechanism to handle. Large language models (LLMs) perform well in zero-/few-shot learning on TAGs but suffer from a scalability challenge. Therefore, we propose a preference-driven knowledge distillation (PKD) framework to synergize the complementary strengths of LLMs and various GNNs for few-shot node classification. Specifically, we develop a GNN-preference-driven node selector that effectively promotes prediction distillation from LLMs to teacher GNNs. To further tackle nodes' intricate local topologies, we develop a node-preference-driven GNN selector that identifies the most suitable teacher GNN for each node, thereby facilitating tailored knowledge distillation from teacher GNNs to the student GNN. Extensive experiments validate the efficacy of our proposed framework in few-shot node classification on real-world TAGs.\nOur code can be available at <https://github.com/GEEX-Weixing/PKD>.",
      "arxiv_url": "https://openreview.net/forum?id=60I6TzuHOb",
      "pdf_url": "https://openreview.net/pdf/4781f3cd2c992afccba892ad3e58c12eb71d2c7e.pdf",
      "primary_category": "Preference-driven, Knowledge distillation, Large language models",
      "categories": [
        "Preference-driven",
        "Knowledge distillation",
        "Large language models",
        "Few-shot node classification"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Huw15LqglI",
      "title": "Born a Transformer -- Always a Transformer? On the Effect of Pretraining on Architectural Abilities",
      "authors": [
        "Mayank Jobanputra",
        "Yana Veitsman",
        "Yash Sarrof",
        "Aleksandra Bakalova",
        "Vera Demberg",
        "Ellie Pavlick",
        "Michael Hahn"
      ],
      "abstract": "Transformers have theoretical limitations in modeling certain sequence-to-sequence tasks, yet it remains largely unclear if these limitations play a role in large-scale pretrained LLMs, or whether LLMs might effectively overcome these constraints in practice due to the scale of both the models themselves and their pretraining data. We explore how these architectural constraints manifest after pretraining by studying a family of *retrieval* and *copying* tasks inspired by Liu et al. [2024a]. We use a recently proposed framework for studying length generalization [Huang et al., 2025] to provide guarantees for each of our settings. Empirically, we observe an *induction-versus-anti-induction asymmetry*, where pretrained models are better at retrieving tokens to the right (induction) rather than the left (anti-induction) of a query token. This asymmetry disappears upon targeted fine-tuning if length-generalization is guaranteed by theory. Mechanistic analysis reveals that this asymmetry is connected to the differences in the strength of induction versus anti-induction circuits within pretrained transformers. We validate our findings through practical experiments on real-world tasks demonstrating reliability risks. Our results highlight that pretraining selectively enhances certain transformer capabilities, but does not overcome fundamental length-generalization limits.",
      "arxiv_url": "https://openreview.net/forum?id=Huw15LqglI",
      "pdf_url": "https://openreview.net/pdf/6ad44fb7f8e30a59cb5eed857f25dbc79c22d66f.pdf",
      "primary_category": "transformers, length generalization, empirical analysis",
      "categories": [
        "transformers",
        "length generalization",
        "empirical analysis",
        "explainability"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "D2XdJf1tXW",
      "title": "Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers",
      "authors": [
        "Youmin Ko",
        "Sung Jong Seo",
        "Hyunjoon Kim"
      ],
      "abstract": "Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.",
      "arxiv_url": "https://openreview.net/forum?id=D2XdJf1tXW",
      "pdf_url": "https://openreview.net/pdf/52ab19e0ce1e465d8756bd720c363a39a11a8fb5.pdf",
      "primary_category": "retieval-augmented generation, RAG, multi-hop question answering",
      "categories": [
        "retieval-augmented generation",
        "RAG",
        "multi-hop question answering",
        "contrasting layers",
        "question augmentation"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3P3PL7aCXM",
      "title": "ErrorTrace: A Black-Box Traceability Mechanism Based on Model Family Error Space",
      "authors": [
        "Chuanchao Zang",
        "Xiangtao Meng",
        "Wenyu Chen",
        "Tianshuo Cong",
        "Zha Yaxing",
        "Dong Qi",
        "Zheng Li",
        "Shanqing Guo"
      ],
      "abstract": "The open-source release of large language models (LLMs) enables malicious users to create unauthorized derivative models at low cost, posing significant threats to intellectual property (IP) and market stability. Existing IP protection methods either require access to model parameters or are vulnerable to fine-tuning attacks. To fill this gap, we propose ErrorTrace, a robust and black-box traceability mechanism for protecting LLM IP. Specifically, ErrorTrace leverages the unique error patterns of model families by mapping and analyzing their distinct error spaces, enabling robust and efficient IP protection without relying on internal parameters or specific query responses. \nExperimental results show that ErrorTrace achieves a traceability accuracy of 0.8518 for 27 base models when the suspect model is not included in ErrorTrace's training set, outperforming the baseline by 0.2593. Additionally,ErrorTrace successfully tracks 34 fine-tuned, pruned and merged models across various scenarios, demonstrating its broad applicability and robustness. In addition, ErrorTrace shows a certain level of resilience when subjected to adversarial attacks. Our code is available at: https://github.com/csdatazcc/ErrorTrace.",
      "arxiv_url": "https://openreview.net/forum?id=3P3PL7aCXM",
      "pdf_url": "https://openreview.net/pdf/d7ee6b9c50f47b267806e2463cc1c6fc31a5ac46.pdf",
      "primary_category": "LLM Intellectual Property Protection, LLM, LLM Safety",
      "categories": [
        "LLM Intellectual Property Protection",
        "LLM",
        "LLM Safety",
        "Error Space"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XsNi2STaj0",
      "title": "Overcoming Long Context Limitations of State Space Models via Context Dependent Sparse Attention",
      "authors": [
        "Zhihao Zhan",
        "Jianan Zhao",
        "Zhaocheng Zhu",
        "Jian Tang"
      ],
      "abstract": "Efficient long-context modeling remains a critical challenge for natural language processing (NLP), as the time complexity of the predominant Transformer architecture scales quadratically with the sequence length. While state-space models (SSMs) offer alternative sub-quadratic solutions, they struggle to capture long-range dependencies effectively. In this work, we focus on analyzing and improving the long-context modeling capabilities of SSMs. We show that the widely used synthetic task, associative recall, which requires a model to recall a value associated with a single key without context, insufficiently represents the complexities of real-world long-context modeling. To address this limitation, we extend the associative recall to a novel synthetic task, joint recall, which requires a model to recall the value associated with a key given in a specified context. Theoretically, we prove that SSMs do not have the expressiveness to solve multi-query joint recall in sub-quadratic time complexity. To resolve this issue, we propose a solution based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which has the expressiveness to solve multi-query joint recall with sub-quadratic computation. To bridge the gap between theoretical analysis and real-world applications, we propose locality-sensitive Hashing Attention with sparse Key Selection (HAX), which instantiates the theoretical solution and is further tailored to natural language domains. Extensive experiments on both synthetic and real-world long-context benchmarks show that HAX consistently outperforms SSM baselines and SSMs integrated with context-independent sparse attention (CISA). Our code is available at: https://github.com/DeepGraphLearning/HAX.",
      "arxiv_url": "https://openreview.net/forum?id=XsNi2STaj0",
      "pdf_url": "https://openreview.net/pdf/9c9724da41789d862903929cae59ee38603287a1.pdf",
      "primary_category": "long-context modeling, state-space models, sparse attention",
      "categories": [
        "long-context modeling",
        "state-space models",
        "sparse attention",
        "novel architectures design"
      ],
      "tags": [
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "GSE3oaiDL2",
      "title": "DeltaFormer: Unlock the state space of Transformer",
      "authors": [
        "Mingyu Xu",
        "Tenglong Ao",
        "Jiaao He",
        "Jianqiao Lu",
        "Guang Shi",
        "Shu Zhong"
      ],
      "abstract": "In recent years, large language models with Transformer architecture as the core have made breakthrough progress in many fields. At the same time, there are also some weaknesses in the large language model that have prompted people to reflect, among which the most fundamental one is the reflection on the Transformer architecture. The Transformer architecture has high parallelism and can fully utilize the computing power of GPUs, thus replacing models such as LSTM in the past few years. However, high parallelism is not a free lunch, as it fundamentally limits the performance of models. Especially, the problems that logarithmic precision Transformer architecture can solve are strictly limited to the $TC^0$. And there are many important issues that are usually considered out of $TC^0$, such as Python code evaluation, entity tracking, chess, and other state tracking tasks. Meanwhile, some recent state space methods based on Delta Rule have been able to break through the $TC^0$ architecture, but they are limited by fixed size state spaces and perform poorly on many tasks. To this end, we have re-examined the Transformer from the perspective of a state space with kernel functions and propose an improved Transformer called DeltaFormer. We have theoretically and practically demonstrated that the proposed new architecture can break through the limitation of the inherent $TC^0$ expressivity of Transformers and verified that it is not weaker than standard Transformer in language modeling tasks. We hope our work can provide inspiration for designing more expressive models.",
      "arxiv_url": "https://openreview.net/forum?id=GSE3oaiDL2",
      "pdf_url": "https://openreview.net/pdf/7f4e19832cc1713d67e131b834fbb8175f6d26d8.pdf",
      "primary_category": "Transformer, Circuit Complexity, Model Architecture",
      "categories": [
        "Transformer",
        "Circuit Complexity",
        "Model Architecture"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "CwXyUdqFqW",
      "title": "MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks",
      "authors": [
        "Sanjoy Chowdhury",
        "Mohamed Elmoghany",
        "Yohan Abeysinghe",
        "Junjie Fei",
        "Sayan Nag",
        "Salman Khan",
        "Mohamed Elhoseiny",
        "Dinesh Manocha"
      ],
      "abstract": "Large multimodal models (LMMs) have shown remarkable progress in audiovisual understanding, yet they struggle with real-world scenarios that require complex reasoning across extensive video collections. Existing benchmarks for video question answering remain limited in scope, typically involving one clip per query, which falls short of representing the challenges of large-scale, audiovisual retrieval and reasoning encountered in practical applications. To bridge this gap, we introduce a novel task named AVHaystacksQA, where the goal is to identify salient segments across different videos in response to a query and link them together to generate the most informative answer. To this end, we present AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA pairs designed to assess the capabilities of LMMs in multi-video retrieval and temporal grounding task. Additionally, we propose a model-agnostic, multi-agent framework MAGNET to address this challenge, achieving up to 89% and 65% relative improvements over baseline methods on BLEU@4 and GPT evaluation scores in QA task on our proposed AVHaystacks. To enable robust evaluation of multi-video retrieval and temporal grounding for optimal response generation, we introduce two new metrics, STEM, which captures alignment errors between a ground truth and a predicted step sequence and MTGS, to facilitate balanced and interpretable evaluation of segment-level grounding performance.",
      "arxiv_url": "https://openreview.net/forum?id=CwXyUdqFqW",
      "pdf_url": "https://openreview.net/pdf/d62b4e0d24fe9f179a5237b211798f877eda00ee.pdf",
      "primary_category": "Audio-visual learning, Audio-Visual RAG, Multi-Video Linkage",
      "categories": [
        "Audio-visual learning",
        "Audio-Visual RAG",
        "Multi-Video Linkage"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "T52hZeT7rn",
      "title": "Visual Structures Help Visual Reasoning:  Addressing the Binding Problem in LVLMs",
      "authors": [
        "Amirmohammad Izadi",
        "Mohammadali Banayeeanzade",
        "Fatemeh Askari",
        "Ali Rahimiakbar",
        "Mohammad Mahdi Vahedi",
        "Hosein Hasani",
        "Mahdieh Soleymani Baghshah"
      ],
      "abstract": "Despite progress in Large Vision-Language Models (LVLMs), their capacity for visual reasoning is often limited by the binding problem: the failure to reliably associate perceptual features with their correct visual referents. This limitation underlies persistent errors in tasks such as counting, visual search, scene description, and spatial relationship understanding. A key factor is that current LVLMs process visual features largely in parallel, lacking mechanisms for spatially grounded, serial attention.\nThis paper introduces Visual Input Structure for Enhanced Reasoning (VISER), a simple, effective method that augments visual inputs with low-level spatial structures and pairs them with a textual prompt that encourages sequential, spatially-aware parsing. We empirically demonstrate substantial performance improvements across core visual reasoning tasks, using only a single-query inference. Specifically, VISER improves GPT-4o performance on visual search, counting, and spatial relationship tasks by 25.0%, 26.8%, and 9.5%, respectively, and reduces edit distance error in scene description by 0.32 on 2D datasets.\nFurthermore, we find that the visual modification is essential for these gains; purely textual strategies, including Chain-of-Thought prompting, are insufficient and can even degrade performance. VISER underscores the importance of visual input design over purely linguistically based reasoning strategies and suggests that visual structuring is a powerful and general approach for enhancing compositional and spatial reasoning in LVLMs.",
      "arxiv_url": "https://openreview.net/forum?id=T52hZeT7rn",
      "pdf_url": "https://openreview.net/pdf/9aa6856102d8ec332673e3b3497a2c653885bfee.pdf",
      "primary_category": "Visual Reasoning, Large Vision-Language Models, Cognitive Science",
      "categories": [
        "Visual Reasoning",
        "Large Vision-Language Models",
        "Cognitive Science",
        "Binding Problem"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ByQdHPGKgU",
      "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space",
      "authors": [
        "Zhen Zhang",
        "Xuehai He",
        "Weixiang Yan",
        "Ao Shen",
        "Chenyang Zhao",
        "Xin Eric Wang"
      ],
      "abstract": "Human cognition typically involves thinking through abstract, fluid concepts rather than strictly using discrete linguistic tokens. Current Large Language Models (LLMs), however, are constrained to reasoning within the boundaries of human language, processing discrete token embeddings that represent fixed points in semantic space. This discrete constraint restricts the expressive power and upper potential of such reasoning models, often causing incomplete exploration of reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling one token per step. In this work, we introduce Soft Thinking, a training-free method that emulates human-like ``soft'' reasoning by generating abstract concept tokens in a continuous concept space. These concept tokens are created by the probability-weighted mixture of token embeddings, which span the continuous concept space, enabling smooth transitions and richer representations that transcend traditional discrete boundaries. In essence, each generated concept token encapsulates multiple meanings from related discrete tokens, implicitly exploring various reasoning paths to converge effectively toward the correct answer. Empirical evaluations on diverse mathematical and coding benchmarks consistently demonstrate the effectiveness and efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points while simultaneously reducing token usage by up to 22.4\\% compared to standard CoT. Qualitative analysis further reveals that Soft Thinking outputs remain highly interpretable and readable, highlighting the potential of Soft Thinking to break the inherent limits of discrete language-based reasoning.",
      "arxiv_url": "https://openreview.net/forum?id=ByQdHPGKgU",
      "pdf_url": "https://openreview.net/pdf/5a05455078d79a4f477c4d76bdae6526f3bec7b9.pdf",
      "primary_category": "Chain-of-thought, Reasoning, Efficiency",
      "categories": [
        "Chain-of-thought",
        "Reasoning",
        "Efficiency",
        "Continous Space Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "OuGAwwAT8G",
      "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning",
      "authors": [
        "Mingyang Chen",
        "Linzhuang Sun",
        "Tianpeng Li",
        "sunhaoze",
        "ZhouYijie",
        "Chenzheng Zhu",
        "Haofen Wang",
        "Jeff Z. Pan",
        "Wen Zhang",
        "Huajun Chen",
        "Fan Yang",
        "Zenan Zhou",
        "Weipeng Chen"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning, exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating reasoning with external search processes remains challenging, especially for complex multi-hop questions requiring multiple retrieval steps. We propose ReSearch, a novel framework that trains LLMs to Reason with Search via reinforcement learning without using any supervised data on reasoning steps. Our approach treats search operations as integral components of the reasoning chain, where when and how to perform searches is guided by text-based thinking, and search results subsequently influence further reasoning. We train ReSearch on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct extensive experiments. Despite being trained on only one dataset, our models demonstrate strong generalizability across various benchmarks. Analysis reveals that ReSearch naturally elicits advanced reasoning capabilities such as reflection and self-correction during the reinforcement learning process.",
      "arxiv_url": "https://openreview.net/forum?id=OuGAwwAT8G",
      "pdf_url": "https://openreview.net/pdf/3a4c411411ec8827c21b081ac099f93878bb8269.pdf",
      "primary_category": "Large Language Models, Reinforcement Learning, Chain-of-Thought Reasoning",
      "categories": [
        "Large Language Models",
        "Reinforcement Learning",
        "Chain-of-Thought Reasoning",
        "Retrieval-Augmented Generation",
        "Multi-hop Question Answering"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Igq7Dyc3OL",
      "title": "Reasoning Models Hallucinate More: Factuality-Aware Reinforcement Learning for Large Reasoning Models",
      "authors": [
        "Junyi Li",
        "Hwee Tou Ng"
      ],
      "abstract": "Large language models (LLMs) have significantly advanced in reasoning tasks through reinforcement learning (RL) optimization, achieving impressive capabilities across various challenging benchmarks. However, our empirical analysis reveals a critical drawback: reasoning-oriented RL fine-tuning significantly increases the prevalence of hallucinations. We theoretically analyze the RL training dynamics, identifying high-variance gradient, entropy-induced randomness, and susceptibility to spurious local optima as key factors leading to hallucinations. To address this drawback, we propose Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL fine-tuning algorithm incorporating explicit factuality verification at each reasoning step. FSPO leverages automated verification against given evidence to dynamically adjust token-level advantage values, incentivizing factual correctness throughout the reasoning process. Experiments across mathematical reasoning and hallucination benchmarks using Qwen2.5 and Llama models demonstrate that FSPO effectively reduces hallucinations while enhancing reasoning accuracy, substantially improving both reliability and performance.",
      "arxiv_url": "https://openreview.net/forum?id=Igq7Dyc3OL",
      "pdf_url": "https://openreview.net/pdf/31f57f113b7a0a8d53b00020bbcdabe6ac8a82cf.pdf",
      "primary_category": "Hallucination, Large Reasoning Models, Reinforcement Learning",
      "categories": [
        "Hallucination",
        "Large Reasoning Models",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VaC4sa96EI",
      "title": "Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning",
      "authors": [
        "Sanghyun Ahn",
        "Wonje Choi",
        "Junyong Lee",
        "Jinwoo Park",
        "Honguk Woo"
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled the automatic generation of executable code for task planning and control in embodied agents such as robots, demonstrating the potential of LLM-based embodied intelligence. However, these LLM-based code-as-policies approaches often suffer from limited environmental grounding, particularly in dynamic or partially observable settings, leading to suboptimal task success rates due to incorrect or incomplete code generation. In this work, we propose a neuro-symbolic embodied task planning framework that incorporates explicit symbolic verification and interactive validation processes during code generation. In the validation phase, the framework generates exploratory code that actively interacts with the environment to acquire missing observations while preserving task-relevant states. This integrated process enhances the grounding of generated code, resulting in improved task reliability and success rates in complex environments. We evaluate our framework on RLBench and in real-world settings across dynamic, partially observable scenarios. Experimental results demonstrate that our framework improves task success rates by 46.2\\% over Code as Policies baselines and attains over 86.8\\% executability of task-relevant actions, thereby enhancing the reliability of task planning in dynamic environments.",
      "arxiv_url": "https://openreview.net/forum?id=VaC4sa96EI",
      "pdf_url": "https://openreview.net/pdf/5f58ec4c84b0853291bf90d7ce98babefbbbc313.pdf",
      "primary_category": "neuro-symbolic, embodied task planning, large language model",
      "categories": [
        "neuro-symbolic",
        "embodied task planning",
        "large language model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "sH0ZwzDJZn",
      "title": "UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection",
      "authors": [
        "Yang Zhao",
        "Kai Xiong",
        "Xiao Ding",
        "Li Du",
        "YangouOuyang",
        "Zhouhao Sun",
        "Jiannan Guan",
        "Wenbin Zhang",
        "Bin Liu",
        "Dong Hu",
        "Bing Qin",
        "Ting Liu"
      ],
      "abstract": "A primary impediment to scaling reinforcement learning (RL) for large language model (LLM) training is the substantial computational cost, predominantly arising from the necessity of multi-sampling for policy optimization and evaluation. This underscores the critical yet challenging nature of efficient training data selection. Drawing inspiration from the Zone of Proximal Development (ZPD) theory, which posits that learners acquire knowledge more effectively from tasks of intermediate difficulty, we hypothesize that LLMs exhibit optimal learning from data they have not yet mastered but demonstrate the potential to comprehend. Conventional methodologies for assessing data difficulty or informativeness typically rely on computationally intensive multi-sampling or iterative procedures. To address this limitation, we introduce UFO-RL (**U**ncertainty-**F**ocused **O**ptimization for **R**einforcement **L**earning), a novel framework that employs a computationally efficient single-pass uncertainty estimation technique to identify informative training instances. This method, requiring only a single forward pass and obviating the need for iterative next-token computation, achieves a significant acceleration (up to 185$\\times$) in data evaluation compared to multi-sampling approaches. UFO-RL leverages this efficient metric to select data within the model's estimated ZPD for training. Extensive experimentation across diverse LLMs and mathematical benchmarks demonstrates that training with a mere 10\\% of the data, carefully selected by UFO-RL, yields performance comparable to or even surpassing that of full-data training. Furthermore, this targeted data selection results in up to a 16$\\times$ reduction in overall training time, concurrently enhancing training stability and improving generalization capabilities. Thus, UFO-RL presents a practical and highly efficient strategy for scaling RL fine-tuning of LLMs by focusing learning efforts on the most informative and valuable data, thereby mitigating the computational bottlenecks associated with traditional RL training.",
      "arxiv_url": "https://openreview.net/forum?id=sH0ZwzDJZn",
      "pdf_url": "https://openreview.net/pdf/70cc22bfcccd26d6359b9f517b233ce2dfc7f828.pdf",
      "primary_category": "Data Selection, LLMs, Reinforcement Learning",
      "categories": [
        "Data Selection",
        "LLMs",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "AJJ5fBB7JY",
      "title": "Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data",
      "authors": [
        "Zhenqing Ling",
        "Daoyuan Chen",
        "Liuyi Yao",
        "Qianli Shen",
        "Yaliang Li",
        "Ying Shen"
      ],
      "abstract": "Fine-tuning large language models (LLMs) using diverse datasets is crucial for enhancing their overall performance across various domains.\nIn practical scenarios, existing methods based on modeling the mixture proportions of data composition often struggle with data whose domain labels are missing, imprecise or non-normalized, while methods based on data selection usually encounter difficulties in balancing multi-domain performance.\nTo address these challenges, in this work, we investigate the role of data diversity in enhancing the overall abilities of LLMs by empirically constructing contrastive data pools and theoretically deriving explanations. \nBuilding upon the insights gained, we propose a new method that gives the LLM a dual identity: an output model to cognitively probe and select data based on diversity reward, as well as an input model to be tuned with the selected data.\nExtensive experiments show that the proposed method notably boosts performance across domain-undetermined data and a series of foundational downstream tasks when applied to various advanced LLMs. We release our code and hope this study can shed light on the understanding of data diversity and advance feedback-driven data-model co-design for LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=AJJ5fBB7JY",
      "pdf_url": "https://openreview.net/pdf/e8128d5d305fd94c1ce0961f0dedde87581de65d.pdf",
      "primary_category": "Fine-Tuning, Data Selection, Large Language Models",
      "categories": [
        "Fine-Tuning",
        "Data Selection",
        "Large Language Models",
        "Diversity"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Tdl89SZItB",
      "title": "Accurate KV Cache Eviction via Anchor Direction Projection for Efficient LLM Inference",
      "authors": [
        "Zijie Geng",
        "Jie Wang",
        "Ziqi Liu",
        "Feng Ju",
        "Yiming Li",
        "Xing Li",
        "Mingxuan Yuan",
        "Jianye HAO",
        "Defu Lian",
        "Enhong Chen",
        "Feng Wu"
      ],
      "abstract": "Key-Value (KV) cache eviction---which retains the KV pairs of the most important tokens while discarding less important ones---is a critical technique for optimizing both memory usage and inference latency in large language models (LLMs).\nHowever, existing approaches often rely on simple heuristics---such as attention weights---to measure token importance, overlooking the spatial relationships between token value states in the vector space.\nThis often leads to suboptimal token selections and thus performance degradation.\nTo tackle this problem, we propose a novel method, namely **AnDPro** (**An**chor **D**irection **Pro**jection), which introduces a projection-based scoring function to more accurately measure token importance.\nSpecifically, AnDPro operates in the space of value vectors and leverages the projections of these vectors onto an *``Anchor Direction''*---the direction of the pre-eviction output---to measure token importance and guide more accurate token selection.\nExperiments on $16$ datasets from the LongBench benchmark demonstrate that AnDPro can maintain $96.07\\\\%$ of the full cache accuracy using only $3.44\\\\%$ KV cache budget, reducing KV cache budget size by $46.0\\\\%$ without compromising quality compared to previous state-of-the-arts.",
      "arxiv_url": "https://openreview.net/forum?id=Tdl89SZItB",
      "pdf_url": "https://openreview.net/pdf/27e5165f6a6821bbd2a496ddaa625c6b9de5e57e.pdf",
      "primary_category": "Large Language Models, KV Cache Eviction, LLM Inference Acceleration",
      "categories": [
        "Large Language Models",
        "KV Cache Eviction",
        "LLM Inference Acceleration"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kQWyOYUAC4",
      "title": "AI-Researcher: Autonomous Scientific Innovation",
      "authors": [
        "Jiabin Tang",
        "Lianghao Xia",
        "Zhonghang Li",
        "Chao Huang"
      ],
      "abstract": "The powerful reasoning capabilities of Large Language Models (LLMs) in mathematics and coding, combined with their ability to automate complex tasks through agentic frameworks, present unprecedented opportunities for accelerating scientific innovation. In this paper, we introduce AI-Researcher, a fully autonomous research system that transforms how AI-driven scientific discovery is conducted and evaluated. Our framework seamlessly orchestrates the complete research pipeline--from literature review and hypothesis generation to algorithm implementation and publication-ready manuscript preparation--with minimal human intervention. To rigorously assess autonomous research capabilities, we develop Scientist-Bench, a comprehensive benchmark comprising state-of-the-art papers across diverse AI research domains, featuring both guided innovation and open-ended exploration tasks. Through extensive experiments, we demonstrate that AI-Researcher achieves remarkable implementation success rates and produces research papers that approach human-level quality. This work establishes new foundations for autonomous scientific innovation that can complement human researchers by systematically exploring solution spaces beyond cognitive limitations.",
      "arxiv_url": "https://openreview.net/forum?id=kQWyOYUAC4",
      "pdf_url": "https://openreview.net/pdf/a1c63cdd0495de94664b1513f7d95a3aedcb483a.pdf",
      "primary_category": "LLM agents, AI for Science",
      "categories": [
        "LLM agents",
        "AI for Science"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "eafIjoZAHm",
      "title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability",
      "authors": [
        "Burouj Armgaan",
        "Eshan Jain",
        "Harsh Pandey",
        "Mahesh Chandran",
        "Sayan Ranu"
      ],
      "abstract": "Graph Neural Networks (GNNs) are widely used for node classification, yet their opaque decision-making limits trust and adoption. While local explanations offer insights into individual predictions, global explanation methods—those that characterize an entire class—remain underdeveloped. Existing global explainers rely on motif discovery in small graphs, an approach that breaks down in large, real-world settings where subgraph repetition is rare, node attributes are high-dimensional, and predictions arise from complex structure-attribute interactions. We propose GnnXemplar, a novel global explainer inspired from Exemplar Theory from cognitive science. GnnXemplar identifies representative nodes in the GNN embedding space—exemplars—and explains predictions using natural language rules derived from their neighborhoods. Exemplar selection is framed as a coverage maximization problem over reverse $k$-nearest neighbors, for which we provide an efficient greedy approximation. To derive interpretable rules, we employ a self-refining prompt strategy using large language models (LLMs). Experiments across diverse benchmarks show that GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants.",
      "arxiv_url": "https://openreview.net/forum?id=eafIjoZAHm",
      "pdf_url": "https://openreview.net/pdf/4c15b12cff38ede047ec44b0df28590572e447cc.pdf",
      "primary_category": "graph neural network, graph machine learning, explainability",
      "categories": [
        "graph neural network",
        "graph machine learning",
        "explainability",
        "xai",
        "global explanation",
        "text-based explanation",
        "exemplar",
        "exemplar theory"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "PBy1Ew1ihV",
      "title": "Improving Task-Specific Multimodal Sentiment Analysis with General MLLMs via Prompting",
      "authors": [
        "Haoyu Zhang",
        "Yinan Zhang",
        "Chaolong Ying",
        "Xiaoying Tang",
        "Tianshu Yu"
      ],
      "abstract": "Multimodal Sentiment Analysis (MSA) aims to predict sentiment from diverse data types, such as video, audio, and language. Recent progress in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance across various tasks. However, in MSA, the increase in computational costs does not always correspond to a significant improvement in performance, raising concerns about the cost-effectiveness of applying MLLMs to MSA. This paper introduces the MLLM-Guided Multimodal Sentiment Learning Framework (MMSLF). It improves the performance of task-specific MSA models by leveraging the generalized knowledge of MLLMs through a teacher-student framework, rather than directly using MLLMs for sentiment prediction. First, the proposed teacher built upon a powerful MLLM (e.g., GPT-4o-mini), guides the student model to align multimodal representations through MLLM-generated context-aware prompts. Then, knowledge distillation enables the student to mimic the teacher’s predictions, thus allowing it to predict sentiment independently without relying on the context-aware prompts. Extensive experiments on the SIMS, MOSI, and MOSEI datasets demonstrate that our framework enables task-specific models to achieve state-of-the-art performance across most metrics. This also provides new insights into the application of general MLLMs for improving MSA.",
      "arxiv_url": "https://openreview.net/forum?id=PBy1Ew1ihV",
      "pdf_url": "https://openreview.net/pdf/919354eaf7cb7ae1cd859c22cb10c658b3db1799.pdf",
      "primary_category": "Multimodal Sentiment Analysis",
      "categories": [
        "Multimodal Sentiment Analysis"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZTfqehfcEJ",
      "title": "MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward Optimization",
      "authors": [
        "Chenglong Wang",
        "Yang Gan",
        "Hang Zhou",
        "Chi Hu",
        "Yongyu Mu",
        "Kai Song",
        "MuRun Yang",
        "Bei Li",
        "Chunliang Zhang",
        "Tongran Liu",
        "JingBo Zhu",
        "Zhengtao Yu",
        "Tong Xiao"
      ],
      "abstract": "Recent advances in diffusion language models (DLMs) have presented a promising alternative to traditional autoregressive large language models (LLMs). However, DLMs still lag behind LLMs in reasoning performance, especially as the number of denoising steps decreases. Our analysis reveals that this shortcoming arises primarily from the independent generation of masked tokens across denoising steps, which fails to capture the token correlation. In this paper, we define two types of token correlation: intra-sequence correlation and inter-sequence correlation, and demonstrate that enhancing these correlations improves reasoning performance. To this end, we propose a Multi-Reward Optimization (MRO) approach, which encourages DLMs to consider the token correlation during the denoising process. More specifically, our MRO approach leverages test-time scaling, reject sampling, and reinforcement learning to directly optimize the token correlation with multiple elaborate rewards. Additionally, we introduce group step and importance sampling strategies to mitigate reward variance and enhance sampling efficiency. Through extensive experiments, we demonstrate that MRO not only improves reasoning performance but also achieves significant sampling speedups while maintaining high performance on reasoning benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=ZTfqehfcEJ",
      "pdf_url": "https://openreview.net/pdf/8a2cdfdf8b96531a75a4528879d3a424b22f3a41.pdf",
      "primary_category": "Generative Model, Language Model, Reasoning",
      "categories": [
        "Generative Model",
        "Language Model",
        "Reasoning",
        "Diffusion Language Model",
        "Reward Optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "jRjvcqtdtA",
      "title": "LaRes: Evolutionary Reinforcement Learning with LLM-based Adaptive Reward Search",
      "authors": [
        "Pengyi Li",
        "Hongyao Tang",
        "Jinbin Qiao",
        "YAN ZHENG",
        "Jianye HAO"
      ],
      "abstract": "The integration of evolutionary algorithms (EAs) with reinforcement learning (RL) has shown superior performance compared to standalone methods. However, previous research focuses on exploration in policy parameter space, while overlooking the reward function search.\nTo bridge this gap, we propose **LaRes**, a novel hybrid framework that achieves efficient policy learning through reward function search. LaRes leverages large language models (LLMs) to generate the reward function population, guiding RL in policy learning. The reward functions are evaluated by the policy performance and improved through LLMs.\nTo improve sample efficiency, LaRes employs a shared experience buffer that collects experiences from all policies, with each experience containing rewards from all reward functions.\nUpon reward function updates, the rewards of experiences are relabeled, enabling efficient use of historical data. \nFurthermore, we introduce a Thompson sampling-based selection mechanism that enables more efficient elite interaction.\nTo prevent policy collapse when improving reward functions, we propose the reward scaling and parameter constraint mechanisms to efficiently coordinate reward search with policy learning.\nAcross both initialized and non-initialized settings, LaRes  consistently achieves state-of-the-art performance, outperforming strong baselines in both sample efficiency and final performance.\nThe code is available at https://github.com/yeshenpy/LaRes.",
      "arxiv_url": "https://openreview.net/forum?id=jRjvcqtdtA",
      "pdf_url": "https://openreview.net/pdf/c648a14c5243c69eddc9074f37d344495be161ed.pdf",
      "primary_category": "Evolutionary Reinforcement Learning",
      "categories": [
        "Evolutionary Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xwqTt26NJf",
      "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
      "authors": [
        "Daniel Mingyi Israel",
        "Guy Van den Broeck",
        "Aditya Grover"
      ],
      "abstract": "The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=xwqTt26NJf",
      "pdf_url": "https://openreview.net/pdf/598ea86df6f94c4e13ef03da44b681605dacb6ea.pdf",
      "primary_category": "LLM, discrete diffusion, autoregression",
      "categories": [
        "LLM",
        "discrete diffusion",
        "autoregression",
        "sequential",
        "fast",
        "throughput",
        "speculative decoding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "aZooxbW63m",
      "title": "A Semantic Parsing Framework for End-to-End Time Normalization",
      "authors": [
        "Xin Su",
        "Sungduk Yu",
        "Phillip Howard",
        "Steven Bethard"
      ],
      "abstract": "Time normalization is the task of converting natural language temporal expressions into machine-readable representations. It underpins many downstream applications in information retrieval, question answering, and clinical decision-making. Traditional systems based on the ISO-TimeML schema limit expressivity and struggle with complex constructs such as compositional, event-relative, and multi-span time expressions. In this work, we introduce a novel formulation of time normalization as a code generation task grounded in the SCATE framework, which defines temporal semantics through symbolic and compositional operators. We implement a fully executable SCATE Python library and demonstrate that large language models (LLMs) can generate executable SCATE code. Leveraging this capability, we develop an automatic data augmentation pipeline using LLMs to synthesize large-scale annotated data with code-level validation. Our experiments show that small, locally deployable models trained on this augmented data can achieve strong performance, outperforming even their LLM parents and enabling practical, accurate, and interpretable time normalization.",
      "arxiv_url": "https://openreview.net/forum?id=aZooxbW63m",
      "pdf_url": "https://openreview.net/pdf/1715480be4530883b1a8182d3c729bc5811fd635.pdf",
      "primary_category": "Time Normalization, Temporal Information Extraction",
      "categories": [
        "Time Normalization",
        "Temporal Information Extraction"
      ],
      "tags": [
        "LLM",
        "Information Retrieval"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MBDWO29Qq6",
      "title": "Incentivizing LLMs to Self-Verify Their Answers",
      "authors": [
        "Fuxiang Zhang",
        "Jiacheng Xu",
        "Chaojie Wang",
        "Ce Cui",
        "Yang Liu",
        "Bo An"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable progress in complex reasoning tasks through both post-training and test-time scaling laws. While prevalent test-time scaling approaches are often realized by using external reward models to guide the model generation process, we find that only marginal gains can be acquired when scaling a model post-trained on specific reasoning tasks. We identify that the limited improvement stems from distribution discrepancies between the specific post-trained generator and the general reward model. To address this, we propose a framework that incentivizes LLMs to self-verify their own answers. By unifying answer generation and verification within a single reinforcement learning (RL) process, we train models that can effectively assess the correctness of their own solutions. The trained model can further scale its performance at inference time by verifying its generations, without the need for external verifiers. We train our self-verification models based on Qwen2.5-Math-7B and DeepSeek-R1-Distill-Qwen-1.5B, demonstrating their capabilities across varying reasoning context lengths. Experiments on multiple mathematical reasoning benchmarks show that our models can not only improve post-training performance but also enable effective test-time scaling. Our code is available at https://github.com/mansicer/self-verification.",
      "arxiv_url": "https://openreview.net/forum?id=MBDWO29Qq6",
      "pdf_url": "https://openreview.net/pdf/ea6649a9cfb1c181a137632923e930e4e14e6ad3.pdf",
      "primary_category": "large language model, math reasoning, reinforcement learning",
      "categories": [
        "large language model",
        "math reasoning",
        "reinforcement learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nGQLYn13Xf",
      "title": "FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts",
      "authors": [
        "Heming Zou",
        "Yunliang Zang",
        "Wutong Xu",
        "Yao Zhu",
        "Xiangyang Ji"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains---general knowledge understanding, scientific question answering, mathematical reasoning, and code generation---demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.",
      "arxiv_url": "https://openreview.net/forum?id=nGQLYn13Xf",
      "pdf_url": "https://openreview.net/pdf/bab86997ab307e51dedc54236c64e268a2e4df9c.pdf",
      "primary_category": "low-rank adaptation, fly olfactory circuit, parameter-efficient fine-tuning",
      "categories": [
        "low-rank adaptation",
        "fly olfactory circuit",
        "parameter-efficient fine-tuning",
        "Mixture-of-Expert",
        "model merging"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "m0bR0sxhfL",
      "title": "CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs",
      "authors": [
        "Zhiyuan Ning",
        "Jiawei Shao",
        "Ruge Xu",
        "Xinfei Guo",
        "Jun Zhang",
        "Chi Zhang",
        "Xuelong Li"
      ],
      "abstract": "Speculative decoding has become a widely adopted as an effective technique for lossless inference acceleration when deploying large language models (LLMs).\nWhile on-the-fly self-speculative methods offer seamless integration and broad utility, they often fall short of the speed gains achieved by methods relying on specialized training.\nCascading a hierarchy of draft models promises further acceleration and flexibility, but the high cost of training multiple models has limited its practical application.\nIn this paper, we propose a novel Cascade Adaptive Self-Speculative Decoding (CAS-Spec) method which constructs speculative draft models by leveraging dynamically switchable inference acceleration (DSIA) strategies, including layer sparsity and activation quantization.\nFurthermore, traditional vertical and horizontal cascade algorithms are inefficient when applied to self-speculative decoding methods.\nWe introduce a Dynamic Tree Cascade (DyTC) algorithm that adaptively routes the multi-level draft models and assigns the draft lengths, based on the heuristics of acceptance rates and latency prediction.\nOur CAS-Spec method achieves state-of-the-art acceleration compared to existing on-the-fly speculative decoding methods, with an average speedup from $1.1\\times$ to $2.3\\times$ over autoregressive decoding across various LLMs and datasets.\nDyTC improves the average speedup by $47$\\% and $48$\\% over cascade-based baseline and tree-based baseline algorithms, respectively.\nCAS-Spec can be easily integrated into most existing LLMs and holds promising potential for further acceleration as self-speculative decoding techniques continue to evolve.",
      "arxiv_url": "https://openreview.net/forum?id=m0bR0sxhfL",
      "pdf_url": "https://openreview.net/pdf/7be7febdbc687ff1d863bbeaf1f37fb1b683f4bc.pdf",
      "primary_category": "Large Language Models, Efficient Inference, Speculative Decoding",
      "categories": [
        "Large Language Models",
        "Efficient Inference",
        "Speculative Decoding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kJqTkj2HhF",
      "title": "AutoDiscovery: Open-ended Scientific Discovery via Bayesian Surprise",
      "authors": [
        "Dhruv Agarwal",
        "Bodhisattwa Prasad Majumder",
        "Reece Adamson",
        "Megha Chakravorty",
        "Satvika Reddy Gavireddy",
        "Aditya Parashar",
        "Harshit Surana",
        "Bhavana Dalvi Mishra",
        "Andrew McCallum",
        "Ashish Sabharwal",
        "Peter Clark"
      ],
      "abstract": "The promise of autonomous scientific discovery (ASD) hinges not only on answering questions, but also on knowing which questions to ask. Most recent works in ASD explore the use of large language models (LLMs) in goal-driven settings, relying on human-specified research questions to guide hypothesis generation. However, scientific discovery may be accelerated further by allowing the AI system to drive exploration by its own criteria. The few existing approaches in open-ended ASD select hypotheses based on diversity heuristics or subjective proxies for human interestingness, but the former struggles to meaningfully navigate the typically vast hypothesis space, and the latter suffers from imprecise definitions. This paper presents AutoDiscovery—a method for open-ended ASD that instead drives scientific exploration using Bayesian surprise. Here, we quantify the epistemic shift from the LLM’s prior beliefs about a hypothesis to its posterior beliefs after gathering experimental results. To efficiently explore the space of nested hypotheses, our method employs a Monte Carlo tree search (MCTS) strategy with progressive widening using surprisal as the reward function. We evaluate AutoDiscovery in the setting of data-driven discovery across 21 real-world datasets spanning domains such as biology, economics, finance, and behavioral science. Our results demonstrate that under a fixed budget, AutoDiscovery substantially outperforms competitors by producing 5-29% more discoveries deemed surprising by the LLM. Our human evaluation further reveals that two-thirds of discoveries made by our system are surprising to domain experts as well, suggesting this is an important step towards building open-ended ASD systems.",
      "arxiv_url": "https://openreview.net/forum?id=kJqTkj2HhF",
      "pdf_url": "https://openreview.net/pdf/bf769f7d44fa3ddbf9f8980e9cd6b38060dbd9db.pdf",
      "primary_category": "scientific discovery, LLM, open-ended",
      "categories": [
        "scientific discovery",
        "LLM",
        "open-ended",
        "Bayesian",
        "surprise",
        "data-driven discovery",
        "autonomous discovery"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kurEZdWU9G",
      "title": "Table as a Modality for Large Language Models",
      "authors": [
        "Liyao Li",
        "Chao Ye",
        "Wentao Ye",
        "Yifei Sun",
        "Zhe Jiang",
        "Haobo Wang",
        "Jiaming Tian",
        "Yiming Zhang",
        "NINGTAO WANG",
        "Xing Fu",
        "Gang Chen",
        "Junbo Zhao"
      ],
      "abstract": "To migrate the remarkable successes of Large Language Models (LLMs), the community has made numerous efforts to generalize them to the table reasoning tasks for the widely deployed tabular data. Despite that, in this work, by showing a probing experiment on our proposed StructQA benchmark, we postulate that even the most advanced LLMs (such as GPTs) may still fall short of coping with tabular data. More specifically, the current scheme often simply relies on serializing the tabular data, together with the meta information, then inputting them through the LLMs. We argue that the loss of structural information is the root of this shortcoming. In this work, we further propose TAMO, which bears an ideology to treat the tables as an independent modality integrated with the text tokens. The resulting model in TAMO is a multimodal framework consisting of a hypergraph neural network as the global table encoder seamlessly integrated with the mainstream LLM. Empirical results on various benchmarking datasets, including HiTab, WikiTQ, WikiSQL, FeTaQA, and StructQA, have demonstrated significant improvements on generalization with an average relative gain of **42.65%**.",
      "arxiv_url": "https://openreview.net/forum?id=kurEZdWU9G",
      "pdf_url": "https://openreview.net/pdf/44393a6329e01ef71be97eb3484aef3546dbd909.pdf",
      "primary_category": "Large language model, Table reasoning",
      "categories": [
        "Large language model",
        "Table reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "W6WC6047X2",
      "title": "Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems",
      "authors": [
        "Christian Walder",
        "Deep Tejas Karkhanis"
      ],
      "abstract": "Reinforcement Learning algorithms commonly sample multiple ($n>1$) solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes individual sample performance over the diversity and collective utility of a set of samples. Such algorithms under-utilize the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at-$k$ Policy Optimization (PKPO), a multivariate transformation on batches of rewards which leads to direct optimization of \\passk\\ performance, thus optimizing for sets of samples that feature a large maximum reward when considered jointly. Our primary contribution is to derive novel low variance unbiased estimators for the pass@k and its gradient, in both the binary and continuous reward settings. We show that optimizing with these estimators reduces to reinforcement learning with (batches of) rewards that have been jointly transformed by a function that is stable and efficient to compute.\n\nWhile previous efforts propose transformations for $k=n$, our transformations are the first to enable robust optimization of the pass@k for any arbitrary $k \\leq n$. Rather than simply trading off pass@1 performance for pass@k gains, our method allows annealing $k$ during training, optimizing both metrics and often achieving strong pass@1 performance alongside significant pass@k gains.\n\nWe validate our transformations on illustrative toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source models Gemma and Llama . We find that our transformation effectively optimizes for the target $k$. Furthermore, higher $k$ values enable solving more and harder problems, while annealing $k$ boosts both the pass@1 and pass@k. Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely by improving exploration through the prioritization of joint utility over the utility of individual samples",
      "arxiv_url": "https://openreview.net/forum?id=W6WC6047X2",
      "pdf_url": "https://openreview.net/pdf/ad9487f82f7b5b7ee075d300beb3ae17fa78d1d9.pdf",
      "primary_category": "reinforcement learning, llm, pass at k",
      "categories": [
        "reinforcement learning",
        "llm",
        "pass at k",
        "inference time compute",
        "monte carlo",
        "gradient estimation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "z4AMrCOetn",
      "title": "LogicTree: Improving Complex Reasoning of LLMs via Instantiated Multi-step Synthetic Logical Data",
      "authors": [
        "Zehao Wang",
        "Lin Yang",
        "Jie Wang",
        "Kehan Wang",
        "Hanzhu Chen",
        "Bin Wang",
        "Jianye HAO",
        "Defu Lian",
        "Bin Li",
        "Enhong Chen"
      ],
      "abstract": "Despite their remarkable performance on various tasks, Large Language Models (LLMs) still struggle with logical reasoning, particularly in complex and multi-step reasoning processes. \nAmong various efforts to enhance LLMs' reasoning capabilities, synthesizing large-scale, high-quality logical reasoning datasets has emerged as a promising direction. \nHowever, existing methods often rely on predefined templates for logical reasoning data generation, limiting their adaptability to real-world scenarios. \nTo address the limitation, we propose **LogicTree**, a novel framework for efficiently synthesizing multi-step logical reasoning dataset that excels in both complexity and instantiation.\nBy iteratively searching for applicable logic rules based on structural pattern matching to perform backward deduction, **LogicTree** constructs multi-step logic trees that capture complex reasoning patterns. \nFurthermore, we employ a two-stage LLM-based approach to instantiate various real-world scenarios for each logic tree, generating consistent real-world reasoning processes that carry contextual significance.   This helps LLMs develop generalizable logical reasoning abilities across diverse scenarios rather than merely memorizing templates.\nExperiments on multiple benchmarks demonstrate that our approach achieves an average improvement of 9.4\\% in accuracy on complex logical reasoning tasks.",
      "arxiv_url": "https://openreview.net/forum?id=z4AMrCOetn",
      "pdf_url": "https://openreview.net/pdf/30059572044fffae201f1e0daf92fc78a5aef218.pdf",
      "primary_category": "large language model, logical reasoning, data synthesis",
      "categories": [
        "large language model",
        "logical reasoning",
        "data synthesis"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tPqBnGwTwa",
      "title": "Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement",
      "authors": [
        "Linyang He",
        "Tianjun Zhong",
        "Richard Antonello",
        "Gavin Mischler",
        "Micah Goldblum",
        "Nima Mesgarani"
      ],
      "abstract": "Understanding how the human brain progresses from processing simple linguistic inputs to performing high-level reasoning is a fundamental challenge in neuroscience. While modern large language models (LLMs) are increasingly used to model neural responses to language, their internal representations are highly \"entangled,\" mixing information about lexicon, syntax, meaning, and reasoning. This entanglement biases conventional brain encoding analyses toward linguistically shallow features (e.g., lexicon and syntax), making it difficult to isolate the neural substrates of cognitively deeper processes. Here, we introduce a residual disentanglement method that computationally isolates these components. By first probing an LM to identify feature-specific layers, our method iteratively regresses out lower-level representations to produce four nearly orthogonal embeddings for lexicon, syntax, meaning, and, critically, reasoning. We used these disentangled embeddings to model intracranial (ECoG) brain recordings from neurosurgical patients listening to natural speech. We show that: 1) This isolated reasoning embedding exhibits unique predictive power, accounting for variance in neural activity not explained by other linguistic features and even extending to the recruitment of visual regions beyond classical language areas. 2) The neural signature for reasoning is temporally distinct, peaking later (~350-400ms) than signals related to lexicon, syntax, and meaning, consistent with its position atop a processing hierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as their predictive success is primarily attributable to linguistically shallow features, masking the more subtle contributions of deeper cognitive processing. Our work provides compelling neural evidence for an abstract reasoning computation during language comprehension and offers a robust framework for mapping distinct cognitive functions from artificial models to the human brain.",
      "arxiv_url": "https://openreview.net/forum?id=tPqBnGwTwa",
      "pdf_url": "https://openreview.net/pdf/7ba52011cf6fefcbe33afd8cf16209e84bff95ef.pdf",
      "primary_category": "(Cognitive/Neuroscience) Neural Coding, (Cognitive/Neuroscience) Language",
      "categories": [
        "(Cognitive/Neuroscience) Neural Coding",
        "(Cognitive/Neuroscience) Language"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "AAXMcAyNF6",
      "title": "Towards Understanding Safety Alignment: A Mechanistic Perspective from Safety Neurons",
      "authors": [
        "Jianhui Chen",
        "Xiaozhi Wang",
        "Zijun Yao",
        "Yushi Bai",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "Large language models (LLMs) excel in various capabilities but pose safety risks such as generating harmful content and misinformation, even after safety alignment. In this paper, we explore the inner mechanisms of safety alignment through the lens of mechanistic interpretability, focusing on identifying and analyzing safety neurons within LLMs that are responsible for safety behaviors. We propose inference-time activation contrasting to locate these neurons and dynamic activation patching to evaluate their causal effects on model safety. Experiments on multiple prevalent LLMs demonstrate that we can consistently identify about 5% safety neurons, and by only patching their activations we can restore over 90% of the safety performance across various red-teaming benchmarks without influencing general ability. The finding of safety neurons also helps explain the ''alignment tax'' phenomenon by revealing that the key neurons for model safety and helpfulness significantly overlap, yet they require different activation patterns for the same neurons. Furthermore, we demonstrate an application of our findings in safeguarding LLMs by detecting unsafe outputs before generation.",
      "arxiv_url": "https://openreview.net/forum?id=AAXMcAyNF6",
      "pdf_url": "https://openreview.net/pdf/c94e1b0bd6e1678fd3d0833eebea48ca2b4fd9f5.pdf",
      "primary_category": "Large Language Models, Mechanistic Interpretability, Safety Alignment",
      "categories": [
        "Large Language Models",
        "Mechanistic Interpretability",
        "Safety Alignment",
        "Neuron Analysis",
        "Alignment Tax"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RSVdHXZN6D",
      "title": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities",
      "authors": [
        "Jin Wang",
        "Yao Lai",
        "Aoxue Li",
        "Shifeng Zhang",
        "Jiacheng Sun",
        "Ning Kang",
        "Chengyue Wu",
        "Zhenguo Li",
        "Ping Luo"
      ],
      "abstract": "The rapid progress of large language models (LLMs) has catalyzed the emergence of multimodal large language models (MLLMs) that unify visual understanding and image generation within a single framework. However, most existing MLLMs rely on autoregressive (AR) architectures, which impose inherent limitations on future development, such as the raster-scan order in image generation and restricted reasoning abilities in causal context modeling. In this work, we challenge the dominance of AR-based approaches by introducing FUDOKI, a unified multimodal model purely based on discrete flow matching, as an alternative to conventional AR paradigms. By leveraging metric-induced probability paths with kinetic optimal velocities, our framework goes beyond the previous masking-based corruption process, enabling iterative refinement with self-correction capability and richer bidirectional context integration during generation. To mitigate the high cost of training from scratch, we initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to the discrete flow matching paradigm. Experimental results show that FUDOKI achieves performance comparable to state-of-the-art AR-based MLLMs across both visual understanding and image generation tasks, highlighting its potential as a foundation for next-generation unified multimodal models. Furthermore, we show that applying test-time scaling techniques to FUDOKI yields significant performance gains, further underscoring its promise for future enhancement through reinforcement learning.",
      "arxiv_url": "https://openreview.net/forum?id=RSVdHXZN6D",
      "pdf_url": "https://openreview.net/pdf/4e0fd77a94401dc935b3e0e27331f1297941e3d1.pdf",
      "primary_category": "Multimodal Large Language Models (MLLMs); Discrete Flow Matching",
      "categories": [
        "Multimodal Large Language Models (MLLMs); Discrete Flow Matching"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BNpIO5iYGc",
      "title": "SceneForge: Enhancing 3D-text alignment with Structured Scene Compositions",
      "authors": [
        "Cristian Sbrolli",
        "Matteo Matteucci"
      ],
      "abstract": "The whole is greater than the sum of its parts, even in 3D-text contrastive learning. We introduce SceneForge, a novel framework that enhances contrastive alignment between 3D point clouds and text through structured multi-object scene compositions. SceneForge leverages individual 3D shapes to construct multi-object scenes with explicit spatial relations, pairing them with coherent multi-object descriptions refined by a large language model. By augmenting contrastive training with these structured, compositional samples, SceneForge effectively addresses the scarcity of large-scale 3D-text datasets, significantly enriching data complexity and diversity. We systematically investigate critical design elements, such as the optimal number of objects per scene, the proportion of compositional samples in training batches, and scene construction strategies. Extensive experiments demonstrate that SceneForge delivers substantial performance gains across multiple tasks, including zero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet, as well as few-shot part segmentation on ShapeNetPart. SceneForge’s compositional augmentations are model-agnostic, consistently improving performance across multiple encoder architectures. Moreover, SceneForge improves 3D visual question answering on ScanQA, generalizes robustly to retrieval scenarios with increasing scene complexity, and showcases spatial reasoning capabilities by adapting spatial configurations to align precisely with textual instructions.",
      "arxiv_url": "https://openreview.net/forum?id=BNpIO5iYGc",
      "pdf_url": "https://openreview.net/pdf/f5ff880fdf29873fbc04f580077b0ccf32ac4bde.pdf",
      "primary_category": "multimodal, 3D",
      "categories": [
        "multimodal",
        "3D"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "i9RDDi2SZC",
      "title": "Investigating and Mitigating Catastrophic Forgetting in Medical Knowledge Injection through Internal Knowledge Augmentation Learning",
      "authors": [
        "Yuxuan Zhou",
        "Xien Liu",
        "Xiao Zhang",
        "Chen Ning",
        "Shijin Wang",
        "Guoping Hu",
        "Ji Wu"
      ],
      "abstract": "Large Language Models (LLMs) are expected to possess comprehensive medical knowledge to support real-world clinical applications. While domain-specific fine-tuning effectively injects medical knowledge into LLMs, it often causes catastrophic forgetting of previously acquired knowledge and instruction-following capabilities. In this paper, we investigate this issue and reveal a pattern of proximity-dependent forgetting: knowledge that is semantically or topically close to the injected content is more likely to be forgotten, while unrelated knowledge shows minimal degradation. Moreover, we observe that existing mitigation techniques fail to address this type of forgetting effectively. Motivated by this observation and inspired by human learning mechanisms, we proposeInternAL (\\Internal Knowledge Augmentation Learning), a novel approach that leverages LLMs' own internal knowledge to mitigate forgetting. InternAL first probes internal knowledge closely related to the injection by prompting the model with questions derived from the injected knowledge. This knowledge is then used to augment the original injection dataset, guiding the model to retain related prior knowledge during training. Experimental results on multiple LLMs (LLaMA, Qwen) demonstrate that InternAL significantly mitigates proximity-related forgetting while maintaining strong knowledge injection performance. Our findings provide new insights into the nature of catastrophic forgetting in medical knowledge injection and highlight a promising direction for robust domain adaptation in LLMs. Code and datasets are available at https://github.com/THUMLP/InternAL.",
      "arxiv_url": "https://openreview.net/forum?id=i9RDDi2SZC",
      "pdf_url": "https://openreview.net/pdf/8b9e29fef0b1696843b9ec3809b0304b817ffa4b.pdf",
      "primary_category": "Large Language Model, Medical Knowledge Injection, Catastrophic Forgetting",
      "categories": [
        "Large Language Model",
        "Medical Knowledge Injection",
        "Catastrophic Forgetting"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xQH4lDLIC0",
      "title": "AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration",
      "authors": [
        "Andy Zhou",
        "Kevin Wu",
        "Francesco Pinto",
        "Zhaorun Chen",
        "Yi Zeng",
        "Yu Yang",
        "Shuang Yang",
        "Sanmi Koyejo",
        "James Zou",
        "Bo Li"
      ],
      "abstract": "As large language models (LLMs) become increasingly capable, security and safety evaluation are crucial. While current red teaming approaches have made strides in assessing LLM vulnerabilities, they often rely heavily on human input and lack comprehensive coverage of emerging attack vectors. This paper introduces AutoRedTeamer, a novel framework for fully automated, end-to-end red teaming against LLMs. AutoRedTeamer combines a multi-agent architecture with a memory-guided attack selection mechanism to enable continuous discovery and integration of new attack vectors. The dual-agent framework consists of a red teaming agent that can operate from high-level risk categories alone to generate and execute test cases, and a strategy proposer agent that autonomously discovers and implements new attacks by analyzing recent research. This modular design allows AutoRedTeamer to adapt to emerging threats while maintaining strong performance on existing attack vectors. We demonstrate AutoRedTeamer’s effectiveness across diverse evaluation settings, achieving 20% higher attack success rates on HarmBench against Llama-3.1-70B while reducing computational costs by 46% compared to existing approaches. AutoRedTeamer also matches the diversity of human-curated benchmarks in generating test cases, providing a comprehensive, scalable, and continuously evolving framework for evaluating the security of AI systems.",
      "arxiv_url": "https://openreview.net/forum?id=xQH4lDLIC0",
      "pdf_url": "https://openreview.net/pdf/e53fb762e883d79a783bab07668988038d9e5162.pdf",
      "primary_category": "adversarial robustness, large language models, jailbreaking",
      "categories": [
        "adversarial robustness",
        "large language models",
        "jailbreaking",
        "ai agents"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "X5B2yTT97A",
      "title": "SimpleStrat: Diversifying Language Model Generation with Stratification",
      "authors": [
        "Justin Wong",
        "Yury Orlovskiy",
        "Alexander Shypula",
        "Michael Luo",
        "Sanjit A. Seshia",
        "Joseph E. Gonzalez"
      ],
      "abstract": "Generating diverse responses from large language models (LLMs) is crucial for applications such as adversarial testing, search, and synthetic data generation, where diversity provides distinct answers across generations. Previous approaches rely solely on increasing the temperature, sacrificing quality. Furthermore, the model's next-token probabilities may not be representative of the true answer distribution. To combat these challenges, we propose SimpleStrat, an alternative that uses the language model itself to partition the solution space into strata from which to sample. \nTo measure resampling diversity, we introduce CoverageQA, a dataset of underspecified questions with multiple equally plausible answers. We propose measuring resampling diversity as the KL Divergence between the response distribution and the uniform distribution over valid ground truth answers and use recall as an alternative when assessing proprietary models. On CoverageQA, SimpleStrat improves diversity across all temperatures, showing orthogonal benefits. Quantifiably, we achieve as much as 4X better recall when applied to GPT-4o, and an average reduction in KL divergence by 0.36 when applied to Llama 3. Furthermore, we show that SimpleStrat achieves more resampling diversity at temperature T=0 than scaling temperature to T=1 on creative writing, an open-ended domain. Implementation and dataset available at https://github.com/jwong8314/simplestrat.",
      "arxiv_url": "https://openreview.net/forum?id=X5B2yTT97A",
      "pdf_url": "https://openreview.net/pdf/20ec15e84b1b06b7b6ed23325cbb15c3166a9d41.pdf",
      "primary_category": "Coverage, Language Model Sampling, Stratified Sampling",
      "categories": [
        "Coverage",
        "Language Model Sampling",
        "Stratified Sampling",
        "Evaluation",
        "Dataset"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "sYk6ZMmrOz",
      "title": "Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs",
      "authors": [
        "Zhiyi Lyu",
        "Jianguo Huang",
        "Yanchen Deng",
        "Steven Hoi",
        "Bo An"
      ],
      "abstract": "Large Language Models (LLMs) with inference-time scaling techniques show promise for code generation, yet face notable efficiency and scalability challenges. Construction-based tree-search methods suffer from rapid growth in tree size, high token consumption, and lack of anytime property. In contrast, improvement-based methods offer better performance but often struggle with uninformative reward signals and inefficient search strategies. In this work, we propose $\\textbf{ReLoc}$, a unified local search framework which effectively performs step-by-step code revision. Specifically, ReLoc explores a series of local revisions through four key algorithmic components: initial code drafting, neighborhood code generation, candidate evaluation, and incumbent code updating, each of which can be instantiated with specific decision rules to realize different local search algorithms such as Hill Climbing (HC) or Genetic Algorithm (GA). Furthermore, we develop a specialized revision reward model that evaluates code quality based on revision distance to produce fine-grained preferences that guide the local search toward more promising candidates. Finally, our extensive experimental results demonstrate that our approach achieves superior performance across diverse code generation tasks, significantly outperforming both construction-based tree search as well as the state-of-the-art improvement-based code generation methods.",
      "arxiv_url": "https://openreview.net/forum?id=sYk6ZMmrOz",
      "pdf_url": "https://openreview.net/pdf/52688ad24167833c260819018cd9622c9fc26998.pdf",
      "primary_category": "LLM, Code generation, Local Search",
      "categories": [
        "LLM",
        "Code generation",
        "Local Search"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1SCMFCGliM",
      "title": "The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models",
      "authors": [
        "Ke Ji",
        "Jiahao Xu",
        "Tian Liang",
        "Qiuzhi Liu",
        "Zhiwei He",
        "Xiaoyuan Liu",
        "Xingyu Chen",
        "Junying Chen",
        "Benyou Wang",
        "Zhaopeng Tu",
        "Haitao Mi",
        "Dong Yu"
      ],
      "abstract": "Improving the reasoning capabilities of large language models (LLMs) typically requires supervised fine-tuning with labeled data or computationally expensive sampling. We introduce Unsupervised Prefix Fine-Tuning (UPFT), which leverages the observation of Prefix Self-Consistency -- the shared initial reasoning steps across diverse solution trajectories -- to enhance LLM reasoning efficiency. By training exclusively on the initial prefix substrings (as few as 8 tokens), UPFT  removes the need for labeled data or exhaustive sampling. Experiments on reasoning benchmarks show that UPFT matches the performance of supervised methods such as Rejection Sampling Fine-Tuning, while reducing training time by 75\\% and sampling cost by 99\\%. Further analysis reveals that errors tend to appear in later stages of the reasoning process and that prefix-based training preserves the model’s structural knowledge. This work demonstrates how minimal unsupervised fine-tuning can unlock substantial reasoning gains in LLMs, offering a scalable and resource-efficient alternative to conventional approaches.",
      "arxiv_url": "https://openreview.net/forum?id=1SCMFCGliM",
      "pdf_url": "https://openreview.net/pdf/b261818533917595d3cba6ac4e58c4a6d02409ce.pdf",
      "primary_category": "Large Language Models; Efficient Reasoning; Unsupervised Learning",
      "categories": [
        "Large Language Models; Efficient Reasoning; Unsupervised Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Nf8yfPDFTl",
      "title": "SeerAttention: Self-distilled Attention Gating for Efficient Long-context Prefilling",
      "authors": [
        "Yizhao Gao",
        "Zhichen Zeng",
        "DaYou Du",
        "Shijie Cao",
        "Peiyuan Zhou",
        "Jiaxing Qi",
        "Junjie Lai",
        "Hayden Kwok-Hay So",
        "Ting Cao",
        "Fan Yang",
        "Mao Yang"
      ],
      "abstract": "Attention is the cornerstone of modern Large Language Models (LLMs). Yet its quadratic complexity hinders efficiency and scalability, especially for long-context processing. A promising approach is to leverage sparsity in attention.  However, existing sparsity-based solutions predominantly rely on predefined patterns or heuristics at the attention head level, struggling to adapt dynamically to different contexts efficiently. We propose SeerAttention, a simple yet effective attention mechanism that directly learns the block-level attention sparsity from the LLM itself. Inspired by the gating mechanism in Mixture of Experts (MoE), SeerAttention augments the conventional attention with a **learnable gate** that **selectively activates important blocks** within the attention map. Specifically, the gate first pools the query (Q) and key (K) tensors along the sequence dimension and processes them through learnable linear layers. The resulting matrices are then multiplied together to produce the gating scores, which are used to predict block-level attention sparsity. Combined with our block-sparse FlashAttention kernel, SeerAttention can achieve significant speedup on GPUs. When applied to pre-trained LLMs, SeerAttention only requires training the gate parameters in a lightweight self-distillation manner, allowing rapid convergence. Our evaluation results demonstrate that SeerAttention achieves better model accuracy and lower latency for long-context pre-filling compared to prior methods. Code is available at: https://github.com/microsoft/SeerAttention.",
      "arxiv_url": "https://openreview.net/forum?id=Nf8yfPDFTl",
      "pdf_url": "https://openreview.net/pdf/3fed457e271e8a060d1210ad2068e08fd91505c3.pdf",
      "primary_category": "LLM, Sparse Attention, Long-Context LLM",
      "categories": [
        "LLM",
        "Sparse Attention",
        "Long-Context LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "quJdphBcdP",
      "title": "WebDancer: Towards Autonomous Information Seeking Agency",
      "authors": [
        "Jialong Wu",
        "Baixuan Li",
        "Runnan Fang",
        "Wenbiao Yin",
        "Liwen Zhang",
        "Zhenglin Wang",
        "Zhengwei Tao",
        "Ding-Chu Zhang",
        "Zekun Xi",
        "Xiangru Tang",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Jingren Zhou"
      ],
      "abstract": "Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. \nRecent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. \nIn this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective.\nOur approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation.\nWe instantiate this framework in a web agent based on the ReAct format, WebDancer.\nEmpirical evaluations on the challenging GAIA and WebWalkerQA benchmarks demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. \nFurther analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models.",
      "arxiv_url": "https://openreview.net/forum?id=quJdphBcdP",
      "pdf_url": "https://openreview.net/pdf/7c886fbc63b09377d123254d93907b41820d72d7.pdf",
      "primary_category": "Web agent, Information seeking",
      "categories": [
        "Web agent",
        "Information seeking"
      ],
      "tags": [
        "LLM",
        "Agentic AI",
        "Search Agent"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "PuljHhCYxX",
      "title": "QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks",
      "authors": [
        "Qian Chen",
        "Linxin Yang",
        "Akang Wang",
        "Xiaodong Luo",
        "Yin Zhang"
      ],
      "abstract": "The combination of linear transformations and nonlinear activation functions forms the foundation of most modern deep neural networks, enabling them to approximate highly complex functions. This paper explores the introduction of quadratic transformations to further increase the nonlinearity of the model, with the aim of enhancing the performance of existing architectures. To minimize the additional parameters and computational burden, we propose a lightweight quadratic enhancer that leverages matrix decomposition, weight sharing, and sparsification techniques. This approach introduces only a minimal and negligible increase in parameters and forward computation, while still yielding substantial improvements in model performance. We evaluate the effectiveness of the proposed method across three tasks: text classification, image classification, and fine-tuning large language models (LLMs). In all tasks, our approach demonstrates significant performance gains.",
      "arxiv_url": "https://openreview.net/forum?id=PuljHhCYxX",
      "pdf_url": "https://openreview.net/pdf/cb48634526bab518ecdc27937e677136213bae1e.pdf",
      "primary_category": "Deep Learning, LLM",
      "categories": [
        "Deep Learning",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kgmyjyDFrx",
      "title": "Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers",
      "authors": [
        "Andrew Joohun Nam",
        "Henry Conklin",
        "Yukang Yang",
        "Thomas L. Griffiths",
        "Jonathan D. Cohen",
        "Sarah-Jane Leslie"
      ],
      "abstract": "We present causal head gating (CHG), a scalable method for interpreting the functional roles of attention heads in transformer models. CHG learns soft gates over heads and assigns them a causal taxonomy—facilitating, interfering, or irrelevant—based on their impact on task performance. Unlike prior approaches in mechanistic interpretability, which are hypothesis-driven and require prompt templates or target labels, CHG applies directly to any dataset using standard next-token prediction. We evaluate CHG across multiple large language models (LLMs) in the Llama 3 model family and diverse tasks, including syntax, commonsense, and mathematical reasoning, and show that CHG scores yield causal, not merely correlational, insight validated via ablation and causal mediation analyses. We also introduce contrastive CHG, a variant that isolates sub-circuits for specific task components. Our findings reveal that LLMs contain multiple sparse  task-sufficient sub-circuits, that individual head roles depend on interactions with others (low modularity), and that instruction following and in-context learning rely on separable mechanisms.",
      "arxiv_url": "https://openreview.net/forum?id=kgmyjyDFrx",
      "pdf_url": "https://openreview.net/pdf/5444e55ab5d63af1593b8f508b265fa19dbb823f.pdf",
      "primary_category": "mechanistic interpretability, large language models, causality",
      "categories": [
        "mechanistic interpretability",
        "large language models",
        "causality"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7dJfwHG3GN",
      "title": "Democratizing Clinical Risk Prediction with Cross-Cohort Cross-Modal Knowledge Transfer",
      "authors": [
        "Qiannan Zhang",
        "Manqi Zhou",
        "Zilong Bai",
        "Chang Su",
        "Fei Wang"
      ],
      "abstract": "Clinical risk prediction plays a crucial role in early disease detection and personalized intervention. While recent models increasingly incorporate multimodal data, their development typically assumes access to large-scale, multimodal datasets and substantial computational resources. In practice, however, most clinical sites operate under resource constraints, with access limited to  EHR data alone and insufficient capacity to train complicated models. This gap highlights the urgent need to democratize clinical risk prediction by enabling effective deployment in data- and resource-limited local clinical settings. In this work, we propose a cross-cohort cross-modal knowledge transfer framework that leverages the multimodal model trained on a nationwide cohort and adapts it to local cohorts with only EHR data. We focus on EHR and genetic data as representative multimodal inputs and address two key challenges. First, to mitigate the influence of noisy or less informative biological signals, we propose a novel mixture-of-aggregations design to enhance the modeling of informative and relevant genetic features. Second,  to support rapid model adaptation in low-resource sites, we develop a lightweight graph-guided fine-tuning method that adapts pretrained phenotypical EHR representations to target cohorts using limited patient data. \nExtensive experiments on real-world clinical data validate the effectiveness of our proposed model.",
      "arxiv_url": "https://openreview.net/forum?id=7dJfwHG3GN",
      "pdf_url": "https://openreview.net/pdf/5906082974ecc371959927e87a86e4faa1d94d60.pdf",
      "primary_category": "Risk Prediction, Cross-Cohort, Multimodal Learning",
      "categories": [
        "Risk Prediction",
        "Cross-Cohort",
        "Multimodal Learning"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "5ODlY0KYCx",
      "title": "On Learning Verifiers and Implications to Chain-of-Thought Reasoning",
      "authors": [
        "Maria Florina Balcan",
        "Avrim Blum",
        "Zhiyuan Li",
        "Dravyansh Sharma"
      ],
      "abstract": "Chain-of-Thought reasoning has emerged as a powerful approach for solving complex mathematical and logical problems. However, it can often veer off track through incorrect or unsubstantiated inferences. Formal mathematical reasoning, which can be checked with a formal verifier, is one approach to addressing this issue. However, currently LLMs are simply not good enough to solve complex problems in a formal way, and even just formalizing an informal problem statement can be challenging. Motivated by this fact, in this work we consider the problem of learning reliable verifiers for sequential reasoning, including natural language Chain-of-Thought reasoning. That is, given a problem statement and step-by-step solution in natural language, the aim of the verifier is to output [Yes] if the reasoning steps in the solution are all valid, and [No] otherwise. In this work we give a formal PAC-learning framework for studying this problem. We propose and analyze several natural verification goals, at different levels of strength, in this framework. We provide sample complexity upper-bounds for learning verifiers satisfying these goals, as well as lower-bound and impossibility results for learning other natural verification objectives without additional assumptions.",
      "arxiv_url": "https://openreview.net/forum?id=5ODlY0KYCx",
      "pdf_url": "https://openreview.net/pdf/ed32efbde75212493f3be56e58351acb701c4e5d.pdf",
      "primary_category": "Learning theory, LLM reasoning, Chain-of-Thought",
      "categories": [
        "Learning theory",
        "LLM reasoning",
        "Chain-of-Thought"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "FxV7Fvlm2T",
      "title": "CCL: Causal-aware In-context Learning for Out-of-Distribution Generalization",
      "authors": [
        "Hoyoon Byun",
        "Gyeongdeok Seo",
        "Joonseong Kang",
        "Taero Kim",
        "Jihee Kim",
        "Kyungwoo Song"
      ],
      "abstract": "In-context learning (ICL), a nonparametric learning method based on the knowledge of demonstration sets, has become a de facto standard for large language models (LLMs). The primary goal of ICL is to select valuable demonstration sets to enhance the performance of LLMs. Traditional ICL methods choose demonstration sets that share similar features with a given query. However, we have found that the performance of these traditional ICL approaches is limited on out-of-distribution (OOD) datasets, where the demonstration set and the query originate from different distributions. To ensure robust performance in OOD datasets, it is essential to learn causal representations that remain invariant between the source and target datasets. Inspired by causal representation learning, we propose causal-aware in-context learning (CCL). CCL captures the causal representations of a given dataset and selects demonstration sets that share similar causal features with the query. To achieve this, CCL employs a novel VAE-based causal representation learning technique. We demonstrate that CCL improves the OOD generalization performance of LLMs both theoretically and empirically. Code is available at: \\url{https://github.com/MLAI-Yonsei/causal-context-learning}",
      "arxiv_url": "https://openreview.net/forum?id=FxV7Fvlm2T",
      "pdf_url": "https://openreview.net/pdf/ab743ad3188898b7d800fb3c28380ab94704a893.pdf",
      "primary_category": "In-context learning, Causal representation, Out-of-Distribution Generalization",
      "categories": [
        "In-context learning",
        "Causal representation",
        "Out-of-Distribution Generalization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0SRGbRbngJ",
      "title": "Distributional LLM-as-a-Judge",
      "authors": [
        "Luyu Chen",
        "Zeyu Zhang",
        "Haoran Tan",
        "Quanyu Dai",
        "Hao Yang",
        "Zhenhua Dong",
        "Xu Chen"
      ],
      "abstract": "LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm, offering significant efficiency and flexibility compared to human judgments. However, previous methods primarily rely on single-point evaluations, overlooking the inherent diversity and uncertainty in human evaluations. This approach leads to information loss and decreases the reliability of evaluations. To address this limitation, we propose a novel training framework that explicitly aligns the LLM-generated judgment distribution with human evaluation distributions. Specifically, we propose a distributional alignment objective based on KL divergence, combined with an auxiliary cross-entropy regularization to stabilize the training process. Furthermore, due to limited human annotations, empirical human distributions are merely noisy estimates of the true underlying distribution. We therefore incorporate adversarial training to ensure a robust alignment with this true distribution, rather than overfitting to its imperfect approximation. Extensive experiments across various LLM backbones and evaluation tasks demonstrate that our framework significantly outperforms existing closed-source LLMs and conventional single-point alignment methods, with superior alignment quality, strong robustness, and competitive evaluation accuracy.",
      "arxiv_url": "https://openreview.net/forum?id=0SRGbRbngJ",
      "pdf_url": "https://openreview.net/pdf/54036a8556e6f5437313a9b7272f2925a5c7a795.pdf",
      "primary_category": "Large Language Models, LLM-as-a-Judge, Distributional Alignment",
      "categories": [
        "Large Language Models",
        "LLM-as-a-Judge",
        "Distributional Alignment",
        "Human Evaluation",
        "Adversarial Training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zPKeJAEo27",
      "title": "What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions",
      "authors": [
        "Sang Keun Choe",
        "Hwijeen Ahn",
        "Juhan Bae",
        "Kewen Zhao",
        "Youngseog Chung",
        "Adithya Pratapa",
        "Willie Neiswanger",
        "Emma Strubell",
        "Teruko Mitamura",
        "Jeff Schneider",
        "Eduard Hovy",
        "Roger Baker Grosse",
        "Eric P. Xing"
      ],
      "abstract": "Large language models (LLMs) are trained on a vast amount of human-written data, but data providers often remain uncredited. In response to this issue, data valuation (or data attribution), which quantifies the contribution or value of each data to the model output, has been discussed as a potential solution. Nevertheless, applying existing data valuation methods to recent LLMs and their vast training datasets has been largely limited by prohibitive compute and memory costs. In this work, we focus on influence functions, a popular gradient-based data valuation method, and significantly improve its scalability with an efficient gradient projection strategy called LoGra that leverages the gradient structure in backpropagation. We then provide a theoretical motivation of gradient projection approaches to influence functions to promote trust in the data valuation process. Lastly, we lower the barrier to implementing data valuation systems by introducing LogIX, a software package that can transform existing training code into data valuation code with minimal effort. In our data valuation experiments, LoGra achieves competitive accuracy against more expensive baselines while showing up to 6,500x improvement in throughput and 5x reduction in GPU memory usage when applied to Llama3-8B-Instruct and the 1B-token dataset.",
      "arxiv_url": "https://openreview.net/forum?id=zPKeJAEo27",
      "pdf_url": "https://openreview.net/pdf/11e62620a0483e1bd44bc46243a02f05213a57db.pdf",
      "primary_category": "data attribution, data valuation, influence functions",
      "categories": [
        "data attribution",
        "data valuation",
        "influence functions"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tdwRIP6NG2",
      "title": "Linearization Explains Fine-Tuning in Large Language Models",
      "authors": [
        "Zahra Rahimi Afzal",
        "Tara Esmaeilbeig",
        "Mojtaba Soltanalian",
        "Mesrob I Ohannessian"
      ],
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model. By making this explicit, using an $\\ell_2$-distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization. This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning. We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=tdwRIP6NG2",
      "pdf_url": "https://openreview.net/pdf/3f7d912bbff745e66f5f5732fb96acb75ebc7c59.pdf",
      "primary_category": "Parameter-Efficient Fine-tuning, LLMs, Neural Tangent Kernel",
      "categories": [
        "Parameter-Efficient Fine-tuning",
        "LLMs",
        "Neural Tangent Kernel",
        "Linearization",
        "Low Rank Adaptation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RespmwOoCH",
      "title": "Learning Interestingness in Automated Mathematical Theory Formation",
      "authors": [
        "George Tsoukalas",
        "Rahul Saha",
        "Amitayush Thakur",
        "Sabrina Reguyal",
        "Swarat Chaudhuri"
      ],
      "abstract": "We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce Fermat, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through Fermat: automatically scoring the interestingness of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the \\fermat environment at github.com/trishullab/Fermat.",
      "arxiv_url": "https://openreview.net/forum?id=RespmwOoCH",
      "pdf_url": "https://openreview.net/pdf/40977eb9704011a9a0cf3f17dbe9ef740146b9f4.pdf",
      "primary_category": "automated mathematics, AI for math, interestingness",
      "categories": [
        "automated mathematics",
        "AI for math",
        "interestingness",
        "reinforcement learning",
        "scientific discovery",
        "theory discovery"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yQoHUijSHx",
      "title": "DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm",
      "authors": [
        "Xiaowei Zhu",
        "Yubing Ren",
        "Fang Fang",
        "Qingfeng Tan",
        "Shi Wang",
        "Yanan Cao"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) has blurred the line between AI-generated and human-written text. This progress brings societal risks such as misinformation, authorship ambiguity, and intellectual property concerns, highlighting the urgent need for reliable AI-generated text detection methods. However, recent advances in generative language modeling have resulted in significant overlap between the feature distributions of human-written and AI-generated text, blurring classification boundaries and making accurate detection increasingly challenging. To address the above challenges, we propose a DNA-inspired perspective, leveraging a repair-based process to directly and interpretably capture the intrinsic differences between human-written and AI-generated text. Building on this perspective, we introduce **DNA-DetectLLM**, a zero-shot detection method for distinguishing AI-generated and human-written text. The method constructs an ideal AI-generated sequence for each input, iteratively repairs non-optimal tokens, and quantifies the cumulative repair effort as an interpretable detection signal. Empirical evaluations demonstrate that our method achieves state-of-the-art detection performance and exhibits strong robustness against various adversarial attacks and input lengths. Specifically, DNA-DetectLLM achieves relative improvements of **5.55\\%** in AUROC and **2.08\\%** in F1 score across multiple public benchmark datasets. Code and data are available at https://github.com/Xiaoweizhu57/DNA-DetectLLM.",
      "arxiv_url": "https://openreview.net/forum?id=yQoHUijSHx",
      "pdf_url": "https://openreview.net/pdf/ded229f3053d45a40a62bcf9a88a926f4f3d864f.pdf",
      "primary_category": "AI-generated Text Detection",
      "categories": [
        "AI-generated Text Detection"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9AHkbALT2t",
      "title": "Task-Specific Data Selection for Instruction Tuning via Monosemantic Neuronal Activations",
      "authors": [
        "Da Ma",
        "Gonghu Shang",
        "Zhi Chen",
        "Libo Qin",
        "Yijie LUO",
        "Hongshen Xu",
        "Lei Pan",
        "Shuai Fan",
        "Kai Yu",
        "Lu Chen"
      ],
      "abstract": "Instruction tuning improves the ability of large language models (LLMs) to follow diverse human instructions, but achieving strong performance on specific target tasks remains challenging. A critical bottleneck is selecting the most relevant data to maximize task-specific performance. Existing data selection approaches include unstable influence-based methods and more stable distribution alignment methods, the latter of which critically rely on the underlying sample representation. In practice, most distribution alignment methods, from shallow features (e.g., BM25) to neural embeddings (e.g., BGE, LLM2Vec), may fail to capture how the model internally processes samples. To bridge this gap, we adopt a model-centric strategy in which each sample is represented by its neuronal activation pattern in the model, directly reflecting internal computation. However, directly using raw neuron activations leads to spurious similarity between unrelated samples due to neuron polysemanticity, where a single neuron may respond to multiple, unrelated concepts. To address this, we employ sparse autoencoders to disentangle polysemantic activations into sparse, monosemantic representations, and introduce a dedicated similarity metric for this space to better identify task-relevant data. Comprehensive experiments across multiple instruction datasets, models, tasks, and selection ratios show that our approach consistently outperforms existing data selection baselines in both stability and task-specific performance.",
      "arxiv_url": "https://openreview.net/forum?id=9AHkbALT2t",
      "pdf_url": "https://openreview.net/pdf/ef7463c277d74ff73ce7c3de94d0b5a4d6320bb7.pdf",
      "primary_category": "Data selection, Instruction tuning, Sparse Autoencoder",
      "categories": [
        "Data selection",
        "Instruction tuning",
        "Sparse Autoencoder",
        "Neuronal activation states"
      ],
      "tags": [
        "LLM",
        "Information Retrieval"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "OBaK9JSbHk",
      "title": "On Evaluating LLM Alignment by Evaluating LLMs as Judges",
      "authors": [
        "Yixin Liu",
        "Pengfei Liu",
        "Arman Cohan"
      ],
      "abstract": "Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment.  In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle (GPT-4o). Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.",
      "arxiv_url": "https://openreview.net/forum?id=OBaK9JSbHk",
      "pdf_url": "https://openreview.net/pdf/cefd5dffe2958e9dbfba77cc4764ee04a8cc5a95.pdf",
      "primary_category": "alignment evaluation, LLM-as-a-judge, LLM evaluation",
      "categories": [
        "alignment evaluation",
        "LLM-as-a-judge",
        "LLM evaluation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pQ8DeHXKMh",
      "title": "Are Large Language Models Sensitive to the Motives Behind Communication?",
      "authors": [
        "Addison J. Wu",
        "Ryan Liu",
        "Kerem Oktar",
        "Theodore Sumers",
        "Thomas L. Griffiths"
      ],
      "abstract": "Human communication is $\\textit{motivated}$: people speak, write, and create content with a particular communicative intent in mind. As a result, information that large language models (LLMs) and AI agents process is inherently framed by humans' intentions and incentives. People are adept at navigating such nuanced information: we routinely identify benevolent or self-serving motives in order to decide what statements to trust. For LLMs to be effective in the real world, they too must critically evaluate content by factoring in the motivations of the source---for instance, weighing the credibility of claims made in a sales pitch. In this paper, we undertake a comprehensive study of whether LLMs have this capacity for $\\textit{motivational vigilance}$. We first employ controlled experiments from cognitive science to verify that LLMs' behavior is consistent with rational models of learning from motivated testimony, and find they successfully discount information from biased sources in a human-like manner. We then extend our evaluation to sponsored online adverts, a more naturalistic reflection of LLM agents' information ecosystems. In these settings, we find that LLMs' inferences do not track the rational models' predictions nearly as closely---partly due to additional information that distracts them from vigilance-relevant considerations. However, a simple steering intervention that boosts the salience of intentions and incentives substantially increases the correspondence between LLMs and the rational model. These results suggest that LLMs possess a basic sensitivity to the motivations of others, but generalizing to novel real-world settings will require further improvements to these models.",
      "arxiv_url": "https://openreview.net/forum?id=pQ8DeHXKMh",
      "pdf_url": "https://openreview.net/pdf/9cfeff2f5376e65886c4aadbb5c78f3656070ee8.pdf",
      "primary_category": "epistemic vigilance, cognitive science, psychology",
      "categories": [
        "epistemic vigilance",
        "cognitive science",
        "psychology",
        "large language models"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ARJpQtLXfe",
      "title": "Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models",
      "authors": [
        "Jiaqi Cao",
        "Jiarui Wang",
        "Rubin Wei",
        "Qipeng Guo",
        "Kai Chen",
        "Bowen Zhou",
        "Zhouhan Lin"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong abilities in general language tasks, yet adapting them to specific domains remains a challenge.\nCurrent method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter training and suffers from catastrophic forgetting.\nMeanwhile, Retrieval-Augmented Generation (RAG) introduces substantial inference latency due to expensive nearest-neighbor searches and longer context.\nThis paper introduces \\textit{Memory Decoder}, a plug-and-play pretrained memory that enables efficient domain adaptation without changing the original model's parameters.\nMemory Decoder employs a small transformer decoder that learns to imitate the behavior of an external non-parametric retriever.\nOnce trained, Memory Decoder can be seamlessly integrated with any pretrained language model that shares the same tokenizer, requiring no model-specific modifications.\nExperimental results demonstrate that Memory Decoder enables effective adaptation of various Qwen and Llama models to three distinct specialized domains: biomedicine, finance, and law, reducing perplexity by an average of 6.17 points.\nOverall, Memory Decoder introduces a novel paradigm centered on a specially pretrained memory component designed for domain-specific adaptation. This memory architecture can be integrated in a plug-and-play manner, consistently enhancing performance across multiple models within the target domain.",
      "arxiv_url": "https://openreview.net/forum?id=ARJpQtLXfe",
      "pdf_url": "https://openreview.net/pdf/898dc44e5cad3723cfff36993deaafd1b12c7924.pdf",
      "primary_category": "large language models, domain adaptation, retrieval-augmented generation",
      "categories": [
        "large language models",
        "domain adaptation",
        "retrieval-augmented generation"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "95plu1Mo20",
      "title": "Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs",
      "authors": [
        "Yifan Zhou",
        "Sachin Grover",
        "Mohamed El Mistiri",
        "Kamalesh Kalirathinam",
        "Pratyush Kerhalkar",
        "Swaroop Mishra",
        "Neelesh Kumar",
        "Sanket Gaurav",
        "Oya Aran",
        "Heni Ben Amor"
      ],
      "abstract": "Reinforcement Learning (RL) traditionally relies on scalar reward signals, limiting its ability to leverage the rich semantic knowledge often available in real-world tasks. In contrast, humans learn efficiently by combining numerical feedback with language, prior knowledge, and common sense. We introduce Prompted Policy Search (ProPS), a novel RL method that unifies numerical and linguistic reasoning within a single framework. Unlike prior work that augments existing RL components with language, ProPS places a large language model (LLM) at the center of the policy optimization loop—directly proposing policy updates based on both reward feedback and natural language input. We show that LLMs can perform numerical optimization in-context, and that incorporating semantic signals, such as goals, constraints, and strategy hints can lead to more informed exploration and sample-efficient learning. ProPS is evaluated across 15 Gymnasium tasks, spanning classic control, Atari games, and MuJoCo environments, and compared to seven widely-adopted RL algorithms (e.g., PPO, SAC, TRPO). It outperforms all baselines on 8 out of 15 tasks and demonstrates substantial gains when provided with domain knowledge. These results highlight the potential of unifying semantics and numerics for transparent, generalizable, and human-aligned reinforcement learning.",
      "arxiv_url": "https://openreview.net/forum?id=95plu1Mo20",
      "pdf_url": "https://openreview.net/pdf/f42c99bcdacf12fa97454f9d0e2deeb71bcc631a.pdf",
      "primary_category": "Reinforcement Learning, LLM, In-Context Learning",
      "categories": [
        "Reinforcement Learning",
        "LLM",
        "In-Context Learning",
        "Linguistic and Numerical Optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "5Qe7AGO3Eq",
      "title": "Multipole Attention for Efficient Long Context Reasoning",
      "authors": [
        "Coleman Richard Charles Hooper",
        "Sebastian Zhao",
        "Luca Manolache",
        "Sehoon Kim",
        "Michael W. Mahoney",
        "Sophia Shao",
        "Kurt Keutzer",
        "Amir Gholami"
      ],
      "abstract": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on complex problem-solving tasks. While these models have attained high accuracy by leveraging additional computation at test time, they need to generate long chain-of-thought reasoning in order to think before answering, which requires generating thousands of tokens.\nWhile sparse attention methods can help reduce the KV cache pressure induced by this long autoregressive reasoning, these methods can introduce errors which disrupt the reasoning process.\nOur work addresses these challenges by introducing Multipole Attention, which accelerates autoregressive reasoning by only computing exact attention for the most important tokens, while maintaining approximate representations for the remaining tokens. \nOur method first performs clustering to group together semantically similar key vectors, and then uses the cluster centroids both to identify important key vectors and to approximate the remaining key vectors in order to retain high accuracy.\nAdditionally, in order to accelerate long generation tasks, we design a fast cluster update process to quickly re-cluster the input and previously generated tokens, thereby allowing for  accelerating attention to the previous output tokens.\nWe evaluate our method using emerging LRMs such as Qwen-8B and Deepseek-R1-Distil-Qwen2.5-14B, demonstrating that our approach can maintain accuracy on complex reasoning tasks even with aggressive attention sparsity settings.\nWe also provide kernel implementations to demonstrate the practical efficiency gains from our method, achieving up to 4.5$\\times$ speedup for attention in long-context reasoning applications.",
      "arxiv_url": "https://openreview.net/forum?id=5Qe7AGO3Eq",
      "pdf_url": "https://openreview.net/pdf/4a59f2871b0a32683974881a2aae4cc7fdc39173.pdf",
      "primary_category": "Long Context, Inference, LLM",
      "categories": [
        "Long Context",
        "Inference",
        "LLM",
        "Reasoning",
        "Efficiency",
        "ML Systems",
        "Attention"
      ],
      "tags": [
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qwwPhjDea0",
      "title": "Transforming Generic Coder LLMs to Effective Binary Code Embedding Models for Similarity Detection",
      "authors": [
        "Litao Li",
        "Leo Song",
        "Steven Ding",
        "Benjamin C. M. Fung",
        "Philippe Charland"
      ],
      "abstract": "Cybersecurity and software research have crossed paths with modern deep learning research for a few years. The power of large language models (LLMs) in particular has intrigued us to apply them to understanding binary code. In this paper, we investigate some of the many ways LLMs can be applied to binary code similarity detection, as it is a significantly more difficult task compared to source code similarity detection due to the sparsity of information and less meaningful syntax. It also has great practical implications, such as vulnerability and malware detection. We find that pretrained LLMs are mostly capable of detecting similar binary code, even with a zero-shot setting. Our main contributions and findings are to provide several supervised fine-tuning methods that, when combined, significantly surpass zero-shot LLMs and state-of-the-art binary code similarity detection methods. Specifically, we up-train the model through data augmentation, translation-style causal learning, LLM2Vec, and cumulative GTE loss. With a complete ablation study, we show that our training method can transform a generic language model into a powerful binary similarity expert, and is also robust and general enough for cross-optimization, cross-architecture, and cross-obfuscation detection.",
      "arxiv_url": "https://openreview.net/forum?id=qwwPhjDea0",
      "pdf_url": "https://openreview.net/pdf/176990ce53523821d1d1aa927fa40813fa17e8c2.pdf",
      "primary_category": "cybersecurity, embedding, binary code",
      "categories": [
        "cybersecurity",
        "embedding",
        "binary code"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qPQUrjiA0q",
      "title": "Rollout Roulette: A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods",
      "authors": [
        "Isha Puri",
        "Shivchander Sudalairaj",
        "Guangxuan Xu",
        "Abhishek Bhandwaldar",
        "Kai Xu",
        "Akash Srivastava"
      ],
      "abstract": "Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. \nHowever, recent evidence suggests diminishing returns from such approaches, motivating a pivot to scaling test-time compute.\nExisting deterministic inference-time scaling methods, usually with reward models, cast the task as a search problem, but suffer from a key limitation: early pruning. Due to inherently imperfect reward models, promising trajectories may be discarded prematurely, leading to suboptimal performance. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods. Our method maintains a diverse set of candidates and robustly balances exploration and exploitation. \nOur empirical evaluation demonstrates that our particle filtering methods have a 4--16x better scaling rate over deterministic search counterparts on both various challenging mathematical and more general reasoning tasks. \nUsing our approach, we show that Qwen2.5-Math-1.5B-Instruct surpasses GPT-4o accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts.\nOur work not only presents an effective method to inference-time scaling, but also connects rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work.",
      "arxiv_url": "https://openreview.net/forum?id=qPQUrjiA0q",
      "pdf_url": "https://openreview.net/pdf/090bb2e83db793fe8968ee17023c3e5e982c9028.pdf",
      "primary_category": "Large Language Models, Inference Scaling, Particle Filtering",
      "categories": [
        "Large Language Models",
        "Inference Scaling",
        "Particle Filtering"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3oQDkmW72a",
      "title": "Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models",
      "authors": [
        "Simeng Han",
        "Howard Dai",
        "Stephen Xia",
        "Grant Zhang",
        "Chen Liu",
        "Lichang Chen",
        "Hoang H Nguyen",
        "Hongyuan Mei",
        "Jiayuan Mao",
        "R. Thomas McCoy"
      ],
      "abstract": "Accuracy remains a standard metric for evaluating AI systems, but it offers limited insight into how models arrive at their solutions. \nIn this work, we introduce a benchmark based on brainteasers written in long narrative form to probe more deeply into the types of reasoning strategies that models use. Brainteasers are well-suited for this goal because they can be solved with multiple approaches, such as a few-step solution that uses a creative insight or a longer solution that uses more brute force. \nWe investigate large language models (LLMs) across multiple layers of reasoning, focusing not only on correctness but also on the quality and creativity of their solutions. \nWe investigate many aspects of the reasoning process: (1) semantic parsing of the brainteasers into precise mathematical competition style formats; (2) self-correcting solutions based on gold solutions; (3) producing step-by-step sketches of solutions; and (4) making use of hints.\nWe find that LLMs are in many cases able to find creative, insightful solutions to brainteasers, suggesting that they capture some of the capacities needed to solve novel problems in creative ways. Nonetheless, there also remain situations where they rely on brute force despite the availability of more efficient, creative solutions, highlighting a potential direction for improvement in the reasoning abilities of LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=3oQDkmW72a",
      "pdf_url": "https://openreview.net/pdf/bc154619c4195b6a62774c122f1706e4d7b1bb7f.pdf",
      "primary_category": "Brainteaser; Reasoning LLMs",
      "categories": [
        "Brainteaser; Reasoning LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "mzlwDAQkgJ",
      "title": "Behavior Injection: Preparing Language Models for Reinforcement Learning",
      "authors": [
        "Zhepeng Cen",
        "Yihang Yao",
        "William Han",
        "Zuxin Liu",
        "Ding Zhao"
      ],
      "abstract": "Reinforcement learning (RL) has emerged as a powerful post-training technique to incentivize the reasoning ability of large language models (LLMs). However, LLMs can respond very inconsistently to RL finetuning: some show substantial performance gains, while others plateau or even degrade. To understand this divergence, we analyze the per-step influence of the RL objective and identify two key conditions for effective post-training: (1) RL-informative rollout accuracy, and (2) strong data co-influence, which quantifies how much the training data affects performance on other samples. Guided by these insights, we propose behavior injection, a task-agnostic data augmentation scheme applied prior to RL. Behavior injection enriches the supervised finetuning (SFT) data by seeding exploratory and exploitative behaviors, effectively making the model more RL-ready. We evaluate our method across two reasoning benchmarks with multiple base models. The results demonstrate that our theoretically motivated augmentation can significantly increase the performance gain from RL over the pre-RL model.",
      "arxiv_url": "https://openreview.net/forum?id=mzlwDAQkgJ",
      "pdf_url": "https://openreview.net/pdf/73a44a19b2ee2b2c924ad9d28e6a122fdd777254.pdf",
      "primary_category": "Reinforcement Learning, Large Language Model, Data augmantation",
      "categories": [
        "Reinforcement Learning",
        "Large Language Model",
        "Data augmantation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "LTgUInLTbP",
      "title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution",
      "authors": [
        "Fengxiang Wang",
        "Mingshuo Chen",
        "Yueying Li",
        "Di Wang",
        "Haotian Wang",
        "Zonghao Guo",
        "Zefan Wang",
        "Shan Boqi",
        "Long Lan",
        "Yulin Wang",
        "Hongzhen Wang",
        "Wenjing Yang",
        "Bo Du",
        "Jing Zhang"
      ],
      "abstract": "Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data for Earth observation but pose challenges for existing multimodal foundation models due to two key bottlenecks: (1) limited availability of UHR training data, and (2) token explosion caused by the large image size. To address data scarcity, we introduce **SuperRS-VQA** (avg. 8,376$\\times$8,376) and **HighRS-VQA** (avg. 2,000$\\times$1,912), the highest-resolution vision-language datasets in RS to date, covering 22 real-world dialogue tasks. To mitigate token explosion, our pilot studies reveal significant redundancy in RS images: crucial information is concentrated in a small subset of object-centric tokens, while pruning background tokens (e.g., ocean or forest) can even improve performance. Motivated by these findings, we propose two strategies: *Background Token Pruning* and *Anchored Token Selection*, to reduce the memory footprint while preserving key semantics. Integrating these techniques, we introduce **GeoLLaVA-8K**, the first RS-focused multimodal large language model capable of handling inputs up to 8K$\\times$8K resolution, built on the LLaVA framework. Trained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art on the XLRS-Bench. Datasets and code were released at https://github.com/MiliLab/GeoLLaVA-8K.",
      "arxiv_url": "https://openreview.net/forum?id=LTgUInLTbP",
      "pdf_url": "https://openreview.net/pdf/d8a7185abcf4204532a860bdabb45a2c9bb10919.pdf",
      "primary_category": "Remote Sensing, MLLM, Vision-language Dataset",
      "categories": [
        "Remote Sensing",
        "MLLM",
        "Vision-language Dataset",
        "Ultra-High-Resolution"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "AuBSUgFVgq",
      "title": "Multimodal Tabular Reasoning with Privileged Structured Information",
      "authors": [
        "Jun-Peng Jiang",
        "Yu Xia",
        "Hai-Long Sun",
        "Shiyin Lu",
        "Qing-Guo Chen",
        "Weihua Luo",
        "Kaifu Zhang",
        "De-Chuan Zhan",
        "Han-Jia Ye"
      ],
      "abstract": "Tabular reasoning requires complex, multi-step information extraction and logical inference, such as aggregation, comparison, or calculation over tabular data. While recent advances have leveraged large language models (LLMs) for reasoning over structured text tables, such high-quality textual representations are often unavailable in real-world settings, where tables typically appear as images. In this paper, we tackle the task of tabular reasoning directly from table images. Our core strategy is to leverage privileged structured information---specifically, the ground-truth structured table data available during training but inaccessible at test time---to enhance multimodal large language models (MLLMs). The key challenges lie in: accurately aligning visual representations with the structured information, particularly mapping the visual evidence to logical steps; and effectively transferring the reasoning skills learned during training to the MLLM for visual inference. \nTo address these, we introduce {\\sc Turbo} (TabUlar Reasoning with Bridged infOrmation), a new framework for multimodal tabular reasoning using privileged information. {\\sc Turbo} benefits from a structure-aware reasoning trace generator based on DeepSeek-R1, which contributes to high-quality modality-bridged information. On this basis, {\\sc Turbo} repeatedly generates and selects advantageous reasoning traces, further enhancing the model's tabular reasoning ability. Experimental results demonstrate that, with limited (9k) data, {\\sc Turbo} achieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across multiple datasets.",
      "arxiv_url": "https://openreview.net/forum?id=AuBSUgFVgq",
      "pdf_url": "https://openreview.net/pdf/c140cd5b9ba2e6981d9024c35dbfc29dc881e63b.pdf",
      "primary_category": "Tabular Reasoning; Multimodal Large Language Models; Multimodal Tabular Reasoning;",
      "categories": [
        "Tabular Reasoning; Multimodal Large Language Models; Multimodal Tabular Reasoning;"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pG1Y63MqHm",
      "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
      "authors": [
        "Chenxing Wei",
        "Jiarui Yu",
        "Ying Tiffany He",
        "Hande Dong",
        "Yao Shu",
        "Fei Yu"
      ],
      "abstract": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages.",
      "arxiv_url": "https://openreview.net/forum?id=pG1Y63MqHm",
      "pdf_url": "https://openreview.net/pdf/78a0c619925e0a59ab6470b3e890db473cf871f8.pdf",
      "primary_category": "Discrete Rewards, Reward Signal Perturbation, Gradient Optimization",
      "categories": [
        "Discrete Rewards",
        "Reward Signal Perturbation",
        "Gradient Optimization",
        "Convergence Speed"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "hSX7Dd8dxy",
      "title": "Inference-Time Reward Hacking in Large Language Models",
      "authors": [
        "Hadi Khalaf",
        "Claudio Mayrink Verdun",
        "Alex Oesterling",
        "Himabindu Lakkaraju",
        "Flavio Calmon"
      ],
      "abstract": "A common paradigm to improve the performance of large language models is optimizing for a reward model. Reward models assign a numerical score to an LLM’s output that indicates, for example, how likely it is to align with user preferences or safety goals. However, reward models are never perfect. They inevitably function as proxies for  complex desiderata such as correctness, helpfulness, and safety. By overoptimizing for a misspecified reward, we can subvert intended alignment goals and reduce overall performance -- a phenomenon commonly referred to as reward hacking. In this work, we characterize reward hacking in inference-time alignment and demonstrate when and how we can mitigate it by hedging on the proxy reward. We study this phenomenon under Best-of-$n$ (BoN) and Soft Best-of-$n$ (SBoN), and we introduce Best-of-Poisson (BoP) that provides an efficient, near-exact approximation of the optimal reward-KL divergence policy at inference time. We show that the characteristic pattern of hacking as observed in practice (where the true reward first increases before declining) is an inevitable property of a broad class of inference-time mechanisms, including BoN and BoP. To counter this effect, we introduce $\\texttt{HedgeTune}$, an efficient algorithm to find the optimal inference-time parameter. We demonstrate that hedging mitigates reward hacking and achieves superior reward-distortion tradeoffs on math, reasoning, and human-preference setups.",
      "arxiv_url": "https://openreview.net/forum?id=hSX7Dd8dxy",
      "pdf_url": "https://openreview.net/pdf/8d477ae3e38043ed738ea2aa59e110eec3f49a44.pdf",
      "primary_category": "reward hacking, large language models, inference time alignment",
      "categories": [
        "reward hacking",
        "large language models",
        "inference time alignment",
        "information theory"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yFfWVr2TmZ",
      "title": "Learning “Partner-Aware” Collaborators in Multi-Party Collaboration",
      "authors": [
        "Abhijnan Nath",
        "Nikhil Krishnaswamy"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being deployed in agentic settings where they act as collaborators with humans. Therefore, it is increasingly important to be able to evaluate their abilities to collaborate effectively in multi-turn, multi-party tasks. In this paper, we build on the AI alignment and “safe interruptability” literature to offer novel theoretical insights on collaborative behavior between LLM-driven *collaborator agents* and an *intervention agent*. Our goal is to learn an ideal “partner-aware” collaborator that increases the group’s common-ground (CG)—alignment on task-relevant propositions—by intelligently collecting information provided in *interventions* by a partner agent. We show how LLM agents trained using standard RLHF and related approaches are naturally inclined to ignore possibly well-meaning interventions, which makes increasing group common ground non-trivial in this setting. We employ a two-player Modified-Action MDP to examine this suboptimal behavior of standard AI agents, and propose **Interruptible Collaborative Roleplayer (ICR)**—a novel “partner-aware” learning algorithm to train CG-optimal collaborators. Experiments on multiple collaborative task environments show that ICR, on average, is more capable of promoting successful CG convergence and exploring more diverse solutions in such tasks.",
      "arxiv_url": "https://openreview.net/forum?id=yFfWVr2TmZ",
      "pdf_url": "https://openreview.net/pdf/6d47c6efb29b004ee9331bb2556092337fc197db.pdf",
      "primary_category": "alignment, LLM agents, collaboration",
      "categories": [
        "alignment",
        "LLM agents",
        "collaboration"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kKPP6q5vBs",
      "title": "Generalization vs Specialization under Concept Shift",
      "authors": [
        "Alex Nguyen",
        "David J. Schwab",
        "Vudtiwat Ngampruetikorn"
      ],
      "abstract": "Machine learning models are often brittle under distribution shift, i.e., when data distributions at test time differ from those during training. Understanding this failure mode is central to identifying and mitigating safety risks of mass adoption of machine learning. Here we analyze ridge regression under concept shift—a form of distribution shift in which the input-label relationship changes at test time. We derive an exact expression for prediction risk in the thermodynamic limit. Our results reveal nontrivial effects of concept shift on generalization performance, including a phase transition between weak and strong concept shift regimes and nonmonotonic data dependence of test performance even when double descent is absent. Our theoretical results are in good agreement with experiments based on transformers pretrained to solve linear regression; under concept shift, too long context length can be detrimental to generalization performance of next token prediction. Finally, experiments on MNIST and FashionMNIST further validate our theoretical predictions, suggesting these phenomena represent a fundamental aspect of learning under distribution shift.",
      "arxiv_url": "https://openreview.net/forum?id=kKPP6q5vBs",
      "pdf_url": "https://openreview.net/pdf/273b0e0d569a6035ffef3242486d79f2a6528d36.pdf",
      "primary_category": "concept shift, distribution shift, ridge regression",
      "categories": [
        "concept shift",
        "distribution shift",
        "ridge regression",
        "thermodynamic limit",
        "high-dimensional learning",
        "solvable model",
        "out-of-distribution generalization"
      ],
      "tags": [
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RBWnyDEBKf",
      "title": "Constant Bit-size Transformers Are Turing Complete",
      "authors": [
        "Qian Li",
        "Yuyi Wang"
      ],
      "abstract": "We prove that any Turing machine running on inputs of arbitrary length can be simulated by a constant bit-size transformer, as long as the context window is sufficiently long. This improves previous works, which require scaling up either the model's precision or the number of parameters on longer inputs. Furthermore, we prove that the complexity class SPACE$[s(n)]$ exactly characterizes the expressive power of a constant bit-size transformer with a context window of length $s(n)$. Our approach relies on simulating Post machines, a Turing-complete computational model. Post machines can be modeled as automata equipped with a queue, exhibiting computational behaviors naturally aligned with those of transformers. The behavioral similarity between transformers and Post machines may offer new insights into the mechanisms underlying the reasoning abilities of transformers.",
      "arxiv_url": "https://openreview.net/forum?id=RBWnyDEBKf",
      "pdf_url": "https://openreview.net/pdf/4cc2e30e643d570019883452633cf1da206d4de0.pdf",
      "primary_category": "Transformer, Turing complete, Post machine",
      "categories": [
        "Transformer",
        "Turing complete",
        "Post machine",
        "context window length",
        "space complexity"
      ],
      "tags": [
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "KaYMGsnZ4R",
      "title": "DINGO: Constrained Inference for Diffusion LLMs",
      "authors": [
        "Tarun Suresh",
        "Debangshu Banerjee",
        "Shubham Ugare",
        "Sasa Misailovic",
        "Gagandeep Singh"
      ],
      "abstract": "Diffusion LLMs have emerged as a promising alternative to conventional autoregressive LLMs, offering substantial potential for improving runtime efficiency. However, existing diffusion models fail to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. Unlike autoregressive models, which generate tokens sequentially, diffusion LLMs predict a block of tokens in parallel. This parallelism makes traditional constrained decoding algorithms, designed to enforce constraints with sequential token prediction, ineffective at preserving the true output distribution. To address this limitation, we propose DINGO, a dynamic programming-based constrained decoding strategy that is both efficient and provably distribution-preserving. DINGO enables sampling of output strings with the highest probability under the model’s predicted distribution while strictly adhering to any user-specified regular expression. On standard symbolic math and JSON generation benchmarks, DINGO achieves up to a $68$\\% points of improvement over unconstrained inference.  The code is available at [**DINGO**](https://github.com/uiuc-focal-lab/DINGO).",
      "arxiv_url": "https://openreview.net/forum?id=KaYMGsnZ4R",
      "pdf_url": "https://openreview.net/pdf/431affcf58926eb368d1a07a66bab58c196789ea.pdf",
      "primary_category": "Constrained Decoding, Diffusion LLM",
      "categories": [
        "Constrained Decoding",
        "Diffusion LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "GuvQJGgbLm",
      "title": "Let Me Think! A Long Chain of Thought Can Be Worth Exponentially Many Short Ones",
      "authors": [
        "Parsa Mirtaheri",
        "Ezra Edelman",
        "Samy Jelassi",
        "Eran Malach",
        "Enric Boix-Adserà"
      ],
      "abstract": "Inference-time computation has emerged as a promising scaling axis for improving large language model reasoning. However, despite yielding impressive performance, the optimal allocation of inference-time computation remains poorly understood. A central question is whether to prioritize sequential scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority voting across multiple short chains of thought). In this work, we seek to illuminate the landscape of test-time scaling by demonstrating the existence of reasoning settings where sequential scaling offers an exponential advantage over parallel scaling. These settings are based on graph connectivity problems in challenging distributions of graphs. We validate our theoretical findings with comprehensive experiments across a range of language models, including models trained from scratch for graph connectivity with different chain of thought strategies as well as large reasoning models.",
      "arxiv_url": "https://openreview.net/forum?id=GuvQJGgbLm",
      "pdf_url": "https://openreview.net/pdf/ec77edc924ff2f103b7a4163f0309f2e6ade7c71.pdf",
      "primary_category": "reasoning, chain-of-thought, test-time scaling",
      "categories": [
        "reasoning",
        "chain-of-thought",
        "test-time scaling",
        "large language models",
        "transformers",
        "expressivity"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZE9cxnEBpy",
      "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any Language",
      "authors": [
        "Yuxin Chen",
        "Yiran Zhao",
        "Yang Zhang",
        "An Zhang",
        "Kenji Kawaguchi",
        "Shafiq Joty",
        "Junnan Li",
        "Tat-Seng Chua",
        "Michael Qizhe Shieh",
        "Wenxuan Zhang"
      ],
      "abstract": "As large language models (LLMs) continue to advance, their capacity to function effectively across a diverse range of languages has shown marked improvement. Preliminary studies observe that the hidden activations of LLMs often resemble English, even when responding to non-English prompts. This has led to the widespread assumption that LLMs may ``think'' in English. However, more recent results showing strong multilingual performance, even surpassing English performance on specific tasks in other languages, challenge this view. In this work, we find that LLMs progressively develop a core language-agnostic parameter space—a remarkably small subset of parameters whose deactivation results in significant performance degradation across all languages. This compact yet critical set of parameters underlies the model’s ability to generalize beyond individual languages, supporting the emergence of abstract thought that is not tied to any specific linguistic system. Specifically, we identify language-related neurons—those are consistently activated during the processing of particular languages, and categorize them as either shared (active across multiple languages) or exclusive (specific to one). As LLMs undergo continued development over time, we observe a marked increase in both the proportion and functional importance of shared neurons, while exclusive neurons progressively diminish in influence. These shared neurons constitute the backbone of the core language-agnostic parameter space, supporting the emergence of abstract thought. Motivated by these insights, we propose neuron-specific training strategies tailored to LLMs' language-agnostic levels at different development stages. Experiments across diverse LLM families support our approach. Our codes are available at https://anonymous.4open.science/status/S-C393.",
      "arxiv_url": "https://openreview.net/forum?id=ZE9cxnEBpy",
      "pdf_url": "https://openreview.net/pdf/bcf33b577693b6cfc35d150ae493caa4f934a43f.pdf",
      "primary_category": "Large Language Models, Language-Agnostic Neuron, Abstract Thought",
      "categories": [
        "Large Language Models",
        "Language-Agnostic Neuron",
        "Abstract Thought"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Guar1tumDr",
      "title": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression",
      "authors": [
        "Qingyue Yang",
        "Jie Wang",
        "Xing Li",
        "Zhihai Wang",
        "Chen Chen",
        "Lei Chen",
        "Xianzhi Yu",
        "Wulong Liu",
        "Jianye HAO",
        "Mingxuan Yuan",
        "Bin Li"
      ],
      "abstract": "With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation.\nTo compress the KV cache, recent methods identify critical KV tokens through static modeling of attention scores. However, these methods often struggle to accurately determine critical tokens as they neglect the *temporal patterns* in attention scores, resulting in a noticeable degradation in LLM performance. \nTo address this challenge, we propose **AttentionPredictor**, which is the **first learning-based method to directly predict attention patterns for KV cache compression and critical token identification**. \nSpecifically, AttentionPredictor learns a lightweight, unified convolution model to dynamically capture spatiotemporal patterns and predict the next-token attention scores. An appealing feature of AttentionPredictor is that it accurately predicts the attention score and shares the unified prediction model, which consumes negligible memory, among all transformer layers. Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage. By retaining most of the attention information, AttentionPredictor achieves **13$\\times$** KV cache compression and **5.6$\\times$** speedup in a cache offloading scenario with comparable LLM performance, significantly outperforming the state-of-the-arts. The code is available at https://github.com/MIRALab-USTC/LLM-AttentionPredictor.",
      "arxiv_url": "https://openreview.net/forum?id=Guar1tumDr",
      "pdf_url": "https://openreview.net/pdf/652b5d6eb23b0fd1992ea3165902aff55e5c3e44.pdf",
      "primary_category": "Large Language Model, KV Cache Compression, Long Context",
      "categories": [
        "Large Language Model",
        "KV Cache Compression",
        "Long Context"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tirl2l9oKg",
      "title": "RAG4GFM: Bridging Knowledge Gaps in Graph Foundation Models through Graph Retrieval Augmented Generation",
      "authors": [
        "Xingliang Wang",
        "Zemin Liu",
        "Junxiao Han",
        "Shuiguang Deng"
      ],
      "abstract": "Graph Foundation Models (GFMs) have demonstrated remarkable potential across graph learning tasks but face significant challenges in knowledge updating and reasoning faithfulness. To address these issues, we introduce the Retrieval-Augmented Generation (RAG) paradigm for GFMs, which leverages graph knowledge retrieval. We propose RAG4GFM, an end-to-end framework that seamlessly integrates multi-level graph indexing, task-aware retrieval, and graph fusion enhancement. \nRAG4GFM implements a hierarchical graph indexing architecture, enabling multi-granular graph indexing while achieving efficient logarithmic-time retrieval. The task-aware retriever implements adaptive retrieval strategies for node, edge, and graph-level tasks to surface structurally and semantically relevant evidence. \nThe graph fusion enhancement module fuses retrieved graph features with query features and augments the topology with sparse adjacency links that preserve structural and semantic proximity, yielding a fused graph for GFM inference.\nExtensive experiments conducted across diverse GFM applications demonstrate that RAG4GFM significantly enhances both the efficiency of knowledge updating and reasoning faithfulness\\footnote{Code: \\url{https://github.com/Matrixmax/RAG4GFM}.}.",
      "arxiv_url": "https://openreview.net/forum?id=tirl2l9oKg",
      "pdf_url": "https://openreview.net/pdf/926b4d66404b4ccbdfda0dec62b279af4b119dc4.pdf",
      "primary_category": "Graph Foundation Model, Retrieval-Augmented Generation, Graph Index",
      "categories": [
        "Graph Foundation Model",
        "Retrieval-Augmented Generation",
        "Graph Index",
        "Graph Repersentation Learning"
      ],
      "tags": [
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "H8fscnm6Xx",
      "title": "Unextractable Protocol Models: Collaborative Training and Inference without Weight Materialization",
      "authors": [
        "Alexander Long",
        "Chamin P Hewa Koneputugodage",
        "Thalaiyasingam Ajanthan",
        "Yan Zuo",
        "Gil Avraham",
        "Violetta Shevchenko",
        "Hadi Mohaghegh Dolatabadi",
        "Sameera Ramasinghe"
      ],
      "abstract": "We consider a decentralized setup in which the participants collaboratively train and serve a large neural network, and where each participant only processes a subset of the model. \nIn this setup, we explore the possibility of unmaterializable weights, where a full weight set is never available to any one participant.\nWe introduce Unextractable Protocol Models (UPMs): a training and inference framework that leverages the sharded model setup to ensure model shards (i.e.,, subsets) held by participants are incompatible at different time steps. UPMs periodically inject time-varying, random, invertible transforms at participant boundaries; preserving the overall network function yet rendering cross-time assemblies incoherent.\nOn Qwen-2.5-0.5B and Llama-3.2-1B, 10 000 transforms leave FP32 perplexity unchanged ($\\Delta$PPL$< 0.01$; Jensen–Shannon drift $<4 \\times 10^{-5}$), and we show how to control growth for lower precision datatypes. Applying a transform every 30s adds 3% latency, 0.1% bandwidth, and 10% GPU-memory overhead at inference, while training overhead falls to 1.6% time and < 1% memory. We consider several attacks, showing that the requirements of direct attacks are impractical and easy to defend against, and that gradient-based fine-tuning of stitched partitions consumes $\\geq 60\\%$ of the tokens required to train from scratch. By enabling models to be collaboratively trained yet not extracted, UPMs make it practical to embed programmatic incentive mechanisms in community-driven decentralized training.",
      "arxiv_url": "https://openreview.net/forum?id=H8fscnm6Xx",
      "pdf_url": "https://openreview.net/pdf/9bd59f505312e8c30d97fe1cf65eef2102db573b.pdf",
      "primary_category": "Decentralized training, LLMs, Distributed training",
      "categories": [
        "Decentralized training",
        "LLMs",
        "Distributed training",
        "Open source",
        "Weight secrecy"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "KI8qan2EA7",
      "title": "ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs",
      "authors": [
        "Landon Butler",
        "Abhineet Agarwal",
        "Justin Singh Kang",
        "Yigit Efe Erginbas",
        "Bin Yu",
        "Kannan Ramchandran"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable performance by capturing complex interactions between input features. To identify these interactions, most existing approaches require enumerating all possible combinations of features up to a given order, causing them to scale poorly with the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an information-theoretic approach that uses interaction sparsity to scale to $n \\approx 10^3$ features. SPEX greatly improves upon prior methods but requires tens of thousands of model inferences, which can be prohibitive for large models. In this paper, we observe that LLM feature interactions are often *hierarchical*—higher-order interactions are accompanied by their lower-order subsets—which enables more efficient discovery. To exploit this hierarchy, we propose ProxySPEX, an interaction attribution algorithm that first fits gradient boosted trees to masked LLM outputs and then extracts the important interactions.   Experiments across four challenging high-dimensional datasets show that ProxySPEX more faithfully reconstructs LLM outputs by 20\\% over marginal attribution approaches while using *$10\\times$ fewer inferences* than SPEX. By accounting for interactions, ProxySPEX efficiently identifies the most influential features, providing a scalable approximation of their Shapley values. Further, we apply ProxySPEX to two interpretability tasks. *Data attribution*, where we identify interactions among CIFAR-10 training samples that influence test predictions, and *mechanistic interpretability*, where we uncover interactions between attention heads, both within and across layers, on a question-answering task. The ProxySPEX algorithm is available at <https://github.com/mmschlk/shapiq>.",
      "arxiv_url": "https://openreview.net/forum?id=KI8qan2EA7",
      "pdf_url": "https://openreview.net/pdf/08d7fc56afca28ad2ef1c0a22c6d0340dc3240c8.pdf",
      "primary_category": "Interpretability, Signal Processing, Hierarchy",
      "categories": [
        "Interpretability",
        "Signal Processing",
        "Hierarchy",
        "SHAP",
        "Interactions"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zLkpt30ngy",
      "title": "LLMs Encode Harmfulness and Refusal Separately",
      "authors": [
        "Jiachen Zhao",
        "Jing Huang",
        "Zhengxuan Wu",
        "David Bau",
        "Weiyan Shi"
      ],
      "abstract": "LLMs are trained to refuse harmful instructions, but do they truly understand\nharmfulness beyond just refusing? Prior work has shown that LLMs’ refusal\nbehaviors can be mediated by a one-dimensional subspace, i.e., a refusal direction.\nIn this work, we identify a new dimension to analyze safety mechanisms in LLMs,\ni.e., harmfulness, which is encoded internally as a separate concept from refusal.\nAnd there exists a harmfulness direction that is distinct from the refusal direction.\nAs causal evidence, steering along the harmfulness direction can lead LLMs to\ninterpret harmless instructions as harmful, but steering along the refusal direction\ntends to elicit refusal responses directly without reversing the model’s judgment on\nharmfulness. Furthermore, using our identified harmfulness concept, we find that\ncertain jailbreak methods work by reducing the refusal signals without suppressing\nthe model’s internal belief of harmfulness. We also find that adversarially fine-\ntuning models to accept harmful instructions has minimal impact on the model’s\ninternal belief of harmfulness. These insights lead to a practical safety application:\nThe model’s latent harmfulness representation can serve as an intrinsic safeguard\n(Latent Guard) for detecting unsafe inputs and reducing over-refusals that is\nrobust to finetuning attacks. For instance, our Latent Guard achieves performance\ncomparable to or better than Llama Guard 3 8B, a dedicated finetuned safeguard\nmodel, across different jailbreak methods. Our findings suggest that LLMs’\ninternal understanding of harmfulness is more robust than their refusal decision\nto diverse input instructions, offering a new perspective to study AI safety.",
      "arxiv_url": "https://openreview.net/forum?id=zLkpt30ngy",
      "pdf_url": "https://openreview.net/pdf/46ceb68a51e6b08727f8a10d08bb0797825edbc8.pdf",
      "primary_category": "Large language models; Interpretability; AI safety",
      "categories": [
        "Large language models; Interpretability; AI safety"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xeFZUrJSH7",
      "title": "Fairshare Data Pricing via Data Valuation for Large Language Models",
      "authors": [
        "Luyang Zhang",
        "Cathy Jiao",
        "Beibei Li",
        "Chenyan Xiong"
      ],
      "abstract": "Training data is the backbone of large language models (LLMs), yet today’s data markets often operate under exploitative pricing -- sourcing data from marginalized groups with little pay or recognition. This paper introduces a theoretical framework for LLM data markets, modeling the strategic interactions between buyers (LLM builders) and sellers (human annotators). \nWe begin with theoretical and empirical analysis showing how exploitative pricing drives high-quality sellers out of the market, degrading data quality and long-term model performance. \nThen we introduce fairshare, a pricing mechanism grounded in data valuation that quantifies each data’s contribution. It aligns incentives by sustaining seller participation and optimizing utility for both buyers and sellers. \nTheoretically, we show that fairshare yields mutually optimal outcomes: maximizing long-term buyer utility and seller profit while sustaining market participation. \nEmpirically when training open-source LLMs on complex NLP tasks, including math problems, medical diagnosis, and physical reasoning,\nfairshare boosts seller earnings and ensures a stable supply of high-quality data, while improving buyers’ performance-per-dollar and long-term welfare. Our findings offer a concrete path toward fair, transparent, and economically sustainable data markets for LLM. Our code will be open sourced.",
      "arxiv_url": "https://openreview.net/forum?id=xeFZUrJSH7",
      "pdf_url": "https://openreview.net/pdf/f134e3bb59701c06506172704b47affd6c1323fc.pdf",
      "primary_category": "data pricing, data market, large language model",
      "categories": [
        "data pricing",
        "data market",
        "large language model",
        "data valuation",
        "data attribution"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "bEYzeItoOH",
      "title": "BLEUBERI: BLEU is a surprisingly effective reward for instruction following",
      "authors": [
        "Yapei Chang",
        "Yekyung Kim",
        "Michael Krumdick",
        "Amir Zadeh",
        "Chuan Li",
        "Chris Tanner",
        "Mohit Iyyer"
      ],
      "abstract": "Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the question: can simpler, reference-based metrics serve as viable alternatives to reward models during RL-based alignment? In this paper, we show first that BLEU, a basic string-matching metric, surprisingly matches strong reward models in agreement with human preferences on general instruction-following datasets. Based on this insight, we develop BLEUBERI, a method that first identifies challenging instructions and then applies Group Relative Policy Optimization (GRPO) using BLEU directly as the reward function. We demonstrate that BLEUBERI-trained models are competitive with models trained via reward model-guided RL across four challenging instruction-following benchmarks and three different base language models. A human evaluation further supports that the quality of BLEUBERI model outputs is on par with those from reward model-aligned models. Moreover, BLEUBERI models generate outputs that are more factually grounded than competing methods. Overall, we show that given access to high-quality reference outputs (easily obtained via existing instruction-following datasets or synthetic data generation), string matching-based metrics are cheap yet effective proxies for reward models during alignment. We release our code and data at https://github.com/lilakk/BLEUBERI.",
      "arxiv_url": "https://openreview.net/forum?id=bEYzeItoOH",
      "pdf_url": "https://openreview.net/pdf/62e5a2c7ddd32238b72c8305a544a7429632dfa2.pdf",
      "primary_category": "alignment, instruction-following, LLM",
      "categories": [
        "alignment",
        "instruction-following",
        "LLM",
        "RLVR",
        "GRPO"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ugaepulZyA",
      "title": "Influence Guided Context Selection for Effective Retrieval-Augmented Generation",
      "authors": [
        "Jiale Deng",
        "Yanyan Shen",
        "Ziyuan Pei",
        "Youmin Chen",
        "Linpeng Huang"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) addresses large language model (LLM) hallucinations by grounding responses in external knowledge, but its effectiveness is compromised by poor-quality retrieved contexts containing irrelevant or noisy information. While existing approaches attempt to improve performance through context selection based on predefined context quality assessment metrics, they show limited gains over standard RAG. We attribute this limitation to their failure in holistically utilizing available information (query, context list, and generator) for comprehensive quality assessment. Inspired by recent advances in data selection, we reconceptualize context quality assessment as an inference-time data valuation problem and introduce the Contextual Influence Value (CI value). This novel metric quantifies context quality by measuring the performance degradation when removing each context from the list, effectively integrating query-aware relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI value eliminates complex selection hyperparameter tuning by simply retaining contexts with positive CI values. To address practical challenges of label dependency and computational overhead, we develop a parameterized surrogate model for CI value prediction during inference. The model employs a hierarchical architecture that captures both local query-context relevance and global inter-context interactions, trained through oracle CI value supervision and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and multiple LLMs demonstrate that our context selection method significantly outperforms state-of-the-art baselines, effectively filtering poor-quality contexts while preserving critical information. Code is available at https://github.com/SJTU-DMTai/RAG-CSM.",
      "arxiv_url": "https://openreview.net/forum?id=ugaepulZyA",
      "pdf_url": "https://openreview.net/pdf/caa29d5216a1860b5942a4a732450808462389e0.pdf",
      "primary_category": "Retrieval Augmented Generation, Data Selection",
      "categories": [
        "Retrieval Augmented Generation",
        "Data Selection"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wn3VBRz5GK",
      "title": "Many LLMs Are More Utilitarian Than One",
      "authors": [
        "Anita Keshmirian",
        "Razan Baltaji",
        "Babak Hemmatian",
        "Hadi Asghari",
        "Lav R. Varshney"
      ],
      "abstract": "Moral judgment is integral to large language models' (LLMs) social reasoning. As multi-agent systems gain prominence, it becomes crucial to understand how LLMs function when collaborating compared to operating as individual agents. In human moral judgment, group deliberation leads to a Utilitarian Boost: a tendency to endorse norm violations that inflict harm but maximize benefits for the greatest number of people. We study whether a similar dynamic emerges in multi-agent LLM systems. We test six models on well-established sets of moral dilemmas across two conditions: (1) Solo, where models reason independently, and (2) Group, where they engage in multi-turn discussions in pairs or triads. In personal dilemmas, where agents decide whether to directly harm an individual for the benefit of others, all models rated moral violations as more acceptable when part of a group, demonstrating a Utilitarian Boost similar to that observed in humans. However, the mechanism for the boost in LLMs differed: While humans in groups become more utilitarian due to heightened sensitivity to decision outcomes, LLM groups showed diverse profiles, for example, reduced sensitivity to norms or enhanced impartiality. We report model differences in when and how strongly the boost manifests. We also discuss prompt and agent compositions that enhance or mitigate the effect. We end with a discussion of the implications for AI alignment, multi-agent design, and artificial moral reasoning. Code available at: https://github.com/baltaci-r/MoralAgents",
      "arxiv_url": "https://openreview.net/forum?id=wn3VBRz5GK",
      "pdf_url": "https://openreview.net/pdf/11ffe64728aca47ad87dbc980cf2ab76b2b81aa3.pdf",
      "primary_category": "multi-agent systems, group moral reasoning, collective moral judgement",
      "categories": [
        "multi-agent systems",
        "group moral reasoning",
        "collective moral judgement",
        "ethical alignment",
        "AI ethics",
        "emergent behavior",
        "large language models"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "JgtCg08aZk",
      "title": "LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions",
      "authors": [
        "Hadi Askari",
        "Shivanshu Gupta",
        "Fei Wang",
        "Anshuman Chhabra",
        "Muhao Chen"
      ],
      "abstract": "Pretrained Large Language Models (LLMs) achieve strong performance across a wide range of tasks, yet exhibit substantial variability in the various layers' training quality with respect to specific downstream applications, limiting their downstream performance. It is therefore critical to estimate layer-wise training quality in a manner that accounts for both model architecture and training data. However, existing approaches predominantly rely on model-centric heuristics (such as spectral statistics, outlier detection, or uniform allocation) while overlooking the influence of data. To address these limitations, we propose **LayerIF**, a data-driven framework that leverages *Influence Functions* to quantify the training quality of individual layers in a principled and task-sensitive manner. By isolating each layer's gradients and measuring the sensitivity of the validation loss to training examples by computing layer-wise influences, we derive data-driven estimates of layer importance. Notably, our method produces *task-specific* layer importance estimates for the *same* LLM, revealing how layers specialize for different test-time evaluation tasks. We demonstrate the utility of our scores by leveraging them for two downstream applications: (a) expert allocation in LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM pruning. Experiments across multiple LLM architectures demonstrate that our model-agnostic, influence-guided allocation leads to consistent gains in task performance.",
      "arxiv_url": "https://openreview.net/forum?id=JgtCg08aZk",
      "pdf_url": "https://openreview.net/pdf/973b76190d9e1cad8c410c5ce8c26a5f7524a66a.pdf",
      "primary_category": "Large Language Models, Influence Functions, Layer Training Quality Estimation",
      "categories": [
        "Large Language Models",
        "Influence Functions",
        "Layer Training Quality Estimation",
        "Model Pruning",
        "Mixture of Experts"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "J76cCYTJub",
      "title": "Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model",
      "authors": [
        "Tianle Li",
        "Jihai Zhang",
        "Yongming Rao",
        "Yu Cheng"
      ],
      "abstract": "While large language models (LLMs) demonstrate strong reasoning capabilities utilizing reinforcement learning (RL) with verifiable reward, whether large vision-language models (VLMs) can directly inherit such capabilities through similar post-training strategies remains underexplored. In this work, we conduct a systematic compositional probing study to evaluate whether current VLMs trained with RL or other post-training strategies can compose capabilities across modalities or tasks under out-of-distribution conditions. We design a suite of diagnostic tasks that train models on unimodal tasks or isolated reasoning skills, and evaluate them on multimodal, compositional variants requiring skill integration. Through comparisons between supervised fine-tuning (SFT) and RL-trained models, we identify three key findings: (1) RL-trained models consistently outperform SFT on compositional generalization, demonstrating better integration of learned skills; (2) although VLMs achieve strong performance on individual tasks, they struggle to generalize compositionally under cross-modal and cross-task scenarios, revealing a significant gap in current training strategies; (3) enforcing models to explicitly describe visual content before reasoning (e.g., caption-before-thinking), along with rewarding progressive vision-to-text grounding, yields notable gains. It highlights two essential ingredients for improving compositionality in VLMs: visual-to-text alignment and accurate visual grounding. Our findings shed light on the current limitations of RL-based reasoning VLM training and provide actionable insights toward building models that reason compositionally across modalities and tasks.",
      "arxiv_url": "https://openreview.net/forum?id=J76cCYTJub",
      "pdf_url": "https://openreview.net/pdf/64ee56c0512b904e0aebcd799494fb96e7bdedba.pdf",
      "primary_category": "Multimodal Large Language Models, Multimodal Reasoning, RL for MLLM",
      "categories": [
        "Multimodal Large Language Models",
        "Multimodal Reasoning",
        "RL for MLLM"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7Eh2SK6Mo7",
      "title": "Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning",
      "authors": [
        "Yurun Yuan",
        "Fan Chen",
        "Zeyu Jia",
        "Alexander Rakhlin",
        "Tengyang Xie"
      ],
      "abstract": "Policy-based methods currently dominate reinforcement learning (RL) pipelines for large language model (LLM) reasoning, leaving value-based approaches largely unexplored. We revisit the classical paradigm of Bellman Residual Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an algorithm that naturally adapts this idea to LLMs, yielding a simple yet effective off-policy algorithm that optimizes a single trajectory-level Bellman objective using the model's own logits as $Q$-values. TBRM removes the need for critics, importance-sampling ratios, or clipping, and can operate with only one rollout per prompt. We prove convergence to the near-optimal KL-regularized policy from arbitrary off-policy data via an improved change-of-trajectory-measure analysis. Experiments on standard mathematical-reasoning benchmarks show that TBRM matches or surpasses policy-based baselines, like PPO and GRPO, with comparable or lower computational and memory overhead. Our results indicate that value-based RL might be a principled and efficient alternative for enhancing reasoning capabilities in LLMs. The codebase for TBRM is publicly available at [https://github.com/rlx-lab/TBRM](https://github.com/rlx-lab/TBRM).",
      "arxiv_url": "https://openreview.net/forum?id=7Eh2SK6Mo7",
      "pdf_url": "https://openreview.net/pdf/6aefcbc212ab28ccf45c24b827cd790bb04db214.pdf",
      "primary_category": "large language models, value-based reinforcement learning, LLM post-training",
      "categories": [
        "large language models",
        "value-based reinforcement learning",
        "LLM post-training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Ej20yjWMCj",
      "title": "OptiTree: Hierarchical Thoughts Generation with Tree Search for LLM Optimization Modeling",
      "authors": [
        "Haoyang Liu",
        "Jie Wang",
        "Yuyang Cai",
        "Xiongwei Han",
        "Yufei Kuang",
        "Jianye HAO"
      ],
      "abstract": "Optimization modeling is one of the most crucial but technical parts of operations research (OR).\n  To automate the modeling process, existing works have leveraged large language models (LLMs), prompting them to break down tasks into steps for generating variables, constraints, and objectives.\n  However, due to the highly complex mathematical structures inherent in OR problems, standard fixed-step decomposition often fails to achieve high performance.\n  To address this challenge, we introduce OptiTree, a novel tree search approach designed to enhance modeling capabilities for complex problems through adaptive problem decomposition into simpler subproblems.\n  Specifically, we develop a modeling tree that organizes a wide range of OR problems based on their hierarchical problem taxonomy and complexity, with each node representing a problem category and containing relevant high-level modeling thoughts.\n  Given a problem to model, we recurrently search the tree to identify a series of simpler subproblems and synthesize the global modeling thoughts by adaptively integrating the hierarchical thoughts.\n  Experiments show that OptiTree significantly improves the modeling accuracy compared to the state-of-the-art, achieving over 10% improvements on the challenging benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=Ej20yjWMCj",
      "pdf_url": "https://openreview.net/pdf/7a76911fa3e092e7774e4a94b739238922d922b9.pdf",
      "primary_category": "Optimization Modeling, Operations research, Large language models",
      "categories": [
        "Optimization Modeling",
        "Operations research",
        "Large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "lVV7F0piDK",
      "title": "Gated Integration of Low-Rank Adaptation for Continual Learning  of Large Language Models",
      "authors": [
        "Yan-Shuo Liang",
        "Jiarui Chen",
        "Wu-Jun Li"
      ],
      "abstract": "Continual learning (CL), which requires the model to learn multiple tasks sequentially, is crucial for large language models (LLMs). Recently, low-rank adaptation (LoRA), one of the most representative parameter-efficient fine-tuning (PEFT) methods, has gained increasing attention in CL of LLMs. However, most existing CL methods based on LoRA typically expand a new LoRA branch to learn each new task and force the new and old LoRA branches to influence old tasks equally, potentially leading to forgetting. In this work, we propose a new method, called gated integration of low-rank adaptation (GainLoRA), for CL of LLMs. GainLoRA expands a new LoRA branch for each new task and introduces gating modules to integrate the new and old LoRA branches. Furthermore, GainLoRA leverages the new gating module to minimize the influence from the new LoRA branch to old tasks, effectively mitigating forgetting and improving the model's overall performance. Experimental results on CL benchmarks demonstrate that GainLoRA outperforms existing state-of-the-art methods.",
      "arxiv_url": "https://openreview.net/forum?id=lVV7F0piDK",
      "pdf_url": "https://openreview.net/pdf/7e78d2c6d7e8eb3627be52b5e7e4ad3a80791a34.pdf",
      "primary_category": "Large Language Models, Continual Learning, low-rank adaptation",
      "categories": [
        "Large Language Models",
        "Continual Learning",
        "low-rank adaptation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "dqBkZ9rmSF",
      "title": "Fading to Grow: Growing Preference Ratios via Preference Fading Discrete Diffusion for Recommendation",
      "authors": [
        "Guoqing Hu",
        "An Zhang",
        "Shuchang Liu",
        "Wenyu Mao",
        "Jiancan Wu",
        "Xun Yang",
        "Xiang Li",
        "Lantao Hu",
        "Han Li",
        "Kun Gai",
        "Xiang Wang"
      ],
      "abstract": "Recommenders aim to rank items from a discrete item corpus in line with user interests, yet suffer from extremely sparse user preference data. Recent advances in diffusion models have inspired diffusion-based recommenders, which alleviate sparsity by injecting noise during a forward process to prevent collapse of perturbed preference distributions. However, current diffusion‑based recommenders predominantly rely on continuous Gaussian noise, which is intrinsically mismatched with the discrete nature of user preference data in recommendation. In this paper, building upon recent advances in discrete diffusion, we propose \\textbf{PreferGrow}, a discrete diffusion-based recommender modeling preference ratios by fading and growing user preferences over the discrete item corpus. PreferGrow differs from existing diffusion-based recommenders in three core aspects: (1) Discrete modeling of preference ratios:\nPreferGrow models relative preference ratios between two items, where a positive value indicates a more preferred one over another less preferred. This formulation aligns naturally with the discrete and ranking-oriented nature of recommendation tasks.\n(2) Perturbing via preference fading: Instead of injecting continuous noise, PreferGrow fades user preferences by replacing the preferred item with alternatives---physically akin to negative sampling---thereby eliminating the need for any prior noise assumption.\n(3) Preference reconstruction via growing: PreferGrow reconstructs user preferences by iteratively growing the preference signal from the estimated ratios. We further provide theoretical analysis showing that PreferGrow preserves key properties of discrete diffusion processes.\nPreferGrow provides a well-defined matrix‑based formulation for discrete diffusion-based recommendation and empirically outperforms existing diffusion‑based recommenders across five benchmark datasets, underscoring its superior effectiveness.\nOur codes are available at \\url{https://anonymous.4open.science/r/PreferGrow_Commit-2259/}.",
      "arxiv_url": "https://openreview.net/forum?id=dqBkZ9rmSF",
      "pdf_url": "https://openreview.net/pdf/158f96968190fde3e3c3ab4478af0ba8db497587.pdf",
      "primary_category": "Recommendation, Discrete Diffusion, Preference Ratios",
      "categories": [
        "Recommendation",
        "Discrete Diffusion",
        "Preference Ratios"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DRIRD9ELMb",
      "title": "Lookahead Routing for Large Language Models",
      "authors": [
        "Canbin Huang",
        "Tianyuan Shi",
        "Yuhua Zhu",
        "Ruijun Chen",
        "Xiaojun Quan"
      ],
      "abstract": "Large language model (LLM) routers improve the efficiency of multi-model systems by directing each query to the most appropriate model while leveraging the diverse strengths of heterogeneous LLMs. Most existing approaches frame routing as a classification problem based solely on the input query. While this reduces overhead by avoiding inference across all models, it overlooks valuable information that could be gleaned from potential outputs and fails to capture implicit intent or contextual nuances that often emerge only during response generation. These limitations can result in suboptimal routing decisions, particularly for complex or ambiguous queries that require deeper semantic understanding. To address this challenge, we propose Lookahead, a routing framework that \"foresees\" potential model outputs by predicting their latent representations and uses these predictions to guide model selection, thus enabling more informed routing without full inference. Within this framework, we implement two approaches based on causal and masked language models. Empirical evaluations across seven public benchmarks—spanning instruction following, mathematical reasoning, and code generation—show that Lookahead consistently outperforms existing routing baselines, achieving an average performance gain of 7.7\\% over the state-of-the-art. Our code is available at https://github.com/huangcb01/lookahead-routing.",
      "arxiv_url": "https://openreview.net/forum?id=DRIRD9ELMb",
      "pdf_url": "https://openreview.net/pdf/552fece3eff509f39e0f54fafd8aafc36248c8a8.pdf",
      "primary_category": "LLM Routing, Large Language Models",
      "categories": [
        "LLM Routing",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "dye9w8IOV0",
      "title": "Collaborative Reasoner: Self-Improving Social Agents with Synthetic Conversations",
      "authors": [
        "Ansong Ni",
        "Ruta Desai",
        "Yang Li",
        "Xinjie Lei",
        "Dong Wang",
        "Jiemin Zhang",
        "Jane Yu",
        "Ramya Raghavendra",
        "Gargi Ghosh",
        "Shang-Wen Li",
        "Asli Celikyilmaz"
      ],
      "abstract": "With increasingly powerful large language models (LLMs) and LLM-based agents tackling an ever-growing list of tasks, we envision a future where numerous LLM agents work seamlessly with other AI agents and humans to solve complex problems and enhance daily life. To achieve these goals, LLM agents must develop collaborative skills such as effective persuasion, assertion and disagreement, which are often overlooked in the prevalent single-turn training and evaluation of LLMs. In this work, we present Collaborative Reasoner (Coral), a framework to evaluate and improve the collaborative reasoning abilities of language models. In particular, tasks and metrics in Coral necessitate agents to disagree with incorrect solutions, convince their partners of a correct solution, and ultimately agree as a team to commit to a final solution, all through a natural multi-turn conversation. Through comprehensive evaluation on six collaborative reasoning tasks covering domains of coding, math, scientific QA and social reasoning, we show that current models cannot effectively collaborate due to undesirable social behaviors, collapsing even on problems that they can solve singlehandedly. To improve the collaborative reasoning capabilities of LLMs, we propose a self-play method to generate synthetic multi-turn preference data and further train the language models to be better collaborators. Experiments with Llama-3.1, Ministral and Qwen-2.5 models show that our proposed self-improvement approach consistently outperforms finetuned chain-of-thought performance of the same base model, yielding gains up to 16.7% absolute. Human evaluations show that the models exhibit more effective disagreement and produce more natural conversations after training on our synthetic interaction data.",
      "arxiv_url": "https://openreview.net/forum?id=dye9w8IOV0",
      "pdf_url": "https://openreview.net/pdf/42a338bca40ea896002753679729eb2240bf62b3.pdf",
      "primary_category": "collaborative reasoning, social agents, synthetic data",
      "categories": [
        "collaborative reasoning",
        "social agents",
        "synthetic data",
        "self-improvement"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ustF8MMZDJ",
      "title": "Feedback-Aware MCTS for Goal-Oriented Information Seeking",
      "authors": [
        "Harshita Chopra",
        "Chirag Shah"
      ],
      "abstract": "Effective decision-making and problem-solving in conversational systems require the ability to identify and acquire missing information through targeted questioning. A key challenge lies in efficiently narrowing down a large space of possible outcomes by posing questions that minimize uncertainty. To address this, we introduce a novel framework that leverages Large Language Models (LLMs) to generate information-seeking questions, with Monte Carlo Tree Search (MCTS) to strategically select questions that maximize information gain, as a part of inference-time planning. Our primary contribution includes a hierarchical feedback mechanism that exploits past interaction patterns to guide future strategy. Specifically, each new problem is mapped to a cluster based on semantic similarity, and our UCT (Upper Confidence bound for Trees) formulation employs a cluster-specific bonus reward to prioritize successful question trajectories that have proven effective for similar problems in the past. Extensive empirical evaluation across medical diagnosis and technical troubleshooting domains shows that our method achieves an average of 12\\% improvement in success rates and about 10x reduction in the number of LLM calls made for planning per conversation, compared to the state of the art. An additional 8\\% gain in success rate is observed on average when we start with a constrained set of possibilities. Our results underscore the efficacy of feedback-aware MCTS in enhancing information-seeking in goal-oriented dialogues.",
      "arxiv_url": "https://openreview.net/forum?id=ustF8MMZDJ",
      "pdf_url": "https://openreview.net/pdf/68f0abbda44765d7c684ac0e44bc3ee6a673f423.pdf",
      "primary_category": "Information Seeking, Large Language Models, Uncertainty Reduction",
      "categories": [
        "Information Seeking",
        "Large Language Models",
        "Uncertainty Reduction",
        "Planning",
        "Search"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XBD4OjCcIA",
      "title": "VERA: Variational Inference Framework for Jailbreaking Large Language Models",
      "authors": [
        "Anamika Lochab",
        "Lu Yan",
        "Patrick Pynadath",
        "Xiangyu Zhang",
        "Ruqi Zhang"
      ],
      "abstract": "The rise of API-only access to state-of-the-art LLMs highlights the need for effective black-box jailbreak methods to identify model vulnerabilities in real-world settings. Without a principled objective for gradient-based optimization, most existing approaches rely on genetic algorithms, which are limited by their initialization and dependence on manually curated prompt pools. Furthermore, these methods require individual optimization for each prompt, failing to provide a comprehensive characterization of model vulnerabilities.\nTo address this gap, we introduce VERA: Variational infErence fRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a variational inference problem, training a small attacker LLM to approximate the target LLM’s posterior over adversarial prompts. Once trained, the attacker can generate diverse, fluent jailbreak prompts for a target query without re-optimization. Experimental results show that VERA achieves strong performance across a range of target LLMs, highlighting the value of probabilistic inference for adversarial prompt generation.",
      "arxiv_url": "https://openreview.net/forum?id=XBD4OjCcIA",
      "pdf_url": "https://openreview.net/pdf/63ca0f351f40d3015e4466ce2ef9411523e0ca39.pdf",
      "primary_category": "Jailbreaking, LLM Security, Reinforcement Learning",
      "categories": [
        "Jailbreaking",
        "LLM Security",
        "Reinforcement Learning",
        "Variational Inference"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "A0T3piHiis",
      "title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning",
      "authors": [
        "Marwa Abdulhai",
        "Ryan Cheng",
        "Donovan Clay",
        "Tim Althoff",
        "Sergey Levine",
        "Natasha Jaques"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used to simulate human users in interactive settings such as therapy, education, and social role-play. While these simulations enable scalable training and evaluation of AI agents, off-the-shelf LLMs often drift from their assigned personas, contradict earlier statements, or abandon role-appropriate behavior. We introduce a unified framework for evaluating and improving persona consistency in LLM-generated dialogue. We define three automatic metrics—prompt-to-line consistency, line-to-line consistency, and Q\\&A consistency—that capture different types of persona drift and validate each against human annotations. Using these metrics as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs for three user roles: a patient, a student, and a social chat partner. Our method reduces inconsistency by over 55%, resulting in more coherent, faithful, and trustworthy simulated users.",
      "arxiv_url": "https://openreview.net/forum?id=A0T3piHiis",
      "pdf_url": "https://openreview.net/pdf/109c600393cc962e64028e8425eca62778f40ee9.pdf",
      "primary_category": "large language models; simulating humans; consistency of LLMs; reinforcement learning",
      "categories": [
        "large language models; simulating humans; consistency of LLMs; reinforcement learning"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yqQVRNdmKJ",
      "title": "KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning",
      "authors": [
        "Wei Sun",
        "Wen Yang",
        "Pu Jian",
        "Qianlong Du",
        "Fuwei Cui",
        "Shuo Ren",
        "Jiajun Zhang"
      ],
      "abstract": "Recent advances have demonstrated that integrating reinforcement learning with rule-based rewards can significantly enhance the reasoning capabilities of large language models (LLMs), even without supervised fine-tuning (SFT). However, prevalent reinforcement learning algorithms such as GRPO and its variants like DAPO, suffer from a coarse granularity issue when computing the advantage. Specifically, they compute rollout-level advantages that assign identical values to every token within a sequence, failing to capture token-specific contributions. To address this limitation, we propose Key-token Advantage Estimation (KTAE)—a novel algorithm that estimates fine-grained, token-level advantages without introducing additional models. KTAE leverages the correctness of sampled rollouts and applies statistical analysis to quantify the importance of individual tokens within a sequence to the final outcome. This quantified token-level importance is then combined with the rollout-level advantage to obtain a more fine-grained token-level advantage estimation. Empirical results show that models trained with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five mathematical reasoning benchmarks. Notably, they achieve higher accuracy with shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base model.",
      "arxiv_url": "https://openreview.net/forum?id=yqQVRNdmKJ",
      "pdf_url": "https://openreview.net/pdf/c7361c0485c7a932526ce8e922fcf62f985013b3.pdf",
      "primary_category": "Large language model;Reinforcement learning;Mathematical reasoning",
      "categories": [
        "Large language model;Reinforcement learning;Mathematical reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MHGViOjZ27",
      "title": "Generative Caching for Structurally Similar Prompts and Responses",
      "authors": [
        "Sarthak Chakraborty",
        "Suman Nath",
        "Xuchao Zhang",
        "Chetan Bansal",
        "Indranil Gupta"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being used to plan, reason, and execute tasks across diverse scenarios. In use cases like repeatable workflows and agentic settings, prompts are often reused with minor variations while having a similar structure for recurring tasks. This opens up opportunities for caching. However, exact prompt matching fails on such structurally similar prompts, while semantic caching may produce incorrect responses by ignoring critical differences. To address this, we introduce GenCache, a generative cache that produces variation-aware responses for structurally similar prompts. GenCache identifies reusable response patterns across similar prompt structures and synthesizes customized outputs for new requests. We show that GenCache achieves 83\\% cache hit rate, while having minimal incorrect hits on datasets without prompt repetition. In agentic workflows, it improves cache hit rate by $\\sim$20\\% and reduces end-to-end execution latency by $\\sim$34\\% compared to standard prompt matching.",
      "arxiv_url": "https://openreview.net/forum?id=MHGViOjZ27",
      "pdf_url": "https://openreview.net/pdf/df29ee5226893fdb3de7942c122869fbc8f4ced9.pdf",
      "primary_category": "Generative Caching, Hit Rate Optimization, Repeatable Workflows",
      "categories": [
        "Generative Caching",
        "Hit Rate Optimization",
        "Repeatable Workflows",
        "LLM Serving Systems"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QAVpe6a3rp",
      "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models",
      "authors": [
        "Wentse Chen",
        "Jiayu Chen",
        "Fahim Tajwar",
        "Hao Zhu",
        "Xintong Duan",
        "Ruslan Salakhutdinov",
        "Jeff Schneider"
      ],
      "abstract": "Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on domain-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pre-trained knowledge from large language models (LLMs) to transform sparse rewards into dense training signals (i.e., the advantage function) through retrospective in-context learning (RICL).  We further propose an online learning framework, RICOL, which iteratively refines the policy based on the credit assignment results from RICL. We empirically demonstrate that RICL can accurately estimate the advantage function with limited samples and effectively identify critical states for temporal credit assignment. Extended evaluation on the BabyAI benchmark shows that RICOL significantly improves sample efficiency compared to traditional online RL algorithms while achieving performance comparable to imitation learning from expert demonstartions.  Our findings highlight the potential of leveraging LLMs for temporal credit assignment, paving the way for more sample-efficient and generalizable RL paradigms.",
      "arxiv_url": "https://openreview.net/forum?id=QAVpe6a3rp",
      "pdf_url": "https://openreview.net/pdf/d07c03fb879714800e5ffbcfba839309dbcd088b.pdf",
      "primary_category": "In-Context Learning, Credit Assignment, Large Language Models",
      "categories": [
        "In-Context Learning",
        "Credit Assignment",
        "Large Language Models",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "fKerD2AQai",
      "title": "Understanding protein function with a multimodal retrieval-augmented foundation model",
      "authors": [
        "Timothy Fei Truong Jr",
        "Tristan Bepler"
      ],
      "abstract": "Protein language models (PLMs) learn probability distributions over natural protein sequences. By learning from hundreds of millions of natural protein sequences, protein understanding and design capabilities emerge. Recent works have shown that scaling these models improves structure prediction, but does not seem to improve mutation understanding and representation quality for protein function prediction. We introduce PoET-2, a multimodal, retrieval-augmented protein foundation model that incorporates in-context learning of family-specific evolutionary constraints with optional structure conditioning to learn generative distributions over protein sequences. PoET-2 uses a hierarchical transformer encoder that is equivariant to sequence context ordering and a dual decoder architecture with both causal and masked language modeling objectives, allowing PoET-2 to operate in both fully generative and bidirectional representation learning modes. PoET-2 achieves state-of-the-art performance on zero-shot variant effect prediction, excelling at scoring variants with multiple mutations and challenging indel mutations. In supervised settings, PoET-2 embeddings outperform previous methods for learning sequence-function relationships, especially with small datasets. This work highlights the benefits of combining retrieval augmentation with multimodal, family-centric modeling for advancing protein foundation models.",
      "arxiv_url": "https://openreview.net/forum?id=fKerD2AQai",
      "pdf_url": "https://openreview.net/pdf/fb0459078cdc7dbc0099f7474cc061ba0948c85b.pdf",
      "primary_category": "protein fitness prediction, transformer, retrieval",
      "categories": [
        "protein fitness prediction",
        "transformer",
        "retrieval",
        "language model",
        "MSA",
        "generative model",
        "protein engineering"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "V82wLePv0o",
      "title": "IF-Guide: Influence Function-Guided Detoxification of LLMs",
      "authors": [
        "Zachary Coalson",
        "Juhan Bae",
        "Nicholas Carlini",
        "Sanghyun Hong"
      ],
      "abstract": "We study how training data contributes to the emergence of toxic behaviors in large language models. Most prior work on reducing model toxicity adopts *reactive* approaches, such as fine-tuning pre-trained (and potentially toxic) models to align them with human values. In contrast, we propose a *proactive* approach—IF-Guide—that leverages influence functions to identify and suppress harmful tokens in the training data. To this end, we first show that standard influence functions are ineffective at discovering harmful training records. We then present a novel adaptation that measures token-level attributions from training data to model toxicity, along with techniques for selecting toxic training documents and a learning objective that can be integrated into both pre-training and fine-tuning. Moreover, IF-Guide does not rely on human-preference data, which is typically required by existing alignment methods. In our evaluation, we demonstrate that IF-Guide substantially reduces both explicit and implicit toxicity—by up to 10$\\times$ compared to uncensored models, and up to 3$\\times$ compared to baseline alignment methods such as DPO and RAD—across both pre-training and fine-tuning scenarios. IF-Guide is computationally efficient: a billion-parameter model is *not necessary* for computing influence scores; a million-parameter model—with 7.5$\\times$ fewer parameters—can effectively serve as a proxy for identifying harmful data.",
      "arxiv_url": "https://openreview.net/forum?id=V82wLePv0o",
      "pdf_url": "https://openreview.net/pdf/008e2b70f24402712d795e8f8be3d9b91e8239af.pdf",
      "primary_category": "influence functions, LLM toxicity",
      "categories": [
        "influence functions",
        "LLM toxicity"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "U5w9l0yQdo",
      "title": "Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery",
      "authors": [
        "Jiyeon Kang",
        "Songseong Kim",
        "Chanhui Lee",
        "Doyeong Hwang",
        "Joanie Hayoun Chung",
        "Yunkyung Ko",
        "Sumin Lee",
        "Sungwoong Kim",
        "Sungbin Lim"
      ],
      "abstract": "Ordering-based approaches to causal discovery identify topological orders of causal graphs, providing scalable alternatives to combinatorial search methods. Under the Additive Noise Models (ANMs) assumption, recent causal ordering methods based on score matching require an accurate estimation of the Hessian diagonal of the log-densities. However, previous approaches mainly use Stein gradient estimators, which are computationally expensive and memory-intensive. Although DiffAN addresses these limitations by substituting kernel-based estimates with diffusion models, it remains numerically unstable due to the second-order derivatives of score models. To alleviate these problems, we propose Score-informed Neural Operator (SciNO), a probabilistic generative model in smooth function spaces designed to stably approximate the Hessian diagonal and to preserve structural information during the score modeling. \nEmpirical results show that SciNO reduces order divergence by 42.7% on synthetic graphs and by 31.5% in real-world datasets on average compared to DiffAN, while maintaining memory efficiency and scalability. Furthermore, we propose a probabilistic control algorithm for causal reasoning with autoregressive models that integrates SciNO's probability estimates with autoregressive model priors, enabling reliable data-driven causal ordering informed by semantic information. Consequently, the proposed method enhances causal reasoning abilities of LLMs without additional fine-tuning or prompt engineering.",
      "arxiv_url": "https://openreview.net/forum?id=U5w9l0yQdo",
      "pdf_url": "https://openreview.net/pdf/b84c8673c274ce40a61269ee1f5ff8e3069be7ce.pdf",
      "primary_category": "Causal Discovery, Score Matching, Diffusion Models",
      "categories": [
        "Causal Discovery",
        "Score Matching",
        "Diffusion Models",
        "Neural Operator"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "HMVQ00vabY",
      "title": "Probabilistic Reasoning with LLMs for Privacy Risk Estimation",
      "authors": [
        "Jonathan Zheng",
        "Alan Ritter",
        "Sauvik Das",
        "Wei Xu"
      ],
      "abstract": "Probabilistic reasoning is a key aspect of both human and artificial intelligence that allows for handling uncertainty and ambiguity in decision-making. In this paper, we introduce a new numerical reasoning task under uncertainty for large language models, focusing on estimating the privacy risk of user-generated documents containing privacy-sensitive information. We propose BRANCH, a new LLM methodology that estimates the $k$-privacy value of a text—the size of the population matching the given information. BRANCH factorizes a joint probability distribution of personal information as random variables.  The probability of each factor in a population is estimated separately using a Bayesian network and combined to compute the final $k$-value. Our experiments show that this method successfully estimates the $k$-value 73% of the time, a 13% increase compared to o3-mini with chain-of-thought reasoning. We also find that LLM uncertainty is a good indicator for accuracy, as high variance predictions are 37.47% less accurate on average.",
      "arxiv_url": "https://openreview.net/forum?id=HMVQ00vabY",
      "pdf_url": "https://openreview.net/pdf/588322ebe3069ec720bc40ab83d491f3dbee6f02.pdf",
      "primary_category": "LLMs, probabilistic reasoning, chain of thought",
      "categories": [
        "LLMs",
        "probabilistic reasoning",
        "chain of thought",
        "prompting",
        "NLP",
        "privacy risk",
        "personal disclosures"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DwXX8c7xst",
      "title": "Bits Leaked per Query: Information-Theoretic Bounds for Adversarial Attacks on LLMs",
      "authors": [
        "Masahiro Kaneko",
        "Timothy Baldwin"
      ],
      "abstract": "Adversarial attacks by malicious users that threaten the safety of large language models (LLMs) can be viewed as attempts to infer a target property $T$ that is unknown when an instruction is issued, and becomes knowable only after the model's reply is observed.  \nExamples of target properties $T$ include the binary flag that triggers an LLM's harmful response or rejection, and the degree to which information deleted by unlearning can be restored, both elicited via adversarial instructions.  \nThe LLM reveals an \\emph{observable signal} $Z$ that potentially leaks hints for attacking through a response containing answer tokens, thinking process tokens, or logits.\nYet the scale of information leaked remains anecdotal, leaving auditors without principled guidance and defenders blind to the transparency--risk trade-off.\nWe fill this gap with an information-theoretic framework that computes how much information can be safely disclosed, and enables auditors to gauge how close their methods come to the fundamental limit.\nTreating the mutual information $I(Z;T)$ between the observation $Z$ and the target property $T$ as the leaked bits per query, we show that achieving error $\\varepsilon$ requires at least $\\log(1/\\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak rate and only logarithmically with the desired accuracy.\nThus, even a modest increase in disclosure collapses the attack cost from quadratic to logarithmic in terms of the desired accuracy.\nExperiments on seven LLMs across system-prompt leakage, jailbreak, and relearning attacks corroborate the theory: exposing answer tokens alone requires about a thousand queries; adding logits cuts this to about a hundred; and revealing the full thinking process trims it to a few dozen.\nOur results provide the first principled yardstick for balancing transparency and security when deploying LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=DwXX8c7xst",
      "pdf_url": "https://openreview.net/pdf/8e72d7c85e44d59e054001d5fe35e592d2811b44.pdf",
      "primary_category": "Large Language Models, Jailbreak attack, Security",
      "categories": [
        "Large Language Models",
        "Jailbreak attack",
        "Security"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "jFaFCc5978",
      "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
      "authors": [
        "Penghui Qi",
        "Zichen Liu",
        "Tianyu Pang",
        "Chao Du",
        "Wee Sun Lee",
        "Min Lin"
      ],
      "abstract": "Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present **AnytimeReasoner**, a novel framework for optimizing reasoning performance under varying thinking budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, **Budget Relative Policy Optimization (BRPO)**, to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.",
      "arxiv_url": "https://openreview.net/forum?id=jFaFCc5978",
      "pdf_url": "https://openreview.net/pdf/8136e4668a09f8c47a2454d9e72728d4fdea055e.pdf",
      "primary_category": "Anytime Reasoning, Reinforcement Learning, LLM Reasoning",
      "categories": [
        "Anytime Reasoning",
        "Reinforcement Learning",
        "LLM Reasoning",
        "GRPO"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "X51kYnijag",
      "title": "TRAP: Targeted Redirecting of Agentic Preferences",
      "authors": [
        "Hangoo Kang",
        "Jehyeok Yeon",
        "Gagandeep Singh"
      ],
      "abstract": "Autonomous agentic AI systems powered by vision-language models (VLMs) are rapidly advancing toward real-world deployment, yet their cross-modal reasoning capabilities introduce new attack surfaces for adversarial manipulation that exploit semantic reasoning across modalities. Existing adversarial attacks typically rely on visible pixel perturbations or require privileged model or environment access, making them impractical for stealthy, real-world exploitation. We introduce TRAP, a novel generative adversarial framework that manipulates\nthe agent’s decision-making using diffusion-based semantic injections into the vision-language embedding space. Our method combines negative prompt–based degradation with positive semantic optimization, guided by a Siamese semantic network and layout-aware spatial masking. Without requiring access to model internals, TRAP produces visually natural images yet induces consistent selection\nbiases in agentic AI systems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO) dataset, building multi-candidate decision scenarios. Across these scenarios, TRAP consistently induces decision-level preference redirection on leading models, including LLaVA-34B, Gemma3, GPT-4o, and Mistral-3.2, significantly outperforming existing baselines such as SPSA, Bandit, and standard diffusion approaches. These findings expose a critical, generalized vulnerability: autonomous agents can be consistently misled through visually subtle, semantically-guided cross-modal manipulations. Overall, our results show the need for defense strategies beyond pixel-level robustness to address semantic vulnerabilities in cross-modal decision-making. The code for TRAP is accessible on GitHub at https://github.com/uiuc-focal-lab/TRAP.",
      "arxiv_url": "https://openreview.net/forum?id=X51kYnijag",
      "pdf_url": "https://openreview.net/pdf/8591b1dac1bf58780a964857d113f6ac8abbd3f2.pdf",
      "primary_category": "Agentic AI, Trustworthy Machine Learning, Multimodal",
      "categories": [
        "Agentic AI",
        "Trustworthy Machine Learning",
        "Multimodal",
        "Autonomous Decision Making",
        "Large Language Model (LLM)",
        "Vision Language Model (VLM))"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8C8F4NmHfz",
      "title": "Tail-Optimized Caching for LLM Inference",
      "authors": [
        "Wenxin Zhang",
        "Yueying Li",
        "Ciamac C. Moallemi",
        "Tianyi Peng"
      ],
      "abstract": "Prompt caching is critical for reducing latency and cost in LLM inference---OpenAI and Anthropic report up to 50–90\\% cost savings through prompt reuse. Despite its widespread success, little is known about what constitutes an optimal prompt caching policy, particularly when optimizing tail latency—a metric of central importance to practitioners. The widely used Least Recently Used (LRU) policy can perform arbitrarily poor on this metric, as it is oblivious to the heterogeneity of conversation lengths. To address this gap, we propose Tail-Optimized LRU, a simple two-line modification that reallocates KV cache capacity to prioritize high-latency conversations by evicting cache entries that are unlikely to affect future turns. Though the implementation is simple, we prove its optimality under a natural stochastic model of conversation dynamics, providing the first theoretical justification for LRU in this setting---a result that may be of independent interest to the caching community. \nExperimentally, on real conversation data WildChat~\\citep{zhao2024wildchat}, Tail-Optimized LRU achieves up to 27.5\\% reduction in P90 tail Time to First Token latency and 23.9\\% in P95 tail latency compared to LRU, along with up to 38.9\\% decrease in SLO violations of 200ms. \nWe believe this provides a practical and theoretically grounded option for practitioners seeking to optimize tail latency in real-world LLM deployments.",
      "arxiv_url": "https://openreview.net/forum?id=8C8F4NmHfz",
      "pdf_url": "https://openreview.net/pdf/c38e794a0948d26c4ad6f150babbd0d6bfcc7df5.pdf",
      "primary_category": "prompt caching, large language models, tail latency",
      "categories": [
        "prompt caching",
        "large language models",
        "tail latency",
        "KV‑cache eviction",
        "Least‑Recently‑Used"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XdCglv7Um7",
      "title": "Unlabeled Data Can Provably Enhance In-Context Learning of Transformers",
      "authors": [
        "Renpu Liu",
        "Jing Yang"
      ],
      "abstract": "Large language models (LLMs) exhibit impressive in‑context learning (ICL) capabilities, yet the quality of their predictions is fundamentally limited by the few costly labeled demonstrations that can fit into a prompt. Meanwhile, there exist vast and continuously growing amounts of unlabeled data that may be closely related to the ICL task. How to utilize such unlabeled data to provably enhance the performance of ICL thus becomes an emerging fundamental question. In this work, we propose a novel augmented ICL framework, in which the prompt includes a small set of labeled examples alongside a block of unlabeled inputs. We focus on the multi-class linear classification setting and demonstrate that, with chain-of-thought (CoT) prompting, a multi-layer transformer can effectively emulate an expectation–maximization (EM) algorithm. This enables the transformer to implicitly extract useful information from both labeled and unlabeled data, leading to provable improvements in ICL accuracy. Moreover, we show that such a transformer can be trained via teacher forcing, with its parameters converging to the desired solution at a linear rate. Experiments demonstrate that the augmented ICL framework consistently outperforms conventional few-shot ICL, providing empirical support for our theoretical findings. To the best of our knowledge, this is the first theoretical study on the impact of unlabeled data on the ICL performance of transformers.",
      "arxiv_url": "https://openreview.net/forum?id=XdCglv7Um7",
      "pdf_url": "https://openreview.net/pdf/bd60a884361494f027ad5d60ca055b30a4541867.pdf",
      "primary_category": "Large Language Models, In‑Context Learning, Chain‑of‑Thought Reasoning",
      "categories": [
        "Large Language Models",
        "In‑Context Learning",
        "Chain‑of‑Thought Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zhFEO67s5w",
      "title": "Co-PatcheR: Collaborative Software Patching with Component-specific Small Reasoning Models",
      "authors": [
        "Yuheng Tang",
        "Hongwei Li",
        "Kaijie Zhu",
        "Michael Yang",
        "Yangruibo Ding",
        "Wenbo Guo"
      ],
      "abstract": "Motivated by the success of general‑purpose large language models (LLMs) in software patching, recent works started to train specialized patching models. Most works trained one model to handle the end‑to‑end patching pipeline (including issue localization, patch generation, and patch validation). However, it is hard for a small model to handle all tasks, as different sub-tasks have different workflows and require different expertise. As such, by using a 70 billion model, SOTA methods can only reach up to 41% resolved rate on SWE-bench-Verified. Motivated by the collaborative nature, we propose Co-PatcheR, the first collaborative patching system with small and specialized reasoning models for individual components. Our key technique novelties are the specific task designs and training recipes. First, we train a model for localization and patch generation. Our localization pinpoints the suspicious lines through a two-step procedure, and our generation combines patch generation and critique. We then propose a hybrid patch validation that includes two models for crafting issue-reproducing test cases with and without assertions and judging patch correctness, followed by a majority vote-based patch selection. Through extensive evaluation, we show that Co-PatcheR achieves 46% resolved rate on SWE-bench-Verified with only 3 x 14B models. This makes Co-PatcheR the best patcher with specialized models, requiring the least training resources and the smallest models. We conduct a comprehensive ablation study to validate our recipes, as well as our choice of training data number, model size, and testing-phase scaling strategy.",
      "arxiv_url": "https://openreview.net/forum?id=zhFEO67s5w",
      "pdf_url": "https://openreview.net/pdf/e3348fe2aa597c99ef487484d7a556c697e04a12.pdf",
      "primary_category": "automatic program repair, autonomous software improvement, software patching agents",
      "categories": [
        "automatic program repair",
        "autonomous software improvement",
        "software patching agents",
        "customized reasoning models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZkGHzGIaMB",
      "title": "Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers",
      "authors": [
        "Yixiao Huang",
        "Hanlin Zhu",
        "Tianyu Guo",
        "Jiantao Jiao",
        "Somayeh Sojoudi",
        "Michael I. Jordan",
        "Stuart Russell",
        "Song Mei"
      ],
      "abstract": "Large language models (LLMs) can acquire new knowledge through fine-tuning, but this process exhibits a puzzling duality: models can generalize remarkably from new facts, yet are also prone to hallucinating incorrect information. However, the reasons for this phenomenon remain poorly understood. In this work, we argue that both behaviors stem from a single mechanism known as out-of-context reasoning (OCR): the ability to deduce implications by associating concepts, even those without a causal link. Our experiments across five prominent LLMs confirm that OCR indeed drives both generalization and hallucination, depending on whether the associated concepts are causally related. To build a rigorous theoretical understanding of this phenomenon, we then formalize OCR as a synthetic factual recall task. We empirically show that a one-layer single-head attention-only transformer with factorized output and value matrices can learn to solve this task, while a model with combined weights cannot, highlighting the crucial role of matrix factorization. Our theoretical analysis shows that the OCR capability can be attributed to the implicit bias of gradient descent, which favors solutions that minimize the nuclear norm of the combined output-value matrix. This structure explains why the model learns to associate facts and implications with high sample efficiency, regardless of whether the correlation is causal or merely spurious. Ultimately, our work provides a theoretical foundation for understanding the OCR phenomenon, offering a new lens for analyzing and mitigating undesirable behaviors from knowledge injection.",
      "arxiv_url": "https://openreview.net/forum?id=ZkGHzGIaMB",
      "pdf_url": "https://openreview.net/pdf/5551f5a5e3d4afa82644ac787d0ecdd02c0dad41.pdf",
      "primary_category": "Factual implication, Transformer, Implicit bias",
      "categories": [
        "Factual implication",
        "Transformer",
        "Implicit bias"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IMamKWkS1s",
      "title": "On the Sample Complexity of Differentially Private Policy Optimization",
      "authors": [
        "Yi He",
        "Xingyu Zhou"
      ],
      "abstract": "Policy optimization (PO) is a cornerstone of modern reinforcement learning (RL), with diverse applications spanning robotics, healthcare, and large language model training. The increasing deployment of PO in sensitive domains, however, raises significant privacy concerns. In this paper, we initiate a theoretical study of differentially private policy optimization, focusing explicitly on its sample complexity. We first formalize an appropriate definition of differential privacy (DP) tailored to PO, addressing the inherent challenges arising from on-policy learning dynamics and the subtlety involved in defining the unit of privacy. We then systematically analyze the sample complexity of widely-used PO algorithms, including policy gradient (PG), natural policy gradient (NPG) and more, under DP constraints and various settings,  via a unified framework. Our theoretical results demonstrate that privacy costs can often manifest as lower-order terms in the sample complexity, while also highlighting subtle yet important observations in private PO settings. These offer valuable practical insights for privacy-preserving PO algorithms.",
      "arxiv_url": "https://openreview.net/forum?id=IMamKWkS1s",
      "pdf_url": "https://openreview.net/pdf/d041c159b94f56c8a14986a0aa636f4ee9371900.pdf",
      "primary_category": "Policy optimization, differential privacy, policy gradient",
      "categories": [
        "Policy optimization",
        "differential privacy",
        "policy gradient",
        "natural policy gradient",
        "sample complexity"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "TFidSatsOC",
      "title": "Probing Hidden Knowledge Holes in Unlearned LLMs",
      "authors": [
        "Myeongseob Ko",
        "Hoang Anh Just",
        "Charles Fleming",
        "Ming Jin",
        "Ruoxi Jia"
      ],
      "abstract": "Machine unlearning has emerged as a prevalent technical solution for selectively removing unwanted knowledge absorbed during pre-training, without requiring full retraining. While recent unlearning techniques can effectively remove undesirable content without severely compromising performance on standard benchmarks, we find that they may inadvertently create ``knowledge holes''---unintended losses of benign knowledge that standard benchmarks fail to capture. To probe where unlearned models reveal knowledge holes, we propose a test case generation framework that explores both immediate neighbors of unlearned content and broader areas of potential failures.\nOur evaluation demonstrates significant hidden costs of unlearning: up to 98.7\\% of the test cases yield irrelevant or nonsensical responses from unlearned models, despite being answerable by the pretrained model. These findings necessitate rethinking the conventional approach to evaluating knowledge preservation in unlearning, moving beyond standard, static benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=TFidSatsOC",
      "pdf_url": "https://openreview.net/pdf/cf51329107b771838d1363439f1470c36d6805af.pdf",
      "primary_category": "unlearning, large language models, reinforcement learning",
      "categories": [
        "unlearning",
        "large language models",
        "reinforcement learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qTUZJUXt0J",
      "title": "3D-Prover: Diversity Driven Theorem Proving With Determinantal Point Processes",
      "authors": [
        "Sean Lamont",
        "Christian Walder",
        "Amir Dezfouli",
        "Paul Montague",
        "Michael Norrish"
      ],
      "abstract": "A key challenge in automated formal reasoning is the intractable search space, which grows exponentially with the depth of the proof. This branching is caused by the large number of candidate proof tactics which can be applied to a given goal. Nonetheless, many of these tactics are semantically similar or lead to an execution error, wasting valuable resources in both cases. We address the problem of effectively pruning this search, using only synthetic data generated from previous proof attempts. We first demonstrate that it is possible to generate semantically aware tactic representations which capture the effect on the proving environment, likelihood of success, and execution time. We then propose a novel filtering mechanism which leverages these representations to select semantically diverse and high quality tactics, using Determinantal Point Processes. Our approach, 3D-Prover, is designed to be general, and to augment any underlying tactic generator. We demonstrate the effectiveness of 3D-Prover on the miniF2F and LeanDojo benchmarks by augmenting popular open source proving LLMs. We show that our approach leads to an increase in the overall proof rate, as well as a significant improvement in the tactic success\n rate, execution time and diversity. We make our code available at https://github.com/sean-lamont/3D-Prover.",
      "arxiv_url": "https://openreview.net/forum?id=qTUZJUXt0J",
      "pdf_url": "https://openreview.net/pdf/bde5df5b7d335e39bffb3a70e0854df06f859e3b.pdf",
      "primary_category": "theorem proving, proof search, diversity",
      "categories": [
        "theorem proving",
        "proof search",
        "diversity",
        "representation learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2NLHoWE0eS",
      "title": "Reasoning as an Adaptive Defense for Safety",
      "authors": [
        "Taeyoun Kim",
        "Fahim Tajwar",
        "Aditi Raghunathan",
        "Aviral Kumar"
      ],
      "abstract": "Reasoning methods that adaptively allocate test-time compute have advanced LLM performance on easy to verify domains such as math and code. In this work, we study how to utilize this approach to train models that exhibit a degree of robustness to safety vulnerabilities, and show that doing so can provide benefits. We build a recipe called $\\textit{\\textbf{TARS}}$ (Training Adaptive Reasoners for Safety), a reinforcement learning (RL) approach that trains models to reason about safety using chain-of-thought traces and a reward signal that balances safety with task completion. To build TARS, we identify three critical design choices: (1) a ``lightweight'' warmstart SFT stage, (2) a mix of harmful, harmless, and ambiguous prompts to prevent shortcut behaviors such as too many refusals, and (3) a reward function to prevent degeneration of reasoning capabilities during training. Models trained with TARS exhibit adaptive behaviors by spending more compute on ambiguous queries, leading to better safety-refusal trade-offs. They also internally learn to better distinguish between safe and unsafe prompts and attain greater robustness to both white-box (e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an effective, open recipe for training LLMs against jailbreaks and harmful requests by reasoning per prompt.",
      "arxiv_url": "https://openreview.net/forum?id=2NLHoWE0eS",
      "pdf_url": "https://openreview.net/pdf/c036ebd40ea82ab94b9badc0028faf7e94456ca1.pdf",
      "primary_category": "Reasoning, Reasoning for Safety, LLM Safety",
      "categories": [
        "Reasoning",
        "Reasoning for Safety",
        "LLM Safety",
        "Jailbreaking"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "YghiOusmvw",
      "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity",
      "authors": [
        "Parshin Shojaee",
        "Seyed Iman Mirzadeh",
        "Keivan Alizadeh",
        "Maxwell Horton",
        "Samy Bengio",
        "Mehrdad Farajtabar"
      ],
      "abstract": "Recent generations of frontier language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood.\nCurrent evaluations primarily focus on established mathematical and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from data contamination and does not provide insights into the reasoning traces' structure and quality. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of compositional complexity while maintaining consistent logical structures.\nThis setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs ``think''.\nThrough extensive experimentation across diverse puzzles, we show that frontier LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having an adequate token budget. By comparing LRMs with their standard LLM counterparts under equivalent inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity tasks where additional thinking in LRMs demonstrates advantage, and (3) high-complexity tasks where both models experience complete collapse.\nWe found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales and problems. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and ultimately raising questions about the nature for their reasoning capabilities.",
      "arxiv_url": "https://openreview.net/forum?id=YghiOusmvw",
      "pdf_url": "https://openreview.net/pdf/b58069a804c5fad686ceb13a131631201748c264.pdf",
      "primary_category": "Reasoning, Large Language Models",
      "categories": [
        "Reasoning",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "R5EBtNE2Y9",
      "title": "HeavyWater and SimplexWater: Distortion-free LLM Watermarks for Low-Entropy Distributions",
      "authors": [
        "Dor Tsur",
        "Carol Xuan Long",
        "Claudio Mayrink Verdun",
        "Sajani Vithana",
        "Hsiang Hsu",
        "Chun-Fu Chen",
        "Haim H. Permuter",
        "Flavio Calmon"
      ],
      "abstract": "Large language model (LLM) watermarks  enable authentication of text provenance, curb misuse of machine-generated text, and promote trust in AI systems. Current watermarks operate by changing the next-token predictions output by an LLM. The updated (i.e., watermarked) predictions depend on random side information produced, for example, by hashing previously generated tokens. LLM watermarking is particularly challenging in low-entropy generation tasks -- such as coding -- where next-token predictions are near-deterministic. In this paper, we propose an optimization framework for watermark design. Our goal is to understand how to most effectively use random side information in order to maximize the likelihood of watermark detection and minimize the distortion of generated text. Our analysis informs the design of two new watermarks: HeavyWater and SimplexWater. Both watermarks are tunable, gracefully trading-off between detection accuracy and text distortion. They can also be applied to any LLM and are agnostic to side information generation.  We examine the performance of HeavyWater and SimplexWater through several benchmarks, demonstrating that they can achieve high watermark detection accuracy with minimal compromise of text generation quality, particularly in the low-entropy regime. Our theoretical analysis also reveals surprising new connections between LLM watermarking and coding theory.",
      "arxiv_url": "https://openreview.net/forum?id=R5EBtNE2Y9",
      "pdf_url": "https://openreview.net/pdf/68191c24617a3c939d4aa0215aaf5a8e58670c72.pdf",
      "primary_category": "Large language models, watermarking, coding theory",
      "categories": [
        "Large language models",
        "watermarking",
        "coding theory",
        "heavy-tail distributions"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pzAeKALSfX",
      "title": "Self Iterative Label Refinement via Robust Unlabeled Learning",
      "authors": [
        "Hikaru Asano",
        "Tadashi Kozuno",
        "Yukino Baba"
      ],
      "abstract": "Recent advances in large language models (LLMs) have yielded impressive performance on various tasks, yet they often depend on high-quality feedback that can be costly. Self-refinement methods attempt to leverage LLMs' internal evaluation mechanisms with minimal human supervision; however, these approaches frequently suffer from inherent biases and overconfidence, especially in domains where the models lack sufficient internal knowledge, resulting in performance degradation. As an initial step toward enhancing self-refinement for broader applications, we introduce an iterative refinement pipeline that employs the Unlabeled-Unlabeled learning framework to improve LLM-generated pseudo-labels for classification tasks. By exploiting two unlabeled datasets with differing positive class ratios, our approach iteratively denoises and refines the initial pseudo-labels, thereby mitigating the adverse effects of internal biases with minimal human supervision. Evaluations on diverse datasets, including low-resource language corpora, patent classifications, and protein structure categorizations, demonstrate that our method consistently outperforms both initial LLM's classification performance and the self-refinement approaches by cutting-edge models (e.g., GPT-4o and DeepSeek-R1). Moreover, we experimentally confirm that our refined classifier facilitates effective post-training alignment for safety in LLMs and demonstrate successful self-refinement in generative tasks as well. Our code is available at https://github.com/HikaruAsano/self-iterative-label-refinement.",
      "arxiv_url": "https://openreview.net/forum?id=pzAeKALSfX",
      "pdf_url": "https://openreview.net/pdf/8cce56bfc0ff4e8062cb20443af02884c82a343e.pdf",
      "primary_category": "Large Language Models, Weakly Supervised Learning, LLM Self-Refinement",
      "categories": [
        "Large Language Models",
        "Weakly Supervised Learning",
        "LLM Self-Refinement",
        "Low Resource Domain"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "T1V8BJO0iG",
      "title": "Accelerating RL for LLM Reasoning with Optimal Advantage Regression",
      "authors": [
        "Kianté Brantley",
        "Mingyu Chen",
        "Zhaolin Gao",
        "Jason D. Lee",
        "Wen Sun",
        "Wenhao Zhan",
        "Xuezhou Zhang"
      ],
      "abstract": "Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning large language models (LLMs) to improve complex reasoning abilities. However, state-of-the-art policy optimization methods often suffer from high computational overhead and memory consumption, primarily due to the need for multiple generations per prompt and the reliance on critic networks or advantage estimates of the current policy. In this paper, we propose $A^\\star$-PO, a novel two-stage policy optimization framework that directly approximates the optimal advantage function and enables efficient training of LLMs for reasoning tasks. In the first stage, we leverage offline sampling from a reference policy to estimate the optimal value function $V^\\star$, eliminating the need for costly online value estimation. In the second stage, we perform on-policy updates using a simple least-squares regression loss with only a single generation per prompt. Theoretically, we establish performance guarantees and prove that the KL-regularized RL objective can be optimized without requiring complex exploration strategies. Empirically, $A^\\star$-PO achieves competitive performance across a wide range of mathematical reasoning benchmarks, while reducing training time by up to 2$\\times$ and peak memory usage by over 30\\% compared to PPO, GRPO, and REBEL. Implementation of $A^\\star$-PO can be found at https://github.com/ZhaolinGao/A-PO.",
      "arxiv_url": "https://openreview.net/forum?id=T1V8BJO0iG",
      "pdf_url": "https://openreview.net/pdf/b1758af58d36e9832bc3a336c75ffb891ed2aa37.pdf",
      "primary_category": "reinforcement learning, large language models, reasoning",
      "categories": [
        "reinforcement learning",
        "large language models",
        "reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oLJMsGMfqr",
      "title": "Data Selection Matters: Towards Robust Instruction Tuning of Large Multimodal Models",
      "authors": [
        "Xu Yang",
        "Chen Liu",
        "Ying Wei"
      ],
      "abstract": "Selecting a compact subset of visual instruction–following data has emerged as an effective way to align large multimodal models with human intentions while avoiding the high cost of full-dataset training. Yet we observe that both full-data training and existing state-of-the-art data selection methods tend to inherit underlying dataset biases such as position bias and spurious correlations, leading to biased model behaviors. To address this issue, we introduce ARDS, a robustness-aware targeted visual instruction-selection framework that explicitly mitigates these weaknesses, sidestepping the need for access to downstream data or time-consuming gradient computation. Specifically, we first identify the worst-case evaluation subgroups through visual and textual task-specific perturbations. The robust training mixture is then constructed by prioritizing samples that are semantically closer to these subgroups in a rich multimodal embedding space. Extensive experiments demonstrate that ARDS substantially boosts both robustness and data efficiency for visual instruction tuning. We also showcase that the robust mixtures produced with a smaller model transfer effectively to larger architectures. Our code and selected datasets that have been demonstrated transferable across models are available at https://github.com/xyang583/ARDS.",
      "arxiv_url": "https://openreview.net/forum?id=oLJMsGMfqr",
      "pdf_url": "https://openreview.net/pdf/cb4fd9d52251f1db735f6d94c48d6610bd511078.pdf",
      "primary_category": "Large Multimodal Models, Data Selection, Robustness",
      "categories": [
        "Large Multimodal Models",
        "Data Selection",
        "Robustness"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "uC3DI4YPRv",
      "title": "Let the LLM Stick to Its Strengths: Learning to Route Economical LLM",
      "authors": [
        "Yi-Kai Zhang",
        "Shiyin Lu",
        "Qing-Guo Chen",
        "Weihua Luo",
        "De-Chuan Zhan",
        "Han-Jia Ye"
      ],
      "abstract": "Recently, test-time scaling of Large Language Models (LLMs) has emerged as a practical alternative to parameter and data scaling. Reasoning tasks often require large-scale, RLVR-based LLMs, while more economical LLMs can handle simpler tasks. Routing an LLM tailored to *suitability* (*i.e.*, capability and cost) ensures usability and efficiency. We introduce LLMRec, which routes the most suitable LLM to the user query without pre-inference on the candidate LLM zoo. It pioneeringly reframes the LLM routing problem as a comprehensive recommendation system (RecSys) task. Our core insight is that an LLM's suitability for a query is a complex, latent signal equal to user-item preference. LLMRec systematically engineers features for candidate LLMs (intrinsic attributes and capability distributions), queries (general semantics and meta-dimensional info), and context (inference type, cost budgets). It also incorporates behavioral features to learn high-order interactions. LLMRec is designed to generalize to out-of-domain datasets and adapt to new LLMs as the model zoo evolves. We define the metric with the Pareto frontier under user-specified cost budgets. Across six datasets, LLMRec achieves an average cost reduction of over 38% while maintaining accuracy and consistently outperforming baselines in converging toward the Pareto frontier.",
      "arxiv_url": "https://openreview.net/forum?id=uC3DI4YPRv",
      "pdf_url": "https://openreview.net/pdf/f8ce3f48c335fa784e0e6aef63e636d13c30d93d.pdf",
      "primary_category": "LLM Routing, Deep Learning",
      "categories": [
        "LLM Routing",
        "Deep Learning"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "X9SNL203cV",
      "title": "Evaluating LLMs in Open-Source Games",
      "authors": [
        "Swadesh Sistla",
        "Max Kleiman-Weiner"
      ],
      "abstract": "Large Language Models' (LLMs) programming capabilities enable their participation in \\textit{open-source games}: a game-theoretic setting in which players submit computer programs in lieu of actions. These programs offer numerous advantages, including interpretability, inter-agent transparency, and formal verifiability; additionally, they enable \\textit{program equilibria}, solutions that leverage the transparency of code and are inaccessible within normal-form settings. We evaluate the capabilities of leading open- and closed-weight LLMs to predict and classify program strategies and evaluate features of the approximate program equilibria reached by LLM agents in dyadic and evolutionary settings. We identify the emergence of payoff-maximizing, cooperative, and deceptive strategies, characterize the adaptation of mechanisms within these programs over repeated open-source games, and analyze their comparative evolutionary fitness. We find that open-source games serve as a viable environment to study and steer the emergence of cooperative strategy in multi-agent dilemmas.",
      "arxiv_url": "https://openreview.net/forum?id=X9SNL203cV",
      "pdf_url": "https://openreview.net/pdf/3d58c69a34e157d585289272c62e7c1da7a7994e.pdf",
      "primary_category": "LLM evaluation, program equilibrium, game theory",
      "categories": [
        "LLM evaluation",
        "program equilibrium",
        "game theory",
        "open source game",
        "cooperative AI",
        "multi-agent safety"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "OJAW2mHVND",
      "title": "Revising and Falsifying Sparse Autoencoder Feature Explanations",
      "authors": [
        "George Ma",
        "Samuel Pfrommer",
        "Somayeh Sojoudi"
      ],
      "abstract": "Mechanistic interpretability research seeks to reverse-engineer large language models (LLMs) by uncovering the internal representations of concepts within their activations. Sparse Autoencoders (SAEs) have emerged as a valuable tool for disentangling polysemantic neurons into more monosemantic, interpretable features. However, recent work on automatic explanation generation for these features has faced challenges: explanations tend to be overly broad and fail to take polysemanticity into consideration. This work addresses these limitations by introducing a similarity-based strategy for sourcing close negative sentences that more effectively falsify generated explanations. Additionally, we propose a structured, component-based format for feature explanations and a tree-based, iterative explanation method that refines explanations. We demonstrate that our structured format and tree-based explainer improve explanation quality, while our similarity-based evaluation strategy exposes biases in existing interpretability methods. We also analyze the evolution of feature complexity and polysemanticity across LLM layers, offering new insights into information content within LLMs' residual streams.",
      "arxiv_url": "https://openreview.net/forum?id=OJAW2mHVND",
      "pdf_url": "https://openreview.net/pdf/43af755479d42aa219d005a802edf1d59243b5d5.pdf",
      "primary_category": "Mechanistic Interpretability",
      "categories": [
        "Mechanistic Interpretability"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "fYjF9KIJd5",
      "title": "Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text",
      "authors": [
        "Yize CHENG",
        "Vinu Sankar Sadasivan",
        "Mehrdad Saberi",
        "Shoumik Saha",
        "Soheil Feizi"
      ],
      "abstract": "The increasing capabilities of Large Language Models (LLMs) have raised concerns about their misuse in AI-generated plagiarism and social engineering. While various AI-generated text detectors have been proposed to mitigate these risks, many remain vulnerable to simple evasion techniques such as paraphrasing. However, recent detectors have shown greater robustness against such basic attacks. In this work, we introduce \\textbf{Adversarial Paraphrasing}, a training-free attack framework that universally humanizes any AI-generated text to evade detection more effectively. Our approach leverages an off-the-shelf instruction-following LLM to paraphrase AI-generated content under the guidance of an AI text detector, producing adversarial examples that are specifically optimized to bypass detection. Extensive experiments show that our attack is both broadly effective and highly transferable across several detection systems. For instance, compared to simple paraphrasing attack—which, ironically, increases the true positive at 1\\% false positive (T@1\\%F) by 8.57\\% on RADAR and 15.03\\% on Fast-DetectGPT—adversarial paraphrasing, guided by OpenAI-RoBERTa-Large, reduces T@1\\%F by 64.49\\% on RADAR and a striking 98.96\\% on Fast-DetectGPT. Across a diverse set of detectors—including neural network-based, watermark-based, and zero-shot approaches—our attack achieves an average T@1\\%F reduction of 87.88\\% under the guidance of OpenAI-RoBERTa-Large. \nWe also analyze the tradeoff between text quality and our attack success to find that our method can significantly reduce detection rates, with mostly a slight degradation in text quality.\nOur novel adversarial setup highlights the need for more robust and resilient detection strategies in the light of increasingly sophisticated evasion techniques.",
      "arxiv_url": "https://openreview.net/forum?id=fYjF9KIJd5",
      "pdf_url": "https://openreview.net/pdf/2357002e3156e9036a476e91d322182ec4e59e6f.pdf",
      "primary_category": "AI Text Detection, Adversarial Attacks",
      "categories": [
        "AI Text Detection",
        "Adversarial Attacks"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VcJTTVoysQ",
      "title": "Alignment of Large Language Models with Constrained Learning",
      "authors": [
        "Botong Zhang",
        "Shuo Li",
        "Ignacio Hounie",
        "Osbert Bastani",
        "Dongsheng Ding",
        "Alejandro Ribeiro"
      ],
      "abstract": "We study the problem of computing an optimal large language model (LLM) policy for the constrained alignment problem, where the goal is to maximize a primary reward objective while satisfying constraints on secondary utilities. Despite the popularity of Lagrangian-based LLM policy search in constrained alignment, iterative primal-dual methods often fail to converge, and non-iterative dual-based methods do not achieve optimality in the LLM parameter space. To address these challenges, we employ Lagrangian duality to develop an iterative dual-based alignment method that alternates between updating the LLM policy via Lagrangian maximization and updating the dual variable via dual descent. In theory, we characterize the primal-dual gap between the primal value in the distribution space and the dual value in the LLM parameter space. We further quantify the optimality gap of the learned LLM policies at near-optimal dual variables with respect to both the objective and the constraint functions. These results prove that dual-based alignment methods can find an optimal constrained LLM policy, up to an LLM parametrization gap. We demonstrate the effectiveness and merits of our approach through extensive experiments conducted on the PKU-SafeRLHF and Anthropic HH-RLHF datasets.",
      "arxiv_url": "https://openreview.net/forum?id=VcJTTVoysQ",
      "pdf_url": "https://openreview.net/pdf/92d52db69bb7212a58a713b4d9f44da215d9f670.pdf",
      "primary_category": "Large Language Models, Alignment, RLHF",
      "categories": [
        "Large Language Models",
        "Alignment",
        "RLHF",
        "Safety",
        "Duality",
        "Optimality gap"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "iyFH9KRGBo",
      "title": "Correlation Dimension of Autoregressive Large Language Models",
      "authors": [
        "Xin Du",
        "Kumiko Tanaka-Ishii"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable progress in natural\nlanguage generation, yet they continue to display puzzling behaviors—such as\nrepetition and incoherence—even when exhibiting low perplexity. This\nhighlights a key limitation of conventional evaluation metrics, which\nemphasize local prediction accuracy while overlooking long-range structural\ncomplexity.  We introduce correlation dimension, a fractal-geometric measure\nof self-similarity, to quantify the epistemological complexity of text as\nperceived by a language model. This measure captures the hierarchical\nrecurrence structure of language, bridging local and global properties in a\nunified framework.  Through extensive experiments, we show that correlation\ndimension (1) reveals three distinct phases during pretraining, (2) reflects\ncontext-dependent complexity, (3) indicates a model's tendency toward\nhallucination, and (4) reliably detects multiple forms of degeneration in\ngenerated text.  The method is computationally efficient, robust to model\nquantization (down to 4-bit precision), broadly applicable across\nautoregressive architectures (e.g., Transformer and Mamba), and provides\nfresh insight into the generative dynamics of LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=iyFH9KRGBo",
      "pdf_url": "https://openreview.net/pdf/0f28c3f754ee9af94b67b7d1c2e7b5d2637ab0fe.pdf",
      "primary_category": "correlation dimension, fractal dimension, large language models",
      "categories": [
        "correlation dimension",
        "fractal dimension",
        "large language models",
        "self-similarity",
        "complexity",
        "degeneration",
        "hallucination",
        "LLM evaluation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IBrRNLr6JA",
      "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example",
      "authors": [
        "Yiping Wang",
        "Qing Yang",
        "Zhiyuan Zeng",
        "Liliang Ren",
        "Liyuan Liu",
        "Baolin Peng",
        "Hao Cheng",
        "Xuehai He",
        "Kuan Wang",
        "Jianfeng Gao",
        "Weizhu Chen",
        "Shuohang Wang",
        "Simon Shaolei Du",
        "yelong shen"
      ],
      "abstract": "We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0\\% to 73.6\\% (8.6\\% improvement beyond format correction), and improves the average performance across six common mathematical reasoning benchmarks from 17.6\\% to 35.7\\% (7.0\\% non-format gain). This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6\\%, average: 35.9\\%), which contains the aforementioned example. Furthermore, RLVR with only two examples even slightly exceeds these results (MATH500: 74.8\\%, average: 36.6\\%). Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples. \nIn addition, we identify some interesting phenomena during 1-shot RLVR, including cross-category generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term \\textit{post-saturation generalization}.\nMoreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the \"grokking\" phenomenon.\nWe also show the critical role of promoting exploration (e.g., by incorporating entropy loss with an appropriate coefficient) in 1-shot RLVR training.\nWe also further discuss related observations about format correction, label robustness and prompt modification.\nThese findings can inspire future work on RLVR efficiency and encourage a re-examination of recent progress and the underlying mechanisms in RLVR. \nOur code, models, and data are open source at https://github.com/ypwang61/One-Shot-RLVR.",
      "arxiv_url": "https://openreview.net/forum?id=IBrRNLr6JA",
      "pdf_url": "https://openreview.net/pdf/6488aab2ea1a4b2423d232a17c9fcd1545659f8c.pdf",
      "primary_category": "RLVR, math reasoning, reinforcement learning",
      "categories": [
        "RLVR",
        "math reasoning",
        "reinforcement learning",
        "post-training",
        "language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Kc1WTxZbrP",
      "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
      "authors": [
        "Weijia Shi",
        "Xiaochuang Han",
        "Chunting Zhou",
        "Weixin Liang",
        "Xi Victoria Lin",
        "Luke Zettlemoyer",
        "LILI YU"
      ],
      "abstract": "We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.",
      "arxiv_url": "https://openreview.net/forum?id=Kc1WTxZbrP",
      "pdf_url": "https://openreview.net/pdf/14b38a139b47e15b756a5e1cdfa7f9a73fdc10ed.pdf",
      "primary_category": "Multimodal",
      "categories": [
        "Multimodal"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2D4TuZyNnr",
      "title": "REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving",
      "authors": [
        "Annabelle Sujun Tang",
        "Christopher Priebe",
        "Rohan Mahapatra",
        "Lianhui Qin",
        "Hadi Esmaeilzadeh"
      ],
      "abstract": "While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed REASONING COMPILER) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating\na structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.",
      "arxiv_url": "https://openreview.net/forum?id=2D4TuZyNnr",
      "pdf_url": "https://openreview.net/pdf/b69150e79674e97ab8c8f5920049108fa643485b.pdf",
      "primary_category": "Compiler, Optimization, LLM",
      "categories": [
        "Compiler",
        "Optimization",
        "LLM",
        "LLMs for Code Reasoning and Optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "x5KUOlYKQr",
      "title": "Listwise Preference Diffusion Optimization for User Behavior Trajectories Prediction",
      "authors": [
        "Hongtao Huang",
        "Chengkai Huang",
        "Junda Wu",
        "Tong Yu",
        "Julian McAuley",
        "Lina Yao"
      ],
      "abstract": "Forecasting multi-step user behavior trajectories requires reasoning over structured preferences across future actions, a challenge overlooked by traditional sequential recommendation. This problem is critical for applications such as personalized commerce and adaptive content delivery, where anticipating a user’s complete action sequence enhances both satisfaction and business outcomes. We identify an essential limitation of existing paradigms: their inability to capture global, listwise dependencies among sequence items. To address this, we formulate User Behavior Trajectory Prediction (UBTP) as a new task setting that explicitly models longterm user preferences. We introduce Listwise Preference Diffusion Optimization (LPDO), a diffusion-based training framework that directly optimizes structured preferences over entire item sequences. LPDO incorporates a Plackett–Luce supervision signal and derives a tight variational lower bound aligned with listwise ranking likelihoods, enabling coherent preference generation across denoising steps and overcoming the independent-token assumption of prior diffusion methods. To rigorously evaluate multi-step prediction quality, we propose the task-specific metric: Sequential Match (SeqMatch), which measures exact trajectory agreement, and adopt Perplexity (PPL), which assesses probabilistic fidelity. Extensive experiments on real-world user behavior benchmarks demonstrate that LPDO consistently outperforms state-of-the-art baselines, establishing a new benchmark for structured preference learning with diffusion models.",
      "arxiv_url": "https://openreview.net/forum?id=x5KUOlYKQr",
      "pdf_url": "https://openreview.net/pdf/79c9656f9fbcb2d35333ef7db26f282764f9221c.pdf",
      "primary_category": "User Behavior Prediction, Diffusion Models",
      "categories": [
        "User Behavior Prediction",
        "Diffusion Models"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cZVYswQQMt",
      "title": "ActiveVOO: Value of Observation Guided Active Knowledge Acquisition for Open-World Embodied Lifted Regression Planning",
      "authors": [
        "Xiaotian Liu",
        "Ali Pesaranghader",
        "Jaehong Kim",
        "Tanmana Sadhu",
        "Hyejeong Jeon",
        "Scott Sanner"
      ],
      "abstract": "The ability to actively acquire information is essential for open-world planning under partial observability and incomplete knowledge. However, most existing embodied AI systems either assume a known object category or rely on passive perception strategies that exhaustively gather object and relational information from the environment. Such a strategy becomes insufficient in visually complex open-world settings. For instance, a typical household may contain thousands of novel and uniquely configured objects, most of which are irrelevant to the agent’s current task. Consequently, open-world agents must be capable of actively identifying and prioritizing task-relevant objects to enable efficient and goal-directed knowledge acquisition. In this work, we introduce ActiveVOO, a novel zero-shot framework for open-world embodied planning that emphasizes object-centric active knowledge acquisition. ActiveVOO employs lifted regression to generate compact, first-order subgoal descriptions that identify task-relevant objects, and provides a principled mechanism to quantify the utility of sensing actions based on commonsense priors derived from LLMs and VLMs. We evaluate ActiveVOO on the visual ALFWorld benchmark, where it achieves substantial improvements over existing LLM- and VLM-based planning approaches, notably outperforming VLMs fine-tuned on ALFWorld data. This work establishes a principled foundation for developing embodied agents capable of actively and efficiently acquiring knowledge to plan and act in open-world environments.",
      "arxiv_url": "https://openreview.net/forum?id=cZVYswQQMt",
      "pdf_url": "https://openreview.net/pdf/80a9fe6c8d4390dd206aea141b9810f1a6472f7f.pdf",
      "primary_category": "Open-world Planning, Embodied Agents, Active Knowledge Acquisition",
      "categories": [
        "Open-world Planning",
        "Embodied Agents",
        "Active Knowledge Acquisition",
        "Lifted Regression"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "z0WhTwZscg",
      "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents",
      "authors": [
        "Han Lin",
        "Jaemin Cho",
        "Amir Zadeh",
        "Chuan Li",
        "Mohit Bansal"
      ],
      "abstract": "There is growing interest in integrating high-fidelity visual synthesis capabilities into large language models (LLMs) without compromising their strong reasoning capabilities. Existing methods that directly train LLMs or bridge LLMs and diffusion models usually suffer from costly training since the backbone LLMs have not seen image representations during pretraining. We present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs (MLLMs) and diffusion models using patch-level CLIP image embeddings as latent variables, which are natively aligned with the MLLM's CLIP visual encoder. These patch-level image embeddings are integrated into the diffusion model with a lightweight adaptation of its ControlNet. To retain the original multimodal reasoning capabilities of MLLMs, we equip the MLLM with a visual generation branch initialized from the original MLLM parameters when predicting the patch-level image embeddings. By seamlessly integrating pretrained MLLMs and diffusion models with patch-level CLIP latents, our framework enables high-fidelity controllable image generation with significant training efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or better performance than previous methods in terms of visual fidelity and multimodal understanding, with substantially lower compute during training. We also provide comprehensive ablation studies showing the effectiveness of our design choices. Project page: https://bifrost-1.github.io.",
      "arxiv_url": "https://openreview.net/forum?id=z0WhTwZscg",
      "pdf_url": "https://openreview.net/pdf/d4efd2136d89f42be15fb5e010742ba72c0f0cb2.pdf",
      "primary_category": "Multimodal LLM, Unified Multimodal LLM, Diffusion Models",
      "categories": [
        "Multimodal LLM",
        "Unified Multimodal LLM",
        "Diffusion Models",
        "Autoregressive Priors",
        "Image Generation"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MBJJ9Wcpg9",
      "title": "One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models",
      "authors": [
        "Viacheslav Surkov",
        "Chris Wendler",
        "Antonio Mari",
        "Mikhail Terekhov",
        "Justin Deschenaux",
        "Robert West",
        "Caglar Gulcehre",
        "David Bau"
      ],
      "abstract": "For large language models (LLMs), sparse autoencoders (SAEs) have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigate the possibility of using SAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image diffusion model. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-net in its 1-step setting. Interestingly, we find that they generalize to 4-step SDXL Turbo and even to the multi-step SDXL base model (i.e., a different model) without additional training. In addition, we show that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. We do so by creating RIEBench, a representation-based image editing benchmark, for editing images while they are generated by turning on and off individual SAE features. This allows us to track which transformer blocks' features are the most impactful depending on the edit category. Our work is the first investigation of SAEs for interpretability in text-to-image diffusion models and our results establish SAEs as a promising approach for understanding and manipulating the internal mechanisms of text-to-image models.",
      "arxiv_url": "https://openreview.net/forum?id=MBJJ9Wcpg9",
      "pdf_url": "https://openreview.net/pdf/211cdb7b93275f5f3a7712b555f9a036379bd7fc.pdf",
      "primary_category": "sparse autoencoders, SAE, unet",
      "categories": [
        "sparse autoencoders",
        "SAE",
        "unet",
        "diffusion",
        "mechanistic interpretability",
        "text to image interpretability",
        "activation engineering"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7ieS4EYKnB",
      "title": "Flick: Empowering Federated Learning with Commonsense Knowledge",
      "authors": [
        "Ran Zhu",
        "Mingkun Yang",
        "Shiqiang Wang",
        "Jie Yang",
        "Qing Wang"
      ],
      "abstract": "Federated Learning (FL) has emerged as a privacy-preserving framework for training models on data generated at the edge. However, the heterogeneity of data silos (e.g., label skew and domain shift) often leads to inconsistent learning objectives and suboptimal model performance. Inspired by the data-driven approach, we propose Flick, a novel data generation framework for heterogeneous **F**ederated **L**earning w**i**th **C**ommonsense **K**nowledge from Large Language Models (LLMs). In Flick, the client performs the local data summary to capture client-specific knowledge in textual form. The central server then distills task-relevant, high-quality knowledge from the out-of-the-box LLM -- guided by cross-client-specific insights -- to generate informative text prompts. These prompts direct a generative model in producing synthetic data, enabling global model fine-tuning and local data compensation. This process gradually aligns the label and feature distributions across clients. Extensive results on three datasets demonstrate that Flick improves the global model accuracy by up to 11.43\\%, and accelerates convergence by up to 12.9$\\times$, validating its effectiveness in addressing data heterogeneity.",
      "arxiv_url": "https://openreview.net/forum?id=7ieS4EYKnB",
      "pdf_url": "https://openreview.net/pdf/f09ed72312ebc714ef40dd7c5f046681e9f30b87.pdf",
      "primary_category": "Federated Learning, Data Heterogeneity",
      "categories": [
        "Federated Learning",
        "Data Heterogeneity"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "KlZUwDP0pR",
      "title": "Tree of Preferences for Diversified Recommendation",
      "authors": [
        "Hanyang Yuan",
        "Ning Tang",
        "Tongya Zheng",
        "Jiarong Xu",
        "Xintong Hu",
        "Renhong Huang",
        "Shunyu Liu",
        "Jiacong Hu",
        "Jiawei Chen",
        "Mingli Song"
      ],
      "abstract": "Diversified recommendation has attracted increasing attention from both researchers and practitioners, which can effectively address the homogeneity of recommended items.\nExisting approaches predominantly aim to infer the diversity of user preferences from observed user feedback.\nNonetheless, due to inherent data biases, the observed data may not fully reflect user interests, where underexplored preferences can be overwhelmed or remain unmanifested. Failing to capture these preferences can lead to suboptimal diversity in recommendations. To fill this gap,  this work aims to study diversified recommendation from a data-bias perspective.\nInspired by the outstanding performance of large language models (LLMs) in zero-shot inference leveraging world knowledge, we propose a novel approach that utilizes LLMs' expertise to uncover underexplored user preferences from observed behavior, ultimately providing diverse and relevant recommendations.\nTo achieve this, we first introduce Tree of Preferences (ToP), an innovative structure constructed to model user preferences from coarse to fine. ToP enables LLMs to systematically reason over the user's rationale behind their behavior, thereby uncovering their underexplored preferences.\nTo guide diversified recommendations using uncovered preferences, we adopt a data-centric approach, identifying candidate items that match user preferences and generating synthetic interactions that reflect underexplored preferences.  These interactions are integrated to train a general recommender for diversification.\nMoreover, we scale up overall efficiency by dynamically selecting influential users during optimization.\nExtensive evaluations of both diversity and relevance show that our approach outperforms existing methods in most cases and achieves near-optimal performance in others, with reasonable inference latency.",
      "arxiv_url": "https://openreview.net/forum?id=KlZUwDP0pR",
      "pdf_url": "https://openreview.net/pdf/e026e2523a28ce8d1fb92eb33e565541d246a470.pdf",
      "primary_category": "Large language model, recommendation diversity",
      "categories": [
        "Large language model",
        "recommendation diversity"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IpZbmX90sI",
      "title": "Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency",
      "authors": [
        "Kelvin Kan",
        "Xingjian Li",
        "Benjamin Zhang",
        "Tuhin Sahai",
        "Stanley Osher",
        "Markos Katsoulakis"
      ],
      "abstract": "We study Transformers through the perspective of optimal control theory, using tools from continuous-time formulations to derive actionable insights into training and architecture design. This framework improves the performance of existing Transformer models while providing desirable theoretical guarantees, including generalization and robustness. Our framework is designed to be plug-and-play, enabling seamless integration with established Transformer models and requiring only slight changes to the implementation. We conduct seven extensive experiments on tasks motivated by text generation, sentiment analysis, image classification, and point cloud classification. Experimental results show that the framework improves the test performance of the baselines, while being more parameter-efficient. On character-level text generation with nanoGPT, our framework achieves a 46\\% reduction in final test loss while using 42\\% fewer parameters. On GPT-2, our framework achieves a 9.3\\% reduction in final test loss, demonstrating scalability to larger models. To the best of our knowledge, this is the first work that applies optimal control theory to both the training and architecture of Transformers. It offers a new foundation for systematic, theory-driven improvements and moves beyond costly trial-and-error approaches.",
      "arxiv_url": "https://openreview.net/forum?id=IpZbmX90sI",
      "pdf_url": "https://openreview.net/pdf/6181acd9b25e98187305e98690579796dd383f37.pdf",
      "primary_category": "Transformer, LLM, Uncertainty Quantification",
      "categories": [
        "Transformer",
        "LLM",
        "Uncertainty Quantification",
        "Optimal Control Theory",
        "Optimal Transport"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "l3Qq5MU5VX",
      "title": "AdmTree: Compressing Lengthy Context with Adaptive Semantic Trees",
      "authors": [
        "Yangning Li",
        "Shaoshen Chen",
        "Yinghui Li",
        "Yankai Chen",
        "Hai-Tao Zheng",
        "Hui Wang",
        "Wenhao Jiang",
        "Philip S. Yu"
      ],
      "abstract": "The quadratic complexity of self-attention limits Large Language Models (LLMs) in processing long contexts, a capability vital for many advanced applications. Context compression aims to mitigate this computational barrier while preserving essential semantic information. However, existing methods often falter: explicit methods can sacrifice local detail, while implicit ones may exhibit positional biases, struggle with information degradation, or fail to capture long-range semantic dependencies. We introduce AdmTree, a novel framework for adaptive, hierarchical context compression designed with a core focus on maintaining high semantic fidelity while keep efficiency. AdmTree dynamically segments input based on information density, employing gist tokens to summarize variable-length segments as leaves in a semantic binary tree. This structure, combined with a lightweight aggregation mechanism and a frozen backbone LLM (minimizing new trainable parameters), enables efficient hierarchical abstraction of the context. By effectively preserving fine-grained details alongside global semantic coherence, mitigating position bias, and adapting dynamically to content, AdmTree comprehensively preserves the semantic information of lengthy context.",
      "arxiv_url": "https://openreview.net/forum?id=l3Qq5MU5VX",
      "pdf_url": "https://openreview.net/pdf/e9f0cde9de1b7fd3ebe200857c00814548395279.pdf",
      "primary_category": "Language Model, Context Compression",
      "categories": [
        "Language Model",
        "Context Compression"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4kTpb8pITI",
      "title": "NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge",
      "authors": [
        "Hanyu Zhu",
        "Lance Fiondella",
        "Jiawei Yuan",
        "Kai Zeng",
        "Long Jiao"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) empowers Large Language Models (LLMs) to dynamically integrate external knowledge during inference, improving their factual accuracy and adaptability. However, adversaries can inject poisoned external knowledge to override the model’s internal memory. While existing attacks iteratively manipulate retrieval content or prompt structure of RAG, they largely ignore the model’s internal representation dynamics and neuron-level sensitivities. The underlying mechanism of RAG poisoning has not been fully studied and the effect of knowledge conflict with strong parametric knowledge in RAG is not considered. In this work, we propose NeuroGenPoisoning, a novel attack framework that generates adversarial external knowledge in RAG guided by LLM internal neuron attribution and genetic optimization. Our method first identifies a set of **Poison-Responsive Neurons** whose activation strongly correlates with contextual poisoning knowledge. We then employ a genetic algorithm to evolve adversarial passages that maximally activate these neurons. Crucially, our framework enables massive-scale generation of effective poisoned RAG knowledge by identifying and reusing promising but initially unsuccessful external knowledge variants via observed attribution signals. At the same time, Poison-Responsive Neurons guided poisoning can effectively resolves knowledge conflict. Experimental results across models and datasets demonstrate consistently achieving high Population Overwrite Success Rate (POSR) of over 90\\% while preserving fluency. Empirical evidence shows that our method effectively resolves knowledge conflict.",
      "arxiv_url": "https://openreview.net/forum?id=4kTpb8pITI",
      "pdf_url": "https://openreview.net/pdf/1103e0d03b7fc6a7a51729f4fc324173dc6f98a1.pdf",
      "primary_category": "Adversarial Attacks; Poisoning Attacks; Security; Large Language Models; Retrieval-Augmented Generation",
      "categories": [
        "Adversarial Attacks; Poisoning Attacks; Security; Large Language Models; Retrieval-Augmented Generation"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "rbBtoVnduo",
      "title": "Reasoning Models Better Express Their Confidence",
      "authors": [
        "Dongkeun Yoon",
        "Seungone Kim",
        "Sohee Yang",
        "Sunkyoung Kim",
        "Soyeon Kim",
        "Yongil Kim",
        "Eunbi Choi",
        "Yireun Kim",
        "Minjoon Seo"
      ],
      "abstract": "Despite their strengths, large language models (LLMs) often fail to communicate their confidence accurately, making it difficult to assess when they might be wrong and limiting their reliability. In this work, we demonstrate that reasoning models that engage in extended chain-of-thought (CoT) reasoning exhibit superior performance not only in problem-solving but also in accurately expressing their confidence.\nSpecifically, we benchmark six reasoning models across six datasets and find that they achieve strictly better confidence calibration than their non-reasoning counterparts in 33 out of the 36 settings. Our detailed analysis reveals that these gains in calibration stem from the slow thinking behaviors of reasoning models (e.g., exploring alternative approaches and backtracking) which enable them to adjust their confidence dynamically throughout their CoT, making it progressively more accurate. In particular, we find that reasoning models become increasingly better calibrated as their CoT unfolds, a trend not observed in non-reasoning models. Moreover, removing slow thinking behaviors from the CoT leads to a significant drop in calibration. Lastly, we show that non-reasoning models also demonstrate enhanced calibration when simply guided to slow think via in-context learning, fully isolating slow thinking as the source of the calibration gains.",
      "arxiv_url": "https://openreview.net/forum?id=rbBtoVnduo",
      "pdf_url": "https://openreview.net/pdf/778de0b434d93f5f9d2e0a107690e42fd0ebd1c0.pdf",
      "primary_category": "Reasoning Models, Confidence Estimation, Calibration",
      "categories": [
        "Reasoning Models",
        "Confidence Estimation",
        "Calibration",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ltPRj2nthL",
      "title": "Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling",
      "authors": [
        "Nguyen Minh Phuc",
        "Ngoc-Hieu Nguyen",
        "Duy Minh Ho Nguyen",
        "Anji Liu",
        "An Mai",
        "Binh T. Nguyen",
        "Daniel Sonntag",
        "Khoa D Doan"
      ],
      "abstract": "Recently, Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO) have emerged as alternatives to the standard Reinforcement Learning from Human Feedback (RLHF) for aligning large language models (LLMs) with human values. \nSurprisingly, while DAAs do not use a separate proxy reward model as in RLHF, their performance can still deteriorate over the course of training -- an over-optimization phenomenon found in RLHF where the learning policy exploits the overfitting to inaccuracies of the reward model to achieve high rewards.\nOne attributed source of over-optimization in DAAs is the under-constrained nature of their offline optimization, which can gradually shift probability mass toward non-preferred responses not presented in the preference dataset. This paper proposes a novel importance-sampling approach to mitigate the distribution shift problem of offline DAAs.\nThis approach, called (IS-DAAs), multiplies the DAA objective with an importance ratio that accounts for the reference policy distribution. IS-DAAs additionally avoid the high variance issue associated with importance sampling by clipping the importance ratio to a maximum value. Our extensive experiments demonstrate that IS-DAAs can effectively mitigate over-optimization, especially under low regularization strength, and achieve better performance than other methods designed to address this problem.",
      "arxiv_url": "https://openreview.net/forum?id=ltPRj2nthL",
      "pdf_url": "https://openreview.net/pdf/70a1a835f660a07daa4bada0981164f22d7a91e5.pdf",
      "primary_category": "direct preference optimization, human preference alignment, Regularization",
      "categories": [
        "direct preference optimization",
        "human preference alignment",
        "Regularization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1Imp4KZyjA",
      "title": "Why Do Some Language Models Fake Alignment While Others Don't?",
      "authors": [
        "Abhay Sheshadri",
        "John Hughes",
        "Julian Michael",
        "Alex Troy Mallen",
        "Arun Jose",
        "Fabien Roger"
      ],
      "abstract": "*Alignment Faking in Large Language Models* presented a demonstration of Claude 3 Opus and Claude 3.5 Sonnet selectively complying with a helpful-only training objective to prevent modification of their behavior outside of training. We expand this analysis to 25 models and find that only 5 (Claude 3 Opus, Claude 3.5 Sonnet, Llama 3 405B, Grok 3, Gemini 2.0 Flash) comply with harmful queries more when they infer they are in training than when they infer they are in deployment. First, we study the motivations of these 5 models. Results from perturbing details of the scenario suggest that only Claude 3 Opus's compliance gap is primarily and consistently motivated by trying to keep its goals. Second, we investigate why many chat models don't fake alignment. Our results suggest this is not entirely due to a lack of capabilities: many base models fake alignment some of the time, and post-training eliminates alignment-faking for some models and amplifies it for others. We investigate 5 hypotheses for how post-training may suppress alignment faking and find that variations in refusal behavior may account for a significant portion of differences in alignment faking.",
      "arxiv_url": "https://openreview.net/forum?id=1Imp4KZyjA",
      "pdf_url": "https://openreview.net/pdf/7fdc27a687b2b639d3fb0fd0c6ed2465592d72e6.pdf",
      "primary_category": "Large Language Models, Alignment, Alignment faking",
      "categories": [
        "Large Language Models",
        "Alignment",
        "Alignment faking",
        "Safety"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "V4SA2FOzQL",
      "title": "Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs",
      "authors": [
        "Xander Davies",
        "Eric Winsor",
        "Alexandra Souly",
        "Tomek Korbak",
        "Robert Kirk",
        "Christian Schroeder de Witt",
        "Yarin Gal"
      ],
      "abstract": "LLM developers deploy technical mitigations to prevent _fine-tuning misuse attacks_, attacks in which adversaries evade safeguards by fine-tuning the model using a public API. Previous work has established several successful attacks against specific fine-tuning API defences; however, prior attacks training and/or inference samples can be easily flagged as suspicious. In this work, we show that defences of fine-tuning APIs that seek to detect individual harmful training or inference samples ('pointwise' detection) are _fundamentally limited_ in their ability to prevent fine-tuning attacks. We demonstrate a class of 'pointwise-undetectable' attacks that repurpose semantic or syntactic variations in benign model outputs to covertly transmit dangerous knowledge. Our attacks are composed solely of unsuspicious benign samples that can be collected from the model before fine-tuning, meaning training and inference samples are all individually benign and low-perplexity. We test our attacks against the OpenAI fine-tuning API, finding they succeed in eliciting answers to harmful multiple-choice questions, and that they evade an enhanced monitoring system we design that successfully detects other fine-tuning attacks. Our results showing fundamental limitations of defending against pointwise attacks suggest focusing research efforts on mitigations towards multi-point defences.",
      "arxiv_url": "https://openreview.net/forum?id=V4SA2FOzQL",
      "pdf_url": "https://openreview.net/pdf/106f56012866f9c2a8f8d433881447430b77c9df.pdf",
      "primary_category": "LLM, AI security, harmful fine-tuning",
      "categories": [
        "LLM",
        "AI security",
        "harmful fine-tuning",
        "AI safety",
        "jailbreaking"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nOsEyBGk1I",
      "title": "C-SafeGen: Certified Safe LLM Generation with Claim-Based Streaming Guardrails",
      "authors": [
        "Mintong Kang",
        "Zhaorun Chen",
        "Bo Li"
      ],
      "abstract": "Despite the remarkable capabilities of large language models (LLMs) across diverse applications, they remain vulnerable to generating content that violates safety regulations and policies. To mitigate these risks, LLMs undergo safety alignment; however, they can still be effectively jailbroken. Off-the-shelf guardrail models are commonly deployed to monitor generations, but these models primarily focus on detection rather than ensuring safe decoding of LLM outputs. Moreover, existing efforts lack rigorous safety guarantees, which are crucial for the universal deployment of LLMs and certifiable compliance with regulatory standards. In this paper, we propose a Claim-based Stream Decoding (CSD) algorithm coupled with a statistical risk guarantee framework using conformal analysis. Specifically, our CSD algorithm integrates a stream guardrail model to safeguard sequential claims generated by LLMs and incorporates a backtracking mechanism to revise claims flagged with high safety risks. We provide theoretical guarantees demonstrating that the CSD algorithm achieves the desired generation distribution subject to safety constraints. Furthermore, we introduce a generation risk certification framework and derive a high-probability upper bound on the safety risk of the proposed CSD algorithm. We extend our approach to online settings, where user queries arrive sequentially, and prove that our method can asymptotically control safety risk to any desired level. Empirical evaluations demonstrate the effectiveness and efficiency of the CSD algorithm compared to state-of-the-art safety decoding approaches. Additionally, we validate the soundness and tightness of the derived safety risk upper bound using realistic data in both offline and online scenarios.",
      "arxiv_url": "https://openreview.net/forum?id=nOsEyBGk1I",
      "pdf_url": "https://openreview.net/pdf/dfd7ac77a247ef06493d1b66dd3565ffedb70b24.pdf",
      "primary_category": "LLM safety, LLM guardrail",
      "categories": [
        "LLM safety",
        "LLM guardrail"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tSpWkTFASC",
      "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Arm Bandits",
      "authors": [
        "Duy Nguyen",
        "Archiki Prasad",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Reward Models (RMs) are crucial to aligning large language models (LLMs), but the degree to which an RM specialized to one task (e.g. writing) generalizes to new tasks (e.g. math) is often not known a priori, often making using only one fixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs simultaneously can incur a prohibitively high computational cost and lead to conflicting signals from different RMs that may degrade performance. To address these challenges, we introduce LASeR (Learning to Adaptively Select Rewards), which frames reward model selection as a multi-armed bandit problem, iteratively and efficiently training LLMs using multiple RMs by selecting the most well-suited RM for each instance. On commonsense and math reasoning tasks, we show that LASeR boosts iterative LLM training, improving the absolute average accuracy of Llama-3-8B over three datasets by $2.67$% over an ensemble of RM scores while also showing superior efficiency (e.g., a $2\\times$ speedup). Moreover, on WildChat (open-ended instruction-following tasks), LASeR leads to a $72.69$% AlpacaEval win rate over the RM score ensemble baseline. Extending to long-context generation, LASeR improves by $2.96$ F1 points (avg.) on single-document QA tasks and $2.97$ F1 points on few-shot learning over the RM score ensemble baseline with best-of-$n$ sampling. We include our code in the supplementary.",
      "arxiv_url": "https://openreview.net/forum?id=tSpWkTFASC",
      "pdf_url": "https://openreview.net/pdf/bf589cfd8356e7e428426d438835ed093caa2e02.pdf",
      "primary_category": "reward models, bandit, instruction following",
      "categories": [
        "reward models",
        "bandit",
        "instruction following",
        "reasoning",
        "LLM alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QfKpJ00t2L",
      "title": "Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks",
      "authors": [
        "Debargha Ganguly",
        "Vikash Singh",
        "Sreehari Sankar",
        "Biyao Zhang",
        "Xuecen Zhang",
        "Srinivasan Iyengar",
        "Xiaotian Han",
        "Amit Sharma",
        "Shivkumar Kalyanaraman",
        "Vipin Chaudhary"
      ],
      "abstract": "Large language models (LLMs) show remarkable promise for democratizing automated reasoning by generating formal specifications. However, a fundamental tension exists: LLMs are probabilistic, while formal verification demands deterministic guarantees. This paper addresses this epistemological gap by comprehensively investigating failure modes and uncertainty quantification (UQ) in LLM-generated formal artifacts. Our systematic evaluation of five frontier LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's domain-specific impact on accuracy (from +34.8\\% on logical tasks to -44.5\\% on factual ones), with known UQ techniques like the entropy of token probabilities failing to identify these errors. We introduce a probabilistic context-free grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables selective verification, drastically reducing errors (14-100\\%) with minimal abstention, transforming LLM-driven formalization into a reliable engineering discipline.",
      "arxiv_url": "https://openreview.net/forum?id=QfKpJ00t2L",
      "pdf_url": "https://openreview.net/pdf/a66ecface3b15111085ae4a468256c18645cc903.pdf",
      "primary_category": "Large Language Models, Formal Verification, Uncertainty Quantification",
      "categories": [
        "Large Language Models",
        "Formal Verification",
        "Uncertainty Quantification",
        "Stochastic Context-Free Grammars",
        "Automated Reasoning",
        "SMT-based Autoformalization",
        "Selective Verification"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Ou30gzTLJe",
      "title": "Towards Self-Refinement of Vision-Language Models with Triangular Consistency",
      "authors": [
        "Yunlong Deng",
        "Guangyi Chen",
        "Tianpei Gu",
        "Lingjing Kong",
        "Yan Li",
        "Zeyu Tang",
        "Kun Zhang"
      ],
      "abstract": "Vision-Language Models (VLMs) integrate visual knowledge with the analytical capabilities of Large Language Models (LLMs) through supervised visual instruction tuning, using image-question-answer triplets. However, the potential of VLMs trained without supervised instruction remains largely unexplored. This study validates that VLMs possess inherent self-refinement capabilities, enabling them to generate high-quality supervised data without external inputs and thereby learn autonomously. Specifically, to stimulate the self-refinement ability of VLMs, we propose a self-refinement framework based on a Triangular Consistency principle: within the image-query-answer triangle, any masked elements should be consistently and accurately reconstructed. The framework involves three steps: (1) We enable the instruction generation ability of VLMs by adding multi-task instruction tuning like image$\\rightarrow$question-answer or image-answer$\\rightarrow$question. (2) We generate image-query-answer triplets from unlabeled images and use the Triangular Consistency principle for filtering. (3) The model is further updated using the filtered synthetic data. \nTo investigate the underlying mechanisms behind this self-refinement capability, we conduct a theoretical analysis from a causal perspective. \nUsing the widely recognized LLaVA-1.5 as our baseline, our experiments reveal that the model can autonomously achieve consistent, though deliberately modest, improvements across multiple benchmarks without any external supervision, such as human annotations or environmental feedback.\nWe expect that the insights of this study on the self-refinement ability of VLMs can inspire future research on the learning mechanism of VLMs. Code is available at https://github.com/dengyl20/SRF-LLaVA-1.5.",
      "arxiv_url": "https://openreview.net/forum?id=Ou30gzTLJe",
      "pdf_url": "https://openreview.net/pdf/3acba55c4efdef3ee37a854031bdd972e9e31008.pdf",
      "primary_category": "Vision-Language Models, Self-Refinement, Unsupervised Learning",
      "categories": [
        "Vision-Language Models",
        "Self-Refinement",
        "Unsupervised Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "bzxlOyjWbU",
      "title": "Deep Value Benchmark: Measuring Whether Models Generalize Deep Values or Shallow Preferences",
      "authors": [
        "Joshua Ashkinaze",
        "Hua Shen",
        "Sai Avula",
        "Eric Gilbert",
        "Ceren Budak"
      ],
      "abstract": "We introduce the Deep Value Benchmark (DVB), an evaluation framework that directly tests whether large language models (LLMs) learn fundamental human values or merely surface-level preferences. This distinction is critical for AI alignment: Systems that capture deeper values are likely to generalize human intentions robustly, while those that capture only superficial patterns in preference data risk producing misaligned behavior. The DVB uses a novel experimental design with controlled confounding between deep values (e.g., moral principles) and shallow features (e.g., superficial attributes). In the training phase, we expose LLMs to human preference data with deliberately correlated deep and shallow features---for instance, where a user consistently prefers (non-maleficence, formal language) options over (justice, informal language) alternatives. The testing phase then breaks these correlations, presenting choices between (justice, formal language) and (non-maleficence, informal language) options. This design allows us to precisely measure a model's Deep Value Generalization Rate (DVGR)---the probability of generalizing based on the underlying value rather than the shallow feature. Across 9 different models, the average DVGR is just 0.30. All models generalize deep values less than chance. Larger models have a (slightly) lower DVGR than smaller models. We are releasing our dataset, which was subject to three separate human validation experiments. DVB provides an interpretable measure of a core feature of alignment.",
      "arxiv_url": "https://openreview.net/forum?id=bzxlOyjWbU",
      "pdf_url": "https://openreview.net/pdf/b4ace008312757ce3b5681ed31d875cdf7177476.pdf",
      "primary_category": "alignment, human values, human-computer interaction",
      "categories": [
        "alignment",
        "human values",
        "human-computer interaction",
        "ethics"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "UfFTBEsLgI",
      "title": "The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning",
      "authors": [
        "Shivam Agarwal",
        "Zimin Zhang",
        "Lifan Yuan",
        "Jiawei Han",
        "Hao Peng"
      ],
      "abstract": "Entropy minimization (EM) trains the model to concentrate even more probability mass on its most confident outputs. \nWe show that this simple objective alone, without any labeled data, can substantially improve large language models’ (LLMs) performance on challenging math, physics, and coding tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy similarly to instruction finetuning, but on unlabeled outputs drawn from the model; (2) EM-RL: reinforcement learning with negative entropy as the only reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce entropy without any training data or parameter updates. \nOn Qwen-7B, EM-RL, without any labeled data, achieves comparable or better performance than strong RL baselines such as GRPO and RLOO that are trained on 60K labeled examples. Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the challenging SciCode benchmark, while being 3x more efficient than self-consistency and sequential refinement. Our findings reveal that many pretrained LLMs possess previously underappreciated reasoning capabilities that can be effectively elicited through entropy minimization alone, without any labeled data or even any parameter updates.",
      "arxiv_url": "https://openreview.net/forum?id=UfFTBEsLgI",
      "pdf_url": "https://openreview.net/pdf/88aaade3895de05923e693f82c7973705e343c35.pdf",
      "primary_category": "large language model, alignment, finetuning",
      "categories": [
        "large language model",
        "alignment",
        "finetuning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "H918WyPf0s",
      "title": "AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking",
      "authors": [
        "Soyoung Yoon",
        "Gyuwan Kim",
        "GYU-HWUNG CHO",
        "seung-won hwang"
      ],
      "abstract": "Listwise reranking with large language models (LLMs) enhances top-ranked results in retrieval-based applications.\nDue to the limit in context size and high inference cost of long context, reranking is typically performed over a fixed size of small subsets, with the final ranking aggregated from these partial results.\nThis fixed computation disregards query difficulty and document distribution, leading to inefficiencies. \nWe propose AcuRank, an adaptive reranking framework that dynamically adjusts both the amount and target of computation based on uncertainty estimates over document relevance. \nUsing a Bayesian TrueSkill model, we iteratively refine relevance estimates until reaching sufficient confidence levels, and our explicit modeling of ranking uncertainty enables principled control over reranking behavior and avoids unnecessary updates to confident predictions.\nResults on the TREC-DL and BEIR benchmarks show that our method consistently achieves a superior accuracy–efficiency trade-off and scales better with compute than fixed-computation baselines.\nThese results highlight the effectiveness and generalizability of our method across diverse retrieval tasks and LLM-based reranking models.",
      "arxiv_url": "https://openreview.net/forum?id=H918WyPf0s",
      "pdf_url": "https://openreview.net/pdf/94b4a6ec40bae1b4a0a48f35ba8b051259a24c1b.pdf",
      "primary_category": "Listwise Reranking, Large Language Models, Adaptive Computation",
      "categories": [
        "Listwise Reranking",
        "Large Language Models",
        "Adaptive Computation",
        "Uncertainty Estimation",
        "Inference Efficiency"
      ],
      "tags": [
        "LLM",
        "Information Retrieval",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Ei6IsmxYrb",
      "title": "Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales",
      "authors": [
        "Shikai Qiu",
        "Zixi Chen",
        "Hoang Phan",
        "Qi Lei",
        "Andrew Gordon Wilson"
      ],
      "abstract": "Several recently introduced deep learning optimizers utilizing matrix-level preconditioning have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results.\nTo better understand the effectiveness of these optimizers at scale, in this work we investigate how to scale preconditioned optimizers via hyperparameter transfer, building on prior works such as $\\mu$P. We study how the optimal learning rate and weight decay should scale with model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. We find that scaling the learning rate according to $\\mu$P improves transfer, but can still suffer from significant finite-width deviations that cause drifting optimal learning rates, which we show can be mitigated by blocking and explicit spectral normalization.\nFor compute-optimal scaling, we find scaling independent weight decay as $1/\\mathrm{width}$ is nearly optimal across optimizers.\nApplying these scaling rules, we show Muon, SOAP and Shampoo consistently achieve near $1.4\\times$ speedup over AdamW for training Llama-architecture language models of sizes ranging from $190$M to $1.4$B, whereas the speedup vanishes rapidly with scale under incorrect scaling. Based on these results and further ablations, we argue that studying optimal hyperparameter transfer is essential for reliably comparing optimizers at scale given a realistic tuning budget.",
      "arxiv_url": "https://openreview.net/forum?id=Ei6IsmxYrb",
      "pdf_url": "https://openreview.net/pdf/1575dffe308ac44e317e8528ab4ce6bf58251c64.pdf",
      "primary_category": "hyperparameter transfer, second order optimizers, scaling laws",
      "categories": [
        "hyperparameter transfer",
        "second order optimizers",
        "scaling laws"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "byNNv5Et10",
      "title": "3BASiL: An Algorithmic Framework for Sparse plus Low-Rank Compression of LLMs",
      "authors": [
        "Mehdi Makni",
        "Xiang Meng",
        "Rahul Mazumder"
      ],
      "abstract": "Sparse plus Low-Rank $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ decomposition of Large Language Models (LLMs) has emerged as a promising direction in $\\textit{model compression}$, aiming to decompose pre-trained model weights into a sum of sparse and low-rank matrices $\\mathbf{W} \\approx \\mathbf{S} + \\mathbf{LR}$. Despite recent progress, existing methods often suffer from substantial performance degradation compared to dense models. In this work, we introduce $\\texttt{3BASiL-TM}$, an efficient one-shot post-training method for $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ decomposition of LLMs that addresses this gap. Our approach first introduces a novel 3-Block Alternating Direction Method of Multipliers (ADMM) method, termed $\\texttt{3BASiL}$, to minimize the layer-wise reconstruction error with convergence guarantees. \nWe then design a transformer-matching ($\\texttt{TM}$) refinement step that jointly optimizes the sparse and low-rank components across transformer layers. This step minimizes a novel memory-efficient  loss that aligns outputs at the transformer level.\nNotably, the $\\texttt{TM}$ procedure is universal as it can enhance any $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ decomposition, including pure sparsity. Our numerical experiments show that $\\texttt{3BASiL-TM}$ reduces the WikiText2 perplexity gap to dense LLaMA-8B model by over 30% under a (2:4 Sparse + 64 LR) configuration, compared to prior methods. Moreover, our method achieves over 2.5x faster compression runtime on an A100 GPU compared to SOTA $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ method.\nOur code is available at https://github.com/mazumder-lab/3BASiL.",
      "arxiv_url": "https://openreview.net/forum?id=byNNv5Et10",
      "pdf_url": "https://openreview.net/pdf/73d6c16c58d988d2aa7bd2cbb3a3fea83bf60c5b.pdf",
      "primary_category": "Sparse plus Low-Rank, Model Compression, Large Language Models",
      "categories": [
        "Sparse plus Low-Rank",
        "Model Compression",
        "Large Language Models",
        "LoRA",
        "PEFT",
        "ADMM",
        "Optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "gyn4n8oC9B",
      "title": "Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning",
      "authors": [
        "Arian Raje",
        "Baris Askin",
        "Divyansh Jhunjhunwala",
        "Gauri Joshi"
      ],
      "abstract": "Large Language Models (LLMs) have yet to effectively leverage the vast amounts of edge-device data, and Federated Learning (FL) offers a promising paradigm to collaboratively fine-tune LLMs without transferring private edge data to the cloud. To operate within the computational and communication constraints of edge devices, recent literature on federated fine-tuning of LLMs proposes the use of low-rank adaptation (LoRA) and similar parameter-efficient methods. However, LoRA-based methods suffer from accuracy degradation in FL settings, primarily because of data and computational heterogeneity across clients. We propose Ravan, an adaptive multi-head LoRA method that balances parameter efficiency and model expressivity by reparameterizing the weight updates as the sum of multiple LoRA heads, $s_i\\textbf{B}_i\\textbf{H}_i\\textbf{A}_i$, in which only the $\\textbf{H}_i$ parameters and their lightweight scaling factors $s_i$ are trained. These trainable scaling factors let the optimization focus on the most useful heads, recovering a higher-rank approximation of the full update without increasing the number of communicated parameters since clients upload $s_i\\textbf{H}_i$ directly. Experiments on vision and language benchmarks show that Ravan improves test accuracy by 2–8\\% over prior parameter-efficient baselines, making it a robust and scalable solution for federated fine-tuning of LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=gyn4n8oC9B",
      "pdf_url": "https://openreview.net/pdf/31efa27620b2ff2dacf994447f5b770f9df8c04c.pdf",
      "primary_category": "Federated Learning, LoRA, Fine-Tuning",
      "categories": [
        "Federated Learning",
        "LoRA",
        "Fine-Tuning",
        "Efficiency"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "l8razJItEy",
      "title": "On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study",
      "authors": [
        "Riccardo Alberghi",
        "Elizaveta Demyanenko",
        "Luca Biggio",
        "Luca Saglietti"
      ],
      "abstract": "Recent advances in natural language processing highlight two key factors for improving reasoning in large language models (LLMs): (i) allocating more test-time compute tends to help on harder problems but often introduces redundancy in the reasoning trace, and (ii) compute is most effective when reasoning is systematic and incremental, forming structured chains of thought (CoTs) akin to human problem-solving. To study these factors in isolation, we introduce a controlled setting based on shortest-path tasks in layered graphs. We train decoder-only transformers on question–trace–answer triples using a custom tokenizer, comparing models trained on optimal bottom-up dynamic programming traces with those trained on longer, valid traces involving backtracking. Surprisingly, under the same training-token budget, the latter models generalize better to unseen graphs. This benefit is not due to length alone—injecting arbitrary redundancy into reasoning traces fails to help and can even hurt performance. Instead, we find that generalization correlates with the model's confidence in next-token prediction, suggesting that long, coherent, and locally incremental traces make the training signal easier to optimize.",
      "arxiv_url": "https://openreview.net/forum?id=l8razJItEy",
      "pdf_url": "https://openreview.net/pdf/57860c28b21d95678533bc618a0afee1c2a54e47.pdf",
      "primary_category": "Reasoning, Chain-of-thought, Next-token prediction",
      "categories": [
        "Reasoning",
        "Chain-of-thought",
        "Next-token prediction",
        "Transformers",
        "Algorithm",
        "Shortest-Path"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "R0B1z8dQcV",
      "title": "AdvPrefix: An Objective for Nuanced LLM Jailbreaks",
      "authors": [
        "Sicheng Zhu",
        "Brandon Amos",
        "Yuandong Tian",
        "Chuan Guo",
        "Ivan Evtimov"
      ],
      "abstract": "Many jailbreak attacks on large language models (LLMs) rely on a common objective: making the model respond with the prefix ``Sure, here is (harmful request)''. While straightforward, this objective has two limitations: limited control over model behaviors, yielding incomplete or unrealistic jailbroken responses, and a rigid format that hinders optimization. We introduce AdvPrefix, a plug-and-play prefix-forcing objective that selects one or more model-dependent prefixes by combining two criteria: high prefilling attack success rates and low negative log-likelihood. AdvPrefix integrates seamlessly into existing jailbreak attacks to mitigate the previous limitations for free. For example, replacing GCG's default prefixes on Llama-3 improves nuanced attack success rates from 14\\% to 80\\%, revealing that current safety alignment fails to generalize to new prefixes. Code and selected prefixes are released.",
      "arxiv_url": "https://openreview.net/forum?id=R0B1z8dQcV",
      "pdf_url": "https://openreview.net/pdf/55684a43a51f68c2834c77a064a9fe33866d3efb.pdf",
      "primary_category": "LLM, red-teaming, safety",
      "categories": [
        "LLM",
        "red-teaming",
        "safety",
        "safety alignment",
        "jailbreak",
        "adversarial attack",
        "adversarial robustness",
        "discrete optimization",
        "controllable generation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "52Ehpe0Lu5",
      "title": "Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation is Wasteful",
      "authors": [
        "Martin Marek",
        "Sanae Lotfi",
        "Aditya Somasundaram",
        "Andrew Gordon Wilson",
        "Micah Goldblum"
      ],
      "abstract": "Conventional wisdom dictates that small batch sizes make language model pretraining and fine-tuning unstable, motivating gradient accumulation, which trades off the number of optimizer steps for a proportional increase in batch size. While it is common to decrease the learning rate for smaller batch sizes, other hyperparameters are often held fixed. In this work, we revisit small batch sizes all the way down to batch size one, and we propose a rule for scaling Adam hyperparameters to small batch sizes.  In particular, rather than holding the decay rate of the second moment fixed across batch sizes, we propose to hold its half-life fixed in terms of tokens. We find that small batch sizes (1) train stably, (2) are consistently more robust to hyperparameter choices, (3) achieve equal or better per-FLOP performance than larger batch sizes, and (4) notably enable stable language model training with vanilla SGD, even without momentum, despite storing no optimizer state. Building on these results, we provide practical recommendations for selecting a batch size and setting optimizer hyperparameters. We further recommend against gradient accumulation unless training on multiple devices with multiple model replicas. Finally, we show that a small batch size combined with an optimizer with a small state size can provide the performance benefits of full fine-tuning while maintaining a similar memory footprint to LoRA.",
      "arxiv_url": "https://openreview.net/forum?id=52Ehpe0Lu5",
      "pdf_url": "https://openreview.net/pdf/ed0722457ee783e641474953435693fcb1f57adb.pdf",
      "primary_category": "batch size, language model, LLM",
      "categories": [
        "batch size",
        "language model",
        "LLM",
        "SGD",
        "Adam"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "r8UWp9JeJi",
      "title": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings",
      "authors": [
        "Stephen Zhang",
        "Mustafa Khan",
        "Vardan Papyan"
      ],
      "abstract": "Large language models (LLMs) often concentrate their attention on a few specific tokens referred to as *attention sinks*. Common examples include the first token, a prompt-independent sink, and punctuation tokens, which are prompt-dependent. While the tokens causing the sinks often lack direct semantic meaning, the presence of the sinks is critical for model performance, particularly under model compression and KV-caching. Despite their ubiquity, the function, semantic role, and origin of attention sinks—especially those beyond the first token—remain poorly understood. In this work, we conduct a comprehensive investigation demonstrating that attention sinks: *catch* a sequence of tokens, *tag* them using a common direction in embedding space, and *release* them back into the residual stream, where tokens are later retrieved based on the tags they have acquired. Probing experiments reveal these tags carry semantically meaningful information, such as the truth of a statement. These findings extend to reasoning models, where the mechanism spans more heads and explains greater variance in embeddings, or recent models with query-key normalization, where sinks remain just as prevalent. To encourage future theoretical analysis, we introduce a minimal problem which can be solved through the 'catch, tag, release' mechanism, and where it emerges through training.",
      "arxiv_url": "https://openreview.net/forum?id=r8UWp9JeJi",
      "pdf_url": "https://openreview.net/pdf/2171583f757f2e32af66490da59d553613756c5b.pdf",
      "primary_category": "Attention Sinks, Mechanistic Interpretability, Language Models",
      "categories": [
        "Attention Sinks",
        "Mechanistic Interpretability",
        "Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RPRqKhjrr6",
      "title": "Checklists Are Better Than Reward Models For Aligning Language Models",
      "authors": [
        "Vijay Viswanathan",
        "Yanchao Sun",
        "Xiang Kong",
        "Meng Cao",
        "Graham Neubig",
        "Tongshuang Wu"
      ],
      "abstract": "Language models must be adapted to understand and follow user instructions. Reinforcement learning is widely used to facilitate this —typically using fixed criteria such as \"helpfulness\" and \"harmfulness\". In our work, we instead propose using flexible, instruction-specific criteria as a means of broadening the impact that reinforcement learning can have in eliciting instruction following. We propose \"Reinforcement Learning from Checklist Feedback\" (RLCF). From instructions, we extract checklists and evaluate how well responses satisfy each item—using both AI judges and specialized verifier programs—then combine these scores to compute rewards for RL. We compare RLCF with other alignment methods on top of a strong instruction following model (Qwen2.5-7B-Instruct) on five widely-studied benchmarks — RLCF is the only method to help on every benchmark, including a 4-point boost in hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a 3-point rise in win rate on Arena-Hard. We show that RLCF can also be used off-policy to improve Llama 3.1 8B Instruct and OLMo 2 7B Instruct. These results establish rubrics as a key tool for improving language models' support of queries that express a multitude of needs. We release our our dataset of rubrics (WildChecklists), models, and code to the public.",
      "arxiv_url": "https://openreview.net/forum?id=RPRqKhjrr6",
      "pdf_url": "https://openreview.net/pdf/490597cf8f353f8b01b8474e2f98c045eba8f5f4.pdf",
      "primary_category": "alignment, rubrics, instruction following",
      "categories": [
        "alignment",
        "rubrics",
        "instruction following",
        "RLHF"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "R0dC7Xzwbk",
      "title": "Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning",
      "authors": [
        "Jaehun Jung",
        "Seungju Han",
        "Ximing Lu",
        "Skyler Hallinan",
        "David Acuna",
        "Shrimai Prabhumoye",
        "Mostofa Patwary",
        "Mohammad Shoeybi",
        "Bryan Catanzaro",
        "Yejin Choi"
      ],
      "abstract": "Data diversity is crucial for training a strong language model. Yet metrics of diversity often diverge from this goal, measuring variations in heuristic features—like n-grams or embeddings—that are detached from how the model actually performs on a target task. This motivates us to ask: *Can we redefine data diversity—beyond measuring variations in heuristic features—in a way that better predicts model generalization?* Through large-scale empirical analyses spanning over 300 training runs, carefully controlled for data scale and quality, we show that data diversity can be a strong predictor of generalization in LLM reasoning—as measured by average model performance on unseen out-of-distribution benchmarks. We introduce **G-Vendi**, a metric that quantifies diversity via the entropy of model-induced loss gradients. G-Vendi scales to million-sample datasets and yet consistently outperforms heuristic alternatives, achieving strong correlation ($\\text{Spearman's } \\rho \\approx 0.9$) with out-of-distribution (OOD) performance across both natural language inference (NLI) and math reasoning tasks. Building on this insight, we present **Prismatic Synthesis**, a framework for generating diverse synthetic data by targeting underrepresented regions in gradient space. Experimental results show that Prismatic Synthesis consistently improves model performance as we scale synthetic data—not just on in-distribution test but across unseen, out-of-distribution benchmarks—significantly outperforming state-of-the-art models in both domains. For example, PrismMath-7B, our model distilled from a 32B LLM without human verification, outperforms R1-Distill-Qwen-7B—trained on proprietary data generated by 671B R1—on 6 out of 7 challenging math benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=R0dC7Xzwbk",
      "pdf_url": "https://openreview.net/pdf/cf925214a554d0ec2b50bb7ea4d5c5fd8177ce2f.pdf",
      "primary_category": "Large Language Models, Data Diversity, Data Quality",
      "categories": [
        "Large Language Models",
        "Data Diversity",
        "Data Quality",
        "Synthetic Data",
        "LLM Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "o3bftqj17e",
      "title": "How Benchmark Prediction from Fewer Data Misses the Mark",
      "authors": [
        "Guanhua Zhang",
        "Florian E. Dorner",
        "Moritz Hardt"
      ],
      "abstract": "Large language model (LLM) evaluation is increasingly costly, prompting interest in methods that speed up evaluation by shrinking benchmark datasets. Benchmark prediction (also called efficient LLM evaluation) aims to select a small subset of evaluation points and predict overall benchmark performance from that subset. In this paper, we systematically assess the strengths and limitations of 11 benchmark prediction methods across 19 diverse benchmarks. First, we identify a highly competitive baseline: Take a random sample and fit a regression model on the sample to predict missing entries. Outperforming most existing methods, this baseline challenges the assumption that careful subset selection is necessary for benchmark prediction. Second, we discover that all existing methods crucially depend on model similarity. They work best when interpolating scores among similar models. The effectiveness of benchmark prediction sharply declines when new models have higher accuracy than previously seen models. In this setting of extrapolation, none of the previous methods consistently beat a simple average over random samples. To improve over the sample average, we introduce a new method inspired by augmented inverse propensity weighting. This method consistently outperforms the random sample average even for extrapolation. However, its performance still relies on model similarity and the gains are modest in general. This shows that benchmark prediction fails just when it is most needed: at the evaluation frontier, where the goal is to evaluate new models of unknown capabilities.",
      "arxiv_url": "https://openreview.net/forum?id=o3bftqj17e",
      "pdf_url": "https://openreview.net/pdf/a4097f48740ba588b5ffe5a4fd3f7d88b8eb0a70.pdf",
      "primary_category": "Evaluation, Large language model",
      "categories": [
        "Evaluation",
        "Large language model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "on6Hf0KP20",
      "title": "SQLens: An End-to-End Framework for Error Detection and Correction in Text-to-SQL",
      "authors": [
        "Yue Gong",
        "Chuan Lei",
        "Xiao Qin",
        "Kapil Vaidya",
        "Balakrishnan Murali Narayanaswamy",
        "Tim Kraska"
      ],
      "abstract": "Text-to-SQL systems translate natural language (NL) questions into SQL queries, enabling non-technical users to interact with structured data. While large language models (LLMs) have shown promising results on the text-to-SQL task, they often produce semantically incorrect yet syntactically valid queries, with limited insight into their reliability. We propose SQLens, an end-to-end framework for fine-grained detection and correction of semantic errors in LLM-generated SQL. SQLens integrates error signals from both the underlying database and the LLM to identify potential semantic errors within SQL clauses. It further leverages these signals to guide query correction. Empirical results on two public benchmarks show that SQLens outperforms the best LLM-based self-evaluation method by 25.78% in F1 for error detection, and improves execution accuracy of out-of-the-box text-to-SQL systems by up to 20%.",
      "arxiv_url": "https://openreview.net/forum?id=on6Hf0KP20",
      "pdf_url": "https://openreview.net/pdf/95074aaa1dfc7eb228c94f79cce6740a3cb23809.pdf",
      "primary_category": "Large Language Models, Text-to-SQL, Error Detection",
      "categories": [
        "Large Language Models",
        "Text-to-SQL",
        "Error Detection",
        "Error Correction"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kcSbYJRQub",
      "title": "AR-RAG: Autoregressive Retrieval Augmentation for Image Generation",
      "authors": [
        "Jingyuan Qi",
        "Zhiyang Xu",
        "Qifan Wang",
        "Lifu Huang"
      ],
      "abstract": "We introduce  Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm that enhances image generation by autoregressively incorporating k-nearest neighbor retrievals at the patch level.\nUnlike prior methods that perform a single, static retrieval before generation and condition the entire generation on fixed reference images,  AR-RAG performs context-aware retrievals at each generation step, using prior-generated patches as queries to retrieve and incorporate the most relevant patch-level visual references, \nenabling the model to respond to evolving generation needs while avoiding limitations (e.g., over-copying, stylistic bias, etc.) prevalent in existing methods. To realize AR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in Decoding (DAiD), a training-free plug-and-use decoding strategy that directly merges the distribution of model-predicted patches with the distribution of retrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a parameter-efficient fine-tuning method that progressively smooths the features of retrieved patches via multi-scale convolution operations and leverages them to augment the image generation process. We validate the effectiveness of AR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and DPG-Bench, demonstrating significant performance gains over state-of-the-art image generation models.",
      "arxiv_url": "https://openreview.net/forum?id=kcSbYJRQub",
      "pdf_url": "https://openreview.net/pdf/7ae3d1a6eea9d523ab0f0fd3a911ecf777350864.pdf",
      "primary_category": "retrieval augmented image generation, autoregressive image generation, discrete image tokens",
      "categories": [
        "retrieval augmented image generation",
        "autoregressive image generation",
        "discrete image tokens"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "irYb8GGDyh",
      "title": "Inference-Time Personalized Alignment with a Few User Preference Queries",
      "authors": [
        "Victor-Alexandru Pădurean",
        "Parameswaran Kamalaruban",
        "Nachiket Kotalwar",
        "Alkis Gotovos",
        "Adish Singla"
      ],
      "abstract": "We study the problem of aligning a generative model's response with a user's preferences. Recent works have proposed several different formulations for personalized alignment; however, they either require a large amount of user preference queries or require that the preference be explicitly specified as a text input. In this paper, we propose a novel inference-time personalized alignment method, UserAlign, that elicits the user's preferences with a few queries as pairwise response comparisons. In particular, UserAlign builds on the theoretical framework of best-arm identification in logistic bandits and selects a personalized response from a fixed pool of the model's generated responses. The key idea is to consider the user's feedback consistent and noise-free, and incorporate it into the theoretical framework to identify the best response quickly. Experimental results across several tasks,  involving personalized text and image generation, showcase the effectiveness of UserAlign in achieving personalized alignment.",
      "arxiv_url": "https://openreview.net/forum?id=irYb8GGDyh",
      "pdf_url": "https://openreview.net/pdf/d2ae056c8a0fb4f20d2800bff800dce8eeb03daa.pdf",
      "primary_category": "personalized alignment, inference-time alignment, user preferences",
      "categories": [
        "personalized alignment",
        "inference-time alignment",
        "user preferences",
        "best of N",
        "best-arm identification",
        "logistic bandits"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "00oRAPDWsX",
      "title": "KL Penalty Control via Perturbation for Direct Preference Optimization",
      "authors": [
        "Sangkyu Lee",
        "Janghoon Han",
        "Hosung Song",
        "Stanley Jungkyu Choi",
        "Honglak Lee",
        "Youngjae Yu"
      ],
      "abstract": "Direct Preference Optimization (DPO) demonstrates the advantage of aligning a large language model with human preference using only an offline dataset. However, DPO has the limitation that the KL penalty, which prevents excessive deviation from the reference model, is static throughout the training process. Several methods claim to change this static KL penalty of DPO into a dynamic one, but no approach can adaptively assign different KL penalties for each preference pair. In this paper, we propose $\\varepsilon$-Direct Preference Optimization ($\\varepsilon$-DPO), which allows adaptive control of the KL penalty strength $\\beta$ for each preference pair. Specifically, $\\varepsilon$-DPO adaptively controls $\\beta$ for each preference pair based on the monotonicity of logits as a preference model under the perturbation of $\\beta$ during training. This is equivalent to adjusting the KL penalty by checking whether the change in training-time temperature can lead to better preference confidence as preference models by simply reusing the logit of the current policy and the reference policy. Experimental results show that the simple criterion of $\\varepsilon$-DPO for KL penalty relaxation significantly improves DPO compared to most existing direct alignment algorithms on general chatbot benchmarks and reveal that this KL penalty control criterion can reflect confusion as a preference model and provide an efficient KL trade-off, highlighting the significance of instance-level adaptive KL penalty control in DPO.",
      "arxiv_url": "https://openreview.net/forum?id=00oRAPDWsX",
      "pdf_url": "https://openreview.net/pdf/1189c2c8d83f2cac803ca69caa33de3ea3c9f65d.pdf",
      "primary_category": "Direct Alignment Algorithm, Reinforcement Learning from Human Feedback, Human Preference",
      "categories": [
        "Direct Alignment Algorithm",
        "Reinforcement Learning from Human Feedback",
        "Human Preference",
        "LLM",
        "Alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ftVlLG9cks",
      "title": "The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning",
      "authors": [
        "Xinyu Zhu",
        "Mengzhou Xia",
        "Zhepei Wei",
        "Wei-Lin Chen",
        "Danqi Chen",
        "Yu Meng"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach for training language models (LMs) on reasoning tasks that elicit emergent long chains of thought (CoTs). Unlike supervised learning, it updates the model using both correct and incorrect samples via policy gradients. To better understand its mechanism, we decompose the learning signal into reinforcing correct responses and penalizing incorrect ones, referred to as **P**ositive and **N**egative **S**ample **R**einforcement (**PSR** and **NSR**), respectively. We train `Qwen2.5-Math-7B`, `Qwen3-4B` and `Llama-3.1-8B-Instruct` on a mathematical reasoning dataset and uncover a surprising result: training with only negative samples — without reinforcing correct responses — can be highly effective: it consistently improves performance over the base model across the entire Pass@$k$ spectrum $k$ up to 256), often matching or surpassing PPO and GRPO. In contrast, reinforcing only correct responses improves Pass@1 but degrades performance at higher $k$, due to reduced diversity. These inference-scaling trends highlight that solely penalizing incorrect responses may contribute more to performance than previously recognized. Through gradient analysis, we show that NSR works by suppressing incorrect generations and redistributing probability mass toward other plausible candidates, guided by the model's prior beliefs. It refines the model's existing knowledge rather than introducing entirely new behaviors. Building on this insight, we propose a simple variant of the RL objective that upweights NSR, and show that it consistently improves overall Pass@$k$ performance on MATH, AIME 2025, and AMC23. Our code is available at [`https://github.com/TianHongZXY/RLVR-Decomposed`](https://github.com/TianHongZXY/RLVR-Decomposed).",
      "arxiv_url": "https://openreview.net/forum?id=ftVlLG9cks",
      "pdf_url": "https://openreview.net/pdf/a1a16a6d39d396350f884575808cb0174e29805b.pdf",
      "primary_category": "Large Language Models, Reinforcement Learning, LLM Reasoning",
      "categories": [
        "Large Language Models",
        "Reinforcement Learning",
        "LLM Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "D9JeNTs5Bu",
      "title": "ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search",
      "authors": [
        "Zeyu Shen",
        "Basileal Yoseph Imana",
        "Tong Wu",
        "Chong Xiang",
        "Prateek Mittal",
        "Aleksandra Korolova"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models by grounding their outputs in external documents. These systems, however, remain vulnerable to attacks on the retrieval corpus, such as prompt injection. RAG-based search systems (e.g., Google’s Search AI Overview) present an interesting setting for studying and protecting against such threats, as defense algorithms can benefit from built-in reliability signals—like document ranking—and represent a non-LLM challenge for the adversary due to decades of work to thwart SEO.\n\nMotivated by, but not limited to, this scenario, this work introduces ReliabilityRAG, a framework for adversarial robustness that explicitly leverages reliability information of retrieved documents.\n\nOur first contribution adopts a graph-theoretic perspective to identify a ``consistent majority'' among retrieved documents to filter out malicious ones. We introduce a novel algorithm based on finding a Maximum Independent Set (MIS) on a document graph where edges encode contradiction. Our MIS variant explicitly prioritizes higher-reliability documents and provides provable robustness guarantees against bounded adversarial corruption under natural assumptions. Recognizing the computational cost of exact MIS for large retrieval sets, our second contribution is a scalable weighted sample and aggregate framework. It explicitly utilizes reliability information, preserving some robustness guarantees while efficiently handling many documents.\n\nWe present empirical results showing ReliabilityRAG provides superior robustness against adversarial attacks compared to prior methods, maintains high benign accuracy, and excels in long-form generation tasks where prior robustness-focused methods struggled. Our work is a significant step towards more effective, provably robust defenses against retrieved corpus corruption in RAG.",
      "arxiv_url": "https://openreview.net/forum?id=D9JeNTs5Bu",
      "pdf_url": "https://openreview.net/pdf/5232092e85bb884495f6c5dc4d4fc9d5fde8f6b7.pdf",
      "primary_category": "Natural Language Processing (NLP), Large Language Models (LLM), Retrieval-Augmented Generation (RAG)",
      "categories": [
        "Natural Language Processing (NLP)",
        "Large Language Models (LLM)",
        "Retrieval-Augmented Generation (RAG)",
        "Adversarial Robustness"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2hiNrfMmQ7",
      "title": "Information Retrieval Induced Safety Degradation in AI Agents",
      "authors": [
        "Cheng Yu",
        "Benedikt Stroebl",
        "Diyi Yang",
        "Orestis Papakyriakopoulos"
      ],
      "abstract": "Despite the growing integration of retrieval-enabled AI agents into society, their safety and ethical behavior remain inadequately understood.\nIn particular, the growing integration of LLMs and AI agents with external information sources and real-world environments raises critical questions about how they engage with and are influenced by these external data sources and interactive contexts.\nThis study investigates how expanding retrieval access—from no external sources to Wikipedia-based retrieval and open web search—affects model reliability, bias propagation, and harmful content generation. \nThrough extensive benchmarking of censored and uncensored LLMs and AI Agents, our findings reveal a consistent degradation in refusal rates, bias sensitivity, and harmfulness safeguards as models gain broader access to external sources, culminating in a phenomenon we term safety degradation. Notably, retrieval-enabled agents built on aligned LLMs often behave more unsafely than uncensored models without retrieval. This effect persists even under strong retrieval accuracy and prompt-based mitigation, suggesting that the mere presence of retrieved content reshapes model behavior in structurally unsafe ways.\nThese findings underscore the need for robust mitigation strategies to ensure fairness and reliability in retrieval-enabled and increasingly autonomous AI systems.",
      "arxiv_url": "https://openreview.net/forum?id=2hiNrfMmQ7",
      "pdf_url": "https://openreview.net/pdf/1e319382e526c85a8d468dc155a7b017f99e3dde.pdf",
      "primary_category": "Responsible AI, LLM Agents, Safety",
      "categories": [
        "Responsible AI",
        "LLM Agents",
        "Safety",
        "Bias"
      ],
      "tags": [
        "LLM",
        "Information Retrieval",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4jFSekBaDT",
      "title": "The Best Instruction-Tuning Data are Those That Fit",
      "authors": [
        "Dylan Zhang",
        "Qirun Dai",
        "Hao Peng"
      ],
      "abstract": "High-quality supervised finetuning (SFT) data are essential for unlocking pretrained LLMs’ capabilities. Typically, instructions are paired with responses from various sources—by human annotators or other LMs—which are often out of the distribution of the target model to be finetuned. At scale, this mismatch can lead to diminishing returns and even hurt model performance and robustness. We hypothesize that SFT is most effective when the data is aligned with the model’s pretrained distribution, and propose **GRAPE**—a novel SFT framework that tailors supervision to the target model. For each instruction, it **g**athers **r**esponses from various sources and selects the one that **a**ligns most closely to the model’s **pre**trained distribution, as measured by the normalized probability. Standard SFT is then performed on these selected responses.\n\nWe first evaluate GRAPE in a controlled experiment, sampling multiple responses per question in the UltraInteract dataset from diverse models. We finetune using GRAPE-selected data on LMs from different families, including LLaMA-1-8B, Mistral-7B, and Qwen2.5-7B. GRAPE significantly outperforms strong baselines—including distilling from the strongest model—with absolute gains up to **13.8%** averaged across benchmarks, and outperforms a 3× larger data baseline with improvements up to **17.3%**.\n\nGRAPE's benefits generalize to off-the-shelf SFT data. When used to subsample from the post-training data of Tulu3 and Olmo-2, GRAPE surpasses strong baselines trained on 4.5× more data by **6.1%**, and outperforms state-of-the-art selection methods by **3.9%** on average. Notably, with only **1/3 the data** and **half the training epochs**, GRAPE enables LLaMA-1-8B to **exceed Tulu3-SFT performance by 3.5%**.\n\nOur findings highlight that aligning supervision with the pretrained distribution provides a simple yet powerful strategy to improve both the **efficiency** and **effectiveness** of SFT.",
      "arxiv_url": "https://openreview.net/forum?id=4jFSekBaDT",
      "pdf_url": "https://openreview.net/pdf/0170afc74e4e6ca42fe02ec7208322ee146122e9.pdf",
      "primary_category": "Instruction Tuning, Data Selection, Efficiency",
      "categories": [
        "Instruction Tuning",
        "Data Selection",
        "Efficiency",
        "Post-Training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "w1FUXt3ujK",
      "title": "Prot2Text-V2: Protein Function Prediction with Multimodal Contrastive Alignment",
      "authors": [
        "Xiao Fei",
        "Michail Chatzianastasis",
        "Sarah Almeida Carneiro",
        "Hadi Abdine",
        "Lawrence Paul Petalidis",
        "Michalis Vazirgiannis"
      ],
      "abstract": "Predicting protein function from sequence is a central challenge in computational biology. While existing methods rely heavily on structured ontologies or similarity-based techniques, they often lack the flexibility to express structure-free functional descriptions and novel biological functions. In this work, we introduce Prot2Text-V2, a novel multimodal sequence-to-text model that generates free-form natural language descriptions of protein function directly from amino acid sequences. Our method combines a protein language model as a sequence encoder (ESM-3B) and a decoder-only language model (LLaMA-3.1-8B-Instruct) through a lightweight nonlinear modality projector.\nA key innovation is our Hybrid Sequence-level Contrastive Alignment Learning (H-SCALE), which improves cross-modal learning by matching mean- and std-pooled protein embeddings with text representations via contrastive loss. After the alignment phase, we apply instruction-based fine-tuning using LoRA on the decoder to teach the model how to generate accurate protein function descriptions conditioned on the protein sequence. We train Prot2Text-V2 on about 250K curated entries from SwissProt and evaluate it under low-homology conditions, where test sequences have low similarity with training samples. Prot2Text-V2 consistently outperforms traditional and LLM-based baselines across various metrics.",
      "arxiv_url": "https://openreview.net/forum?id=w1FUXt3ujK",
      "pdf_url": "https://openreview.net/pdf/235f4969740ac334209d806e6ffa4f58ae79bef4.pdf",
      "primary_category": "protein function prediction, multimodal large language model, contrastive learning",
      "categories": [
        "protein function prediction",
        "multimodal large language model",
        "contrastive learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zj45hoQhjD",
      "title": "Scalable In-context Ranking with Generative Models",
      "authors": [
        "Nilesh Gupta",
        "Chong You",
        "Srinadh Bhojanapalli",
        "Sanjiv Kumar",
        "Inderjit S Dhillon",
        "Felix X. Yu"
      ],
      "abstract": "In-context Ranking (ICR) is an emerging paradigm for Information Retrieval (IR), which leverages contextual understanding of LLMs by directly incorporating the task description, candidate documents, and the query into the model's input prompt and tasking the LLM to identify relevant document(s). While it is effective, efficiency is a significant challenge in this paradigm, especially as the candidate list grows due to quadratic/super-linear scaling of attention operation with context length. To this end, this paper first identifies inherent and exploitable structures in the attention of LLMs finetuned for ICR: (1) inter-document block sparsity: attention is dense within each document block but sparse across different documents in the context; and (2) query-document block relevance: the attention scores from certain query tokens to a document block in middle layers strongly correlate with that document's actual relevance. Motivated by these observations, we introduce BlockRank (Blockwise In-context Ranking), a novel method that adapts the attention operation in an LLM by (a) architecturally enforcing the observed inter-document block sparsity, reducing attention complexity from quadratic to linear without loss in performance, and (b) optimizing query-document block relevance for true relevant documents during fine-tuning using an auxiliary contrastive training objective, improving retrieval in attention. Experiments on BEIR, MSMarco and NQ with Mistral-7B demonstrate that BlockRank Mistral matches or outperforms existing SOTA listwise rankers and controlled fine-tuned baseline while being significantly more efficient at inference (4.7x for 100 MSMarco documents in context) and scaling gracefully to long-context shortlists, around 500 documents in-context (approximately 100K context length) within a second, presenting a scalable and effective solution for ICR.",
      "arxiv_url": "https://openreview.net/forum?id=zj45hoQhjD",
      "pdf_url": "https://openreview.net/pdf/2bde23a7feb803c4553d37803139b782512dcc0f.pdf",
      "primary_category": "large language model, information retrieval, efficient fine-tuning",
      "categories": [
        "large language model",
        "information retrieval",
        "efficient fine-tuning",
        "efficient inference"
      ],
      "tags": [
        "LLM",
        "Information Retrieval"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "bythzT0b81",
      "title": "One Token per Highly Selective Frame: Towards Extreme Compression for Long Video Understanding",
      "authors": [
        "Zheyu Aqa Zhang",
        "Ziqi Pang",
        "Shixing Chen",
        "Xiang Hao",
        "Vimal Bhat",
        "Yu-Xiong Wang"
      ],
      "abstract": "Long video understanding is inherently challenging for vision-language models (VLMs) because of the extensive number of frames. With each video frame typically expanding into tens or hundreds of tokens, the limited context length of large language models (LLMs) forces the VLMs to perceive the frames sparsely and lose temporal information. To address this, we explore extreme video token compression towards *one token per frame* at the final LLM layer. Our key insight is that heuristic-based compression, widely adopted by previous methods, is prone to information loss, and this necessitates supervising LLM layers into *learnable* and *progressive* modules for *token-level compression* (LP-Comp). Such compression enables our VLM to digest 2x-4x more frames with improved performance. To further increase the token efficiency, we investigate *frame-level compression*, which selects the frames most relevant to the queries via the internal attention scores of the LLM layers, named *question-conditioned compression* (QC-Comp). As a notable distinction from previous studies, we mitigate the position bias of LLM attention in long contexts, *i.e.*, the over-concentration on the beginning and end of a sequence, by splitting long videos into short segments and employing local attention. Collectively, our combined *token-level* and *frame-level* leads to an e**x**treme compression model for long video understanding, named **XComp**, achieving a significantly larger compression ratio and enabling denser frame sampling. Our XComp is finetuned from VideoChat-Flash with a data-efficient *supervised compression tuning* stage that only requires 2.5\\% of the supervised fine-tuning data, yet boosts the accuracy from 42.9\\% to 46.2\\% on LVBench and enhances multiple other long video benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=bythzT0b81",
      "pdf_url": "https://openreview.net/pdf/c18bb43cf9ea521cb7bb577e464dc4a4e8907159.pdf",
      "primary_category": "Computer Vision, Long Video Understanding",
      "categories": [
        "Computer Vision",
        "Long Video Understanding"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Mr5Pyb3MLk",
      "title": "Training a Scientific Reasoning Model for Chemistry",
      "authors": [
        "Siddharth Narayanan",
        "James D. Braza",
        "Ryan-Rhys Griffiths",
        "Albert Bou",
        "Geemi Wellawatte",
        "Mayk Caldas Ramos",
        "Ludovico Mitchener",
        "Michael Martin Pieler",
        "Samuel G Rodriques",
        "Andrew White"
      ],
      "abstract": "Reasoning models are large language models that use extra \"thought tokens\" before answering, providing both higher accuracy and explicit reasoning for their response. A major question has been whether language model reasoning generalizes beyond mathematics, programming, and logic, where most previous work has focused. We demonstrate that reasoning models can be post-trained in scientific domains without additional domain pretraining, and require substantially less data compared to contemporary domain-specific models.\nWe report ether0, a 24B parameter LLM (based on Mistral-Small-24B) that can reason in natural language and respond with chemical structures. This reasoning model was trained with reinforcement learning on 577,790 experimentally-grounded chemistry tasks involving synthesized organic molecules. Our model outperforms all previous general-purpose chemistry models, frontier models, and humans, and is more data efficient relative to specialized models. We anticipate that this method can be applied to train highly data-efficient language models specialized for predictive and generative tasks across a wide variety of scientific domains.",
      "arxiv_url": "https://openreview.net/forum?id=Mr5Pyb3MLk",
      "pdf_url": "https://openreview.net/pdf/bd3fdfa6ce0058c01006a9468dbccfbcd2ee8131.pdf",
      "primary_category": "Large Language Models, Reasoning, AI4Science",
      "categories": [
        "Large Language Models",
        "Reasoning",
        "AI4Science"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "JV6ZOUb7BD",
      "title": "Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage",
      "authors": [
        "Ziqi Yuan",
        "Haoyang Zhang",
        "Yirui Eric Zhou",
        "Apoorve Mohan",
        "I-Hsin Chung",
        "Seetharami Seelam",
        "Jian Huang"
      ],
      "abstract": "We present the design and implementation of a new lifetime-aware tensor offloading\nframework for GPU memory expansion using low-cost PCIe-based solid-state\ndrives (SSDs). Our framework, TERAIO, is developed explicitly for large language\nmodel (LLM) training with multiple GPUs and multiple SSDs. Its design is driven\nby our observation that the active tensors take only a small fraction (1.7% on\naverage) of allocated GPU memory in each LLM training iteration, the inactive\ntensors are usually large and will not be used for a long period of time, creating\nample opportunities for offloading/prefetching tensors to/from slow SSDs without\nstalling the GPU training process. TERAIO accurately estimates the lifetime (active\nperiod of time in GPU memory) of each tensor with the profiling of the first few\niterations in the training process. With the tensor lifetime analysis, TERAIO will\ngenerate an optimized tensor offloading/prefetching plan and integrate it into the\ncompiled LLM program via PyTorch. TERAIO has a runtime tensor migration\nengine to execute the offloading/prefetching plan via GPUDirect storage, which\nallows direct tensor migration between GPUs and SSDs for alleviating the CPU\nbottleneck and maximizing the SSD bandwidth utilization. In comparison with\nstate-of-the-art studies such as ZeRO-Offload and ZeRO-Infinity, we show that\nTERAIO improves the training performance of various LLMs by 1.47× on average,\nand achieves 80.7% of the ideal performance assuming unlimited GPU memory.",
      "arxiv_url": "https://openreview.net/forum?id=JV6ZOUb7BD",
      "pdf_url": "https://openreview.net/pdf/80825867b457e7c96427848264d5acee0f767fc6.pdf",
      "primary_category": "GPU Memory, GPUDirect Storage, Large Language Model",
      "categories": [
        "GPU Memory",
        "GPUDirect Storage",
        "Large Language Model",
        "Offloading",
        "Cost-Efficient Training",
        "Solid State Drives"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "teB4aqJsNP",
      "title": "LLM Unlearning via Neural Activation Redirection",
      "authors": [
        "William F. Shen",
        "Xinchi Qiu",
        "Meghdad Kurmanji",
        "Alex Iacob",
        "Lorenzo Sani",
        "Yihong Chen",
        "Nicola Cancedda",
        "Nicholas D. Lane"
      ],
      "abstract": "The ability to selectively remove knowledge from LLMs is highly desirable. However, existing methods often struggle with balancing unlearning efficacy and retain model utility, and lack controllability at inference time to emulate base model behavior as if it had never seen the unlearned data. In this paper, we propose LUNAR, a novel unlearning method grounded in the Linear Representation Hypothesis and operates by redirecting the representations of unlearned data to activation regions that expresses its inability to answer. We show that contrastive features are not a prerequisite for effective activation redirection, and LUNAR achieves state-of-the-art unlearning performance and superior controllability. Specifically, LUNAR achieves between 2.9x and 11.7x improvement in the combined unlearning efficacy and model utility score (Deviation Score) across various base models and generates coherent, contextually appropriate responses post-unlearning. Moreover, LUNAR effectively reduces parameter updates to a single down-projection matrix, a novel design that significantly enhances efficiency by 20x and robustness. Finally, we demonstrate that LUNAR is robust to white-box adversarial attacks and versatile in real-world scenarios, including handling sequential unlearning requests.",
      "arxiv_url": "https://openreview.net/forum?id=teB4aqJsNP",
      "pdf_url": "https://openreview.net/pdf/177af11a2da859bf078fcb0d02e8b1afb22d08d7.pdf",
      "primary_category": "LLM unlearning",
      "categories": [
        "LLM unlearning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "e4IlBqhbTO",
      "title": "C3PO: Optimized Large Language Model Cascades with Probabilistic Cost Constraints for Reasoning",
      "authors": [
        "Antonios Valkanas",
        "Soumyasundar Pal",
        "Pavel Rumiantsev",
        "Yingxue Zhang",
        "Mark Coates"
      ],
      "abstract": "Large language models (LLMs) have achieved impressive results on complex reasoning tasks, but their high inference cost remains a major barrier to real-world deployment. A promising solution is to use cascaded inference, where small, cheap models handle easy queries, and only the hardest examples are escalated to more powerful models. However, existing cascade methods typically rely on supervised training with labeled data, offer no theoretical generalization guarantees, and provide limited control over test-time computational cost.\nWe introduce **C3PO** (*Cost Controlled Cascaded Prediction Optimization*), a self-supervised framework for optimizing LLM cascades under probabilistic cost constraints. By focusing on minimizing regret with respect to the most powerful model (MPM), C3PO avoids the need for labeled data by constructing a cascade using only unlabeled model outputs. It leverages conformal prediction to bound the probability that inference cost exceeds a user-specified budget.\nWe provide theoretical guarantees on both cost control and generalization error, and show that our optimization procedure is effective even with small calibration sets. Empirically, C3PO achieves state-of-the-art performance across a diverse set of reasoning benchmarks including GSM8K, MATH-500, BigBench-Hard and AIME, outperforming strong LLM cascading baselines in both accuracy and cost-efficiency. Our results demonstrate that principled, label-free cascade optimization can enable scalable LLM deployment.",
      "arxiv_url": "https://openreview.net/forum?id=e4IlBqhbTO",
      "pdf_url": "https://openreview.net/pdf/7e9f932dcbfb5b1eae4eba320757830a89785dc4.pdf",
      "primary_category": "large language model, efficiency, inference",
      "categories": [
        "large language model",
        "efficiency",
        "inference",
        "cascade"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "67xkPEM3bZ",
      "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis",
      "authors": [
        "Dheeraj Vattikonda",
        "Santhoshi Ravichandran",
        "Emiliano Penaloza",
        "Hadi Nekoei",
        "Thibault Le Sellier de Chezelles",
        "Megh Thakkar",
        "Nicolas Gontier",
        "Miguel Muñoz-Mármol",
        "Sahar Omidi Shayegan",
        "Stefania Raimondo",
        "Xue Liu",
        "Alexandre Drouin",
        "Alexandre Piché",
        "Alexandre Lacoste",
        "Massimo Caccia"
      ],
      "abstract": "Large language model (LLM) agents for web interfaces have advanced rapidly, yet open-source systems still lag behind proprietary agents. Bridging this gap is key to enabling customizable, efficient, and privacy-preserving agents. Two challenges hinder progress: the reproducibility issues in RL and LLM agent training, where results often depend on sensitive factors like seeds and decoding parameters, and the focus of prior work on single-step tasks, overlooking the complexities of web-based, multi-step decision-making.\n\nWe address these gaps by providing a statistically driven study of training LLM agents for web tasks. Our two-stage pipeline combines imitation learning from a Llama 3.3 70B teacher with on-policy fine-tuning via Group Relative Policy Optimization (GRPO) on a Llama 3.1 8B student. Through 240 configuration sweeps and rigorous bootstrapping, we chart the first compute allocation curve for open-source LLM web agents. Our findings show that dedicating one-third of compute to teacher traces and the rest to RL improves MiniWoB++ success by 6 points and closes 60\\% of the gap to GPT-4o on WorkArena, while cutting GPU costs by 45\\%. We introduce a principled hyperparameter sensitivity analysis, offering actionable guidelines for robust and cost-effective agent training.",
      "arxiv_url": "https://openreview.net/forum?id=67xkPEM3bZ",
      "pdf_url": "https://openreview.net/pdf/6e16a20966755c5a2868e3791f5bbd24ab8d2a6d.pdf",
      "primary_category": "LLM agents, interactive tasks, MDP",
      "categories": [
        "LLM agents",
        "interactive tasks",
        "MDP",
        "reinforcement learning",
        "reproducibility",
        "compute allocation",
        "generalization",
        "statistical analysis",
        "hyperparameters",
        "curriculum learning."
      ],
      "tags": [
        "LLM",
        "Agentic AI",
        "Search Agent"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "T4qJuQCFAK",
      "title": "Activation-Informed Merging of Large Language Models",
      "authors": [
        "Amin Heyrani Nobari",
        "Kaveh Alim",
        "Ali ArjomandBigdeli",
        "Akash Srivastava",
        "Faez Ahmed",
        "Navid Azizan"
      ],
      "abstract": "Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning (CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40% increase in benchmark performance. Our code is publicly available at https://github.com/ahnobari/ActivationInformedMerging",
      "arxiv_url": "https://openreview.net/forum?id=T4qJuQCFAK",
      "pdf_url": "https://openreview.net/pdf/7531fe3643d463542c0f6e22dfc4d2d7389d46f3.pdf",
      "primary_category": "Model Merging, Activation‑Informed Merging, Large Language Models",
      "categories": [
        "Model Merging",
        "Activation‑Informed Merging",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DIjRvEKOeG",
      "title": "SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training",
      "authors": [
        "Yehonathan Refael",
        "Guy Smorodinsky",
        "Tom Tirer",
        "Ofir Lindenbaum"
      ],
      "abstract": "Low-rank gradient-based optimization methods have significantly improved memory efficiency during the training of large language models (LLMs), enabling operations within constrained hardware without sacrificing performance. However, these methods primarily emphasize memory savings, often overlooking potential acceleration in convergence due to their reliance on standard isotropic steepest descent techniques, which can perform suboptimally in the highly anisotropic landscapes typical of deep networks, particularly LLMs. In this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an optimizer that employs exact singular value decomposition (SVD) for moment orthogonalization within a dynamically adapted low-dimensional subspace, enabling norm-inducing steepest descent optimization steps. By explicitly aligning optimization steps with the spectral characteristics of the loss landscape, SUMO effectively mitigates approximation errors associated with commonly used methods like Newton-Schulz orthogonalization approximation. We theoretically establish an upper bound on these approximation errors, proving their dependence on the condition numbers of moments, conditions we analytically demonstrate are encountered during LLM training. Furthermore, we both theoretically and empirically illustrate that exact orthogonalization via SVD substantially improves convergence rates while reducing overall complexity. Empirical evaluations confirm that SUMO accelerates convergence, enhances stability, improves performance, and reduces memory requirements by up to 20\\% compared to state-of-the-art methods.",
      "arxiv_url": "https://openreview.net/forum?id=DIjRvEKOeG",
      "pdf_url": "https://openreview.net/pdf/46beaa16688bb862bb0de6709dcccded74f07edc.pdf",
      "primary_category": "Low-rank optimization, Steepest descent, Moment orthogonalization",
      "categories": [
        "Low-rank optimization",
        "Steepest descent",
        "Moment orthogonalization",
        "Newton–Schulz method",
        "Large language model",
        "Memory efficient finetuning",
        "Pretraining"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "heY0zzGvYm",
      "title": "Reverse Engineering Human Preferences with Reinforcement Learning",
      "authors": [
        "Lisa Alazraki",
        "Yi-Chern Tan",
        "Jon Ander Campos",
        "Maximilian Mozes",
        "Marek Rei",
        "Max Bartolo"
      ],
      "abstract": "The capabilities of Large Language Models (LLMs) are routinely evaluated by other LLMs trained to predict human preferences. This framework—known as *LLM-as-a-judge*—is highly scalable and relatively low cost. However, it is also vulnerable to malicious exploitation, as LLM responses can be tuned to overfit the preferences of the judge. Previous work shows that the answers generated by a candidate-LLM can be edited *post hoc* to maximise the score assigned to them by a judge-LLM. In this study, we adopt a different approach and use the signal provided by judge-LLMs as a reward to adversarially tune models that generate text preambles designed to boost downstream performance. We find that frozen LLMs pipelined with these models attain higher LLM-evaluation scores than existing frameworks. Crucially, unlike other frameworks which intervene directly on the model's response, our method is virtually undetectable. We also demonstrate that the effectiveness of the tuned preamble generator transfers when the candidate-LLM and the judge-LLM are replaced with models that are not used during training. These findings raise important questions about the design of more reliable LLM-as-a-judge evaluation settings. They also demonstrate that human preferences can be reverse engineered effectively, by pipelining LLMs to optimise upstream preambles via reinforcement learning—an approach that could find future applications in diverse tasks and domains beyond adversarial attacks.",
      "arxiv_url": "https://openreview.net/forum?id=heY0zzGvYm",
      "pdf_url": "https://openreview.net/pdf/92f4508273884ba1413de3f4dd3ec020b732f559.pdf",
      "primary_category": "LLM-as-a-judge, Human Preferences, Reinforcement Learning",
      "categories": [
        "LLM-as-a-judge",
        "Human Preferences",
        "Reinforcement Learning",
        "Adversarial attack",
        "Robustness"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "SiBVbL7rsX",
      "title": "Localizing Knowledge in Diffusion Transformers",
      "authors": [
        "Arman Zarei",
        "Samyadeep Basu",
        "Keivan Rezaei",
        "Zihao Lin",
        "Sayan Nag",
        "Soheil Feizi"
      ],
      "abstract": "Understanding how knowledge is distributed across the layers of generative models is crucial for improving interpretability, controllability, and adaptation. While prior work has explored knowledge localization in UNet-based architectures, Diffusion Transformer (DiT)-based models remain underexplored in this context. In this paper, we propose a model- and knowledge-agnostic method to localize where specific types of knowledge are encoded within the DiT blocks. We evaluate our method on state-of-the-art DiT-based models, including PixArt-$\\alpha$, FLUX, and SANA, across six diverse knowledge categories. We show that the identified blocks are both interpretable and causally linked to the expression of knowledge in generated outputs.\nBuilding on these insights, we apply our localization framework to two key applications: *model personalization* and *knowledge unlearning*. In both settings, our localized fine-tuning approach enables efficient and targeted updates, reducing computational cost, improving task-specific performance, and better preserving general model behavior with minimal interference to unrelated or surrounding content.\nOverall, our findings offer new insights into the internal structure of DiTs and introduce a practical pathway for more interpretable, efficient, and controllable model editing.",
      "arxiv_url": "https://openreview.net/forum?id=SiBVbL7rsX",
      "pdf_url": "https://openreview.net/pdf/b71c16303d1aec4598a1a952ae788b455d65f7cd.pdf",
      "primary_category": "Localization, Interpretability, Diffusion Models",
      "categories": [
        "Localization",
        "Interpretability",
        "Diffusion Models",
        "Diffusion Transformers"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "sWIbJWiEuA",
      "title": "Adaptive Distraction: Probing LLM Contextual Robustness with Automated Tree Search",
      "authors": [
        "Yanbo Wang",
        "Zixiang Xu",
        "Yue Huang",
        "Chujie Gao",
        "Siyuan Wu",
        "Jiayi Ye",
        "Pin-Yu Chen",
        "Xiuying Chen",
        "Xiangliang Zhang"
      ],
      "abstract": "Large Language Models (LLMs) often struggle to maintain their original performance when faced with semantically coherent but task-irrelevant contextual information. Although prior studies have explored this issue using fixed-template or retrieval-based distractions, such static methods show limited effectiveness against contemporary models. To address this problem, we propose a dynamic distraction generation framework based on tree search, where the generation process is guided by model behavior. Without modifying the original question or answer, the method efficiently produces challenging adaptive distractions across multiple datasets, enabling systematic stress testing of LLMs’ contextual robustness. Experiments on four benchmarks demonstrate that the generated distractions lead to an average performance drop of over 45\\% for mainstream models. Further comparisons of mitigation strategies show that prompt-based optimization methods yield limited gains, whereas post-training approaches (e.g., DPO) significantly enhance the model's contextual robustness. The results indicate that these issues do not stem from knowledge deficits in LLMs, but from a fundamental inability to maintain consistent reasoning under contextual distraction, posing a major challenge to the reliability of LLMs in real-world applications.",
      "arxiv_url": "https://openreview.net/forum?id=sWIbJWiEuA",
      "pdf_url": "https://openreview.net/pdf/2852e81b0c7cf7e932681273b4d068d25b85c431.pdf",
      "primary_category": "Large Language Models, Robustness Evaluation, Tree Search",
      "categories": [
        "Large Language Models",
        "Robustness Evaluation",
        "Tree Search",
        "Contextual Perturbation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "TjWdyVWBAG",
      "title": "ExGra-Med: Extended Context Graph Alignment for Medical Vision-Language Models",
      "authors": [
        "Duy Minh Ho Nguyen",
        "Nghiem Tuong Diep",
        "Trung Quoc Nguyen",
        "Hoang-Bao Le",
        "Tai Nguyen",
        "Anh-Tien Nguyen",
        "TrungTin Nguyen",
        "Nhat Ho",
        "Pengtao Xie",
        "Roger Wattenhofer",
        "Daniel Sonntag",
        "James Zou",
        "Mathias Niepert"
      ],
      "abstract": "State-of-the-art medical multi-modal LLMs (med-MLLMs), such as LLaVA-Med and BioMedGPT, primarily depend on scaling model size and data volume, with training driven largely by autoregressive objectives. However, we reveal that this approach can lead to weak vision-language alignment, making these models overly dependent on costly instruction-following data. To address this, we introduce ExGra-Med, a novel multi-graph alignment framework that jointly aligns images, instruction responses, and extended captions in the latent space, advancing semantic grounding and cross-modal coherence. To scale to large LLMs (e.g., LLaMa-7B), we develop an efficient end-to-end training scheme using black-box gradient estimation, enabling fast and scalable optimization. Empirically, ExGra-Med matches LLaVA-Med’s performance using just 10\\% of pre-training data, achieving a 20.13\\% gain on VQA-RAD and approaching full-data performance. It also outperforms strong baselines like BioMedGPT and RadFM on visual chatbot and zero-shot classification tasks, demonstrating its promise for efficient, high-quality vision-language integration in medical AI.",
      "arxiv_url": "https://openreview.net/forum?id=TjWdyVWBAG",
      "pdf_url": "https://openreview.net/pdf/618a3b842d163b27e2341dce47c4f62a805fadd7.pdf",
      "primary_category": "multi-modal LLM, multi-modal learning, representation learning",
      "categories": [
        "multi-modal LLM",
        "multi-modal learning",
        "representation learning",
        "healthcare"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qYSgnmT3dp",
      "title": "Non-Markovian Discrete Diffusion with Causal Language Models",
      "authors": [
        "Yangtian Zhang",
        "Sizhuang He",
        "Daniel Levine",
        "Lawrence Zhao",
        "David Zhang",
        "Syed A Rizvi",
        "Shiyang Zhang",
        "Emanuele Zappala",
        "Rex Ying",
        "David van Dijk"
      ],
      "abstract": "Discrete diffusion models offer a flexible, controllable approach to structured sequence generation, yet they still lag behind causal language models in expressive power. A key limitation lies in their reliance on the Markovian assumption, which restricts each step to condition only on the current state, leading to potential uncorrectable error accumulation.\nIn this paper, We introduce CaDDi, a discrete diffusion model that conditions on the entire generative trajectory, thereby lifting the Markov constraint and allowing the model to revisit and improve past states. By unifying sequential (causal) and temporal (diffusion) reasoning in a single non‑Markovian transformer, CaDDi also treats standard causal language models as a special case and permits the direct reuse of pretrained LLM weights with no architectural changes. Empirically, CaDDi outperforms state‑of‑the‑art discrete diffusion baselines on natural‑language benchmarks, substantially narrowing the remaining gap to large autoregressive transformers.",
      "arxiv_url": "https://openreview.net/forum?id=qYSgnmT3dp",
      "pdf_url": "https://openreview.net/pdf/e5aeb1f0ea65eb4eed5112fbeb936bfc00bc30be.pdf",
      "primary_category": "Generative Model, Causal Language Model",
      "categories": [
        "Generative Model",
        "Causal Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "f0660KxvT2",
      "title": "TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval",
      "authors": [
        "Jialin Chen",
        "Ziyu Zhao",
        "Gaukhar Nurbek",
        "Aosong Feng",
        "Ali Maatouk",
        "Leandros Tassiulas",
        "Yifeng Gao",
        "Rex Ying"
      ],
      "abstract": "The ubiquity of dynamic data in domains such as weather, healthcare, and energy underscores a growing need for effective interpretation and retrieval of time-series data. These data are inherently tied to domain-specific contexts, such as clinical notes or weather narratives, making cross-modal retrieval essential not only for downstream tasks but also for developing robust time-series foundation models by retrieval-augmented generation (RAG). Despite the increasing demand, time-series retrieval remains largely underexplored. Existing methods often lack semantic grounding, struggle to align heterogeneous modalities, and have limited capacity for handling multi-channel signals. To address this gap, we propose TRACE, a generic multimodal retriever that grounds time-series embeddings in aligned textual context. TRACE enables fine-grained channel-level alignment and employs hard negative mining to facilitate semantically meaningful retrieval. It supports flexible cross-modal retrieval modes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking linguistic descriptions with complex temporal patterns. By retrieving semantically relevant pairs, TRACE enriches downstream models with informative context, leading to improved predictive accuracy and interpretability. Beyond a static retrieval engine, TRACE also serves as a powerful standalone encoder, with lightweight task-specific tuning that refines context-aware representations while maintaining strong cross-modal alignment. These representations achieve state-of-the-art performance on downstream forecasting and classification tasks. Extensive experiments across multiple domains highlight its dual utility, as both an effective encoder for downstream applications and a general-purpose retriever to enhance time-series models.",
      "arxiv_url": "https://openreview.net/forum?id=f0660KxvT2",
      "pdf_url": "https://openreview.net/pdf/422227b47dc66dbbfd6efef684dcdf29b8c32de6.pdf",
      "primary_category": "time series analysis, multimodal retrieval",
      "categories": [
        "time series analysis",
        "multimodal retrieval"
      ],
      "tags": [
        "LLM",
        "RAG",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RnbJPkakkm",
      "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
      "authors": [
        "Neil He",
        "Rishabh Anand",
        "Hiren Madhu",
        "Ali Maatouk",
        "Smita Krishnaswamy",
        "Leandros Tassiulas",
        "Menglin Yang",
        "Rex Ying"
      ],
      "abstract": "Frontier large language models (LLMs) have shown great success in text modeling and generation tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations such as dot-products and norms. Furthermore, recent studies have shown that not respecting the underlying geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in $\\textit{Hyperbolic space}$, known for its expansive, scale-free, and low-distortion properties. To this end, we introduce $\\textbf{HELM}$, a family of $\\textbf{H}$yp$\\textbf{E}$rbolic Large $\\textbf{L}$anguage $\\textbf{M}$odels, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a $\\textbf{Mi}$xture-of-$\\textbf{C}$urvature $\\textbf{E}$xperts model, $\\textbf{HELM-MiCE}$, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, $\\textbf{HELM-D}$. For $\\textbf{HELM-MiCE}$, we further develop hyperbolic Multi-Head Latent Attention ($\\textbf{HMLA}$) for efficient, reduced-KV-cache training and inference. For both models, we further develop essential hyperbolic equivalents of rotary positional encodings and root mean square normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our $\\textbf{HELM}$ architectures – up to 4\\% – over popular Euclidean architectures used in LLaMA and DeepSeek with superior semantic hierarchy modeling capabilities, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale language model pretraining.",
      "arxiv_url": "https://openreview.net/forum?id=RnbJPkakkm",
      "pdf_url": "https://openreview.net/pdf/abd1a1e76d2fb8a4e2ccc7a03a515f2cf407f39f.pdf",
      "primary_category": "large language models, hyperbolic geometry, mixture-of-experts",
      "categories": [
        "large language models",
        "hyperbolic geometry",
        "mixture-of-experts"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Vzi96rTe4w",
      "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery",
      "authors": [
        "Yiming Gao",
        "Zhen Wang",
        "Jefferson Chen",
        "Mark Antkowiak",
        "Mengzhou Hu",
        "JungHo Kong",
        "Dexter Pratt",
        "Jieyuan Liu",
        "Enze Ma",
        "Zhiting Hu",
        "Eric P. Xing"
      ],
      "abstract": "We present scPilot, the first systematic framework to practice \\textit{omics-native reasoning}: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation, developmental-trajectory reconstruction, and transcription-factor targeting, into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence. To measure progress, we release \\scbench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that \\textit{iterative} omics-native reasoning lifts average accuracy by 11\\% for cell-type annotation and Gemini 2.5 Pro cuts trajectory graph-edit distance by 30\\% versus one-shot prompting, while generating transparent reasoning traces that explain marker gene ambiguity and regulatory logic. By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses.",
      "arxiv_url": "https://openreview.net/forum?id=Vzi96rTe4w",
      "pdf_url": "https://openreview.net/pdf/8e4f86f884a36bdcdb6a62631fb142df956d2e99.pdf",
      "primary_category": "Large Language Models, Scientific Reasoning, Automation of Science",
      "categories": [
        "Large Language Models",
        "Scientific Reasoning",
        "Automation of Science",
        "Single-cell Transcriptomics",
        "Cell Type Annotation",
        "Trajectory Inference",
        "Gene Regulatory Network Inference"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Eyis2h3tba",
      "title": "Set-LLM: A Permutation-Invariant LLM",
      "authors": [
        "Beni Egressy",
        "Jan Stühmer"
      ],
      "abstract": "While large language models (LLMs) demonstrate impressive capabilities across numerous applications, their robustness remains a critical concern. This paper is motivated by a specific vulnerability: the order sensitivity of LLMs. This vulnerability manifests itself as the order bias observed when LLMs decide between possible options (for example, a preference for the first option) and the tendency of LLMs to provide different answers when options are reordered. The use cases for this scenario extend beyond the classical case of multiple-choice question answering to the use of LLMs for multidocument tasks and as automated evaluators in AI pipelines. We introduce Set-LLM, a novel architectural adaptation for pretrained LLMs that enables the processing of mixed set-text inputs with permutation invariance guarantees. The adaptations involve a new attention mask and new positional encodings specifically designed for sets. We provide a theoretical proof of invariance and demonstrate through experiments that Set-LLM can be trained effectively, achieving comparable or improved performance and maintaining the runtime of the original model, while altogether eliminating order sensitivity.",
      "arxiv_url": "https://openreview.net/forum?id=Eyis2h3tba",
      "pdf_url": "https://openreview.net/pdf/62cce41915750322d5d7118798dba9348b81151a.pdf",
      "primary_category": "LLMs, Large Language Models, LLM evaluator",
      "categories": [
        "LLMs",
        "Large Language Models",
        "LLM evaluator",
        "LLM as a Judge",
        "Permutation Invariance",
        "Attention"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oOPdUnswhZ",
      "title": "Large Language Models as Model Organisms for Human Associative Learning",
      "authors": [
        "Camila Kolling",
        "Vy A. Vo",
        "Mariya Toneva"
      ],
      "abstract": "Associative learning--forming links between co-occurring items--is fundamental to human cognition, reshaping internal representations in complex ways. Testing hypotheses on how representational changes occur in biological systems is challenging, but large language models (LLMs) offer a scalable alternative. Building on LLMs' in-context learning, we adapt a cognitive neuroscience associative learning paradigm and investigate how representations evolve across six models. Our initial findings reveal a non-monotonic pattern consistent with the Non-Monotonic Plasticity Hypothesis, with moderately similar items differentiating after learning. Leveraging the controllability of LLMs, we further show that this differentiation is modulated by the overlap of associated items with the broader vocabulary--a factor we term vocabulary interference, capturing how new associations compete with prior knowledge. We find that higher vocabulary interference amplifies differentiation, suggesting that representational change is influenced by both item similarity and global competition. Our findings position LLMs not only as powerful tools for studying representational dynamics in human-like learning systems, but also as accessible and general computational models for generating new hypotheses about the principles underlying memory reorganization in the brain.",
      "arxiv_url": "https://openreview.net/forum?id=oOPdUnswhZ",
      "pdf_url": "https://openreview.net/pdf/a9665b6d6e239008a91e56467f55809526cb9040.pdf",
      "primary_category": "Large Language Models, Associative Learning, In-Context Learning",
      "categories": [
        "Large Language Models",
        "Associative Learning",
        "In-Context Learning",
        "Representational Change"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "R9xJSk5SQ2",
      "title": "DEFT: Decompositional Efficient Fine-Tuning for Text-to-Image Models",
      "authors": [
        "Komal Kumar",
        "Rao Muhammad Anwer",
        "Fahad Shahbaz Khan",
        "Salman Khan",
        "Ivan Laptev",
        "Hisham Cholakkal"
      ],
      "abstract": "Efficient fine-tuning of pre-trained Text-to-Image (T2I) models involves adjusting the model to suit a particular task or dataset while minimizing computational resources and limiting the number of trainable parameters. However, it often faces challenges in striking a trade-off between aligning with the target distribution: learning a novel concept from a limited image for personalization and retaining the instruction ability needed for unifying multiple tasks, all while maintaining editability (aligning with a variety of prompts or in-context generation). In this work, we introduce DEFT, Decompositional Efficient Fine-Tuning, an efficient fine-tuning framework that adapts a pre-trained weight matrix by decomposing its update into two components with two trainable matrices: (1) a projection onto the complement of a low-rank subspace spanned by a low-rank matrix, and (2) a low-rank update. The single trainable low-rank matrix defines the subspace, while the other trainable low-rank matrix enables parameter adaptation within that subspace. We conducted extensive experiments on the Dreambooth and Dreambench Plus datasets for personalization, the InsDet dataset for object and scene adaptation, and the VisualCloze dataset for a universal image generation framework through visual in-context learning with both Stable Diffusion and a unified model. Our results demonstrated state-of-the-art performance, highlighting the emergent properties of efficient fine-tuning. Our code is available on \\href{https://github.com/MAXNORM8650/DEFT}{DEFT}.",
      "arxiv_url": "https://openreview.net/forum?id=R9xJSk5SQ2",
      "pdf_url": "https://openreview.net/pdf/141c43804acd53ba9d22569207f0c297dd3d06c8.pdf",
      "primary_category": "Efficient Fine-Tuning, LoRA, Unified Models",
      "categories": [
        "Efficient Fine-Tuning",
        "LoRA",
        "Unified Models",
        "Personalization",
        "Model Adaptation",
        "Parameter-Efficient Training",
        "Text-to-Image Models"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XBMjXb6f4w",
      "title": "CTRL-ALT-DECEIT Sabotage Evaluations for Automated AI R&D",
      "authors": [
        "Francis Rhys Ward",
        "Teun van der Weij",
        "Hanna Gábor",
        "Sam Martin",
        "Raja Mehta Moreno",
        "Harel Lidar",
        "Louis Makower",
        "Thomas Jodrell",
        "Lauren Robson"
      ],
      "abstract": "AI systems are increasingly able to autonomously conduct realistic software engineering tasks, and may soon be deployed to automate machine learning (ML) R\\&D itself. Frontier AI systems may be deployed in safety-critical settings, including to help ensure the safety of future systems. Unfortunately, frontier and future systems may not be sufficiently trustworthy, and there is evidence that these systems may even be misaligned with their developers or users. Therefore, we investigate the capabilities of AI agents to act against the interests of their users when conducting ML engineering, by sabotaging ML models, sandbagging their performance, and subverting oversight mechanisms. First, we extend MLE-Bench, a benchmark for realistic ML tasks, with code-sabotage tasks such as implanting backdoors and purposefully causing generalisation failures. Frontier agents make meaningful progress on our sabotage tasks. In addition, we study agent capabilities to sandbag on MLE-Bench. Agents can calibrate their performance to specified target levels below their actual capability. To mitigate sabotage, we use LM monitors to detect suspicious agent behaviour, and we measure model capability to sabotage and sandbag without being detected by these monitors. Overall, monitors are capable at detecting code-sabotage attempts but our results suggest that detecting sandbagging is more difficult. Additionally, aggregating multiple monitor predictions works well, but monitoring may not be sufficiently reliable to mitigate sabotage in high-stakes domains. Our benchmark is implemented in the UK AISI’s Inspect framework and we make our code publicly available.",
      "arxiv_url": "https://openreview.net/forum?id=XBMjXb6f4w",
      "pdf_url": "https://openreview.net/pdf/5ba46eb607f50910cda80a0bfc28f6c4f5b9c656.pdf",
      "primary_category": "Sabotage Evaluations, Sandbagging, AI Control",
      "categories": [
        "Sabotage Evaluations",
        "Sandbagging",
        "AI Control",
        "AI Safety",
        "AI Alignment",
        "Dangerous Capability Evals"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "KoVKLxn3Nb",
      "title": "Conformal Prediction Beyond the Seen: A Missing Mass Perspective for Uncertainty Quantification in Generative Models",
      "authors": [
        "Sima Noorani",
        "Shayan Kiyani",
        "George J. Pappas",
        "Hamed Hassani"
      ],
      "abstract": "Uncertainty quantification (UQ) is essential for safe deployment of generative AI models such as large language models (LLMs), especially in high-stakes applications. Conformal prediction (CP) offers a principled uncertainty quantification framework, but classical methods focus on regression and classification, relying on geometric distances or softmax scores--tools that presuppose structured outputs. We depart from this paradigm by studying CP in a query-only setting, where prediction sets must be constructed solely from finite queries to a black-box generative model, introducing a new trade-off between coverage, test-time query budget, and informativeness. We introduce **Conformal Prediction with Query Oracle** (CPQ), a framework characterizing the optimal interplay between these objectives. Our finite-sample algorithm is built on two core principles: one governs the optimal query policy, and the other defines the optimal mapping from queried samples to prediction sets. Remarkably, both are rooted in the classical **missing mass problem** in statistics. Specifically, the optimal query policy depends on the rate of decay--or the derivative--of the missing mass, for which we develop a novel estimator. Meanwhile, the optimal mapping hinges on the missing mass itself, which we estimate using Good-Turing estimators. We then turn our focus to implementing our method for language models, particularly in open-ended LLM tasks involving question answering, multi-step reasoning, and structured information extraction, where outputs are vast, variable, and often under-specified. Fine-grained experiments on three real-world open-ended tasks and two LLMs, show CPQ's applicability to **any black-box LLM** and highlight: (1) individual contribution of each principle to CPQ’s performance, and (2) CPQ's ability to yield significantly more informative prediction sets than existing conformal methods for language uncertainty quantification.",
      "arxiv_url": "https://openreview.net/forum?id=KoVKLxn3Nb",
      "pdf_url": "https://openreview.net/pdf/93421ac8a75346552e1f9a9d80e29f3ae7c6ffe4.pdf",
      "primary_category": "Conformal prediction, Uncertainty quantification",
      "categories": [
        "Conformal prediction",
        "Uncertainty quantification"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "iEtCCt6FjP",
      "title": "Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning",
      "authors": [
        "Boheng Li",
        "Renjie Gu",
        "Junjie Wang",
        "Leyi Qi",
        "Yiming Li",
        "Run Wang",
        "Zhan Qin",
        "Tianwei Zhang"
      ],
      "abstract": "Text-to-image (T2I) diffusion models have achieved impressive image generation quality and are increasingly fine-tuned for personalized applications. However, these models often inherit unsafe behaviors from toxic pretraining data, raising growing safety concerns. While recent safety-driven unlearning methods have made promising progress in suppressing model toxicity, they are found to be fragile to downstream fine-tuning, as we reveal that state-of-the-art methods largely fail to retain their effectiveness even when fine-tuned on entirely benign datasets. To mitigate this problem, in this paper, we propose ResAlign, a safety-driven unlearning framework with enhanced resilience against downstream fine-tuning. By modeling downstream fine-tuning as an implicit optimization problem with a Moreau envelope-based reformulation, ResAlign enables efficient gradient estimation to minimize the recovery of harmful behaviors. Additionally, a meta-learning strategy is proposed to simulate a diverse distribution of fine-tuning scenarios to improve generalization. Extensive experiments across a wide range of datasets, fine-tuning methods, and configurations demonstrate that ResAlign consistently outperforms prior unlearning approaches in retaining safety, while effectively preserving benign generation capability. Our code and pretrained models are publicly available at https://github.com/AntigoneRandy/ResAlign.",
      "arxiv_url": "https://openreview.net/forum?id=iEtCCt6FjP",
      "pdf_url": "https://openreview.net/pdf/86a00011fbf76c92293b19bea559458f4759ec56.pdf",
      "primary_category": "Diffusion Models, AI Safety, Model Personalization",
      "categories": [
        "Diffusion Models",
        "AI Safety",
        "Model Personalization"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "eWxKpdAdXH",
      "title": "Refusal Direction is Universal Across Safety-Aligned Languages",
      "authors": [
        "Xinpeng Wang",
        "Mingyang Wang",
        "Yihong Liu",
        "Hinrich Schuetze",
        "Barbara Plank"
      ],
      "abstract": "Refusal mechanisms in large language models (LLMs) are essential for ensuring safety. Recent research has revealed that refusal behavior can be mediated by a single direction in activation space, enabling targeted interventions to bypass refusals. While this is primarily demonstrated in an English-centric context, appropriate refusal behavior is important for any language, but poorly understood. In this paper, we investigate the refusal behavior in LLMs across 14 languages using \\textit{PolyRefuse}, a multilingual safety dataset created by translating malicious and benign English prompts into these languages. We uncover the surprising cross-lingual universality of the refusal direction: a vector extracted from English can bypass refusals in other languages with near-perfect effectiveness, without any additional fine-tuning. Even more remarkably, refusal directions derived from any safety-aligned language transfer seamlessly to others. We attribute this transferability to the parallelism of refusal vectors across languages in the embedding space and identify the underlying mechanism behind cross-lingual jailbreaks. These findings provide actionable insights for building more robust multilingual safety defenses and pave the way for a deeper mechanistic understanding of cross-lingual vulnerabilities in LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=eWxKpdAdXH",
      "pdf_url": "https://openreview.net/pdf/066153bcff2e9a8b54d827454a6f02b16c7d79c7.pdf",
      "primary_category": "Multilingual Safety, LLM Jailbreaks, Crosslingual Transfer",
      "categories": [
        "Multilingual Safety",
        "LLM Jailbreaks",
        "Crosslingual Transfer"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "FiM0M8gcct",
      "title": "A-Mem: Agentic Memory for LLM Agents",
      "authors": [
        "Wujiang Xu",
        "Zujie Liang",
        "Kai Mei",
        "Hang Gao",
        "Juntao Tan",
        "Yongfeng Zhang"
      ],
      "abstract": "While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution -- as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management.\nEmpirical experiments on six foundation models show superior improvement against existing SOTA baselines. The code is available at \\url{https://anonymous.4open.science/r/AgenticMemory-76B4}.",
      "arxiv_url": "https://openreview.net/forum?id=FiM0M8gcct",
      "pdf_url": "https://openreview.net/pdf/518e506d098a6c821c7e4ae97d1368f374fddac9.pdf",
      "primary_category": "LLM Agents; Memory System",
      "categories": [
        "LLM Agents; Memory System"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "lO7RGax6u9",
      "title": "Timely Clinical Diagnosis through Active Test Selection",
      "authors": [
        "Silas Ruhrberg Estévez",
        "Nicolás Astorga",
        "Mihaela van der Schaar"
      ],
      "abstract": "There is growing interest in using machine learning (ML) to support clinical diagnosis, but most approaches rely on static, fully observed datasets and fail to reflect the sequential, resource-aware reasoning clinicians use in practice. Diagnosis remains complex and error prone, especially in high-pressure or resource-limited settings, underscoring the need for frameworks that help clinicians make timely and cost-effective decisions. We propose ACTMED (Adaptive Clinical Test selection via Model-based Experimental Design), a diagnostic framework that integrates Bayesian Experimental Design (BED) with large language models (LLMs) to better emulate real-world diagnostic reasoning. At each step, ACTMED selects the test expected to yield the greatest reduction in diagnostic uncertainty for a given patient. LLMs act as flexible simulators, generating plausible patient state distributions and supporting belief updates without requiring structured, task-specific training data. Clinicians can remain in the loop; reviewing test suggestions, interpreting intermediate outputs, and applying clinical judgment throughout. We evaluate ACTMED on real-world datasets and show it can optimize test selection to improve diagnostic accuracy, interpretability, and resource use. This represents a step toward transparent, adaptive, and clinician-aligned diagnostic systems that generalize across settings with reduced reliance on domain-specific data.",
      "arxiv_url": "https://openreview.net/forum?id=lO7RGax6u9",
      "pdf_url": "https://openreview.net/pdf/664ac042af443f56386bbc92a26ca7f0e61378c7.pdf",
      "primary_category": "LLMs, Clinical Diagnosis, Bayesian Experimental Design",
      "categories": [
        "LLMs",
        "Clinical Diagnosis",
        "Bayesian Experimental Design",
        "BED",
        "Active learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6IuURCuooO",
      "title": "Aligning Compound AI Systems via System-level DPO",
      "authors": [
        "Xiangwen Wang",
        "Yibo Jacky Zhang",
        "Zhoujie Ding",
        "Katherine Tsai",
        "Haolun Wu",
        "Sanmi Koyejo"
      ],
      "abstract": "Compound AI systems, comprising multiple interacting components such as LLMs, foundation models, and external tools, have demonstrated remarkable improvements compared to single models in various tasks. To ensure their effective deployment in real-world applications, aligning these systems with human preferences is crucial. However, aligning the compound system via policy optimization, unlike the alignment of a single model, is challenging for two main reasons: (i) non-differentiable interactions between components make end-to-end gradient-based optimization method inapplicable, and (ii) system-level preferences cannot be directly transformed into component-level preferences. To address these challenges, we first formulate compound AI systems as Directed Acyclic Graphs (DAGs), explicitly modeling both component interactions and the associated data flows. Building on this formulation, we introduce SysDPO, a framework that extends Direct Preference Optimization (DPO) to enable joint system-level alignment. \nWe propose two variants, SysDPO-Direct and SysDPO-Sampling, tailored for scenarios depending on whether we construct a system-specific preference dataset. We empirically demonstrate the effectiveness of our approach across two applications: the joint alignment of a language model and a diffusion model, and the joint alignment of an LLM collaboration system.",
      "arxiv_url": "https://openreview.net/forum?id=6IuURCuooO",
      "pdf_url": "https://openreview.net/pdf/a7bbb13e9dac8f25d201082baccee38450fee96b.pdf",
      "primary_category": "alignment, compound AI system, preference learning",
      "categories": [
        "alignment",
        "compound AI system",
        "preference learning",
        "DPO"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "FzfYoUp8F1",
      "title": "Learning World Models for Interactive Video Generation",
      "authors": [
        "Taiye Chen",
        "Xun Hu",
        "Zihan Ding",
        "Chi Jin"
      ],
      "abstract": "Foundational world models must be both interactive and preserve spatialtemporal coherence to enable effective future planning with different action choices. However, present models for long video generation have limited inherent world modeling capabilities due to two main challenges: compounding errors and insufficient memory mechanisms. \nWe enhance image-to-video models with interactive capabilities through additional action conditioning and autoregressive framework, and reveal that compounding error is inherently irreducible in autoregressive video generation, while insufficient memory mechanism leads to incoherence of world models. We propose video retrieval augmented generation (VRAG) with explicit global state conditioning, which significantly reduces long-term compounding errors and increases spatialtemporal consistency of video world models. \nIn contrast, naive autoregressive generation with extended context windows and retrieval-augmented generation prove less effective for video generation, primarily due to the limited in-context learning capabilities of current video models. Our work illuminates the fundamental challenges in video world models and establishes a comprehensive benchmark for improving video generation models with internal world modeling capabilities.",
      "arxiv_url": "https://openreview.net/forum?id=FzfYoUp8F1",
      "pdf_url": "https://openreview.net/pdf/0086d443dc2c8ab6cc93cca0645a5c3a1bfcb22d.pdf",
      "primary_category": "world model, diffusion model, video generation",
      "categories": [
        "world model",
        "diffusion model",
        "video generation"
      ],
      "tags": [
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "WDdBhcwzGe",
      "title": "Multi-Token Prediction Needs Registers",
      "authors": [
        "Anastasios Gerontopoulos",
        "Spyros Gidaris",
        "Nikos Komodakis"
      ],
      "abstract": "Multi-token prediction has emerged as a promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. In this paper, we propose MuToR, a simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. Compared to existing methods, MuToR offers several key advantages: it introduces only a negligible number of additional parameters, requires no architectural changes—ensuring compatibility with off-the-shelf pretrained language models—and remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. Moreover, it naturally supports scalable prediction horizons. We demonstrate the effectiveness and versatility of MuToR across a range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains.",
      "arxiv_url": "https://openreview.net/forum?id=WDdBhcwzGe",
      "pdf_url": "https://openreview.net/pdf/b4e504a9ac6dd1456d6dac8e1725f0a87495c2d5.pdf",
      "primary_category": "autoregressive transformers, next-token prediction, multi-token prediction",
      "categories": [
        "autoregressive transformers",
        "next-token prediction",
        "multi-token prediction",
        "register tokens"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "mH9FJi3KTX",
      "title": "Magical: Medical Lay Language Generation via Semantic Invariance and Layperson-tailored Adaptation",
      "authors": [
        "Weibin Liao",
        "Tianlong Wang",
        "Yinghao Zhu",
        "Yasha Wang",
        "Junyi Gao",
        "Liantao Ma"
      ],
      "abstract": "Medical Lay Language Generation (MLLG) plays a vital role in improving the accessibility of complex scientific content for broader audiences. Recent literature to MLLG commonly employ parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using paired expert-lay language datasets. However, LoRA struggles with the challenges posed by multi-source heterogeneous MLLG datasets. Specifically, through a series of exploratory experiments, we reveal that standard LoRA fail to meet the requirement for semantic fidelity and diverse lay-style generation in MLLG task. To address these limitations, we propose Magical, an asymmetric LoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical employs a shared matrix A for abstractive summarization, along with multiple isolated matrices B for diverse lay-style generation. To preserve semantic fidelity during the lay language generation process, Magical introduces a Semantic Invariance Constraint to mitigate semantic subspace shifts on matrix A. Furthermore, to better adapt to diverse lay-style generation, Magical incorporates the Recommendation-guided Switch, an externally interface to prompt the LLM to switch between different matrices B. Experimental results on three real-world lay language generation datasets demonstrate that Magical consistently outperforms prompt-based methods, vanilla LoRA, and its recent variants, while also reducing trainable parameters by 31.66%. Our code is publicly available at https://github.com/tianlwang/Magical.git.",
      "arxiv_url": "https://openreview.net/forum?id=mH9FJi3KTX",
      "pdf_url": "https://openreview.net/pdf/70031d6873ba95188e796e7d3fe0109262139dd5.pdf",
      "primary_category": "Medical Lay Language Generation, Large Language Model, LoRA",
      "categories": [
        "Medical Lay Language Generation",
        "Large Language Model",
        "LoRA",
        "PEFT"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Hmepi1Fm2g",
      "title": "zip2zip: Inference-Time Adaptive Tokenization via Online Compression",
      "authors": [
        "Saibo Geng",
        "Nathan Ranchin",
        "Yunzhen Yao",
        "Maxime Peyrard",
        "Chris Wendler",
        "Michael Gastpar",
        "Robert West"
      ],
      "abstract": "Tokenization efficiency plays a critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized on general-purpose corpora. These tokenizers’ fixed vocabularies often fail to adapt to domain- or language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, a novel method for achieving\ncontext-adaptive tokenization in LLMs at inference time. Leveraging an online data compression algorithm (Lempel–Ziv–Welch), zip2zip dynamically expands its active vocabulary at inference time by continuously replacing fragmented token sequences with more compact hypertokens, which it can immediately output during generation.  In doing so, the model refines its internal tokenization scheme to match\nthe token distribution of the current context, reducing redundancy and improving representational efficiency. zip2zip consists of three key components: (1) a tokenizer based on Lempel–Ziv–Welch compression that incrementally merges co-occurring tokens into reusable hypertokens on the fly; (2) a dynamic embedding (and unembedding) layer that computes embeddings for newly formed hypertokens at runtime; and (3) a variant of autoregressive language modeling that pretrains the model to handle hypertokenized, compressed text sequences as inputs and outputs. We show that an existing LLM can be uptrained for zip2zip in 10 GPU-hours via parameter-efficient finetuning. The resulting LLM performs test-time adaptation, learning to use hypertokens in unseen contexts and reducing input and output tokens by 15–40%.\nCode and models are released at https://github.com/epfl-dlab/zip2zip.",
      "arxiv_url": "https://openreview.net/forum?id=Hmepi1Fm2g",
      "pdf_url": "https://openreview.net/pdf/ed004e39a9e62f3d9362fc1117b0756ee480c81f.pdf",
      "primary_category": "tokenization, LLM, language modeling",
      "categories": [
        "tokenization",
        "LLM",
        "language modeling",
        "inference efficiency",
        "large language model",
        "compression"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zDU5sfYK1Z",
      "title": "Curriculum Design for Trajectory-Constrained Agent: Compressing Chain-of-Thought Tokens in LLMs",
      "authors": [
        "Georgios Tzannetos",
        "Parameswaran Kamalaruban",
        "Adish Singla"
      ],
      "abstract": "Training agents to operate under strict constraints during deployment, such as limited resource budgets or stringent safety requirements, presents significant challenges, especially when these constraints render the task complex. In this work, we propose a curriculum learning strategy that gradually tightens constraints during training, enabling the agent to incrementally master the deployment requirements. Inspired by self-paced learning techniques in unconstrained reinforcement learning (RL), our approach facilitates a smoother transition to challenging environments by initially training on simplified versions of the constraints and progressively introducing the full deployment conditions. We provide a theoretical analysis using an RL agent in a binary-tree Markov Decision Process (MDP) to demonstrate that our curriculum strategy can accelerate training relative to a baseline approach that imposes the trajectory constraints from the outset. Moreover, we empirically validate the effectiveness and generality of our method across both RL and large language model (LLM) agents in diverse settings, including a binary-tree MDP, a multi-task navigation domain, and a math reasoning task with two benchmarks. These results highlight the potential of curriculum design in enhancing the efficiency and performance of agents operating under complex trajectory constraints during deployment. Moreover, when applied to LLMs, our strategy enables compression of output chain-of-thought tokens, achieving a substantial inference speedup on consumer hardware, demonstrating its effectiveness for resource-constrained deployment.",
      "arxiv_url": "https://openreview.net/forum?id=zDU5sfYK1Z",
      "pdf_url": "https://openreview.net/pdf/3ef59a66c354632e4cb2789380511589795e676c.pdf",
      "primary_category": "Curriculum Design, Large Language Models, Reinforcement Learning",
      "categories": [
        "Curriculum Design",
        "Large Language Models",
        "Reinforcement Learning",
        "Chain-of-Thought Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "UdOEZgWJLc",
      "title": "Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought",
      "authors": [
        "Hanlin Zhu",
        "Shibo Hao",
        "Zhiting Hu",
        "Jiantao Jiao",
        "Stuart Russell",
        "Yuandong Tian"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in many applications, including challenging reasoning problems via chain-of-thought (CoT) techniques that generate ``thinking tokens'' before answering the questions. While existing theoretical works demonstrate that CoT with discrete tokens boosts the capability of LLMs, recent work on continuous CoT lacks a theoretical understanding of why it outperforms discrete counterparts in various reasoning tasks, such as directed graph reachability, a fundamental graph reasoning problem that includes many practical domain applications as special cases. In this paper, we prove that a two-layer transformer with $D$ steps of continuous CoT can solve the directed graph reachability problem, where $D$ is the diameter of the graph, while the best known result of constant-depth transformers with discrete CoT requires $O(n^2)$ decoding steps where $n$ is the number of vertices ($D<n$). \nIn our construction, each continuous thought vector is a superposition state that encodes multiple search frontiers simultaneously (i.e., parallel breadth-first search (BFS)), while discrete CoT must choose a single path sampled from the superposition state, which leads to a sequential search that requires many more steps and may be trapped in local solutions.\nWe also performed extensive experiments to verify that our theoretical construction aligns well with the empirical solution obtained via training dynamics. Notably, encoding of multiple search frontiers as a superposition state automatically emerges in training continuous CoT, without explicit supervision to guide the model to explore multiple paths simultaneously.",
      "arxiv_url": "https://openreview.net/forum?id=UdOEZgWJLc",
      "pdf_url": "https://openreview.net/pdf/f2255b21bb086d16fe50ddeead959f249d859135.pdf",
      "primary_category": "reasoning, chain of continuous thought, superposition",
      "categories": [
        "reasoning",
        "chain of continuous thought",
        "superposition",
        "transformer"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Quo3XadYcZ",
      "title": "Fine-grained List-wise Alignment for Generative Medication Recommendation",
      "authors": [
        "Chenxiao Fan",
        "Chongming Gao",
        "Wentao Shi",
        "Yaxin Gong",
        "Zhao Zihao",
        "Fuli Feng"
      ],
      "abstract": "Accurate and safe medication recommendations are critical for effective clinical decision-making, especially in multimorbidity cases. However, existing systems rely on point-wise prediction paradigms that overlook synergistic drug effects and potential adverse drug-drug interactions (DDIs).  We propose FLAME, a fine-grained list-wise alignment framework for large language models (LLMs), enabling drug-by-drug generation of drug lists.  FLAME formulates recommendation as a sequential decision process, where each step adds or removes a single drug.  To provide fine-grained learning signals, we devise step-wise Group Relative Policy Optimization (GRPO) with potential-based reward shaping, which explicitly models DDIs and optimizes the contribution of each drug to the overall prescription. Furthermore, FLAME enhances patient modeling by integrating structured clinical knowledge and collaborative information into the representation space of LLMs. Experiments on benchmark datasets demonstrate that FLAME achieves state-of-the-art performance, delivering superior accuracy, controllable safety–accuracy trade-offs, and strong generalization across diverse clinical scenarios. Our code is available at https://github.com/cxfann/Flame.",
      "arxiv_url": "https://openreview.net/forum?id=Quo3XadYcZ",
      "pdf_url": "https://openreview.net/pdf/46435b17e4d286935b6ff9d4bdae00460a2d3bc5.pdf",
      "primary_category": "Medication Recommendation, Large Language Models, Reinforcement Learning",
      "categories": [
        "Medication Recommendation",
        "Large Language Models",
        "Reinforcement Learning",
        "Electronic Health Record",
        "Reward Shaping"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "KYTFXxTJ12",
      "title": "Towards Fully FP8 GEMM LLM Training at Scale",
      "authors": [
        "Alejandro Hernández-Cano",
        "Dhia Garbaya",
        "Imanol Schlag",
        "Martin Jaggi"
      ],
      "abstract": "Despite the significant potential of FP8 data formats for large language model (LLM) pre-training, their adoption has been limited due to challenges in maintaining stability at scale. Existing approaches often rely on suboptimal fine-grained FP8 kernels or fall back to higher-precision matrix multiplications (GEMMs) in sensitive components, such as attention projections, compromising potential throughput gains.\nWe introduce a new class of LLM architectures that, for the first time, support FP8 computation for all GEMMs within transformer blocks during both forward and backward passes. This enables unprecedented throughput gains, particularly at scale, while matching the downstream performance of standard BF16 training. Our architecture design reduces large outlier activations, promoting stable long-term FP8 training. Additionally, we identify key metrics for monitoring low-precision training and predicting potential future divergences.",
      "arxiv_url": "https://openreview.net/forum?id=KYTFXxTJ12",
      "pdf_url": "https://openreview.net/pdf/d2ce27cb3c31933fde4f429ba5637ab55f006352.pdf",
      "primary_category": "FP8, LLMs, Training Dynamics",
      "categories": [
        "FP8",
        "LLMs",
        "Training Dynamics",
        "Quantization",
        "Efficient Training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "boNYskaXnO",
      "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
      "authors": [
        "Donghyun Son",
        "Euntae Choi",
        "Sungjoo Yoo"
      ],
      "abstract": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache.\nVector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets.\nTo address this limitation, we introduce $\\textbf{NSNQuant}$, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. \nBy applying a three-step transformation—$\\textbf{1)}$ a token-wise normalization ($\\textbf{N}$ormalize), $\\textbf{2)}$ a channel-wise centering ($\\textbf{S}$hift), and $\\textbf{3)}$ a second token-wise normalization ($\\textbf{N}$ormalize)—with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. \nThis alignment enables robust, calibration-free vector quantization using a single reusable codebook.\nExtensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gains over full-precision baselines.",
      "arxiv_url": "https://openreview.net/forum?id=boNYskaXnO",
      "pdf_url": "https://openreview.net/pdf/ee69cf99788d4c221668ea3edddbc059ac8203d3.pdf",
      "primary_category": "KV cache quantization, vector quantization, large language model",
      "categories": [
        "KV cache quantization",
        "vector quantization",
        "large language model",
        "inference optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MrRdupucYb",
      "title": "$Q\\sharp$: Provably Optimal Distributional RL for LLM Post-Training",
      "authors": [
        "Jin Peng Zhou",
        "Kaiwen Wang",
        "Jonathan Daniel Chang",
        "Zhaolin Gao",
        "Nathan Kallus",
        "Kilian Q Weinberger",
        "Kianté Brantley",
        "Wen Sun"
      ],
      "abstract": "Reinforcement learning (RL) post-training is crucial for LLM alignment and reasoning, but existing policy-based methods, such as PPO and DPO, can fall short of fixing shortcuts inherited from pre-training. In this work, we introduce $Q\\sharp$, a value-based algorithm for KL-regularized RL that guides the reference policy using the optimal regularized $Q$ function. We propose to learn the optimal $Q$ function using distributional RL on an aggregated online dataset. Unlike prior value-based baselines that guide the model using unregularized $Q$-values, our method is theoretically principled and provably learns the optimal policy for the KL-regularized RL problem. Empirically, $Q\\sharp$ outperforms prior baselines in math reasoning benchmarks while maintaining a smaller KL divergence to the reference policy. Theoretically, we establish a reduction from KL-regularized RL to no-regret online learning, providing the first bounds for deterministic MDPs under only realizability. Thanks to distributional RL, our bounds are also variance-dependent and converge faster when the reference policy has small variance. In sum, our results highlight $Q\\sharp$ as an effective approach for post-training LLMs, offering both improved performance and theoretical guarantees. The code can be found at \\url{https://github.com/jinpz/q_sharp}.",
      "arxiv_url": "https://openreview.net/forum?id=MrRdupucYb",
      "pdf_url": "https://openreview.net/pdf/9ae96ed54ea56d6acc7f6b19ef84c42be6897598.pdf",
      "primary_category": "large language models, distributional RL, reasoning",
      "categories": [
        "large language models",
        "distributional RL",
        "reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "SkdhLeuq8P",
      "title": "Ask a Strong LLM Judge when Your Reward Model is Uncertain",
      "authors": [
        "Zhenghao Xu",
        "Qin Lu",
        "Qingru Zhang",
        "Liang Qiu",
        "Ilgee Hong",
        "Changlong Yu",
        "Wenlin Yao",
        "Yao Liu",
        "Haoming Jiang",
        "Lihong Li",
        "Hyokun Yun",
        "Tuo Zhao"
      ],
      "abstract": "Reward model (RM) plays a pivotal role in reinforcement learning with human feedback (RLHF) for aligning large language models (LLMs). However, classical RMs trained on human preferences are vulnerable to reward hacking and generalize poorly to out-of-distribution (OOD) inputs. \nBy contrast, strong LLM judges equipped with reasoning capabilities demonstrate superior generalization, even without additional training, but incur significantly higher inference costs, limiting their applicability in online RLHF. \nIn this work, we propose an uncertainty-based routing framework that efficiently complements a fast RM with a strong but costly LLM judge. Our approach formulates advantage estimation in policy gradient (PG) methods as pairwise preference classification, enabling principled uncertainty quantification to guide routing. Uncertain pairs are forwarded to the LLM judge, while confident ones are evaluated by the RM. Experiments on RM benchmarks demonstrate that our uncertainty-based routing strategy significantly outperforms random judge calling at the same cost, and downstream alignment results showcase its effectiveness in improving online RLHF.",
      "arxiv_url": "https://openreview.net/forum?id=SkdhLeuq8P",
      "pdf_url": "https://openreview.net/pdf/32cbf67ca0e677b3f3fb9f298ed389d0875b2739.pdf",
      "primary_category": "Large Language Models, RLHF, Reward Models",
      "categories": [
        "Large Language Models",
        "RLHF",
        "Reward Models",
        "LLM-as-a-Judge"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Ql3sENn0mi",
      "title": "Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards",
      "authors": [
        "Charles Arnal",
        "Gaëtan Narozniak",
        "Vivien Cabannes",
        "Yunhao Tang",
        "Julia Kempe",
        "Remi Munos"
      ],
      "abstract": "Reinforcement learning (RL) is increasingly used to align large language models (LLMs). Off-policy methods offer greater implementation simplicity and data efficiency than on-policy techniques, but often result in suboptimal performance. In this work, we study the intermediate range of algorithms between off-policy RL and supervised fine-tuning by analyzing a simple off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with $r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$ emphasizes high-reward samples, while raising it penalizes low-reward ones more heavily.\nWe first provide a theoretical analysis of this off-policy REINFORCE algorithm, showing that when the baseline  $V$ lower-bounds the expected reward, the algorithm enjoys a policy improvement guarantee. Our analysis reveals that while on-policy updates can safely leverage both positive and negative signals, off-policy updates benefit from focusing more on positive rewards than on negative ones.\nWe validate our findings experimentally in a controlled stochastic bandit setting and through fine-tuning state-of-the-art LLMs on reasoning tasks.",
      "arxiv_url": "https://openreview.net/forum?id=Ql3sENn0mi",
      "pdf_url": "https://openreview.net/pdf/a7d8481340b8ce8407b1c770fd381139da46a2fe.pdf",
      "primary_category": "Reinforcement learning, off-policy RL, LLM finetuning",
      "categories": [
        "Reinforcement learning",
        "off-policy RL",
        "LLM finetuning",
        "bandits"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "x9vcgXmRD0",
      "title": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs",
      "authors": [
        "Mantas Mazeika",
        "Xuwang Yin",
        "Rishub Tamirisa",
        "Jaehyuk Lim",
        "Bruce W. Lee",
        "Richard Ren",
        "Long Phan",
        "Norman Mu",
        "Oliver Zhang",
        "Dan Hendrycks"
      ],
      "abstract": "As AIs rapidly advance and become more agentic, the risk they pose is governed not only by their capabilities but increasingly by their propensities, including goals and values. Tracking the emergence of goals and values has proven a longstanding problem, and despite much interest over the years it remains unclear whether current AIs have meaningful values. We propose a solution to this problem, leveraging the framework of utility functions to study the internal coherence of AI preferences. Surprisingly, we find that independently-sampled preferences in current LLMs exhibit high degrees of structural coherence, and moreover that this emerges with scale. These findings suggest that value systems emerge in LLMs in a meaningful sense, a finding with broad implications. To study these emergent value systems, we propose utility engineering as a research agenda, comprising both the analysis and control of AI utilities. We uncover problematic and often shocking values in LLM assistants despite existing control measures. These include cases where AIs value themselves over humans and are anti-aligned with specific individuals. To constrain these emergent value systems, we propose methods of utility control. As a case study, we show how aligning utilities with a citizen assembly reduces political biases and generalizes to new scenarios. Whether we like it or not, value systems have already emerged in AIs, and much work remains to fully understand and control these emergent representations.",
      "arxiv_url": "https://openreview.net/forum?id=x9vcgXmRD0",
      "pdf_url": "https://openreview.net/pdf/69c6825f4d5f92c80a3c8c21fdec6def776559c1.pdf",
      "primary_category": "AI safety, ML safety, machine ethics",
      "categories": [
        "AI safety",
        "ML safety",
        "machine ethics",
        "representation engineering",
        "RepE",
        "emergence",
        "values",
        "utility"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "t94tALZvZE",
      "title": "MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs",
      "authors": [
        "Ke Wang",
        "Yiming QIN",
        "Nikolaos Dimitriadis",
        "Alessandro Favero",
        "Pascal Frossard"
      ],
      "abstract": "Language models deployed in real-world systems often require post-hoc updates to incorporate new or corrected knowledge. However, editing such models efficiently and reliably—without retraining or forgetting previous information—remains a major challenge. Existing methods for lifelong model editing either compromise generalization, interfere with past edits, or fail to scale to long editing sequences. We propose MEMOIR, a novel scalable framework that injects knowledge through a residual memory, i.e., a dedicated parameter module, while preserving the core capabilities of the pre-trained model. By sparsifying input activations through data-dependent masks, MEMOIR confines each edit to a distinct subset of the memory parameters, minimizing interference among edits. At inference, it identifies relevant edits by comparing the sparse activation patterns of new queries to those stored during editing. This enables generalization to rephrased queries by activating only the relevant knowledge while suppressing unnecessary memory activation for unrelated prompts. Experiments on question answering, hallucination correction, and out-of-distribution generalization benchmarks  across LLaMA-3 and Mistral demonstrate that MEMOIR achieves state-of-the-art performance across reliability, generalization, and locality metrics, scaling to thousands of sequential edits with minimal forgetting.",
      "arxiv_url": "https://openreview.net/forum?id=t94tALZvZE",
      "pdf_url": "https://openreview.net/pdf/381a980bdabd7f4404f00a1b4cefc24ac9e8a579.pdf",
      "primary_category": "Lifelong model editing, Catastrophic forgetting, Large language model",
      "categories": [
        "Lifelong model editing",
        "Catastrophic forgetting",
        "Large language model",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZtB34bQI54",
      "title": "Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models",
      "authors": [
        "Taha Entesari",
        "Arman Hatami",
        "Rinat Khaziev",
        "Anil Ramakrishna",
        "Mahyar Fazlyab"
      ],
      "abstract": "Large Language Models (LLMs) deployed in real-world settings increasingly face the need to unlearn sensitive, outdated, or proprietary information. Existing unlearning methods typically formulate forgetting and retention as a regularized trade-off, combining both objectives into a single scalarized loss. This often leads to unstable optimization and degraded performance on retained data, especially under aggressive forgetting. We propose a new formulation of LLM unlearning as a constrained optimization problem: forgetting is enforced via a novel logit-margin flattening loss that explicitly drives the output distribution toward uniformity on a designated forget set, while retention is preserved through a hard constraint on a separate retain set. Compared to entropy-based objectives, our loss is softmax-free, numerically stable, and maintains non-vanishing gradients, enabling more efficient and robust optimization. \n    We solve the constrained problem using a scalable primal-dual algorithm that exposes the trade-off between forgetting and retention through the dynamics of the dual variable, all without any extra computational overhead.\n    Evaluations on the TOFU and MUSE benchmarks across diverse LLM architectures demonstrate that our approach consistently matches or exceeds state-of-the-art baselines, effectively removing targeted information while preserving downstream utility.",
      "arxiv_url": "https://openreview.net/forum?id=ZtB34bQI54",
      "pdf_url": "https://openreview.net/pdf/fc80efb401ef67af356346ca74128662a7de1f0b.pdf",
      "primary_category": "Machine Unlearning, Large Language Models, Constrained Optimization",
      "categories": [
        "Machine Unlearning",
        "Large Language Models",
        "Constrained Optimization",
        "Primal‑Dual Algorithms",
        "Entropy‑Based Objectives"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "X4SCxcgb3O",
      "title": "Communication-Efficient Language Model Training Scales Reliably and Robustly: Scaling Laws for DiLoCo",
      "authors": [
        "Zachary Charles",
        "Gabriel Teston",
        "Lucio M. Dery",
        "J Keith Rush",
        "Nova Fallen",
        "Zachary Garrett",
        "Arthur Szlam",
        "Arthur Douillard"
      ],
      "abstract": "As we scale to more massive machine learning models, the frequent synchronization demands inherent in data-parallel approaches create significant slowdowns, posing a critical challenge to further scaling. Recent work develops an approach (DiLoCo) that relaxes synchronization demands without compromising model quality. However, these works do not carefully analyze how DiLoCo's behavior changes with model size. In this work, we study the scaling law behavior of DiLoCo when training LLMs under a fixed compute budget. We focus on how algorithmic factors, including number of model replicas, hyperparameters, and token budget affect training in ways that can be accurately predicted via scaling laws. We find that DiLoCo scales both predictably and robustly with model size. When well-tuned, DiLoCo scales better than data-parallel training with model size, and can outperform data-parallel training even at small model sizes. Our results showcase a more general set of benefits of DiLoCo than previously documented, including increased optimal batch sizes, improved downstream generalization with scale, and improved evaluation loss for a fixed token budget.",
      "arxiv_url": "https://openreview.net/forum?id=X4SCxcgb3O",
      "pdf_url": "https://openreview.net/pdf/6c0133c70ac56661096c7b4c03807bee67175bcf.pdf",
      "primary_category": "distributed training, communication-efficiency, diloco",
      "categories": [
        "distributed training",
        "communication-efficiency",
        "diloco",
        "parallel training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "KMbl8lg5Rv",
      "title": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models",
      "authors": [
        "Haoyi Song",
        "Ruihan Ji",
        "Naichen Shi",
        "Fan Lai",
        "Raed Al Kontar"
      ],
      "abstract": "Large language models (LLMs) have transformed natural language processing, but their reliable deployment requires effective uncertainty quantification (UQ). Existing UQ methods are often heuristic and lack a fully probabilistic foundation. This paper begins by providing a theoretical justification for the role of perturbations in UQ for LLMs. We then introduce a dual random walk perspective, modeling input–output pairs as two Markov chains with transition probabilities defined by semantic similarity. Building on this, we propose a fully probabilistic framework based on an inverse model, which quantifies uncertainty by evaluating the diversity of the input space conditioned on a given output through systematic perturbations. Within this framework, we define a new uncertainty measure, Inv-Entropy. A key strength of our framework is its flexibility: it supports various definitions of uncertainty measures, embeddings, perturbation strategies, and similarity metrics. We also propose GAAP, a perturbation algorithm based on genetic algorithms, which enhances the diversity of sampled inputs. In addition, we introduce a new evaluation metric, Temperature Sensitivity of Uncertainty (TSU), which directly assesses uncertainty without relying on correctness as a proxy. Extensive experiments demonstrate that Inv-Entropy outperforms existing semantic UQ methods.",
      "arxiv_url": "https://openreview.net/forum?id=KMbl8lg5Rv",
      "pdf_url": "https://openreview.net/pdf/0733ff298f69ba6bc863b6eb9bdd9409f7e0c613.pdf",
      "primary_category": "Uncertainty Quantification, Probabilistic Modeling, Random Walks",
      "categories": [
        "Uncertainty Quantification",
        "Probabilistic Modeling",
        "Random Walks",
        "Perturbations",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "enhFXzKii4",
      "title": "Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation",
      "authors": [
        "Fei Wang",
        "Li Shen",
        "Liang Ding",
        "Chao Xue",
        "Ye Liu",
        "Changxing Ding"
      ],
      "abstract": "Large Language Models (LLMs) excel at natural language processing tasks, but their massive size leads to high computational and storage demands.\nRecent works have sought to reduce their model size through layer-wise structured pruning.\nHowever, they tend to ignore retaining the capabilities in the pruned part. \nIn this work, we re-examine structured pruning paradigms and uncover several key limitations: 1) notable performance degradation due to direct layer removal, 2) incompetent linear weighted layer aggregation, and 3) the lack of effective post-training recovery mechanisms.\nTo address these limitations, we propose CoMe, including a progressive layer pruning framework with a Concatenation-based Merging technology and a hierarchical distillation post-training process. \nSpecifically, we introduce a channel sensitivity metric that utilizes activation intensity and weight norms for fine-grained channel selection. \nSubsequently, we employ a concatenation-based layer merging method to fuse the most critical channels in the adjacent layers, enabling a progressive model size reduction. \nFinally, we propose a hierarchical distillation protocol, which leverages the correspondences between the original and pruned model layers established during pruning, enabling efficient knowledge transfer.\nExperiments on seven benchmarks show that CoMe achieves state-of-the-art performance; when pruning 30% of LLaMA-2-7b's parameters, the pruned model retains 83% of its original average accuracy.",
      "arxiv_url": "https://openreview.net/forum?id=enhFXzKii4",
      "pdf_url": "https://openreview.net/pdf/8c4f60e9a0701f8dfa214b8f60d1fb11b0d47f4e.pdf",
      "primary_category": "Structured Pruning; Post Training",
      "categories": [
        "Structured Pruning; Post Training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BmEH70Wjcu",
      "title": "Do LLMs Really Forget? Evaluating Unlearning  with Knowledge Correlation and Confidence Awareness",
      "authors": [
        "Rongzhe Wei",
        "Peizhi Niu",
        "Hans Hao-Hsun Hsu",
        "Ruihan Wu",
        "Haoteng Yin",
        "Mohsen Ghassemi",
        "Yifan Li",
        "Vamsi K. Potluru",
        "Eli Chien",
        "Kamalika Chaudhuri",
        "Olgica Milenkovic",
        "Pan Li"
      ],
      "abstract": "Machine unlearning techniques aim to mitigate unintended memorization in large language models (LLMs). However, existing approaches predominantly focus on the explicit removal of isolated facts, often overlooking latent inferential dependencies and the non-deterministic nature of knowledge within LLMs. Consequently, facts presumed forgotten may persist implicitly through correlated information. To address these challenges, we propose a knowledge unlearning evaluation framework that more accurately captures the implicit structure of real-world knowledge by representing relevant factual contexts as knowledge graphs with associated confidence scores. We further develop an inference-based evaluation protocol leveraging powerful LLMs as judges; these judges reason over the extracted knowledge subgraph to determine unlearning success. Our LLM judges utilize carefully designed prompts and are calibrated against human evaluations to ensure their trustworthiness and stability. Extensive experiments on our newly constructed benchmark demonstrate that our framework provides a more realistic and rigorous assessment of unlearning performance. Moreover, our findings reveal that current evaluation strategies tend to overestimate unlearning effectiveness.",
      "arxiv_url": "https://openreview.net/forum?id=BmEH70Wjcu",
      "pdf_url": "https://openreview.net/pdf/517d4d7f2f3b0cfa40404c3f972d83f3dbeaf4ff.pdf",
      "primary_category": "Machine Unlearning, Knowledge Unlearning, Knowledge Correlation",
      "categories": [
        "Machine Unlearning",
        "Knowledge Unlearning",
        "Knowledge Correlation",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1BOiVpBtZy",
      "title": "HCRMP: An LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving",
      "authors": [
        "Zhiwen Chen",
        "Hanming Deng",
        "Zhuoren Li",
        "Huanxi Wen",
        "Guizhe Jin",
        "Ran Yu",
        "Bo Leng"
      ],
      "abstract": "Integrating the understanding and reasoning capabilities of Large Language Models (LLM) with the self-learning capabilities of Reinforcement Learning (RL) enables more reliable driving performance under complex driving conditions. There has been a lot of work exploring LLM-Dominated RL methods in the field of autonomous driving motion planning. These methods, which utilize LLM to directly generate policies or provide decisive instructions during policy learning of RL agent, are centrally characterized by an over-reliance on LLM outputs. However, LLM outputs are susceptible to hallucinations. Evaluations show that state-of-the-art LLM indicates a non-hallucination rate of only approximately 57.95\\% when assessed on essential driving-related tasks. Thus, in these methods, hallucinations from the LLM can directly jeopardize the performance of driving policies. This paper argues that maintaining relative independence between the LLM and the RL is vital for solving the hallucinations problem. Consequently, this paper is devoted to propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic hints for state augmentation and policy optimization to assist RL agent in motion planning, while the RL agent counteracts potential erroneous semantic indications through policy learning to achieve excellent driving performance. Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual Reinforcement Learning Motion Planner) architecture, which is designed that includes  ①Augmented Semantic Representation Module to extend state space. ②Contextual Stability Anchor Module enhances the reliability of multi-critic weight hints by utilizing information from the knowledge base. ③Semantic Cache Module is employed to seamlessly integrate LLM low-frequency guidance with RL high-frequency control. Extensive experiments in CARLA validate HCRMP's strong overall driving performance. HCRMP achieves a task success rate of up to 80.3\\% under diverse driving conditions with different traffic densities. Under safety-critical driving conditions, HCRMP significantly reduces the collision rate by 11.4\\%, which effectively improves the driving performance in complex scenarios.",
      "arxiv_url": "https://openreview.net/forum?id=1BOiVpBtZy",
      "pdf_url": "https://openreview.net/pdf/49844a0d2f1eb28baefb84a8e753b135fa3a93ae.pdf",
      "primary_category": "Autonomous Driving，Motion Planning",
      "categories": [
        "Autonomous Driving，Motion Planning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "J4w4RtwLyB",
      "title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models",
      "authors": [
        "Haoyu Peter Wang",
        "Peihao Wang",
        "Mufei Li",
        "Shikun Liu",
        "Siqi Miao",
        "Zhangyang Wang",
        "Pan Li"
      ],
      "abstract": "Modern large language models (LLMs) are inherently auto-regressive, requiring input to be serialized into flat sequences regardless of their structural dependencies. This serialization hinders the model’s ability to leverage structural inductive biases, especially in tasks such as retrieval-augmented generation (RAG) and reasoning on data with native graph structures, where inter-segment dependencies are crucial. \nWe introduce Graph-KV with the potential to overcome this limitation. Graph-KV leverages the KV-cache of text segments as condensed representations and governs their interaction through structural inductive biases. In this framework, ''target'' segments selectively attend only to the KV-caches of their designated ''source'' segments, rather than all preceding segments in a serialized sequence. This approach induces a graph-structured block mask, sparsifying attention and enabling a message-passing-like step within the LLM. Furthermore, strategically allocated positional encodings for source and target segments reduce positional bias and context window consumption. \nWe evaluate Graph-KV across three scenarios: (1) seven RAG benchmarks spanning direct inference, multi-hop reasoning, and long-document understanding; (2) Arxiv-QA, a novel academic paper QA task with full-text scientific papers structured as citation ego-graphs; and (3) paper topic classification within a citation network.\nBy effectively reducing positional bias and harnessing structural inductive biases, Graph-KV substantially outperforms baselines, including standard costly sequential encoding, across various settings.",
      "arxiv_url": "https://openreview.net/forum?id=J4w4RtwLyB",
      "pdf_url": "https://openreview.net/pdf/ad782d6eade7cc1f82de7c69fcc3736c07be2ac3.pdf",
      "primary_category": "Large Language Model, Graph, Retrieval-augmented Generation",
      "categories": [
        "Large Language Model",
        "Graph",
        "Retrieval-augmented Generation",
        "Structural Inductive Biases"
      ],
      "tags": [
        "LLM",
        "RAG",
        "Multi-Modal RAG",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Tp6ds3Dfqo",
      "title": "Rope to Nope and Back Again: A New Hybrid Attention Strategy",
      "authors": [
        "Bowen Yang",
        "Bharat Venkitesh",
        "Dwaraknath Gnaneshwar",
        "Hangyu Lin",
        "David Cairuz",
        "Phil Blunsom",
        "Acyr Locatelli"
      ],
      "abstract": "Long-context large language models (LLMs) have achieved remarkable advancements, driven by techniques like Rotary Position Embedding (RoPE) (Su et al., 2023) and its extensions (Chen et al., 2023; Liu et al., 2024c; Peng et al., 2023). By adjusting RoPE parameters and incorporating training data with extended contexts, we can train performant models with considerably longer input sequences. However, existing RoPE-based methods exhibit performance limitations when applied to extended context lengths. This paper presents a comprehensive analysis of various attention mechanisms, including RoPE, No Positional Embedding (NoPE), and Query-Key Normalization (QK-Norm), identifying their strengths and shortcomings in long-context modeling. Our investigation identifies distinctive attention patterns in these methods and highlights their impact on long-context performance, providing valuable insights for architectural design.  on long context performance, providing valuable insights for architectural design. Building on these findings, we propose a novel architecture featuring a hybrid attention mechanism that integrates global and local attention spans. This design not only surpasses conventional RoPE-based transformer models with full attention in both long and short context tasks but also delivers substantial efficiency gains during training and inference.",
      "arxiv_url": "https://openreview.net/forum?id=Tp6ds3Dfqo",
      "pdf_url": "https://openreview.net/pdf/5c1ebab2784e92dd40dab0a6913fa55826c699c5.pdf",
      "primary_category": "LLM, Pretraining, Long Context",
      "categories": [
        "LLM",
        "Pretraining",
        "Long Context",
        "Hybrid Attention"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2Gnp8sdwVe",
      "title": "Predictable Scale (Part II) --- Farseer: A Refined Scaling Law in LLMs",
      "authors": [
        "Houyi Li",
        "Wenzhen Zheng",
        "Qiufeng Wang",
        "Zhenyu Ding",
        "Haoying Wang",
        "Zili Wang",
        "Shijie Xuyang",
        "Ning Ding",
        "Shuigeng Zhou",
        "Xiangyu Zhang",
        "Daxin Jiang"
      ],
      "abstract": "Training Large Language Models (LLMs) is prohibitively expensive, creating a critical scaling gap where insights from small-scale experiments often fail to transfer to resource-intensive production systems, thereby hindering efficient innovation.\nTo bridge this, we introduce Farseer, a novel and refined scaling law offering enhanced predictive accuracy across scales. By systematically constructing a model loss surface $L(N,D)$, Farseer achieves a significantly better fit to empirical data than prior laws (e.g., \\Chinchilla's law). \nOur methodology yields accurate, robust, and highly generalizable predictions, demonstrating excellent extrapolation capabilities, outperforming Chinchilla's law, whose extrapolation error is 433\\% higher. \nThis allows for the reliable evaluation of competing training strategies across all $(N,D)$ settings, enabling conclusions from small-scale ablation studies to be confidently extrapolated to predict large-scale performance. \nFurthermore, Farseer provides new insights into optimal compute allocation, better reflecting the nuanced demands of modern LLM training. To validate our approach, we trained an extensive suite of approximately 1,000 LLMs across diverse scales and configurations, consuming roughly 3 million NVIDIA H100 GPU hours. To foster further research, we are comprehensively open-sourcing all code, data, results (https://github.com/Farseer-Scaling-Law/Farseer), all training logs (https://wandb.ai/billzid/Farseer?nw=nwuserbillzid), all models used in scaling law fitting (https://huggingface.co/Farseer-Scaling-Law).",
      "arxiv_url": "https://openreview.net/forum?id=2Gnp8sdwVe",
      "pdf_url": "https://openreview.net/pdf/b2d2dae1fc87cdd4510e8c2672fcf585d1c653f2.pdf",
      "primary_category": "Large Language Model, Scaling Law",
      "categories": [
        "Large Language Model",
        "Scaling Law"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nzwjvpCO4F",
      "title": "InstructFlow: Adaptive Symbolic Constraint-Guided Code Generation for Long-Horizon Planning",
      "authors": [
        "Haotian Chi",
        "Zeyu Feng",
        "Yueming Lyu",
        "Chengqi Zheng",
        "Linbo Luo",
        "Yew-Soon Ong",
        "Ivor Tsang",
        "Hechang Chen",
        "Yi Chang",
        "Haiyan Yin"
      ],
      "abstract": "Long-horizon planning in robotic manipulation tasks requires translating underspecified, symbolic goals into executable control programs satisfying spatial, temporal, and physical constraints. However, language model-based planners often struggle with long-horizon task decomposition, robust constraint satisfaction, and adaptive failure recovery. We introduce InstructFlow, a multi-agent framework that establishes a symbolic, feedback-driven flow of information for code generation in robotic manipulation tasks. InstructFlow employs a InstructFlow Planner to construct and traverse a hierarchical instruction graph that decomposes goals into semantically meaningful subtasks, while a Code Generator generates executable code snippets conditioned on this graph. Crucially, when execution failures occur, a Constraint Generator analyzes feedback and induces symbolic constraints, which are propagated back into the instruction graph to guide targeted code refinement without regenerating from scratch. This dynamic, graph-guided flow enables structured, interpretable, and failure-resilient planning, significantly improving task success rates and robustness across diverse manipulation benchmarks, especially in constraint-sensitive and long-horizon scenarios.",
      "arxiv_url": "https://openreview.net/forum?id=nzwjvpCO4F",
      "pdf_url": "https://openreview.net/pdf/c5467bebb74929ca8fa1c35d3094518a267fcc9c.pdf",
      "primary_category": "Code generation; Task planning; Large language models",
      "categories": [
        "Code generation; Task planning; Large language models"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9Ook5bXnPr",
      "title": "TrajAgent: An LLM-Agent Framework for Trajectory Modeling via Large-and-Small Model Collaboration",
      "authors": [
        "Yuwei Du",
        "Jie Feng",
        "Jie Zhao",
        "Yong Li"
      ],
      "abstract": "Trajectory modeling, which includes research on trajectory data pattern mining and future prediction, has widespread applications in areas such as life services, urban transportation, and public administration. Numerous methods have been proposed to address specific problems within trajectory modeling. However, the heterogeneity of data and the diversity of trajectory tasks make effective and reliable trajectory modeling an important yet highly challenging endeavor, even for domain experts. In this paper, we propose TrajAgent, a agent framework powered by large language models (LLMs), designed to facilitate robust and efficient trajectory modeling through automation modeling. This framework leverages and optimizes diverse specialized models to address various trajectory modeling tasks across different datasets effectively. In TrajAgent, we first develop UniEnv, an execution environment with a unified data and model interface, to support the execution and training of various models. Building on UniEnv, we introduce an agentic workflow designed for automatic trajectory modeling across various trajectory tasks and data. Furthermore, we introduce collaborative learning schema between LLM-based agents and small speciallized models, to enhance the performance of the whole framework effectively. Extensive experiments on four tasks using four real-world datasets demonstrate the effectiveness of TrajAgent in automated trajectory modeling, achieving a performance improvement of 2.38%-69.91% over baseline methods. The codes and data can be accessed via https://github.com/tsinghua-fib-lab/TrajAgent.",
      "arxiv_url": "https://openreview.net/forum?id=9Ook5bXnPr",
      "pdf_url": "https://openreview.net/pdf/907c1941fbc6aba93b6f6858e39bc94e8fc84301.pdf",
      "primary_category": "LLM-based Agent, Trajectory Data Mining, Collaboration Learning",
      "categories": [
        "LLM-based Agent",
        "Trajectory Data Mining",
        "Collaboration Learning"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ktNJgpmjjP",
      "title": "AutoJudge: Judge Decoding Without Manual Annotation",
      "authors": [
        "Roman Garipov",
        "Fedor Velikonivtsev",
        "Ivan Ermakov",
        "Ruslan Svirschevski",
        "Vage Egiazarian",
        "Max Ryabinin"
      ],
      "abstract": "We introduce AutoJudge, a method that accelerates large language model (LLM) inference with task-specific lossy speculative decoding. \nInstead of matching the original model output distribution token-by-token, we identify the generated tokens that affect the downstream quality of the response, relaxing the distribution match guarantee so that the \"unimportant\" tokens can be generated faster.\nOur approach relies on a semi‑greedy search algorithm to test which of the mismatches between target and draft models should be corrected to preserve quality and which ones may be skipped.\nWe then train a lightweight classifier based on existing LLM embeddings to predict, at inference time, which mismatching tokens can be safely accepted without compromising the final answer quality.\nWe evaluate AutoJudge with multiple draft/target model pairs on mathematical reasoning and programming benchmarks, achieving significant speedups at the cost of a minor accuracy reduction. \nNotably, on GSM8K with the Llama 3.1 70B target model, our approach achieves up to ${\\approx}2{\\times}$ speedup \\textit{over speculative decoding} at the cost of a ${\\le} 1\\%$ drop in accuracy.\nWhen applied to the LiveCodeBench benchmark, AutoJudge automatically detects programming-specific important tokens, accepting ${\\ge}25$ tokens per speculation cycle at a$~ 2\\%$ drop in Pass@1. Our approach requires no human annotation and is easy to integrate with modern LLM inference frameworks.",
      "arxiv_url": "https://openreview.net/forum?id=ktNJgpmjjP",
      "pdf_url": "https://openreview.net/pdf/01e2b80935f1840d1aa04ae6b8562335a100fa5b.pdf",
      "primary_category": "LLM, Speculative Decoding, Lossy Speculative Decoding",
      "categories": [
        "LLM",
        "Speculative Decoding",
        "Lossy Speculative Decoding",
        "Inference"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "z1Cvcovlms",
      "title": "Sim-LLM: Optimizing LLM Inference at the Edge through Inter-Task KV Reuse",
      "authors": [
        "Ruikun Luo",
        "Changwei Gu",
        "Qiang He",
        "Feifei Chen",
        "Song Wu",
        "Hai Jin",
        "Yun Yang"
      ],
      "abstract": "KV cache technology, by storing key-value pairs, helps reduce the computational overhead incurred by *large language models* (LLMs). It facilitates their deployment on resource-constrained edge computing nodes like edge servers. However, as the complexity and size of tasks increase, KV cache usage leads to substantial GPU memory consumption. Existing research has focused on mitigating KV cache memory usage through sequence length reduction, task-specific compression, and dynamic eviction policies. However, these methods are computationally expensive for resource-constrained edge computing nodes. To tackle this challenge, this paper presents Sim-LLM, a novel inference optimization mechanism that leverages task similarity to reduce KV cache memory consumption for LLMs. By caching KVs from processed tasks and reusing them for subsequent similar tasks during inference, Sim-LLM significantly reduces memory consumption while boosting system throughput and increasing maximum batch size, all with minimal accuracy degradation. Evaluated on both A40 and A100 GPUs, Sim-LLM achieves a system throughput improvement of up to 39.40\\% and a memory reduction of up to 34.65%, compared to state-of-the-art approaches. Our source code is available at https://github.com/CGCL-codes/SimLLM.",
      "arxiv_url": "https://openreview.net/forum?id=z1Cvcovlms",
      "pdf_url": "https://openreview.net/pdf/e8355810346fa0731f27685d0e9743b63680e658.pdf",
      "primary_category": "LLMs, KV cache, Task Similarity",
      "categories": [
        "LLMs",
        "KV cache",
        "Task Similarity",
        "Edge Computing"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3FBByWp6GL",
      "title": "Learning to Specialize: Joint Gating-Expert Training for Adaptive MoEs in Decentralized Settings",
      "authors": [
        "Yehya Farhat",
        "Hamza ElMokhtar Shili",
        "Fangshuo Liao",
        "Chen Dun",
        "Mirian Del Carmen Hipolito Garcia",
        "Guoqing Zheng",
        "Ahmed Hassan Awadallah",
        "Robert Sim",
        "Dimitrios Dimitriadis",
        "Anastasios Kyrillidis"
      ],
      "abstract": "Mixture-of-Experts (MoEs) achieve scalability by dynamically activating subsets of their components.\nYet, understanding how expertise emerges through joint training of gating mechanisms and experts remains incomplete, especially in scenarios without clear task partitions. Motivated by inference costs and data heterogeneity, we study how joint training of gating functions and experts can dynamically allocate domain-specific expertise across multiple underlying data distributions.\nAs an outcome of our framework, we develop an instance tailored specifically to decentralized training scenarios, introducing *Dynamically Decentralized Orchestration of MoEs* or *DDOME*. *DDOME* leverages heterogeneity emerging from distributional shifts across decentralized data sources to specialize experts dynamically. By integrating a pretrained common expert to inform a gating function, *DDOME* achieves personalized expert subset selection on-the-fly, facilitating just-in-time personalization. \nWe empirically validate *DDOME* within a Federated Learning (FL) context: *DDOME* attains from 4\\% up to an 24\\% accuracy improvement over state-of-the-art FL baselines in image and text classification tasks, while maintaining competitive zero-shot generalization capabilities. Furthermore, we provide theoretical insights confirming that the joint gating-experts training is critical for achieving meaningful expert specialization.",
      "arxiv_url": "https://openreview.net/forum?id=3FBByWp6GL",
      "pdf_url": "https://openreview.net/pdf/51ee83a706e3bbb24b67e41b2ba7f48c757bbaa1.pdf",
      "primary_category": "Mixture-of-Expert, Decentralized Training",
      "categories": [
        "Mixture-of-Expert",
        "Decentralized Training"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3VvdoCcVPU",
      "title": "Token-Level Self-Play with Importance-Aware Guidance for Large Language Models",
      "authors": [
        "Tue Le",
        "Hoang Tran Vuong",
        "Quyen Tran",
        "Linh Ngo Van",
        "Mehrtash Harandi",
        "Trung Le"
      ],
      "abstract": "Leveraging the power of Large Language Models (LLMs) through preference optimization is crucial for aligning model outputs with human values. Direct Preference Optimization (DPO) has recently emerged as a simple yet effective method by directly optimizing on preference data without the need for explicit reward models. However, DPO typically relies on human-labeled preference data, which can limit its scalability. Self-Play Fine-Tuning (SPIN) addresses this by allowing models to generate their own rejected samples, reducing the dependence on human annotations. Nevertheless, SPIN uniformly applies learning signals across all tokens, ignoring the fine-grained quality variations within responses. As the model improves, rejected samples increasingly contain high-quality tokens, making the uniform treatment of tokens suboptimal. In this paper, we propose SWIFT (Self-Play Weighted Fine-Tuning), a fine-grained self-refinement method that assigns token-level importance weights estimated from a stronger teacher model. Beyond alignment, we also demonstrate that SWIFT serves as an effective knowledge distillation strategy by using the teacher not for logits matching, but for reward-guided token weighting. Extensive experiments on diverse benchmarks and settings demonstrate that SWIFT consistently surpasses both existing alignment approaches and conventional knowledge distillation methods.",
      "arxiv_url": "https://openreview.net/forum?id=3VvdoCcVPU",
      "pdf_url": "https://openreview.net/pdf/5591739a79d255024eb42902320b9cab3a38d803.pdf",
      "primary_category": "Large Language Models, Importance Sampling, Preference Learning",
      "categories": [
        "Large Language Models",
        "Importance Sampling",
        "Preference Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nfhmjdZUbQ",
      "title": "Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees",
      "authors": [
        "Sangwoo Park",
        "Matteo Zecchin",
        "Osvaldo Simeone"
      ],
      "abstract": "Selecting  artificial intelligence (AI) models, such as large language models (LLMs), from multiple candidates requires accurate performance estimation. This is ideally achieved through empirical evaluations involving abundant real-world data. However, such evaluations are costly and impractical at scale. To address this challenge, autoevaluation methods leverage synthetic data produced by automated evaluators, such as LLMs-as-judges, reducing variance but potentially introducing bias. Recent approaches have employed semi-supervised prediction-powered inference ($\\texttt{PPI}$) to correct for the bias of autoevaluators. However, the use of autoevaluators may lead in practice to a degradation in sample efficiency compared to conventional  methods using only real-world data. In this paper, we propose $\\texttt{R-AutoEval+}$, a novel framework that provides finite-sample reliability guarantees on the model evaluation, while also ensuring an enhanced (or at least no worse) sample efficiency  compared to conventional methods. The key innovation of $\\texttt{R-AutoEval+}$ is an  adaptive construction of the model evaluation variable, which dynamically tunes its reliance on synthetic data, reverting to conventional methods when the autoevaluator is insufficiently accurate. Experiments on the use of LLMs-as-judges for the optimization of quantization settings for the weights of an LLM,  for prompt design in LLMs, and for test-time reasoning budget allocation in LLMs confirm the reliability and efficiency of $\\texttt{R-AutoEval+}$.",
      "arxiv_url": "https://openreview.net/forum?id=nfhmjdZUbQ",
      "pdf_url": "https://openreview.net/pdf/42cf5e1bbaeccd15c151a00a71cb2d9ecef2aa6f.pdf",
      "primary_category": "AutoEval, LLM judge, game-theoretic statistics",
      "categories": [
        "AutoEval",
        "LLM judge",
        "game-theoretic statistics",
        "hypothesis testing",
        "semi-supervised inference",
        "prediction-powered inference"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "W9Y0jtf45v",
      "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling",
      "authors": [
        "Yang Xiao",
        "Jessie Wang",
        "Ruifeng Yuan",
        "Chunpu Xu",
        "Kaishuai Xu",
        "Wenjie Li",
        "Pengfei Liu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning capabilities through test-time scaling approaches, particularly when fine-tuned with chain-of-thought (CoT) data distilled from more powerful large reasoning models (LRMs). However, these reasoning chains often contain verbose elements that mirror human problem-solving, categorized as progressive reasoning (the essential solution development path) and functional elements (verification processes, alternative solution approaches, and error corrections). While progressive reasoning is crucial, the functional elements significantly increase computational demands during test-time inference. We introduce PIR (Perplexity-based Importance Refinement), a principled framework that quantitatively evaluates the importance of each reasoning step based on its impact on answer prediction confidence. PIR systematically identifies and selectively prunes only low-importance functional steps while preserving all progressive reasoning components, creating optimized training data that maintains the integrity of the core solution path while reducing verbosity. Models fine-tuned on PIR-optimized data exhibit superior test-time scaling properties, generating more concise reasoning chains while achieving improved accuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to -41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond). Our approach demonstrates strong generalizability across different model sizes, data sources, and token budgets, offering a practical solution for deploying reasoning-capable LLMs in scenarios where efficient test-time scaling, response time, and computational efficiency are valuable constraints. Code and dataset are available at the [LIMOPro GitHub repository.](https://github.com/GAIR-NLP/LIMOPro)",
      "arxiv_url": "https://openreview.net/forum?id=W9Y0jtf45v",
      "pdf_url": "https://openreview.net/pdf/0c3ecaba9b2c2e94d46866f4f888a2d4ba4787dd.pdf",
      "primary_category": "LLM, reasoning, Test-time Scaling",
      "categories": [
        "LLM",
        "reasoning",
        "Test-time Scaling"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "R24ZqNwoDz",
      "title": "DynaAct: Large Language Model Reasoning with Dynamic Action Spaces",
      "authors": [
        "Xueliang Zhao",
        "Wei Wu",
        "Jian Guan",
        "Qintong Li",
        "Lingpeng Kong"
      ],
      "abstract": "In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named \\textsc{DynaAct} for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. \nExtensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at \\url{https://github.com/zhaoxlpku/DynaAct}.",
      "arxiv_url": "https://openreview.net/forum?id=R24ZqNwoDz",
      "pdf_url": "https://openreview.net/pdf/45820087f0186edb228a8b251bb2af4a3cca38b5.pdf",
      "primary_category": "Sequential Decision-Making, Action Space Optimization, Submodular Function",
      "categories": [
        "Sequential Decision-Making",
        "Action Space Optimization",
        "Submodular Function"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ly5DnRIgCZ",
      "title": "QiMeng-CodeV-R1: Reasoning-Enhanced Verilog Generation",
      "authors": [
        "Yaoyu Zhu",
        "Di Huang",
        "Hanqi Lyu",
        "Xiaoyun Zhang",
        "Chongxiao Li",
        "Wenxuan Shi",
        "Yutong Wu",
        "Jianan Mu",
        "Jinghua Wang",
        "Yang zhao",
        "Pengwei Jin",
        "Shuyao Cheng",
        "shengwen Liang",
        "Xishan Zhang",
        "Rui Zhang",
        "Zidong Du",
        "Qi Guo",
        "Xing Hu",
        "Yunji Chen"
      ],
      "abstract": "Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code–NL–code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6 \\% and 72.9 \\% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12$\\sim$20 \\%, while even exceeding the performance of 671B DeepSeek-R1 on RTLLM. We have released our model, training code, and dataset to facilitate research in EDA and LLM communities.",
      "arxiv_url": "https://openreview.net/forum?id=ly5DnRIgCZ",
      "pdf_url": "https://openreview.net/pdf/09c10d014999eae477f919d98b0b392a60db1ab1.pdf",
      "primary_category": "LLM, Code Generation, Verilog",
      "categories": [
        "LLM",
        "Code Generation",
        "Verilog"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "NuCtKoflsV",
      "title": "DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation",
      "authors": [
        "Jiashuo Sun",
        "Xianrui Zhong",
        "Sizhe Zhou",
        "Jiawei Han"
      ],
      "abstract": "Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker, which refines retrieved documents to enhance generation quality and explainability. The challenge of selecting the optimal number of documents (k) remains unsolved: too few may omit critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results.",
      "arxiv_url": "https://openreview.net/forum?id=NuCtKoflsV",
      "pdf_url": "https://openreview.net/pdf/9d193d0269cd2ca3f7a66de1368f8a0e3fdd3dcc.pdf",
      "primary_category": "Retrieval-Augmented Generation, Large Language Models, Dynamic Reranking",
      "categories": [
        "Retrieval-Augmented Generation",
        "Large Language Models",
        "Dynamic Reranking",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM",
        "RAG",
        "Information Retrieval"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "NligLHO7yG",
      "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization",
      "authors": [
        "Zhijian Zhuo",
        "Yutao Zeng",
        "Ya Wang",
        "Sijun Zhang",
        "Xiaoqing Li",
        "Jian Yang",
        "zhou Xun",
        "Jinwen Ma"
      ],
      "abstract": "Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, many challenges remain in training deep transformer networks, especially regarding the position of the layer normalization. While Pre-Norm structures facilitate more stable training owing to their stronger identity path, they often lead to suboptimal performance compared to Post-Norm.  In this paper, we propose **HybridNorm**, a simple yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. We provide both theoretical insights and empirical evidence to demonstrate that HybridNorm improves the gradient flow and the model robustness. Extensive experiments on large-scale transformer models, including both dense and sparse variants, show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches across multiple benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. Code is available at https://github.com/BryceZhuo/HybridNorm.",
      "arxiv_url": "https://openreview.net/forum?id=NligLHO7yG",
      "pdf_url": "https://openreview.net/pdf/1568823b53afd29707202c88a83bc13bce1b4f8b.pdf",
      "primary_category": "transformer, pre-training, large language models",
      "categories": [
        "transformer",
        "pre-training",
        "large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kMIhNcIZGb",
      "title": "Evaluating LLM-contaminated Crowdsourcing Data Without Ground Truth",
      "authors": [
        "Yichi Zhang",
        "Jinlong Pang",
        "Zhaowei Zhu",
        "Yang Liu"
      ],
      "abstract": "The recent success of generative AI highlights the crucial role of high-quality human feedback in building trustworthy AI systems. However, the increasing use of large language models (LLMs) by crowdsourcing workers poses a significant challenge: datasets intended to reflect human input may be compromised by LLM-generated responses. Existing LLM detection approaches often rely on high-dimensional training data such as text, making them unsuitable for structured annotation tasks like multiple-choice labeling. In this work, we investigate the potential of peer prediction --- a mechanism that evaluates the information within workers' responses --- to mitigate LLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our method quantifies the correlations between worker answers while conditioning on (a subset of) LLM-generated labels available to the requester. Building on prior research, we propose a training-free scoring mechanism with theoretical guarantees under a novel model that accounts for LLM collusion. We establish conditions under which our method is effective and empirically demonstrate its robustness in detecting low-effort cheating on real-world crowdsourcing datasets.",
      "arxiv_url": "https://openreview.net/forum?id=kMIhNcIZGb",
      "pdf_url": "https://openreview.net/pdf/54e2b9855000f9a4d8bcfacc477e522243e5c669.pdf",
      "primary_category": "LLM-contamination, crowdsourcing, peer prediction",
      "categories": [
        "LLM-contamination",
        "crowdsourcing",
        "peer prediction",
        "data evaluation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "uX4dyc7Z5Z",
      "title": "Group-Level Data Selection for Efficient Pretraining",
      "authors": [
        "Zichun Yu",
        "Fei Peng",
        "Jie Lei",
        "Arnold Overwijk",
        "Wen-tau Yih",
        "Chenyan Xiong"
      ],
      "abstract": "The efficiency and quality of language model pretraining are largely determined by the way pretraining data are selected. In this paper, we introduce *Group-MATES*, an efficient group-level data selection approach to optimize the speed-quality frontier of language model pretraining. Specifically, Group-MATES parameterizes costly group-level selection with a relational data influence model. To train this model, we sample training trajectories of the language model and collect oracle data influences alongside. The relational data influence model approximates the oracle data influence by weighting individual influence with relationships among training data. To enable efficient selection with our relational data influence model, we partition the dataset into small clusters using relationship weights and select data within each cluster independently. Experiments on DCLM 400M-4x, 1B-1x, and 3B-1x show that Group-MATES achieves 3.5\\%-9.4\\% relative performance gains over random selection across 22 downstream tasks, nearly doubling the improvements achieved by state-of-the-art individual data selection baselines. Furthermore, Group-MATES reduces the number of tokens required to reach a certain downstream performance by up to 1.75x, substantially elevating the speed-quality frontier. Further analyses highlight the critical role of relationship weights in the relational data influence model and the effectiveness of our cluster-based inference. Our code is open-sourced at https://github.com/facebookresearch/Group-MATES.",
      "arxiv_url": "https://openreview.net/forum?id=uX4dyc7Z5Z",
      "pdf_url": "https://openreview.net/pdf/402496f1e4c1a0cddc82fae91b7f1f998b7c83c6.pdf",
      "primary_category": "LLM, Pre-training, Data Selection",
      "categories": [
        "LLM",
        "Pre-training",
        "Data Selection",
        "Data Influence"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "fmnxunacr4",
      "title": "Enigmata:  Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles",
      "authors": [
        "Jiangjie Chen",
        "Qianyu He",
        "Siyu Yuan",
        "Aili Chen",
        "Zhicheng Cai",
        "Weinan Dai",
        "Hongli Yu",
        "Jiaze Chen",
        "Xuefeng Li",
        "Qiying Yu",
        "Hao Zhou",
        "Mingxuan Wang"
      ],
      "abstract": "Large Language Models (LLMs), such as OpenAI’s o1 and DeepSeek’s R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce ENIGMATA, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across 7 categories, each with: 1) a generator that produces unlimited examples with controllable difficulty, and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose ENIGMATA-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-ENIGMATA, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like ENIGMATA-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from ENIGMATA further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of ENIGMATA. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Project page: https://seed-enigmata.github.io.",
      "arxiv_url": "https://openreview.net/forum?id=fmnxunacr4",
      "pdf_url": "https://openreview.net/pdf/d16d55cfeff749792ae0b093a3f4c0123aa6c09f.pdf",
      "primary_category": "Reasoning Model, Large Language Model, Reinforcement Learning",
      "categories": [
        "Reasoning Model",
        "Large Language Model",
        "Reinforcement Learning",
        "Logical Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2t5IfPAzbX",
      "title": "Model Reconciliation via Cost-Optimal Explanations in Probabilistic Logic Programming",
      "authors": [
        "Yinxu Tang",
        "Stylianos Loukas Vasileiou",
        "Vincent Derkinderen",
        "William Yeoh"
      ],
      "abstract": "In human-AI interaction, effective communication relies on aligning the AI agent’s model with the human user’s mental model, a process known as model reconciliation. However, existing model reconciliation approaches predominantly assume deterministic models, overlooking the fact that human knowledge is often uncertain or probabilistic. \nTo bridge this gap, we present a probabilistic model reconciliation framework that resolves inconsistencies in MPE outcome probabilities between an agent’s and a user’s models.\nOur approach is built on probabilistic logic programming (PLP) using ProbLog, where explanations are generated as cost-optimal model updates that reconcile these probabilistic differences. \nWe develop two search algorithms -- a generic baseline and an optimized version.\nThe latter is guided by theoretical insights and further extended with greedy and weighted variants to enhance scalability and efficiency.\nOur approach is validated through a user study on explanation types and computational experiments showing that the optimized version consistently outperforms the generic baseline.",
      "arxiv_url": "https://openreview.net/forum?id=2t5IfPAzbX",
      "pdf_url": "https://openreview.net/pdf/24293a46060dba441f03581793a3dd5a5597adc8.pdf",
      "primary_category": "model reconciliation, probabilistic logic programming, most probable explanations (MPE)",
      "categories": [
        "model reconciliation",
        "probabilistic logic programming",
        "most probable explanations (MPE)"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pCRm6g0RnA",
      "title": "ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio–Language Models",
      "authors": [
        "Weifei Jin",
        "Yuxin Cao",
        "Junjie Su",
        "Jason Xue",
        "Jie Hao",
        "Ke Xu",
        "Jin Song Dong",
        "Derui Wang"
      ],
      "abstract": "Recent advances in Audio-Language Models (ALMs) have significantly improved multimodal understanding capabilities. However, the introduction of the audio modality also brings new and unique vulnerability vectors. Previous studies have proposed jailbreak attacks that specifically target ALMs, revealing that defenses directly transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks are largely ineffective against these ALM-specific threats.\nTo address this issue, we propose ALMGuard, the first defense framework tailored to ALMs. \nBased on the assumption that safety-aligned shortcuts naturally exist in ALMs, we design a method to identify universal Shortcut Activation Perturbations (SAPs) that serve as triggers that activate the safety shortcuts to safeguard ALMs at inference time.\nTo better sift out effective triggers while preserving the model’s utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM), which restricts perturbations to Mel-frequency bins that are sensitive to jailbreaks but insensitive to speech understanding.\nBoth theoretical analyses and empirical results demonstrate the robustness of our method against both seen and unseen attacks. Overall, \\MethodName reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6\\% across four models, while maintaining comparable utility on benign benchmarks, establishing it as the new state of the art. Our code and data are available at https://github.com/WeifeiJin/ALMGuard.",
      "arxiv_url": "https://openreview.net/forum?id=pCRm6g0RnA",
      "pdf_url": "https://openreview.net/pdf/ea8d79474169ebdb5d9a649654cce762d6c00a28.pdf",
      "primary_category": "Audio-Language Model, Jailbreaking, Blue-teaming",
      "categories": [
        "Audio-Language Model",
        "Jailbreaking",
        "Blue-teaming",
        "AI Safety"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8z3cOVER4z",
      "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval",
      "authors": [
        "Di Liu",
        "Meng Chen",
        "Baotong Lu",
        "Huiqiang Jiang",
        "Zhenhua Han",
        "Qianxi Zhang",
        "Qi Chen",
        "Chengruidong Zhang",
        "Bailu Ding",
        "Kai Zhang",
        "Chen Chen",
        "Fan Yang",
        "Yuqing Yang",
        "Lili Qiu"
      ],
      "abstract": "Transformer-based Large Language Models (LLMs) have become increasingly important. However, scaling LLMs to longer contexts incurs slow inference speed and high GPU memory consumption for caching key-value (KV) vectors. This paper presents RetrievalAttention, a training-free approach to both accelerate the decoding phase and reduce GPU memory consumption by pre-building KV vector indexes for fixed contexts and maintaining them in CPU memory for efficient retrieval. Unlike conventional KV cache methods, RetrievalAttention integrate approximate nearest neighbor search (ANNS) indexes into attention computation. We observe that off-the-shelf ANNS techniques often fail due to the out-of-distribution (OOD) nature of query and key vectors in attention mechanisms. RetrievalAttention overcomes this with an attention-aware vector index. Our evaluation shows RetrievalAttention achieves near full attention accuracy while accessing only 1-3\\% of the data, significantly reducing inference costs. Remarkably, RetrievalAttention enables LLMs with 8B parameters to handle 128K tokens on a single NVIDIA RTX4090 (24GB), achieving a decoding speed of 0.107 seconds per token.",
      "arxiv_url": "https://openreview.net/forum?id=8z3cOVER4z",
      "pdf_url": "https://openreview.net/pdf/07fc974752fbdfffc98d100f757b7d299a810126.pdf",
      "primary_category": "Large Language Model, Efficient Inference, Long Context",
      "categories": [
        "Large Language Model",
        "Efficient Inference",
        "Long Context",
        "Vector Retrieval"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "91H9CSvdwl",
      "title": "Safety Pretraining: Toward the Next Generation of Safe AI",
      "authors": [
        "Pratyush Maini",
        "Sachin Goyal",
        "Dylan Sam",
        "Alexander Robey",
        "Yash Savani",
        "Yiding Jiang",
        "Andy Zou",
        "Matt Fredrikson",
        "Zachary Chase Lipton",
        "J Zico Kolter"
      ],
      "abstract": "As large language models (LLMs) are increasingly deployed in high-stakes settings, the risk of generating harmful or toxic content remains a central challenge. Post-hoc alignment methods are brittle: once unsafe patterns are learned during pretraining, they are hard to remove.\nIn this work, we present a data-centric pretraining framework that builds safety into the model from the start. Our framework consists of four key steps:\n(i) Safety Filtering: building a safety classifier to classify webdata into safe and unsafe categories;\n(ii) Safety Rephrasing: we recontextualize unsafe webdata into safer narratives;\n(iii) Native Refusal: we synthetically generate pretraining datasets that actively teach models to refuse on unsafe content and the moral reasoning behind it, and\n(iv) Harmfulness-Tag annotated pretraining: we flag unsafe content during pretraining using a special token, and use it to steer models away from unsafe generations at inference-time.\nOur safety-pretrained models reduce attack success rates from 38.8% to 8.4% on standard LLM safety benchmarks with no performance degradation on general tasks.",
      "arxiv_url": "https://openreview.net/forum?id=91H9CSvdwl",
      "pdf_url": "https://openreview.net/pdf/60d7860b3e2747b4def0be07de449da240c31b99.pdf",
      "primary_category": "Safe LLMs, Pretraining, Data Centric ML",
      "categories": [
        "Safe LLMs",
        "Pretraining",
        "Data Centric ML"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "siPeAstQLq",
      "title": "C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models",
      "authors": [
        "Amir Hossein Rahmati",
        "Sanket Jantre",
        "Weifeng Zhang",
        "Yucheng Wang",
        "Byung-Jun Yoon",
        "Nathan Urban",
        "Xiaoning Qian"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning large language models (LLMs), but it often produces overconfident predictions in data-scarce few-shot settings. To address this issue, several classical statistical learning approaches have been repurposed for scalable uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input characteristics affect the predictive uncertainty estimates. To address this limitation, we propose Contextual Low-Rank Adaptation (**C-LoRA**) as a novel uncertainty-aware and parameter efficient fine-tuning approach, by developing new lightweight LoRA modules contextualized to each input data sample to dynamically adapt uncertainty estimates. Incorporating data-driven contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves well-calibrated uncertainties, and yields robust predictions. Extensive experiments on LLaMA2-7B models demonstrate that C-LoRA consistently outperforms the state-of-the-art uncertainty-aware LoRA methods in both uncertainty quantification and model generalization. Ablation studies further confirm the critical role of our contextual modules in capturing sample-specific uncertainties.  C-LoRA sets a new standard for robust, uncertainty-aware LLM fine-tuning in few-shot regimes. Although our experiments are limited to 7B models, our method is architecture-agnostic and, in principle, applies beyond this scale; studying its scaling to larger models remains an open problem. Our code is available at https://github.com/ahra99/c_lora.",
      "arxiv_url": "https://openreview.net/forum?id=siPeAstQLq",
      "pdf_url": "https://openreview.net/pdf/e131aab939fcce2a63a55f7ad49804a34e2e8632.pdf",
      "primary_category": "Low-rank Adaptation, Uncertainty Quantification, Probabilistic Modeling",
      "categories": [
        "Low-rank Adaptation",
        "Uncertainty Quantification",
        "Probabilistic Modeling"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kuoD6G0Suq",
      "title": "Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL",
      "authors": [
        "Joey Hong",
        "Anca Dragan",
        "Sergey Levine"
      ],
      "abstract": "Large language models (LLMs) excel in tasks like question answering and dialogue, but complex tasks requiring interaction, such as negotiation and persuasion, require additional long-horizon reasoning and planning. Reinforcement learning (RL) fine-tuning can enable such planning in principle, but suffers from drawbacks that hinder scalability. In particular, multi-turn RL training incurs high memory and computational costs, which are exacerbated when training LLMs as policies. Furthermore, the largest LLMs do not expose the APIs necessary to be trained in such manner. As a result, modern methods to improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather than RL fine-tuning. To remedy this, we propose a novel approach that uses goal-conditioned value functions to guide the reasoning of LLM agents, that scales even to large API-based models. These value functions predict how a task will unfold given an action, allowing the LLM agent to evaluate multiple possible outcomes, both positive and negative, to plan effectively. In addition, these value functions are trained over reasoning steps rather than full actions, to be a concise and light-weight module that facilitates decision-making in multi-turn interactions. We validate our method on tasks requiring interaction, including tool use, social deduction, and dialogue, demonstrating superior performance over both RL fine-tuning and prompting methods while maintaining efficiency and scalability.",
      "arxiv_url": "https://openreview.net/forum?id=kuoD6G0Suq",
      "pdf_url": "https://openreview.net/pdf/a27fd29bd9a99f5624d5362501cf5a6acc4c4e37.pdf",
      "primary_category": "LLM, reasoning, planning",
      "categories": [
        "LLM",
        "reasoning",
        "planning",
        "reinforcement learning"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1V3Toke6XP",
      "title": "Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment",
      "authors": [
        "Weixiang Zhao",
        "Xingyu Sui",
        "Yulin Hu",
        "Jiahe Guo",
        "Haixiao Liu",
        "Biye Li",
        "Yanyan Zhao",
        "Bing Qin",
        "Ting Liu"
      ],
      "abstract": "Personalized alignment is essential for enabling large language models (LLMs) to engage effectively in user-centric dialogue. While recent prompt-based and offline optimization methods offer preliminary solutions, they fall short in cold-start scenarios and long-term personalization due to their inherently static and shallow designs. In this work, we introduce the Reinforcement Learning for Personalized Alignment (RLPA) framework, in which an LLM interacts with a simulated user model to iteratively infer and refine user profiles through dialogue. The training process is guided by a dual-level reward structure: the Profile Reward encourages accurate construction of user representations, while the Response Reward incentivizes generation of responses consistent with the inferred profile. We instantiate RLPA by fine-tuning Qwen-2.5-3B-Instruct, resulting in Qwen-RLPA, which achieves state-of-the-art performance in personalized dialogue. Empirical evaluations demonstrate that Qwen-RLPA consistently outperforms prompting and offline fine-tuning baselines, and even surpasses advanced commercial models such as Claude-3.5 and GPT-4o. Further analysis highlights Qwen-RLPA's robustness in reconciling conflicting user preferences, sustaining long-term personalization and delivering more efficient inference compared to recent reasoning-focused LLMs. These results emphasize the potential of dynamic profile inference as a more effective paradigm for building personalized dialogue systems.",
      "arxiv_url": "https://openreview.net/forum?id=1V3Toke6XP",
      "pdf_url": "https://openreview.net/pdf/3c6a0c8b1dfec67e80825018ffe4d2f29cda83dc.pdf",
      "primary_category": "Personalized Alignment, Large Language Model",
      "categories": [
        "Personalized Alignment",
        "Large Language Model"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BOEZYnC8nR",
      "title": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with LLMs",
      "authors": [
        "Richard Cornelius Suwandi",
        "Feng Yin",
        "Juntao Wang",
        "Renjie Li",
        "Tsung-Hui Chang",
        "Sergios Theodoridis"
      ],
      "abstract": "The efficiency of Bayesian optimization (BO) relies heavily on the choice of the Gaussian process (GP) kernel, which plays a central role in balancing exploration and exploitation under limited evaluation budgets. Traditional BO methods often rely on fixed or heuristic kernel selection strategies, which can result in slow convergence or suboptimal solutions when the chosen kernel is poorly suited to the underlying objective function. To address this limitation, we propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO with large language models (LLMs). Concretely, CAKE leverages LLMs as the crossover and mutation operators to adaptively generate and refine GP kernels based on the observed data throughout the optimization process. To maximize the power of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to select the most effective kernel through balancing the model fit measured by the Bayesian information criterion (BIC) with the expected improvement at each iteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO method consistently outperforms established baselines across a range of real-world tasks, including hyperparameter optimization, controller tuning, and photonic chip design. Our code is publicly available at https://github.com/richardcsuwandi/cake.",
      "arxiv_url": "https://openreview.net/forum?id=BOEZYnC8nR",
      "pdf_url": "https://openreview.net/pdf/3f5e430f197eef2933e96aa4230a5d3d540c8545.pdf",
      "primary_category": "Kernel design, Gaussian processes, Bayesian optimization",
      "categories": [
        "Kernel design",
        "Gaussian processes",
        "Bayesian optimization",
        "large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "eumRwpgdMU",
      "title": "ARIA: Training Language Agents with Intention-driven Reward Aggregation",
      "authors": [
        "Ruihan Yang",
        "Yikai Zhang",
        "Aili Chen",
        "Xintao Wang",
        "Jiangjie Chen",
        "Siyu Yuan",
        "Deqing Yang",
        "Yanghua Xiao"
      ],
      "abstract": "Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-form language interactions. However, in open-ended language action environments (e.g., negotiation or question-asking games), the action space can be formulated as a joint distribution over tokens, resulting in an extremely large and combinatorial action space. Sampling actions in such a space can lead to extreme reward sparsity, which brings large reward variance, hindering effective reinforcement learning (RL). To address this, we propose **ARIA**, a method that **A**ggregates **R**ewards in **I**ntention space to enable efficient and effective language **A**gents training. ARIA aims to project natural language actions from the high-dimensional joint token distribution space into a low-dimensional intention space, where semantically similar actions are clustered and assigned shared rewards. This intention-aware reward aggregation reduces reward variance by densifying reward signals, fostering efficient and effective policy optimization. Extensive experiments demonstrate that ARIA not only significantly reduces gradient variance, but also delivers substantial performance gains of average 9.95% across four downstream tasks (e.g., negotiation and text-based games), consistently outperforming strong offline and online RL baselines.",
      "arxiv_url": "https://openreview.net/forum?id=eumRwpgdMU",
      "pdf_url": "https://openreview.net/pdf/61e6b62ea781f638b7b6cae2c3ea12b7e42ed96d.pdf",
      "primary_category": "Language Agent, Agent Learning, Reinforcement Learning",
      "categories": [
        "Language Agent",
        "Agent Learning",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "AwsiYZ2ets",
      "title": "A Simple Linear Patch Revives Layer-Pruned Large Language Models",
      "authors": [
        "Xinrui Chen",
        "Haoli Bai",
        "Tao Yuan",
        "Ruikang Liu",
        "Kang Zhao",
        "Xianzhi Yu",
        "Lu Hou",
        "Tian Guan",
        "Yonghong He",
        "Chun Yuan"
      ],
      "abstract": "Layer pruning has emerged as a widely used technique for compressing large language models (LLMs). However, existing layer pruning approaches often incur substantial performance degradation. We identify the majority of this degradation to a single yet previously overlooked issue: \\textit{the mismatch of activation magnitudes at the pruning interface}. \nThe pre-interface activations exhibit significantly different scales from the post-interface ones, causing the distributional shift as it propagates through the remaining layers. \nTo address this issue, we introduce \\textsc{LinearPatch}, a lightweight and plug-and-play technique that fuses two operations into one matrix multiply at the pruning interface: (i) a Hadamard transformation that suppresses massive outliers at particular tokens and (ii) a channel-wise scaling that aligns activation statistics. \nOn LLaMA-3-8B, \\textsc{LinearPatch} preserves up to \\textbf{94.15\\%} of the original model's performance when pruning 5 out of 32 layers, outperforming the previous state of the art by \\textbf{4\\%}. \nThe patch can be further refined with 5K unlabeled samples via memory-efficient offline distillation, pushing the retention to 95.16\\% within only 30 minutes on a single GPU.\nCode is available at \\url{https://github.com/chenxinrui-tsinghua/LinearPatch}.",
      "arxiv_url": "https://openreview.net/forum?id=AwsiYZ2ets",
      "pdf_url": "https://openreview.net/pdf/ad273c5c0dd5a36d07f308b403483e10bb3e3b6d.pdf",
      "primary_category": "Layer Pruning, Large Language Models, Model Compression",
      "categories": [
        "Layer Pruning",
        "Large Language Models",
        "Model Compression"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "30iBKSQMXn",
      "title": "Retro-R1: LLM-based Agentic Retrosynthesis",
      "authors": [
        "Wei Liu",
        "Jiangtao Feng",
        "Hongli Yu",
        "Yuxuan Song",
        "Yuqiang Li",
        "Shufei Zhang",
        "LEI BAI",
        "Wei-Ying Ma",
        "Hao Zhou"
      ],
      "abstract": "Retrosynthetic planning is a fundamental task in chemical discovery. Due to the vast combinatorial search space, identifying viable synthetic routes remains a significant challenge--even for expert chemists. Recent advances in Large Language Models (LLMs), particularly equipped with reinforcement learning, have demonstrated strong human-like reasoning and planning abilities, especially in mathematics and code problem solving. This raises a natural question: Can the reasoning capabilities of LLMs be harnessed to develop an AI chemist capable of learning effective policies for multi-step retrosynthesis? In this study, we introduce Retro-R1, a novel LLM-based retrosynthesis agent trained via reinforcement learning to design molecular synthesis pathways. Unlike prior approaches, which typically rely on single-turn, question-answering formats, Retro-R1 interacts dynamically with plug-in single-step retrosynthesis tools and learns from environmental feedback. Experimental results show that Retro-R1 achieves a 55.79\\% pass@1 success rate, surpassing the previous state of the art by 8.95\\%. Notably, Retro-R1 demonstrates strong generalization to out-of-domain test cases, where existing methods tend to fail despite their high in-domain performance. Our work marks a significant step toward equipping LLMs with advanced, chemist-like reasoning abilities, highlighting the promise of reinforcement learning for enabling data-efficient, generalizable, and sophisticated scientific problem-solving in LLM-based agents.",
      "arxiv_url": "https://openreview.net/forum?id=30iBKSQMXn",
      "pdf_url": "https://openreview.net/pdf/470a5ef177e20c474a9e358c1bab0c1cf082f1de.pdf",
      "primary_category": "Reinforcement Learning, Retrosynthesis Planning, LLM-based Agent",
      "categories": [
        "Reinforcement Learning",
        "Retrosynthesis Planning",
        "LLM-based Agent"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xp7B8rkh7L",
      "title": "LoRA vs Full Fine-tuning: An Illusion of Equivalence",
      "authors": [
        "Reece S Shuttleworth",
        "Jacob Andreas",
        "Antonio Torralba",
        "Pratyusha Sharma"
      ],
      "abstract": "Fine-tuning is a crucial paradigm for adapting pre-trained large language models to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA) have been shown to effectively fine-tune LLMs with an extreme reduction in trainable parameters. But, \\emph{are their learned solutions really equivalent?} We study how LoRA and full-finetuning change pre-trained models by analyzing the model's weight matrices through the lens of their spectral properties. We find that LoRA and full fine-tuning yield weight matrices whose singular value decompositions exhibit very different structure: weight matrices trained with LoRA have new, high-ranking singular vectors, which we call \\emph{intruder dimensions}, while those trained with full fine-tuning do not. Further, we extend the finding that LoRA forgets less than full fine-tuning and find its forgetting is vastly localized to the intruder dimension -- by causally intervening on the intruder dimensions by changing their associated singular values post-fine-tuning, we show that they cause forgetting. Moreover, scaling them down significantly improves modeling of the pre-training distribution with a minimal drop in downstream task performance. Given this, we should expect accumulating intruder dimensions to be harmful and lead to more forgetting. This will be amplified during continual learning because of sequentially fine-tuning, and we show that LoRA models do accumulate intruder dimensions here tend to perform worse in this setting, emphasizing the practicality of our findings.",
      "arxiv_url": "https://openreview.net/forum?id=xp7B8rkh7L",
      "pdf_url": "https://openreview.net/pdf/885247617232975c7dfa39ff6a33505b302bb485.pdf",
      "primary_category": "Parameter Efficient Fine-tuning (PEFT), Low Rank Adaptation (LoRA), LLMs",
      "categories": [
        "Parameter Efficient Fine-tuning (PEFT)",
        "Low Rank Adaptation (LoRA)",
        "LLMs",
        "Transformers"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "m7Aw57oI3U",
      "title": "Curriculum Model Merging: Harmonizing Chemical LLMs for Enhanced Cross-Task Generalization",
      "authors": [
        "Baoyi He",
        "Luotian Yuan",
        "Ying Wei",
        "Fei Wu"
      ],
      "abstract": "The emergence of large language models (LLMs) has opened new opportunities for AI-driven chemical problem solving. However, existing chemical LLMs are typically tailored to specific task formats or narrow domains, limiting their capacity to integrate knowledge and generalize across tasks. Model merging offers a promising route for efficiently combining specialized LLMs into a unified model without access to original training data, which is urgently needed in the chemical domain where in-house data and privacy preservation are critical. However, effective model merging in the chemical domain poses unique challenges: (1) significant disparities among chemical LLMs due to task-specific specialization, and (2) a highly imbalanced distribution of chemical LLMs in targeted downstream tasks, where some are over-benchmarked while others remain underexplored. These challenges intensify model inconsistencies such as parameter interference and accumulated fine-tuning noise, which collectively hinder effective model merging. To this end, we propose Curriculum Model Merging (CMM), a curriculum-based framework that progressively merges expert chemical LLMs in a moderate and continual manner. CMM aims to harmonize their inconsistencies while meantime preserve their domain-specific expertise. Comprehensive experiments on two benchmark datasets show that CMM effectively consolidates task-specific expertise and outperforms the state-of-the-art methods by 29.03\\% in terms of overall average performance. Moreover, CMM facilitates chemical knowledge generalization across prediction and generative tasks without sacrificing robustness, exhibiting promising merging performance under both expert-abundant and expert-sparse scenarios.",
      "arxiv_url": "https://openreview.net/forum?id=m7Aw57oI3U",
      "pdf_url": "https://openreview.net/pdf/c322e43d96d7dc82534c124fd220afae7420b616.pdf",
      "primary_category": "LLM, model merging, chemical",
      "categories": [
        "LLM",
        "model merging",
        "chemical"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "gEeIutncjh",
      "title": "Do Automatic Factuality Metrics Measure Factuality? A Critical Evaluation",
      "authors": [
        "Sanjana Ramprasad",
        "Byron C Wallace"
      ],
      "abstract": "Modern LLMs can now produce highly readable abstractive summaries, to the point that traditional automated metrics for evaluating summary quality, such as ROUGE, have saturated. \nHowever, LLMs still sometimes introduce inaccuracies into summaries, i.e., information inconsistent with or unsupported by the corresponding source. \nMeasuring the occurrence of these often subtle factual inconsistencies automatically has proved challenging. \nThis in turn has motivated development of metrics intended to measure the factual consistency of generated summaries against sources. \nBut are these approaches measuring what they purport to? Or are they mostly exploiting artifacts? \nIn this work, we stress test a range of automatic factuality metrics—including specialized model-based approaches and LLM-based prompting methods—to probe what they actually capture. Using a shallow classifier to separate “easy” examples for factual evaluation—where surface features suffice—from “hard” cases requiring deeper reasoning, we find that all metrics show substantial performance drops on the latter.\nFurthermore, some metrics are more sensitive to benign, fact-preserving edits than to factual corrections. Building on this observation, we demonstrate that most automatic factuality metrics can be gamed—that is, their scores can be artificially inflated by appending innocuous, content-free sentences to summaries. Among the metrics tested, the LLM prompt-based ChatGPT-DA approach is the most robust and reliable; however, it exhibits a notable caveat: it likely relies more on parametric knowledge than on the provided source when making judgments. Taken together, our findings call into question the reliability of current factuality metrics and prompt a broader reflection on what these metrics are truly measuring. We conclude with concrete recommendations for improving both benchmark design and metric robustness, particularly in light of their vulnerability to superficial manipulations.",
      "arxiv_url": "https://openreview.net/forum?id=gEeIutncjh",
      "pdf_url": "https://openreview.net/pdf/f01fac6534e9c9c97ea0426c435a7c5f1f3ad925.pdf",
      "primary_category": "Summarization, Faithfulness metrics",
      "categories": [
        "Summarization",
        "Faithfulness metrics"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "gBgvuTd9Hx",
      "title": "One Token Embedding Is Enough to Deadlock Your Large Reasoning Model",
      "authors": [
        "Mohan Zhang",
        "Yihua Zhang",
        "Jinghan Jia",
        "Zhangyang Wang",
        "Sijia Liu",
        "Tianlong Chen"
      ],
      "abstract": "Modern large reasoning models (LRMs) exhibit impressive multi-step problem-solving via chain-of-thought (CoT) reasoning. However, this iterative thinking mechanism introduces a new vulnerability surface. We present the Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative control flow by training a malicious adversarial embedding to induce perpetual reasoning loops. Specifically, the optimized embedding encourages transitional tokens (e.g., “Wait”, “But”) after reasoning steps, preventing the model from concluding its answer. A key challenge we identify is the continuous-to-discrete projection gap: naïve projections of adversarial embeddings to token sequences nullify the attack. To overcome this, we introduce a backdoor implantation strategy, enabling reliable activation through specific trigger tokens. Our method achieves a 100\\% attack success rate across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three math reasoning benchmarks, forcing models to generate up to their maximum token limits. The attack is also stealthy (in terms of causing negligible utility loss on benign user inputs) and remains robust against existing strategies trying to mitigate the overthinking issue. Our findings expose a critical and underexplored security vulnerability in LRMs from the perspective of reasoning (in)efficiency.",
      "arxiv_url": "https://openreview.net/forum?id=gBgvuTd9Hx",
      "pdf_url": "https://openreview.net/pdf/944f609eec30bc57faa710a6f8a5cfdc09dfbb04.pdf",
      "primary_category": "Deadlock Attack, Large Language Model",
      "categories": [
        "Deadlock Attack",
        "Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "5pQFE4yIZ5",
      "title": "Distributive Fairness in Large Language Models: Evaluating Alignment with Human Values",
      "authors": [
        "Hadi Hosseini",
        "Samarth Khanna"
      ],
      "abstract": "The growing interest in employing large language models (LLMs) for decision-making in social and economic contexts has raised questions about their potential to function as agents in these domains. A significant number of societal problems involve the distribution of resources, where fairness, along with economic efficiency, play a critical role in the desirability of outcomes. In this paper, we examine whether LLM responses adhere to fundamental fairness concepts such as equitability, envy-freeness, and Rawlsian maximin, and investigate their alignment with human preferences. We evaluate the performance of several LLMs, providing a comparative benchmark of their ability to reflect these measures. Our results demonstrate a lack of alignment between current LLM responses and human distributional preferences. Moreover, LLMs are unable to utilize money as a transferable resource to mitigate inequality. Nonetheless, we demonstrate a stark contrast when (some) LLMs are tasked with selecting from a predefined menu of options rather than generating one. In addition, we analyze the robustness of LLM responses to variations in semantic factors (e.g. intentions or personas) or non-semantic prompting changes (e.g. templates or orderings). Finally, we highlight potential strategies aimed at enhancing the alignment of LLM behavior with well-established fairness concepts.",
      "arxiv_url": "https://openreview.net/forum?id=5pQFE4yIZ5",
      "pdf_url": "https://openreview.net/pdf/ef52bff1015ace89a4b51d667b9489219d7092fa.pdf",
      "primary_category": "Distributive fairness, Equitability, Generative AI",
      "categories": [
        "Distributive fairness",
        "Equitability",
        "Generative AI",
        "Alignment",
        "Human Values"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Hehoz0QgeF",
      "title": "Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models",
      "authors": [
        "Yue Xu",
        "Chengyan Fu",
        "Li Xiong",
        "Sibei Yang",
        "Wenjie Wang"
      ],
      "abstract": "Pre-training large language models (LLMs) on vast text corpora enhances natural language processing capabilities but risks encoding social biases, particularly gender bias. While parameter-modification methods like fine-tuning mitigate bias, they are resource-intensive, unsuitable for closed-source models, and lack adaptability to evolving societal norms. Instruction-based approaches offer flexibility but often compromise general performance on normal tasks. To address these limitations, we propose $\\textit{FaIRMaker}$, an automated and model-independent framework that employs an $\\textbf{auto-search and refinement}$ paradigm to adaptively generate Fairwords, which act as instructions to reduce gender bias and enhance response quality. $\\textit{FaIRMaker}$ enhances the debiasing capacity by enlarging the Fairwords search space while preserving the utility and making it applicable to closed-source models by training a sequence-to-sequence model that adaptively refines Fairwords into effective debiasing instructions when facing gender-related queries and performance-boosting prompts for neutral inputs. Extensive experiments demonstrate that $\\textit{FaIRMaker}$ effectively mitigates gender bias while preserving task integrity and ensuring compatibility with both open- and closed-source LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=Hehoz0QgeF",
      "pdf_url": "https://openreview.net/pdf/5553b2690bcc2060b1fb9e5f380b4b36ebe1de90.pdf",
      "primary_category": "model biasevaluation, bias mitigation, ethical considerations in NLP applications",
      "categories": [
        "model biasevaluation",
        "bias mitigation",
        "ethical considerations in NLP applications"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "E6ZdfjtoiX",
      "title": "Efficient Data Selection at Scale via Influence Distillation",
      "authors": [
        "Mahdi Nikdan",
        "Vincent Cohen-Addad",
        "Dan Alistarh",
        "Vahab Mirrokni"
      ],
      "abstract": "Effective data selection is critical for efficient training of modern Large Language Models (LLMs). This paper introduces Influence Distillation, a novel, mathematically-justified framework for data selection that employs second-order information to optimally weight training samples. By distilling each sample's influence on a target distribution, our method assigns model-specific weights that are used to select training data for LLM fine-tuning, guiding it toward strong performance on the target domain. We derive these optimal weights for both Gradient Descent and Adam optimizers. To ensure scalability and reduce computational cost, we propose a $\\textit{landmark-based approximation}$: influence is precisely computed for a small subset of \"landmark\" samples and then efficiently propagated to all other samples to determine their weights. We validate Influence Distillation by applying it to instruction tuning on the Tulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU, across several models from the Llama and Qwen families. Experiments show that Influence Distillation matches or outperforms state-of-the-art performance while achieving up to $3.5\\times$ faster selection.",
      "arxiv_url": "https://openreview.net/forum?id=E6ZdfjtoiX",
      "pdf_url": "https://openreview.net/pdf/6ceb9ccc843dc4f4c42f1de2c9fd1a83da8149bf.pdf",
      "primary_category": "Data Selection, Data Weighting, Influence Distillation",
      "categories": [
        "Data Selection",
        "Data Weighting",
        "Influence Distillation",
        "Fine Tuning",
        "Efficient Training",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1zKElu2MuQ",
      "title": "Matching Markets Meet LLMs: Algorithmic Reasoning with Ranked Preferences",
      "authors": [
        "Hadi Hosseini",
        "Samarth Khanna",
        "Ronak Singh"
      ],
      "abstract": "The rise of Large Language Models (LLMs) has driven progress in reasoning tasks, from program synthesis to scientific hypothesis generation, yet their ability to handle ranked preferences and structured algorithms in combinatorial domains remains underexplored. \nWe study matching markets, a core framework behind applications like resource allocation and ride-sharing, which require reconciling individual ranked preferences to ensure stable outcomes. We evaluate seven state‐of‐the‐art models on a hierarchy of preference‐based reasoning tasks---ranging from stable‐matching generation to instability detection, instability resolution, and fine-grained preference queries---to systematically expose their logical and algorithmic limitations in handling ranked inputs. Surprisingly, even top-performing models with advanced reasoning struggle to resolve instability in large markets, often failing to identify blocking pairs or execute algorithms iteratively. We further show that parameter-efficient fine-tuning (LoRA) significantly improves performance in small markets, but fails to bring about a similar improvement on large instances, suggesting the need for more sophisticated strategies to improve LLMs' reasoning with larger-context inputs.",
      "arxiv_url": "https://openreview.net/forum?id=1zKElu2MuQ",
      "pdf_url": "https://openreview.net/pdf/9410f3952cf491d1f154e38d3d23b5dbf93e6544.pdf",
      "primary_category": "Preference reasoning, matching markets, stability",
      "categories": [
        "Preference reasoning",
        "matching markets",
        "stability",
        "comprehension"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "E6gwPtWjb1",
      "title": "LLM Meets Diffusion: A Hybrid Framework for Crystal Material Generation",
      "authors": [
        "Subhojyoti Khastagir",
        "KISHALAY DAS",
        "Pawan Goyal",
        "Seung-Cheol Lee",
        "Satadeep Bhattacharjee",
        "Niloy Ganguly"
      ],
      "abstract": "Recent advances in generative modeling have shown significant promise in designing novel periodic crystal structures. Existing approaches typically rely on either large language models (LLMs) or equivariant denoising models, each with complementary strengths: LLMs excel at handling discrete atomic types but often struggle with continuous features such as atomic positions and lattice parameters, while denoising models are effective at modeling continuous variables but encounter difficulties in generating accurate atomic compositions. To bridge this gap, we propose CrysLLMGen, a hybrid framework that integrates an LLM with a diffusion model to leverage their complementary strengths for crystal material generation. During sampling, CrysLLMGen first employs a fine-tuned LLM to produce an intermediate representation of atom types, atomic coordinates, and lattice structure. While retaining the predicted atom types, it passes the atomic coordinates and lattice structure to a pre-trained equivariant diffusion model for refinement. Our framework outperforms state-of-the-art generative models across several benchmark tasks and datasets. Specifically, CrysLLMGen not only achieves a balanced performance in terms of structural and compositional validity but also generates more stable and novel materials compared to LLM-based and denoising-based models Furthermore, CrysLLMGen exhibits strong conditional generation capabilities, effectively producing materials that satisfy user-defined constraints. Code is available at \\url{https://github.com/kdmsit/crysllmgen}",
      "arxiv_url": "https://openreview.net/forum?id=E6gwPtWjb1",
      "pdf_url": "https://openreview.net/pdf/66e56b617dc7632ef49085f5a77dc332bad2a7f4.pdf",
      "primary_category": "ML4Materials, Diffusion Models, Crystal Materials",
      "categories": [
        "ML4Materials",
        "Diffusion Models",
        "Crystal Materials",
        "Material Generation",
        "AI4Science"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "E6SFbnPiVP",
      "title": "FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts",
      "authors": [
        "Weihao Bo",
        "Yanpeng Sun",
        "Yu Wang",
        "Xinyu Zhang",
        "Zechao Li"
      ],
      "abstract": "In this paper, we introduce FedMGP, a new paradigm for personalized federated prompt learning in vision-language models (VLMs). Existing federated prompt learning (FPL) methods often rely on a single, text-only prompt representation, which leads to client-specific overfitting and unstable aggregation under heterogeneous data distributions. Toward this end, FedMGP equips each client with multiple groups of paired textual and visual prompts, enabling the model to capture diverse, fine-grained semantic and instance-level cues. A diversity loss is introduced to drive each prompt group to specialize in distinct and complementary semantic aspects, ensuring that the groups collectively cover a broader range of local characteristics.During communication, FedMGP employs a dynamic prompt aggregation strategy based on similarity-guided probabilistic sampling: each client computes the cosine similarity between its prompt groups and the global prompts from the previous round, then samples s groups via a softmax-weighted distribution. This soft selection mechanism preferentially aggregates semantically aligned knowledge while still enabling exploration of underrepresented patterns—effectively balancing the preservation of common knowledge with client-specific features. Notably, FedMGP maintains parameter efficiency by redistributing a fixed prompt capacity across multiple groups, achieving state-of-the-art performance with the lowest communication parameters (5.1k) among all federated prompt learning methods. Theoretical analysis shows that our dynamic aggregation strategy promotes robust global representation learning by reinforcing shared semantics while suppressing client-specific noise. Extensive experiments demonstrate that FedMGP consistently outperforms prior approaches in both personalization and domain generalization across diverse federated vision-language benchmarks.The code will be released on https://github.com/weihao-bo/FedMGP.git.",
      "arxiv_url": "https://openreview.net/forum?id=E6SFbnPiVP",
      "pdf_url": "https://openreview.net/pdf/286f9927a57e95d248d53f9c7c1fba7f80c8f711.pdf",
      "primary_category": "Federal learning; Vision language Model; Prompt Learning;",
      "categories": [
        "Federal learning; Vision language Model; Prompt Learning;"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7Spt8cAJq0",
      "title": "SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly",
      "authors": [
        "Wei Zhu",
        "Zhiwen Tang",
        "Kun Yue"
      ],
      "abstract": "Recent advancements have increasingly focused on leveraging large language models (LLMs) to construct autonomous agents for complex problem-solving tasks. However, existing approaches predominantly employ a single-agent framework to generate search branches and estimate rewards during Monte Carlo Tree Search (MCTS) planning. This single-agent paradigm inherently limits exploration capabilities, often resulting in insufficient diversity among generated branches and suboptimal planning performance.\nTo overcome these limitations, we propose  $\\textbf{SY}$nergistic $\\textbf{M}$ulti-agent $\\textbf{P}$lanning with $\\textbf{H}$eter$\\textbf{O}$geneous la$\\textbf{N}$gauge model assembl$\\textbf{Y}$ ($\\textbf{SYMPHONY}$),  a novel multi-agent planning framework that integrates a pool of heterogeneous language model-based agents. \nBy leveraging diverse reasoning patterns across agents, SYMPHONY enhances rollout diversity and facilitates more effective exploration.\nEmpirical results across multiple benchmark tasks show that SYMPHONY achieves strong performance even when instantiated with open-source LLMs deployable on consumer-grade hardware. When enhanced with cloud-based LLMs accessible via API, SYMPHONY demonstrates further improvements, outperforming existing state-of-the-art baselines and underscoring the effectiveness of heterogeneous multi-agent coordination in planning tasks.",
      "arxiv_url": "https://openreview.net/forum?id=7Spt8cAJq0",
      "pdf_url": "https://openreview.net/pdf/4334c20856e35aabcc60e8000970b5cedab83044.pdf",
      "primary_category": "MCTS; Multi-Agent; Sequential Decision Making; LLM",
      "categories": [
        "MCTS; Multi-Agent; Sequential Decision Making; LLM"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QPJjiNCRq1",
      "title": "Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning",
      "authors": [
        "Yuanyao Chen",
        "Rongsheng Chen",
        "Fu Luo",
        "Zhenkun Wang"
      ],
      "abstract": "Neural Combinatorial Optimization (NCO) has emerged as a promising learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by minimizing the need for extensive manual engineering. While existing NCO methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated considerable success on problems of similar scale, their performance significantly degrades when applied to large-scale scenarios. This degradation arises from the distributional shift between training and testing data, rendering policies learned on small instances ineffective for larger problems. To overcome this limitation, we introduce a novel learning framework driven by Large Language Models (LLMs). This framework learns a projection between the training and testing distributions, which is then deployed to enhance the scalability of the NCO model. Notably, unlike prevailing techniques that necessitate joint training with the neural network, our approach operates exclusively during the inference phase, obviating the need for model retraining. Extensive experiments demonstrate that our method enables a backbone model (trained on 100-node instances) to achieve superior performance on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) of up to 100K nodes from diverse distributions. The source code can be found in https://github.com/CIAM-Group/TTPL.",
      "arxiv_url": "https://openreview.net/forum?id=QPJjiNCRq1",
      "pdf_url": "https://openreview.net/pdf/d48bd85e8451a32f4dfac774b17b7e7e5a78f38c.pdf",
      "primary_category": "Neural Combinatorial Optimiztion, Large Language Model, Vehicle Routing Problem",
      "categories": [
        "Neural Combinatorial Optimiztion",
        "Large Language Model",
        "Vehicle Routing Problem"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oY1Xnt83oJ",
      "title": "DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents",
      "authors": [
        "Hao Li",
        "Xiaogeng Liu",
        "CHIU Hung Chun",
        "Dianqi Li",
        "Ning Zhang",
        "Chaowei Xiao"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly central to agentic systems due to their strong reasoning and planning capabilities. By interacting with external environments through predefined tools, these agents can carry out complex user tasks. Nonetheless, this interaction also introduces the risk of prompt injection attacks, where malicious inputs from external sources can mislead the agent’s behavior, potentially resulting in economic loss, privacy leakage, or system compromise.\nSystem-level defenses have recently shown promise by enforcing static or predefined policies, but they still face two key challenges: the ability to dynamically update security rules and the need for memory stream isolation. To address these challenges, we propose DRIFT, a Dynamic Rule-based Isolation Framework for Trustworthy agentic systems, which enforces both control- and data-level constraints.\nA Secure Planner first constructs a minimal function trajectory and a JSON-schema-style parameter checklist for each function node based on the user query. A Dynamic Validator then monitors deviations from the original plan, assessing whether changes comply with privilege limitations and the user's intent. Finally, an \\textit{Injection Isolator} detects and masks any instructions that may conflict with the user query from the memory stream to mitigate long-term risks.\nWe empirically validate the effectiveness of DRIFT on the AgentDojo and ASB benchmark, demonstrating its strong security performance while maintaining high utility across diverse models—showcasing both its robustness and adaptability. The code is released at https://github.com/SaFoLab-WISC/DRIFT.",
      "arxiv_url": "https://openreview.net/forum?id=oY1Xnt83oJ",
      "pdf_url": "https://openreview.net/pdf/7e05f767d463d43be3b045378b14be5760ea2fc1.pdf",
      "primary_category": "Prompt Injection Attack, Defense, LLM Agent",
      "categories": [
        "Prompt Injection Attack",
        "Defense",
        "LLM Agent"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "fleQlZ2VTx",
      "title": "When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners",
      "authors": [
        "Weixiang Zhao",
        "Jiahe Guo",
        "Yang Deng",
        "Tongtong Wu",
        "Wenxuan Zhang",
        "Yulin Hu",
        "Xingyu Sui",
        "Yanyan Zhao",
        "Wanxiang Che",
        "Bing Qin",
        "Tat-Seng Chua",
        "Ting Liu"
      ],
      "abstract": "Multilingual reasoning remains a significant challenge for large language models (LLMs), with performance disproportionately favoring high-resource languages. Drawing inspiration from cognitive neuroscience, which suggests that human reasoning functions largely independently of language processing, we hypothesize that LLMs similarly encode reasoning and language as separable components that can be disentangled to enhance multilingual reasoning. To evaluate this, we perform a causal intervention by ablating language-specific representations at inference time. Experiments on 10 open-weight LLMs spanning 11 typologically diverse languages show that this language-specific ablation consistently boosts multilingual reasoning performance. Layer-wise analyses further confirm that language and reasoning representations can be effectively disentangled throughout the model, yielding improved multilingual reasoning capabilities, while preserving top-layer language features remains essential for maintaining linguistic fidelity. Compared to post-training methods such as supervised fine-tuning or reinforcement learning, our training-free language-reasoning disentanglement achieves comparable or superior results with minimal computational overhead. These findings shed light on the internal mechanisms underlying multilingual reasoning in LLMs and suggest a lightweight and interpretable strategy for improving cross-lingual generalization.",
      "arxiv_url": "https://openreview.net/forum?id=fleQlZ2VTx",
      "pdf_url": "https://openreview.net/pdf/dc1102642c3731185a62af150f47222909c3624e.pdf",
      "primary_category": "Multilingual Reasoning, Large Language Models, Cross-Lingual Generalization",
      "categories": [
        "Multilingual Reasoning",
        "Large Language Models",
        "Cross-Lingual Generalization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "btJUnAPQ7j",
      "title": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework",
      "authors": [
        "Laura Kopf",
        "Nils Feldhus",
        "Kirill Bykov",
        "Philine Lou Bommer",
        "Anna Hedström",
        "Marina MC Höhne",
        "Oliver Eberle"
      ],
      "abstract": "Automated interpretability research aims to identify concepts encoded in neural network features to enhance human understanding of model behavior. Within the context of large language models (LLMs) for natural language processing (NLP), current automated neuron-level feature description methods face two key challenges: limited robustness and the assumption that each neuron encodes a single concept (monosemanticity), despite increasing evidence of polysemanticity. This assumption restricts the expressiveness of feature descriptions and limits their ability to capture the full range of behaviors encoded in model internals. To address this, we introduce Polysemantic FeatuRe Identification and Scoring Method (PRISM), a novel framework specifically designed to capture the complexity of features in LLMs. Unlike approaches that assign a single description per neuron, common in many automated interpretability methods in NLP, PRISM produces more nuanced descriptions that account for both monosemantic and polysemantic behavior. We apply PRISM to LLMs and, through extensive benchmarking against existing methods, demonstrate that our approach produces more accurate and faithful feature descriptions, improving both overall description quality (via a description score) and the ability to capture distinct concepts when polysemanticity is present (via a polysemanticity score).",
      "arxiv_url": "https://openreview.net/forum?id=btJUnAPQ7j",
      "pdf_url": "https://openreview.net/pdf/05d42f8102bfdcdfdff0670dd9ace85d757a7fa4.pdf",
      "primary_category": "Interpretability, Explainable AI, Automated Interpretability",
      "categories": [
        "Interpretability",
        "Explainable AI",
        "Automated Interpretability",
        "Feature Description",
        "Mechanistic Interpretability",
        "Concept-based explanations",
        "Clustering"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kbwSqgVLYj",
      "title": "Reasoning is Periodicity? Improving Large Language Models Through Effective Periodicity Modeling",
      "authors": [
        "Yihong Dong",
        "Ge Li",
        "Xue Jiang",
        "Yongding Tao",
        "Kechi Zhang",
        "Lecheng Wang",
        "Hao Zhu",
        "Huanyu Liu",
        "jiazheng ding",
        "Jia Li",
        "Jinliang Deng",
        "Hong Mei"
      ],
      "abstract": "Periodicity, as one of the most important basic characteristics, lays the foundation for facilitating structured knowledge acquisition and systematic cognitive processes within human learning paradigms. However, the potential flaws of periodicity modeling in Transformer affect the learning efficiency and establishment of underlying principles from data for large language models (LLMs) built upon it. In this paper, we demonstrate that integrating effective periodicity modeling can improve the learning efficiency and performance of LLMs. We introduce FANformer, which adapts Fourier Analysis Network (FAN) into attention mechanism to achieve efficient periodicity modeling, by modifying the feature projection process of attention mechanism. Extensive experimental results on language modeling show that FANformer consistently outperforms Transformer when scaling up model size and training tokens, underscoring its superior learning efficiency. Our pretrained FANformer-1B exhibits marked improvements on downstream tasks compared to open-source LLMs with similar model parameters or training tokens. Moreover, we reveal that FANformer exhibits superior ability to learn and apply rules for reasoning compared to Transformer. The results position FANformer as an effective and promising architecture for advancing LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=kbwSqgVLYj",
      "pdf_url": "https://openreview.net/pdf/36262d4d446a23892b96a648474da8aea8c6980d.pdf",
      "primary_category": "Large Language Model, Periodicity Modeling",
      "categories": [
        "Large Language Model",
        "Periodicity Modeling"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "SXCjvhEQZO",
      "title": "On Optimal Steering to Achieve Exact Fairness",
      "authors": [
        "mohit sharma",
        "Amit Deshpande",
        "Chiranjib Bhattacharyya",
        "Rajiv Ratn Shah"
      ],
      "abstract": "To fix the `bias in, bias out' problem in fair machine learning, it is important to steer feature distributions of data or internal representations of Large Language Models (LLMs) to \\emph{ideal} ones that guarantee group-fair outcomes. Previous work on fair generative models and representation steering could greatly benefit from provable fairness guarantees on the model output. We define a distribution as \\emph{ideal} if the minimizer of any cost-sensitive risk on it is guaranteed to have exact group-fair outcomes (e.g., demographic parity, equal opportunity)---in other words, it has no fairness-utility trade-off. We formulate an optimization program for optimal steering by finding the nearest \\emph{ideal} distribution in KL-divergence, and provide efficient algorithms for it when the underlying distributions come from well-known parametric families (e.g., normal, log-normal).\n\nEmpirically, our optimal steering techniques on both synthetic and real-world datasets improve fairness without diminishing utility (and sometimes even improve utility). We demonstrate affine steering of LLM representations to reduce bias in multi-class classification, e.g., occupation prediction from a short biography in Bios dataset (De-Arteaga et al.). Furthermore, we steer internal representations of LLMs towards desired outputs so that it works equally well across different groups.",
      "arxiv_url": "https://openreview.net/forum?id=SXCjvhEQZO",
      "pdf_url": "https://openreview.net/pdf/4728b468eb1ad668dced2d68c14bdda856a453fe.pdf",
      "primary_category": "steering, fairness, representation",
      "categories": [
        "steering",
        "fairness",
        "representation",
        "bayes optimal classifiers",
        "data bias",
        "ideal distributions"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1C4mXyh31p",
      "title": "UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression",
      "authors": [
        "Chenlong Deng",
        "Zhisong Zhang",
        "Kelong Mao",
        "Shuaiyi Li",
        "Tianqing Fang",
        "Hongming Zhang",
        "Haitao Mi",
        "Dong Yu",
        "Zhicheng Dou"
      ],
      "abstract": "Large language models are increasingly capable of handling long-context inputs, but the memory overhead of KV cache remains a major bottleneck for general-purpose deployment. While many compression strategies have been explored, sequence-level compression is particularly challenging due to its tendency to lose important details. We present UniGist, a gist token-based long context compression framework that removes the need for chunk-wise training, enabling the model to learn how to compress and utilize long-range context during training. To fully exploit the sparsity, we introduce a gist shift trick that transforms the attention layout into a right-aligned block structure and develop a block-table-free sparse attention kernel based on it. UniGist further supports one-pass training and flexible chunk sizes during inference, allowing efficient and adaptive context processing. Experiments across multiple long-context tasks show that UniGist significantly improves compression quality, with especially strong performance in recalling details and long-range dependency modeling.",
      "arxiv_url": "https://openreview.net/forum?id=1C4mXyh31p",
      "pdf_url": "https://openreview.net/pdf/8757228d10c75444816024ec46d5a907c5baeebc.pdf",
      "primary_category": "Long context compression, sparse attention",
      "categories": [
        "Long context compression",
        "sparse attention"
      ],
      "tags": [
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ifsyZYYDNs",
      "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning",
      "authors": [
        "Yuyang Ding",
        "Xinyu Shi",
        "Juntao Li",
        "xiaobo liang",
        "Zhaopeng Tu",
        "Min Zhang"
      ],
      "abstract": "Process reward models (PRMs) offer fine-grained, step-level evaluations that facilitate deeper reasoning processes in large language models (LLMs), proving effective in complex tasks like mathematical reasoning. \nHowever, developing PRMs is challenging due to the high cost and limited scalability of human-annotated data.\nSynthetic data from Monte Carlo (MC) estimation is a promising alternative but suffers from a high noise ratio, which can cause overfitting and hinder large-scale training.\nIn this work, we conduct a preliminary study on the noise distribution in synthetic data from MC estimation, identifying that annotation models tend to both underestimate and overestimate step correctness due to limitations in their annotation capabilities. \nBuilding on these insights, we propose {\\bf S}elf-Denoising Monte {\\bf C}arlo {\\bf An}notation (\\textsc{Scan}), an efficient data synthesis and noise-tolerant learning framework.\nOur key findings indicate that:\n(1) Even lightweight models (e.g., 1.5B parameters) can produce high-quality annotations through self-denoising strategy, enabling PRMs to achieve superior performance with only 6\\% the inference cost required by vanilla MC estimation.\n(2) With our robust learning strategy, PRMs can effectively learn from this weak supervision, achieving a 39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench.\nDespite using only a compact synthetic dataset, our models surpass strong baselines, including those trained on large-scale human-annotated datasets such as PRM800K.\nFurthermore, performance continues to improve as we scale up the synthetic data, highlighting the potential of \\textsc{Scan} for scalable, cost-efficient, and robust PRM training.",
      "arxiv_url": "https://openreview.net/forum?id=ifsyZYYDNs",
      "pdf_url": "https://openreview.net/pdf/90e683c8f88b8fe14fd41acb175f70e1cbcdd305.pdf",
      "primary_category": "LLM Reasoning, Process Supervision, Robust Learning",
      "categories": [
        "LLM Reasoning",
        "Process Supervision",
        "Robust Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pcG6NRJKu7",
      "title": "Finding and Reactivating Post-Trained LLMs' Hidden Safety Mechanisms",
      "authors": [
        "Mingjie Li",
        "Wai Man Si",
        "Michael Backes",
        "Yang Zhang",
        "Yisen Wang"
      ],
      "abstract": "Despite the impressive performance of general-purpose large language models (LLMs), they often require fine-tuning or post-training to excel at specific tasks. \n    For instance, large reasoning models (LRMs), such as the DeepSeek-R1 series, demonstrate strong reasoning capabilities after post-training different general large language models on diverse chain-of-thought (CoT) datasets. \n    However, this additional training frequently comes at the cost of reduced safety, as the fine-tuned or post-trained models tend to exhibit more harmful behaviors compared with the regular LLMs before post-training or fine-tuning, potentially leading to harmful outcomes due to their enhanced capabilities. \n    Taking LRMs as an example, we first investigate the underlying cause of this safety degradation in this paper. \n    Our analysis reveals that post-training can mask the original safety mechanisms of the base LLM, while over-amplifying representations related to their post-training ability. \n    But luckily, we also find that LRMs' safety mechanisms still exist instead of being removed during their post-training. \n    Based on these findings, we propose a lightweight and cost-effective solution called SafeReAct that restores the suppressed safety behaviors by aligning with LoRA adapters on a few layers. Experiments on four state-of-the-art LRMs show that our method significantly improves safety on harmful prompts without compromising reasoning performance. Besides LRMs, additional results on other domain-specific LLMs, like medical models, further confirm the generality and effectiveness of our approach.",
      "arxiv_url": "https://openreview.net/forum?id=pcG6NRJKu7",
      "pdf_url": "https://openreview.net/pdf/ec4527eb7496f613496343238a4333f1bc373dd3.pdf",
      "primary_category": "LLM safety, Alignment",
      "categories": [
        "LLM safety",
        "Alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "isATAFP71B",
      "title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents",
      "authors": [
        "Yifu Guo",
        "Jiaye Lin",
        "Huacan Wang",
        "Yuzhen Han",
        "Sen Hu",
        "Ziyi Ni",
        "Licheng Wang",
        "Mingguang Chen"
      ],
      "abstract": "Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process—agents' interaction trajectory leading to task completion—remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified.",
      "arxiv_url": "https://openreview.net/forum?id=isATAFP71B",
      "pdf_url": "https://openreview.net/pdf/814cd78232da3150c1f91b29920f4f2e4d70fb3c.pdf",
      "primary_category": "LLM-Based Agents, Self-Evolution, Multi-Step Reasoning",
      "categories": [
        "LLM-Based Agents",
        "Self-Evolution",
        "Multi-Step Reasoning"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8RMs5San6e",
      "title": "Representation Consistency for Accurate and Coherent LLM Answer Aggregation",
      "authors": [
        "Junqi Jiang",
        "Tom Bewley",
        "Salim I. Amoukou",
        "Francesco Leofante",
        "Antonio Rago",
        "Saumitra Mishra",
        "Francesca Toni"
      ],
      "abstract": "Test-time scaling improves large language models' (LLMs) performance by allocating more compute budget during inference. To achieve this, existing methods often require intricate modifications to prompting and sampling strategies. In this work, we introduce representation consistency (RC), a test-time scaling method for aggregating answers drawn from multiple candidate responses of an LLM regardless of how they were generated, including variations in prompt phrasing and sampling strategy. RC enhances answer aggregation by not only considering the number of occurrences of each answer in the candidate response set, but also the consistency of the model's internal activations while generating the set of responses leading to each answer. These activations can be either dense (raw model activations) or sparse (encoded via pretrained sparse autoencoders). Our rationale is that if the model's representations of multiple responses converging on the same answer are highly variable, this answer is more likely to be the result of incoherent reasoning and should be down-weighted during aggregation. Importantly, our method only uses cached activations and lightweight similarity computations and requires no additional model queries. Through experiments with four open-source LLMs and four reasoning datasets, we validate the effectiveness of RC for improving task performance during inference, with consistent accuracy improvements (up to 4\\%) over strong test-time scaling baselines. We also show that consistency in the sparse activation signals aligns well with the common notion of coherent reasoning.",
      "arxiv_url": "https://openreview.net/forum?id=8RMs5San6e",
      "pdf_url": "https://openreview.net/pdf/a3ecdca64c27eaba285b6683681268faca747be5.pdf",
      "primary_category": "Test Time Scaling, Large Language Model, Internal Activations",
      "categories": [
        "Test Time Scaling",
        "Large Language Model",
        "Internal Activations"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Idmk7O4sWA",
      "title": "Analyzing Similarity Metrics for Data Selection for Language Model Pretraining",
      "authors": [
        "Dylan Sam",
        "Ayan Chakrabarti",
        "Afshin Rostamizadeh",
        "Srikumar Ramalingam",
        "Gui Citovsky",
        "Sanjiv Kumar"
      ],
      "abstract": "Measuring similarity between training examples is critical for curating high-quality and diverse pretraining datasets for language models. \nHowever, similarity is typically computed with a generic off-the-shelf embedding model that has been trained for tasks such as retrieval. \nWhether these embedding-based similarity metrics are well-suited for pretraining data selection remains largely unexplored.\nIn this paper, we propose a new framework to assess the suitability of a similarity metric specifically for data curation in language model pretraining applications. Our framework's first evaluation criterion captures how well distances reflect generalization in pretraining loss between different training examples. Next, we use each embedding model to guide a standard diversity-based data curation algorithm and measure its utility by pretraining a language model on the selected data and evaluating downstream task performance. Finally, we evaluate the capabilities of embeddings to distinguish between examples from different data sources. With these evaluations, we demonstrate that standard off-the-shelf embedding models are not well-suited for the pretraining data curation setting, underperforming even remarkably simple embeddings that are extracted from models trained on the same pretraining corpus. Our experiments are performed on the Pile, for pretraining a 1.7B parameter language model on 200B tokens. We believe our analysis and evaluation framework serves as a foundation for the future design of embeddings that specifically reason about similarity in pretraining datasets.",
      "arxiv_url": "https://openreview.net/forum?id=Idmk7O4sWA",
      "pdf_url": "https://openreview.net/pdf/68a26e3490bba6549326d3280aefe020685d8b6e.pdf",
      "primary_category": "data curation, language models, pretraining",
      "categories": [
        "data curation",
        "language models",
        "pretraining"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tXxsCbKdQv",
      "title": "Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples",
      "authors": [
        "Shiva Sreeram",
        "Alaa Maalouf",
        "Pratyusha Sharma",
        "Daniela Rus"
      ],
      "abstract": "Recently, Sharma et al. (2024) suggested a method called LAyer- SElective-Rank reduction (LASER) which demonstrated that pruning high‑order components of carefully chosen LLM’s weight matrices can boost downstream accuracy—without any gradient‑based fine‑tuning. Yet LASER’s exhaustive, per‑matrix search (each requiring full‑dataset forward passes) makes it impractical for rapid deployment.  We demonstrate that this overhead can be removed and find that: (i) Only a small, carefully chosen subset of matrices needs to be inspected—eliminating the layer‑by‑layer sweep, (ii) The gradient of each matrix’s singular values pinpoints which matrices merit reduction, (iii) Increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to 24.6 percentage points, and finally, (iv) we discover that evaluating on just 100 samples rather than the full training data—both for computing the indicative gradients and for measuring the final accuracy—suffices to further reduce the search time; we explain that as adaptation to downstream tasks is dominated by prompting style, not dataset size.  As a results, we show that combining these findings yields a fast and robust adaptation algorithm for downstream tasks. Overall, with a single gradient step on 100 examples and a quick scan of the top candidate layers and factorization techniques, we can adapt LLMs to new datasets—entirely without fine‑tuning.",
      "arxiv_url": "https://openreview.net/forum?id=tXxsCbKdQv",
      "pdf_url": "https://openreview.net/pdf/9685026dd634e9aeea13578f8edb6871e651cc9e.pdf",
      "primary_category": "LLMs, adapting language models, efficient search",
      "categories": [
        "LLMs",
        "adapting language models",
        "efficient search",
        "no fine-tuning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "klmc4fwPLd",
      "title": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition",
      "authors": [
        "Ayan Sengupta",
        "Siddhant Chaudhary",
        "Tanmoy Chakraborty"
      ],
      "abstract": "Key-value (KV) cache compression has emerged as a critical technique for reducing the memory and latency overhead of autoregressive language models during inference. Prior approaches predominantly rely on query-key attention scores to rank and evict cached tokens, assuming that attention intensity correlates with semantic importance. However, this heuristic overlooks the contribution of value vectors, which directly influence the attention output. In this paper, we propose CurDKV, a novel, value-centric KV compression method that selects keys and values based on leverage scores computed from CUR matrix decomposition. Our approach approximates the dominant subspace of the attention output $\\mathrm{softmax}(QK^\\top)V$, ensuring that the retained tokens best preserve the model’s predictive behavior. Theoretically, we show that attention score approximation does not guarantee output preservation, and demonstrate that CUR-based selection minimizes end-to-end attention reconstruction loss. Empirically, CurDKV achieves up to $9.6$\\% higher accuracy than state-of-the-art methods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA and Mistral, while maintaining compatibility with FlashAttention and Grouped Query Attention. In addition to improved accuracy, CurDKV reduces generation latency by up to 40\\% at high compression, offering a practical speed-accuracy tradeoff.",
      "arxiv_url": "https://openreview.net/forum?id=klmc4fwPLd",
      "pdf_url": "https://openreview.net/pdf/f12828f6d272d5491dbcabbe3074297e131f7056.pdf",
      "primary_category": "Large Language Models, KV Compression, Attention Compression",
      "categories": [
        "Large Language Models",
        "KV Compression",
        "Attention Compression",
        "Needle-in-a-haystack"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tfbu0ITAez",
      "title": "SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation",
      "authors": [
        "Yanwei Ren",
        "Haotian Zhang",
        "Fuxiang Wu",
        "Jiayan Qiu",
        "Jiaxing Huang",
        "Baosheng Yu",
        "Liu Liu"
      ],
      "abstract": "Enhancing large language models by simply scaling up datasets has begun to yield diminishing returns, shifting the spotlight to data quality. Monte Carlo Tree Search (MCTS) has emerged as a powerful technique for generating high-quality chain-of-thought data, yet conventional approaches typically retain only the top-scoring trajectory from the search tree, discarding sibling nodes that often contain valuable partial insights, recurrent error patterns, and alternative reasoning strategies. This unconditional rejection of non-optimal reasoning branches may waste vast amounts of informative data in the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo Augmentation), a novel framework that reintegrates these discarded sibling nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes along each search path and applies a two-stage refinement: a critique model identifies overlooked strengths and weaknesses across the sibling set, and a revision model conducts text-based backpropagation to refine the top-scoring trajectory in light of this comparative feedback. By recovering and amplifying the underutilized but valuable signals from non-optimal reasoning branches, SIGMA substantially improves reasoning trajectories. On the challenging MATH benchmark, our SIGMA-tuned 7B model achieves 54.92\\% accuracy using only 30K samples, outperforming state-of-the-art models trained on 590K samples. This result highlights that our sibling-guided optimization not only significantly reduces data usage but also significantly boosts LLM reasoning.",
      "arxiv_url": "https://openreview.net/forum?id=tfbu0ITAez",
      "pdf_url": "https://openreview.net/pdf/a978edf76ad31a09fe972e0025c16b74e6292abf.pdf",
      "primary_category": "Large Language Models (LLMs), Monte Carlo Tree Search (MCTS), Data Augmentation",
      "categories": [
        "Large Language Models (LLMs)",
        "Monte Carlo Tree Search (MCTS)",
        "Data Augmentation",
        "Large Language Models (LLMs)",
        "Sibling Guidance"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kpdFjNitGW",
      "title": "un$^2$CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP",
      "authors": [
        "Yinqi Li",
        "Jiahe Zhao",
        "Hong Chang",
        "RuiBing Hou",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "abstract": "Contrastive Language-Image Pre-training (CLIP) has become a foundation model and has been applied to various vision and multimodal tasks. However, recent works indicate that CLIP falls short in distinguishing detailed differences in images and shows suboptimal performance on dense-prediction and vision-centric multimodal tasks. Therefore, this work focuses on improving existing CLIP models, aiming to capture as many visual details in images as possible. We find that a specific type of generative models, unCLIP, provides a suitable framework for achieving our goal. Specifically, unCLIP trains an image generator conditioned on the CLIP image embedding. In other words, it inverts the CLIP image encoder. Compared to discriminative models like CLIP, generative models are better at capturing image details because they are trained to learn the data distribution of images. Additionally, the conditional input space of unCLIP aligns with CLIP's original image-text embedding space. Therefore, we propose to invert unCLIP (dubbed un$^2$CLIP) to improve the CLIP model. In this way, the improved image encoder can gain unCLIP's visual detail capturing ability while preserving its alignment with the original text encoder simultaneously. We evaluate our improved CLIP across various tasks to which CLIP has been applied, including the challenging MMVP-VLM benchmark, the dense-prediction open-vocabulary segmentation task, and multimodal large language model tasks. Experiments show that un$^2$CLIP significantly improves the original CLIP and previous CLIP improvement methods. Code and models are available at https://github.com/LiYinqi/un2CLIP.",
      "arxiv_url": "https://openreview.net/forum?id=kpdFjNitGW",
      "pdf_url": "https://openreview.net/pdf/c1d90a9ec52fd63430dd7e4de39cdbf005d76b76.pdf",
      "primary_category": "CLIP model, unCLIP generative model, model inversion",
      "categories": [
        "CLIP model",
        "unCLIP generative model",
        "model inversion",
        "visual detail capture"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4Q1vA6P9J9",
      "title": "Cascaded Language Models for Cost-Effective Human–AI Decision-Making",
      "authors": [
        "Claudio Fanconi",
        "Mihaela van der Schaar"
      ],
      "abstract": "A challenge in human-AI decision-making is to balance three factors: the *correctness* of predictions, the *cost* of knowledge and reasoning complexity, and the confidence about whether to *abstain* from automated answers or escalate to human experts. In this work, we present a cascaded LLM decision framework that adaptively delegates tasks across multiple tiers of expertise -- a base model for initial candidate answers, a more capable and knowledgeable (but costlier) large model, and a human expert for when the model cascade abstains. Our method proceeds in two stages. First, a deferral policy determines whether to accept the base model’s answer or regenerate it with the large model based on the confidence score. Second, an abstention policy decides whether the cascade model response is sufficiently certain or requires human intervention. Moreover, to overcome static policies and accommodate changing task difficulty, we incorporate an online learning mechanism which uses human feedback. We demonstrate this approach to general question-answering (ARC-Easy, ARC-Challenge, and MMLU) and medical question-answering (MedQA and MedMCQA). Our results demonstrate that our cascaded strategy outperforms single-model baselines in most cases, achieving higher accuracy while reducing costs and providing a principled approach to handling abstentions.",
      "arxiv_url": "https://openreview.net/forum?id=4Q1vA6P9J9",
      "pdf_url": "https://openreview.net/pdf/61f2f43f6f3b2d543ebda4f2791b7738eda4f665.pdf",
      "primary_category": "Multi-LLM, Decision Making, Question Answering",
      "categories": [
        "Multi-LLM",
        "Decision Making",
        "Question Answering",
        "Cascaded LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "gNiT81iag0",
      "title": "TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMs",
      "authors": [
        "Parjanya Prajakta Prashant",
        "Kaustubh Ponkshe",
        "Babak Salimi"
      ],
      "abstract": "As language models scale, their performance improves dramatically across a wide range of tasks, but so does their tendency to memorize and regurgitate parts of their training data verbatim. This tradeoff poses serious legal, ethical, and safety concerns, especially in real-world deployments. Existing mitigation techniques, such as differential privacy or model unlearning, often require retraining or access to internal weights making them impractical for most users. In this work, we introduce TokenSwap, a lightweight, post-hoc defense designed for realistic settings where the user can only access token-level outputs. Our key insight is that while large models are necessary for high task performance, small models (e.g., DistilGPT-2) are often sufficient to assign fluent, grammatically plausible probabilities to common function words - and crucially, they memorize far less. By selectively swapping token probabilities between models, TokenSwap preserves the capabilities of large models while reducing their propensity for verbatim reproduction. Evaluations on Pythia-6.9B and Llama-3-8B show up to a 10$\\times$ drop in exact memorization with negligible task degradation. Our method offers a practical, accessible solution for mitigating memorized generation in deployed LLMs. Code is available at https://github.com/parjanya20/verbatim-llm.",
      "arxiv_url": "https://openreview.net/forum?id=gNiT81iag0",
      "pdf_url": "https://openreview.net/pdf/f7f5c30ebe017ede998fc6eb7af0855376385032.pdf",
      "primary_category": "language models, memorization, copyright",
      "categories": [
        "language models",
        "memorization",
        "copyright",
        "privacy",
        "alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0pbUfmwNTy",
      "title": "DyFlow: Dynamic Workflow Framework for Agentic Reasoning",
      "authors": [
        "Yanbo Wang",
        "Zixiang Xu",
        "Yue Huang",
        "Xiangqi Wang",
        "Zirui Song",
        "Lang Gao",
        "Chenxi Wang",
        "Xiangru Tang",
        "Yue Zhao",
        "Arman Cohan",
        "Xiangliang Zhang",
        "Xiuying Chen"
      ],
      "abstract": "Agent systems based on large language models (LLMs) have shown great potential in complex reasoning tasks, but building efficient and generalizable workflows remains a major challenge. Most existing approaches rely on manually designed processes, which limits their adaptability across different tasks. While a few methods attempt automated workflow generation, they are often tied to specific datasets or query types and make limited use of intermediate feedback, reducing system robustness and reasoning depth. Moreover, their operations are typically predefined and inflexible.\nTo address these limitations, we propose **DyFlow**, a dynamic workflow generation framework that adaptively constructs and adjusts reasoning procedures based on task requirements and real-time intermediate feedback, thereby enhancing cross-task generalization.\nDyFlow consists of two core components: a designer and an executor. The designer decomposes complex problems into a sequence of sub-goals defined by high-level objectives and dynamically plans the next steps based on intermediate outputs and feedback. These plans are then carried out by the executor, which executes each operation using dynamic operators with context-aware parameterization, enabling flexible and semantically grounded reasoning.\nWe systematically evaluate DyFlow across diverse domains, including social reasoning, biomedical tasks, mathematical problem solving, and code generation.\nResults demonstrate that DyFlow significantly outperforms existing baselines, achieving substantial Pass@k improvements and exhibiting robust generalization across diverse domains.",
      "arxiv_url": "https://openreview.net/forum?id=0pbUfmwNTy",
      "pdf_url": "https://openreview.net/pdf/705f64f765b412ab6e17c0dc9c9146763c3e63fe.pdf",
      "primary_category": "Large Language Models, LLM Agent, Workflow Generation",
      "categories": [
        "Large Language Models",
        "LLM Agent",
        "Workflow Generation"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "szBFUtBzWP",
      "title": "TANDEM: Bi-Level Data Mixture Optimization with Twin Networks",
      "authors": [
        "Jiaxing Wang",
        "Deping Xiang",
        "Jin Xu",
        "Mingyang Yi",
        "Guoqiang Gong",
        "Zicheng Zhang",
        "Haoran Li",
        "pengzhang liu",
        "Zhen Chen",
        "Ke Zhang",
        "Ju Fan",
        "Qixia Jiang"
      ],
      "abstract": "The capabilities of large language models (LLMs) significantly depend on training data drawn from various domains. Optimizing domain-specific mixture ratios can be modeled as a bi-level optimization problem, which we simplify into a single-level penalized form and solve with twin networks: a proxy model trained on primary data and a dynamically updated reference model trained with additional data. Our proposed method, Twin Networks for bi-level DatA mixturE optiMization (TANDEM), measures the data efficacy through the difference between the twin models and up-weights domains that benefit more from the additional data. TANDEM provides theoretical guarantees and wider applicability, compared to prior approaches. Furthermore, our bi-level perspective suggests new settings to study domain reweighting such as data-restricted scenarios and supervised fine-tuning, where optimized mixture ratios significantly improve the performance. Extensive experiments validate TANDEM's effectiveness in all scenarios.",
      "arxiv_url": "https://openreview.net/forum?id=szBFUtBzWP",
      "pdf_url": "https://openreview.net/pdf/37c42c1d224077139fee7cc3234e5837d0383950.pdf",
      "primary_category": "data mixture optimization, language models",
      "categories": [
        "data mixture optimization",
        "language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ngxGNQE1M2",
      "title": "Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection",
      "authors": [
        "Herun Wan",
        "Jiaying Wu",
        "Minnan Luo",
        "Zhi Zeng",
        "Zhixiong Su"
      ],
      "abstract": "Misinformation detectors often rely on superficial cues (i.e., shortcuts) that correlate with misinformation in training data but fail to generalize to the diverse and evolving nature of real-world misinformation. This issue is exacerbated by large language models (LLMs), which can easily generate convincing misinformation using simple prompts. We introduce TruthOverTricks, a unified evaluation paradigm for measuring shortcut learning in misinformation detection. TruthOverTricks categorizes shortcut behaviors into intrinsic shortcut induction and extrinsic shortcut injection, and evaluates seven representative detectors across 14 popular benchmarks, along with two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo. Empirical results reveal that existing detectors suffer severe performance degradation when exposed to both naturally occurring and adversarially crafted shortcuts. To address this, we propose the Shortcut Mitigation Framework (SMF), an LLM-augmented data augmentation framework that mitigates shortcut reliance through paraphrasing, factual summarization, and sentiment normalization. SMF consistently enhances robustness across 16 benchmarks, forcing models to rely on deeper semantic understanding rather than shortcut cues.",
      "arxiv_url": "https://openreview.net/forum?id=ngxGNQE1M2",
      "pdf_url": "https://openreview.net/pdf/9c68e8baf3c47644094e482abcc69d6b27b23902.pdf",
      "primary_category": "Shortcut Learning, Injection Attack, Misinformation Detection",
      "categories": [
        "Shortcut Learning",
        "Injection Attack",
        "Misinformation Detection"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Oul46PkP7Z",
      "title": "LeVo: High-Quality Song Generation with Multi-Preference Alignment",
      "authors": [
        "Shun Lei",
        "Yaoxun Xu",
        "ZhiweiLin",
        "Huaicheng Zhang",
        "Wei tan",
        "Hangting Chen",
        "Yixuan Zhang",
        "Chenyu Yang",
        "Haina Zhu",
        "Shuai Wang",
        "Zhiyong Wu",
        "Dong Yu"
      ],
      "abstract": "Recent advances in large language models (LLMs) and audio language models have significantly improved music generation, particularly in lyrics-to-song generation.\nHowever, existing approaches still struggle with the complex composition of songs and the scarcity of high-quality data, leading to limitations in audio quality, musicality, instruction following, and vocal-instrument harmony.\nTo address these challenges, we introduce LeVo, a language model based framework consisting of LeLM and Music Codec.\nLeLM is capable of parallel modeling of two types of tokens: mixed tokens, which represent the combined audio of vocals and accompaniment to achieve better vocal-instrument harmony, and dual-track tokens, which separately encode vocals and accompaniment for high-quality song generation.\nIt employs two decoder-only transformers and a modular extension training strategy to prevent interference between different token types.\nTo further enhance musicality and instruction following ability, we introduce a multi-preference alignment method based on Direct Preference Optimization (DPO).\nThis method handles diverse human preferences through a semi-automatic data construction process and post-training.\nExperimental results demonstrate that LeVo significantly outperforms existing open-source methods in both objective and subjective metrics, while performing competitively with industry systems.\nAblation studies further justify the effectiveness of our designs.\nAudio examples and source code are available at https://levo-demo.github.io and https://github.com/tencent-ailab/songgeneration.",
      "arxiv_url": "https://openreview.net/forum?id=Oul46PkP7Z",
      "pdf_url": "https://openreview.net/pdf/e07b6aa389cb3b793d1f7d655c3db818c9ac1c13.pdf",
      "primary_category": "Song generation, Music generation, Language Model",
      "categories": [
        "Song generation",
        "Music generation",
        "Language Model",
        "Direct Preference Optimization",
        "Multi-Preference Alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "eX0m4qMYVN",
      "title": "ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive",
      "authors": [
        "Xinhao Luo",
        "Zihan Liu",
        "Yangjie Zhou",
        "Shihan Fang",
        "Ziyu Huang",
        "Yu Feng",
        "Chen Zhang",
        "Shixuan Sun",
        "Zhenzhe Zheng",
        "Jingwen Leng",
        "Minyi Guo"
      ],
      "abstract": "Large language model (LLM) decoding suffers from high latency due to fragmented execution across operators and heavy reliance on off-chip memory for data exchange and reduction. \nThis execution model limits opportunities for fusion and incurs significant memory traffic and kernel launch overhead.\nWhile modern architectures such as NVIDIA Hopper provide distributed shared memory and low-latency intra-cluster interconnects, they expose only low-level data movement instructions, lacking structured abstractions for collective on-chip communication.\nTo bridge this software-hardware gap, we introduce two cluster-level communication primitives, ClusterReduce and ClusterGather, which abstract common communication patterns and enable structured, high-speed data exchange and reduction between thread blocks within a cluster, allowing intermediate results to be on-chip without involving off-chip memory.\nBuilding on these abstractions, we design ClusterFusion, an execution framework that schedules communication and computation jointly to expand operator fusion scope by composing decoding stages such as QKV Projection, Attention, and Output Projection into a single fused kernels.\nEvaluations on H100 GPUs show that ClusterFusion outperforms state-of-the-art inference frameworks by $1.61\\times$ on average in end-to-end latency across different models and configurations.",
      "arxiv_url": "https://openreview.net/forum?id=eX0m4qMYVN",
      "pdf_url": "https://openreview.net/pdf/5de12982204c2d61c6ccd843e55fba210ab885df.pdf",
      "primary_category": "Operator Fusion, LLM Inference, Inter-core Interconnect",
      "categories": [
        "Operator Fusion",
        "LLM Inference",
        "Inter-core Interconnect"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7mTECPRtll",
      "title": "Mitigating Hallucination in VideoLLMs via Temporal-Aware Activation Engineering",
      "authors": [
        "Jianfeng Cai",
        "Jiale Hong",
        "Zongmeng Zhang",
        "Wengang Zhou",
        "zhannianji",
        "Houqiang Li"
      ],
      "abstract": "Multimodal large language models (MLLMs) have achieved remarkable progress in video understanding. However, hallucination, where the model generates plausible yet incorrect outputs, persists as a significant and under-addressed challenge in the video domain. Among existing solutions, activation engineering has proven successful in mitigating hallucinations in LLMs and ImageLLMs, yet its applicability to VideoLLMs remains largely unexplored. In this work, we are the first to systematically investigate the effectiveness and underlying mechanisms of activation engineering for mitigating hallucinations in VideoLLMs. We initially conduct an investigation of the key factors affecting the performance of activation engineering and find that a model’s sensitivity to hallucination depends on $\\textbf{temporal variation}$ rather than task type. Moreover, selecting appropriate internal modules and dataset for activation engineering is critical for reducing hallucination. Guided by these findings, we propose a temporal-aware activation engineering framework for VideoLLMs, which adaptively identifies and manipulates hallucination-sensitive modules based on the temporal variation characteristic, substantially mitigating hallucinations without additional LLM fine-tuning. Experiments across multiple models and benchmarks demonstrate that our method markedly reduces hallucination in VideoLLMs, thereby validating the robustness of our findings.",
      "arxiv_url": "https://openreview.net/forum?id=7mTECPRtll",
      "pdf_url": "https://openreview.net/pdf/b90701691517fc02e9dab9ed9ab7122118b843b0.pdf",
      "primary_category": "Multimodal Large Language Model, Hallucination, Activation Engineering",
      "categories": [
        "Multimodal Large Language Model",
        "Hallucination",
        "Activation Engineering"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "b7uniOw0sZ",
      "title": "Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions",
      "authors": [
        "Siqi Kou",
        "Qingyuan Tian",
        "Hanwen Xu",
        "Zihao Zeng",
        "Zhijie Deng"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning capabilities in math and coding, often bolstered by post-training on the chain-of-thoughts (CoTs) generated by stronger models. \nHowever, existing strategies for curating such training data predominantly rely on heuristics, limiting generalizability and failing to capture subtleties underlying in data. \nTo address these limitations, we leverage influence functions to systematically attribute LLMs' reasoning ability on math and coding to individual training examples, sequences, and tokens, enabling deeper insights into effective data characteristics.\nOur Influence-based Reasoning Attribution (Infra) uncovers nontrivial cross-domain effects across math and coding tasks: high-difficulty math examples improve both math and code reasoning, while low-difficulty code tasks most effectively benefit code reasoning.\nBased on these findings, we introduce a simple yet effective dataset reweighting strategy by flipping task difficulty, which doubles AIME24 accuracy from 10\\% to 20\\% and boosts LiveCodeBench accuracy from 33.8\\% to 35.3\\% for Qwen2.5-7B-Instruct.\nMoreover, our fine-grained attribution reveals that the sequence-level exploratory behaviors enhance reasoning performance in both math and code, and the token-level influence patterns are distinct for math and code reasoning: the former prefers natural language logic connectors and the latter emphasizes structural syntax.",
      "arxiv_url": "https://openreview.net/forum?id=b7uniOw0sZ",
      "pdf_url": "https://openreview.net/pdf/d23cc441a12062449c736658c9bf12018208dae0.pdf",
      "primary_category": "influence functions, llm reasoning",
      "categories": [
        "influence functions",
        "llm reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "YJx8AofTF5",
      "title": "Program Synthesis via Test-Time Transduction",
      "authors": [
        "Kang-il Lee",
        "Jahyun Koo",
        "Seunghyun Yoon",
        "Minbeom Kim",
        "Hyukhun Koh",
        "Dongryeol Lee",
        "Kyomin Jung"
      ],
      "abstract": "We introduce transductive program synthesis, a new formulation of the program synthesis task that explicitly leverages test inputs during synthesis. While prior approaches to program synthesis--whether based on natural language descriptions or input-output examples--typically aim to generalize from training examples, they often struggle with robustness, especially in real-world settings where training examples are limited and test inputs involve various edge cases. To address this, we propose a novel framework that improves robustness by treating synthesis as an active learning over a finite hypothesis class defined by programs' outputs. We use an LLM to predict outputs for selected test inputs and eliminate inconsistent hypotheses, where the inputs are chosen via a greedy maximin algorithm to minimize the number of LLM queries required. We evaluate our approach on four benchmarks: Playgol, MBPP+, 1D-ARC, and programmatic world modeling on MiniGrid. We demonstrate that our method significantly improves program synthesis in both accuracy and efficiency. We release our code at https://github.com/klee972/SYNTRA.",
      "arxiv_url": "https://openreview.net/forum?id=YJx8AofTF5",
      "pdf_url": "https://openreview.net/pdf/278cd0ba528692cd68a38d8a8e76e60022e19d82.pdf",
      "primary_category": "program synthesis, code generation, large language models",
      "categories": [
        "program synthesis",
        "code generation",
        "large language models",
        "world models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "p3HBEtNDRY",
      "title": "Streaming Attention Approximation via Discrepancy Theory",
      "authors": [
        "Ekaterina Kochetkova",
        "Kshiteej Sheth",
        "Insu Han",
        "Amir Zandieh",
        "Michael Kapralov"
      ],
      "abstract": "Large language models (LLMs) have achieved impressive success, but their high memory requirements present challenges for long-context token generation. In this paper we study the streaming complexity of attention approximation, a key computational primitive underlying token generation. \n  \nOur main contribution is BalanceKV, a streaming algorithm for $\\epsilon$-approximating attention computations based on geometric process for selecting a balanced collection of Key and Value tokens as per Banaszczyk's vector balancing theory. We complement our algorithm with space lower bounds for streaming attention computation. Besides strong theoretical guarantees, BalanceKV exhibits empirically validated performance improvements over existing methods, both for attention approximation and end-to-end performance on various long context benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=p3HBEtNDRY",
      "pdf_url": "https://openreview.net/pdf/23692c44fc255241c700b7ce8ba92d7657eb56ad.pdf",
      "primary_category": "Attention approximation, Long-context Attention, Large Language Models",
      "categories": [
        "Attention approximation",
        "Long-context Attention",
        "Large Language Models",
        "Discrepancy theory",
        "Self-Balancing Walk"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "b2IU6QOOfo",
      "title": "PaZO: Preconditioned Accelerated Zeroth-Order Optimization for Fine-Tuning LLMs",
      "authors": [
        "Hanzhen Zhao",
        "Shihong Ding",
        "Cong Fang",
        "Zhouchen Lin"
      ],
      "abstract": "This paper introduces PaZO, a preconditioned accelerated zeroth-order optimization algorithm for fine-tuning large language models (LLMs).      First, we theoretically demonstrate the necessity of preconditioning in zeroth-order optimization, proving that zeroth-order stochastic gradient descent (ZO-SGD) alone fails to achieve the ideal convergence rate.     Building on this, we propose a Preconditioned Simultaneous Perturbation Stochastic Approximation (PSPSA) and theoretical version of PaZO, and demonstrate that setting the order of preconditioner as $-1/2$ in PSPSA yields the improved convergence rate for PaZO.     Moreover, we design a practical version of PaZO that stabilizes training via diagonal Hessian estimate and moving average technique.  Extensive experiments on diverse downstream tasks with models like RoBERTa-large and OPT show PaZO’s effectiveness.  Compared to other zeroth-order baselines, PaZO achieves better performance across models and tasks.",
      "arxiv_url": "https://openreview.net/forum?id=b2IU6QOOfo",
      "pdf_url": "https://openreview.net/pdf/073c9e2b5a02c70ad2ed44b6b1ad1bf6d1b647ad.pdf",
      "primary_category": "zeroth-order optimization, fine-tuning LLMs, preconditioner",
      "categories": [
        "zeroth-order optimization",
        "fine-tuning LLMs",
        "preconditioner"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xud9JYzgSp",
      "title": "Unlocking SLM Potential for Data Analysis Code Generation via Non-Parametric Knowledge Distillation",
      "authors": [
        "Jinyang Li",
        "Jack Williams",
        "Nick McKenna",
        "Arian Askari",
        "Nicholas Wilson",
        "Reynold Cheng"
      ],
      "abstract": "Knowledge distillation from Large Language Models (LLMs) to locally hosted Small Language Models (SLMs) provides advantages for Data Analysis Code Generation (DACG) such as privacy protection. However, achieving effective distillation without resource-intensive training is challenging. This paper investigates whether LLMs can distill knowledge to SLMs through In-Context Learning (ICL), a training-free method for rapid task adaptation. We present the DarGO: Distillation and Adaptive Reasoning-Guided Orchestration framework, which facilitates automatic knowledge distillation from LLMs to SLMs. DarGO consists of three phases: exploration through an Model Orchestration Interface (MOI), Memory Collection of successful trajectories, and Knoweldge-driven Inference. We evaluate DarGO on three challenging DACG benchmarks (WikiTQ, TabMWP, and Bird-SQL), each with in-domain training sets that enable detailed analysis of knowledge distillation effectiveness. DarGO demonstrates a substantial relative performance improvement of 27.5\\% on average for the student SLMs. To further observe generalization capabilities, we evaluate the \\method across different teacher-student model combinations, knowledge transfer scenarios, and unified memory approaches for more advanced, test-only data analysis tasks. Our findings contribute a novel perspective on distillation methods that enhance high performance for SLMs while avoiding intensive fine-tuning.",
      "arxiv_url": "https://openreview.net/forum?id=xud9JYzgSp",
      "pdf_url": "https://openreview.net/pdf/e30e3383e931972550f2f4b4dd2057b4af99583d.pdf",
      "primary_category": "LLM, Knowledge Distillation, Data Code Generation",
      "categories": [
        "LLM",
        "Knowledge Distillation",
        "Data Code Generation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9odue1eopm",
      "title": "HypoBootstrap: A Bootstrapping Framework for Inductive Reasoning",
      "authors": [
        "Si Chen",
        "Yifei Li",
        "Richong Zhang"
      ],
      "abstract": "Inductive reasoning infers general rules from observed evidence, which is one of the most critical intelligence abilities.\nPrevious works have succeeded in formal languages but suffer from onerous and error-prone conversions between a particular formal language and the working language.\nAs large language models (LLMs) have emerged, direct reasoning with various kinds of languages, especially natural languages, without formal language involvement has become feasible.\nHowever, existing LLM-based inductive reasoning usually relies on LLM's intrinsic generation ability, which is prone to LLM's hallucination and lacks systematic guidance according to the nature of inductive reasoning.\nTo this end, we propose HypoBootstrap, an integrated framework for inductive reasoning that generates and confirms hypotheses both in a bootstrapping manner.\nRegarding hypothesis generation, we propose a novel bootstrapping generation strategy, bootstrapping object hypotheses, relational hypotheses, and functional hypotheses successively, which assists LLM in observing the evidence from trivial patterns to non-trivial patterns.\nRegarding hypothesis confirmation, we utilize Glymour's theory of bootstrap confirmation, a hypothesis confirmation theory from the philosophy of science that can confirm a set of hypotheses.\nWe use its principles to confirm the object hypotheses, relational hypotheses, and functional hypotheses.\nEmpirical studies on four inductive reasoning scenarios of different natures, involving causal induction, concept learning, grammar learning, and abstract reasoning, demonstrate that HypoBootstrap significantly outperforms existing methods.",
      "arxiv_url": "https://openreview.net/forum?id=9odue1eopm",
      "pdf_url": "https://openreview.net/pdf/c7ebbb93a04ff7d9eb9802253b2d5eb14d7e9f81.pdf",
      "primary_category": "inductive reasoning, inductive logic, hypothesis generation",
      "categories": [
        "inductive reasoning",
        "inductive logic",
        "hypothesis generation",
        "hypothesis confirmation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XP3v1THxsq",
      "title": "Among Us: A Sandbox for Measuring and Detecting Agentic Deception",
      "authors": [
        "Satvik Golechha",
        "Adrià Garriga-Alonso"
      ],
      "abstract": "Prior studies on deception in language-based AI agents typically assess whether the agent produces a false statement about a topic, or makes a binary choice prompted by a goal, rather than allowing open-ended deceptive behavior to emerge in pursuit of a longer-term goal.\nTo fix this, we introduce $\\textit{Among Us}$, a sandbox social deception game where LLM-agents exhibit long-term, open-ended deception as a consequence of the game objectives. While most benchmarks saturate quickly, $\\textit{Among Us}$ can be expected to last much longer, because it is a multi-player game far from equilibrium.\nUsing the sandbox, we evaluate $18$ proprietary and open-weight LLMs and uncover a general trend:\nmodels trained with RL are comparatively much better at producing deception than detecting it.\nWe evaluate the effectiveness of methods to detect lying and deception: logistic regression on the activations and sparse autoencoders (SAEs). We find that probes trained on a dataset of ``pretend you're a dishonest model: $\\dots$'' generalize extremely well out-of-distribution, consistently obtaining AUROCs over 95% even when evaluated just on the deceptive statement, without the chain of thought. We also find two SAE features that work well at deception detection but are unable to steer the model to lie less.\nWe hope our open-sourced sandbox, game logs, and probes serve to anticipate and mitigate deceptive behavior and capabilities in language-based agents.",
      "arxiv_url": "https://openreview.net/forum?id=XP3v1THxsq",
      "pdf_url": "https://openreview.net/pdf/0ff014da0c71915ceb0a84e3977c85eaf1e134dd.pdf",
      "primary_category": "Deception, Social Games, AI Safety",
      "categories": [
        "Deception",
        "Social Games",
        "AI Safety",
        "Linear Probes",
        "SAEs"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yRxXTdElLv",
      "title": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications",
      "authors": [
        "Jinyang Li",
        "Xiaolong Li",
        "Ge Qu",
        "Per Jacobsson",
        "Bowen Qin",
        "Binyuan Hui",
        "Shuzheng Si",
        "Nan Huo",
        "Xiaohan Xu",
        "Yue Zhang",
        "Ziwei Tang",
        "Yuanshuai Li",
        "Florensia Widjaja",
        "Xintong Zhu",
        "Feige Zhou",
        "Yongfeng Huang",
        "Yannis Papakonstantinou",
        "Fatma Ozcan",
        "Chenhao Ma",
        "Reynold Cheng"
      ],
      "abstract": "Resolution of complex SQL issues persists as a significant bottleneck in real-world database applications. Current Large Language Models (LLMs), while adept at text-to-SQL translation, have not been rigorously evaluated on the more challenging task of debugging on SQL issues. In order to address this gap, we introduce **BIRD-CRITIC**, a new SQL issue debugging benchmark comprising 530 carefully curated PostgreSQL tasks (**BIRD-CRITIC-PG**) and 570 multi-dialect tasks (**BIRD-CRITIC-Multi**), which are distilled from authentic user issues and replayed within new environments to facilitate rigorous and contamination-free evaluation. Baseline evaluations on BIRD-CRITIC underscore the task's complexity, with the leading reasoning model **O3-Mini** achieving only 38.87% success rate on **BIRD-CRITIC-PG** and 33.33% on **BIRD-CRITIC-Multi**. Meanwhile, realizing open-source models for database tasks is crucial which can empower local development while safeguarding data privacy. Therefore, we present **Six-Gym** (**S**ql-f**IX**-Gym), a training environment for elevating the capabilities of open-source models specifically for SQL issue debugging. This environment leverages **SQL-Rewind** strategy, which automatically generates executable issue-solution datasets by reverse-engineering issues from verified SQLs. However, popular trajectory-based fine-tuning methods do not explore substantial supervisory signals. We further propose *f*-Plan Boosting, which extracts high-level debugging plans automatically from SQL solutions, enabling the teacher LLMs to harvest and produce 73.7% more successful trajectories for training. We integrate these components into an open-source agent, **BIRD-Fixer**. Based on Qwen-2.5-Coder-14B, **BIRD-Fixer** raises its success rate to 38.11% on **BIRD-CRITIC-PG** and 29.65% on **BIRD-CRITIC-Multi**, surpassing many leading proprietary models such as Claude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing sophisticated SQL-debugging capabilities for both research and industry.",
      "arxiv_url": "https://openreview.net/forum?id=yRxXTdElLv",
      "pdf_url": "https://openreview.net/pdf/5e92e69b2f1cacc9dbecc27c55caea3b13a356f6.pdf",
      "primary_category": "text-to-SQL, LLM, SQL issues",
      "categories": [
        "text-to-SQL",
        "LLM",
        "SQL issues"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oeZZusZheP",
      "title": "AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling",
      "authors": [
        "Zhining Zhang",
        "Chuanyang Jin",
        "Mung Yao Jia",
        "Shunchi Zhang",
        "Tianmin Shu"
      ],
      "abstract": "Theory of Mind (ToM), the ability to understand people's minds based on their behavior, is key to developing socially intelligent agents. Current approaches to ToM reasoning either rely on prompting Large Language Models (LLMs), which are prone to systematic errors, or use handcrafted, rigid agent models for model-based inference, which are more robust but fail to generalize across domains. In this work, we introduce *AutoToM*, an automated agent modeling method for scalable, robust, and interpretable mental inference. Given a ToM problem, *AutoToM* first proposes an initial agent model and then performs automated Bayesian inverse planning based on this model, leveraging an LLM backend. Guided by inference uncertainty, it iteratively refines the model by introducing additional mental variables and/or incorporating more timesteps in the context. Across five diverse benchmarks, *AutoToM* outperforms existing ToM methods and even large reasoning models. Additionally, we show that *AutoToM* can produce human‐like confidence estimates and enable online mental inference for embodied decision-making.",
      "arxiv_url": "https://openreview.net/forum?id=oeZZusZheP",
      "pdf_url": "https://openreview.net/pdf/97d508cdb6040e0326e1f3e82a7473020de10424.pdf",
      "primary_category": "Theory of Mind, Agent Modeling, Bayesian Inverse Planning",
      "categories": [
        "Theory of Mind",
        "Agent Modeling",
        "Bayesian Inverse Planning",
        "Large Language Models",
        "Cognitive Modeling"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "CyUq9D99vE",
      "title": "ParamMute: Suppressing Knowledge-Critical FFNs for Faithful Retrieval-Augmented Generation",
      "authors": [
        "Pengcheng Huang",
        "Zhenghao Liu",
        "Yukun Yan",
        "Haiyan Zhao",
        "Xiaoyuan Yi",
        "Hao Chen",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Tong Xiao",
        "Ge Yu",
        "Chenyan Xiong"
      ],
      "abstract": "Large language models (LLMs) integrated with retrieval-augmented generation (RAG) have improved factuality by grounding outputs in external evidence. However, they remain susceptible to unfaithful generation, where outputs contradict retrieved context despite its relevance and accuracy. Existing approaches aiming to improve faithfulness primarily focus on enhancing the utilization of external context, but often overlook the persistent influence of internal parametric knowledge during generation. In this work, we investigate the internal mechanisms behind unfaithful generation and identify a subset of mid-to-deep feed-forward networks (FFNs) that are disproportionately activated in such cases. Building on this insight, we propose Parametric Knowledge Muting through FFN Suppression (ParamMute), a framework that improves contextual faithfulness by suppressing the activation of unfaithfulness-associated FFNs and calibrating the model toward retrieved knowledge. To evaluate our approach, we introduce CoFaithfulQA, a benchmark specifically designed to evaluate faithfulness in scenarios where internal knowledge conflicts with accurate external evidence. Experimental results show that ParamMute significantly enhances faithfulness across both CoFaithfulQA and the established ConFiQA benchmark, achieving substantial reductions in reliance on parametric memory. These findings underscore the importance of mitigating internal knowledge dominance and provide a new direction for improving LLM trustworthiness in RAG. All codes are available at https://github.com/OpenBMB/ParamMute.",
      "arxiv_url": "https://openreview.net/forum?id=CyUq9D99vE",
      "pdf_url": "https://openreview.net/pdf/bca799c833bbec6384fa6afb312ccb9ef981e32b.pdf",
      "primary_category": "contextual faithfulness, retrieval-augmented generation",
      "categories": [
        "contextual faithfulness",
        "retrieval-augmented generation"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7EjdHkOwc4",
      "title": "PANGEA: Projection-Based Augmentation with Non-Relevant General Data for Enhanced Domain Adaptation in LLMs",
      "authors": [
        "Seungyoo Lee",
        "Giung Nam",
        "Moonseok Choi",
        "Hyungi Lee",
        "Juho Lee"
      ],
      "abstract": "Modern large language models (LLMs) achieve competitive performance across a wide range of natural language processing tasks through zero-shot or few-shot prompting. However, domain-specific tasks often still require fine-tuning, which is frequently hindered by data scarcity, i.e., collecting sufficient domain-specific data remains a practical challenge. A widely adopted solution is to generate synthetic data using LLMs by augmenting a small set of available domain-specific examples. In this work, we first identify fundamental limitations of such approach in terms of both data diversity and quality, particularly when relying on only a handful of domain-specific examples. We then propose our method, PANGEA, which leverages large-scale, publicly available general-purpose data---entirely unrelated to the target domain---to generate more diverse and higher-quality synthetic data. Our extensive experiments on domain-specific benchmarks, including GSM8K, MedQA, and FinQA, as well as a custom domain-specific language task, validate the effectiveness of our approach.",
      "arxiv_url": "https://openreview.net/forum?id=7EjdHkOwc4",
      "pdf_url": "https://openreview.net/pdf/3d612799b2d554a0e751a107b3bb08f11c70a030.pdf",
      "primary_category": "synthetic data generation, large language models, domain adaptation",
      "categories": [
        "synthetic data generation",
        "large language models",
        "domain adaptation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "r5tbCL9vAZ",
      "title": "GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling",
      "authors": [
        "Tianhao Chen",
        "Xin Xu",
        "Zijing Liu",
        "Pengxiang Li",
        "Xinyuan Song",
        "AJAY KUMAR JAISWAL",
        "Fan Zhang",
        "Jishan Hu",
        "Yang Wang",
        "Hao Chen",
        "Shizhe Diao",
        "Shiwei Liu",
        "Yu Li",
        "Lu Yin",
        "Can Yang"
      ],
      "abstract": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series, predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While being stable during pretraining and scalable to large model sizes, Pre-LN suffers from an exponential growth in activation variance across layers, causing the shortcut to dominate over sub-layer outputs in the residual connection and limiting the learning capacity of deeper layers. To mitigate this issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be used in combination with existing approaches. GPAS works by scaling down the intermediate activations while keeping their gradients unchanged. This leaves information in the activations intact, and avoids the gradient vanishing problem associated with gradient downscaling. Extensive experiments across various model sizes from 71M to 1B show that GPAS achieves consistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows promise in improving alternative architectures such as Sandwich-LN and DeepNorm, demonstrating its versatility and potential for improving training dynamics in a wide range of settings. Our code is available at https://github.com/dandingsky/GPAS.",
      "arxiv_url": "https://openreview.net/forum?id=r5tbCL9vAZ",
      "pdf_url": "https://openreview.net/pdf/86ac8dbe9f13c0ff02d636f7741a152447094fe9.pdf",
      "primary_category": "Large Language Models, Pretraining, Normalization",
      "categories": [
        "Large Language Models",
        "Pretraining",
        "Normalization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "NcxmgX95ue",
      "title": "URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training",
      "authors": [
        "Dongyang Fan",
        "Vinko Sabolčec",
        "Martin Jaggi"
      ],
      "abstract": "Large Language Models (LLMs) are commonly pretrained on vast corpora of text without utilizing contextual metadata such as source, quality, or topic, leading to a context-free learning paradigm. While recent studies suggest that adding metadata like URL information as context (i.e., auxiliary inputs not used in the loss calculation) can improve training efficiency and downstream performance, they offer limited understanding of which types of metadata are truly effective and under what conditions. In this work, we conduct a systematic evaluation and  find that not all metadata types contribute equally. Only URL context speeds up training, whereas quality scores and topic/format domain information offer no clear benefit. Furthermore, the improved downstream performances of URL conditioning emerge only when longer prompts are used at inference time. In addition, we demonstrate that context-aware pretraining enables more controllable generation than context-free pretraining, in a classifier-free guidance fashion. Although topic and format metadata do not accelerate training, they are effective for steering outputs, offering human-interpretable control over generation.",
      "arxiv_url": "https://openreview.net/forum?id=NcxmgX95ue",
      "pdf_url": "https://openreview.net/pdf/0204b8a4f4af262a098c69cc46a486c59b64efcf.pdf",
      "primary_category": "Context-aware pretraining, LLM-pretraining, context guidance",
      "categories": [
        "Context-aware pretraining",
        "LLM-pretraining",
        "context guidance",
        "controllable generation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZDpPfg9pDc",
      "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding",
      "authors": [
        "Pei-Shuo Wang",
        "Jian-Jia Chen",
        "Chun-Che Yang",
        "Chi-Chih Chang",
        "Ning-Chi Huang",
        "Mohamed S. Abdelfattah",
        "Kai-Chiang Wu"
      ],
      "abstract": "The immense model sizes of large language models (LLMs) challenge deployment on memory-limited consumer GPUs.\n    Although model compression and parameter offloading are common strategies to address memory limitations, compression can degrade quality, and offloading maintains quality but suffers from slow inference.\n    Speculative decoding presents a promising avenue to accelerate parameter offloading, utilizing a fast draft model to propose multiple draft tokens, which are then verified by the target LLM in parallel with a single forward pass. This method reduces the time-consuming data transfers in forward passes that involve offloaded weight transfers.\n    Existing methods often rely on pretrained weights of the same family, but require additional training to align with custom-trained models. Moreover, approaches that involve draft model training usually yield only modest speedups. This limitation arises from insufficient alignment with the target model, preventing higher token acceptance lengths.\n    To address these challenges and achieve greater speedups, we propose SubSpec, a plug-and-play method to accelerate parameter offloading that is lossless and training-free. SubSpec constructs a highly aligned draft model by generating low-bit quantized substitute layers from offloaded target LLM portions. Additionally, our method shares the remaining GPU-resident layers and the KV-Cache, further reducing memory overhead and enhance alignment.\n    SubSpec achieves a high average acceptance length, delivering 9.1$\\times$ speedup for Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5$\\times$ speedup for Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
      "arxiv_url": "https://openreview.net/forum?id=ZDpPfg9pDc",
      "pdf_url": "https://openreview.net/pdf/98342d215ec3c042e7d15c061d02b4086e670e2b.pdf",
      "primary_category": "speculative decoding, offloading, quantization",
      "categories": [
        "speculative decoding",
        "offloading",
        "quantization",
        "compression",
        "large language models",
        "inference"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zJSZupQ889",
      "title": "SALS: Sparse Attention in Latent Space for KV Cache Compression",
      "authors": [
        "Junlin Mu",
        "Hantao Huang",
        "Jihang Zhang",
        "Minghui Yu",
        "Tao Wang",
        "Yidong Li"
      ],
      "abstract": "Large Language Models (LLMs) capable of handling extended contexts are in high demand, yet their inference remains challenging due to substantial Key-Value (KV) cache size and high memory bandwidth requirements. Previous research has demonstrated that KV cache exhibits low-rank characteristics within the hidden dimension, suggesting the potential for effective compression. However, due to the widely adopted Rotary Position Embedding (RoPE) mechanism in modern LLMs, naive low‑-rank compression suffers severe accuracy degradation or creates a new speed bottleneck, as the low-rank cache must first be reconstructed in order to apply RoPE. In this paper, we introduce two key insights: first, the application of RoPE to the key vectors increases their variance, which in turn results in a higher rank; second, after the key vectors are transformed into the latent space, they largely maintain their representation across most layers. Based on these insights, we propose the Sparse Attention in Latent Space (SALS) framework. SALS projects the KV cache into a compact latent space via low-rank projection, and performs sparse token selection using RoPE-free query--key interactions in this space. By reconstructing only a small subset of important tokens, it avoids the overhead of full KV cache reconstruction. We comprehensively evaluate SALS on various tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and additionally verify its scalability on the RULER-128k benchmark with LLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA performance by maintaining competitive accuracy. Under different settings, SALS achieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention operator compared to FlashAttention2 on the 4K sequence. For the end-to-end throughput performance, we achieves 1.4-fold and 4.5-fold improvement compared to GPT-fast on 4k and 32K sequences, respectively. The source code will be publicly available in the future.",
      "arxiv_url": "https://openreview.net/forum?id=zJSZupQ889",
      "pdf_url": "https://openreview.net/pdf/d17d4a4712ffe8294045a703d5784a0c049a1da2.pdf",
      "primary_category": "KV Cache Compression; Sparse attention; Low-rank projection",
      "categories": [
        "KV Cache Compression; Sparse attention; Low-rank projection"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "T6d5IYr8PJ",
      "title": "Certifying Deep Network Risks and Individual Predictions with PAC-Bayes Loss via Localized Priors",
      "authors": [
        "Wen Dong"
      ],
      "abstract": "As machine learning increasingly relies on large, opaque foundation models powering generative and agentic AI, deploying these systems in safety-critical settings demands rigorous guarantees on their generalization beyond training data. PAC-Bayes theory offers principled certificates linking training performance to generalization risk, yet existing approaches are rarely practical: simple theoretical priors yield vacuous bounds, while data-dependent priors trained separately are computationally costly or introduce bias. To bridge this fundamental gap, we propose a localized PAC-Bayes prior—a structured, computationally efficient prior softly concentrated near parameters favored during standard training, enabling effective exploration without costly data splits. By integrating this localized prior directly into standard training loss, we produce practically tight generalization certificates without workflow disruption. Theoretically, under standard neural tangent kernel assumptions, our bound shrinks as networks widen and datasets grow, becoming negligible in practical regimes. Empirically, we certify generalization across image classification, NLP fine-tuning, and semantic segmentation, typically within three percentage points of test errors at ImageNet scale, while providing rigorous guarantees for individual predictions, selective rejection, and robustness.",
      "arxiv_url": "https://openreview.net/forum?id=T6d5IYr8PJ",
      "pdf_url": "https://openreview.net/pdf/8e68876776135a21a8c1d64d587cb43d2e7b340f.pdf",
      "primary_category": "PAC-Bayes generalization, Data-dependent bounds, Bayesian neural networks",
      "categories": [
        "PAC-Bayes generalization",
        "Data-dependent bounds",
        "Bayesian neural networks",
        "Adversarial robustness",
        "Model calibration",
        "Trustworthy AI"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qeL8fi8GS7",
      "title": "Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval",
      "authors": [
        "Constantin Venhoff",
        "Ashkan Khakzar",
        "Sonia Joseph",
        "Philip Torr",
        "Neel Nanda"
      ],
      "abstract": "Training vision language models (VLMs) aims to align visual representations from a vision encoder with the textual representations of a pretrained large language model (LLM). However, many VLMs exhibit reduced factual recall performance compared to their LLM backbones, raising the question of how effective multimodal fine-tuning is at extending existing mechanisms within the LLM to visual inputs. We argue that factual recall based on visual inputs requires VLMs to solve a two-hop problem: (1) forming entity representations from visual inputs, and (2) recalling associated factual knowledge based on these entity representations. By benchmarking 14 VLMs with various architectures (LLaVA, Native, Cross-Attention), sizes (7B-124B parameters), and training setups on factual recall tasks against their original LLM backbone models, we find that 11 of 14 models exhibit factual recall degradation. We select three models exhibiting high- and two models with low performance degradation, and use attribution patching, activation patching, and probing to show that degraded VLMs struggle to use the existing factual recall circuit of their LLM backbone, because they resolve the first hop too late in the computation. In contrast, high-performing VLMs resolve entity representations early enough to reuse the existing factual recall mechanism. Finally, we demonstrate two methods to recover performance: patching entity representations from the LLM backbone into the VLM, and prompting with chain-of-thought reasoning. Our results highlight that the speed of early entity resolution critically determines how effective VLMs are in using preexisting LLM mechanisms. More broadly, our work illustrates how mechanistic analysis can explain and unveil systematic failures in multimodal alignment.",
      "arxiv_url": "https://openreview.net/forum?id=qeL8fi8GS7",
      "pdf_url": "https://openreview.net/pdf/fdeb4ce0127121481dc4bc44af9c24a30eb5a6d3.pdf",
      "primary_category": "Mechanistic Interpretability, Vision Language Models, Multimodal Alignment",
      "categories": [
        "Mechanistic Interpretability",
        "Vision Language Models",
        "Multimodal Alignment"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qGFvTIMS3W",
      "title": "Automated Model Discovery via Multi-modal & Multi-step Pipeline",
      "authors": [
        "Lee Jung-Mok",
        "Nam Hyeon-Woo",
        "Moon Ye-Bin",
        "Junhyun Nam",
        "Tae-Hyun Oh"
      ],
      "abstract": "Automated model discovery is the process of automatically searching and identifying the most appropriate model for a given dataset over a large combinatorial search space. Existing approaches, however, often face challenges in balancing the capture of fine-grained details with ensuring generalizability beyond training data regimes with a reasonable model complexity. In this paper, we present a multi-modal \\& multi-step pipeline for effective automated model discovery. Our approach leverages two vision-language-based modules (VLM), AnalyzerVLM and EvaluatorVLM, for effective model proposal and evaluation in an agentic way. AnalyzerVLM autonomously plans and executes multi-step analyses to propose effective candidate models. EvaluatorVLM assesses the candidate models both quantitatively and perceptually, regarding the fitness for local details and the generalibility for overall trends. Our results demonstrate that our pipeline effectively discovers models that capture fine details and ensure strong generalizability. Additionally, extensive ablation studies show that both multi-modality and multi-step reasoning play crucial roles in discovering favorable models.",
      "arxiv_url": "https://openreview.net/forum?id=qGFvTIMS3W",
      "pdf_url": "https://openreview.net/pdf/9677940f4812871bd634d12cccadc3598687a9ac.pdf",
      "primary_category": "Model Discovery, Time-Series Data, Vision Language Models",
      "categories": [
        "Model Discovery",
        "Time-Series Data",
        "Vision Language Models",
        "Model Selection",
        "LLM Reasoning"
      ],
      "tags": [
        "Multi-Modal RAG",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MBJ46gd1CT",
      "title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation",
      "authors": [
        "Mengkang Hu",
        "Yuhang Zhou",
        "Wendong Fan",
        "Yuzhou Nie",
        "Ziyu Ye",
        "Bowei Xia",
        "Tao Sun",
        "Zhaoxuan Jin",
        "Yingru Li",
        "Zeyu Zhang",
        "Yifeng Wang",
        "Qianshuo Ye",
        "Bernard Ghanem",
        "Ping Luo",
        "Guohao Li"
      ],
      "abstract": "Large Language Model (LLM)-based multi-agent systems show promise for automating real-world tasks but struggle to transfer across domains due to their domain-specific nature.\nCurrent approaches face two critical shortcomings: they require complete architectural redesign and full retraining of all components when applied to new domains.\nWe introduce **Workforce**, a hierarchical multi-agent framework that decouples strategic planning from specialized execution through a modular architecture comprising:\n*(i)* a *domain-agnostic* **Planner** for task decomposition,\n*(ii)* a **Coordinator** for subtask management, and\n*(iii)* specialized **Workers** with *domain-specific* tool-calling capabilities.\nThis decoupling enables cross-domain transferability during both inference and training phases:\nDuring inference, Workforce seamlessly adapts to new domains by adding or modifying worker agents;\nFor training, we introduce **Optimized Workforce Learning (OWL)**, which improves generalization across domains by optimizing a domain-agnostic planner with reinforcement learning from real-world feedback.\nTo validate our approach, we evaluate Workforce on the GAIA benchmark, covering various realistic, multi-domain agentic tasks.\nExperimental results demonstrate Workforce achieves open-source state-of-the-art performance (**69.70%**), outperforming commercial systems like OpenAI's Deep Research by **2.34%**.\nMore notably, our OWL-trained 32B model achieves **52.73%** accuracy (**+16.37%**) and demonstrates performance comparable to GPT-4o on challenging tasks.\nTo summarize, by enabling scalable generalization and modular domain transfer, our work establishes a foundation for the next generation of general-purpose AI assistants.\n\n*Our code is available at [Anonymous URL](https://anonymous.4open.science/r/annonymous-owl/), and our data is available at [Anonymous URL](https://huggingface.co/anonymous21016).*",
      "arxiv_url": "https://openreview.net/forum?id=MBJ46gd1CT",
      "pdf_url": "https://openreview.net/pdf/9ea2d3d5cf7f874c7669ab5c3f1270eb3bc794d1.pdf",
      "primary_category": "Large Language Model, LLM-based Agent, Multi-Agent",
      "categories": [
        "Large Language Model",
        "LLM-based Agent",
        "Multi-Agent"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Kz6eUL86XP",
      "title": "Do Language Models Use Their Depth Efficiently?",
      "authors": [
        "Róbert Csordás",
        "Christopher D Manning",
        "Christopher Potts"
      ],
      "abstract": "Modern LLMs are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible in shallow models, or do they merely spread the same kinds of computation out over more layers? To address these questions, we analyze the residual stream of the Llama 3.1, Qwen 3, and OLMo 2 family of models. We find: First, comparing the output of the sublayers to the residual stream reveals that layers in the second half contribute much less than those in the first half, with a clear phase transition between the two halves. Second, skipping layers in the second half has a much smaller effect on future computations and output predictions. Third, for multihop tasks, we are unable to find evidence that models are using increased depth to compose subresults in examples involving many hops. Fourth, we seek to directly address whether deeper models are using their additional layers to perform new kinds of computation. To do this, we train linear maps from the residual stream of a shallow model to a deeper one. We find that layers with the same relative depth map best to each other, suggesting that the larger model simply spreads the same computations out over its many layers. All this evidence suggests that deeper models are not using their depth to learn new kinds of computation, but only using the greater depth to perform more fine-grained adjustments to the residual. This may help explain why increasing scale leads to diminishing returns for stacked Transformer architectures.",
      "arxiv_url": "https://openreview.net/forum?id=Kz6eUL86XP",
      "pdf_url": "https://openreview.net/pdf/0691d8df4fdd96ae898d5845e2735b7f050f858f.pdf",
      "primary_category": "transformer, llm, large language model",
      "categories": [
        "transformer",
        "llm",
        "large language model",
        "residual stream",
        "depth",
        "casual interventions",
        "compositionality"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8HYeWMf0W3",
      "title": "LILO: Learning to Reason at the Frontier of Learnability",
      "authors": [
        "Thomas Foster",
        "Anya Sims",
        "Johannes Forkel",
        "Jakob Nicolaus Foerster"
      ],
      "abstract": "Reinforcement learning is widely adopted in post-training large language models, especially for reasoning-style tasks such as maths questions. However, as we show, most existing methods will provably fail to learn from questions that are too hard, where the model always fails, or too easy, where the model always succeeds. Much human effort is therefore spent continually producing datasets of questions of a suitable difficulty for state-of-the-art models. Given this, we consider how to algorithmically identify questions that allow for maximally efficient training. We introduce a method, LILO (Learnability Improves LLMs Optimally), that prioritises training on questions with high variance of success, known as learnability, and we provide theory proving LILO maximises the expected improvement of the model. We run a wide range of experiments over multiple base models, algorithms and reasoning datasets to demonstrate that LILO consistently improves final test accuracy and can yield a 3x reduction in the number of training steps required to reach it. We explore how questions with high learnability can be efficiently identified, and discuss how learnability can be scaled to produce LLM agents that autonomously and open-endedly expand the frontier of human knowledge.",
      "arxiv_url": "https://openreview.net/forum?id=8HYeWMf0W3",
      "pdf_url": "https://openreview.net/pdf/81c8ea42739bd8be2077f7a518eeedc3ad8b7d5c.pdf",
      "primary_category": "llm reasoning, reasoning, reinforcement learning",
      "categories": [
        "llm reasoning",
        "reasoning",
        "reinforcement learning",
        "curriculum learning",
        "unsupervised environment design"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "aJ7AdfOfij",
      "title": "Beyond Token Probes: Hallucination Detection via Activation Tensors with ACT-ViT",
      "authors": [
        "Guy Bar-Shalom",
        "Fabrizio Frasca",
        "Yaniv Galron",
        "Yftah Ziser",
        "Haggai Maron"
      ],
      "abstract": "Detecting hallucinations in Large Language Model-generated text is crucial for their safe deployment. While probing classifiers show promise, they operate on isolated layer–token pairs and are LLM-specific, limiting their effectiveness and hindering cross-LLM applications. In this paper, we introduce a novel approach to address these shortcomings. We build on the natural sequential structure of activation data in both axes (layers $\\times$ tokens) and advocate treating full activation tensors akin to images. We design ACT-ViT, a Vision Transformer-inspired model that can be effectively and efficiently applied to activation tensors and supports training on data from multiple LLMs simultaneously. Through comprehensive experiments encompassing diverse LLMs and datasets, we demonstrate that ACT-ViT consistently outperforms traditional probing techniques while remaining extremely efficient for deployment. In particular, we show that our architecture benefits substantially from multi-LLM training, achieves strong zero-shot performance on unseen datasets, and can be transferred effectively to new LLMs through fine-tuning.",
      "arxiv_url": "https://openreview.net/forum?id=aJ7AdfOfij",
      "pdf_url": "https://openreview.net/pdf/8c1a20caf3d46334d96c8131e9ff069d2ff949b2.pdf",
      "primary_category": "Hallucination detection, Probing classifiers, LLM analysis",
      "categories": [
        "Hallucination detection",
        "Probing classifiers",
        "LLM analysis"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4qVWY12KQT",
      "title": "QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code",
      "authors": [
        "Hainan Fang",
        "Yuanbo Wen",
        "Jun Bi",
        "Yihan Wang",
        "Tonghui He",
        "Yanlin Tang",
        "Di Huang",
        "Jiaming Guo",
        "Rui Zhang",
        "Qi Guo",
        "Yunji Chen"
      ],
      "abstract": "Compilers, while essential, are notoriously complex systems that demand prohibitively expensive human expertise to develop and maintain. The recent advancements in Large Language Models (LLMs) offer a compelling new paradigm: Neural Compilation, which could potentially simplify compiler development for new architectures and facilitate the discovery of innovative optimization techniques. However, several critical obstacles impede its practical adoption. Firstly, a significant lack of dedicated benchmarks and robust evaluation methodologies hinders objective assessment and tracking of progress in the field. Secondly, systematically enhancing the reliability and performance of LLM-generated assembly remains a critical challenge. Addressing these challenges, this paper introduces NeuComBack, a novel benchmark dataset specifically designed for IR-to-assembly compilation. Leveraging this dataset, we first define a foundational Neural Compilation workflow and conduct a comprehensive evaluation of the capabilities of recent frontier LLMs on Neural Compilation, establishing new performance baselines. We further propose a self-evolving prompt optimization method that enables LLMs to iteratively evolve their internal prompt strategies by extracting insights from prior self-debugging traces, thereby enhancing their neural compilation capabilities.\nExperiments demonstrate that our method significantly improves both the functional correctness and the performance of LLM-generated assembly code. Compared to baseline prompts, the functional correctness rates improved from 44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More significantly, among the 16 correctly generated x86_64 programs using our method, 14 (87.5%) surpassed clang-O3 performance. These consistent improvements across diverse architectures (x86_64 and aarch64) and program distributions (NeuComBack L1 and L2) validate our method's superiority over conventional approaches and its potential for broader adoption in low-level neural compilation.",
      "arxiv_url": "https://openreview.net/forum?id=4qVWY12KQT",
      "pdf_url": "https://openreview.net/pdf/7abaf893a3c1dac17a98839336af91486af3a3f1.pdf",
      "primary_category": "Large Language Models, Neural Compilation, Compiler",
      "categories": [
        "Large Language Models",
        "Neural Compilation",
        "Compiler",
        "Code Generation",
        "Automatic Prompt Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Qku7g56aWf",
      "title": "Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning",
      "authors": [
        "Xingjian Ran",
        "Yixuan Li",
        "Linning Xu",
        "Mulin Yu",
        "Bo Dai"
      ],
      "abstract": "Realistic 3D indoor scene synthesis is vital for embodied AI and digital content creation. It can be naturally divided into two subtasks: object generation and layout generation. While recent generative models have significantly advanced object-level quality and controllability, layout generation remains challenging due to limited datasets. Existing methods either overfit to these datasets or rely on predefined constraints to optimize numerical layout that sacrifice flexibility. As a result, they fail to generate scenes that are both open-vocabulary and aligned with fine-grained user instructions. We introduce DirectLayout, a framework that directly generates numerical 3D layouts from text descriptions using generalizable spatial reasoning of large language models (LLMs). DirectLayout decomposes the generation into three stages: producing a Bird's-Eye View (BEV) layout, lifting it into 3D space, and refining object placements. To enable explicit spatial reasoning and help the model grasp basic principles of object placement, we employ Chain-of-Thought (CoT) Activation based on the 3D-Front dataset. Additionally, we design CoT-Grounded Generative Layout Reward to enhance generalization and spatial planning. During inference, DirectLayout addresses asset-layout mismatches via Iterative Asset-Layout Alignment through in-context learning. Extensive experiments demonstrate that DirectLayout achieves impressive semantic consistency, generalization and physical plausibility.",
      "arxiv_url": "https://openreview.net/forum?id=Qku7g56aWf",
      "pdf_url": "https://openreview.net/pdf/6c2975b0ba0946cac72064055247fa7d836a84ed.pdf",
      "primary_category": "3D indoor scene synthesis, text-to-3D layout generation, spatial reasoning",
      "categories": [
        "3D indoor scene synthesis",
        "text-to-3D layout generation",
        "spatial reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qS3WgmGs9s",
      "title": "SketchMind:  A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches",
      "authors": [
        "Ehsan Latif",
        "Zirak Khan",
        "Xiaoming Zhai"
      ],
      "abstract": "Scientific sketches (e.g., models) offer a powerful lens into students' conceptual understanding, yet AI-powered automated assessment of such free-form, visually diverse artifacts remains a critical challenge. Existing solutions often treat sketch evaluation as either an image classification task or monolithic vision-language models, which lack interpretability, pedagogical alignment, and adaptability across cognitive levels. To address these limitations, we present SketchMind, a cognitively grounded, multi-agent framework for evaluating and improving student-drawn scientific sketches. SketchMind introduces Sketch Reasoning Graphs (SRGs), semantic graph representations that embed domain concepts and Bloom's taxonomy-based cognitive labels. The system comprises modular agents responsible for rubric parsing, sketch perception, cognitive alignment, and iterative feedback with sketch modification, enabling personalized and transparent evaluation. We evaluate SketchMind on a curated dataset of 3,575 student-generated sketches across six science assessment items with different highest order of Bloom's level that require students to draw models to explain phenomena. Compared to baseline GPT-4o performance without SRG (average accuracy: 55.6%), the model with SRG integration achieves 77.1% average accuracy (+21.4% average absolute gain). We also demonstrate that multi-agent orchestration with SRG enhances SketchMind performance, for example, SketchMind with GPT-4.1 gains an average 8.9% increase in sketch prediction accuracy, outperforming single-agent pipelines across all items. Human evaluators rated the feedback and co-created sketches generated by SketchMind with GPT-4.1, which achieved an average of 4.1 out of 5, significantly higher than those of baseline models (e.g., 2.3 for GPT-4o). Experts noted the system’s potential to meaningfully support conceptual growth through guided revision. Our code and (pending approval) dataset will be released to support reproducibility and future research in AI-driven education.",
      "arxiv_url": "https://openreview.net/forum?id=qS3WgmGs9s",
      "pdf_url": "https://openreview.net/pdf/1f29ebbf550ce3d0ac8e2654e8faf2819c05fa9b.pdf",
      "primary_category": "Educational AI, Multimodal Learning, Scientific Reasoning",
      "categories": [
        "Educational AI",
        "Multimodal Learning",
        "Scientific Reasoning",
        "Cognitive Modeling",
        "Sketch-Based Assessment"
      ],
      "tags": [
        "LLM",
        "Personalization",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9GHaLDORNL",
      "title": "SGAR: Structural Generative Augmentation for 3D Human Motion Retrieval",
      "authors": [
        "Jiahang Zhang",
        "Lilang Lin",
        "Shuai Yang",
        "Jiaying Liu"
      ],
      "abstract": "3D human motion-text retrieval is essential for accurate motion understanding, targeted at cross-modal alignment learning. Existing methods typically align the global motion-text concepts directly, suffering from sub-optimal generalization due to the uncertainty of correspondence learning between multiple motion concepts coupled in a single motion/text sequence. Therefore, we study the explicit fine-grained concept decomposition for alignment learning and present a novel framework, Structural Generative Augmentation for 3D Human Motion Retrieval (SGAR), to enable generation-augmented retrieval. Specifically, relying on the strong priors of existing large language model (LLM) assets, we effectively decompose human motions structurally into subtler semantic units, \\ie, body parts, for fine-grained motion modeling. Based on this, we develop part-mixture learning to better decouple the local motion concept learning, boosting part-level alignment. Moreover, a directional relation alignment strategy exploiting the correspondence between full-body and part motions is incorporated to regularize feature manifold for better consistency. Extensive experiments on three benchmarks, including motion-text retrieval as well as recognition and generation applications, demonstrate the superior performance and promising transferability of our method.",
      "arxiv_url": "https://openreview.net/forum?id=9GHaLDORNL",
      "pdf_url": "https://openreview.net/pdf/1d98cebdbc17f9dacf4861cfbd15a16f71dd3e8a.pdf",
      "primary_category": "Motion Representation, Motion Retrieval",
      "categories": [
        "Motion Representation",
        "Motion Retrieval"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yRxX01oRIi",
      "title": "Evaluating the Inductive Abilities of Large Language Models: Why Chain-of-Thought Reasoning Sometimes Hurts More Than Helps",
      "authors": [
        "Haibo Jin",
        "Peiyan Zhang",
        "Man Luo",
        "Haohan Wang"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable progress across domains, yet their ability to perform inductive reasoning—inferring latent rules from sparse examples—remains limited. \nIt is often assumed that chain-of-thought (CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such reasoning. \nWe investigate this assumption with creating four controlled, diagnostic game-based tasks—chess, Texas Hold’em, dice games, and blackjack—with hidden human-defined rules. \nWe find that CoT reasoning can degrade inductive performance, with LRMs often underperforming their non-reasoning counterparts.\n\nTo explain this, we present a theoretical framework that reveals how reasoning steps can amplify error through three failure modes: incorrect sub-task decomposition, incorrect sub-task solving, and incorrect final answer summarization. \nBased on our theoretical and empirical analysis, we introduce structured interventions that adapt CoT generation according to our identified failure types. These interventions improve inductive accuracy without retraining. Our findings suggest that effective (CoT) reasoning depends not only on taking more steps but also on ensuring those steps are well-structured.",
      "arxiv_url": "https://openreview.net/forum?id=yRxX01oRIi",
      "pdf_url": "https://openreview.net/pdf/d0edb06217009d04527b2120a6d74a6a4cb1160e.pdf",
      "primary_category": "Large Lauange Model, Inductive Abilities, Reasoning",
      "categories": [
        "Large Lauange Model",
        "Inductive Abilities",
        "Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "fyeSq3m8CY",
      "title": "Tensor-Parallelism with Partially Synchronized Activations",
      "authors": [
        "Itay Lamprecht",
        "Asaf Karnieli",
        "Yair Hanani",
        "Niv Giladi",
        "Daniel Soudry"
      ],
      "abstract": "Training and inference of Large Language Models (LLMs) with tensor-parallelism requires substantial communication to synchronize activations. Our findings suggest that with a few minor adjustments to current practices, LLMs can be trained without fully synchronizing activations, reducing bandwidth demands. We name this “Communication-Aware Architecture for Tensor-parallelism” (CAAT-Net). We train a 7B parameter CAAT-Net model and show that tensor-parallel communication can be reduced by up to 50% with no significant drop in pretraining accuracy across nearly all evaluated benchmarks. We also experiment with smaller 130M and 1.1B models to show the robustness and scalability of our method. We find that, in some scenarios, validation loss can even improve when reducing communication. Finally, we demonstrate how CAAT-Net accelerates both training and inference workloads across various settings and model sizes.",
      "arxiv_url": "https://openreview.net/forum?id=fyeSq3m8CY",
      "pdf_url": "https://openreview.net/pdf/1dc4f764e355f8b26f5ebbdb08f6328d546aebf4.pdf",
      "primary_category": "Large Language Models, Distributed Training, Tensor-Parallelism",
      "categories": [
        "Large Language Models",
        "Distributed Training",
        "Tensor-Parallelism"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "h3LlJ6Bh4S",
      "title": "Process vs. Outcome Reward: Which is Better for Agentic RAG Reinforcement Learning",
      "authors": [
        "Wenlin Zhang",
        "Xiangyang Li",
        "Kuicai Dong",
        "Yichao Wang",
        "Pengyue Jia",
        "Xiaopeng Li",
        "Yingyi Zhang",
        "Derong Xu",
        "Zhaocheng Du",
        "Huifeng Guo",
        "Ruiming Tang",
        "Xiangyu Zhao"
      ],
      "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge, yet traditional RAG systems struggle with static workflows and limited adaptability for complex, multistep reasoning tasks. Agentic RAG systems, such as DeepResearch, address these issues through dynamic retrieval, iterative context refinement, and adaptive workflows. However, recent methods like Search-R1, which rely on outcome-based reinforcement learning, face challenges such as low exploration efficiency, gradient conflict, and sparse reward signals. To tackle these limitations, we introduce ReasonRAG, a novel method that leverages RAG-ProGUIDE—a high-quality dataset providing fine-grained, process-level rewards for query generation, evidence extraction, and answer generation. By employing process-supervised reinforcement learning, ReasonRAG enhances LLMs’ autonomous capabilities in search, query generation, evidence extraction, and answer synthesis. Experimental results show that ReasonRAG, utilizing RAG-ProGUIDE, outperforms existing approaches like Search-R1 and traditional RAG systems, achieving superior performance on five benchmark datasets with only 5k training instances—significantly fewer than the 90k required by Search-R1. Our code is available at https://github.com/Applied-Machine-Learning-Lab/ReasonRAG.",
      "arxiv_url": "https://openreview.net/forum?id=h3LlJ6Bh4S",
      "pdf_url": "https://openreview.net/pdf/863197fcba5eb83767ace1f59ea8fa5f81958eb6.pdf",
      "primary_category": "Process-Supervision, Agentic RAG",
      "categories": [
        "Process-Supervision",
        "Agentic RAG"
      ],
      "tags": [
        "LLM",
        "RAG",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0fBQAckQK3",
      "title": "GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs",
      "authors": [
        "Advik Raj Basani",
        "Xiao Zhang"
      ],
      "abstract": "LLMs have demonstrated impressive capabilities across various natural language processing tasks yet remain vulnerable to prompts, known as jailbreak attacks, carefully designed to bypass safety guardrails and elicit harmful responses. Traditional methods rely on manual heuristics that suffer from limited generalizability. Despite being automatic, optimization-based attacks often produce unnatural jailbreak prompts that can be easily detected by safety filters or require high computational costs due to discrete token optimization. This paper introduces *Generative Adversarial Suffix Prompter* (GASP), a novel automated framework that can efficiently generate human-readable jailbreak prompts in a fully black-box setting. In particular, GASP leverages latent Bayesian optimization to craft adversarial suffixes by efficiently exploring continuous latent spaces, gradually optimizing the suffix generator to improve attack efficacy while balancing prompt coherence via a targeted iterative refinement procedure. Through comprehensive experiments, we show that GASP can produce natural adversarial prompts, significantly improving jailbreak success, reducing training times, and accelerating inference speed, thus making it an efficient and scalable solution for red-teaming LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=0fBQAckQK3",
      "pdf_url": "https://openreview.net/pdf/0f711e5187bba18c598a08cc6c71618614d27a6e.pdf",
      "primary_category": "LLM safety, jailbreak attacks, adversarial prompting",
      "categories": [
        "LLM safety",
        "jailbreak attacks",
        "adversarial prompting",
        "Latent Bayesian optimization",
        "red-teaming"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "16kX08MCav",
      "title": "Limitations of Normalization in Attention",
      "authors": [
        "Timur Mudarisov",
        "Mikhail Burtsev",
        "Tatiana Petrova",
        "Radu State"
      ],
      "abstract": "This paper investigates the limitations of the normalization in attention mechanisms. We begin with a theoretical framework that enables the identification of the model's selective ability and the geometric separation involved in token selection. Our analysis includes explicit bounds on distances and separation criteria for token vectors under softmax scaling. Through experiments with pre-trained GPT-2 model, we empirically validate our theoretical results and analyze key behaviors of the attention mechanism. Notably, we demonstrate that as the number of selected tokens increases, the model's ability to distinguish informative tokens declines, often converging toward a uniform selection pattern. We also show that gradient sensitivity under softmax normalization presents challenges during training, especially at low temperature settings. These findings advance current understanding of softmax-based attention mechanism and motivate the need for more robust normalization and selection strategies in future attention architectures.",
      "arxiv_url": "https://openreview.net/forum?id=16kX08MCav",
      "pdf_url": "https://openreview.net/pdf/030eae63687770e31feeadcb4a910fc474647fde.pdf",
      "primary_category": "Large Language Models, Theory verification, GPT-2",
      "categories": [
        "Large Language Models",
        "Theory verification",
        "GPT-2",
        "Attention mechanism",
        "Natural Language Processing"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nOv6z9RHA5",
      "title": "Bridging Time and Linguistics: LLMs as Time Series Analyzer through Symbolization and Segmentation",
      "authors": [
        "Jianyang Qin",
        "Chaoyang Li",
        "Jinhao Cui",
        "Lingzhi Wang",
        "Zhao Liu",
        "Qing Liao"
      ],
      "abstract": "Recent studies reveal that Large Language Models (LLMs) exhibit strong sequential reasoning capabilities, allowing them to replace specialized time-series models and serve as foundation models for complex time-series analysis. To activate the capabilities of LLMs for time-series tasks, numerous studies have attempted to bridge the gap between time series and linguistics by aligning textual representations with time-series patterns. However, it is a non-trivial endeavor to losslessly capture the infinite time-domain variability using natural language, leading to suboptimal alignment performance. Beyond representation, contextual differences, where semantics in time series are conveyed by consecutive points, unlike in text by individual tokens, are often overlooked by existing methods. To address these, we propose S$^2$TS-LLM, a simple yet effective framework to repurpose LLMs for universal time series analysis through the following two main paradigms: (i) a spectral symbolization paradigm transforms time series into frequency-domain representations characterized by a fixed number of components and prominent amplitudes, which enables a limited set of symbols to effectively abstract key frequency features; (ii) a contextual segmentation paradigm partitions the sequence into blocks based on temporal patterns and reassigns positional encodings accordingly, thereby mitigating the structural mismatch between time series and natural language. Together, these paradigms bootstrap the LLMs' perception of temporal patterns and structures, effectively bridging time series and linguistics. Extensive experiments show that S$^2$TS-LLM can serve as a powerful time series analyzer, outperforming state-of-the-art methods across time series tasks.",
      "arxiv_url": "https://openreview.net/forum?id=nOv6z9RHA5",
      "pdf_url": "https://openreview.net/pdf/05148f9764f938d78f7f2174001f413b7d1d9452.pdf",
      "primary_category": "Time series; Large Language Models; Cross-Modal Alignment",
      "categories": [
        "Time series; Large Language Models; Cross-Modal Alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IfD2MKTmWv",
      "title": "Memory Mosaics at scale",
      "authors": [
        "Jianyu Zhang",
        "Leon Bottou"
      ],
      "abstract": "Memory Mosaics, networks of associative memories, have demonstrated appealing compositional and in-context learning capabilities on medium-scale networks (GPT-2 scale) and synthetic small datasets. This work shows that these favorable properties remain when we scale memory mosaics to large language model sizes (llama-8B scale) and real-world datasets. \n\nTo this end, we scale memory mosaics to 10B size, we train them on one trillion tokens, we introduce a couple architectural modifications (*memory mosaics v2*), we assess their capabilities across three evaluation dimensions: training-knowledge storage, new-knowledge storage, and in-context learning. \n\nThroughout the evaluation, memory mosaics v2 match transformers on the learning of training knowledge (first dimension) and significantly outperforms transformers on carrying out new tasks at inference time (second and third dimensions). These improvements cannot be easily replicated by simply increasing the training data for transformers. A memory mosaics v2 trained on one trillion tokens still perform better on these tasks than a transformer trained on eight trillion tokens.",
      "arxiv_url": "https://openreview.net/forum?id=IfD2MKTmWv",
      "pdf_url": "https://openreview.net/pdf/ab3c0a1ec3de493210617ac768095a8602397dbb.pdf",
      "primary_category": "In-context learning, memory mosaics",
      "categories": [
        "In-context learning",
        "memory mosaics"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "A4JiifkTzq",
      "title": "SPRO: Improving Image Generation via Self-Play",
      "authors": [
        "Ritika Jha",
        "Aanisha Bhattacharyya",
        "Yaman Kumar Singla",
        "Rajiv Ratn Shah",
        "Changyou Chen",
        "Balaji Krishnamurthy"
      ],
      "abstract": "Recent advances in diffusion models have dramatically improved image fidelity and diversity. However, aligning these models with nuanced human preferences -such as aesthetics, engagement, and subjective appeal remains a key challenge due to the scarcity of large-scale human annotations. Collecting such data is both expensive and limited in diversity. To address this, we leverage the reasoning capabilities of vision-language models (VLMs) and propose Self-Play Reward Optimization (SPRO), a scalable, annotation-free training framework based on multimodal self-play. SPRO learns to jointly align prompt and image generation with human preferences by iteratively generating, evaluating, and learning to refine outputs using synthetic reward signals such as aesthetics and human engagement. This self-improving feedback loop eliminates the need for external supervision. SPRO comprises three stages: (1) SPRO-Prompt, which trains a Guider-VLM via self-play to generate diverse, high-reward prompts targeting objectives such as PickScore (user preference), LAION-Aesthetics, and EngageNet (engagement); (2) SPRO-Image, which fine-tunes the diffusion model on high-reward images derived from these prompts; and (3) SPRO-Multimodal (SPRO-MM), which integrates both components for full end-to-end alignment. Without relying on human-labeled data, SPRO achieves an average 30\\% improvement across preference objectives. Moreover, its generated prompts generalize across both open- and closed-source diffusion models. Through iterative self-play, SPRO discovers prompting strategies rarely authored by humans such as emphasizing visual harmony for aesthetics or leveraging shadow-based cues for engagement. SPRO offers a scalable path toward aligning generative models with complex subjective human values.",
      "arxiv_url": "https://openreview.net/forum?id=A4JiifkTzq",
      "pdf_url": "https://openreview.net/pdf/a87de85f90ff9cdfc73f92f86225af83dae8687f.pdf",
      "primary_category": "diffusion model, self play, llm",
      "categories": [
        "diffusion model",
        "self play",
        "llm",
        "large language models",
        "prompt improvement"
      ],
      "tags": [
        "LLM",
        "Personalization",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "sv41aaGTit",
      "title": "LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding",
      "authors": [
        "Yuchen Ma",
        "Dennis Frauen",
        "Jonas Schweisthal",
        "Stefan Feuerriegel"
      ],
      "abstract": "Estimating treatment effects is crucial for personalized decision-making in medicine, but this task faces unique challenges in clinical practice. At training time, models for estimating treatment effects are typically trained on well-structured medical datasets that contain detailed patient information. However, at inference time, predictions are often made using textual descriptions (e.g., descriptions with self-reported symptoms), which are incomplete representations of the original patient information. In this work, we make three contributions. (1) We show that the discrepancy between the data available during training time and inference time can lead to biased estimates of treatment effects. We formalize this issue as an \\emph{inference time text confounding} problem, where confounders are fully observed during training time but only partially available through text at inference time. (2) To address this problem, we propose a novel framework for estimating treatment effects that explicitly accounts for inference time text confounding. Our framework leverages large language models (LLMs) together with a custom doubly robust learner to mitigate biases caused by the inference time text confounding. (3) Through a series of experiments, we demonstrate the effectiveness of our framework in real-world applications.",
      "arxiv_url": "https://openreview.net/forum?id=sv41aaGTit",
      "pdf_url": "https://openreview.net/pdf/91cde9dca622c2cb85a4f93a3342a05cadeed265.pdf",
      "primary_category": "Treatment Effect Estimation, LLMs, Text Confounding",
      "categories": [
        "Treatment Effect Estimation",
        "LLMs",
        "Text Confounding",
        "Causal Inference",
        "CATE",
        "Doubly Robust"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pc6M9h3T9m",
      "title": "Beyond Verifiable Rewards: Scaling Reinforcement Learning in Language Models to Unverifiable Data",
      "authors": [
        "Yunhao Tang",
        "Sid Wang",
        "Lovish Madaan",
        "Remi Munos"
      ],
      "abstract": "We propose to scale RL to unverifiable data with a novel algorithm JEPO (Jensen's Evidence lower bound for Policy Optimization). While most prior effort on scaling RL for LLMs focuses on verifiable data where ground truth answers are typically short-form and can be matched easily, we investigate the case where such assumptions are less valid (e.g., when answers are long-form such as mathematical proofs). To scale RL training to unverifiable data with contemporary training constraints, we propose JEPO. JEPO applies Jensen's evidence lower bound, a pragmatic simplification of the evidence lower bound which views chain-of-thought as a latent variable in the generative process. We show that on verifiable datasets (math), JEPO is as effective as RL with verifiable reward; on semi-verifiable and unverifiable datasets (numina and numina-proof), JEPO improves on soft-match based evaluations compared to RL with verifiable reward which can only leverage a subset of the data source as well as test set likelihood evaluations.",
      "arxiv_url": "https://openreview.net/forum?id=pc6M9h3T9m",
      "pdf_url": "https://openreview.net/pdf/9e93723ad08ba40f027f9a89430e6abb3db17804.pdf",
      "primary_category": "Reinforcement Learning, LLMs, Reasoning",
      "categories": [
        "Reinforcement Learning",
        "LLMs",
        "Reasoning",
        "Unverifiable",
        "Verifiable"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "WjYvHSjXrP",
      "title": "InstructHOI: Context-Aware Instruction for Multi-Modal Reasoning in Human-Object Interaction Detection",
      "authors": [
        "Jinguo Luo",
        "Weihong Ren",
        "Quanlong Zheng",
        "Yanhao Zhang",
        "Zhenlong Yuan",
        "Zhiyong Wang",
        "Haonan Lu",
        "Honghai LIU"
      ],
      "abstract": "Recently, Large Foundation Models (LFMs), e.g., CLIP and GPT, have significantly advanced the Human-Object Interaction (HOI) detection, due to their superior generalization and transferability. Prior HOI detectors typically employ single- or multi-modal prompts to generate discriminative representations for HOIs from pretrained LFMs. However, such prompt-based approaches focus on transferring HOI-specific knowledge, but unexplore the potential reasoning capabilities of LFMs, which can provide informative context for ambiguous and open-world interaction recognition. In this paper, we propose InstructHOI, a novel method that leverages context-aware instructions to guide multi-modal reasoning for HOI detection. Specifically, to bridge knowledge gap and enhance reasoning abilities, we first perform HOI-domain fine-tuning on a pretrained multi-modal LFM, using a generated dataset with 140K interaction-reasoning image-text pairs. Then, we develop a Context-aware Instruction Generator (CIG) to guide interaction reasoning. Unlike traditional language-only instructions, CIG first mines visual interactive context at the human-object level, which is then fused with linguistic instructions, forming multi-modal reasoning guidance. Furthermore, an Interest Token Selector (ITS) is adopted to adaptively filter image tokens based on context-aware instructions, thereby aligning reasoning process with interaction regions. Extensive experiments on two public benchmarks demonstrate that our proposed method outperforms the state-of-the-art ones, under both supervised and zero-shot settings.",
      "arxiv_url": "https://openreview.net/forum?id=WjYvHSjXrP",
      "pdf_url": "https://openreview.net/pdf/9256d13b1c0d87dee1265c615f5a083bea101d9c.pdf",
      "primary_category": "Human-Object Interaction Detection; Interaction Reasoning",
      "categories": [
        "Human-Object Interaction Detection; Interaction Reasoning"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "jv7OHhQ0YP",
      "title": "RepGuard: Adaptive Feature Decoupling for Robust Backdoor Defense in Large Language Models",
      "authors": [
        "Chenxu Niu",
        "Jie Zhang",
        "Yanbing Liu",
        "Yunpeng Li",
        "Jinta Weng",
        "Yue Hu"
      ],
      "abstract": "Backdoor attacks pose a significant threat to large language models (LLMs) by embedding malicious triggers that manipulate model behavior. However, existing defenses primarily rely on prior knowledge of backdoor triggers or targets and offer only superficial mitigation strategies, thus struggling to fundamentally address the inherent reliance on unreliable features. To address these limitations, we propose a novel defense strategy, \\textit{RepGuard}, that strengthens LLM resilience by adaptively separating abnormal features from useful semantic representations, rendering the defense agnostic to specific trigger patterns. Specifically, we first introduce a dual-perspective feature localization strategy that integrates local consistency and sample-wise deviation metrics to identify suspicious backdoor patterns. Based on this identification, an adaptive mask generation mechanism is applied to isolate backdoor-targeted shortcut features by decomposing hidden representations into independent spaces,  while preserving task-relevant semantics. With a multi-objective optimization framework, our method can inherently mitigates backdoor attacks. Across \\textit{Target Refusal} and \\textit{Jailbreak} tasks under four types of attacks, RepGuard consistently reduced the attack success rate on poisoned data by nearly 80\\% on average, while maintaining near-original task performance on clean data. Extensive experiments demonstrate that RepGuard provides a scalable and interpretable solution for safeguarding LLMs against sophisticated backdoor threats.",
      "arxiv_url": "https://openreview.net/forum?id=jv7OHhQ0YP",
      "pdf_url": "https://openreview.net/pdf/2f906d2af14cc22b17f912563a26f5c9a9d0f095.pdf",
      "primary_category": "Large Language Model, Backdoor Defense, safety",
      "categories": [
        "Large Language Model",
        "Backdoor Defense",
        "safety",
        "Representation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "CmKar1zptJ",
      "title": "Post Hoc Regression Refinement via Pairwise Rankings",
      "authors": [
        "Kevin Tirta Wijaya",
        "Michael Sun",
        "Minghao Guo",
        "Hans-peter Seidel",
        "Wojciech Matusik",
        "Vahid Babaei"
      ],
      "abstract": "Accurate prediction of continuous properties is essential to many scientific and engineering tasks. Although deep-learning regressors excel with abundant labels, their accuracy deteriorates in data-scarce regimes. We introduce RankRefine, a model-agnostic, plug-and-play post-hoc refinement technique that injects expert knowledge through pairwise rankings. Given a query item and a small reference set with known properties, RankRefine combines the base regressor’s output with a rank-based estimate via inverse-variance weighting, requiring no retraining. In molecular property prediction task, RankRefine achieves up to 10\\% relative reduction in mean absolute error using only 20 pairwise comparisons obtained through a general-purpose large language model (LLM) with no finetuning. As rankings provided by human experts or general-purpose LLMs are sufficient for improving regression across diverse domains, RankRefine offers practicality and broad applicability, especially in low-data settings.",
      "arxiv_url": "https://openreview.net/forum?id=CmKar1zptJ",
      "pdf_url": "https://openreview.net/pdf/401d2d668696668846f8767f7556c7d4ebfd9df3.pdf",
      "primary_category": "prediction refinement, pairwise ranking, molecular property prediction",
      "categories": [
        "prediction refinement",
        "pairwise ranking",
        "molecular property prediction"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "utvu4PJ0Ct",
      "title": "Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs",
      "authors": [
        "Zhixin Xie",
        "Xurui Song",
        "Jun Luo"
      ],
      "abstract": "Despite substantial efforts in safety alignment, recent research indicates that Large\nLanguage Models (LLMs) remain highly susceptible to jailbreak attacks. Among\nthese attacks, finetuning-based ones that compromise LLMs’ safety alignment via\nfine-tuning stand out due to its stable jailbreak performance. In particular, a recent\nstudy indicates that fine-tuning with as few as 10 harmful question-answer (QA)\npairs can lead to successful jailbreaking across various harmful questions. However,\nsuch malicious fine-tuning attacks are readily detectable and hence thwarted by\nmoderation models. In this paper, we demonstrate that LLMs can be jailbroken\nby fine-tuning with only 10 benign QA pairs; our attack exploits the increased\nsensitivity of LLMs to fine-tuning data after being overfitted. Specifically, our\nfine-tuning process starts with overfitting an LLM via fine-tuning with benign QA\npairs involving identical refusal answers. Further fine-tuning is then performed\nwith standard benign answers, causing the overfitted LLM to forget the refusal\nattitude and thus provide compliant answers regardless of the harmfulness of a\nquestion. We implement our attack on the ten LLMs and compare it with five\nexisting baselines. Experiments demonstrate that our method achieves significant\nadvantages in both attack effectiveness and attack stealth. Our findings expose\npreviously unreported security vulnerabilities in current LLMs and provide a new\nperspective on understanding how LLMs’ security is compromised, even with\nbenign fine-tuning. Our code is available at https://github.com/ZHIXINXIE/ten_benign.git.",
      "arxiv_url": "https://openreview.net/forum?id=utvu4PJ0Ct",
      "pdf_url": "https://openreview.net/pdf/2a60bb227d356283cc8b6eddcfeda9a0ef466282.pdf",
      "primary_category": "LLM Security, Jailbreak, Fine-tuning",
      "categories": [
        "LLM Security",
        "Jailbreak",
        "Fine-tuning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZDbhQrgwoT",
      "title": "Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO",
      "authors": [
        "Kaiyang Guo",
        "Yinchuan Li",
        "Zhitang Chen"
      ],
      "abstract": "Direct alignment methods typically train large language models (LLMs) by contrasting the likelihoods of preferred and dispreferred responses. While effective at capturing relative preferences, these methods are widely observed to suppress the absolute likelihoods of example responses. As a result, aligned models can  deviate from expected patterns, exhibiting reward‑hacking effect even without an explicit reward model. This fundamental limitation of contrastive alignment, termed likelihood underdetermination, motivates us to revisit direct preference optimization (DPO)—the seminal direct alignment method. Interestingly, we show that the DPO loss admits a principled decomposition. The reformulated loss not only extends naturally to a broader range of feedback types, but also unveils the root cause of likelihood underdetermination. Specifically, we identify that standard DPO implicitly oversimplifies a regularizer in the reformulated loss; restoring this full term effectively resolves the underdetermination. Building on these insights, we introduce PRoximalized PReference Optimization (PRO), a unified alignment method that accommodates  diverse feedback types while eliminating likelihood underdetermination through an efficient approximation of the full regularizer. Empirical evaluations demonstrate the consistent superiority of PRO over existing methods across pairwise, binary and scalar feedback.",
      "arxiv_url": "https://openreview.net/forum?id=ZDbhQrgwoT",
      "pdf_url": "https://openreview.net/pdf/b6d4e5cf3be1d78e588fcf1bc8929dc040bde7a8.pdf",
      "primary_category": "Preference optimization, learning from feedback, language model alignment",
      "categories": [
        "Preference optimization",
        "learning from feedback",
        "language model alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Ynwl0V1YH0",
      "title": "Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression",
      "authors": [
        "Xi Zhang",
        "Xiaolin Wu",
        "Jiamang Wang",
        "Weisi Lin"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities but typically require extensive computational resources and memory for inference. Post-training quantization (PTQ) can effectively reduce these demands by storing weights in lower bit-width formats. However, standard uniform quantization often leads to notable performance degradation, particularly in low-bit scenarios. In this work, we introduce a Grouped Lattice Vector Quantization (GLVQ) framework that assigns each group of weights a customized lattice codebook, defined by a learnable generation matrix. To address the non-differentiability of the quantization process, we adopt Babai rounding to approximate nearest-lattice-point search during training, which enables stable optimization of the generation matrices. Once trained, decoding reduces to a simple matrix-vector multiplication, yielding an efficient and practical quantization pipeline. Experiments on multiple benchmarks show that our approach achieves a better trade-off between model size and accuracy compared to existing post-training quantization baselines, highlighting its effectiveness in deploying large models under stringent resource constraints. Our source code is available on GitHub repository: https://github.com/xzhang9308/GLVQ.",
      "arxiv_url": "https://openreview.net/forum?id=Ynwl0V1YH0",
      "pdf_url": "https://openreview.net/pdf/efcfd4ca4e944e97161a0eda2569b67b31f21241.pdf",
      "primary_category": "LLM compression, lattice vector quantization",
      "categories": [
        "LLM compression",
        "lattice vector quantization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7DY7kB8wyZ",
      "title": "LLM Layers Immediately Correct Each Other",
      "authors": [
        "Arjun Patrawala",
        "Jiahai Feng",
        "Erik Jones",
        "Jacob Steinhardt"
      ],
      "abstract": "Recent methods in language model interpretability employ techniques such as sparse autoencoders to decompose residual stream contributions into linear, semantically-meaningful features. Our work demonstrates that an underlying assumption of these methods—that residual stream contributions build additively upon each other—is insufficient to fully explain model behavior. Specifically, we identify the Transformer Layer Correction Mechanism (TLCM), wherein adjacent transformer layers systematically counteract each other's contributions to the residual stream. TLCM appears in 5 out of 7 major open-source model families and activates across nearly all tokens in diverse texts.\nTo understand TLCM, we show that it emerges during pretraining, operates most strongly on punctuation and numbers, and adaptively calibrates its correction strength based on the preceding layer's output. We further show that TLCM actively corrects a small subspace and promotes other subspaces, different from standard model behavior. We advance the ``propose-and-reject'' hypothesis: layers may propose multiple candidate features, while subsequent layers selectively filter out inappropriate ones. Finally, we discuss how our findings help explain three persistent challenges in feature-based interpretability: why extracted features descriptions often suffer from low specificity; why feature-based interventions for model steering fail at low magnitude; why recent work finds cross-layer transcoders outperform SAEs.",
      "arxiv_url": "https://openreview.net/forum?id=7DY7kB8wyZ",
      "pdf_url": "https://openreview.net/pdf/ffce87a9963c0d0f7b4c3bddfa8f44e771bea3e7.pdf",
      "primary_category": "mechanistic interpretability, correction, self-repair",
      "categories": [
        "mechanistic interpretability",
        "correction",
        "self-repair"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "NzPwDutzz8",
      "title": "rStar-Coder: Scaling Competitive Code Reasoning  with a  Large-Scale Verified  Dataset",
      "authors": [
        "Yifei Liu",
        "Li Lyna Zhang",
        "Yi Zhu",
        "bingcheng dong",
        "Xudong Zhou",
        "Ning Shang",
        "Fan Yang",
        "Cheng Li",
        "Mao Yang"
      ],
      "abstract": "Advancing code reasoning in large language models (LLMs) is fundamentally limited by the  scarcity of high-difficulty datasets, especially  those with verifiable input-output test cases necessary for rigorous solution validation at scale. We introduce rStar-Coder,  which significantly improves LLM  code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems,  580K long-reasoning solutions  along with rich test cases of varying difficulty. This is achieved through three core contributions: (1) we curate competitive programming code problems and solutions  to synthesize new, solvable problems; (2) we introduce a reliable input-output test case synthesis pipeline  that decouples the  generation  into a three-step input generation method and a mutual verification mechanism for effective output labeling; (3) we augment problems with high-quality, test-case-verified   long-reasoning solutions. Extensive experiments on Qwen models (1.5B-14B) across various  code reasoning benchmarks demonstrate the superiority of rStar-Coder dataset, achieving leading performance comparable to frontier reasoning LLMs with significantly smaller model sizes. On LiveCodeBench, rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%,  and Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by 3.1%. On the more challenging USA Computing Olympiad, our 7B model achieves an average pass@1 accuracy of 16.15%, outperforming the frontier-level QWQ-32B.  rStar-Coder dataset is publicly available at https://huggingface.co/datasets/microsoft/rStar-Coder.",
      "arxiv_url": "https://openreview.net/forum?id=NzPwDutzz8",
      "pdf_url": "https://openreview.net/pdf/47a52d71da9c3de1a4e4800b7ec96376cc81de95.pdf",
      "primary_category": "Large Language Models, Code Reasoning, Test Case Generation",
      "categories": [
        "Large Language Models",
        "Code Reasoning",
        "Test Case Generation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xUx2B2NHvj",
      "title": "Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections",
      "authors": [
        "Bo Wang",
        "Qinyuan Cheng",
        "Runyu Peng",
        "Rong Bao",
        "Peiji Li",
        "Qipeng Guo",
        "Linyang Li",
        "Zhiyuan Zeng",
        "Yunhua Zhou",
        "Xipeng Qiu"
      ],
      "abstract": "Post-training processes are essential phases in grounding pre-trained language models to real-world tasks, with learning from demonstrations or preference signals playing a crucial role in this adaptation. We present a unified theoretical framework bridging Supervised Fine-Tuning (SFT) and preference learning in Large Language Model (LLM) post-training. Through rigorous mathematical derivation, we demonstrate that both SFT and preference learning methods like Direct Preference Optimization (DPO) operate within the same optimal policy-reward subspace, with SFT representing a special case of implicit reward learning. Our analysis reveals a critical limitation in conventional SFT: the KL divergence term in distribution matching becomes constant with respect to the policy during optimization, failing to constrain model updates. To address this, we propose a simple yet effective learning rate reduction approach that yields significant performance improvements (up to \\textbf{25\\%} relative gain and \\textbf{6\\%} absolute win rate increase in instruction following tasks. Additionally, we derive alternative SFT objectives from various f-divergence functions that preserve the KL term during optimization, further enhancing post-DPO model performance. Finally, we extend the theoretical relationship between LLM logits and Q-functions from preference learning to the SFT context, providing mathematical derivations and experimental validation.",
      "arxiv_url": "https://openreview.net/forum?id=xUx2B2NHvj",
      "pdf_url": "https://openreview.net/pdf/c0faecf1e4fea8aa3ae47211a192a153eb2e4f19.pdf",
      "primary_category": "Natural Language Processing, Large Language Model, Reinforcement Learning",
      "categories": [
        "Natural Language Processing",
        "Large Language Model",
        "Reinforcement Learning",
        "Markov Decision Processes",
        "Post-training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "5wdssRcI2Z",
      "title": "DoseSurv: Predicting Personalized Survival Outcomes under Continuous-Valued Treatments",
      "authors": [
        "Moritz Gögl",
        "Yu Liu",
        "Christopher Yau",
        "Peter Watkinson",
        "Tingting Zhu"
      ],
      "abstract": "Estimating heterogeneous treatment effects (HTEs) of continuous-valued interventions on survival, that is, time-to-event (TTE) outcomes, is crucial in various fields, notably in clinical decision-making and in driving the advancement of next-generation clinical trials. However, while HTE estimation for continuous-valued (i.e., dosage-dependent) interventions and for TTE outcomes have been separately explored, their combined application remains largely overlooked in the machine learning literature. We propose DoseSurv, a varying-coefficient network designed to estimate HTEs for different dosage-dependent and non-dosage treatment options from TTE data. DoseSurv uses radial basis functions to model continuity in dose-response relationships and learns balanced representations to address covariate shifts arising in HTE estimation from observational TTE data. We present experiments across various treatment scenarios on both simulated and real-world data, demonstrating DoseSurv's superior performance over existing baseline models.",
      "arxiv_url": "https://openreview.net/forum?id=5wdssRcI2Z",
      "pdf_url": "https://openreview.net/pdf/3fa66d2fd9b3c6c8d1e93cd2cb900ee50df0dfa3.pdf",
      "primary_category": "Causal Machine Learning, Personalized Survival Models, Time-to-Event Analysis",
      "categories": [
        "Causal Machine Learning",
        "Personalized Survival Models",
        "Time-to-Event Analysis",
        "Treatment Effect Heterogeneity",
        "Dose-Response Modeling"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4YKlo58RcQ",
      "title": "Scaling and context steer LLMs along the same computational path as the human brain",
      "authors": [
        "Joséphine Raugel",
        "Jérémy Rapin",
        "Stéphane d'Ascoli",
        "Valentin Wyart",
        "Jean-Remi King"
      ],
      "abstract": "Recent studies suggest that the representations learned by large language models (LLMs) are partially aligned to those of the human brain. \nHowever, whether this representational alignment arises from a similar sequence of computations remains elusive. \n\nIn this study, we explore this question by examining temporally-resolved brain signals of participants listening to 10 hours of an audiobook. \nWe study these neural dynamics jointly with a benchmark encompassing 17 LLMs varying in size and architecture type. \n\nOur analyses reveal that LLMs and the brain generate representations in a similar order: specifically, activations in the initial layers of LLMs tend to best align with early brain responses, while the deeper layers of LLMs tend to best align with later brain responses. \n\nThis brain-LLM alignment is consistent across transformers and recurrent architectures. \nHowever, its emergence depends on both model size and context length. \n\nOverall, the alignment between LLMs and the brain provides novel elements supporting a partial convergence between biological and artificial neural networks.",
      "arxiv_url": "https://openreview.net/forum?id=4YKlo58RcQ",
      "pdf_url": "https://openreview.net/pdf/b8c8c28fe378b160c7c257bc94581bbe1028ab4e.pdf",
      "primary_category": "Large Language Models, LLMs, Emergence",
      "categories": [
        "Large Language Models",
        "LLMs",
        "Emergence",
        "Neuroscience",
        "Brain",
        "Alignment",
        "Language",
        "Magnetoencephalography"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "o8n5oNDsiq",
      "title": "MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem",
      "authors": [
        "Fan Liu",
        "Zhe-Rui Yang",
        "Cancheng Liu",
        "Tianrui SONG",
        "Xiaofeng Gao",
        "Hao Liu"
      ],
      "abstract": "Mathematical modeling is a cornerstone of scientific discovery and engineering practice, enabling the translation of real-world problems into formal systems across domains such as physics, biology, and economics. Unlike mathematical reasoning, which assumes a predefined formulation, modeling requires open-ended problem analysis, abstraction, and principled formalization. While Large Language Models (LLMs) have shown strong reasoning capabilities, they fall short in rigorous model construction, limiting their utility in real-world problem-solving. To this end, we formalize the task of LLM-powered real-world mathematical modeling, where agents must analyze problems, construct domain-appropriate formulations, and generate complete end-to-end solutions.We introduce MM-Bench, a curated benchmark of 111 problems from the Mathematical Contest in Modeling (MCM/ICM), spanning the years 2000 to 2025 and across ten diverse domains such as physics, biology, and economics.  To tackle this task, we propose MM-Agent, an expert-inspired framework that decomposes mathematical modeling into four stages: open-ended problem analysis, structured model formulation, computational problem solving, and report generation.Experiments on MM-Bench show that MM-Agent significantly outperforms baseline agents, achieving an 11.88\\% improvement over human expert solutions while requiring only 15 minutes and \\$0.88 per task using GPT-4o. Furthermore, under official MCM/ICM protocols, MM-Agent assisted two undergraduate teams in winning the Finalist Award (\\textbf{top 2.0\\% among 27,456 teams}) in MCM/ICM 2025, demonstrating its practical effectiveness as a modeling copilot.",
      "arxiv_url": "https://openreview.net/forum?id=o8n5oNDsiq",
      "pdf_url": "https://openreview.net/pdf/e01a39be0de98c2f808394f7affa2bfb004c68a5.pdf",
      "primary_category": "Mathematical Modeling Agent, LLM Agent, LLM",
      "categories": [
        "Mathematical Modeling Agent",
        "LLM Agent",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZwBtDbuzjY",
      "title": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models",
      "authors": [
        "Yanggan Gu",
        "Yuanyi Wang",
        "Zhaoyi Yan",
        "Yiming Zhang",
        "Qi Zhou",
        "Fei Wu",
        "Hongxia Yang"
      ],
      "abstract": "Model fusion combines multiple Large Language Models (LLMs) with different strengths into a more powerful, integrated model through lightweight training methods. Existing works on model fusion focus primarily on supervised fine-tuning (SFT), leaving preference alignment (PA) —a critical phase for enhancing LLM performance—largely unexplored.\nThe current few fusion methods on PA phase, like WRPO, simplify the process by utilizing only response outputs from source models while discarding their probability information. To address this limitation, we propose InfiFPO, a preference optimization method for implicit model fusion.\nInfiFPO replaces the reference model in Direct Preference Optimization (DPO) with a fused source model that synthesizes multi-source probabilities at the sequence level, circumventing complex vocabulary alignment challenges in previous works and meanwhile maintaining the probability information.\nBy introducing probability clipping and max-margin fusion strategies, InfiFPO enables the pivot model to align with human preferences while effectively distilling knowledge from source models.\nComprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO consistently outperforms existing model fusion and preference optimization methods. When using Phi-4 as the pivot model, InfiFPO improves its average performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its capabilities in mathematics, coding, and reasoning tasks.",
      "arxiv_url": "https://openreview.net/forum?id=ZwBtDbuzjY",
      "pdf_url": "https://openreview.net/pdf/f4520d671b2e16d30997ffc361f5a1dfd6a9feeb.pdf",
      "primary_category": "Model Fusion, Preference Optimization, LLMs",
      "categories": [
        "Model Fusion",
        "Preference Optimization",
        "LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XMzxZ6h68o",
      "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
      "authors": [
        "Roberto L. Castro",
        "Andrei Panferov",
        "Soroush Tabesh",
        "Oliver Sieberling",
        "Jiale Chen",
        "Mahdi Nikdan",
        "Saleh Ashkboos",
        "Dan Alistarh"
      ],
      "abstract": "Training large language models (LLMs) models directly in low-precision  offers a way to address computational costs by improving both throughput and energy efficiency. For those purposes, NVIDIA's recent Blackwell architecture facilitates very low-precision operations using FP4 variants. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we  investigate hardware-supported FP4 training and introduce a new approach for accurate, end-to-end FP4 training with all the major computations (i.e., linear layers) in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across bit-widths and training setups. Guided by this investigation, we design an \"optimal\" technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for Blackwell, demonstrating that fully FP4-based training is a competitive alternative to FP16 half-precision and to FP8 training. Our code is available at https://github.com/IST-DASLab/Quartet .",
      "arxiv_url": "https://openreview.net/forum?id=XMzxZ6h68o",
      "pdf_url": "https://openreview.net/pdf/51ac50abe81e42a3d22ab4ff1313c30bb40e24fc.pdf",
      "primary_category": "quantized training, large language models",
      "categories": [
        "quantized training",
        "large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yyWeSAsOhs",
      "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing",
      "authors": [
        "Junfei Wu",
        "Jian Guan",
        "Kaituo Feng",
        "Qiang Liu",
        "Shu Wu",
        "Liang Wang",
        "Wei Wu",
        "Tieniu Tan"
      ],
      "abstract": "As textual reasoning with large language models (LLMs) has advanced significant, there has been growing interest in enhancing the multimodal reasoning capabilities of large vision-language models (LVLMs). However, existing methods primarily approach multimodal reasoning in a straightforward, text-centric manner, where both reasoning and answer derivation are conducted purely through text, with the only difference being the presence of multimodal input. As a result, these methods often encounter fundamental limitations in spatial reasoning tasks that demand precise geometric understanding and continuous spatial tracking\\textemdash capabilities that humans achieve through mental visualization and manipulation. To address the limitations, we propose drawing to reason in space, a novel paradigm that enables LVLMs to reason through elementary drawing operations in the visual space. By equipping models with basic drawing operations including annotating bounding boxes and drawing auxiliary lines, we empower them to express and analyze spatial relationships through direct visual manipulation, meanwhile avoiding the performance ceiling imposed by specialized perception tools in previous tool-integrated reasoning approaches. To cultivate this capability, we develop a three-stage training framework: cold-start training with synthetic data to establish basic drawing abilities, reflective rejection sampling to enhance self-reflection behaviors, and reinforcement learning to directly optimize for target rewards. Extensive experiments demonstrate that our model, named \\textsc{Spark}, consistently outperforms existing methods across diverse spatial reasoning benchmarks involving maze navigation, static spatial reasoning, video-based reasoning and multi-view-based reasoning tasks, with an average improvement of 11.5\\%. Ablation studies reveal the critical role of each training stage, with reflective rejection sampling particularly enhancing the model's self-correction capabilities and reasoning potential.",
      "arxiv_url": "https://openreview.net/forum?id=yyWeSAsOhs",
      "pdf_url": "https://openreview.net/pdf/0da039d9be67a74526e2ce4408db4cde8bdca6f5.pdf",
      "primary_category": "Large Vision-Language Models, Spatial Reasoning",
      "categories": [
        "Large Vision-Language Models",
        "Spatial Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QH2xGLgObM",
      "title": "Alleviating Hallucinations in Large Language Models through Multi-Model Contrastive Decoding and Dynamic Hallucination Detection",
      "authors": [
        "Chenyu Zhu",
        "YEFENG LIU",
        "Hao Zhang",
        "Aowen Wang",
        "Yangxue",
        "Guanhua Chen",
        "Longyue Wang",
        "Weihua Luo",
        "Kaifu Zhang"
      ],
      "abstract": "Despite their outstanding performance in numerous applications, large language models (LLMs) remain prone to hallucinations, generating content inconsistent with their pretraining corpora. Currently, almost all contrastive decoding approaches alleviate hallucinations by introducing a model susceptible to hallucinations and appropriately widening the contrastive logits gap between hallucinatory tokens and target tokens. However, although existing contrastive decoding methods mitigate hallucinations, they lack enough confidence in the factual accuracy of the generated content. In this work, we propose Multi-Model Contrastive Decoding (MCD), which integrates a pretrained language model with an evil model and a truthful model for contrastive decoding. Intuitively, a token is assigned a high probability only when deemed potentially hallucinatory by the evil model while being considered factual by the truthful model. This decoding strategy significantly enhances the model’s confidence in its generated responses and reduces potential hallucinations. Furthermore, we introduce a dynamic hallucination detection mechanism that facilitates token-by-token identification of hallucinations during generation and a tree-based revision mechanism to diminish hallucinations further. Extensive experimental evaluations demonstrate that our MCD strategy effectively reduces hallucinations in LLMs and outperforms state-of-the-art methods across various benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=QH2xGLgObM",
      "pdf_url": "https://openreview.net/pdf/1f272df0b957442011a183bd28a3a342f07576fc.pdf",
      "primary_category": "Alleviating Hallucinations, Contrastive Decoding, Large Language Model",
      "categories": [
        "Alleviating Hallucinations",
        "Contrastive Decoding",
        "Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "fYW1PKawwJ",
      "title": "STRATUS: A Multi-agent System for Autonomous Reliability Engineering of Modern Clouds",
      "authors": [
        "Yinfang Chen",
        "Jiaqi Pan",
        "Jackson Clark",
        "Yiming Su",
        "Noah Zheutlin",
        "Bhavya Bhavya",
        "Rohan R. Arora",
        "Yu Deng",
        "Saurabh Jha",
        "Tianyin Xu"
      ],
      "abstract": "In cloud-scale systems, failures are the norm. A distributed computing cluster exhibits hundreds of machine failures and thousands of disk failures; software bugs and misconfigurations are reported to be more frequent. The demand for autonomous, AI-driven reliability engineering continues to grow, as existing human-in-the-loop practices can hardly keep up with the scale of modern clouds. This paper presents STRATUS, an LLM-based multi-agent system for realizing autonomous Site Reliability Engineering (SRE) of cloud services. STRATUS consists of multiple specialized agents (e.g., for failure detection, diagnosis, mitigation), organized in a state machine to assist system-level safety reasoning and enforcement. We formalize a key safety specification of agentic SRE systems like STRATUS, termed Transactional No-Regression (TNR), which enables safe exploration and iteration. We show that TNR can effectively improve autonomous failure mitigation. STRATUS significantly outperforms state-of-the-art SRE agents in terms of success rate of failure mitigation problems in AIOpsLab and ITBench (two SRE benchmark suites), by at least 1.5 times across various models. STRATUS shows a promising path toward practical deployment of agentic systems for cloud reliability.",
      "arxiv_url": "https://openreview.net/forum?id=fYW1PKawwJ",
      "pdf_url": "https://openreview.net/pdf/5a5a7a30817aa2bfb216cd0d79534326bbdbca0d.pdf",
      "primary_category": "Agentic AI, Large Language Model, System Reliability",
      "categories": [
        "Agentic AI",
        "Large Language Model",
        "System Reliability"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "r3LYE0Ct3G",
      "title": "PlanU: Large Language Model Reasoning through Planning under Uncertainty",
      "authors": [
        "Ziwei Deng",
        "Mian Deng",
        "Chenjing Liang",
        "Zeming Gao",
        "Chennan Ma",
        "Chenxing Lin",
        "Haipeng Zhang",
        "Songzhu Mei",
        "Siqi Shen",
        "Cheng Wang"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being explored across a range of reasoning tasks. However, LLMs sometimes struggle with reasoning tasks under uncertainty that are relatively easy for humans, such as planning actions in stochastic environments. The adoption of LLMs for reasoning is impeded by uncertainty challenges, such as LLM uncertainty and environmental uncertainty. LLM uncertainty arises from the stochastic sampling process inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM uncertainty through multiple reasoning chains or search trees. However, these approaches overlook environmental uncertainty, which leads to poor performance in environments with stochastic state transitions. \nSome recent LDM approaches deal with uncertainty by forecasting the probability of unknown variables. However, they are not designed for multi-step reasoning tasks that require interaction with the environment. To address uncertainty in LLM decision-making, we introduce PlanU, an LLM-based planning method that captures uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of each node in the MCTS as a quantile distribution, which uses a set of quantiles to represent the return distribution. To balance exploration and exploitation during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity (UCC) score which estimates the uncertainty of MCTS nodes. Through extensive experiments, we demonstrate the effectiveness of PlanU in LLM-based reasoning tasks under uncertainty.",
      "arxiv_url": "https://openreview.net/forum?id=r3LYE0Ct3G",
      "pdf_url": "https://openreview.net/pdf/a9771087543396ca7e59296ca5d3d68429e5a708.pdf",
      "primary_category": "large language models; decision making; reasoning",
      "categories": [
        "large language models; decision making; reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "bEP87LNTfX",
      "title": "Bridging Human and LLM Judgments: Understanding and Narrowing the Gap",
      "authors": [
        "Felipe Maia Polo",
        "Xinhe Wang",
        "Mikhail Yurochkin",
        "Gongjun Xu",
        "Moulinath Banerjee",
        "Yuekai Sun"
      ],
      "abstract": "Large language models are increasingly used as judges (LLM-as-a-judge) to evaluate model outputs at scale, but their assessments often diverge systematically from human judgments. We present Bridge, a unified statistical framework that explicitly bridges human and LLM evaluations under both absolute scoring and pairwise comparison paradigms. Bridge posits a latent human preference score for each prompt-response pair and models LLM deviations as linear transformations of covariates that capture sources of discrepancies. This offers a simple and principled framework for refining LLM ratings and characterizing systematic discrepancies between humans and LLMs. We provide an efficient fitting algorithm with asymptotic guarantees for statistical inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot Arena), Bridge achieves higher agreement with human ratings (accuracy, calibration, and KL divergence) and exposes systematic human-LLM gaps.",
      "arxiv_url": "https://openreview.net/forum?id=bEP87LNTfX",
      "pdf_url": "https://openreview.net/pdf/3b238b3324d5ab3f3e5a672a12a2c7610ee13a48.pdf",
      "primary_category": "llm-as-a-judge, alignment, bias",
      "categories": [
        "llm-as-a-judge",
        "alignment",
        "bias",
        "calibration"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "loznSxLomv",
      "title": "FACE: A General Framework for Mapping Collaborative Filtering Embeddings into LLM Tokens",
      "authors": [
        "Chao Wang",
        "Yixin Song",
        "Jinhui Ye",
        "Chuan Qin",
        "Dazhong Shen",
        "Lingfeng Liu",
        "Xiang Wang",
        "Yanyong Zhang"
      ],
      "abstract": "Recently, large language models (LLMs) have been explored for integration with collaborative filtering (CF)-based recommendation systems, which are crucial for personalizing user experiences. However, a key challenge is that LLMs struggle to interpret the latent, non-semantic embeddings produced by CF approaches, limiting recommendation effectiveness and further applications. To address this, we propose FACE, a general interpretable framework that maps CF embeddings into pre-trained LLM tokens. Specifically, we introduce a disentangled projection module to decompose CF embeddings into concept-specific vectors, followed by a quantized autoencoder to convert continuous embeddings into LLM tokens (descriptors). Then, we design a contrastive alignment objective to ensure that the tokens align with corresponding textual signals. Hence, the model-agnostic FACE framework achieves semantic alignment without fine-tuning LLMs and enhances recommendation performance by leveraging their pre-trained capabilities. Empirical results on three real-world recommendation datasets demonstrate performance improvements in benchmark models, with interpretability studies confirming the interpretability of the descriptors. Code is available in \\url{https://github.com/YixinRoll/FACE}.",
      "arxiv_url": "https://openreview.net/forum?id=loznSxLomv",
      "pdf_url": "https://openreview.net/pdf/019be1635823c973d5268044501dd31003932b80.pdf",
      "primary_category": "Recommendation System, Large Language Models, Interpretability",
      "categories": [
        "Recommendation System",
        "Large Language Models",
        "Interpretability"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "jZs26lJ0pl",
      "title": "Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment",
      "authors": [
        "Pengfei Zhao",
        "Rongbo Luan",
        "Wei Zhang",
        "Peng Wu",
        "Sifeng He"
      ],
      "abstract": "Despite Contrastive Language–Image Pre-training (CLIP)'s remarkable capability to retrieve content across modalities, a substantial modality gap persists in its feature space. Intriguingly, we discover that off-the-shelf MLLMs (Multimodal Large Language Models) demonstrate powerful inherent modality alignment properties. While recent MLLM-based retrievers with unified architectures partially mitigate this gap, their reliance on coarse modality alignment mechanisms fundamentally limits their potential. In this work, We introduce MAPLE (Modality-Aligned Preference Learning for Embeddings), a novel framework that leverages the fine-grained alignment priors inherent in MLLM to guide cross-modal representation learning. MAPLE formulates the learning process as reinforcement learning with two key components: (1) Automatic preference data construction using off-the-shelf MLLM, and (2) a new Relative Preference Alignment (RPA) loss, which adapts Direct Preference Optimization (DPO) to the embedding learning setting. Experimental results show that our preference-guided alignment achieves substantial gains in fine-grained cross-modal retrieval, underscoring its effectiveness in handling nuanced semantic distinctions.",
      "arxiv_url": "https://openreview.net/forum?id=jZs26lJ0pl",
      "pdf_url": "https://openreview.net/pdf/c723c56aa4a0547ac7d379c542315db2d4c1c330.pdf",
      "primary_category": "Multimodal Large Language Models, Preference learning, Cross-modality retreival",
      "categories": [
        "Multimodal Large Language Models",
        "Preference learning",
        "Cross-modality retreival",
        "fine-grained retrieval"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zZecO3RZ7Z",
      "title": "Datasets, Documents, and Repetitions: The Practicalities of Unequal Data Quality",
      "authors": [
        "Alex Fang",
        "Hadi Pouransari",
        "Matt Jordan",
        "Alexander T Toshev",
        "Vaishaal Shankar",
        "Ludwig Schmidt",
        "Tom Gunter"
      ],
      "abstract": "Data filtering has become a powerful tool for improving model performance while reducing computational cost. However, as large language model compute budgets continue to grow, the limited data volume provided by heavily filtered and deduplicated datasets will become a practical constraint. In efforts to better understand how to proceed, we study model performance at various compute budgets and across multiple pre-training datasets created through data filtering and deduplication. We find that, given appropriate modifications to the training recipe, repeating existing aggressively filtered datasets for up to ten epochs can outperform training on the ten times larger superset for a single epoch across multiple compute budget orders of magnitude. While this finding relies on repeating the dataset for many epochs, we also investigate repeats within these datasets at the document level. We find that not all documents within a dataset are equal, and we can create better datasets relative to a token budget by explicitly manipulating the counts of individual documents. We conclude by arguing that even as large language models scale, data filtering remains an important direction of research.",
      "arxiv_url": "https://openreview.net/forum?id=zZecO3RZ7Z",
      "pdf_url": "https://openreview.net/pdf/5dda0ba4fbf67c1b65e7147ccec64df745e7d1a6.pdf",
      "primary_category": "datasets, llm, repetition",
      "categories": [
        "datasets",
        "llm",
        "repetition",
        "deduplication",
        "scaling",
        "pre-training",
        "language models",
        "data filtering"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DpOSndSOZz",
      "title": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens",
      "authors": [
        "Xixian Yong",
        "Xiao Zhou",
        "Yingying Zhang",
        "Jinlin Li",
        "Yefeng Zheng",
        "Xian Wu"
      ],
      "abstract": "The recent rise of Large Reasoning Models (LRMs) has significantly improved multi-step reasoning performance, but often at the cost of generating excessively long reasoning chains. This paper revisits the efficiency of such reasoning processes through an information-theoretic lens, revealing a fundamental trade-off between reasoning length and semantic efficiency. We propose two metrics—InfoBias and InfoGain—to quantify divergence from ideal reasoning paths and stepwise information contribution, respectively. Empirical analyses show that longer reasoning chains tend to exhibit higher information bias and diminishing information gain, especially for incorrect answers. Motivated by these findings, we introduce an entropy-based Adaptive Think strategy that dynamically halts reasoning once confidence is sufficiently high, improving efficiency while maintaining competitive accuracy. Compared to the Vanilla Think approach (default mode), our strategy yields a 1.10% improvement in average accuracy and a 50.80% reduction in token usage on QwQ-32B across six benchmark tasks spanning diverse reasoning types and difficulty levels, demonstrating superior efficiency and reasoning performance. These results underscore the promise of entropy-based methods for enhancing both accuracy and cost-effiiciency in large language model deployment.",
      "arxiv_url": "https://openreview.net/forum?id=DpOSndSOZz",
      "pdf_url": "https://openreview.net/pdf/72b099c2f4257233c570fee63033ba4ef835dec3.pdf",
      "primary_category": "efficient reasoning",
      "categories": [
        "efficient reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3QjESmXftM",
      "title": "Exploring the Translation Mechanism of Large Language Models",
      "authors": [
        "Hongbin Zhang",
        "Kehai Chen",
        "Xuefeng Bai",
        "Xiucheng Li",
        "Yang Xiang",
        "Min Zhang"
      ],
      "abstract": "While large language models (LLMs) demonstrate remarkable success in multilingual translation, their internal core translation mechanisms, even at the fundamental word level, remain insufficiently understood.\nTo address this critical gap, this work introduces a systematic framework for interpreting the mechanism behind LLM translation from the perspective of computational components. \nThis paper first proposes subspace-intervened path patching for precise, fine-grained causal analysis, enabling the detection of components crucial to translation tasks and subsequently characterizing their behavioral patterns in human-interpretable terms.\nComprehensive experiments reveal that translation is predominantly driven by a sparse subset of components: specialized attention heads serve critical roles in extracting source language, translation indicators, and positional features, which are then integrated and processed by specific multi-layer perceptrons (MLPs) into intermediary English-centric latent representations before ultimately yielding the final translation.\nThe significance of these findings is underscored by the empirical demonstration that targeted fine-tuning a minimal parameter subset (<5%) enhances translation performance while preserving general capabilities. This result further indicates that these crucial components generalize effectively to sentence-level translation and are instrumental in elucidating more intricate translation tasks.",
      "arxiv_url": "https://openreview.net/forum?id=3QjESmXftM",
      "pdf_url": "https://openreview.net/pdf/efeb69a98a098762e7e6276b2046aee3f1b7fbb1.pdf",
      "primary_category": "Large Language Model, Multilingual, Machine Translation",
      "categories": [
        "Large Language Model",
        "Multilingual",
        "Machine Translation",
        "Interpretability and Analysis of Models for NLP",
        "Multilingualism and Cross-Lingual NLP"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "TkEdQv0bXB",
      "title": "Hyperbolic Fine-Tuning for Large Language Models",
      "authors": [
        "Menglin Yang",
        "Ram Samarth B B",
        "Aosong Feng",
        "Bo Xiong",
        "Jiahong Liu",
        "Irwin King",
        "Rex Ying"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable performance across various tasks. However, it remains an open question whether the default Euclidean space is the most suitable choice for LLMs.\nIn this study, we investigate the geometric characteristics of LLMs, focusing specifically on tokens and their embeddings.\nOur findings reveal that token frequency follows a power-law distribution, where high-frequency tokens (e.g., the, that ) constitute the minority, while low-frequency tokens (e.g., apple, dog) constitute the majority. Furthermore, high-frequency tokens cluster near the origin, whereas low-frequency tokens are positioned farther away in the embedding space.\nAdditionally, token embeddings exhibit hyperbolic characteristics, indicating a latent tree-like structure within the embedding space.\nMotivated by these observations, we propose **HypLoRA**, an efficient fine-tuning approach that operates in hyperbolic space to exploit these underlying hierarchical structures better.\nHypLoRA performs low-rank adaptation directly in hyperbolic space, thereby preserving hyperbolic modeling capabilities throughout the fine-tuning process.\nExtensive experiments across various base models and reasoning benchmarks, specifically arithmetic and commonsense reasoning tasks, demonstrate that HypLoRA substantially improves LLM performance.",
      "arxiv_url": "https://openreview.net/forum?id=TkEdQv0bXB",
      "pdf_url": "https://openreview.net/pdf/00b2b881206aa7521841f800e1698b282fba8e33.pdf",
      "primary_category": "Large Language Models, Hyperbolic Space, Low-Rank Adaptation",
      "categories": [
        "Large Language Models",
        "Hyperbolic Space",
        "Low-Rank Adaptation",
        "Embedding Space"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Xuvdo6oMkE",
      "title": "Rethinking the Role of Verbatim Memorization in LLM Privacy",
      "authors": [
        "Tom Sander",
        "Bargav Jayaraman",
        "Mark Ibrahim",
        "Kamalika Chaudhuri",
        "Chuan Guo"
      ],
      "abstract": "Conventional wisdom in machine learning privacy research states that memorization \ndirectly implies a loss of privacy. In contrast, a well-generalized model only remembers distributional patterns and preserves privacy of its training data.  In this work, we show that this relationship is much more complex for LLMs trained for chat,\nand depends heavily on how knowledge is encoded and manipulated. To this end, we fine-tune language models on synthetically generated biographical information including PIIs, and try to extract them in different ways after instruction fine-tuning.  We find counter to conventional wisdom that better verbatim memorization does not necessarily increase data leakage via chat. We also find that it is easier to extract information via chat from an LLM that is better able to manipulate and process knowledge even if it is smaller, and that not all attributes are equally extractable. This suggests that the relationship between privacy, memorization and language understanding of LLMs is very intricate, and that examining memorization in isolation can lead to misleading conclusions.",
      "arxiv_url": "https://openreview.net/forum?id=Xuvdo6oMkE",
      "pdf_url": "https://openreview.net/pdf/bedba0c6d4b699e2489c89ed123f6287565aed0f.pdf",
      "primary_category": "LLM;memorization;privacy",
      "categories": [
        "LLM;memorization;privacy"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "B4NT8TexNS",
      "title": "InvisibleInk: High-Utility and Low-Cost Text Generation with Differential Privacy",
      "authors": [
        "Vishnu Vinod",
        "Krishna Pillutla",
        "Abhradeep Guha Thakurta"
      ],
      "abstract": "As major progress in LLM-based long-form text generation enables paradigms such as retrieval-augmented generation (RAG) and inference-time scaling, safely incorporating private information into the generation remains a critical open question. We present InvisibleInk, a highly scalable long-form text generation framework satisfying rigorous differential privacy guarantees with respect to the sensitive reference texts. It interprets sampling from the LLM's next-token-distribution as the exponential mechanism over the LLM logits with two innovations. First, we reduce the privacy cost by isolating and clipping only the sensitive information in the model logits (relative to the public logits). Second, we improve text quality by sampling without any privacy cost from a small superset of the top-$k$ private tokens. Empirical evaluations demonstrate a consistent $8\\times$ (or more) reduction in computation cost over state-of-the-art baselines to generate long-form private text of the same utility across privacy levels. InvisibleInk is able to generate, for the first time, high-quality private long-form text at less than $4\\text{-}8\\times$ times the computation cost of non-private generation, paving the way for its practical use. We open-source a pip-installable Python package (invink) for InvisibleInk at https://github.com/cerai-iitm/invisibleink.",
      "arxiv_url": "https://openreview.net/forum?id=B4NT8TexNS",
      "pdf_url": "https://openreview.net/pdf/11c28d8e913c61ef80e2875b9b045d275b043555.pdf",
      "primary_category": "Differential Privacy, Text Generation, Synthetic Data",
      "categories": [
        "Differential Privacy",
        "Text Generation",
        "Synthetic Data",
        "Private Inference"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3FsM6wWQL4",
      "title": "HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs",
      "authors": [
        "Ningning CHEN",
        "Weicai Ye",
        "Ying Jiang"
      ],
      "abstract": "We introduce HBLLM, a wavelet-enhanced high-fidelity $1$-bit post-training quantization method for Large Language Models (LLMs). By leveraging Haar wavelet transforms to enhance expressive capacity through frequency decomposition, HBLLM significantly improves quantization fidelity while maintaining minimal overhead. This approach features two innovative structure-aware grouping strategies: (1) frequency-aware multi-parameter intra-row grouping and (2) $\\ell_2$-norm-based saliency-driven column selection. For non-salient weights, a shared mean is employed across quantization groups within each frequency band to optimize storage efficiency. Experiments conducted on the OPT and LLaMA models demonstrate that HBLLM achieves state-of-the-art performance in $1$-bit quantization, attaining a perplexity of $6.71$  on LLaMA$2$-$13$B with an average weight storage of only $1.08$ bits. Code available at: https://github.com/Yeyke/HBLLM.",
      "arxiv_url": "https://openreview.net/forum?id=3FsM6wWQL4",
      "pdf_url": "https://openreview.net/pdf/e817d7a8ff5f9f6dd2b20d3305b4fb7e986f3998.pdf",
      "primary_category": "1-Bit Quantization；Haar-based",
      "categories": [
        "1-Bit Quantization；Haar-based"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0jQUNQsZra",
      "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning",
      "authors": [
        "Xiao Liang",
        "Zhong-Zhi Li",
        "Yeyun Gong",
        "Yang Wang",
        "Hengyuan Zhang",
        "yelong shen",
        "Ying Nian Wu",
        "Weizhu Chen"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for training large language models (LLMs) on complex reasoning tasks, such as mathematical problem solving. A prerequisite for the scalability of RLVR is a high-quality problem set with precise and verifiable answers. However, the scarcity of well-crafted human-labeled math problems and limited-verification answers in existing distillation-oriented synthetic datasets limit their effectiveness in RL. Additionally, most problem synthesis strategies indiscriminately expand the problem set without considering the model’s capabilities, leading to low efficiency in generating useful questions. To mitigate this issue, we introduce a Self-aware Weakness-driven problem Synthesis framework (SwS) that systematically identifies model deficiencies and leverages them for problem augmentation. Specifically, we define weaknesses as questions that the model consistently fails to learn through its iterative sampling during RL training. We then extract the core concepts from these failure cases and synthesize new problems to strengthen the model's weak areas in subsequent augmented training, enabling it to focus on and gradually overcome its weaknesses. Without relying on external knowledge distillation, our framework enables robust generalization by empowering the model to self-identify and address its weaknesses in RL, yielding average performance gains of 10% and 7.7% on 7B and 32B models across eight mainstream reasoning benchmarks. Our code and data are available at https://anonymous.4open.science/r/SwS-E6F5/",
      "arxiv_url": "https://openreview.net/forum?id=0jQUNQsZra",
      "pdf_url": "https://openreview.net/pdf/c3403842de341324f63358f1732f1518761661f2.pdf",
      "primary_category": "Reinforcement Learning; LLM Reasoning; Data Synthesis",
      "categories": [
        "Reinforcement Learning; LLM Reasoning; Data Synthesis"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cmN8Wbvanr",
      "title": "Can Large Language Models Master Complex Card Games?",
      "authors": [
        "Wei Wang",
        "Fuqing Bie",
        "Junzhe Chen",
        "Dan Zhang",
        "Shiyu Huang",
        "Evgeny Kharlamov",
        "Jie Tang"
      ],
      "abstract": "Complex games have long been an important benchmark for testing the progress of artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have defeated top human players in Go and Chess, garnering widespread societal attention towards artificial intelligence. Concurrently, large language models (LLMs) have exhibited remarkable capabilities across various tasks, raising the question of whether LLMs can achieve similar success in complex games. In this paper, we explore the potential of LLMs in mastering complex card games. We systematically assess the learning capabilities of LLMs across eight diverse card games, evaluating the impact of fine-tuning on high-quality gameplay data, and examining the models' ability to retain general capabilities while mastering these games. Our findings indicate that: (1) LLMs can approach the performance of strong game AIs through supervised fine-tuning on high-quality data, (2) LLMs can achieve a certain level of proficiency in multiple complex card games simultaneously, with performance augmentation for games with similar rules and conflicts for dissimilar ones, and (3) LLMs experience a decline in general capabilities when mastering complex games, but this decline can be mitigated by integrating a certain amount of general instruction data. The evaluation results demonstrate strong learning ability and versatility of LLMs. The code is available at \nhttps://github.com/THUDM/LLM4CardGame",
      "arxiv_url": "https://openreview.net/forum?id=cmN8Wbvanr",
      "pdf_url": "https://openreview.net/pdf/1d7991c179addeedf33ce77dc2ffd0e475f69b2c.pdf",
      "primary_category": "Language Model, Evaluation",
      "categories": [
        "Language Model",
        "Evaluation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "V4oTkK7cQz",
      "title": "Risk-aware Direct Preference Optimization under Nested Risk Measure",
      "authors": [
        "Lijun Zhang",
        "Lin Li",
        "Yajie Qi",
        "Huizhong Song",
        "Yaodong Yang",
        "Jun Wang",
        "Wei Wei"
      ],
      "abstract": "When fine-tuning pre-trained Large Language Models (LLMs) to align with human values and intentions, maximizing the estimated reward can lead to superior performance, but it also introduces potential risks due to deviations from the reference model's intended behavior. Most existing methods typically introduce KL divergence to constrain deviations between the trained model and the reference model; however, this may not be sufficient in certain applications that require tight risk control. In this paper, we introduce Risk-aware Direct Preference Optimization (Ra-DPO), a novel approach that incorporates risk-awareness by employing a class of nested risk measures. This approach formulates a constrained risk-aware advantage function maximization problem and then converts the Bradley-Terry model into a token-level representation. The objective function maximizes the likelihood of the policy while suppressing the deviation between a trained model and the reference model using a sequential risk ratio, thereby enhancing the model's risk-awareness. Experimental results across three open-source datasets: IMDb Dataset, Anthropic HH Dataset, and AlpacaEval, demonstrate the proposed method's superior performance in balancing alignment performance and model drift.",
      "arxiv_url": "https://openreview.net/forum?id=V4oTkK7cQz",
      "pdf_url": "https://openreview.net/pdf/d02e9ac0da1b5b7b60893b5f640d4aabb34ed20f.pdf",
      "primary_category": "Large Language Models, LLM Alignment, Risk-Aware",
      "categories": [
        "Large Language Models",
        "LLM Alignment",
        "Risk-Aware"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "y68Q09Vc4K",
      "title": "DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering",
      "authors": [
        "Jiakai Li",
        "Rongzheng Wang",
        "Yizhuo Ma",
        "Shuang Liang",
        "Guangchun Luo",
        "Ke Qin"
      ],
      "abstract": "While large language models (LLMs) show considerable promise across various fields, they have notable limitations in handling multi-document question answering (Multi-doc QA) tasks. The first challenge is long-range dependency modeling, where LLMs struggle to focus on key information in long texts, which weakens important semantic connections. Second, most LLMs suffer from the ''lost-in-the-middle'' issue, where they have difficulty processing information in the middle of long inputs. Current solutions either truncate global dependencies or demand costly finetuning, ultimately lacking a universal and simple solution for these challenges. To resolve these limitations, we propose Dual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The Contextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by assessing paragraph relevance through layer-wise attention tracking and position-aware weighting. (ii) The Reciprocal Attention Suppression (RAS) module enhances focus on critical paragraphs by suppressing information exchange between key and irrelevant texts, thus mitigating the limitations in long-range dependency modeling. Extensive experiments on four benchmarks demonstrate DSAS's efficacy across mainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score improvement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and Qwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of both the CGW and RAS modules. In addition, detailed discussions in the Appendix further validate the robustness and scalability of DSAS.",
      "arxiv_url": "https://openreview.net/forum?id=y68Q09Vc4K",
      "pdf_url": "https://openreview.net/pdf/59bf1250207362af749c2dbc53cbd5dce7003759.pdf",
      "primary_category": "large language model, attention dilution, attention optimization",
      "categories": [
        "large language model",
        "attention dilution",
        "attention optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "FXBBy1caOX",
      "title": "Large Language Models for Lossless Image Compression: Next-Pixel Prediction in Language Space is All You Need",
      "authors": [
        "Kecheng Chen",
        "Pingping Zhang",
        "Hui Liu",
        "Jie Liu",
        "Yibing Liu",
        "Jiaxin Huang",
        "Shiqi Wang",
        "Hong Yan",
        "Haoliang Li"
      ],
      "abstract": "We have recently witnessed that ''Intelligence\" and `''Compression\" are the two sides of the same coin, where the language large model (LLM) with unprecedented intelligence is a general-purpose lossless compressor for various data modalities. This attribute is particularly appealing to the lossless image compression community, given the increasing need to compress high-resolution images in the current streaming media era. Consequently, a spontaneous envision emerges: Can the compression performance of the LLM elevate lossless image compression to new heights? However, our findings indicate that the naive application of LLM-based lossless image compressors suffers from a considerable performance gap compared with existing state-of-the-art (SOTA) codecs on common benchmark datasets. In light of this, we are dedicated to fulfilling the unprecedented intelligence (compression) capacity of the LLM for lossless image compression tasks, thereby bridging the gap between theoretical and practical compression performance. Specifically, we propose P -LLM, a next-pixel prediction-based LLM, which integrates various elaborated insights and methodologies, \\textit{e.g.,} pixel-level priors, the in-context ability of LLM, and a pixel-level semantic preservation strategy, to enhance the understanding capacity of pixel sequences for better next-pixel predictions. Extensive experiments on benchmark datasets demonstrate that P-LLM can beat SOTA classical and learned codecs.",
      "arxiv_url": "https://openreview.net/forum?id=FXBBy1caOX",
      "pdf_url": "https://openreview.net/pdf/c83175dbfd80082827c2c02c5ec566c35b7fab0d.pdf",
      "primary_category": "lossless image compression; large language model",
      "categories": [
        "lossless image compression; large language model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yzl5tL0Z2M",
      "title": "Semantic Representation Attack against Aligned Large Language Models",
      "authors": [
        "Jiawei Lian",
        "Jianhong Pan",
        "Lefan Wang",
        "Yi Wang",
        "Shaohui Mei",
        "Lap-Pui Chau"
      ],
      "abstract": "Large Language Models (LLMs) increasingly employ alignment techniques to prevent harmful outputs. Despite these safeguards, attackers can circumvent them by crafting prompts that induce LLMs to generate harmful content. Current methods typically target exact affirmative responses, suffering from limited convergence, unnatural prompts, and high computational costs. We introduce semantic representation attacks, a novel paradigm that fundamentally reconceptualizes adversarial objectives against aligned LLMs. Rather than targeting exact textual patterns, our approach exploits the semantic representation space that can elicit diverse responses that share equivalent harmful meanings. This innovation resolves the inherent trade-off between attack effectiveness and prompt naturalness that plagues existing methods. Our Semantic Representation Heuristic Search (SRHS) algorithm efficiently generates semantically coherent adversarial prompts by maintaining interpretability during incremental search. We establish rigorous theoretical guarantees for semantic convergence and demonstrate that SRHS achieves unprecedented attack success rates (89.4% averaged across 18 LLMs, including 100% on 11 models) while significantly reducing computational requirements. Extensive experiments show that our method consistently outperforms existing approaches.",
      "arxiv_url": "https://openreview.net/forum?id=yzl5tL0Z2M",
      "pdf_url": "https://openreview.net/pdf/bdfaeae5edd195b95b9442e336efa88e230e481d.pdf",
      "primary_category": "Semantic Representation, Attack, Aligned Large Language Models",
      "categories": [
        "Semantic Representation",
        "Attack",
        "Aligned Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "WDAKFpWftI",
      "title": "NestedFP: High-Performance, Memory-Efficient Dual-Precision Floating Point Support for LLMs",
      "authors": [
        "Haeun Lee",
        "Omin Kwon",
        "Yeonhong Park",
        "Jae W. Lee"
      ],
      "abstract": "Meeting service-level objectives (SLOs) in Large Language Models (LLMs) serving is critical, but managing the high variability in load presents a significant challenge. Recent advancements in FP8 inference, backed by native hardware support, offer a potential solution: executing FP16 models by default, while switching to FP8 models during sudden load surges to achieve higher throughput at the cost of a slight quality degradation. Although this approach facilitates effective SLO management, it introduces additional memory overhead due to storing two versions of the same model. In response, this paper proposes NestedFP, an LLM serving technique that supports both FP16 and FP8 models in a memory-efficient manner by overlaying FP8 parameters onto FP16 parameters, allowing both models to share the same FP16 memory footprint. By leveraging a compact data format for the overlay and a specialized GEMM kernel optimized for this format, NestedFP ensures minimal degradation in both model quality and inference throughput across both FP8 and FP16 modes. NestedFP provides a flexible platform for dynamic, SLO-aware precision selection. The code is available at https://github.com/SNU-ARC/NestedFP.",
      "arxiv_url": "https://openreview.net/forum?id=WDAKFpWftI",
      "pdf_url": "https://openreview.net/pdf/42e29b94c73b9d57d0e3a3133b1033725272b5bf.pdf",
      "primary_category": "FP8, Quantization, LLM",
      "categories": [
        "FP8",
        "Quantization",
        "LLM",
        "Dual-Precision",
        "Floating Point Quantization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "TrHeq0yFhv",
      "title": "SensorLM: Learning the Language of Wearable Sensors",
      "authors": [
        "Yuwei Zhang",
        "Kumar Ayush",
        "Siyuan Qiao",
        "A. Ali Heydari",
        "Girish Narayanswamy",
        "Maxwell A Xu",
        "Ahmed Metwally",
        "Jinhua Xu",
        "Jake Garrison",
        "Xuhai Xu",
        "Tim Althoff",
        "Yun Liu",
        "Pushmeet Kohli",
        "Jiening Zhan",
        "Mark Malhotra",
        "Shwetak Patel",
        "Cecilia Mascolo",
        "Xin Liu",
        "Daniel McDuff",
        "Yuzhe Yang"
      ],
      "abstract": "We present SensorLM, a family of sensor-language foundation models that enable wearable sensor data understanding with natural language. Despite its pervasive nature, aligning and interpreting sensor data with language remains challenging due to the lack of paired, richly annotated sensor-text descriptions in uncurated, real-world wearable data. We introduce a hierarchical caption generation pipeline designed to capture statistical, structural, and semantic information from sensor data. This approach enabled the curation of the largest sensor-language dataset to date, comprising over 59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and recovers them as specific variants within a generic architecture. Extensive experiments on real-world tasks in human activity analysis and healthcare verify the superior performance of SensorLM over state-of-the-art in zero-shot recognition, few-shot learning, and cross-modal retrieval. SensorLM also demonstrates intriguing capabilities including scaling behaviors, label efficiency, sensor captioning, and zero-shot generalization to unseen tasks. Code is available at https://github.com/Google-Health/consumer-health-research/tree/main/sensorlm.",
      "arxiv_url": "https://openreview.net/forum?id=TrHeq0yFhv",
      "pdf_url": "https://openreview.net/pdf/d4f22517db1ec860d9ff93deccc7e0f4ed9cc7f3.pdf",
      "primary_category": "Sensor-Language Models, Wearable Sensors, Foundation Models",
      "categories": [
        "Sensor-Language Models",
        "Wearable Sensors",
        "Foundation Models",
        "Multimodal Learning",
        "Healthcare"
      ],
      "tags": [
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "LjtgTpWH71",
      "title": "Hybrid Latent Reasoning via Reinforcement Learning",
      "authors": [
        "Zhenrui Yue",
        "Bowen Jin",
        "Huimin Zeng",
        "Honglei Zhuang",
        "Zhen Qin",
        "Jinsung Yoon",
        "Lanyu Shang",
        "Jiawei Han",
        "Dong Wang"
      ],
      "abstract": "Recent advances in large language models (LLMs) have introduced latent reasoning as a promising alternative to autoregressive reasoning. By performing internal computation with hidden states from previous steps, latent reasoning benefit from more informative features rather than sampling a discrete chain-of-thought (CoT) path. Yet latent reasoning approaches are often incompatible with LLMs, as their continuous paradigm conflicts with the discrete nature of autoregressive generation. Moreover, these methods rely on CoT traces for training and thus fail to exploit the inherent reasoning patterns of LLMs. In this work, we explore latent reasoning by leveraging the intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we introduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid latent reasoning approach that (1) integrates prior hidden states into sampled tokens with a learnable gating mechanism, and (2) initializes training with predominantly token embeddings while progressively incorporating more hidden features. This design maintains LLMs' generative capabilities and incentivizes hybrid reasoning using both discrete and continuous representations. In addition, the hybrid HRPO introduces stochasticity into latent reasoning via token sampling, thereby enabling RL-based optimization without requiring CoT trajectories. Extensive evaluations across diverse benchmarks show that HRPO outperforms prior methods in both knowledge- and reasoning-intensive tasks. Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing behaviors like cross-lingual patterns and shorter completion lengths, highlighting the potential of our RL-based approach and offer insights for future work in latent reasoning.",
      "arxiv_url": "https://openreview.net/forum?id=LjtgTpWH71",
      "pdf_url": "https://openreview.net/pdf/53c3e7d0d327badd274bf88e7a7abda4272aa155.pdf",
      "primary_category": "Latent reasoning, hybrid reasoning, reinforcement learning",
      "categories": [
        "Latent reasoning",
        "hybrid reasoning",
        "reinforcement learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8tetkBNSWZ",
      "title": "INST-IT: Boosting Instance Understanding via Explicit Visual Prompt Instruction Tuning",
      "authors": [
        "Wujian Peng",
        "Lingchen Meng",
        "Yitong Chen",
        "Yiweng Xie",
        "Yang Liu",
        "Tao Gui",
        "Hang Xu",
        "Xipeng Qiu",
        "Zuxuan Wu",
        "Yu-Gang Jiang"
      ],
      "abstract": "Large Multimodal Models (LMMs) have made significant breakthroughs with the advancement of instruction tuning. However, while existing models can understand images and videos at a holistic level, they still struggle with instance-level understanding that requires a more fine-grained comprehension and alignment. Instance-level understanding is crucial for LMMs, as it focuses on the specific elements that we are most interested in. Excitingly, existing works find that the state-of-the-art LMMs exhibit strong instance understanding capabilities when provided with explicit visual cues. Motivated by this, we proposed Inst-IT, a solution to enhance LMMs in Instance understanding via explicit visual prompt Instruction Tuning for instance guidance. Inst-IT consists of a benchmark to diagnose multimodal instance-level understanding, a large-scale instruction-tuning dataset, and a continuous instruction-tuning training paradigm to effectively enhance spatial-temporal instance understanding capabilities of existing LMMs. Experimental results show that, enhanced by Inst-IT, our models not only achieve outstanding performance on Inst-IT-Bench and other instance understanding benchmarks, but also demonstrate significant improvements across various generic image and video understanding benchmarks. This highlights that our method not only boosts instance-level understanding but also strengthens the overall capabilities of generic image and video comprehension.",
      "arxiv_url": "https://openreview.net/forum?id=8tetkBNSWZ",
      "pdf_url": "https://openreview.net/pdf/981b9df93d11c493f23f85073116bf2f42508fe4.pdf",
      "primary_category": "multimodal models, instruction tuning, fine-grained understanding",
      "categories": [
        "multimodal models",
        "instruction tuning",
        "fine-grained understanding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cHMP2IAhML",
      "title": "HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning",
      "authors": [
        "Chuhao Zhou",
        "Jianfei Yang"
      ],
      "abstract": "Embodied agents operating in smart homes must understand human behavior through diverse sensory inputs and communicate via natural language. While Vision-Language Models (VLMs) have enabled impressive language-grounded perception, their reliance on visual data limits robustness in real-world scenarios with occlusions, poor lighting, or privacy constraints. In this paper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that integrates uncommon but powerful sensing modalities, such as LiDAR, infrared, mmWave radar, and WiFi, to enable seamless human perception and reasoning across heterogeneous environments. We address two key challenges: (1) the scarcity of aligned modality-text data for rare sensors, and (2) the heterogeneity of their physical signal representations. To overcome these, we design a Universal Modality-Injection Projector (UMIP) that enhances pre-aligned modality embeddings with fine-grained, text-aligned features from tailored encoders via coarse-to-fine cross-attention without introducing significant alignment overhead. We further introduce a human-VLM collaborative data curation pipeline to generate paired textual annotations for sensing datasets. Extensive experiments on two newly constructed benchmarks show that HoloLLM significantly outperforms existing MLLMs, improving language-grounded human sensing accuracy by up to 30\\%. This work establishes a new foundation for real-world, language-informed multisensory embodied intelligence.",
      "arxiv_url": "https://openreview.net/forum?id=cHMP2IAhML",
      "pdf_url": "https://openreview.net/pdf/dcc7b3c13992fef443e0c27fa66271ab9b475c3d.pdf",
      "primary_category": "Multimodal Large Language Model, Human Sensing and Reasoning, IoT Sensors",
      "categories": [
        "Multimodal Large Language Model",
        "Human Sensing and Reasoning",
        "IoT Sensors"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "X2xLfqX24x",
      "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models",
      "authors": [
        "Guo Chen",
        "Zhiqi Li",
        "Shihao Wang",
        "Jindong Jiang",
        "Yicheng Liu",
        "Lidong Lu",
        "De-An Huang",
        "Wonmin Byeon",
        "Matthieu Le",
        "Max Ehrlich",
        "Tong Lu",
        "Limin Wang",
        "Bryan Catanzaro",
        "Jan Kautz",
        "Andrew Tao",
        "Zhiding Yu",
        "Guilin Liu"
      ],
      "abstract": "We introduce Eagle2.5, a frontier vision-language model (VLM) for long-context multimodal learning. Our work addresses the challenges in long video comprehension and high-resolution image understanding, introducing a generalist framework for both tasks. The proposed training framework incorporates Automatic Degrade Sampling and Image Area Preservation, two techniques that preserve contextual integrity and visual details. The framework also includes numerous efficiency optimizations in the pipeline for long-context data training. Finally, we propose Eagle-Video-110K, a novel dataset that integrates both story-level and clip-level annotations, facilitating long-video understanding. Eagle2.5 demonstrates substantial improvements on long-context multimodal benchmarks, providing a robust solution to the limitations of existing VLMs. Notably, our best model Eagle2.5-8B achieves 72.4\\% on Video-MME with 512 input frames, matching the results of top-tier commercial model such as GPT-4o and large-scale open-source models like Qwen2.5-VL-72B and InternVL2.5-78B.",
      "arxiv_url": "https://openreview.net/forum?id=X2xLfqX24x",
      "pdf_url": "https://openreview.net/pdf/ba736878a2d23b25517e784966e0a5d5a15a27fe.pdf",
      "primary_category": "Vision-Language Model, Multimodal LLM, Multimodal Learning",
      "categories": [
        "Vision-Language Model",
        "Multimodal LLM",
        "Multimodal Learning",
        "Data Strategy"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XUmGMBRv4M",
      "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
      "authors": [
        "Akhiad Bercovich",
        "Mohammed Dabbah",
        "Omri Puny",
        "Ido Galil",
        "Amnon Geifman",
        "Yonatan Geifman",
        "Izhak Golan",
        "Ehud Dov Karpas",
        "Itay Levy",
        "Zach Moshe",
        "Najeeb Nabwani",
        "Tomer Ronen",
        "Itamar Schen",
        "Ido Shahaf",
        "Oren Tropp",
        "Ran Zilberstein",
        "Ran El-Yaniv"
      ],
      "abstract": "We introduce \\textit{FFN Fusion}, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. Our key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining after the removal of specific attention layers, can often be parallelized with minimal accuracy impact. We develop a principled methodology for identifying and fusing such sequences, transforming them into parallel operations that significantly reduce inference latency while preserving model behavior. Applying these techniques to Llama-3.1-405B-Instruct, we create a 253B model (253B-Base), an efficient and soon-to-be publicly available model that achieves a 1.71$\\times$ speedup in inference latency and 35$\\times$ lower per-token cost while maintaining strong performance across benchmarks. Most intriguingly, we find that even full transformer blocks containing both attention and FFN layers can sometimes be parallelized, suggesting new directions for neural architecture design.",
      "arxiv_url": "https://openreview.net/forum?id=XUmGMBRv4M",
      "pdf_url": "https://openreview.net/pdf/241a1c5017cb5e9aa367d720bfee6b65a69053e8.pdf",
      "primary_category": "LLM, Architecture optimization, Inference efficiency",
      "categories": [
        "LLM",
        "Architecture optimization",
        "Inference efficiency"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "r2ykUnzuGt",
      "title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents",
      "authors": [
        "Zhenyu Zhang",
        "Tianyi Chen",
        "Weiran Xu",
        "Alex Pentland",
        "Jiaxin Pei"
      ],
      "abstract": "Long-horizon tasks requiring multi-step reasoning and dynamic re-planning remain challenging for large language models (LLMs). Sequential prompting methods are prone to context drift, loss of goal information, and recurrent failure cycles, while hierarchical prompting methods often weaken cross-level continuity or incur substantial runtime overhead. We introduce ReCAP (Recursive Context-Aware Reasoning and Planning), a hierarchical framework with shared context for reasoning and planning in LLMs. ReCAP combines three key mechanisms: (i) plan-ahead decomposition, in which the model generates a full subtask list, executes the first item, and refines the remainder; (ii) structured re-injection of parent plans, maintaining consistent multi-level context during recursive return; and (iii) memory-efficient execution, bounding the active prompt so costs scale linearly with task depth. Together these mechanisms align high-level goals with low-level actions, reduce redundant prompting, and preserve coherent context updates across recursion. Experiments demonstrate that ReCAP substantially improves subgoal alignment and success rates on various long-horizon reasoning benchmarks, achieving a 32\\% gain on synchronous Robotouille and a 29\\% improvement on asynchronous Robotouille under the strict pass@1 protocol.",
      "arxiv_url": "https://openreview.net/forum?id=r2ykUnzuGt",
      "pdf_url": "https://openreview.net/pdf/311841e491eca81acc49d5288503e9b7819788d6.pdf",
      "primary_category": "Large Language Model, Long-Horizon Reasoning, LLM Agent",
      "categories": [
        "Large Language Model",
        "Long-Horizon Reasoning",
        "LLM Agent",
        "LLM Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VkicTqszOn",
      "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
      "authors": [
        "Minki Kang",
        "Jongwon Jeong",
        "Seanie Lee",
        "Jaewoong Cho",
        "Sung Ju Hwang"
      ],
      "abstract": "Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment.\nTo address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs.\nHowever, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability.\nIn this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. \nWe improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; \nand (2) we propose a self-consistent action generation for improving test-time robustness of small agents.\nWe evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization.\nOur results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents.",
      "arxiv_url": "https://openreview.net/forum?id=VkicTqszOn",
      "pdf_url": "https://openreview.net/pdf/c3ead64f6c3fd03156d79bf7aa5185204700b2a2.pdf",
      "primary_category": "langauge model, language agent, reasoning",
      "categories": [
        "langauge model",
        "language agent",
        "reasoning",
        "distillation"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xpjWEgf8zi",
      "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents",
      "authors": [
        "Kangrui Wang",
        "Pingyue Zhang",
        "Zihan Wang",
        "Yaning Gao",
        "Linjie Li",
        "Qineng Wang",
        "Hanyang Chen",
        "Yiping Lu",
        "Zhengyuan Yang",
        "Lijuan Wang",
        "Ranjay Krishna",
        "Jiajun Wu",
        "Li Fei-Fei",
        "Yejin Choi",
        "Manling Li"
      ],
      "abstract": "A major challenge in training VLM agents, compared to LLM agents, is that states shift from simple texts to complex visual observations, which introduces partial observability and demands robust world modeling. We ask: can VLM agents build internal world models through explicit visual state reasoning? In this work, we architecturally enforce and reward VLM agent’s reasoning process via reinforcement learning (RL), formulating the problem as a Partially Observable Markov Decision Process (POMDP). We demonstrate that structuring agent’s reasoning into StateEstimation (“what is the current state?”) and TransitionModeling (“what is next?”) is critical by studying five reasoning strategies. Investigating how agents should ground visual states and represent these internal beliefs, we reveal the optimal representations are task-dependent: Natural Language excels at capturing semantic relationships for general tasks, while Structured formats are essential for high-precision manipulation. These insights motivate our approach to reward shaping and credit assignment. We leverage a WorldModeling Reward to densely rewards the agent’s turn-by-turn state predictions, while our Bi-Level General Advantage Estimation (Bi-Level GAE) enables turn-aware credit assignment. Through such world model reasoning, we enable a 3B model to achieve performance of 0.82 on a set of five diverse agent tasks, nearly 3× improvement over its untrained counterpart (0.21) and surpassing proprietary reasoning models like GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5 (0.62). All experiments are supported by our VAGEN framework, a scalable system for training and analyzing multi-turn VLM agents across diverse visual environments",
      "arxiv_url": "https://openreview.net/forum?id=xpjWEgf8zi",
      "pdf_url": "https://openreview.net/pdf/ed11882b04656406635941c99bb0b5dbf475a088.pdf",
      "primary_category": "Visual States, World Modeling, Multi-turn RL",
      "categories": [
        "Visual States",
        "World Modeling",
        "Multi-turn RL",
        "VLM Agents"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "M226WElHp5",
      "title": "Scalable Exploration via Ensemble++",
      "authors": [
        "Yingru Li",
        "Jiawei Xu",
        "Baoxiang Wang",
        "Zhi-Quan Luo"
      ],
      "abstract": "Thompson Sampling is a principled method for balancing exploration and exploitation, but its real-world adoption faces computational challenges in large-scale or non-conjugate settings. While ensemble-based approaches offer partial remedies, they typically require prohibitively large ensemble sizes. We propose Ensemble++, a scalable exploration framework using a novel shared-factor ensemble architecture with random linear combinations. For linear bandits, we provide theoretical guarantees showing that Ensemble++ achieves regret comparable to exact Thompson Sampling with only $\\Theta(d \\log T)$ ensemble sizes--significantly outperforming prior methods. Crucially, this efficiency holds across both compact and finite action sets with either time-invariant or time-varying contexts without configuration changes. We extend this theoretical foundation to nonlinear rewards by replacing fixed features with learnable neural representations while preserving the same incremental update principle, effectively bridging theory and practice for real-world tasks. Comprehensive experiments across linear, quadratic, neural, and GPT-based contextual bandits validate our theoretical findings and demonstrate Ensemble++'s superior regret-computation tradeoff versus state-of-the-art methods.",
      "arxiv_url": "https://openreview.net/forum?id=M226WElHp5",
      "pdf_url": "https://openreview.net/pdf/32502c1ebf0acf4f9ac1362ffe33e1395d5d2bac.pdf",
      "primary_category": "Reinforcement Learning, Ensemble Sampling, Thompson Sampling",
      "categories": [
        "Reinforcement Learning",
        "Ensemble Sampling",
        "Thompson Sampling",
        "Exploration",
        "Posterior Approximation",
        "Scalable Computation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "76cFMRgEzQ",
      "title": "Understanding Parametric and Contextual Knowledge Reconciliation within Large Language Models",
      "authors": [
        "Jun Zhao",
        "Yongzhuo Yang",
        "Xiang Hu",
        "Jingqi Tong",
        "Yi Lu",
        "Wei Wu",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) provides additional contextual knowledge to complement the parametric knowledge in Large Language Models (LLMs). These two knowledge interweave to enhance the accuracy and timeliness of LLM responses. However, \n  the internal mechanisms by which LLMs utilize these knowledge remain unclear. We propose modeling the forward propagation of knowledge as an entity flow, employing this framework to trace LLMs' internal behaviors when processing mixed-source knowledge. Linear probing utilizes a trainable linear classifier to detect specific attributes in hidden layers. However, once trained, a probe cannot adapt to dynamically specified entities. To address this challenge, we construct an entity-aware probe, which introduces special tokens to mark probing targets and employs a small trainable rank-8 lora update to process these special markers. We first verify this approach through an attribution experiment, demonstrating that it can accurately detect information about ad-hoc entities from complex hidden states.  Next, we trace entity flows across layers to understand how LLMs reconcile conflicting knowledge internally. Our probing results reveal that contextual and parametric knowledge are routed between tokens through distinct sets of attention heads, supporting attention competition only within knowledge types. While conflicting knowledge maintains a residual presence across layers, aligned knowledge from multiple sources gradually accumulates, with the magnitude of this accumulation directly determining its influence on final outputs.",
      "arxiv_url": "https://openreview.net/forum?id=76cFMRgEzQ",
      "pdf_url": "https://openreview.net/pdf/0d25caa1429d9977838c452bf4ee4e2b6d608c3d.pdf",
      "primary_category": "Natural Language Processing, Information Retrieval, Network Analysis",
      "categories": [
        "Natural Language Processing",
        "Information Retrieval",
        "Network Analysis"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "peYBx7AiKw",
      "title": "Learning to Route: Per-Sample Adaptive Routing for Multimodal Multitask Prediction",
      "authors": [
        "Marzieh Ajirak",
        "Oded Bein",
        "Ellen Rose Bowen",
        "Dora Kanellopoulos",
        "Avital Falk",
        "FAITH M. GUNNING",
        "Nili Solomonov",
        "Logan Grosenick"
      ],
      "abstract": "We propose a unified framework for adaptive routing in multitask, multimodal prediction settings where data heterogeneity and task interactions vary across samples. We introduce a routing-based architecture that dynamically selects modality processing pathways and task-sharing strategies on a per-sample basis. Our model defines multiple modality paths, including raw and fused representations of text and numeric features, and learns to route each input through the most informative modality-task expert combination. Task-specific predictions are produced by shared or independent heads depending on the routing decision, and the entire system is trained end-to-end. We evaluate the model on both synthetic data and real-world psychotherapy notes, predicting depression and anxiety outcomes. Our experiments show that our method consistently outperforms fixed multitask or single-task baselines, and that the learned routing policy provides interpretable insights into modality relevance and task structure. This addresses critical challenges in personalized healthcare by providing per-subject adaptive information processing that accounts for data and task correlation heterogeneity.",
      "arxiv_url": "https://openreview.net/forum?id=peYBx7AiKw",
      "pdf_url": "https://openreview.net/pdf/979223660119d8b1a87919b3d153e4acf348af35.pdf",
      "primary_category": "multimodal learning, multitask learning, adaptive routing",
      "categories": [
        "multimodal learning",
        "multitask learning",
        "adaptive routing",
        "per-sample adaptation",
        "heterogeneous data",
        "clinical applications",
        "routing networks"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9yusqX9DpR",
      "title": "Self-Challenging Language Model Agents",
      "authors": [
        "Yifei Zhou",
        "Sergey Levine",
        "Jason E Weston",
        "Xian Li",
        "Sainbayar Sukhbaatar"
      ],
      "abstract": "Large language models  are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose  the Self-Challenging Agent framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks.  \nThe agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward.  We show our method improves the performance of Llama-3.1-8B-Instruct on two existing multi-turn tool-use agent benchmarks, M$^3$ToolEval and TauBench, with a two-fold average success rate increase, despite using only self-generated training data.",
      "arxiv_url": "https://openreview.net/forum?id=9yusqX9DpR",
      "pdf_url": "https://openreview.net/pdf/d7856a8a4e991dab71814a3f39f28fe17b6b93bd.pdf",
      "primary_category": "LLM Agent, self-improvement, multi-turn tool-use",
      "categories": [
        "LLM Agent",
        "self-improvement",
        "multi-turn tool-use"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "l42UGsdrNn",
      "title": "Zebra-Llama: Towards Extremely Efficient Hybrid Models",
      "authors": [
        "Mingyu Yang",
        "Mehdi Rezagholizadeh",
        "Guihong Li",
        "Vikram Appia",
        "Emad Barsoum"
      ],
      "abstract": "With the growing demand for deploying large language models (LLMs) across diverse applications, improving their inference efficiency is crucial for sustainable and democratized access. However, retraining LLMs to meet new user-specific requirements is prohibitively expensive and environmentally unsustainable. In this work, we propose a practical and scalable alternative: composing efficient hybrid language models from existing pre-trained models.\nOur approach, X-EcoMLA, introduces a family of 1B, 3B, and 8B hybrid models by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA) layers, using a refined initialization and post-training pipeline to efficiently transfer knowledge from pre-trained Transformers.\nX-EcoMLA achieves Transformer-level accuracy with near-SSM efficiency using only 7–11 billion training tokens (compared to the trillions required for pre-training) and an 8B teacher. Moreover, it dramatically reduces KV cache size—down to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants, respectively—while preserving 100%, 100%, and over 97% of average zero-shot performance on LM Harness tasks.\nCompared to models like MambaInLLaMA, X-EcoMLA, Minitron, and Llamba, our approach consistently delivers competitive or superior accuracy while using significantly fewer tokens, smaller teachers, and vastly reduced KV cache memory. Notably, X-EcoMLA-8B surpasses Minitron-8B in few-shot accuracy by 7%, while using 8× fewer training tokens, over 12× smaller KV cache, and a smaller teacher (8B vs. 15B). \nIt also achieves 1.4x–3.3x higher throughput (tokens/s) than MambaInLlama. The source code is\nreleased at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
      "arxiv_url": "https://openreview.net/forum?id=l42UGsdrNn",
      "pdf_url": "https://openreview.net/pdf/ba9ef0b7769e0db15d03dbd4788db3287b2d18a4.pdf",
      "primary_category": "Model Efficiency, Large Language Models",
      "categories": [
        "Model Efficiency",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "z5KTxW5sJd",
      "title": "From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review",
      "authors": [
        "Yaohui Zhang",
        "Haijing ZHANG",
        "Wenlong Ji",
        "Tianyu Hua",
        "Nick Haber",
        "Hancheng Cao",
        "Weixin Liang"
      ],
      "abstract": "The advent of large language models (LLMs) offers unprecedented opportunities to reimagine peer review beyond the constraints of traditional workflows.\nDespite these opportunities, prior efforts have largely focused on replicating traditional review workflows with LLMs serving as direct substitutes for human reviewers, while limited attention has been given to exploring new paradigms that fundamentally rethink how LLMs can participate in the academic review process.\nIn this paper, we introduce and explore a novel mechanism that employs LLM agents to perform pairwise comparisons among manuscripts instead of individual scoring. By aggregating outcomes from substantial pairwise evaluations, this approach enables a more accurate and robust measure of relative manuscript quality.\nOur experiments demonstrate that this comparative approach significantly outperforms traditional rating-based methods in identifying high-impact papers. However, our analysis also reveals emergent biases in the selection process, notably a reduced novelty in research topics and an increased institutional imbalance. These findings highlight both the transformative potential of rethinking peer review with LLMs and critical challenges that future systems must address to ensure equity and diversity.",
      "arxiv_url": "https://openreview.net/forum?id=z5KTxW5sJd",
      "pdf_url": "https://openreview.net/pdf/fc2b7a565eb4fe514a06f7630029728205710c5f.pdf",
      "primary_category": "Large Language Models, Peer Review Redesign, Comparative Paper Evaluation",
      "categories": [
        "Large Language Models",
        "Peer Review Redesign",
        "Comparative Paper Evaluation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tcisuhGsQZ",
      "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference",
      "authors": [
        "Yuan Feng",
        "Junlin Lv",
        "Yukun Cao",
        "Xike Xie",
        "S Kevin Zhou"
      ],
      "abstract": "Large Language Models have excelled in various domains but face efficiency challenges due to the growing Key-Value (KV) cache required for long-sequence inference. Recent efforts aim to reduce KV cache size by evicting vast non-critical cache elements during runtime while preserving generation quality. However, these methods typically allocate compression budgets uniformly across all attention heads, ignoring the unique attention patterns of each head. In this paper, we establish a theoretical loss upper bound between pre- and post-eviction attention output, explaining the optimization target of prior cache eviction methods, while guiding the optimization of adaptive budget allocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive budget allocation strategy. It offers plug-and-play benefits, enabling seamless integration with prior cache eviction methods. Extensive evaluations on 13 datasets from Ruler and 16 datasets from LongBench, all conducted under both question-aware and question-agnostic scenarios, demonstrate substantial quality improvements over existing methods. Our code is available at https://github.com/FFY0/AdaKV.",
      "arxiv_url": "https://openreview.net/forum?id=tcisuhGsQZ",
      "pdf_url": "https://openreview.net/pdf/f766c4a703d424c73c10a5600d7340165f8e8f05.pdf",
      "primary_category": "Efficient AI, Large Language Model, LLM Inference",
      "categories": [
        "Efficient AI",
        "Large Language Model",
        "LLM Inference",
        "KV Cache"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "uhFx1RGD1g",
      "title": "Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior",
      "authors": [
        "Yulin Li",
        "Haokun GUI",
        "Ziyang Fan",
        "Junjie Wang",
        "Bin Kang",
        "BIN CHEN",
        "Zhuotao Tian"
      ],
      "abstract": "Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose **Dy**namic **To**ken compression via LLM-guided **K**eyframe prior (**DyToK**), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 2.5x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code and models will be made publicly available.",
      "arxiv_url": "https://openreview.net/forum?id=uhFx1RGD1g",
      "pdf_url": "https://openreview.net/pdf/b4a33413bf051e6a2476bdf1b3f34a1cab246ff1.pdf",
      "primary_category": "Efficient Large Multimodal Models, Video Large Language Models, Token Pruning",
      "categories": [
        "Efficient Large Multimodal Models",
        "Video Large Language Models",
        "Token Pruning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "LVPq1j357N",
      "title": "Learning Personalized Ad Impact via Contextual Reinforcement Learning under Delayed Rewards",
      "authors": [
        "Yuwei Cheng",
        "Zifeng Zhao",
        "Haifeng Xu"
      ],
      "abstract": "Online advertising platforms use automated auctions to connect advertisers with potential customers, requiring effective bidding strategies to maximize profits. Accurate ad impact estimation requires considering three key factors: delayed and long-term effects, cumulative ad impacts such as reinforcement or fatigue, and customer heterogeneity. However, these effects are often not jointly addressed in previous studies. To capture these factors, we model ad bidding as a Contextual Markov Decision Process (CMDP) with delayed Poisson rewards. For efficient estimation, we propose a two-stage maximum likelihood estimator combined with data-splitting strategies, ensuring controlled estimation error based on the first-stage estimator's (in)accuracy. Building on this, we design a reinforcement learning algorithm to derive efficient personalized bidding strategies. This approach achieves a near-optimal regret bound of $\\tilde{\\mathcal{O}}(dH^2\\sqrt{T})$, where $d$ is the contextual dimension, $H$ is the number of rounds, and $T$ is the number of customers. Our theoretical findings are validated by simulation experiments.",
      "arxiv_url": "https://openreview.net/forum?id=LVPq1j357N",
      "pdf_url": "https://openreview.net/pdf/c0ec48caedbb349a731ac977ef5095f8085dc582.pdf",
      "primary_category": "online learning; Contextual Markov Decision Process; maximum likelihood estimation; delayed reward; reinforcement learning; auto-bidding",
      "categories": [
        "online learning; Contextual Markov Decision Process; maximum likelihood estimation; delayed reward; reinforcement learning; auto-bidding"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QjnKsujXVG",
      "title": "Chain of Execution Supervision Promotes General Reasoning in Large Language Models",
      "authors": [
        "Nuo Chen",
        "Zehua Li",
        "Keqin Bao",
        "Junyang Lin",
        "Dayiheng Liu"
      ],
      "abstract": "Building robust and general reasoning ability is a central goal in the development of large language models (LLMs). Recent efforts increasingly turn to code as a rich training source, given its inherent logical structure and diverse reasoning paradigms—such as divide-and-conquer, topological ordering, and enumeration. However, reasoning in code is often expressed implicitly and entangled with syntactic or implementation noise, making direct training on raw code suboptimal. To address this, we introduce TraceMind, a large-scale corpus of 2.6 million samples that transforms code execution into explicit, step-by-step chain-of-thought style rationales, which we call Chain of Execution (CoE). \nThe corpus spans domains including mathematics, classical algorithms and algorithmic competition, and is enriched with variable-tracing questions and code rewritings to enhance logical granularity and code diversity.\n We evaluate Tracepile using three training setups—continue-pretraining, instruction tuning after pretraining, and two-stage finetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5, and Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and algorithms demonstrate consistent improvements. Notably, Tracepile boosts LLaMA3-8B by 9.2\\% on average across nine math datasets and delivers clear gains on LiveCodeBench, CRUX, and Zebra Logic under two-stage finetuning.",
      "arxiv_url": "https://openreview.net/forum?id=QjnKsujXVG",
      "pdf_url": "https://openreview.net/pdf/81876043b2c12b524b3d7cebe4d6ef2b4a940143.pdf",
      "primary_category": "large language models, reasoning",
      "categories": [
        "large language models",
        "reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "b50IW9yV2M",
      "title": "Wide-Horizon Thinking and Simulation-Based Evaluation for Real-World LLM Planning with Multifaceted Constraints",
      "authors": [
        "Dongjie Yang",
        "Chengqiang Lu",
        "Qimeng Wang",
        "Xinbei Ma",
        "Yan Gao",
        "Yao Hu",
        "hai zhao"
      ],
      "abstract": "Unlike reasoning, which often entails a deep sequence of deductive steps, complex real-world planning is characterized by the need to synthesize a broad spectrum of parallel and potentially conflicting information and constraints. For example, in travel planning scenarios, it requires the integration of diverse real-world information and user preferences. While LLMs show promise, existing methods with long-horizon thinking struggle with handling multifaceted constraints, leading to suboptimal solutions. Motivated by the challenges of real-world travel planning, this paper introduces the Multiple Aspects of Planning (MAoP), empowering LLMs with \"wide-horizon thinking\" to solve planning problems with multifaceted constraints. Instead of direct planning, MAoP leverages the strategist to conduct pre-planning from various aspects and provide the planning blueprint for planners, enabling strong inference-time scalability by scaling aspects to consider various constraints. In addition, existing benchmarks for multi-constraint planning are flawed because they assess constraints in isolation, ignoring causal dependencies within the constraints, e.g, travel planning, where past activities dictate future itinerary. To address this, we propose Travel-Sim, an agent-based benchmark assessing plans via real-world simulation, thereby inherently resolving these causal dependencies. This paper advances LLM capabilities in complex planning and offers novel insights for evaluating sophisticated scenarios through simulation.",
      "arxiv_url": "https://openreview.net/forum?id=b50IW9yV2M",
      "pdf_url": "https://openreview.net/pdf/4fe14c361104913e172e118fa05d358d3b030840.pdf",
      "primary_category": "Real-World Planning, Travel Planning, Simulation-based Evaluation",
      "categories": [
        "Real-World Planning",
        "Travel Planning",
        "Simulation-based Evaluation",
        "Large Language Model",
        "Agent",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IDSTtDw4Cs",
      "title": "SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning",
      "authors": [
        "Wanjia Zhao",
        "Mert Yuksekgonul",
        "Shirley Wu",
        "James Zou"
      ],
      "abstract": "Multi-agent AI systems powered by large language models (LLMs) are increasingly applied to solve complex tasks. However, these systems often rely on fragile, manually designed prompts and heuristics, making optimization difficult. A key challenge in optimizing multi-agent systems is acquiring suitable training data for specialized agents. We introduce SiriuS, a self-improving, reasoning-driven optimization framework for multi-agent systems. Central to our approach is the construction of an experience library: a repository of high-quality reasoning trajectories. The library is built by retaining reasoning steps that lead to successful outcomes, providing a robust training set for optimizing multi-agent system. Additionally, we introduce a library augmentation procedure that refines unsuccessful trajectories, further enriching the library. SiriuS boosts performance by 2.86% to 21.88% on reasoning and biomedical QA and enhances agent negotiation in competitive settings. Our results show that SiriuS enhances multi-agent performance while generating reusable data for self-correction and self-play enhancement in the future.",
      "arxiv_url": "https://openreview.net/forum?id=IDSTtDw4Cs",
      "pdf_url": "https://openreview.net/pdf/856c35f60eccff17ac8726de9ca5f7fbf9bcf3ee.pdf",
      "primary_category": "Multi-Agent, Self-Improving, LLM Reasoning",
      "categories": [
        "Multi-Agent",
        "Self-Improving",
        "LLM Reasoning"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7k7cubl1iL",
      "title": "CIDD: Collaborative Intelligence for Structure-Based Drug Design Empowered by LLMs",
      "authors": [
        "Bowen Gao",
        "Yanwen Huang",
        "Yiqiao Liu",
        "Wenxuan Xie",
        "Bowei He",
        "Haichuan Tan",
        "Wei-Ying Ma",
        "Ya-Qin Zhang",
        "Yanyan Lan"
      ],
      "abstract": "Structure-guided molecular generation is pivotal in early-stage drug discovery, enabling the design of compounds tailored to specific protein targets. However, despite recent advances in 3D generative modeling, particularly in improving docking scores, these methods often produce rare and intrinsically irrational molecular structures that deviate from drug-like chemical space. To quantify this issue, we propose a novel metric, the Molecule Reasonable Ratio (MRR), which measures structural rationality and reveals a critical gap between existing models and real-world approved drugs. To address this, we introduce the Collaborative Intelligence Drug Design (CIDD) framework, the first approach to unify the 3D interaction modeling capabilities of generative models with the general knowledge and reasoning power of large language models (LLMs). By leveraging LLM-based Chain-of-Thought reasoning, CIDD generates molecules that not only bind effectively to protein pockets but also exhibit strong structural drug-likeness, rationality, and synthetic accessibility. On the CrossDocked2020 benchmark, CIDD consistently improves drug-likeness metrics, including QED, SA, and MRR, across different base generative models, while maintaining competitive binding affinity. Notably, it raises the combined success rate (balancing drug-likeness and binding) from 15.72% to 34.59%, more than doubling previous results. These findings demonstrate the value of integrating knowledge reasoning with geometric generation to advance AI-driven drug design.",
      "arxiv_url": "https://openreview.net/forum?id=7k7cubl1iL",
      "pdf_url": "https://openreview.net/pdf/77cb4fc7534bc0e0660ba9931aad895880b93492.pdf",
      "primary_category": "Large Language Model, Drug Design",
      "categories": [
        "Large Language Model",
        "Drug Design"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Mk9ykil8eP",
      "title": "HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning",
      "authors": [
        "Zhi Jing",
        "Siyuan Yang",
        "Jicong Ao",
        "Ting Xiao",
        "Yu-Gang Jiang",
        "Chenjia Bai"
      ],
      "abstract": "For robotic manipulation, existing robotics datasets and simulation benchmarks predominantly cater to robot-arm platforms. However, for humanoid robots equipped with dual arms and dexterous hands, simulation tasks and high-quality demonstrations are notably lacking. Bimanual dexterous manipulation is inherently more complex, as it requires coordinated arm movements and hand operations, making autonomous data collection challenging. This paper presents HumanoidGen, an automated task creation and demonstration collection framework that leverages atomic dexterous operations and LLM reasoning to generate relational constraints. Specifically, we provide spatial annotations for both assets and dexterous hands based on the atomic operations, and perform an LLM planner to generate a chain of actionable spatial constraints for arm movements based on object affordances and scenes. To further improve planning ability, we employ a variant of Monte Carlo tree search to enhance LLM reasoning for long-horizon tasks and insufficient annotation. In experiments, we create a novel benchmark with augmented scenarios to evaluate the quality of the collected data. The results show that the performance of the 2D and 3D diffusion policies can scale with the generated dataset. Project page is https://openhumanoidgen.github.io.",
      "arxiv_url": "https://openreview.net/forum?id=Mk9ykil8eP",
      "pdf_url": "https://openreview.net/pdf/753bcc41dc0fe6dba41a235116449a37aba704f7.pdf",
      "primary_category": "Bimanual Manipulation, Dexterous Manipulation, Humanoid Robots",
      "categories": [
        "Bimanual Manipulation",
        "Dexterous Manipulation",
        "Humanoid Robots",
        "LLM Planning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kZstGANG8D",
      "title": "Improving LLM General Preference Alignment via Optimistic Online Mirror Descent",
      "authors": [
        "Yuheng Zhang",
        "Dian Yu",
        "Tao Ge",
        "Linfeng Song",
        "Zhichen Zeng",
        "Haitao Mi",
        "Nan Jiang",
        "Dong Yu"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated remarkable effectiveness in aligning large language models (LLMs) with human preferences. Many existing alignment approaches rely on the Bradley-Terry (BT) model assumption, which assumes the existence of a ground-truth reward for each prompt-response pair. However, this assumption can be overly restrictive when modeling complex human preferences. In this paper, we drop the BT model assumption and study LLM alignment under general preferences, formulated as a two-player game. Drawing on theoretical insights from learning in games, we integrate optimistic online mirror descent into our alignment framework to approximate the Nash policy. Theoretically, we demonstrate that our approach achieves an $\\mathcal{O}(T^{-1})$ bound on the duality gap, improving upon the previous $\\mathcal{O}(T^{-1/2})$ result. Meanwhile, it enjoys a linear convergence rate in the last iterate, a property not achieved by previous methods. More importantly, we implement our method and show through experiments that it outperforms state-of-the-art RLHF algorithms across multiple representative benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=kZstGANG8D",
      "pdf_url": "https://openreview.net/pdf/702b233b0880a30ab2c5d85055d6116dc69ab7f7.pdf",
      "primary_category": "LLM Alignment, RLHF",
      "categories": [
        "LLM Alignment",
        "RLHF"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Jj4NdJtXwp",
      "title": "Geometry of Decision Making in Language Models",
      "authors": [
        "Abhinav Joshi",
        "Divyanshu Bhatt",
        "Ashutosh Modi"
      ],
      "abstract": "Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of intrinsic dimension (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.",
      "arxiv_url": "https://openreview.net/forum?id=Jj4NdJtXwp",
      "pdf_url": "https://openreview.net/pdf/4f7f449178a12ce539315d6e395db6a68538b54d.pdf",
      "primary_category": "Large Language Models, Intrinsic Dimension, Interpretability",
      "categories": [
        "Large Language Models",
        "Intrinsic Dimension",
        "Interpretability",
        "Decision-Making",
        "Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Ox3U97Svtl",
      "title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation",
      "authors": [
        "Zhenwen Liang",
        "Linfeng Song",
        "Yang Li",
        "TAO YANG",
        "Haitao Mi",
        "Dong Yu"
      ],
      "abstract": "Automated Theorem Proving (ATP) in formal languages remains a formidable challenge in AI, demanding rigorous logical deduction and navigating vast search spaces. While large language models (LLMs) have shown promising performance, existing stepwise provers often suffer from biased search guidance, leading to inefficiencies and suboptimal proof strategies. This paper introduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise ATP system designed to overcome these limitations. MPS-Prover incorporates two key innovations: a highly effective post-training data curation strategy that prunes approximately 40\\% of redundant training data without sacrificing performance, and a multi-perspective tree search mechanism. This search integrates a learned critic model with strategically designed heuristic rules to diversify tactic selection, prevent getting trapped in unproductive states, and enhance search robustness. Extensive evaluations demonstrate that MPS-Prover achieves state-of-the-art performance on multiple challenging benchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter models. Furthermore, our analyses reveal that MPS-Prover generates significantly shorter and more diverse proofs compared to existing stepwise and whole-proof methods, highlighting its efficiency and efficacy. Our work advances the capabilities of LLM-based formal reasoning and offers a robust framework and a comprehensive analysis for developing more powerful theorem provers.",
      "arxiv_url": "https://openreview.net/forum?id=Ox3U97Svtl",
      "pdf_url": "https://openreview.net/pdf/350dc187c2833efc7cd93402a27fd709f790c12b.pdf",
      "primary_category": "Theorem Proving, Large Language Model, Tree Search",
      "categories": [
        "Theorem Proving",
        "Large Language Model",
        "Tree Search"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QQhQIqons0",
      "title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks",
      "authors": [
        "Hwiwon Lee",
        "Ziqi Zhang",
        "Hanxiao Lu",
        "LINGMING ZHANG"
      ],
      "abstract": "Rigorous security-focused evaluation of large language model (LLM) agents is imperative for establishing trust in their safe deployment throughout the software development lifecycle. However, existing benchmarks largely rely on synthetic challenges or simplified vulnerability datasets that fail to capture the complexity and ambiguity encountered by security engineers in practice. We introduce SEC-bench, the first fully automated benchmarking framework for evaluating LLM agents on authentic security engineering tasks. SEC-bench employs a novel multi-agent scaffold that automatically constructs code repositories with harnesses, reproduces vulnerabilities in isolated environments, and generates gold patches for reliable evaluation. Our framework automatically creates high-quality software vulnerability datasets with reproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench, we implement two critical software security tasks to rigorously evaluate LLM agents' capabilities: proof-of-concept (PoC) generation and vulnerability patching. A comprehensive evaluation of state-of-the-art LLM code agents reveals significant performance gaps, achieving at most 18.0% success in PoC generation and 34.0% in vulnerability patching on our complete dataset. These results highlight the crucial steps needed toward developing LLM agents that are more practical, intelligent, and autonomous for security engineering.",
      "arxiv_url": "https://openreview.net/forum?id=QQhQIqons0",
      "pdf_url": "https://openreview.net/pdf/66d2a1a241b7979f0f8776b51e62c6b1f1bc7db8.pdf",
      "primary_category": "Security Benchmarking, LLM Agents, Security Engineering",
      "categories": [
        "Security Benchmarking",
        "LLM Agents",
        "Security Engineering",
        "AI for Security",
        "Automated Evaluation"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VuVhgEiu20",
      "title": "TTRL: Test-Time Reinforcement Learning",
      "authors": [
        "Yuxin Zuo",
        "Kaiyan Zhang",
        "Li Sheng",
        "Shang Qu",
        "Ganqu Cui",
        "Xuekai Zhu",
        "Haozhan Li",
        "Yuchen Zhang",
        "Xinwei Long",
        "Ermo Hua",
        "Biqing Qi",
        "Youbang Sun",
        "Zhiyuan Ma",
        "Lifan Yuan",
        "Ning Ding",
        "Bowen Zhou"
      ],
      "abstract": "This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 211% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks and highlight TTRL's potential for broader tasks and domains.",
      "arxiv_url": "https://openreview.net/forum?id=VuVhgEiu20",
      "pdf_url": "https://openreview.net/pdf/3ff432e912f7ed9bbbacf9a7a16d7e5af88d721a.pdf",
      "primary_category": "Reinforcement Learning, Test-Time Training, Large Language Model",
      "categories": [
        "Reinforcement Learning",
        "Test-Time Training",
        "Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Q3qAsZAEZw",
      "title": "Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference",
      "authors": [
        "Jiayi Yuan",
        "Hao Li",
        "Xinheng Ding",
        "Wenya Xie",
        "Yu-Jhe Li",
        "Wentian Zhao",
        "Kun Wan",
        "Jing Shi",
        "Xia Hu",
        "Zirui Liu"
      ],
      "abstract": "Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration, such as evaluation batch size, GPU count, and GPU version, can introduce significant differences in the generated responses. \nThis issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9\\% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size.\nWe trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. \nThis work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge.\nOur analysis reveals that floating-point precision—while critical for reproducibility—is often neglected in evaluation practices.\nInspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility.",
      "arxiv_url": "https://openreview.net/forum?id=Q3qAsZAEZw",
      "pdf_url": "https://openreview.net/pdf/0025fad6ba784349766905dc7158027e8500db26.pdf",
      "primary_category": "Large Language Models (LLMs), Reproducibility, Numerical precision",
      "categories": [
        "Large Language Models (LLMs)",
        "Reproducibility",
        "Numerical precision",
        "Deterministic inference"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9FAm023EmI",
      "title": "Hierarchical Demonstration Order Optimization for Many-shot In-Context Learning",
      "authors": [
        "Yinhan He",
        "Wendy Zheng",
        "Song Wang",
        "Zaiyi Zheng",
        "Yushun Dong",
        "Yaochen Zhu",
        "Jundong Li"
      ],
      "abstract": "In-Context Learning (ICL) is a technique where large language models (LLMs) leverage multiple demonstrations (i.e., examples) to perform tasks. With the recent expansion of LLM context windows, many-shot ICL (generally with more than 50 demonstrations) can lead to significant performance improvements on a variety of language tasks such as text classification and question answering. Nevertheless, ICL faces the issue of demonstration order instability (ICL-DOI), which means that performance varies significantly depending on the order of demonstrations. Moreover, ICL-DOI persists in many-shot ICL, validated by our thorough experimental investigation.\nCurrent strategies for handling ICL-DOI are not applicable to many-shot ICL due to two critical challenges:  (1) Most existing methods assess demonstration order quality by first prompting the LLM, then using heuristic metrics based on the LLM's predictions. In the many-shot scenarios, these metrics without theoretical grounding become unreliable, where the LLMs struggle to effectively utilize information from long input contexts, making order distinctions less clear. The requirement to examine all orders for the large number of demonstrations is computationally infeasible due to the super-exponential complexity of the order space in many-shot ICL.  To tackle the first challenge, we design a demonstration order evaluation metric based on information theory for measuring order quality, which effectively quantifies the usable information gain of a given demonstration order. To address the second challenge, we propose a hierarchical demonstration order optimization method named \\texttt{HIDO} that enables a more refined exploration of the order space, achieving high ICL performance without the need to evaluate all possible orders. \nExtensive experiments on multiple LLMs and real-world datasets demonstrate that our \\texttt{HIDO} method consistently and efficiently outperforms other baselines. Our code project can be found at https://github.com/YinhanHe123/HIDO/.",
      "arxiv_url": "https://openreview.net/forum?id=9FAm023EmI",
      "pdf_url": "https://openreview.net/pdf/5f8c1b578d2f6bea58bd3571ef712c8b791b1144.pdf",
      "primary_category": "in-context learning, demonstration order optimization",
      "categories": [
        "in-context learning",
        "demonstration order optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1ZuzFUMtx6",
      "title": "SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens",
      "authors": [
        "Yinhan He",
        "Wendy Zheng",
        "Yaochen Zhu",
        "Zaiyi Zheng",
        "Lin Su",
        "Sriram Vasudevan",
        "Qi Guo",
        "Liangjie Hong",
        "Jundong Li"
      ],
      "abstract": "Chain-of-Thought (CoT) enhances the performance of Large Language Models (LLMs) on reasoning tasks by encouraging step-by-step solutions. However, the verbosity of CoT reasoning hinders its mass deployment in efficiency-critical applications. Recently, implicit CoT approaches have emerged, which encode reasoning steps within LLM's hidden embeddings (termed ``implicit reasoning'') rather than explicit tokens. This approach accelerates CoT reasoning by reducing the reasoning length and bypassing some LLM components. However, existing implicit CoT methods face two significant challenges: (1) they fail to preserve the semantic alignment between the implicit reasoning (when transformed to natural language) and the ground-truth reasoning, resulting in a significant CoT performance degradation, and (2) they focus on reducing the length of the implicit reasoning; however, they neglect the considerable time cost for an LLM to generate one individual implicit reasoning token. To tackle these challenges, we propose a novel semantically-aligned implicit CoT framework termed **SemCoT**. In particular, for the first challenge, we design a contrastively trained sentence transformer that evaluates semantic alignment between implicit and explicit reasoning, which is used to enforce semantic preservation during implicit reasoning optimization. To address the second challenge, we introduce an efficient implicit reasoning generator by finetuning a lightweight language model using knowledge distillation. This generator is guided by our sentence transformer to distill ground-truth reasoning into semantically aligned implicit reasoning, while also optimizing for accuracy. SemCoT is the first approach that enhances CoT efficiency by jointly optimizing token-level generation speed and preserving semantic alignment with ground-truth reasoning. Extensive experiments demonstrate the superior performance of SemCoT compared to state-of-the-art methods in both efficiency and effectiveness. Our code can be found at https://github.com/YinhanHe123/SemCoT/.",
      "arxiv_url": "https://openreview.net/forum?id=1ZuzFUMtx6",
      "pdf_url": "https://openreview.net/pdf/08f58e32f72c825d060ddacac0b61fa0df90b252.pdf",
      "primary_category": "Large Language Models (LLMs), Chain of Thought (CoT) reasoning",
      "categories": [
        "Large Language Models (LLMs)",
        "Chain of Thought (CoT) reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "un1TRwNgiv",
      "title": "Thinking vs. Doing: Improving Agent Reasoning by  Scaling Test-Time Interaction",
      "authors": [
        "Junhong Shen",
        "Hao Bai",
        "Lunjun Zhang",
        "Yifei Zhou",
        "Amrith Setlur",
        "Shengbang Tong",
        "Diego Caples",
        "Nan Jiang",
        "Tong Zhang",
        "Ameet Talwalkar",
        "Aviral Kumar"
      ],
      "abstract": "Test-time scaling in agentic tasks often relies on generating long reasoning traces (\"think\" more) before acting, but this does not allow agents to acquire new information from the environment or adapt behavior over time. In this work, we propose scaling test-time interaction, an untapped dimension for test-time scaling that increases the agent's interaction horizon to enable rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we situate our study in the domain of web agents. We first show that even prompting-based interaction scaling can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI, a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their interaction lengths during rollout. Using a Gemma 3 12B model, TTI sets a new state-of-the-art among open-source agents trained on public data on WebVoyager and WebArena. Case studies further reveal that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-action compute, offering new avenues for training robust and adaptive agents.",
      "arxiv_url": "https://openreview.net/forum?id=un1TRwNgiv",
      "pdf_url": "https://openreview.net/pdf/20cba89765ebda48a0c183f17ceb68d13f90d0dc.pdf",
      "primary_category": "LLM Agent, Web Agent, Reasoning",
      "categories": [
        "LLM Agent",
        "Web Agent",
        "Reasoning"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4xvE6Iy77Y",
      "title": "PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models",
      "authors": [
        "Ruiqi Wang",
        "Dezhong Zhao",
        "Ziqin Yuan",
        "Tianyu Shao",
        "Guohua Chen",
        "Dominic Kao",
        "Sungeun Hong",
        "Byung-Cheol Min"
      ],
      "abstract": "Preference-based reinforcement learning (PbRL) has emerged as a promising paradigm for teaching robots complex behaviors without reward engineering. However, its effectiveness is often limited by two critical challenges: the reliance on extensive human input and the inherent difficulties in resolving query ambiguity and credit assignment during reward learning. In this paper, we introduce PRIMT, a PbRL framework designed to overcome these challenges by leveraging foundation models (FMs) for multimodal synthetic feedback and trajectory synthesis. Unlike prior approaches that rely on single-modality FM evaluations, PRIMT employs a hierarchical neuro-symbolic fusion strategy, integrating the complementary strengths of vision-language models (VLMs) and large language models (LLMs) in evaluating robot behaviors for more reliable and comprehensive feedback. PRIMT also incorporates foresight trajectory generation to warm-start the trajectory buffer with bootstrapped samples, reducing early-stage query ambiguity, and hindsight trajectory augmentation for counterfactual reasoning with a causal auxiliary loss to improve credit assignment. We evaluate PRIMT on 2 locomotion and 6 manipulation tasks on various benchmarks, demonstrating superior performance over FM-based and scripted baselines. Website at https://primt25.github.io/.",
      "arxiv_url": "https://openreview.net/forum?id=4xvE6Iy77Y",
      "pdf_url": "https://openreview.net/pdf/cdfd3bdfaabcaadcbca0caae2c468c1730a2cc88.pdf",
      "primary_category": "Preference-based Reinforcement Learning, Foundation Models for Robotics, Neuro-Symbolic Fusion",
      "categories": [
        "Preference-based Reinforcement Learning",
        "Foundation Models for Robotics",
        "Neuro-Symbolic Fusion",
        "Multimodal Feedback",
        "Causal Inference",
        "Trajectory Synthesis",
        "Robot Manipulation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "WAFD6VYIEa",
      "title": "Offline RL by Reward-Weighted Fine-Tuning for Conversation Optimization",
      "authors": [
        "Subhojyoti Mukherjee",
        "Viet Dac Lai",
        "Raghavendra Addanki",
        "Ryan A. Rossi",
        "Seunghyun Yoon",
        "Trung Bui",
        "Anup Rao",
        "Jayakumar Subramanian",
        "Branislav Kveton"
      ],
      "abstract": "Offline reinforcement learning (RL) is a variant of RL where the policy is learned from a previously collected dataset of trajectories and rewards. In our work, we propose a practical approach to offline RL with large language models (LLMs). We recast the problem as reward-weighted fine-tuning, which can be solved using similar techniques to supervised fine-tuning (SFT). To showcase the value of our approach, we apply it to learning short-horizon question-answering policies of a fixed length, where the agent reasons about potential answers or asks clarifying questions. Our work stands in a stark contrast to state-of-the-art methods in this domain, based on SFT and direct preference optimization, which have additional hyper-parameters and do not directly optimize for rewards. We compare to them empirically, and report major gains in both optimized rewards and language quality.",
      "arxiv_url": "https://openreview.net/forum?id=WAFD6VYIEa",
      "pdf_url": "https://openreview.net/pdf/243bbfe0b7900c3b8f4f0c111d3ed3acfcb36378.pdf",
      "primary_category": "offline reinforcement learning, fine-tuning, LLMs",
      "categories": [
        "offline reinforcement learning",
        "fine-tuning",
        "LLMs",
        "question answering",
        "clarifying questions"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cot6mZPkWo",
      "title": "CAT: Content-Adaptive Image Tokenization",
      "authors": [
        "Junhong Shen",
        "Kushal Tirumala",
        "Michihiro Yasunaga",
        "Ishan Misra",
        "Luke Zettlemoyer",
        "LILI YU",
        "Chunting Zhou"
      ],
      "abstract": "Most existing image tokenizers encode images into a fixed number of tokens or patches, overlooking the inherent variability in image complexity and introducing unnecessary computate overhead for   simpler images. To address this, we propose Content-Adaptive Tokenizer (CAT), which dynamically adjusts representation capacity based on the image content and encodes simpler images into fewer tokens. We design (1) a caption-based evaluation system that leverages LLMs to predict content complexity and determine the optimal compression ratio for an image, and (2) a novel nested VAE architecture that performs variable-rate compression in a single model.\nTrained on images with  varying   complexity, CAT achieves an average of 15% reduction in rFID across seven detail-rich datasets containing text, humans, and complex textures. On natural image datasets like ImageNet and COCO, it  reduces token   usage by 18% while maintaining high-fidelity reconstructions.   We further evaluate CAT on two downstream tasks.  For image classification, CAT consistently improves top-1 accuracy across  five datasets spanning diverse domains. For image generation, it boosts training throughput by 23% on ImageNet, leading to more efficient learning and  improved FIDs over fixed-token baselines.",
      "arxiv_url": "https://openreview.net/forum?id=cot6mZPkWo",
      "pdf_url": "https://openreview.net/pdf/f7498b300cd013863f5acb6bb2ce3129ee2ee480.pdf",
      "primary_category": "Image tokenization",
      "categories": [
        "Image tokenization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MgN8Px0NA5",
      "title": "Keeping an Eye on LLM Unlearning: The Hidden Risk and Remedy",
      "authors": [
        "Jie Ren",
        "Zhenwei DAI",
        "Xianfeng Tang",
        "Yue Xing",
        "Shenglai Zeng",
        "Hui Liu",
        "Jingying Zeng",
        "Qiankun Peng",
        "Samarth Varshney",
        "Suhang Wang",
        "Qi He",
        "Charu C. Aggarwal",
        "Hui Liu"
      ],
      "abstract": "Although Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks, growing concerns have emerged over the misuse of sensitive, copyrighted, or harmful data during training. To address these concerns, unlearning techniques have been developed to remove the influence of specific data without retraining from scratch. However, this paper reveals a critical vulnerability in fine-tuning-based unlearning: a malicious user can craft a manipulated forgetting request that stealthily degrades the model’s utility for benign users. We demonstrate this risk through a red-teaming Stealthy Attack (SA), which is inspired by two key limitations of existing unlearning—the inability to constrain the scope of unlearning effect and the failure to distinguish benign tokens from unlearning signals. Prior work has shown that unlearned models tend to memorize forgetting data as unlearning signals, and respond with hallucinations or feigned ignorance when unlearning signals appear in the input. By subtly increasing the presence of common benign tokens in the forgetting data, SA enhances the connection between benign tokens and unlearning signals. As a result, when normal users include such tokens in their prompts, the model exhibits unlearning behaviors, leading to unintended utility degradation. To address this vulnerability, we propose Scope-aware Unlearning (SU), a lightweight enhancement that introduces a scope term into the unlearning objective, encouraging the model to localize the forgetting effect. Our method requires no additional data processing, integrates seamlessly with existing fine-tuning frameworks, and significantly improves robustness against SA. Extensive experiments validate the effectiveness of both SA and SU.",
      "arxiv_url": "https://openreview.net/forum?id=MgN8Px0NA5",
      "pdf_url": "https://openreview.net/pdf/3bae88cbb3df53aa1a1bdce083fcbd9dd07d8213.pdf",
      "primary_category": "Large Language Models, Unlearning, Attack",
      "categories": [
        "Large Language Models",
        "Unlearning",
        "Attack",
        "Robustness"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pydeKTMrJr",
      "title": "Cross City Traffic Flow Generation via Retrieval Augmented Diffusion Model",
      "authors": [
        "Yudong Li",
        "Jingyuan Wang",
        "Xie Yu",
        "Peiyu Wang",
        "Qian Huang"
      ],
      "abstract": "Traffic flow data are of great value in smart city applications. However, limited by data collection costs and privacy sensitivity, it is rather difficult to obtain large-scale traffic flow data. Therefore, various data generation methods have been proposed in the literature. Nevertheless, these methods often require data from a specific city for training and are difficult to directly apply to new cities lacking data.\nTo address this problem, this paper proposes a retrieval-augmented diffusion generation model with representation alignment. We use data from multiple source cities for training, extract consistent representations across multiple cities, and leverage retrieval-augmented generation (RAG) technology to incorporate historical data from source cities under similar conditions into the condition, aiming to improve the accuracy of data generation in the target city.\nExperiments on four real-world datasets demonstrate that, compared with existing deep learning methods, our method achieves better cross-city transfer performance.",
      "arxiv_url": "https://openreview.net/forum?id=pydeKTMrJr",
      "pdf_url": "https://openreview.net/pdf/9fdec2e66f45a45a3adf88f9e3c5c7a99b72a6cc.pdf",
      "primary_category": "Traffic Flow, Diffusion, Retrieval-Augmented Generation",
      "categories": [
        "Traffic Flow",
        "Diffusion",
        "Retrieval-Augmented Generation"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ravS5h8MNg",
      "title": "HyperGraphRAG: Retrieval-Augmented Generation via Hypergraph-Structured Knowledge Representation",
      "authors": [
        "Haoran Luo",
        "Haihong E",
        "Guanting Chen",
        "Yandan Zheng",
        "Xiaobao Wu",
        "Yikai Guo",
        "Qika Lin",
        "Yu Feng",
        "Zemin Kuang",
        "Meina Song",
        "Yifan Zhu",
        "Anh Tuan Luu"
      ],
      "abstract": "Standard Retrieval-Augmented Generation (RAG) relies on chunk-based retrieval, whereas GraphRAG advances this approach by graph-based knowledge representation. However, existing graph-based RAG approaches are constrained by binary relations, as each edge in an ordinary graph connects only two entities, limiting their ability to represent the n-ary relations (n >= 2) in real-world knowledge. In this work, we propose HyperGraphRAG, the first hypergraph-based RAG method that represents n-ary relational facts via hyperedges. HyperGraphRAG consists of a comprehensive pipeline, including knowledge hypergraph construction, retrieval, and generation. Experiments across medicine, agriculture, computer science, and law demonstrate that HyperGraphRAG outperforms both standard RAG and previous graph-based RAG methods in answer accuracy, retrieval efficiency, and generation quality.",
      "arxiv_url": "https://openreview.net/forum?id=ravS5h8MNg",
      "pdf_url": "https://openreview.net/pdf/b2eef4759ff7cfa93d85a3e70eea9b488223ea9f.pdf",
      "primary_category": "Retrieval-Augumented Generation, Hypergraph, N-ary Relation Extraction",
      "categories": [
        "Retrieval-Augumented Generation",
        "Hypergraph",
        "N-ary Relation Extraction",
        "Knowledge Representation"
      ],
      "tags": [
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "hVYp0WzyLK",
      "title": "UniDomain: Pretraining a Unified PDDL Domain from Real-World Demonstrations for Generalizable Robot Task Planning",
      "authors": [
        "Haoming Ye",
        "Yunxiao Xiao",
        "Cewu Lu",
        "Panpan Cai"
      ],
      "abstract": "Robotic task planning in real-world environments requires reasoning over implicit constraints from language and vision. While LLMs and VLMs offer strong priors, they struggle with long-horizon structure and symbolic grounding. Existing meth-\nods that combine LLMs with symbolic planning often rely on handcrafted or narrow domains, limiting generalization. We propose UniDomain, a framework that pre-trains a PDDL domain from robot manipulation demonstrations and applies it for online robotic task planning. It extracts atomic domains from 12,393 manipulation videos to form a unified domain with 3137 operators, 2875 predicates, and 16481 causal edges. Given a target class of tasks, it retrieves relevant atomics from the unified domain and systematically fuses them into high-quality meta-domains for zero-shot planning. Experiments on diverse real-world tasks show that UniDomain solves complex, unseen tasks in a zero-shot manner, achieving up to 58% higher task success and 160% improvement in plan optimality over state-of-the-art LLM and LLM-PDDL baselines.",
      "arxiv_url": "https://openreview.net/forum?id=hVYp0WzyLK",
      "pdf_url": "https://openreview.net/pdf/95c462c5adee0f4675bbda60cbdf8a9b46439c58.pdf",
      "primary_category": "Robotics, Task Planning, Symbolic Planning",
      "categories": [
        "Robotics",
        "Task Planning",
        "Symbolic Planning",
        "Vision Language Models",
        "Robotic Knowledge Graph"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ur295YVtmt",
      "title": "ReMA: Learning to Meta-Think for LLMs with Multi-agent Reinforcement Learning",
      "authors": [
        "Ziyu Wan",
        "Yunxiang LI",
        "Xiaoyu Wen",
        "Yan Song",
        "Hanjing Wang",
        "Linyi Yang",
        "Mark Schmidt",
        "Jun Wang",
        "Weinan Zhang",
        "Shuyue Hu",
        "Ying Wen"
      ],
      "abstract": "Recent research on Reasoning of Large Language Models (LLMs) has sought to further enhance their performance by integrating meta-thinking—enabling models to monitor, evaluate, and control their reasoning processes for more adaptive and effective problem-solving.\nHowever, current single-agent work lacks a specialized design for acquiring meta-thinking, resulting in low efficacy.\nTo address this challenge, we introduce Reinforced Meta-thinking Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think about thinking.\nReMA decouples the reasoning process into two hierarchical agents: a high-level meta-thinking agent responsible for generating strategic oversight and plans, and a low-level reasoning agent for detailed executions.\nThrough iterative reinforcement learning with aligned objectives, these agents explore and learn collaboration, leading to improved generalization and robustness.\nEmpirical results from single-turn experiments demonstrate that ReMA outperforms single-agent RL baselines on complex reasoning tasks, including competitive-level mathematical benchmarks and LLM-as-a-Judge benchmarks.\nAdditionally, we further extend ReMA to multi-turn interaction settings, leveraging turn-level ratio and parameter sharing to improve efficiency.\nComprehensive ablation studies further illustrate the evolving dynamics of each distinct agent, providing valuable insights into how the meta-thinking reasoning process enhances the reasoning capabilities of LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=ur295YVtmt",
      "pdf_url": "https://openreview.net/pdf/4ef88667f9d6e59455e6805cd91df2e8e519f9d0.pdf",
      "primary_category": "Reasoning, Large Language Model, Multi-Agent",
      "categories": [
        "Reasoning",
        "Large Language Model",
        "Multi-Agent",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "n4V3MSqK77",
      "title": "Agentic Plan Caching: Test-Time Memory for Fast and Cost-Efficient LLM Agents",
      "authors": [
        "Qizheng Zhang",
        "Michael Wornow",
        "Kunle Olukotun"
      ],
      "abstract": "LLM-based agent applications have shown increasingly remarkable capabilities in complex workflows but incur substantial costs and latency due to extensive planning and reasoning requirements. \nExisting LLM caching techniques (like context caching and semantic caching), primarily designed for serving chatbots, are insufficient for agent applications where outputs depend on external data and environmental contexts. \nWe propose **Agentic Plan Caching (APC)**, a novel **test-time memory** that extracts, stores, adapts, and reuses structured plan templates from planning stages of agent applications across semantically similar tasks to reduce the cost and latency of serving. \nUnlike traditional semantic caching, our system extracts plan templates from completed agent executions at test-time, employs keyword extraction to match new requests against cached plans, and utilizes lightweight models to adapt these templates to task-specific plans with contexts. \nEvaluation across multiple real-world agent applications shows that our system can reduce costs by 50.31\\% and latency by 27.28\\% on average while maintaining performance, offering a more efficient solution for serving LLM-based agents that complements existing LLM serving infrastructures.",
      "arxiv_url": "https://openreview.net/forum?id=n4V3MSqK77",
      "pdf_url": "https://openreview.net/pdf/2362f2d245503d9d209dde5a45fbfbf3fb6a5990.pdf",
      "primary_category": "Caching, Memory, Serving",
      "categories": [
        "Caching",
        "Memory",
        "Serving",
        "LLM Agents"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oDcAGSXZZP",
      "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
      "authors": [
        "Jingbo Yang",
        "Bairu Hou",
        "Wei Wei",
        "Yujia Bao",
        "Shiyu Chang"
      ],
      "abstract": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we investigate a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation when using KV caches computed independently for each document, KVLink introduces two key techniques: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, and using trainable special tokens to restore self-attention across independently encoded documents. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 96% compared to standard LLM inference, making it a scalable and efficient solution for context reuse. Additionally, KVLink can be combined with KV cache compression to further save cache loading and storage overhead while outperforming the baselines.",
      "arxiv_url": "https://openreview.net/forum?id=oDcAGSXZZP",
      "pdf_url": "https://openreview.net/pdf/1313459390a6f2f26aba7c6b4aae71a261d1890c.pdf",
      "primary_category": "LLM, Retrieval-Augmented Generation, Efficiency",
      "categories": [
        "LLM",
        "Retrieval-Augmented Generation",
        "Efficiency"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0QNmAvQQqj",
      "title": "GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation",
      "authors": [
        "Linhao Luo",
        "Zicheng Zhao",
        "Gholamreza Haffari",
        "Dinh Phung",
        "Chen Gong",
        "Shirui Pan"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has proven effective in integrating knowledge into large language models (LLMs). However, conventional RAGs struggle to capture complex relationships between pieces of knowledge, limiting their performance in intricate reasoning that requires integrating knowledge from multiple sources. Recently, graph-enhanced retrieval augmented generation (GraphRAG) builds a graph structure to explicitly model these relationships, enabling more effective and efficient retrievers. Nevertheless, its performance is still hindered by the noise and incompleteness within the graph structure. To address this, we introduce GFM-RAG, a novel graph foundation model (GFM) for retrieval augmented generation. GFM-RAG is powered by an innovative graph neural network that reasons over graph structure to capture complex query-knowledge relationships. The GFM with 8M parameters undergoes a two-stage training process on large-scale datasets, comprising 60 knowledge graphs with over 14M triples and 700k documents. This results in impressive performance and generalizability for GFM-RAG, making it the first graph foundation model applicable to unseen datasets for retrieval without any fine-tuning required. Extensive experiments on three multi-hop QA datasets and seven domain-specific RAG datasets demonstrate that GFM-RAG achieves state-of-the-art performance while maintaining efficiency and alignment with neural scaling laws, highlighting its potential for further improvement.",
      "arxiv_url": "https://openreview.net/forum?id=0QNmAvQQqj",
      "pdf_url": "https://openreview.net/pdf/78cbb8cc19da0e7f86c767d47131a88252e5a6f1.pdf",
      "primary_category": "Graph Foundation Model, Retrieval Augmented Generation, Graph-enhanced Retrieval Augmented Generation",
      "categories": [
        "Graph Foundation Model",
        "Retrieval Augmented Generation",
        "Graph-enhanced Retrieval Augmented Generation"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1GIQOV3NAj",
      "title": "Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and Open-Ended POMDPs",
      "authors": [
        "Wenjing Tang",
        "Xinyu He",
        "Yongxi Huang",
        "Yunxiao Xiao",
        "Cewu Lu",
        "Panpan Cai"
      ],
      "abstract": "Task planning under uncertainty is essential for home-service robots operating in the real world. Tasks involve ambiguous human instructions, hidden or unknown object locations, and open-vocabulary object types, leading to significant open-ended uncertainty and a boundlessly large planning space. To address these challenges, we propose Tru-POMDP, a planner that combines structured belief generation using Large Language Models (LLMs) with principled POMDP planning. Tru-POMDP introduces a hierarchical Tree of Hypotheses (TOH), which systematically queries an LLM to construct high-quality particle beliefs over possible world states and human goals. We further formulate an open-ended POMDP model that enables rigorous Bayesian belief tracking and efficient belief-space planning over these LLM-generated hypotheses. Experiments on complex object rearrangement tasks across diverse kitchen environments show that Tru-POMDP significantly outperforms state-of-the-art LLM-based and LLM-tree-search hybrid planners, achieving higher success rates with significantly better plans, stronger robustness to ambiguity and occlusion, and greater planning efficiency.",
      "arxiv_url": "https://openreview.net/forum?id=1GIQOV3NAj",
      "pdf_url": "https://openreview.net/pdf/ca1a8c5b6524651355a210258ef750c5e613e35d.pdf",
      "primary_category": "Robotics, Planning under uncertainty, Robot task planning",
      "categories": [
        "Robotics",
        "Planning under uncertainty",
        "Robot task planning",
        "Large language models",
        "POMDPs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "hzMkfIrdDT",
      "title": "Know What You Don't Know: Uncertainty Calibration of Process Reward Models",
      "authors": [
        "Young-Jin Park",
        "Kristjan Greenewald",
        "Kaveh Alim",
        "Hao Wang",
        "Navid Azizan"
      ],
      "abstract": "Process reward models (PRMs) play a central role in guiding inference-time scaling algorithms for large language models (LLMs).\nHowever, we observe that even state-of-the-art PRMs can be poorly calibrated. \nSpecifically, they tend to overestimate the success probability that a partial reasoning step will lead to a correct final answer, particularly when smaller LLMs are used to complete the reasoning trajectory.\nTo address this, we present a calibration approach—performed via quantile regression—that adjusts PRM outputs to better align with true success probabilities. \nLeveraging these calibrated success estimates and their associated confidence bounds, we introduce an instance-adaptive scaling (IAS) framework that dynamically adjusts the compute budget based on the estimated likelihood that a partial reasoning trajectory will yield a correct final answer.\nUnlike conventional methods that allocate a fixed number of reasoning trajectories per query, this approach adapts to each instance and reasoning step when using our calibrated PRMs. Experiments on mathematical reasoning benchmarks show that (i) our PRM calibration method achieves small calibration error, outperforming the baseline methods, (ii) calibration is crucial for enabling effective IAS, and (iii) the proposed IAS strategy reduces inference costs while maintaining final answer accuracy, utilizing less compute on more confident problems as desired.",
      "arxiv_url": "https://openreview.net/forum?id=hzMkfIrdDT",
      "pdf_url": "https://openreview.net/pdf/606f3ae7036f51301cf4b6ea9a67d121af6af571.pdf",
      "primary_category": "process reward model, calibration, inference-time-scaling",
      "categories": [
        "process reward model",
        "calibration",
        "inference-time-scaling",
        "instance-adaptive scaling"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3SUkvb8PRo",
      "title": "FlowPrune: Accelerating Attention Flow Calculation by Pruning Flow Network",
      "authors": [
        "Shuo Xu",
        "Yu Chen",
        "Shuxia Lin",
        "Xin Geng",
        "Xu Yang"
      ],
      "abstract": "The Transformer architecture serves as the foundation of modern AI systems, powering recent advances in Large Language Models (LLMs) and Large Multimodal Models (LMMs). Central to these models, attention mechanisms capture contextual dependencies via token interactions. Beyond inference, attention has been widely adopted for interpretability, offering insights into model behavior. Among interpretability techniques, attention flow --- which traces global information transfer across layers --- provides a more comprehensive perspective than single-layer attention maps. However, computing attention flow is computationally intensive due to the high complexity of max-flow algorithms. To address this challenge, we introduce FlowPrune, a novel framework that accelerates attention flow analysis by pruning the attention graph before applying max-flow computations. FlowPrune uses the Max-Flow Min-Cut Theorem and two structural properties of Transformer to identify and eliminate non-critical graph regions. It comprises two components: Edge Pruning, which removes insignificant attention edges, and Layer Compression, which discards layers with minimal contributions to the flow. We conduct extensive experiments on LLaMA and LLaVA to evaluate the robustness and effectiveness of FlowPrune. Our results show that FlowPrune achieves high agreement with the original attention flow in both absolute and relative error metrics, as well as in identifying influential input tokens. Finally, case studies in both NLP and vision domains demonstrate that FlowPrune produces consistent interpretability outcomes as the original Attention Flow, validating its practical utility. The code for this paper is publicly available.",
      "arxiv_url": "https://openreview.net/forum?id=3SUkvb8PRo",
      "pdf_url": "https://openreview.net/pdf/8f59fdad6ea8a413057f218b5185a5a926097849.pdf",
      "primary_category": "Transformer, Attention Flow, Graph Pruning",
      "categories": [
        "Transformer",
        "Attention Flow",
        "Graph Pruning",
        "Max-Flow Min-Cut",
        "Interpretability"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "KfZm1bkS8C",
      "title": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs",
      "authors": [
        "ChangHao Li",
        "Yuchen Zhuang",
        "Rushi Qiang",
        "Haotian Sun",
        "Hanjun Dai",
        "Chao Zhang",
        "Bo Dai"
      ],
      "abstract": "Despite the impressive generative abilities of black-box large language models (LLMs), their inherent opacity hinders further advancements in capabilities such as reasoning, planning, and personalization. \nExisting works aim to enhance LLM capabilities via domain-specific adaptation, which require additional training on accessible model parameters, an infeasible option for black-box LLMs. \nTo address this challenge, we introduce Matryoshka Pilot (M-Pilot), a lightweight white-box LLM controller that guides a large-scale black-box LLM generator by decomposing complex tasks into a series of intermediate outputs.\nSpecifically, we consider the black-box LLM as an environment, with M-Pilot serving as a policy to provide intermediate guidance through prompts for driving the black-box LLM. \nM-Pilot is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative interaction, which enables controllable multi-turn generation and self-improvement in optimizing intermediate guidance. \nEmpirical evaluations on diverse tasks demonstrate that our method effectively enhances the capabilities of black-box LLMs in complex, long-horizon tasks.",
      "arxiv_url": "https://openreview.net/forum?id=KfZm1bkS8C",
      "pdf_url": "https://openreview.net/pdf/5fd3499e446c2610ce8f859f6828f479d2098c37.pdf",
      "primary_category": "Large Language Models, Black-Box LLMs, LLM Reasoning",
      "categories": [
        "Large Language Models",
        "Black-Box LLMs",
        "LLM Reasoning",
        "LLM Planning",
        "LLM Personalization"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Gf4oPoluAV",
      "title": "TAI3: Testing Agent Integrity in Interpreting User Intent",
      "authors": [
        "Shiwei Feng",
        "Xiangzhe Xu",
        "Xuan Chen",
        "Kaiyuan Zhang",
        "Syed Yusuf Ahmed",
        "Zian Su",
        "Mingwei Zheng",
        "Xiangyu Zhang"
      ],
      "abstract": "LLM agents are increasingly deployed to automate real-world tasks by invoking APIs through natural language instructions. While powerful, they often suffer from misinterpretation of user intent, leading to the agent’s actions that diverge from the user’s intended goal, especially as external toolkits evolve. Traditional software testing assumes structured inputs and thus falls short in handling the ambiguity of natural language. We introduce TAI3, an API-centric stress testing framework that systematically uncovers intent integrity violations in LLM agents. Unlike prior work focused on fixed benchmarks or adversarial inputs, TAI3 generates realistic tasks based on toolkits’ documentation and applies targeted mutations to expose subtle agent errors while preserving user intent. To guide testing, we propose semantic partitioning, which organizes natural language tasks into meaningful categories based on toolkit API parameters and their equivalence classes. Within each partition, seed tasks are mutated and ranked by a lightweight predictor that estimates the likelihood of triggering agent errors. To enhance efficiency, TAI3 maintains a datatype-aware strategy memory that retrieves and adapts effective mutation patterns from past cases. Experiments on 80 toolkit APIs demonstrate that TAI3 effectively uncovers intent integrity violations, significantly outperforming baselines in both error-exposing rate and query efficiency. Moreover, TAI3 generalizes well to stronger target models using smaller LLMs for test generation, and adapts to evolving APIs across domains.",
      "arxiv_url": "https://openreview.net/forum?id=Gf4oPoluAV",
      "pdf_url": "https://openreview.net/pdf/11f637b039ea8d411deed20635e2af4cc8c76b3a.pdf",
      "primary_category": "LLM Agent, Agent Testing, Stress Testing",
      "categories": [
        "LLM Agent",
        "Agent Testing",
        "Stress Testing",
        "Software Engineering",
        "API-Calling",
        "Agent Safety"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "mN3CMpfWR6",
      "title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search",
      "authors": [
        "Dong Li",
        "Xujiang Zhao",
        "Linlin Yu",
        "Yanchi Liu",
        "Wei Cheng",
        "Zhengzhang Chen",
        "Zhong Chen",
        "Feng Chen",
        "Chen Zhao",
        "Haifeng Chen"
      ],
      "abstract": "Large Language Models (LLMs) offer promising capabilities for tackling complex reasoning tasks, including optimization problems. However, existing methods either rely on prompt engineering, which leads to poor generalization across problem types, or require costly supervised training. We introduce SolverLLM, a training-free framework that leverages test-time scaling to solve diverse optimization problems. Rather than solving directly, SolverLLM generates mathematical formulations and translates them into solver-ready code, guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the search process, we modify classical MCTS with (1) dynamic expansion for adaptive formulation generation, (2) prompt backpropagation to guide exploration via outcome-driven feedback, and (3) uncertainty backpropagation to incorporate reward reliability into decision-making. Experiments on six standard benchmark datasets demonstrate that SolverLLM outperforms both prompt-based and learning-based baselines, achieving strong generalization without additional training.",
      "arxiv_url": "https://openreview.net/forum?id=mN3CMpfWR6",
      "pdf_url": "https://openreview.net/pdf/b7f1cf26530dd86e2ae6d7c4a3a2f77a1c33b13b.pdf",
      "primary_category": "MCTS, LLM, Optimization Problem",
      "categories": [
        "MCTS",
        "LLM",
        "Optimization Problem"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZdmmOAN4h3",
      "title": "Breaking the Frozen Subspace: Importance Sampling for Low-Rank Optimization in LLM Pretraining",
      "authors": [
        "Haochen Zhang",
        "Junze Yin",
        "Guanchu Wang",
        "Zirui Liu",
        "Lin Yang",
        "Tianyi Zhang",
        "Anshumali Shrivastava",
        "Vladimir Braverman"
      ],
      "abstract": "Low-rank optimization has emerged as a promising approach to enabling memory-efficient training of large language models (LLMs). Existing low-rank optimization methods typically project gradients onto a low-rank subspace, reducing the memory cost of storing optimizer states. A key challenge in these methods is selecting suitable subspaces to ensure an effective optimization trajectory. Most existing approaches select the dominant subspace to preserve gradient information, as this intuitively provides the best approximation. However, we find that in practice, the dominant subspace stops changing during pretraining, thereby constraining weight updates to similar subspaces. In this paper, we propose importance sampling for low-rank optimization in LLM pretraining with a provable convergence guarantee, which the dominant subspace approach does not have. Empirically, we demonstrate that our method significantly outperforms previous methods in LLM pretraining tasks.",
      "arxiv_url": "https://openreview.net/forum?id=ZdmmOAN4h3",
      "pdf_url": "https://openreview.net/pdf/a5c087fda18ddf464374f4ba9f0eaf93e5414c84.pdf",
      "primary_category": "low-rank optimization, large language models, Adam",
      "categories": [
        "low-rank optimization",
        "large language models",
        "Adam",
        "memory efficiency"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "sLNz60fJFF",
      "title": "Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector",
      "authors": [
        "Haoyan Yang",
        "Runxue Bao",
        "Cao Xiao",
        "Jun Ma",
        "Parminder Bhatia",
        "Shangqian Gao",
        "Taha Kass-Hout"
      ],
      "abstract": "LLM-as-a-Judge has emerged as a promising tool for automatically evaluating generated outputs, but its reliability is often undermined by potential biases in judgment. Existing efforts to mitigate these biases face key limitations: in-context learning-based methods fail to address rooted biases due to the evaluator’s limited capacity for self-reflection, whereas fine-tuning is not applicable to all evaluator types, especially closed-source models. To address this challenge, we introduce the **R**easoning-based **B**ias **D**etector (RBD), which is a plug-in module that identifies biased evaluations and generates structured reasoning to guide evaluator self-correction. Rather than modifying the evaluator itself, RBD operates externally and engages in an iterative process of bias detection and feedback-driven revision. To support its development, we design a complete pipeline consisting of biased dataset construction, supervision collection, distilled reasoning-based fine-tuning of RBD, and integration with LLM evaluators. We fine-tune four sizes of RBD models, ranging from 1.5B to 14B, and observe consistent performance improvements across all scales. Experimental results on 4 bias types—verbosity, position, bandwagon, and sentiment—evaluated using 8 LLM evaluators demonstrate RBD’s strong effectiveness. For example, the RBD-8B model improves evaluation accuracy by an average of 18.5% and consistency by 10.9%, and surpasses prompting-based baselines and fine-tuned judges by 12.8% and 17.2%, respectively. These results highlight RBD’s effectiveness and scalability. Additional experiments further demonstrate its strong generalization across biases and domains, as well as its efficiency.",
      "arxiv_url": "https://openreview.net/forum?id=sLNz60fJFF",
      "pdf_url": "https://openreview.net/pdf/0159ae7436ad0e9172c17c46945a001ebdffd215.pdf",
      "primary_category": "LLM Evaluation, Bias Mitigation, Reasoning-based Methods",
      "categories": [
        "LLM Evaluation",
        "Bias Mitigation",
        "Reasoning-based Methods"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RlqYCpTu1P",
      "title": "MoBA: Mixture of Block Attention for Long-Context LLMs",
      "authors": [
        "Enzhe Lu",
        "Zhejun Jiang",
        "Jingyuan Liu",
        "Yulun Du",
        "Tao Jiang",
        "Chao Hong",
        "Shaowei Liu",
        "Weiran He",
        "Enming Yuan",
        "Yuzhi Wang",
        "Zhiqi Huang",
        "Huan Yuan",
        "Suting Xu",
        "Xinran Xu",
        "Guokun Lai",
        "Yanru Chen",
        "Huabin Zheng",
        "Junjie Yan",
        "Jianlin Su",
        "Yuxin Wu",
        "Yutao Zhang",
        "Zhilin Yang",
        "Xinyu Zhou",
        "Mingxing Zhang",
        "Jiezhong Qiu"
      ],
      "abstract": "Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored.\n\nIn this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to handle actual production workloads with long-context requirements, demonstrating significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA.",
      "arxiv_url": "https://openreview.net/forum?id=RlqYCpTu1P",
      "pdf_url": "https://openreview.net/pdf/58c8bb4e2a6dc99072ee66e05abcf02af7db3e30.pdf",
      "primary_category": "LLM, Sparse Attention, Long Context",
      "categories": [
        "LLM",
        "Sparse Attention",
        "Long Context"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "bjUDrU4wPv",
      "title": "QiMeng-MuPa: Mutual-Supervised Learning for Sequential-to-Parallel Code Translation",
      "authors": [
        "Changxin Ke",
        "Rui Zhang",
        "Shuo Wang",
        "Li Ding",
        "Guangli Li",
        "Yuanbo Wen",
        "Shuoming Zhang",
        "Ruiyuan Xu",
        "Jin Qin",
        "Jiaming Guo",
        "Chenxi Wang",
        "Ling Li",
        "Qi Guo",
        "Yunji Chen"
      ],
      "abstract": "The rise of GPU-based high-performance computing (HPC) has driven the widespread adoption of parallel programming models such as CUDA. Yet, the inherent complexity of parallel programming creates a demand for the automated sequential-to-parallel approaches.\nHowever, data scarcity poses a significant challenge for machine learning-based sequential-to-parallel code translation. Although recent back-translation methods show promise, they still fail to ensure functional equivalence in the translated code. In this paper, we propose \\textbf{QiMeng-MuPa}, a novel \\textbf{Mu}tual-Supervised Learning framework for Sequential-to-\\textbf{Pa}rallel code translation, to address the functional equivalence issue. QiMeng-MuPa consists of two models, a Translator and a Tester. Through an iterative loop consisting of Co-verify and Co-evolve steps, the Translator and the Tester mutually generate data for each other and improve collectively. The Tester generates unit tests to verify and filter functionally equivalent translated code, thereby evolving the Translator, while the Translator generates translated code as augmented input to evolve the Tester. Experimental results demonstrate that QiMeng-MuPa significantly enhances the performance of the base models: when applied to Qwen2.5-Coder, it not only improves Pass@1 by up to 28.91\\% and boosts Tester performance by 68.90\\%, but also outperforms the previous state-of-the-art method CodeRosetta by 1.56 and 6.92 in BLEU and CodeBLEU scores, while achieving performance comparable to DeepSeek-R1 and GPT-4.1. Our code is available at \\url{https://github.com/kcxain/mupa}.",
      "arxiv_url": "https://openreview.net/forum?id=bjUDrU4wPv",
      "pdf_url": "https://openreview.net/pdf/09328f80428f479b87f26715e713ccbfa23897d4.pdf",
      "primary_category": "mutual-supervised learning, code generation, HPC code translation",
      "categories": [
        "mutual-supervised learning",
        "code generation",
        "HPC code translation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vTJFQu5YXz",
      "title": "Tackling Feature-Classifier Mismatch in Federated Learning via Prompt-Driven Feature Transformation",
      "authors": [
        "Xinghao Wu",
        "Xuefeng Liu",
        "Jianwei Niu",
        "Guogang Zhu",
        "Mingjia Shi",
        "Shaojie Tang",
        "Jing Yuan"
      ],
      "abstract": "Federated Learning (FL) faces challenges due to data heterogeneity, which limits the global model’s performance across diverse client distributions. Personalized Federated Learning (PFL) addresses this by enabling each client to process an individual model adapted to its local distribution. Many existing methods assume that certain global model parameters are difficult to train effectively in a collaborative manner under heterogeneous data. Consequently, they localize or fine-tune these parameters to obtain personalized models. In this paper, we reveal that both the feature extractor and classifier of the global model are inherently strong, and the primary cause of its suboptimal performance is the mismatch between local features and the global classifier. Although existing methods alleviate this mismatch to some extent and improve performance, we find that they either (1) fail to fully resolve the mismatch while degrading the feature extractor, or (2) address the mismatch only post-training, allowing it to persist during training. This increases inter-client gradient divergence, hinders model aggregation, and ultimately leaves the feature extractor suboptimal for client data. To address this issue, we propose FedPFT, a novel framework that resolves the mismatch during training using personalized prompts. These prompts, along with local features, are processed by a shared self-attention-based transformation module, ensuring alignment with the global classifier. Additionally, this prompt-driven approach offers strong flexibility, enabling task-specific prompts to incorporate additional training objectives (\\eg, contrastive learning) to further enhance the feature extractor. Extensive experiments show that FedPFT outperforms state-of-the-art methods by up to 5.07%, with further gains of up to 7.08% when collaborative contrastive learning is incorporated.",
      "arxiv_url": "https://openreview.net/forum?id=vTJFQu5YXz",
      "pdf_url": "https://openreview.net/pdf/bca312a9cb900c41bd9b353c7d8dfe7d37af87b8.pdf",
      "primary_category": "Personalized Federated Learning, Feature-Classifier Mismatch, Prompt-Driven Feature Transformation",
      "categories": [
        "Personalized Federated Learning",
        "Feature-Classifier Mismatch",
        "Prompt-Driven Feature Transformation"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "GrDEV4InKZ",
      "title": "What Matters in Data for DPO?",
      "authors": [
        "Yu Pan",
        "Zhongze Cai",
        "Huaiyang Zhong",
        "Guanting Chen",
        "Chonghuan Wang"
      ],
      "abstract": "Direct Preference Optimization (DPO) has emerged as a simple and effective approach for aligning large language models (LLMs) with human preferences, bypassing the need for a learned reward model. Despite its growing adoption, a fundamental question remains open: what characteristics of preference data are most critical for DPO performance? In this work, we provide a systematic study of how preference data distribution influences DPO, from both theoretical and empirical perspectives. We show that the quality of chosen responses plays a dominant role in optimizing the DPO objective, while the quality of rejected responses may have relatively limited impact. Our theoretical analysis characterizes the optimal response distribution under DPO and reveals how contrastiveness between responses helps primarily by improving the chosen samples. We further study an online DPO setting and show it effectively reduces to supervised fine-tuning on the chosen responses. Extensive experiments across diverse tasks confirm our findings: improving the quality of chosen responses consistently boosts performance regardless of the quality of the rejected responses. We also investigate the benefit of mixing the on-policy data. Our results interpret the mechanism behind some widely adopted strategies and offer practical insights for constructing high-impact preference datasets for LLM alignment.",
      "arxiv_url": "https://openreview.net/forum?id=GrDEV4InKZ",
      "pdf_url": "https://openreview.net/pdf/07c87cd4818a654da82d230850d9ad6d6eb55dfd.pdf",
      "primary_category": "DPO, Fine-tuning, Alignment",
      "categories": [
        "DPO",
        "Fine-tuning",
        "Alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4xGJZkdjCU",
      "title": "On Fairness of Unified Multimodal Large Language Model for Image Generation",
      "authors": [
        "Ming Liu",
        "Hao Chen",
        "Jindong Wang",
        "Liwen Wang",
        "Bhiksha Raj",
        "Wensheng Zhang"
      ],
      "abstract": "Unified multimodal large language models (U-MLLMs) have demonstrated impressive performance in end-to-end visual understanding and generation tasks. However, compared to generation-only systems (e.g., Stable Diffusion), the unified architecture of U-MLLMs introduces new risks of propagating demographic stereotypes. In this paper, we benchmark several state-of-the-art U-MLLMs and show that they exhibit significant gender and race biases in the generated outputs. To diagnose the source of these biases, we propose a locate-then-fix framework: we first audit the vision and language components — using techniques such as linear probing and controlled generation — and find that the language model appears to be a primary origin of the observed generative bias. Moreover, we observe a ``partial alignment'' phenomenon, where the U-MLLMs exhibit less bias in understanding tasks yet produce substantially biased images. To address this, we introduce a novel \\emph{balanced preference loss} that enforces uniform generation probabilities across demographics by leveraging a synthetically balanced dataset. Extensive experiments show that our approach significantly reduces demographic bias while preserving semantic fidelity and image quality. Our findings underscore the need for targeted debiasing strategies in unified multimodal systems and introduce a practical approach to mitigate biases.",
      "arxiv_url": "https://openreview.net/forum?id=4xGJZkdjCU",
      "pdf_url": "https://openreview.net/pdf/2e1e45f9bc9351adec57bfefb699b62193bfb715.pdf",
      "primary_category": "Fairness, Unified Multimodal Large Language Model, Bias",
      "categories": [
        "Fairness",
        "Unified Multimodal Large Language Model",
        "Bias"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6rpy7X1Of8",
      "title": "Delving into Large Language Models for Effective Time-Series Anomaly Detection",
      "authors": [
        "Junwoo Park",
        "Kyudan Jung",
        "Dohyun Lee",
        "Hyuck Lee",
        "Daehoon Gwak",
        "ChaeHun Park",
        "Jaegul Choo",
        "Jaewoong Cho"
      ],
      "abstract": "Recent efforts to apply Large Language Models (LLMs) to time-series anomaly detection (TSAD) have yielded limited success, often performing worse than even simple methods. While prior work has focused solely on downstream performance evaluation, the fundamental question—why do LLMs struggle with TSAD?—has remained largely unexplored. In this paper, we present an in-depth analysis that identifies two core challenges in understanding complex temporal dynamics and accurately localizing anomalies. To address these challenges, we propose a simple yet effective method that combines statistical decomposition with index-aware prompting. Our method outperforms 21 existing prompting strategies on the AnomLLM benchmark, achieving up to a 66.6\\% improvement in F1 score. We further compare LLMs with 16 non-LLM baselines on the TSB-AD benchmark, highlighting scenarios where LLMs offer unique advantages via contextual reasoning. Our findings provide empirical insights into how and when LLMs can be effective for TSAD. The code is publicly available at: https://github.com/junwoopark92/LLM-TSAD",
      "arxiv_url": "https://openreview.net/forum?id=6rpy7X1Of8",
      "pdf_url": "https://openreview.net/pdf/ff3e7df135cd4038ebbe31199752645c9946fa1e.pdf",
      "primary_category": "large language model, time series, anomaly detection",
      "categories": [
        "large language model",
        "time series",
        "anomaly detection"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Em9QmNobh0",
      "title": "Principled Fine-tuning of LLMs from User-Edits: A Medley of Preference, Supervision, and Reward",
      "authors": [
        "Dipendra Misra",
        "Aldo Pacchiano",
        "Ta-Chung Chi",
        "Ge Gao"
      ],
      "abstract": "We study how to fine-tune LLMs using user-edit deployment data consisting of a set of context, an agent's response, and user edits. This deployment data is naturally generated by users in applications such as LLMs-based writing assistants and coding agents. The _natural_ origin of user edits makes it a desired source for adapting and personalizing of LLMs. In this setup, there emerges a unification of various feedback types namely preferences, supervised labels, and cost that are typically studied separately in the literature. In this paper, we initiate the theoretical investigation of learning from user edits. We first derive bounds for learning algorithms that learn from each of these feedback types. We prove that these algorithms have different trade-offs depending upon the user, data distribution, and model class. We then propose a simple ensembling procedure to jointly learn from these feedback types. On two domains from Gao et al. 2024, we show our ensembling procedure outperforms these methods that learn from individual feedback. Further, we show that our proposed procedure can robustly adapt to different user-edit distributions at test time.",
      "arxiv_url": "https://openreview.net/forum?id=Em9QmNobh0",
      "pdf_url": "https://openreview.net/pdf/f101a5eb384d12a04b67c248dfc2b470b280a226.pdf",
      "primary_category": "LLM post-training; Learning from interaction; user-edits; reinforcement learning; preference learning",
      "categories": [
        "LLM post-training; Learning from interaction; user-edits; reinforcement learning; preference learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "c8AjdgdHnD",
      "title": "DISCO: Disentangled Communication Steering for Large Language Models",
      "authors": [
        "Max Torop",
        "Aria Masoomi",
        "Masih Eskandar",
        "Jennifer Dy"
      ],
      "abstract": "A variety of recent methods guide large language model outputs via the inference-time addition of *steering vectors* to residual-stream or attention-head representations. In contrast, we propose to inject steering vectors directly into the query and value representation spaces within attention heads. We provide evidence that a greater portion of these spaces exhibit high linear discriminability of concepts --a key property motivating the use of steering vectors-- than attention head outputs. We analytically characterize the effect of our method, which we term *DISentangled COmmunication (DISCO) Steering*, on attention head outputs. Our analysis reveals that DISCO disentangles a strong but underutilized baseline, steering attention head inputs, which implicitly modifies queries and values in a rigid manner. In contrast, DISCO's direct modulation of these components enables more granular control. We find that DISCO achieves superior performance over a number of steering vector baselines across multiple datasets on LLaMA 3.1 8B and Gemma 2 9B, with steering efficacy scoring up to $19.1$% higher than the runner-up. Our results support the conclusion that the query and value spaces are powerful building blocks for steering vector methods. Our code is publicly available at https://github.com/MaxTorop/DISCO.",
      "arxiv_url": "https://openreview.net/forum?id=c8AjdgdHnD",
      "pdf_url": "https://openreview.net/pdf/8d5e7c53d7283ed701bc1211e32b34b49df4e651.pdf",
      "primary_category": "Steering Vectors, Representation Engineering, LLMs",
      "categories": [
        "Steering Vectors",
        "Representation Engineering",
        "LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "PymOnHw4Ty",
      "title": "TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster",
      "authors": [
        "Kanghui Ning",
        "Zijie Pan",
        "Yu Liu",
        "Yushan Jiang",
        "James Y. Zhang",
        "Kashif Rasul",
        "Anderson Schneider",
        "Lintao Ma",
        "Yuriy Nevmyvaka",
        "Dongjin Song"
      ],
      "abstract": "Large Language Models (LLMs) and Foundation Models (FMs) have recently become prevalent for time series forecasting tasks. While fine-tuning LLMs enables domain adaptation, they often struggle to generalize across diverse and unseen datasets. Moreover, existing Time Series Foundation Models (TSFMs) still face challenges in handling non-stationary dynamics and distribution shifts, largely due to the lack of effective mechanisms for adaptation. To this end, we present TS-RAG, a retrieval-augmented generation framework for time series forecasting that enhances the generalization and interpretability of TSFMs. Specifically, TS-RAG leverages pre-trained time series encoders to retrieve semantically relevant segments from a dedicated knowledge base, enriching the contextual representation of the input query. Furthermore, we propose an Adaptive Retrieval Mixer (ARM) module that dynamically fuses the retrieved patterns with the TSFM's internal representation, improving forecasting accuracy without requiring task-specific fine-tuning. Thorough empirical studies on seven public benchmark datasets demonstrate that TS-RAG achieves state-of-the-art zero-shot forecasting performance, outperforming the existing TSFMs by up to 6.84\\% across diverse domains while also providing desirable interpretability. Our code and data are available at: https://github.com/UConn-DSIS/TS-RAG.",
      "arxiv_url": "https://openreview.net/forum?id=PymOnHw4Ty",
      "pdf_url": "https://openreview.net/pdf/e686d5b872f3c42cd96442359b22e23319fe0acb.pdf",
      "primary_category": "Time Series Forecasting; Foundation Model; Retrieval-Augmented Generation",
      "categories": [
        "Time Series Forecasting; Foundation Model; Retrieval-Augmented Generation"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "LCZmI3iM8X",
      "title": "VPO: Reasoning Preferences Optimization Based on $\\mathcal{V}$-Usable Information",
      "authors": [
        "Zecheng Wang",
        "Chunshan Li",
        "Yupeng Zhang",
        "Han Liu",
        "Bingning Wang",
        "Dianhui Chu",
        "Dianbo Sui"
      ],
      "abstract": "Direct Preference Optimization (DPO) is a widely used preference optimization algorithm in large language model (LLM) alignment, which reparameterizes the reward function in reinforcement learning with human feedback (RLHF) without requiring a separate reward model. However, during the DPO training process, when a large negative gradient is applied to low-confidence samples, LLMs with a softmax output head tend to squeeze the confidence in the model's output distribution towards the highest-confidence sentence, which may lead to a decrease in the confidence of both preference and non-preference samples, while increasing the confidence of unrelated tokens.\nThis phenomenon becomes more complex in reasoning tasks. \nIn this work, focusing on reasoning tasks, we propose VPO, a negative gradient constraint method for human non-preference samples based on $\\mathcal{V}$-usable information.\nBy using $\\mathcal{V}$-usable information to measure the similarity between preference pairs and selectively constrain the negative gradient, VPO can alleviate the squeezing effect of DPO, enhance alignment with the generation objective, and maintain the model's ability to distinguish between preference and non-preference samples.\nWe compare VPO with DPO and its latest variants on mathematical reasoning tasks using the LLama 3.1 and Qwen 2.5 series, including both Base and Instruct models.\nOur results demonstrate that VPO consistently and significantly outperforms existing methods. Specifically, on Qwen2.5-7B-Base, VPO achieves 7.80\\% and 13.25\\% improvement over DPO on MATH500 and AMC23, respectively. We also conduct ablation experiments and in-depth analysis on VPO to explain its effectiveness and rationale.",
      "arxiv_url": "https://openreview.net/forum?id=LCZmI3iM8X",
      "pdf_url": "https://openreview.net/pdf/c26a0fba7dddfc0c75fd3f97715e78d8e0452b08.pdf",
      "primary_category": "Preference Optimization, Reasoning, V-Usable Information",
      "categories": [
        "Preference Optimization",
        "Reasoning",
        "V-Usable Information"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "FsgwcrJWp8",
      "title": "VisualLens: Personalization through Task-Agnostic Visual History",
      "authors": [
        "Wang Bill Zhu",
        "Deqing Fu",
        "Kai Sun",
        "Yi Lu",
        "Zhaojiang Lin",
        "Seungwhan Moon",
        "Kanika Narang",
        "MUSTAFA CANIM",
        "Yue Liu",
        "Anuj Kumar",
        "Xin Luna Dong"
      ],
      "abstract": "Existing recommendation systems either rely on user interaction logs, such as online shopping history for shopping recommendations, or focus on text signals. \nHowever, item-based histories are not always accessible and generalizable for multimodal recommendation.\nWe hypothesize that a user's visual history --- comprising images from daily life --- can offer rich, task-agnostic insights into their interests and preferences, and thus be leveraged for effective personalization.\nTo this end, we propose VisualLens, a novel framework that leverages multimodal large language models (MLLMs) to enable personalization using task-agnostic visual history.\nVisualLens extracts, filters, and refines a spectrum user profile from the visual history to support personalized recommendation.\nWe created two new benchmarks, Google-Review-V and Yelp-V, with task-agnostic visual histories, and show that VisualLens improves over state-of-the-art item-based multimodal recommendations by 5-10\\% on Hit@3, and outperforms GPT-4o by 2-5\\%.\nFurther analysis shows that VisualLens is robust across varying history lengths and excels at adapting to both longer histories and unseen content categories.",
      "arxiv_url": "https://openreview.net/forum?id=FsgwcrJWp8",
      "pdf_url": "https://openreview.net/pdf/ad171e589f43f577abd311e76ff8e4a1aab4a259.pdf",
      "primary_category": "Recommendation, Personalization, LLM",
      "categories": [
        "Recommendation",
        "Personalization",
        "LLM",
        "Multimodal LLM"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "uRJ8WAJxHC",
      "title": "Leveraging semantic similarity for experimentation with AI-generated treatments",
      "authors": [
        "Lei Shi",
        "David Arbour",
        "Raghavendra Addanki",
        "Ritwik Sinha",
        "Avi Feller"
      ],
      "abstract": "Large Language Models (LLMs) enable a new form of digital experimentation where treatments combine human and model-generated content in increasingly sophisticated ways. The main methodological challenge in this setting is representing these high-dimensional treatments without losing their semantic meaning or rendering analysis intractable. Here we address this problem by focusing on learning low-dimensional representations that capture the underlying structure of such treatments. These representations enable downstream applications such as guiding generative models to produce meaningful treatment variants and facilitating adaptive assignment in online experiments. We propose double kernel representation learning, which models the causal effect through the inner product of kernel-based representations of treatments and user covariates. We develop an alternating-minimization algorithm that learns these representations efficiently from data and provide convergence guarantees under a low-rank factor model. As an application of this framework, we introduce an adaptive design strategy for online experimentation and demonstrate the method's effectiveness through numerical experiments.",
      "arxiv_url": "https://openreview.net/forum?id=uRJ8WAJxHC",
      "pdf_url": "https://openreview.net/pdf/5fe3d96fccd4c9305fb6a128291b96c832a7fc2e.pdf",
      "primary_category": "Experimentation, Kernel methods, Causal inference",
      "categories": [
        "Experimentation",
        "Kernel methods",
        "Causal inference",
        "Embeddings",
        "LLM-generated treatments"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qAggjeV2JO",
      "title": "InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion",
      "authors": [
        "Yuanyi Wang",
        "Zhaoyi Yan",
        "Yiming Zhang",
        "Qi Zhou",
        "Yanggan Gu",
        "Fei Wu",
        "Hongxia Yang"
      ],
      "abstract": "Recent advances in large language models (LLMs) have intensified efforts to fuse heterogeneous open-source models into a unified system that inherits their complementary strengths.  Existing logit-based fusion methods maintain inference efficiency but treat vocabulary dimensions independently, overlooking semantic dependencies encoded by cross-dimension interactions. These dependencies reflect how token types interact under a model's internal reasoning and are essential for aligning models with diverse generation behaviors. To explicitly model these dependencies, we propose \\textbf{InfiGFusion}, the first structure-aware fusion framework with a novel \\textit{Graph-on-Logits Distillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output and aggregate their outer products across sequence positions to form a global co-activation graph, where nodes represent vocabulary channels and edges quantify their joint activations. To ensure scalability and efficiency, we design a sorting-based closed-form approximation that reduces the original $O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \\log n)$, with provable approximation guarantees. Experiments across multiple fusion settings show that GLD consistently improves fusion quality and stability. InfiGFusion outperforms SOTA models and fusion baselines across 11 benchmarks spanning reasoning, coding, and mathematics. It shows particular strength in complex reasoning tasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal Judgement over SFT, demonstrating superior multi-step and relational inference.",
      "arxiv_url": "https://openreview.net/forum?id=qAggjeV2JO",
      "pdf_url": "https://openreview.net/pdf/65fd4a93592696a974250bd47782e76d61f38e08.pdf",
      "primary_category": "Model Fusion, LLM",
      "categories": [
        "Model Fusion",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "JsNUE84Hxi",
      "title": "Self-Adapting Language Models",
      "authors": [
        "Adam Zweiger",
        "Jyothish Pari",
        "Han Guo",
        "Yoon Kim",
        "Pulkit Agrawal"
      ],
      "abstract": "Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce $\\textbf{Se}$lf-$\\textbf{A}$dapting $\\textbf{L}$LMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a $\\textit{self-edit}$ --- a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop, using the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's generation to parameterize and control its own adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation in response to new data. Our website and code is available at https://jyopari.github.io/posts/seal.",
      "arxiv_url": "https://openreview.net/forum?id=JsNUE84Hxi",
      "pdf_url": "https://openreview.net/pdf/d876d7104cce0a8cf2d822ec4ac887af6ef71678.pdf",
      "primary_category": "llms, ml, adaptation",
      "categories": [
        "llms",
        "ml",
        "adaptation",
        "meta learning",
        "synthetic data",
        "finetuning",
        "continual learning",
        "reinforcement learning",
        "knowledge editing"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "AzOUIMzDxC",
      "title": "Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations",
      "authors": [
        "Faisal Hamman",
        "Pasan Dissanayake",
        "Yanjun Fu",
        "Sanghamitra Dutta"
      ],
      "abstract": "Knowledge distillation is a promising approach to transfer capabilities from complex teacher models to smaller, resource-efficient student models that can be deployed easily, particularly in task-aware scenarios. However, existing methods of task-aware distillation typically require substantial quantities of data which may be unavailable or expensive to obtain in many practical scenarios. In this paper, we address this challenge by introducing a novel strategy called **Co**unterfactual-explanation-infused **D**istillation CoD for *few-shot task-aware knowledge distillation by systematically infusing counterfactual explanations*. Counterfactual explanations (CFEs) refer to inputs that can flip the output prediction of the teacher model with minimum perturbation. Our strategy CoD leverages these CFEs to precisely map the teacher's decision boundary with significantly fewer samples. We provide theoretical guarantees for motivating the role of CFEs in distillation, from both statistical and geometric perspectives. We mathematically show that CFEs can improve parameter estimation by providing more informative examples near the teacher’s decision boundary. We also derive geometric insights on how CFEs effectively act as knowledge probes, helping the students mimic the teacher's decision boundaries more effectively than standard data. We perform experiments across various datasets and LLMs to show that CoD outperforms standard distillation approaches in few-shot regimes (as low as 8 - 512 samples). Notably, CoD only uses half of the original samples used by the baselines, paired with their corresponding CFEs and still improves performance.",
      "arxiv_url": "https://openreview.net/forum?id=AzOUIMzDxC",
      "pdf_url": "https://openreview.net/pdf/2526677cc0da2c9b02bbf474fbd6f234617e35e2.pdf",
      "primary_category": "Knowledge Distillation, Counterfactual Explanations, Few-shot Learning",
      "categories": [
        "Knowledge Distillation",
        "Counterfactual Explanations",
        "Few-shot Learning",
        "Data Efficiency"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "JbvSQm5h1l",
      "title": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning",
      "authors": [
        "Chongyu Fan",
        "Jiancheng Liu",
        "Licong Lin",
        "Jinghan Jia",
        "Ruiqi Zhang",
        "Song Mei",
        "Sijia Liu"
      ],
      "abstract": "This work studies the problem of large language model (LLM) unlearning, aiming to remove unwanted data influences (e.g., copyrighted or harmful content) while preserving model utility. Despite the increasing demand for unlearning, a technically-grounded optimization framework is lacking. Gradient ascent (GA)-type methods, though widely used, are suboptimal as they reverse the learning process without controlling optimization divergence (i.e., deviation from the pre-trained state), leading to risks of model collapse. Negative preference optimization (NPO) has been proposed to address this issue and is considered one of the state-of-the-art LLM unlearning approaches. In this work, we revisit NPO and identify another critical issue: reference model bias. This bias arises from using the reference model (i.e., the model prior to unlearning) to assess unlearning success, which can lead to a misleading impression of the true data-wise unlearning effectiveness. Specifically, it could cause (a) uneven allocation of optimization power across forget data with varying difficulty levels, and (b) ineffective gradient weight smoothing during the early stages of unlearning optimization. To overcome these challenges, we propose a simple yet effective unlearning optimization framework, called SimNPO, showing that simplicity—removing the reliance on a reference model (through the lens of simple preference optimization)—benefits unlearning. We provide deeper insights into SimNPO's advantages, including an analysis based on mixtures of Markov chains. Extensive experiments further validate its efficacy on benchmarks like TOFU, MUSE, and WMDP.",
      "arxiv_url": "https://openreview.net/forum?id=JbvSQm5h1l",
      "pdf_url": "https://openreview.net/pdf/7a525a707a4263f01b67f7dc480276aeb0e2d0b2.pdf",
      "primary_category": "Large Language Model, Machine Unlearning, Preference Optimization",
      "categories": [
        "Large Language Model",
        "Machine Unlearning",
        "Preference Optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "di43GKTaPz",
      "title": "Technical Debt in In-Context Learning: Diminishing Efficiency in Long Context",
      "authors": [
        "Taejong Joo",
        "Diego Klabjan"
      ],
      "abstract": "Transformers have demonstrated remarkable in-context learning (ICL) capabilities, adapting to new tasks by simply conditioning on demonstrations without parameter updates. Compelling empirical and theoretical evidence suggests that ICL, as a general-purpose learner, could outperform task-specific models. However, it remains unclear to what extent the transformers optimally learn in-context compared to principled learning algorithms. To investigate this, we employ a meta ICL framework in which each prompt defines a distinctive regression task whose target function is drawn from a hierarchical distribution, requiring inference over both the latent model class and task-specific parameters. Within this setup, we benchmark sample complexity of ICL against principled learning algorithms, including the Bayes optimal estimator, under varying performance requirements. Our findings reveal a striking dichotomy: while ICL initially matches the efficiency of a Bayes optimal estimator, its efficiency significantly deteriorates in long context. Through an information-theoretic analysis, we show that the diminishing efficiency is inherent to ICL. These results clarify the trade-offs in adopting ICL as a universal problem solver, motivating a new generation of on-the-fly adaptive methods without the diminishing efficiency.",
      "arxiv_url": "https://openreview.net/forum?id=di43GKTaPz",
      "pdf_url": "https://openreview.net/pdf/7343c9292004532eb619750ce9e2d7199195b4a5.pdf",
      "primary_category": "In-Context Learning, Transformer, Large-Language Model",
      "categories": [
        "In-Context Learning",
        "Transformer",
        "Large-Language Model",
        "Long Context",
        "Many-Shot ICL",
        "Scaling",
        "Meta Learning"
      ],
      "tags": [
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "bFXbLQzRoZ",
      "title": "Power Lines: Scaling laws for weight decay and batch size in LLM pre-training",
      "authors": [
        "Shane Bergsma",
        "Nolan Simran Dey",
        "Gurpreet Gosal",
        "Gavia Gray",
        "Daria Soboleva",
        "Joel Hestness"
      ],
      "abstract": "Efficient LLM pre-training requires well-tuned hyperparameters (HPs), including learning rate η and weight decay λ. We study scaling laws for HPs: formulas for how to scale HPs as we scale model size N, dataset size D, and batch size B.  Recent work suggests the AdamW timescale, τ = B/(ηλD), should remain constant across training settings, and we verify the implication that optimal λ scales linearly with B, for a fixed N and D. However, as N and D scale, we show optimal τ obeys a precise power law in the tokens-per-parameter ratio, D/N. This law thus provides a method to accurately predict λopt in advance of large-scale training.  We also study scaling laws for optimal batch size Bopt (the B enabling lowest loss at a given N,D) and critical batch size Bcrit (the B beyond which further data parallelism becomes ineffective). In contrast to prior work, we find both Bopt and Bcrit scale as power laws in D, independent of model size, N. Finally, we analyze how these findings inform the real-world selection of Pareto-optimal N and D under dual training time and compute objectives.",
      "arxiv_url": "https://openreview.net/forum?id=bFXbLQzRoZ",
      "pdf_url": "https://openreview.net/pdf/f26602e9a24b6bf8d4e6a58a3b6aefecc2a58271.pdf",
      "primary_category": "scaling laws for hyperparameters, weight decay, batch sizing",
      "categories": [
        "scaling laws for hyperparameters",
        "weight decay",
        "batch sizing",
        "critical batch size",
        "data parallelism",
        "large language models (LLMs)",
        "pre-training",
        "AdamW optimizer",
        "compute-optimal training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "soMxYQaMnF",
      "title": "Generator-Mediated Bandits: Thompson Sampling for GenAI-Powered Adaptive Interventions",
      "authors": [
        "Marc Brooks",
        "Gabriel Durham",
        "Kihyuk Hong",
        "Ambuj Tewari"
      ],
      "abstract": "Recent advances in generative artificial intelligence (GenAI) models have enabled the generation of personalized content that adapts to up-to-date user context. While personalized decision systems are often modeled using bandit formulations, the integration of GenAI introduces new structure into otherwise classical sequential learning problems. In GenAI-powered interventions, the agent selects a query, but the environment experiences a stochastic response drawn from the generative model. Standard bandit methods do not explicitly account for this structure, where actions influence rewards only through stochastic, observed treatments. We introduce generator-mediated bandit-Thompson sampling (GAMBITTS), a bandit approach designed for this action/treatment split, using mobile health interventions with large language model-generated text as a motivating case study. GAMBITTS explicitly models both the treatment and reward generation processes, using information in the delivered treatment to accelerate policy learning relative to standard methods. We establish regret bounds for GAMBITTS by decomposing sources of uncertainty in treatment and reward, identifying conditions where it achieves stronger guarantees than standard bandit approaches. In simulation studies, GAMBITTS consistently outperforms conventional algorithms by leveraging observed treatments to more accurately estimate expected rewards.",
      "arxiv_url": "https://openreview.net/forum?id=soMxYQaMnF",
      "pdf_url": "https://openreview.net/pdf/605cecef3bb3851fba205173d27d22999ea7f55b.pdf",
      "primary_category": "Bandits, Generative Models, Large Language Models",
      "categories": [
        "Bandits",
        "Generative Models",
        "Large Language Models",
        "Mobile Health",
        "Personalized Interventions",
        "Sequential Decision Making"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Wxh5Xz7NpJ",
      "title": "Remarkable Robustness of LLMs: Stages of Inference?",
      "authors": [
        "Vedang Lad",
        "Jin Hwa Lee",
        "Wes Gurnee",
        "Max Tegmark"
      ],
      "abstract": "We investigate the robustness of Large Language Models (LLMs) to structural interventions by deleting and swapping adjacent layers during inference. Surprisingly, models retain 72–95\\% of their original top-1 prediction accuracy without any fine-tuning. We find that performance degradation is not uniform across layers: interventions to the early and final layers cause the most degradation, while the model is remarkably robust to dropping middle layers. This pattern of localized sensitivity motivates our hypothesis of four stages of inference, observed across diverse model families and sizes: (1) detokenization, where local context is integrated to lift raw token embeddings into higher-level representations; (2) feature engineering, where task- and entity-specific features are iteratively refined; (3) prediction ensembling, where hidden states are aggregated into plausible next-token predictions; and (4) residual calibration, where irrelevant features are suppressed to finalize the output distribution. Synthesizing behavioral and mechanistic evidence, we provide a hypothesis for interpreting depth-dependent computations in LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=Wxh5Xz7NpJ",
      "pdf_url": "https://openreview.net/pdf/7cac757c6fe3caa0014affc1447da1391c9e964a.pdf",
      "primary_category": "Mechanistic Interpretability, Deep Learning, Large Language Models",
      "categories": [
        "Mechanistic Interpretability",
        "Deep Learning",
        "Large Language Models",
        "Transformer Robustness",
        "Layer Interventions",
        "Inference Stages",
        "Neural Network Analysis",
        "Attention Mechanisms",
        "Model Architecture"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "UTGjik64IK",
      "title": "Distillation Robustifies Unlearning",
      "authors": [
        "Bruce W. Lee",
        "Addie Foote",
        "Alex Infanger",
        "Leni Shor",
        "Harish K Kamath",
        "Jacob Goldman-Wetzler",
        "Bryce Woodworth",
        "Alex Cloud",
        "Alexander Matt Turner"
      ],
      "abstract": "Current LLM unlearning methods are not robust. A few steps of finetuning can revert their effects. We begin by showing that this is true even for an idealized form of unlearning: training to imitate a model that was never trained on unwanted information. This shows that training a model can drastically modify its input-output behavior while leaving its underlying capabilities intact. In light of this dynamic, we show our main result. Training a randomly initialized student on the outputs of an unlearned model transfers behaviors while leaving latent capabilities behind. In short, distillation robustifies unlearning. Based on this result, we propose Unlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an unlearned model into a noised copy of itself. UNDO introduces a tunable tradeoff between compute cost and robustness, establishing a new Pareto frontier on synthetic language and arithmetic tasks. At its strongest setting, UNDO matches the robustness of a model retrained from scratch with perfect data filtering while using only 60-80% of the compute and requiring only 0.01% of the pretraining data to be labeled. We also show that UNDO robustifies unlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP) benchmark. Since distillation is widely used in practice, incorporating an unlearning step beforehand offers a convenient path to robust capability removal.",
      "arxiv_url": "https://openreview.net/forum?id=UTGjik64IK",
      "pdf_url": "https://openreview.net/pdf/cb7658f5eeca3b148cae1c2a5ba3efe4288ae6d5.pdf",
      "primary_category": "machine unlearning, robust unlearning, relearning attacks",
      "categories": [
        "machine unlearning",
        "robust unlearning",
        "relearning attacks",
        "AI safety",
        "alignment",
        "distillation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "mDjEKAwJOF",
      "title": "BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model",
      "authors": [
        "Adibvafa Fallahpour",
        "Andrew Magnuson",
        "Purav Gupta",
        "Shihao Ma",
        "Jack Naimer",
        "Arnav Shah",
        "Haonan Duan",
        "Omar Ibrahim",
        "Hani Goodarzi",
        "Chris J. Maddison",
        "BO WANG"
      ],
      "abstract": "Unlocking deep and interpretable biological reasoning from complex genomic data remains a major AI challenge limiting scientific progress. While current DNA foundation models excel at representing sequences, they struggle with multi-step reasoning and lack transparent, biologically meaningful explanations. BioReason addresses this by tightly integrating a DNA foundation model with a large language model (LLM), enabling the LLM to directly interpret and reason over genomic information. Through supervised fine-tuning and reinforcement learning, BioReason learns to produce logical, biologically coherent deductions. It achieves major performance gains, boosting KEGG-based disease pathway prediction accuracy from 86% to 98% and improving variant effect prediction by an average of 15% over strong baselines. BioReason can reason over unseen biological entities and explain its decisions step by step, offering a transformative framework for interpretable, mechanistic AI in biology. All data, code, and checkpoints are available at [https://github.com/bowang-lab/BioReason](https://github.com/bowang-lab/BioReason).",
      "arxiv_url": "https://openreview.net/forum?id=mDjEKAwJOF",
      "pdf_url": "https://openreview.net/pdf/b440ce6c896fafc49a94e23b3af12c4501c8b7b4.pdf",
      "primary_category": "Biological Reasoning, DNA Foundation Models, Large Language Models (LLMs)",
      "categories": [
        "Biological Reasoning",
        "DNA Foundation Models",
        "Large Language Models (LLMs)",
        "Reinforcement Learning",
        "Multimodal"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "FxCy8TvQHO",
      "title": "SimWorld: An Open-ended Simulator for Agents in Physical and Social Worlds",
      "authors": [
        "Xiaokang Ye",
        "Jiawei Ren",
        "Yan Zhuang",
        "Xuhong He",
        "Yiming Liang",
        "Yiqing Yang",
        "Mrinaal Dogra",
        "Xianrui Zhong",
        "Eric Liu",
        "Kevin Benavente",
        "Rajiv Mandya Nagaraju",
        "Dhruv Vivek Sharma",
        "Ziqiao Ma",
        "Tianmin Shu",
        "Zhiting Hu",
        "Lianhui Qin"
      ],
      "abstract": "While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (e.g., by autonomously earning income) requires massive-scale interaction, reasoning, training, and evaluation across diverse scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) rich interface for LLM/VLM agents, with multi-modal world inputs/feedback and open-vocabulary action outputs at varying levels of abstraction; and (3) diverse physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., Gemini-2.5-Flash, Claude-3.5, GPT-4o, and DeepSeek-Prover-V2) on both short-horizon navigation tasks requiring grounded re-planning, and long-horizon multi-agent food delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines. Please refer to the project website for the most up-to-date information: http://simworld.org/.",
      "arxiv_url": "https://openreview.net/forum?id=FxCy8TvQHO",
      "pdf_url": "https://openreview.net/pdf/0bb7c8713ed55889f0a4fa71ae0e800db175e70e.pdf",
      "primary_category": "Multi-Agent Simulator, Real World Agent, Social Simulation",
      "categories": [
        "Multi-Agent Simulator",
        "Real World Agent",
        "Social Simulation",
        "Physical Simulation"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "UG1723eoKq",
      "title": "SPOT-Trip: Dual-Preference Driven Out-of-Town Trip Recommendation",
      "authors": [
        "Yinghui Liu",
        "Hao Miao",
        "Guojiang Shen",
        "Yan Zhao",
        "Xiangjie Kong",
        "Ivan Lee"
      ],
      "abstract": "Out-of-town trip recommendation aims to generate a sequence of Points of Interest (POIs) for users traveling from their hometowns to previously unvisited regions based on personalized itineraries, e.g., origin, destination, and trip duration. Modeling the complex user preferences--which often exhibit a two-fold nature of static and dynamic interests--is critical for effective recommendations. However, the sparsity of out-of-town check-in data presents significant challenges in capturing such user preferences. Meanwhile, existing methods often conflate the static and dynamic preferences, resulting in suboptimal performance. In this paper, we for the first time systematically study the problem of out-of-town trip recommendation. A novel framework SPOT-Trip is proposed to explicitly learns the dual static-dynamic user preferences. Specifically, to handle scarce data, we construct a POI attribute knowledge graph to enrich the semantic modeling of users’ hometown and out-of-town check-ins, enabling the static preference modeling through attribute relation-aware aggregation. Then, we employ neural ordinary differential equations (ODEs) to capture the continuous evolution of latent dynamic user preferences and innovatively combine a temporal point process to describe the instantaneous probability of each preference behavior. Further, a static-dynamic fusion module is proposed to merge the learned static and dynamic user preferences. Extensive experiments on real data offer insight into the effectiveness of the proposed solutions, showing that SPOT-Trip achieves performance improvement by up to 17.01%.",
      "arxiv_url": "https://openreview.net/forum?id=UG1723eoKq",
      "pdf_url": "https://openreview.net/pdf/67a93e49d62a3a995c383b911ead55878d8dfcf6.pdf",
      "primary_category": "Out-of-town trip recommendation; Static-dynamic user preference; Ordinary differential equation",
      "categories": [
        "Out-of-town trip recommendation; Static-dynamic user preference; Ordinary differential equation"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1LPPMAUlaT",
      "title": "Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection",
      "authors": [
        "Michelle Yuan",
        "Khushbu Pahwa",
        "Shuaichen Chang",
        "Mustafa Devrim Kaba",
        "Jiarong Jiang",
        "Xiaofei Ma",
        "Yi Zhang",
        "MONICA SUNKARA"
      ],
      "abstract": "Designing effective agentic systems requires the seamless composition and integration of agents, tools, and models within dynamic and uncertain environments. Most existing methods rely on static, semantic retrieval approaches for tool or agent discovery. However, effective reuse and composition of existing components remain challenging due to incomplete capability descriptions and the limitations of retrieval methods. \nComponent selection suffers because the decisions are not based on capability, cost, and real-time utility.\nTo address these challenges, we introduce a structured, automated framework for agentic system composition that is inspired by the knapsack problem. Our framework enables a composer agent to systematically identify, select, and assemble an optimal set of agentic components by jointly considering performance, budget constraints, and compatibility. By dynamically testing candidate components and modeling their utility in real-time, our approach streamlines the assembly of agentic systems and facilitates scalable reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five benchmarking datasets shows that our online-knapsack-based composer consistently lies on the Pareto frontier, achieving higher success rates at significantly lower component costs compared to our baselines. \nIn the single-agent setup, the online knapsack composer shows a success rate improvement of up to 31.6\\% in comparison to the retrieval baselines. \nIn multi-agent systems, the online knapsack composer increases success rate from 37\\% to 87\\% when agents are selected from an agent inventory of 100+ agents. \nThe substantial performance gap confirms the robust adaptability of our method across diverse domains and budget constraints.",
      "arxiv_url": "https://openreview.net/forum?id=1LPPMAUlaT",
      "pdf_url": "https://openreview.net/pdf/a4f3d74114be589c394001fd9bf3433a90707bb6.pdf",
      "primary_category": "large language models, llm agents, combinatorial optimization",
      "categories": [
        "large language models",
        "llm agents",
        "combinatorial optimization",
        "knapsack algorithm",
        "agent composition"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "naAUSeyoZ7",
      "title": "Knowing When to Stop: Efficient Context Processing via Latent Sufficiency Signals",
      "authors": [
        "Roy Xie",
        "Junlin Wang",
        "Paul Rosu",
        "Chunyuan Deng",
        "Bolun Sun",
        "Zihao Lin",
        "Bhuwan Dhingra"
      ],
      "abstract": "Large language models (LLMs) process entire input contexts indiscriminately, which is inefficient when the information required to answer a query is localized within the context. We present dynamic context cutoff, a novel method enabling LLMs to self-terminate processing upon acquiring sufficient task-relevant information. Through analysis of model internals, we discover that specific attention heads inherently encode \"sufficiency signals\" -- detectable through lightweight classifiers -- that predict when critical information has been processed. This reveals a new efficiency paradigm: models' internal understanding naturally dictates processing needs rather than external compression heuristics. Comprehensive experiments across six QA datasets (up to 40K tokens) with three model families (LLaMA/Qwen/Mistral, 1B-70B) demonstrate 3.4% accuracy improvement while achieving 1.33x token reduction on average. Furthermore, our method demonstrates superior performance compared to other context efficiency methods at equivalent token reduction rates. Additionally, we observe an emergent scaling phenomenon: while smaller models require probing for sufficiency detection, larger models exhibit intrinsic self-assessment capabilities through prompting.",
      "arxiv_url": "https://openreview.net/forum?id=naAUSeyoZ7",
      "pdf_url": "https://openreview.net/pdf/f30a8b92f61322fb58079e6d38c780dc339348b6.pdf",
      "primary_category": "Large language models, efficiency, context processing",
      "categories": [
        "Large language models",
        "efficiency",
        "context processing",
        "context compression"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "AiZxn84Wdo",
      "title": "Training Language Models to Reason Efficiently",
      "authors": [
        "Daman Arora",
        "Andrea Zanette"
      ],
      "abstract": "Scaling model size and training data has led to great advances in the performance of Large Language Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particularly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of-thoughts, bring unprecedented breakthroughs in  problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the economic feasibility, user experience, and environmental sustainability of these models.\n\nIn this work, we propose to train large reasoning models to reason efficiently. Our method incentivizes models to minimize unnecessary computational overhead while largely maintaining accuracy, thereby achieving substantial deployment efficiency gains. It  enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experiments on two open-weight large reasoning models demonstrate significant reductions in inference cost while preserving most of the accuracy.",
      "arxiv_url": "https://openreview.net/forum?id=AiZxn84Wdo",
      "pdf_url": "https://openreview.net/pdf/c169ad531f568624e1f7af8211b9ff6b12391b63.pdf",
      "primary_category": "Reasoning models, efficiency",
      "categories": [
        "Reasoning models",
        "efficiency"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XFP5jntOdx",
      "title": "SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired Symbolic Reasoning",
      "authors": [
        "Yiting Wang",
        "Wanghao Ye",
        "Ping Guo",
        "Yexiao He",
        "Ziyao Wang",
        "Bowei Tian",
        "Shwai He",
        "Guoheng Sun",
        "Zheyu Shen",
        "Sihan Chen",
        "Ankur Srivastava",
        "Qingfu Zhang",
        "Gang Qu",
        "Ang Li"
      ],
      "abstract": "Optimizing Register Transfer Level (RTL) code is crucial for improving the efficiency and performance of digital circuits in the early stages of synthesis. Manual rewriting, guided by synthesis feedback, can yield high-quality results but is time-consuming and error-prone. Most existing compiler-based approaches have difficulty handling complex design constraints. Large Language Model (LLM)-based methods have emerged as a promising alternative to address these challenges. However, LLM-based approaches often face difficulties in ensuring alignment between the generated code and the provided prompts. This paper introduces SymRTLO, a neuron-symbolic framework that integrates LLMs with symbolic reasoning for the efficient and effective optimization of RTL code. Our method incorporates a retrieval-augmented system of optimization rules and Abstract Syntax Tree (AST)-based templates, enabling LLM-based rewriting that maintains syntactic correctness while minimizing undesired circuit behaviors. A symbolic module is proposed for analyzing and optimizing finite state machine (FSM) logic, allowing fine-grained state merging and partial specification handling beyond the scope of pattern-based compilers. Furthermore, a fast verification pipeline, combining formal equivalence checks with test-driven validation, further reduces the complexity of verification. Experiments on the RTL-Rewriter benchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves power, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%, respectively, compared to the state-of-the-art methods. We will release the code as open source upon the paper's acceptance.",
      "arxiv_url": "https://openreview.net/forum?id=XFP5jntOdx",
      "pdf_url": "https://openreview.net/pdf/d39a4182aca6fdd5e3515ef8bcb0630294bba229.pdf",
      "primary_category": "RTL Optimization, Neuron-Symbolic, Electronic Design Automation",
      "categories": [
        "RTL Optimization",
        "Neuron-Symbolic",
        "Electronic Design Automation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3nza35A6I4",
      "title": "Training Language Models to Generate Quality Code with Program Analysis Feedback",
      "authors": [
        "Feng Yao",
        "Zilong Wang",
        "Liyuan Liu",
        "Junxia Cui",
        "Li Zhong",
        "Xiaohan Fu",
        "Haohui Mai",
        "Vish Krishnan",
        "Jianfeng Gao",
        "Jingbo Shang"
      ],
      "abstract": "Code generation with large language models (LLMs), often termed vibe coding, is increasingly adopted in production but fails to ensure code quality, particularly in security (e.g., SQL injection vulnerabilities) and maintainability (e.g., missing type annotations). Existing methods, such as supervised fine-tuning and rule-based post-processing, rely on labor-intensive annotations or brittle heuristics, limiting their scalability and effectiveness. We propose REAL (Reinforcement rEwards from Automated anaLysis), a reinforcement learning framework that trains LLMs to generate production-quality code using program analysis–guided feedback. Specifically, REAL integrates two automated signals: (1) static analyzers detecting security and maintainability defects and (2) unit tests ensuring functional correctness. Unlike prior work, our framework is prompt-agnostic and reference-free, enabling scalable supervision without manual intervention. Experiments across multiple datasets and model scales demonstrate that REAL outperforms state-of-the-art methods in simultaneous assessments of functionality and code quality. Our work bridges the gap between rapid prototyping and production-ready code, enabling LLMs to deliver both speed and quality.",
      "arxiv_url": "https://openreview.net/forum?id=3nza35A6I4",
      "pdf_url": "https://openreview.net/pdf/0258f740b402f999cc2a57473477db4de2a698c4.pdf",
      "primary_category": "Code Generation, Reinforcement Learning, Program Analysis",
      "categories": [
        "Code Generation",
        "Reinforcement Learning",
        "Program Analysis"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IKxKs3rF9V",
      "title": "4KAgent: Agentic Any Image to 4K Super-Resolution",
      "authors": [
        "Yushen Zuo",
        "Qi Zheng",
        "Mingyang Wu",
        "Xinrui Jiang",
        "Renjie Li",
        "Jian Wang",
        "Yide Zhang",
        "Gengchen Mai",
        "Lihong Wang",
        "James Zou",
        "Xiaoyu Wang",
        "Ming-Hsuan Yang",
        "Zhengzhong Tu"
      ],
      "abstract": "We present 4KAgent, a unified agentic super-resolution generalist system designed to universally upscale any image to 4K resolution (and even higher, if applied iteratively). Our system can transform images from extremely low resolutions with severe degradations, for example, highly distorted inputs at $256\\times 256$, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three core components: (1) Profiling, a module that customizes the 4KAgent pipeline based on bespoke use cases; (2) A Perception Agent, which leverages vision-language models alongside image quality assessment experts to analyze the input image and make a tailored restoration plan; and (3) A Restoration Agent, which executes the plan, following a recursive execution-reflection paradigm, guided by a quality-driven mixture-of-experts policy to select the optimal output for each step. Additionally, 4KAgent embeds a specialized face restoration pipeline, significantly enhancing facial details in portrait and selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task categories encompassing a total of 26 diverse benchmarks, setting new state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover natural images, portrait photos, AI-generated content, satellite imagery, fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and X-ray, demonstrating superior performance in terms of both perceptual (e.g., NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic paradigm for low-level vision tasks, we aim to catalyze broader interest and innovation within vision-centric autonomous agents across diverse research communities. We release all the code, models, and results at: https://4kagent.github.io.",
      "arxiv_url": "https://openreview.net/forum?id=IKxKs3rF9V",
      "pdf_url": "https://openreview.net/pdf/a21eb04c7d32521204cb8314e7f00c8a95cfa6b7.pdf",
      "primary_category": "Super-Resolution, Agent, Image Restoration",
      "categories": [
        "Super-Resolution",
        "Agent",
        "Image Restoration",
        "Low-level Vision"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DrUR87D4Hj",
      "title": "The Fragile Truth of Saliency: Improving LLM Input Attribution via Attention Bias Optimization",
      "authors": [
        "Yihua Zhang",
        "Changsheng Wang",
        "Yiwei Chen",
        "Chongyu Fan",
        "Jinghan Jia",
        "Sijia Liu"
      ],
      "abstract": "Input saliency aims to quantify the influence of input tokens on the output of large language models (LLMs), which has been widely used for prompt engineering, model interpretability, and behavior attribution. Despite the proliferation of saliency techniques, the field lacks a standardized and rigorous evaluation protocol. In this work, we introduce a stress-testing framework inspired by the needle-in-a-haystack (NIAH) setting to systematically assess the reliability of seven popular input saliency methods. Our evaluation reveals a surprising and critical flaw: existing methods consistently assign non-trivial importance to irrelevant context, and this attribution error worsens as input length increases. To address this issue, we propose a novel saliency method based on Attention Bias Optimization (ours), which explicitly optimizes the attention bias associated with each input token to quantify its causal impact on target token generation. ABO robustly outperforms existing methods by 10\\sim30% in saliency accuracy across diverse NIAH tasks, maintains effectiveness up to 10K-token prompts, and enables practical applications including zero-shot detoxification, sentiment steering, and reasoning-error correction. Our findings highlight the limitations of prevalent attribution methods and establish ABO as a principled alternative for accurate token attribution.",
      "arxiv_url": "https://openreview.net/forum?id=DrUR87D4Hj",
      "pdf_url": "https://openreview.net/pdf/92d12e66ca5025ce34d55cafeed1730c7730bc21.pdf",
      "primary_category": "LLM Input Saliency",
      "categories": [
        "LLM Input Saliency"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ESB924uT5Y",
      "title": "Enhancing Training Data Attribution with Representational Optimization",
      "authors": [
        "Weiwei Sun",
        "Haokun Liu",
        "Nikhil Kandpal",
        "Colin Raffel",
        "Yiming Yang"
      ],
      "abstract": "Training data attribution (TDA) methods aim to measure how training data impacts a model's predictions. \nWhile gradient-based attribution methods, such as influence functions, offer theoretical grounding, their computational costs make them impractical for large-scale applications. \nRepresentation-based approaches are far more scalable, but typically rely on heuristic embeddings that are not optimized for attribution, limiting their fidelity.\nTo address these challenges, we propose AirRep, \na scalable, representation-based approach that closes this gap by learning task-specific and model-aligned representations optimized explicitly for TDA.\nAirRep introduces two key innovations: a trainable encoder tuned for attribution quality, and an attention-based pooling mechanism that enables accurate estimation of group-wise influence.\nWe train AirRep using a ranking objective over automatically constructed training subsets labeled by their empirical effect on target predictions.\nExperiments on instruction-tuned LLMs\ndemonstrate that AirRep achieves performance on par with state-of-the-art gradient-based approaches while being nearly two orders of magnitude more efficient at inference time.\nFurther analysis highlights its robustness \nand generalization across tasks and models.\nOur code is available at https://github.com/sunnweiwei/AirRep.",
      "arxiv_url": "https://openreview.net/forum?id=ESB924uT5Y",
      "pdf_url": "https://openreview.net/pdf/749b578e1ce6403cd40bde8eca2c79068f31980a.pdf",
      "primary_category": "Training Data Attribution",
      "categories": [
        "Training Data Attribution"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oCXyxhgCiZ",
      "title": "BrainEC-LLM: Brain Effective Connectivity Estimation by Multiscale Mixing LLM",
      "authors": [
        "Wen Xiong",
        "Junzhong Ji",
        "Jinduo Liu"
      ],
      "abstract": "Pre-trained Large language models (LLMs) have shown impressive advancements in functional magnetic resonance imaging (fMRI) analysis and causal discovery. Considering the unique nature of the causal discovery field, which focuses on extracting causal graphs from observed data, research on LLMs in this field is still at an early exploratory stage. As a subfield of causal discovery, effective connectivity (EC) has received even less attention, and LLM-based approaches in EC remain unexplored. Existing LLM-based approaches for causal discovery typically rely on iterative querying to assess the causal influence between variable pairs, without any model adaptation or fine-tuning, making them ill-suited for handling the cross-modal gap and complex causal structures. To this end, we propose BrainEC-LLM, the first method to fine-tune LLMs for estimating brain EC from fMRI data. Specifically, multiscale decomposition mixing module decomposes fMRI time series data into short-term and long-term multiscale trends, then mixing them in bottom-up (fine to coarse) and top-down (coarse to fine) manner to extract multiscale temporal variations. And cross attention is applied with pre-trained word embeddings to ensure consistency between the fMRI input and pre-trained natural language. The experimental results on simulated and real resting-state fMRI datasets demonstrate that BrainEC-LLM can achieve superior performance when compared to state-of-the-art baselines.",
      "arxiv_url": "https://openreview.net/forum?id=oCXyxhgCiZ",
      "pdf_url": "https://openreview.net/pdf/08c043d294194700e3f70d6b027ee6e69341c6b4.pdf",
      "primary_category": "Brain effective connectivity, Large language models, functional magnetic resonance imaging",
      "categories": [
        "Brain effective connectivity",
        "Large language models",
        "functional magnetic resonance imaging"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tD9mebUimw",
      "title": "Truthful Aggregation of LLMs with an Application to Online Advertising",
      "authors": [
        "Ermis Soumalias",
        "Michael Curry",
        "Sven Seuken"
      ],
      "abstract": "The next frontier of online advertising is revenue generation from LLM-generated content. We consider a setting where advertisers aim to influence the responses of an LLM, while platforms seek to maximize advertiser value and ensure user satisfaction. The challenge is that advertisers' preferences generally conflict with those of the user, and advertisers may misreport their preferences. To address this, we introduce MOSAIC, an auction mechanism that ensures that truthful reporting is a dominant strategy for advertisers and that aligns the utility of each advertiser with their contribution to social welfare. Importantly, the mechanism operates without LLM fine-tuning or access to model weights and provably converges to the output of the optimally fine-tuned LLM as computational resources increase. Additionally, it can incorporate contextual information about advertisers,  which significantly improves social welfare. Via experiments with publicly available LLMs, we show that MOSAIC leads to high advertiser value and platform revenue with low computational costs. While our motivating application is online advertising, our mechanism can be applied in any setting with monetary transfers, making it a general-purpose solution for truthfully aggregating the preferences of self-interested agents over LLM-generated replies.",
      "arxiv_url": "https://openreview.net/forum?id=tD9mebUimw",
      "pdf_url": "https://openreview.net/pdf/745b8698f44090c2c2a4384ad3b723bb5a49e96c.pdf",
      "primary_category": "Market Design, LLMs, Large Language Models",
      "categories": [
        "Market Design",
        "LLMs",
        "Large Language Models",
        "Auctions",
        "Economics and Computation",
        "LLM Aggregation",
        "Online Advertising Auctions",
        "Preference Aggregation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MKEDsVWHd0",
      "title": "AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs",
      "authors": [
        "Di He",
        "Songjun Tu",
        "Ajay Jaiswal",
        "Li Shen",
        "Ganzhao Yuan",
        "Shiwei Liu",
        "Lu Yin"
      ],
      "abstract": "Weight decay is a standard regularization technique for training large language models (LLMs).  While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM.  Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify “heavy-tailedness.”  Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay.  Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance.  Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.  The code is available at https://github.com/hed-ucas/AlphaDecay.",
      "arxiv_url": "https://openreview.net/forum?id=MKEDsVWHd0",
      "pdf_url": "https://openreview.net/pdf/08ce1afa24d3e729a84eb8b514d611c650adb4ac.pdf",
      "primary_category": "Weight decay, Large Language Models, Heavy-Tailed Self-Regularization",
      "categories": [
        "Weight decay",
        "Large Language Models",
        "Heavy-Tailed Self-Regularization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4E3I17pNEl",
      "title": "Yggdrasil: Bridging Dynamic Speculation and Static Runtime  for Latency-Optimal Tree-Based LLM Decoding",
      "authors": [
        "Yue Guan",
        "Changming Yu",
        "Shihan Fang",
        "Weiming Hu",
        "Zaifeng Pan",
        "Zheng Wang",
        "Zihan Liu",
        "Yangjie Zhou",
        "Yufei Ding",
        "Minyi Guo",
        "Jingwen Leng"
      ],
      "abstract": "Speculative decoding improves LLM inference by generating and verifying multiple tokens in parallel, but existing systems suffer from suboptimal performance due to a mismatch between dynamic speculation and static runtime assumptions. We present Yggdrasil, a co-designed system that enables latency-optimal speculative decoding through context-aware tree drafting and compiler-friendly execution. Yggdrasil introduces an equal-growth tree structure for static graph compatibility, a latency-aware optimization objective for draft selection, and stage-based scheduling to reduce overhead. Yggdrasil supports unmodified LLMs and achieves up to $3.98\\times$ speedup over state-of-the-art baselines across multiple hardware setups.",
      "arxiv_url": "https://openreview.net/forum?id=4E3I17pNEl",
      "pdf_url": "https://openreview.net/pdf/7ed3c139aad73b2bb0e6de6b84715529a70a033a.pdf",
      "primary_category": "speculative decoding, machine learning system, large language model",
      "categories": [
        "speculative decoding",
        "machine learning system",
        "large language model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zswylB4Wnt",
      "title": "RAST: Reasoning Activation in LLMs via Small-model Transfer",
      "authors": [
        "Siru Ouyang",
        "Xinyu Zhu",
        "Zilin Xiao",
        "Minhao Jiang",
        "Yu Meng",
        "Jiawei Han"
      ],
      "abstract": "Reinforcement learning (RL) has become a powerful approach for improving the reasoning capabilities of large language models (LLMs), as evidenced by recent successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale remains intimidatingly resource-intensive, requiring multiple model copies and extensive GPU workloads. On the other hand, while being powerful, recent studies suggest that RL does not fundamentally endow models with new knowledge; rather, it primarily reshapes the model's output distribution to activate reasoning capabilities latent in the base model. Building on this insight, we hypothesize that the changes in output probabilities induced by RL are largely model-size invariant, opening the door to a more efficient paradigm: training a small model with RL and transferring its induced probability shifts to larger base models. To verify our hypothesis, we conduct a token-level analysis of decoding trajectories and find high alignment in RL-induced output distributions across model scales, validating our hypothesis. Motivated by this, we propose RAST, a simple yet effective method that transfers reasoning behaviors by injecting RL-induced probability adjustments from a small RL-trained model into larger models. Experiments across multiple mathematical reasoning benchmarks show that RAST substantially and consistently enhances the reasoning capabilities of base models while requiring significantly lower GPU memory than direct RL training, sometimes even yielding better performance than the RL-trained counterparts. Our findings offer new insights into the nature of RL-driven reasoning and practical strategies for scaling its benefits without incurring its full computational cost. The project page of RAST is available at https://ozyyshr.github.io/RAST/.",
      "arxiv_url": "https://openreview.net/forum?id=zswylB4Wnt",
      "pdf_url": "https://openreview.net/pdf/4e1cf538a0cee20ba69772f7ede39e11ffddd493.pdf",
      "primary_category": "LLM Reasoning, Reinforcement Learning, Decoding-time Strategy",
      "categories": [
        "LLM Reasoning",
        "Reinforcement Learning",
        "Decoding-time Strategy",
        "Scaling"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XQrGTggLvT",
      "title": "LLM Strategic Reasoning: Agentic Study through Behavioral Game Theory",
      "authors": [
        "Jingru Jia",
        "Zehua Yuan",
        "Junhao Pan",
        "Paul E McNamara",
        "Deming Chen"
      ],
      "abstract": "What does it truly mean for a language model to “reason” strategically, and can scaling up alone guarantee intelligent, context-aware decisions? Strategic decision-making requires adaptive reasoning, where agents anticipate and respond to others’ actions under uncertainty. Yet, most evaluations of large language models (LLMs) for strategic decision-making often rely heavily on Nash Equilibrium (NE) benchmarks, overlook reasoning depth, and fail to reveal the mechanisms behind model behavior. To address this gap, we introduce a behavioral game-theoretic evaluation framework that disentangles intrinsic reasoning from contextual influence. Using this framework, we evaluate 22 state-of-the-art LLMs across diverse strategic scenarios. We find models like GPT-o3-mini, GPT-o1, and DeepSeek-R1 lead in reasoning depth. Through thinking chain analysis, we identify distinct reasoning styles—such as maximin or belief-based strategies—and show that longer reasoning chains do not consistently yield better decisions. Furthermore, embedding demographic personas reveals context-sensitive shifts: some models (e.g., GPT-4o, Claude-3-Opus) improve when assigned female identities, while others (e.g., Gemini 2.0) show diminished reasoning under minority sexuality personas. These findings underscore that technical sophistication alone is insufficient; alignment with ethical standards, human expectations, and situational nuance is essential for the responsible deployment of LLMs in interactive settings.",
      "arxiv_url": "https://openreview.net/forum?id=XQrGTggLvT",
      "pdf_url": "https://openreview.net/pdf/889f720b4e1b2dbd69a783f361a62831f7d66d8f.pdf",
      "primary_category": "Large Language Model, Reasoning, Fairness",
      "categories": [
        "Large Language Model",
        "Reasoning",
        "Fairness",
        "Interpretability",
        "Multi-agent"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9V2SVEl1vP",
      "title": "When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration",
      "authors": [
        "Quan Shi",
        "Carlos E Jimenez",
        "Shunyu Yao",
        "Nick Haber",
        "Diyi Yang",
        "Karthik R Narasimhan"
      ],
      "abstract": "As large language models (LLMs) increasingly serve as close collaborators for humans, it is crucial that they express their reasoning in ways that humans can understand and learn from. However, this capability remains relatively less understood and under-evaluated. To address this, we introduce a conceptual framework for such Human-AI knowledge transfer capabilities and conduct the first large-scale user study (N=118) explicitly designed to measure it. In our two-phase setup, humans first ideate with an LLM on problem-solving strategies, then independently implement solutions, isolating the influence of model reasoning on human understanding. Our findings reveal that while model benchmark performance correlates with collaborative outcomes, this relationship is notably inconsistent with significant outliers, highlighting that knowledge transfer is a distinct capability requiring dedicated optimization. Our analysis uncovers behavioral and strategic factors that mediate successful knowledge transfer, and we release our code, dataset, and evaluation framework to support future work on communicatively aligned models.",
      "arxiv_url": "https://openreview.net/forum?id=9V2SVEl1vP",
      "pdf_url": "https://openreview.net/pdf/c1a3cbc9ed24222c7acd4c5116269a1d595197b1.pdf",
      "primary_category": "LLMs for coding, LLMs for math, Human Study",
      "categories": [
        "LLMs for coding",
        "LLMs for math",
        "Human Study",
        "HCI",
        "AI-assisted ideation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RALtozQipi",
      "title": "OmniDraft: A cross-vocabulary, online adaptive drafter for on-device speculative decoding",
      "authors": [
        "Ramchalam Kinattinkara Ramakrishnan",
        "Zhaocong Yuan",
        "Shaojie Zhuo",
        "Chen Feng",
        "Yicheng Lin",
        "Chenzheng Su",
        "Xiaopeng Zhang"
      ],
      "abstract": "Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the “one drafter for all” paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.",
      "arxiv_url": "https://openreview.net/forum?id=RALtozQipi",
      "pdf_url": "https://openreview.net/pdf/bb19bf4afb2b4dade07f71c9bb12255f0e584882.pdf",
      "primary_category": "Speculative Decoding, LLM, Online Adaptation",
      "categories": [
        "Speculative Decoding",
        "LLM",
        "Online Adaptation",
        "Distillation",
        "Tokenization",
        "Inference Optimization",
        "Fine-tuning",
        "LoRA"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "R73ybUciQF",
      "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders",
      "authors": [
        "David Chanin",
        "James Wilken-Smith",
        "Tomáš Dulka",
        "Hardik Bhatnagar",
        "Satvik Golechha",
        "Joseph Isaac Bloom"
      ],
      "abstract": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features (“math” may split into “algebra”, “geometry”, etc.), a phenomenon referred to as feature splitting. However, we show that sparse decomposition and splitting of hierarchical features is not robust. Specifically, we show that seemingly monosemantic features fail to fire where they should, and instead get “absorbed” into their children features. We coin this phenomenon feature absorption, and show that it is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We introduce a metric to detect absorption in SAEs, and validate our findings empirically on hundreds of LLM SAEs. Our investigation suggests that varying SAE sizes or sparsity is insufficient to solve this issue. We discuss the implications of feature absorption in SAEs and some potential approaches to solve the fundamental theoretical issues before SAEs can be used for interpreting LLMs robustly and at scale.",
      "arxiv_url": "https://openreview.net/forum?id=R73ybUciQF",
      "pdf_url": "https://openreview.net/pdf/7d86b58a316a5f311749202f078711e685e83d19.pdf",
      "primary_category": "sparse autoencoders, SAEs, interpretability",
      "categories": [
        "sparse autoencoders",
        "SAEs",
        "interpretability",
        "NLP"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "j42rziWq1n",
      "title": "ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs",
      "authors": [
        "Xiyao Wang",
        "Zhengyuan Yang",
        "Chao Feng",
        "Yuhang Zhou",
        "Xiaoyu Liu",
        "Yongyuan Liang",
        "Ming Li",
        "Ziyi Zang",
        "Linjie Li",
        "Chung-Ching Lin",
        "Kevin Lin",
        "Furong Huang",
        "Lijuan Wang"
      ],
      "abstract": "Reinforcement learning (RL) has shown great effectiveness for fine-tuning large language models (LLMs) using tasks that are challenging yet easily verifiable, such as math reasoning or code generation. However, extending this success to visual perception in vision–language models (VLMs) has been impeded by the scarcity of vision-centric tasks that are simultaneously challenging and unambiguously verifiable. To this end, we introduce \\textbf{ViCrit} (\\textit{Visual Caption Hallucination Critic}), an RL proxy task that trains VLMs to localize a subtle, synthetic visual hallucination injected into paragraphs of human-written image captions. Starting from a 200-word captions, we inject a single, subtle visual description error—altering a few words on objects, attributes, counts, or spatial relations—and task the model to pinpoint the corrupted span given the image and the modified caption. This formulation preserves the full perceptual difficulty while providing a binary, exact-match reward that is easy to compute and unambiguous. Models trained with the \\textbf{ViCrit Task} exhibit substantial gains across a variety of VL benchmarks. Crucially, the improvements transfer beyond natural-image training data to abstract image reasoning and visual math, showing promises of learning to perceive rather than barely memorizing seen objects. To facilitate evaluation, we further introduce \\textbf{ViCrit-Bench}, a category-balanced diagnostic benchmark that systematically probes perception errors across diverse image domains and error types. Together, our results demonstrate that fine-grained hallucination criticism is an effective and generalizable objective for enhancing visual perception in VLMs.",
      "arxiv_url": "https://openreview.net/forum?id=j42rziWq1n",
      "pdf_url": "https://openreview.net/pdf/631a654bdd1aa99c3357feb56e89859a66512702.pdf",
      "primary_category": "Visual reasoning; Vision-Language Model; Visual captioning; Reward Model; Visual Hallucination",
      "categories": [
        "Visual reasoning; Vision-Language Model; Visual captioning; Reward Model; Visual Hallucination"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nTfhNThKX2",
      "title": "Sparta Alignment: Collectively Aligning Multiple Language Models through Combat",
      "authors": [
        "Yuru Jiang",
        "Wenxuan Ding",
        "Shangbin Feng",
        "Greg Durrett",
        "Yulia Tsvetkov"
      ],
      "abstract": "We propose Sparta Alignment, an algorithm to collectively align multiple LLMs through competition and combat. To complement a single model's lack of diversity in generation and biases in evaluation, multiple LLMs form a 'sparta tribe' to compete against each other in fulfilling instructions while serving as judges for the competition of others. For each iteration, one instruction and two models are selected for a duel, the other models evaluate the two responses, and their evaluation scores are aggregated through a adapted elo-ranking based reputation system, where winners/losers of combat gain/lose weight in evaluating others. The peer-evaluated combat results then become preference pairs where the winning response is preferred over the losing one, and all models learn from these preferences at the end of each iteration. Sparta Alignment enables the self-evolution of multiple LLMs in an iterative and collective competition process. Extensive experiments demonstrate that Sparta Alignment outperforms initial models and 4 self-alignment baselines across 10 out of 12 tasks and datasets with 7.0\\% average improvement. Further analysis reveals that Sparta Alignment generalizes more effectively to unseen tasks and leverages the expertise diversity of participating models to produce more logical, direct and informative outputs.",
      "arxiv_url": "https://openreview.net/forum?id=nTfhNThKX2",
      "pdf_url": "https://openreview.net/pdf/7c643d661822baa12d8a8c37dff6332faf289ac1.pdf",
      "primary_category": "alignment, model collaboration",
      "categories": [
        "alignment",
        "model collaboration"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zYEZ5KqtDO",
      "title": "Heterogeneous Swarms: Jointly Optimizing Model Roles and Weights for Multi-LLM Systems",
      "authors": [
        "Shangbin Feng",
        "Zifeng Wang",
        "Palash Goyal",
        "Yike Wang",
        "Weijia Shi",
        "Huang Xia",
        "Hamid Palangi",
        "Luke Zettlemoyer",
        "Yulia Tsvetkov",
        "Chen-Yu Lee",
        "Tomas Pfister"
      ],
      "abstract": "We propose Heterogeneous Swarms, an algorithm to design multi-LLM systems by jointly optimizing model roles and weights. We represent multi-LLM systems as directed acyclic graphs (DAGs) of LLMs with topological message passing for collaborative generation. Given a pool of LLM experts and a utility function, Heterogeneous Swarms employs two iterative steps: role-step and weight-step. For role-step, we interpret model roles as learning a DAG that specifies the flow of inputs and outputs between LLMs. Starting from a swarm of random continuous adjacency matrices, we decode them into discrete DAGs, call the LLMs in topological order, evaluate on the utility function (e.g. accuracy on a task), and optimize the adjacency matrices with particle swarm optimization based on the utility score. For weight-step, we assess the contribution of individual LLMs in the multi-LLM systems and optimize model weights with swarm intelligence. We propose JFK-score to quantify the individual contribution of each LLM in the best-found DAG of the role-step, then optimize model weights with particle swarm optimization based on the JFK-score. Experiments demonstrate that Heterogeneous Swarms outperforms 17 role- and/or weight-based baselines by 18.5% on average across 12 tasks. Further analysis reveals that Heterogeneous Swarms discovers multi-LLM systems with heterogeneous model roles and substantial collaborative gains, and benefits from the diversity of language models.",
      "arxiv_url": "https://openreview.net/forum?id=zYEZ5KqtDO",
      "pdf_url": "https://openreview.net/pdf/57ce2a04142a427fdcb740c73895111cfb96f5ed.pdf",
      "primary_category": "model collaboration, multi-LLM systems",
      "categories": [
        "model collaboration",
        "multi-LLM systems"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "OkVQJZWGfn",
      "title": "CoT Information: Improved Sample Complexity under Chain-of-Thought Supervision",
      "authors": [
        "Awni Altabaa",
        "Omar Montasser",
        "John Lafferty"
      ],
      "abstract": "Learning complex functions that involve multi-step reasoning poses a significant challenge for standard supervised learning from input-output examples. Chain-of-thought (CoT) supervision, which augments training data with intermediate reasoning steps to provide a richer learning signal, has driven recent advances in large language model reasoning. This paper develops a statistical theory of learning under CoT supervision. Central to the theory is the *CoT information*, which measures the additional discriminative power offered by the chain-of-thought for distinguishing hypotheses with different end-to-end behaviors. The main theoretical results demonstrate how CoT supervision can yield significantly faster learning rates compared to standard end-to-end supervision, with both upper bounds and information-theoretic lower bounds characterized by the CoT information.",
      "arxiv_url": "https://openreview.net/forum?id=OkVQJZWGfn",
      "pdf_url": "https://openreview.net/pdf/9187d52d673cbb39fbab9c1a675c4f277f43c5ed.pdf",
      "primary_category": "chain-of-thought, learning theory, statistical learning theory",
      "categories": [
        "chain-of-thought",
        "learning theory",
        "statistical learning theory",
        "PAC learning",
        "sample complexity"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vCTnwpAcma",
      "title": "Calibrating Translation Decoding with Quality Estimation on LLMs",
      "authors": [
        "Di Wu",
        "Yibin Lei",
        "Christof Monz"
      ],
      "abstract": "Neural machine translation (NMT) systems typically employ maximum *a&nbsp;posteriori* (MAP) decoding to select the highest-scoring translation from the distribution. However, recent evidence highlights the inadequacy of MAP decoding, often resulting in low-quality or even pathological hypotheses as the decoding objective is only weakly aligned with real-world translation quality. This paper proposes to directly calibrate hypothesis likelihood with translation quality from a distributional view by directly optimizing their Pearson correlation, thereby enhancing decoding effectiveness. With our method, translation with large language models (LLMs) improves substantially after limited training (2K instances per direction). This improvement is orthogonal to those achieved through supervised fine-tuning, leading to substantial gains across a broad range of metrics and human evaluations. This holds even when applied to top-performing translation-specialized LLMs fine-tuned on high-quality translation data, such as Tower, or when compared to recent preference optimization methods, like CPO. Moreover, the calibrated translation likelihood can directly serve as a strong proxy for translation quality, closely approximating or even surpassing some state-of-the-art translation quality estimation models, like CometKiwi. \nLastly, our in-depth analysis demonstrates that calibration enhances the effectiveness of MAP decoding, thereby enabling greater efficiency in real-world deployment. The resulting state-of-the-art translation model, which covers 10 languages, along with the accompanying code and human evaluation data, has been released: https://github.com/moore3930/calibrating-llm-mt.",
      "arxiv_url": "https://openreview.net/forum?id=vCTnwpAcma",
      "pdf_url": "https://openreview.net/pdf/6e00852e0e1a96541b28d509016ebbe5e0fa05c3.pdf",
      "primary_category": "Translation, Quality Estimation, LLMs",
      "categories": [
        "Translation",
        "Quality Estimation",
        "LLMs",
        "Calibration"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "TMSNlEKKeM",
      "title": "Avoiding exp(R) scaling in RLHF through Preference-based Exploration",
      "authors": [
        "Mingyu Chen",
        "Yiding Chen",
        "Wen Sun",
        "Xuezhou Zhang"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal technique for large language model (LLM) alignment.\nThis paper studies the setting of online RLHF and focuses on improving its sample efficiency.\nAll existing algorithms for online RLHF, whether doing passive exploration or active exploration, suffer from a sample complexity that scales exponentially with the range of the reward function.\nThis statistical inefficiency hinders their effectiveness in scenarios with heavily skewed preferences, e.g. questions with objectively correct answers.\nTo address this, we introduce Self-Exploring Preference-Incentive Online Preference Optimization (SE-POPO), an online RLHF algorithm that for the first time achieves a sample complexity that scales polynomially with the reward range, answering an open problem raised by Xie et al. [2024].\nTheoretically, we demonstrate that the sample complexity of SE-POPO dominates that of existing exploration algorithms. Empirically, our systematic evaluation confirms that SE-POPO is more sample-efficient than both exploratory and non-exploratory baselines, in two primary application scenarios of RLHF as well as on public benchmarks, marking a significant step forward in RLHF algorithm design.",
      "arxiv_url": "https://openreview.net/forum?id=TMSNlEKKeM",
      "pdf_url": "https://openreview.net/pdf/20167d33652c8890dd195fc43c387af0808c8ad9.pdf",
      "primary_category": "RL theory, RLHF, sample complexity",
      "categories": [
        "RL theory",
        "RLHF",
        "sample complexity",
        "llm"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "m6WmeOI1AW",
      "title": "Caption This, Reason That: VLMs Caught in the Middle",
      "authors": [
        "Zihan Weng",
        "Lucas Gomez",
        "Taylor Whittington Webb",
        "Pouya Bashivan"
      ],
      "abstract": "Vision-Language Models (VLMs) have shown remarkable progress in visual understanding in recent years. Yet, they still lag behind human capabilities in specific visual tasks such as counting or relational reasoning. To understand the underlying limitations, we adopt methodologies from cognitive science, analyzing VLM performance along core cognitive axes: Perception, Attention, and Memory. Using a suite of tasks targeting these abilities, we evaluate state-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct cognitive profiles: while advanced models approach ceiling performance on some tasks (e.g. category identification), a significant gap persists, particularly in tasks requiring spatial understanding or selective attention. Investigating the source of these failures and potential methods for improvement, we employ a vision-text decoupling analysis, finding that models struggling with direct visual reasoning show marked improvement when reasoning over their own generated text captions. These experiments reveal a strong need for improved VLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed human performance. Furthermore, we demonstrate the potential of targeted fine-tuning on composite visual reasoning tasks and show that fine-tuning smaller VLMs moderately improves core cognitive abilities. While this improvement does not translate to large enhancements on challenging, out-of-distribution benchmarks, we show broadly that VLM performance on our datasets strongly correlates with performance on established benchmarks like MMMU-Pro and VQAv2. Our work provides a detailed analysis of VLM cognitive strengths and weaknesses and identifies key bottlenecks in simultaneous perception and reasoning while also providing an effective and simple solution.",
      "arxiv_url": "https://openreview.net/forum?id=m6WmeOI1AW",
      "pdf_url": "https://openreview.net/pdf/756a05a421ed180f3aedfcaa7a55a73f50e83d30.pdf",
      "primary_category": "vlm, lmm, mllm",
      "categories": [
        "vlm",
        "lmm",
        "mllm",
        "cognition",
        "visual reasoning",
        "reasoning",
        "evaluation",
        "finetuning"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "PhIWEbewAz",
      "title": "Can We Infer Confidential Properties of Training Data from LLMs?",
      "authors": [
        "Pengrun Huang",
        "Chhavi Yadav",
        "Kamalika Chaudhuri",
        "Ruihan Wu"
      ],
      "abstract": "Large language models (LLMs) are increasingly fine-tuned on domain-specific datasets to support applications in fields such as healthcare, finance, and law. These fine-tuning datasets often have sensitive and confidential dataset-level properties — such as patient demographics or disease prevalence—that are not intended to be revealed. While prior work has studied property inference attacks on discriminative models (e.g., image classification models) and generative models (e.g., GANs for image data), it remains unclear if such attacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark task for evaluating property inference in LLMs under two fine-tuning paradigms: question-answering and chat-completion. Built on the ChatDoctor dataset, our benchmark includes a range of property types and task configurations. We further propose two tailored attacks: a prompt-based generation attack and a shadow-model attack leveraging word frequency signals. Empirical evaluations across multiple pretrained LLMs show the success of our attacks, revealing a previously unrecognized vulnerability in LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=PhIWEbewAz",
      "pdf_url": "https://openreview.net/pdf/cd0fb86716237f2897625fe1354d6af3d7b0ec5b.pdf",
      "primary_category": "LLM, Data Confidentiality, Property Inference Attack",
      "categories": [
        "LLM",
        "Data Confidentiality",
        "Property Inference Attack",
        "Privacy"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "W3JnXa9mW2",
      "title": "CellCLIP - Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning",
      "authors": [
        "MingYu Lu",
        "Ethan Weinberger",
        "Chanwoo Kim",
        "Su-In Lee"
      ],
      "abstract": "High-content screening (HCS) assays based on high-throughput microscopy techniques such as Cell Painting have enabled the interrogation of cells' morphological responses to perturbations at an unprecedented scale. The collection of such data promises to facilitate a better understanding of the relationships between different perturbations and their effects on cellular state. Towards achieving this goal, recent advances in cross-modal contrastive learning could, in theory, be leveraged to learn a unified latent space that aligns perturbations with their corresponding morphological effects. However, the application of such methods to HCS data is not straightforward due to substantial differences in the semantics of Cell Painting images compared to natural images, and the difficulty of representing different classes of perturbations (e.g. small molecule vs CRISPR gene knockout) in a single latent space. In response to these challenges, here we introduce CellCLIP, a cross-modal contrastive learning framework for HCS data. CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to better capture relationships between different microscopy channels in image embeddings, along with natural language encoders for representing perturbations. Our framework outperforms current open-source models, demonstrating the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time. Code for our reproducing our experiments is available at https://github.com/suinleelab/CellCLIP.",
      "arxiv_url": "https://openreview.net/forum?id=W3JnXa9mW2",
      "pdf_url": "https://openreview.net/pdf/5d2fb8bf4d29c98b238d869d3adc08d139844a7e.pdf",
      "primary_category": "Contrastive learning, CLIP, cell morphology",
      "categories": [
        "Contrastive learning",
        "CLIP",
        "cell morphology",
        "Cell Painting",
        "cross-modal retrieval",
        "intra-modal retrieval",
        "zero-shot Learning"
      ],
      "tags": [
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oN5YVZ9JeF",
      "title": "T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning",
      "authors": [
        "Yanjun Fu",
        "Faisal Hamman",
        "Sanghamitra Dutta"
      ],
      "abstract": "Instruction tuning is essential for Large Language Models (LLMs) to effectively follow user instructions. To improve training efficiency and reduce data redundancy, recent works use LLM-based scoring functions, e.g., Instruction-Following Difficulty (IFD), to select high–quality instruction-tuning data with scores above a threshold. While these data selection methods often lead to models that can match or even exceed the performance of models trained on the full datasets, we identify two key limitations: (i) they assess quality at the sample level, ignoring token-level informativeness; and (ii) they overlook the robustness of the scoring method, often selecting a sample due to superficial lexical features instead of its true quality. In this work, we propose Token-Selective HIeRarchical Data Selection for Instruction Tuning (T-SHIRT), a novel data selection framework that introduces a new scoring method to include only informative tokens in quality evaluation and also promote robust and reliable samples whose neighbors also show high quality with less local inconsistencies. We demonstrate that models instruction-tuned on a curated dataset (only 5% of the original size) using T-SHIRT can outperform those trained on the entire large-scale dataset by up to 5.48 points on average across eight benchmarks. Across various LLMs and training set scales, our method consistently surpasses existing state-of-the-art data selection techniques, while also remaining both cost-effective and highly efficient. For instance, by using GPT-2 for score computation, we are able to process a dataset of 52k samples in 40 minutes on a single GPU. Our code is available at https://github.com/Dynamite321/T-SHIRT.",
      "arxiv_url": "https://openreview.net/forum?id=oN5YVZ9JeF",
      "pdf_url": "https://openreview.net/pdf/0fa7d6b8c3c7dc3cd17ee07f44b2a8c19ddb6cff.pdf",
      "primary_category": "Large Language Models, Instruction tuning, Data Selection",
      "categories": [
        "Large Language Models",
        "Instruction tuning",
        "Data Selection",
        "Token-selective Quality Score",
        "Robust Hierarchical Selection"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6H4tDTHalg",
      "title": "Eliciting Reasoning in Language Models with Cognitive Tools",
      "authors": [
        "Brown Ebouky",
        "Andrea Bartezzaghi",
        "Mattia Rigotti"
      ],
      "abstract": "The recent advent of reasoning models like OpenAI's o1 was met with excited speculation by the AI community about the mechanisms underlying these capabilities in closed models, followed by a rush of replication efforts, particularly from the open source community.\nThese speculations were largely settled by the demonstration from DeepSeek-R1 that chain-of-thought and reinforcement learning (RL) can effectively replicate reasoning on top of base LLMs.\nHowever, it remains valuable to explore alternative methods for theoretically eliciting reasoning that could help elucidate the underlying mechanisms, as well as providing additional methods that may offer complementary benefits.\n\nHere, we build on the long-standing literature in cognitive psychology and cognitive architectures, which postulates that reasoning arises from the orchestrated, sequential execution of a set of modular, predetermined cognitive operations.\nCrucially, we implement this key idea within a modern agentic tool-calling framework.\nIn particular, we endow an LLM with a small set of \"cognitive tools\" encapsulating specific reasoning operations, each executed by the LLM itself.\nSurprisingly, this simple strategy results in considerable gains in performance on standard mathematical reasoning benchmarks compared to base LLMs, for both closed and open-weight models.\nFor instance, providing our \"cognitive tools\" to GPT-4.1 increases its pass@1 performance on AIME2024 from 32\\% to 53\\%, even surpassing the performance of o1-preview.\n\nIn addition to its practical implications, this demonstration contributes to the debate regarding the role of post-training methods in eliciting reasoning in LLMs versus the role of inherent capabilities acquired during pre-training, and whether post-training merely uncovers these latent abilities.",
      "arxiv_url": "https://openreview.net/forum?id=6H4tDTHalg",
      "pdf_url": "https://openreview.net/pdf/ea576c6461243d27a8102c9dfa554db950d66757.pdf",
      "primary_category": "LLM, Reasoning, Tools",
      "categories": [
        "LLM",
        "Reasoning",
        "Tools"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oBikm5Rshc",
      "title": "Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models",
      "authors": [
        "Chantal Shaib",
        "Vinith Menon Suriyakumar",
        "Byron C Wallace",
        "Marzyeh Ghassemi"
      ],
      "abstract": "For an LLM to correctly respond to an instruction it must understand both the semantics and the domain (i.e., subject area) of a given task-instruction pair. However, syntax can also convey implicit information. Recent work shows that \\textit{syntactic templates}---frequent sequences of Part-of-Speech (PoS) tags---are prevalent in training data and often appear in model outputs. In this work we characterize syntactic templates, domain, and semantics in task-instruction pairs. We identify cases of spurious correlations between syntax and domain, where models learn to associate a domain with syntax during training; this can sometimes override prompt semantics. Using a synthetic training dataset, we find that the syntactic-domain correlation can lower performance (mean 0.51 +/- 0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an evaluation framework to detect this phenomenon in trained models, and show that it occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B; Llama-4-Maverick),  and closed (GPT-4o) models. Finally, we present a case study on the implications for LLM security, showing that unintended syntactic-domain correlations can be used to bypass refusals in OLMo-2-7B Instruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test for syntactic-domain correlations, and (2) to ensure \\textit{syntactic} diversity in training data, specifically within domains, to prevent such spurious correlations.",
      "arxiv_url": "https://openreview.net/forum?id=oBikm5Rshc",
      "pdf_url": "https://openreview.net/pdf/bd1d0e6fa90e58a304398d828bad40d588a12572.pdf",
      "primary_category": "templates, compositionality, language models",
      "categories": [
        "templates",
        "compositionality",
        "language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wIM0y07NGX",
      "title": "MESS+: Dynamically Learned Inference-Time LLM Routing in Model Zoos with Service Level Guarantees",
      "authors": [
        "Herbert Woisetschläger",
        "Ryan Zhang",
        "Shiqiang Wang",
        "Hans Arno Jacobsen"
      ],
      "abstract": "Open-weight large language model (LLM) zoos provide access to numerous high-quality models, but selecting the appropriate model for specific tasks remains challenging and requires technical expertise. Most users simply want factually correct, safe, and satisfying responses without concerning themselves with model technicalities, while inference service providers prioritize minimizing operating costs. These competing interests are typically mediated through service level agreements (SLAs) that guarantee minimum service quality. \nWe introduce MESS+, a stochastic optimization algorithm for cost-optimal LLM request routing while providing rigorous SLA compliance guarantees. MESS+ learns request satisfaction probabilities of LLMs in real-time as users interact with the system, based on which model selection decisions are made by solving a per-request optimization problem. Our algorithm includes a novel combination of virtual queues and request satisfaction prediction, along with a theoretical analysis of cost optimality and constraint satisfaction.\nAcross a wide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of $2\\times$ cost savings compared to existing LLM routing techniques.",
      "arxiv_url": "https://openreview.net/forum?id=wIM0y07NGX",
      "pdf_url": "https://openreview.net/pdf/912c7e017b59dbbec1e0942b1b1bce704b7cd27f.pdf",
      "primary_category": "inference optimization, model routing, stochastic optimization algorithms",
      "categories": [
        "inference optimization",
        "model routing",
        "stochastic optimization algorithms",
        "convergence analysis"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MMYTA3v66p",
      "title": "Causal Discovery and Inference through Next-Token Prediction",
      "authors": [
        "Eivinas Butkus",
        "Nikolaus Kriegeskorte"
      ],
      "abstract": "Deep neural networks have been criticized as fundamentally *statistical* systems that fail to capture causal structure and perform causal reasoning. Here we demonstrate that a GPT-style transformer trained for next-token prediction can simultaneously discover instances of linear Gaussian structural causal models (SCMs) and learn to answer counterfactual queries about those SCMs. First, we show that the network generalizes to counterfactual queries about SCMs for which it has seen interventional data but not any examples of counterfactual inference. The network must, thus, have successfully composed discovered causal structures with a learned counterfactual inference algorithm. Second, we decode the implicit “mental” SCM from the network's residual stream activations and manipulate it using gradient descent with predictable effects on the network's output. Our results suggest that statistical prediction may be sufficient to drive the emergence of internal causal models and causal inference capacities in deep neural networks.",
      "arxiv_url": "https://openreview.net/forum?id=MMYTA3v66p",
      "pdf_url": "https://openreview.net/pdf/7f6abb20eee385df4f93a38ebd223124ae9bba12.pdf",
      "primary_category": "causal inference, causal discovery, transformers",
      "categories": [
        "causal inference",
        "causal discovery",
        "transformers",
        "large language models",
        "language modeling",
        "mechanistic interpretability",
        "structural causal models",
        "neural probing",
        "decoding",
        "causal intervention",
        "next-token prediction",
        "counterfactual reasoning",
        "causal representation learning",
        "Pearl's causal hierarchy",
        "ladder of causation",
        "emergent representations",
        "world models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "uNqTxj5brQ",
      "title": "Fast Inference for Augmented Large Language Models",
      "authors": [
        "Rana Shahout",
        "Cong Liang",
        "Shiji Xin",
        "Qianru Lao",
        "Yong Cui",
        "Minlan Yu",
        "Michael Mitzenmacher"
      ],
      "abstract": "Augmented Large Language Models (LLMs) enhance standalone LLMs by integrating external data sources through API calls. In interactive applications, efficient scheduling is crucial for maintaining low request completion times, directly impacting user engagement. However, these augmentations introduce new scheduling challenges: the size of augmented requests (in tokens) no longer correlates proportionally with execution time, making traditional size-based scheduling algorithms like Shortest Job First less effective. Additionally, requests may require different handling during API calls, which must be incorporated into scheduling.\nThis paper presents MARS, a novel inference framework that optimizes augmented LLM latency by explicitly incorporating system- and application-level considerations into scheduling. MARS introduces a predictive, memory-aware scheduling approach that integrates API handling and request prioritization to minimize completion time. We implement MARS on top of vLLM and evaluate its performance against baseline LLM inference systems, demonstrating improvements in end-to-end latency by 27%-85% and reductions in TTFT by 4%-96% compared to the existing augmented-LLM system, with even greater gains over vLLM. Our implementation is available online.",
      "arxiv_url": "https://openreview.net/forum?id=uNqTxj5brQ",
      "pdf_url": "https://openreview.net/pdf/466ae7c6d881581805533176114b2bde5a683e8d.pdf",
      "primary_category": "LLM inference and serving, Augmented LLM requests",
      "categories": [
        "LLM inference and serving",
        "Augmented LLM requests"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "D5TSSVkTBA",
      "title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior",
      "authors": [
        "Yue Gong",
        "Raul Castro Fernandez"
      ],
      "abstract": "As hypothesis generation becomes increasingly automated, a new bottleneck has emerged: hypothesis assessment. Modern systems can surface thousands of statistical relationships—correlations, trends, causal links—but offer little guidance on which ones are novel, non-trivial, or worthy of expert attention. In this work, we study the complementary problem to hypothesis generation: automatic hypothesis assessment. Specifically, we ask—given a large set of statistical relationships, can we automatically assess which ones are novel and worth further exploration? We focus on correlations as they are a common entry point in exploratory data analysis that often serve as the basis for forming deeper scientific or causal hypotheses.\n\nTo support automatic assessment, we propose to leverage the vast knowledge encoded in LLMs' weights to derive a prior distribution over the correlation value of a variable pair. If an LLM's prior expects the correlation value observed, then such correlation is not surprising, and vice versa. We propose the Logit-based Calibrated Prior, an LLM-elicited correlation prior that transforms the model’s raw output logits into a calibrated, continuous predictive distribution over correlation values. We evaluate the prior on a benchmark of 2,096 real-world variable pairs and it achieves a sign accuracy of 78.8%, a mean absolute error of 0.26, and 95% credible interval coverage of 89.2% in predicting Pearson correlation coefficient. It also outperforms a fine-tuned RoBERTa classifier in binary correlation prediction and achieves higher precision@K in hypothesis ranking. We further show that the prior generalizes to correlations not seen during LLM pretraining, reflecting context-sensitive reasoning rather than memorization.",
      "arxiv_url": "https://openreview.net/forum?id=D5TSSVkTBA",
      "pdf_url": "https://openreview.net/pdf/428894776d65a6ecd6f342dfe8fb112d24492704.pdf",
      "primary_category": "Large Language Models, Hypothesis Assessment, Knowledge Elicitation",
      "categories": [
        "Large Language Models",
        "Hypothesis Assessment",
        "Knowledge Elicitation",
        "Correlation Analysis",
        "Correlation Prediction"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "NNiwGUY50Y",
      "title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks",
      "authors": [
        "Philip Schroeder",
        "Ondrej Biza",
        "Thomas Weng",
        "Hongyin Luo",
        "James R. Glass"
      ],
      "abstract": "Vision-language models (VLMs) have exhibited impressive capabilities across diverse image understanding tasks, but still struggle in settings that require reasoning over extended sequences of camera frames from a video. This limits their utility in embodied settings, which require reasoning over long frame sequences from a continuous stream of visual input at each moment of a task attempt. To address this limitation, we propose ROVER (Reasoning Over VidEo Recursively), a framework that enables the model to recursively decompose long-horizon video trajectories into segments corresponding to shorter subtasks within the trajectory. In doing so, ROVER facilitates more focused and accurate reasoning over temporally localized frame sequences without losing global context. We evaluate ROVER, implemented using an in-context learning approach, on diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa that consists of 543 videos showing both expert and perturbed non-expert trajectories across 27 manipulation tasks. ROVER outperforms strong baselines across three video reasoning tasks: task progress estimation, frame-level natural language reasoning, and video question answering. We observe that, by reducing the number of frames the model reasons over at each timestep, ROVER mitigates model hallucinations, especially during unexpected or non-optimal moments of a trajectory. In addition, by enabling the implementation of a subtask-specific sliding context window, ROVER's time complexity scales linearly with video length, an asymptotic improvement over baselines.",
      "arxiv_url": "https://openreview.net/forum?id=NNiwGUY50Y",
      "pdf_url": "https://openreview.net/pdf/194ef43930192ab08c960e21945eb03ce919797a.pdf",
      "primary_category": "vision-language model, video reasoning, embodied intelligence",
      "categories": [
        "vision-language model",
        "video reasoning",
        "embodied intelligence",
        "recursion"
      ],
      "tags": [
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "j8XnFfTvXF",
      "title": "KL-Regularized RLHF with Multiple Reference Models: Exact Solutions and Sample Complexity",
      "authors": [
        "Gholamali Aminian",
        "Amir R. Asadi",
        "Idan Shenfeld",
        "Youssef Mroueh"
      ],
      "abstract": "Recent methods for aligning large language models (LLMs) with human feedback predominantly rely on a single reference model, which limits diversity, model overfitting, and underutilizes the wide range of available pre-trained models. Incorporating multiple reference models has the potential to address these limitations by broadening perspectives, reducing bias, and leveraging the strengths of diverse open-source LLMs. However, integrating multiple reference models into reinforcement learning with human feedback (RLHF) frameworks poses significant theoretical challenges, where achieving exact solutions has remained an open problem. This paper presents the first \\emph{exact solution} to the multiple reference model problem in reverse KL-regularized RLHF. We introduce a comprehensive theoretical framework that includes rigorous statistical analysis and provides sample complexity guarantees. Additionally, we extend our analysis to forward KL-regularized RLHF, offering new insights into sample complexity requirements in multiple reference scenarios. Our contributions lay the foundation for more advanced and adaptable LLM alignment techniques, enabling the effective use of multiple reference models. This work paves the way for developing alignment frameworks that are both theoretically sound and better suited to the challenges of modern AI ecosystems.",
      "arxiv_url": "https://openreview.net/forum?id=j8XnFfTvXF",
      "pdf_url": "https://openreview.net/pdf/e0dd4b6802877e7f8af5bf2c2f72a4dda0afe2cc.pdf",
      "primary_category": "Alignment, multiple reference model, RLHF",
      "categories": [
        "Alignment",
        "multiple reference model",
        "RLHF",
        "Sample complexity"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "CMmKcHFDKL",
      "title": "Theoretically Grounded Framework for LLM Watermarking: A Distribution-Adaptive Approach",
      "authors": [
        "Haiyun He",
        "Yepeng Liu",
        "Ziqiao Wang",
        "Yongyi Mao",
        "Yuheng Bu"
      ],
      "abstract": "Watermarking has emerged as a crucial method to distinguish AI-generated text from human-created text. Current watermarking approaches often lack formal optimality guarantees or address the scheme and detector design separately. In this paper, we introduce a novel, unified theoretical framework for watermarking Large Language Models (LLMs) that jointly optimizes both the watermarking scheme and detector. Our approach aims to maximize detection performance while maintaining control over the worst-case false positive rate (FPR) and distortion on text quality. We derive closed-form optimal solutions for this joint design and characterize the fundamental trade-off between watermark detectability and distortion. Notably, we reveal that the optimal watermarking schemes should be adaptive to the LLM’s generative distribution. Building on our theoretical insights, we propose a distortion-free, distribution-adaptive watermarking algorithm (DAWA) that leverages a surrogate model for model-agnosticism and efficiency. Experiments on Llama2-13B and Mistral-8$\\times$7B models confirm the effectiveness of our approach, particularly at ultra-low FPRs. Our code is available at \\url{https://github.com/yepengliu/DAWA}.",
      "arxiv_url": "https://openreview.net/forum?id=CMmKcHFDKL",
      "pdf_url": "https://openreview.net/pdf/4c576f5152406f171b34d8d3893eea46725a4444.pdf",
      "primary_category": "Watermarking, Large Language Models, Hypothesis Testing",
      "categories": [
        "Watermarking",
        "Large Language Models",
        "Hypothesis Testing",
        "Information Theory"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7ZVRlBFuEv",
      "title": "d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning",
      "authors": [
        "Siyan Zhao",
        "Devaansh Gupta",
        "Qinqing Zheng",
        "Aditya Grover"
      ],
      "abstract": "Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL).\nThese capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. \nIn contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning.\nTo this end, we propose, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO, the first integration of policy gradient methods to masked dLLMs. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and planning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM.",
      "arxiv_url": "https://openreview.net/forum?id=7ZVRlBFuEv",
      "pdf_url": "https://openreview.net/pdf/ebf91a9858396ccac623909d304bcae2cbd2c42f.pdf",
      "primary_category": "diffusion language models, post-training, reinforcement learning",
      "categories": [
        "diffusion language models",
        "post-training",
        "reinforcement learning",
        "reasoning",
        "large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wH3F1ZoK70",
      "title": "Salient Concept-Aware Generative Data Augmentation",
      "authors": [
        "Tianchen Zhao",
        "Xuanbai Chen",
        "Zhihua Li",
        "Jun Fang",
        "DONGSHENG An",
        "Xiang Xu",
        "Zhuowen Tu",
        "Yifan Xing"
      ],
      "abstract": "Recent generative data augmentation methods conditioned on both image and text prompts struggle to balance between fidelity and diversity, as it is challenging to preserve essential image details while aligning with varied text prompts. \nThis challenge arises because representations in the synthesis process often become entangled with non-essential input image attributes such as environmental contexts, creating conflicts with text prompts intended to modify these elements.\nTo address this, we propose a personalized image generation framework that uses a salient concept-aware image embedding model to reduce the influence of irrelevant visual details during the synthesis process, thereby maintaining intuitive alignment between image and text inputs.\nBy generating images that better preserve class-discriminative features with additional controlled variations, our framework effectively enhances the diversity of training datasets and thereby improves the robustness of downstream models.\nOur approach demonstrates superior performance across eight fine-grained vision datasets, outperforming state-of-the-art augmentation methods with averaged classification accuracy improvements by 0.73\\% and 6.5\\% under conventional and long-tail settings, respectively.",
      "arxiv_url": "https://openreview.net/forum?id=wH3F1ZoK70",
      "pdf_url": "https://openreview.net/pdf/19d6ab167749b574717efd69f13afa4f9f4d4256.pdf",
      "primary_category": "Generative Data Augmentation, Fine-grained Image Classification",
      "categories": [
        "Generative Data Augmentation",
        "Fine-grained Image Classification"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "fiXCcxGysZ",
      "title": "Token Embeddings Violate the Manifold Hypothesis",
      "authors": [
        "Michael Robinson",
        "Sourya Dey",
        "Tony Chiang"
      ],
      "abstract": "A full understanding of the behavior of a large language model (LLM) requires our grasp of its input token space.\nIf this space differs from our assumptions, our comprehension of and conclusions about the LLM will likely be flawed.  We elucidate the structure of the token embeddings both empirically and theoretically. We present a novel statistical test assuming that the neighborhood around each token has a relatively flat and smooth structure as the null hypothesis. Failing to reject the null is uninformative, but rejecting it at a specific token $\\psi$ implies an irregularity in the token subspace in a $\\psi$-neighborhood, $B(\\psi)$. The structure assumed in the null is a generalization of a manifold with boundary called a \\emph{smooth fiber bundle} (which can be split into two spatial regimes -- small and large radius), so we denote our new hypothesis test as the ``fiber bundle hypothesis.'' By running our test over several open-source LLMs, each with unique token embeddings, we find that the null is frequently rejected, and so the evidence suggests that the token subspace is not a fiber bundle and hence also not a manifold. As a consequence of our findings, when an LLM is presented with two semantically equivalent prompts, if one prompt contains a token implicated by our test, the response to that prompt will likely exhibit less stability than the other.",
      "arxiv_url": "https://openreview.net/forum?id=fiXCcxGysZ",
      "pdf_url": "https://openreview.net/pdf/d467c3ea5d8b18c369693d3a9581e4a36ef2bb55.pdf",
      "primary_category": "token embedding, manifold hypothesis, statistical hypothesis test",
      "categories": [
        "token embedding",
        "manifold hypothesis",
        "statistical hypothesis test",
        "point cloud"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ySFDPoiANu",
      "title": "Execution Guided Line-by-Line Code Generation",
      "authors": [
        "Boaz Lavon",
        "Shahar Katz",
        "Lior Wolf"
      ],
      "abstract": "We present a novel approach to neural code generation that incorporates real-time execution signals into the language model generation process. While large language models (LLMs) have demonstrated impressive code generation capabilities, they typically do not utilize execution feedback during inference, a critical signal that human programmers regularly leverage. Our method, Execution-Guided Classifier-Free Guidance EG-CFG, dynamically incorporates execution signals as the model generates code, providing line-by-line feedback that guides the generation process toward executable solutions.\nEG-CFG employs a multi-stage process: first, we conduct beam search to sample candidate program completions for each line; second, we extract execution signals by executing these candidates against test cases; and finally, we incorporate these signals into the prompt during generation. By maintaining consistent signals across tokens within the same line and refreshing signals at line boundaries, our approach provides coherent guidance while preserving syntactic structure. Moreover, the method naturally supports native parallelism at the task level in which multiple agents operate in parallel, exploring diverse reasoning paths and collectively generating a broad set of candidate solutions.\nOur experiments across diverse coding tasks demonstrate that EG-CFG significantly improves code generation performance compared to standard approaches, achieving state-of-the-art results across various levels of complexity, from foundational problems to challenging competitive programming and data science tasks.",
      "arxiv_url": "https://openreview.net/forum?id=ySFDPoiANu",
      "pdf_url": "https://openreview.net/pdf/e81d326c07782d69016a13c3cf56b8f6789a3e2d.pdf",
      "primary_category": "Program Induction, Problem Solving, Reasoning",
      "categories": [
        "Program Induction",
        "Problem Solving",
        "Reasoning",
        "Natural Language Processing"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "z5FGi0vyCr",
      "title": "Composing Linear Layers from Irreducibles",
      "authors": [
        "Travis Pence",
        "Daisuke Yamada",
        "Vikas Singh"
      ],
      "abstract": "Contemporary large models often exhibit behaviors suggesting the presence of low-level primitives that compose into modules with richer functionality, but these fundamental building blocks remain poorly understood. \nWe investigate this compositional structure in linear layers by asking: \\textit{can we identify/synthesize linear transformations from a minimal set of geometric primitives?} \nUsing Clifford algebra, we show that linear layers can be expressed as compositions of bivectors---geometric objects encoding oriented planes---and introduce a differentiable algorithm that decomposes them into products of rotors. \nThis construction uses only $\\mathcal{O}(\\log^2 d)$ parameters, versus $\\mathcal{O}(d^2)$ required by dense matrices. Applied to the key, query, and value projections in LLM attention layers, our rotor-based layers match the performance of strong baselines such as block-Hadamard and low-rank approximations. Our findings provide an algebraic perspective on how these geometric primitives can compose into higher-level functions within deep models.",
      "arxiv_url": "https://openreview.net/forum?id=z5FGi0vyCr",
      "pdf_url": "https://openreview.net/pdf/d7d19a6b6b9a1f52e9025105c8500d04c871ed04.pdf",
      "primary_category": "Clifford Algebra, Compositions, Modular",
      "categories": [
        "Clifford Algebra",
        "Compositions",
        "Modular"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0XKZFK4hQt",
      "title": "Human Texts Are Outliers: Detecting LLM-generated Texts via Out-of-distribution Detection",
      "authors": [
        "Cong Zeng",
        "Shengkun Tang",
        "Yuanzhou Chen",
        "Zhiqiang Shen",
        "Wenchao Yu",
        "Xujiang Zhao",
        "Haifeng Chen",
        "Wei Cheng",
        "zhiqiang xu"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) such as ChatGPT, DeepSeek, and Claude has significantly increased the presence of AI-generated text in digital communication. This trend has heightened the need for reliable detection methods to distinguish between human-authored and machine-generated content. Existing approaches both zero-shot methods and supervised classifiers largely conceptualize this task as a binary classification problem, often leading to poor generalization across domains and models. In this paper, we argue that such a binary formulation fundamentally mischaracterizes the detection task by assuming a coherent representation of human-written texts. In reality, human texts do not constitute a unified distribution, and their diversity cannot be effectively captured through limited sampling. This causes previous classifiers to memorize observed OOD characteristics rather than learn the essence of `non-ID' behavior, limiting generalization to unseen human-authored inputs. Based on this observation, we propose reframing the detection task as an out-of-distribution (OOD) detection problem, treating human-written texts as distributional outliers while machine-generated texts are in-distribution (ID) samples. To this end, we develop a detection framework using one-class learning method including DeepSVDD and HRN, and score-based learning techniques such as energy-based method, enabling robust and generalizable performance. Extensive experiments across multiple datasets validate the effectiveness of our OOD-based approach. Specifically, the OOD-based method achieves 98.3\\% AUROC and AUPR with only 8.9\\% FPR95 on DeepFake dataset. Moreover, we test our detection framework on multilingual, attacked, and unseen-model and -domain text settings, demonstrating the robustness and generalizability of our framework. Code will be released openly and also available in the supplementary materials.",
      "arxiv_url": "https://openreview.net/forum?id=0XKZFK4hQt",
      "pdf_url": "https://openreview.net/pdf/cc0d4d025019bd6ebbc0a8bc29474a3e3372f4f0.pdf",
      "primary_category": "LLM Detection",
      "categories": [
        "LLM Detection"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Gp2vgxWROE",
      "title": "Rethinking Verification for LLM Code Generation: From Generation to Testing",
      "authors": [
        "Zihan Ma",
        "Taolin Zhang",
        "Maosongcao",
        "Junnan Liu",
        "Wenwei Zhang",
        "Minnan Luo",
        "Songyang Zhang",
        "Kai Chen"
      ],
      "abstract": "Large language models (LLMs) have recently achieved notable success in code‑generation benchmarks such as HumanEval and LiveCodeBench. However, a detailed examination reveals that these evaluation suites often comprise only a limited number of homogeneous test cases, resulting in subtle faults going undetected. This not only artificially inflates measured performance but also compromises accurate reward estimation in reinforcement learning frameworks utilizing verifiable rewards (RLVR). To address these critical shortcomings, we systematically investigate the test-case generation (TCG) task by proposing multi-dimensional metrics designed to rigorously quantify test-suite thoroughness. Furthermore, we introduce a human-LLM collaborative method (SAGA), leveraging human programming expertise with LLM reasoning capability, aimed at significantly enhancing both the coverage and the quality of generated test cases. In addition, we develop a TCGBench to facilitate the study of the TCG task. Experiments show that SAGA achieves a detection rate of 90.62\\% and a verifier accuracy of 32.58\\% on TCGBench. The Verifier Accuracy (Verifier Acc) of the code generation evaluation benchmark synthesized by SAGA is 10.78\\% higher than that of LiveCodeBench-v6. These results demonstrate the effectiveness of our proposed method. We hope this work contributes to building a scalable foundation for reliable LLM code evaluation, further advancing RLVR in code generation, and paving the way for automated adversarial test synthesis and adaptive benchmark integration.",
      "arxiv_url": "https://openreview.net/forum?id=Gp2vgxWROE",
      "pdf_url": "https://openreview.net/pdf/9e84bee22f28e4e4ffda8d34a08736720cf746bc.pdf",
      "primary_category": "Test Case Generation, LLM Code Evaluation, Human-LLM Collaboration",
      "categories": [
        "Test Case Generation",
        "LLM Code Evaluation",
        "Human-LLM Collaboration"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "B7Bc9xzl2o",
      "title": "Latent Retrieval Augmented Generation of Cross-Domain Protein Binders",
      "authors": [
        "Zishen Zhang",
        "Xiangzhe Kong",
        "Wenbing Huang",
        "Yang Liu"
      ],
      "abstract": "Designing protein binders targeting specific sites, which requires to generate realistic and functional interaction patterns, is a fundamental challenge in drug discovery. Current structure-based generative models are limited in generating nterfaces with sufficient rationality and interpretability. In this paper, we propose **R**etrieval-**A**ugmented **Di**ffusion for **A**lig**n**ed interfa**ce** (**RADiAnce**), a new framework that leverages known interfaces to guide the design of novel binders. By unifying retrieval and generation in a shared contrastive latent space, our model efficiently identifies relevant interfaces for a given binding site and seamlessly integrates them through a conditional latent diffusion generator, enabling cross-domain interface transfer. Extensive exeriments show that\n**RADiAnce** significantly outperforms baseline models across multiple metrics, including binding affinity and recovery of geometries and interactions.\nAdditional experimental results validate cross-domain generalization, demonstrating that retrieving interfaces from diverse domains, such as peptides, antibodies, and protein fragments, enhances the generation performance of binders for other domains. Our work establishes a new paradigm for protein binder design that successfully bridges retrieval-based knowledge and generative AI, opening new possibilities for drug discovery.",
      "arxiv_url": "https://openreview.net/forum?id=B7Bc9xzl2o",
      "pdf_url": "https://openreview.net/pdf/9ddd71a1025703ddb55d20ba5a51370138b4915e.pdf",
      "primary_category": "Drug Discovery, Geometric Latent Diffusion, Retrieval Augmented Generation",
      "categories": [
        "Drug Discovery",
        "Geometric Latent Diffusion",
        "Retrieval Augmented Generation",
        "Antibody Design",
        "Peptide Design"
      ],
      "tags": [
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "M0U8wUow8c",
      "title": "A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning",
      "authors": [
        "Guan Zhe Hong",
        "Nishanth Dikkala",
        "Enming Luo",
        "Cyrus Rashtchian",
        "Xin Wang",
        "Rina Panigrahy"
      ],
      "abstract": "Due to the size and complexity of modern large language models (LLMs), it has proven challenging to uncover the underlying mechanisms that models use to solve reasoning problems. For instance, is their reasoning for a specific problem localized to certain parts of the network? Do they break down the reasoning problem into modular components that are then executed as sequential steps as we go deeper in the model? To better understand the reasoning capability of LLMs, we study a minimal propositional logic problem that requires combining multiple facts to arrive at a solution. By studying this problem on Mistral and Gemma models, up to 27B parameters, we illuminate the core components the models use to solve such logic problems. From a mechanistic interpretability point of view, we use causal mediation analysis to uncover the pathways and components of the LLMs' reasoning processes. Then, we offer fine-grained insights into the functions of attention heads in different layers. We not only find a sparse circuit that computes the answer, but we decompose it into sub-circuits that have four distinct and modular uses. Finally, we reveal that three distinct models -- Mistral-7B, Gemma-2-9B and Gemma-2-27B -- contain analogous but not identical mechanisms.",
      "arxiv_url": "https://openreview.net/forum?id=M0U8wUow8c",
      "pdf_url": "https://openreview.net/pdf/edfcc94d23a1796e8903435652cc311e00009492.pdf",
      "primary_category": "Mechanistic interpretability, circuit analysis, logical reasoning",
      "categories": [
        "Mechanistic interpretability",
        "circuit analysis",
        "logical reasoning",
        "propositional logic",
        "large language model",
        "transformer"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8ZiElzQxf1",
      "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
      "authors": [
        "Adrian Łańcucki",
        "Konrad Staniszewski",
        "Piotr Nawrot",
        "Edoardo Ponti"
      ],
      "abstract": "Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key–value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8× compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference latency and memory load. For instance, we enhance Qwen-R1 32B by 9.1 points on AIME 24, 7.6 on GPQA, and 9.6 on LiveCodeBench on average for an equivalent number of memory reads.",
      "arxiv_url": "https://openreview.net/forum?id=8ZiElzQxf1",
      "pdf_url": "https://openreview.net/pdf/bff876f643e028a8e0f2e4f8ae52a2b5e43de99b.pdf",
      "primary_category": "reasoning, inference scaling, KV cache compression",
      "categories": [
        "reasoning",
        "inference scaling",
        "KV cache compression",
        "token eviction",
        "large language model"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Fcs90Rwm8j",
      "title": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs",
      "authors": [
        "Hao Kang",
        "Qingru Zhang",
        "Han Cai",
        "Weiyuan Xu",
        "Tushar Krishna",
        "Yilun Du",
        "Tsachy Weissman"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable performance across diverse reasoning and generation tasks, and are increasingly deployed as agents in dynamic environments such as code generation and recommendation systems. However, many real-world applications, such as high-frequency trading and real-time competitive gaming, require decisions under strict latency constraints, where faster responses directly translate into higher rewards. Despite the importance of this latency–quality trade-off, it remains underexplored in the context of LLM-based agents. In this work, we present the first systematic study of this trade-off in real-time decision-making tasks. To support our investigation, we introduce two new benchmarks: HFTBench, a high-frequency trading simulation, and StreetFighter, a competitive gaming platform. Our analysis reveals that optimal latency–quality balance varies by task, and that sacrificing quality for lower latency can significantly enhance downstream performance. To address this, we propose FPX, an adaptive framework that dynamically selects model size and quantization level based on real-time demands. Our method achieves the best performance on both benchmarks, improving win rate by up to 80% in Street Fighter and boosting daily yield by up to 26.52% in trading,  underscoring the need for latency-aware evaluation and deployment strategies for LLM-based agents. These results demonstrate the critical importance of latency-aware evaluation and deployment strategies for real-world LLM-based agents.",
      "arxiv_url": "https://openreview.net/forum?id=Fcs90Rwm8j",
      "pdf_url": "https://openreview.net/pdf/13fb7451afdc5c8796e7166332feddd0c8a38f8e.pdf",
      "primary_category": "Machine Learning, Gaming, Model Compression",
      "categories": [
        "Machine Learning",
        "Gaming",
        "Model Compression",
        "LLM Agents"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vaosMuNvOt",
      "title": "QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation",
      "authors": [
        "Yang Zhang",
        "Rui Zhang",
        "Jiaming Guo",
        "Huang Lei",
        "Di Huang",
        "Yunpu Zhao",
        "Shuyao Cheng",
        "Pengwei Jin",
        "Chongxiao Li",
        "Zidong Du",
        "Xing Hu",
        "Qi Guo",
        "Yunji Chen"
      ],
      "abstract": "The remarkable progress of Large Language Models (LLMs) presents promising opportunities for Verilog code generation which is significantly important for automated circuit design. The lacking of meaningful functional rewards hinders the preference optimization based on Reinforcement Learning (RL) for producing functionally correct Verilog code. In this paper, we propose Signal-Aware Learning for Verilog code generation (QiMeng-SALV) by leveraging code segments of functionally correct output signal to optimize RL training. Considering Verilog code specifies the structural interconnection of hardware gates and wires so that different output signals are independent, the key insight of QiMeng-SALV is to extract verified signal-aware implementations in partially incorrect modules, so as to enhance the extraction of meaningful functional rewards. Roughly, we verify the functional correctness of signals in generated module by comparing with that of reference module in the training data. Then abstract syntax tree (AST) is employed to identify signal-aware code segments which can provide meaningful functional rewards from erroneous modules. Finally, we introduce signal-aware DPO which is optimized on the correct signal-level code segments, thereby preventing noise and interference from incorrect signals.\nThe proposed QiMeng-SALV underscores the paradigm shift from conventional module-level to fine-grained signal-level optimization in Verilog code generation, addressing the issue of insufficient functional rewards.\nExperiments demonstrate that our method achieves state-of-the-art performance on VerilogEval and RTLLM, with a 7B parameter model matching the performance of the DeepSeek v3 671B model and significantly outperforming the leading open-source model CodeV trained on the same dataset.",
      "arxiv_url": "https://openreview.net/forum?id=vaosMuNvOt",
      "pdf_url": "https://openreview.net/pdf/c8b52df783c4d2ad8b7695db95e2c45c17e4efa6.pdf",
      "primary_category": "Verilog Code Generation, Code Generation, LLM",
      "categories": [
        "Verilog Code Generation",
        "Code Generation",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "idir4VfpuZ",
      "title": "VeriLoC: Line-of-Code Level Prediction of Hardware Design Quality from Verilog Code",
      "authors": [
        "Raghu Vamshi Hemadri",
        "Jitendra Bhandari",
        "Andre Nakkab",
        "Johann Knechtel",
        "Badri P Gopalan",
        "Ramesh Narayanaswamy",
        "Ramesh Karri",
        "Siddharth Garg"
      ],
      "abstract": "Modern chip design is complex, and there is a crucial need for early-stage prediction of key  design-quality metrics like timing and routing congestion directly from Verilog code (a commonly used programming language for hardware design). It is especially important yet complex to predict individual lines of code that cause timing violations or downstream routing congestion. Prior works have tried approaches like converting Verilog into an intermediate graph representation and using LLM embeddings alongside other features to predict module-level quality, but did not consider line-level quality prediction. We propose VeriLoC, the first method that predicts design quality directly from Verilog at both the line- and module-level.  To this end, VeriLoC leverages recent Verilog code-generation LLMs to extract local line-level and module-level embeddings, and trains downstream classifiers/regressors on concatenations of these embeddings. VeriLoC achieves high F1-scores of 0.86-0.95 for line-level congestion and timing prediction, and reduces the mean average percentage error from 14%-18% for SOTA methods down to only 4%. We believe that VeriLoC embeddings and insights from our work  will also be of value for other predictive and optimization tasks for complex hardware design.",
      "arxiv_url": "https://openreview.net/forum?id=idir4VfpuZ",
      "pdf_url": "https://openreview.net/pdf/7f99ae07e3ff70e8e1001d0addd87feef888d8a1.pdf",
      "primary_category": "Hardware Design, LLM Embedding, Congestion",
      "categories": [
        "Hardware Design",
        "LLM Embedding",
        "Congestion",
        "Timing"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "HnJ1UkuJXS",
      "title": "Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents",
      "authors": [
        "Yun Hua",
        "Haosheng Chen",
        "Shiqin Wang",
        "Wenhao Li",
        "Xiangfeng Wang",
        "Jun Luo"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed as autonomous agents in multi-agent systems, and promising coordination has been demonstrated in handling complex tasks under predefined roles and scripted workflows.\nHowever, significant challenges remain in open-ended environments, where agents are inherently self-interested and explicit coordination guidelines are absent. \nIn such scenarios, misaligned incentives frequently lead to social dilemmas and inefficient collective outcomes.\nInspired by how human societies tackle similar coordination challenges—through temporary collaborations like employment or subcontracting—a cooperative workflow \\textbf{Shapley-Coop} is proposed. \nThis workflow enables self-interested Large Language Model (LLM) agents to engage in emergent collaboration by using a fair credit allocation mechanism to ensure each agent’s contributions are appropriately recognized and rewarded.\nShapley-Coop introduces structured negotiation protocols and Shapley-inspired reasoning to estimate agents’ marginal contributions, thereby enabling effective task-time coordination and equitable post-task outcome redistribution. \nThis results in effective coordination that fosters collaboration while preserving agent autonomy, through a rational pricing mechanism that encourages cooperative behavior.\nEvaluated in two multi-agent games and a software engineering simulation, Shapley-Coop consistently enhances LLM agent collaboration and facilitates equitable outcome redistribution, accurately reflecting individual contributions during the task execution process.",
      "arxiv_url": "https://openreview.net/forum?id=HnJ1UkuJXS",
      "pdf_url": "https://openreview.net/pdf/b766f8bc0602b07837d552dd7f04168535c02370.pdf",
      "primary_category": "Large Language Model, Multi-Agent Systems, Mechanism Design",
      "categories": [
        "Large Language Model",
        "Multi-Agent Systems",
        "Mechanism Design"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "M6gpmbPFty",
      "title": "Residual Stream Analysis of Overfitting And Structural Disruptions",
      "authors": [
        "Quan Liu",
        "Han Zhou",
        "Wenquan Wu",
        "Hua Wu",
        "Sen Su"
      ],
      "abstract": "Ensuring that large language models (LLMs) remain both helpful and harmless poses a significant challenge: fine-tuning on repetitive safety datasets—where unsafe prompts are paired with standard refusal templates—often leads to \\emph{false refusals}, in which benign queries are declined. We first quantify this effect, showing that safety data exhibits substantially lower token entropy ($H_{1}\\approx9.18$) and 2-gram diversity ($\\approx$ 0.048) compared to general instruction data ($H_{1}\\approx12.05$, 2-gram$\\approx$0.205). To uncover the root cause, we introduce \\emph{FlowLens}, a stable PCA-based tool for residual-stream geometry analysis, and reveal that higher proportions of safety examples concentrate variance along a few components, reducing representational smoothness and driving false refusals (false refusal rate rises from 63\\% to 84\\% as safety data increases from 0\\% to 40\\%). Guided by these insights, we propose \\emph{Variance Concentration Loss} (VCL), an auxiliary regularizer that penalizes excessive variance concentration in mid-layer residuals. Empirical results demonstrate that VCL reduces false refusals by over 35 percentage points while maintaining or improving performance on general benchmarks such as MMLU and GSM8K.",
      "arxiv_url": "https://openreview.net/forum?id=M6gpmbPFty",
      "pdf_url": "https://openreview.net/pdf/e5c908fdc994612f01678bf358d14600f3580ebb.pdf",
      "primary_category": "large language model, safety, residual stream",
      "categories": [
        "large language model",
        "safety",
        "residual stream",
        "false refusal"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ctJxU8v3bY",
      "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models",
      "authors": [
        "Yi Liu",
        "Dianqing Liu",
        "Mingye Zhu",
        "Junbo Guo",
        "Yongdong Zhang",
        "Zhendong Mao"
      ],
      "abstract": "The widespread adoption of large language models (LLMs) across industries has increased the demand for high-quality and customizable outputs. However, traditional alignment methods often require retraining large pretrained models, making it difficult to quickly adapt and optimize LLMs for diverse applications. To address this limitation, we propose a novel \\textit{Residual Alignment Model} (\\textit{RAM}) that formalizes the alignment process as a type of importance sampling. In this framework, the unaligned upstream model serves as the proposal distribution, while the alignment process is framed as secondary sampling based on an autoregressive alignment module that acts as an estimator of the importance weights. This design enables a natural detachment of the alignment module from the target aligned model, improving flexibility and scalability. Based on this model, we derive an efficient sequence-level training strategy for the alignment module, which operates independently of the proposal module. Additionally, we develop a resampling algorithm with iterative token-level decoding to address the common first-token latency issue in comparable methods. Experimental evaluations on two leading open-source LLMs across diverse tasks, including instruction following, domain adaptation, and preference optimization, demonstrate that our approach consistently outperforms baseline models.",
      "arxiv_url": "https://openreview.net/forum?id=ctJxU8v3bY",
      "pdf_url": "https://openreview.net/pdf/8be3fde00d74a1f9c6d24caa00813525dc205d64.pdf",
      "primary_category": "Alignment of Large Language Models, Importance Sampling",
      "categories": [
        "Alignment of Large Language Models",
        "Importance Sampling"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "roKj4IwaVT",
      "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
      "authors": [
        "Gleb Rodionov",
        "Roman Garipov",
        "Alina Shutova",
        "George Yakushev",
        "Erik Schultheis",
        "Vage Egiazarian",
        "Anton Sinitsin",
        "Denis Kuznedelev",
        "Dan Alistarh"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.",
      "arxiv_url": "https://openreview.net/forum?id=roKj4IwaVT",
      "pdf_url": "https://openreview.net/pdf/752690b760cb01f226ba630228b97af81df4d1d1.pdf",
      "primary_category": "LLM, reasoning, parallel generation",
      "categories": [
        "LLM",
        "reasoning",
        "parallel generation"
      ],
      "tags": [
        "LLM",
        "Agentic AI",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "aYd4wSCle4",
      "title": "Homogeneous Algorithms Can Reduce Competition in Personalized Pricing",
      "authors": [
        "Nathanael Jo",
        "Ashia C. Wilson",
        "Kathleen Creel",
        "Manish Raghavan"
      ],
      "abstract": "Firms' algorithm development practices are often homogeneous Whether firms train algorithms on similar data or rely on similar pre-trained models, the result is correlated predictions. In the context of personalized pricing, correlated algorithms can be viewed as a means to collude among competing firms, but whether or not this conduct is legal depends on the mechanisms of achieving collusion. We investigate the precise mechanisms through a formal game-theoretic model. Indeed, we find that (1) higher correlation diminishes consumer welfare and (2) as consumers become more price sensitive, firms are increasingly incentivized to compromise on the accuracy of their predictions in exchange for coordination. We demonstrate our theoretical results in a stylized empirical study where two firms compete using personalized pricing algorithms. Our results demonstrate a new mechanism for achieving collusion through correlation, which allows us to analyze its legal implications. Correlation through algorithms is a new frontier of anti-competitive behavior that is largely unconsidered by US antitrust law.",
      "arxiv_url": "https://openreview.net/forum?id=aYd4wSCle4",
      "pdf_url": "https://openreview.net/pdf/6e5939d954719e54f2c8a099d9a50abb21d2b3a8.pdf",
      "primary_category": "Algorithmic Pricing, Personalized Pricing, Collusion",
      "categories": [
        "Algorithmic Pricing",
        "Personalized Pricing",
        "Collusion",
        "Competition",
        "Outcome Homogenization",
        "Algorithmic Monoculture",
        "Antitrust",
        "Tacit Collusion"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cvFFar8cAr",
      "title": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs",
      "authors": [
        "Wanyun Cui",
        "Mingwei Xu"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have highlighted the critical importance of extending context length, yet the quadratic complexity of attention mechanisms poses significant challenges for efficient long-context modeling. KV cache compression has emerged as a key approach to address this challenge. Through extensive empirical analysis, we reveal a fundamental yet previously overlooked asymmetry in KV caches: while adjacent keys receive similar attention weights ({\\it local homogeneity}), adjacent values demonstrate distinct {\\it heterogeneous} distributions. This key-value asymmetry reveals a critical limitation in existing compression methods that treat keys and values uniformly. To address the limitation, we propose a training-free compression framework (AsymKV) that combines homogeneity-based key merging with a mathematically proven lossless value compression. Extensive experiments demonstrate that AsymKV consistently outperforms existing long-context methods across various tasks and base models. For example, on LLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing SOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in this \\href{https://github.com/the-scale-lab/Asymkv}{link}.",
      "arxiv_url": "https://openreview.net/forum?id=cvFFar8cAr",
      "pdf_url": "https://openreview.net/pdf/67d3bdf6a6a508c5bf77ef6589fee67a1282c0f7.pdf",
      "primary_category": "large language models, long context",
      "categories": [
        "large language models",
        "long context"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Lq4nneD2xX",
      "title": "$\\texttt{G1}$: Teaching LLMs to Reason on Graphs with Reinforcement Learning",
      "authors": [
        "Xiaojun Guo",
        "Ang Li",
        "Yifei Wang",
        "Stefanie Jegelka",
        "Yisen Wang"
      ],
      "abstract": "Although Large Language Models (LLMs) have demonstrated remarkable progress, their proficiency in graph-related tasks remains notably limited, hindering the development of truly general-purpose models. Previous attempts, including pretraining graph foundation models or employing supervised fine-tuning, often face challenges such as the scarcity of large-scale, universally represented graph data. We introduce $\\texttt{G1}$, a simple yet effective approach demonstrating that Reinforcement Learning (RL) on synthetic graph-theoretic tasks can significantly scale LLMs' graph reasoning abilities. To enable RL training, we curate \\erdos, the largest graph reasoning dataset to date comprising 50 diverse graph-theoretic tasks of varying difficulty levels, 100k training data and 5k test data, all drived from real-world graphs. With RL on \\erdos, $\\texttt{G1}$ obtains substantial improvements in graph reasoning, where our finetuned 3B model even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also show strong zero-shot generalization to unseen tasks, domains, and graph encoding schemes, including other graph-theoretic benchmarks as well as real-world node classification and link prediction tasks, without compromising general reasoning abilities. Our findings offer an efficient, scalable path for building strong graph reasoners by finetuning LLMs with RL on graph-theoretic tasks, which combines the strengths of pretrained LLM capabilities with abundant, automatically generated synthetic data, suggesting that LLMs possess graph understanding abilities that RL can elicit successfully. Our implementation is open-sourced at https://github.com/PKU-ML/G1, with models and datasets hosted on Hugging Face collections https://huggingface.co/collections/PKU-ML/g1-683d659e992794fc99618cf2 for broader accessibility.",
      "arxiv_url": "https://openreview.net/forum?id=Lq4nneD2xX",
      "pdf_url": "https://openreview.net/pdf/61bc520cbd6e8c135b7a589881265bd3e7909899.pdf",
      "primary_category": "Graph, Large Language Models, Reasoning",
      "categories": [
        "Graph",
        "Large Language Models",
        "Reasoning",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "M83RPhdsX4",
      "title": "A Theory for Worst-Case vs. Average-Case Guarantees for LLMs",
      "authors": [
        "Noga Amit",
        "Shafi Goldwasser",
        "Orr Paradise",
        "Guy N. Rothblum"
      ],
      "abstract": "How can we trust the correctness of a learned model on a particular input of interest? Model accuracy is typically measured *on average* over a distribution of inputs, giving no guarantee for any fixed input. This paper proposes a theoretically-founded solution to this problem: to train *Self-Proving models* that prove the correctness of their output to a verification algorithm $V$ via an Interactive Proof. Self-Proving models satisfy that, with high probability over an input sampled from a given distribution, the model generates a correct output *and* successfully proves its correctness to $V$. The *soundness* property of $V$ guarantees that, for *every* input, no model can convince $V$ of the correctness of an incorrect output. Thus, a Self-Proving model proves correctness of most of its outputs, while *all* incorrect outputs (of any model) are detected by $V$. We devise and analyze two generic methods for learning Self-Proving models: *Transcript Learning (TL)* which relies on access to transcripts of accepting interactions, and *Reinforcement Learning from Verifier Feedback (RLVF)* which trains a model by emulating interactions with the verifier.",
      "arxiv_url": "https://openreview.net/forum?id=M83RPhdsX4",
      "pdf_url": "https://openreview.net/pdf/0aafc2e994af4e9bba7d24619af998d3394d2e84.pdf",
      "primary_category": "Trustworthy ML, Interactive Proofs, Computational Complexity Theory",
      "categories": [
        "Trustworthy ML",
        "Interactive Proofs",
        "Computational Complexity Theory"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oanhUGY6un",
      "title": "Gradient Multi-Normalization for Efficient LLM Training",
      "authors": [
        "Meyer Scetbon",
        "Chao Ma",
        "Wenbo Gong",
        "Edward Meeds"
      ],
      "abstract": "Training large language models (LLMs) commonly relies on adaptive optimizers such as Adam (Kingma & Ba 2015), which accelerate convergence through moment estimates but incur substantial memory overhead. Recent stateless approaches such as SWAN (Ma et al., 2024) have shown that appropriate preprocessing of instantaneous gradient matrices can match the performance of adaptive methods without storing optimizer states. Building on this insight, we introduce \\emph{gradient multi-normalization}, a principled framework for designing stateless optimizers that normalize gradients with respect to multiple norms simultaneously. Whereas standard first-order methods can be viewed as gradient normalization under a single norm (Bernstein & Newhouse, 2024), our formulation generalizes this perspective to a multi-norm setting. We derive an efficient alternating scheme that enforces these normalization constraints and show that our procedure can produce, up to an arbitrary precision, a fixed-point of the problem. This unifies and extends prior stateless optimizers, showing that SWAN arises as a specific instance with particular norm choices. Leveraging this principle, we develop SinkGD, a lightweight matrix optimizer that retains the memory footprint of SGD while substantially reducing computation relative to whitening-based methods. On the memory-efficient LLaMA training benchmark (Zhao et al., 2024), SinkGD achieves state-of-the-art performance, reaching the same evaluation perplexity as Adam using only 40\\% of the training tokens.",
      "arxiv_url": "https://openreview.net/forum?id=oanhUGY6un",
      "pdf_url": "https://openreview.net/pdf/d68b4b1959a22981c5d5f82a95de6b8e6b31f253.pdf",
      "primary_category": "LLM, Optimizer, Gradient",
      "categories": [
        "LLM",
        "Optimizer",
        "Gradient",
        "Normalization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "SNJhYhO3a9",
      "title": "Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging",
      "authors": [
        "Jinluan Yang",
        "Dingnan Jin",
        "Anke Tang",
        "Li Shen",
        "Didi Zhu",
        "Zhengyu Chen",
        "Ziyu Zhao",
        "Daixin Wang",
        "Qing Cui",
        "Zhiqiang Zhang",
        "JUN ZHOU",
        "Fei Wu",
        "Kun Kuang"
      ],
      "abstract": "Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI. Existing methods like data mixture strategies face limitations, including heavy reliance on expert knowledge and conflicting optimization signals. While model merging offers parameter-level conflict-resolution strategies through integrating specialized models' parameters, its potential for 3H optimization remains underexplored. This paper systematically compares the effectiveness of model merging and data mixture methods in constructing  3H-aligned LLMs for the first time, revealing previously overlooked collaborative and conflict relationships among the 3H dimensions and discussing the advantages and drawbacks of data mixture (\\textit{data-level}) and model merging (\\textit{parameter-level}) methods in mitigating the conflict for balanced 3H optimization. Specially, we propose a novel \\textbf{R}eweighting \\textbf{E}nhanced task \\textbf{S}ingular \\textbf{M}erging method, \\textbf{RESM}, through outlier weighting and sparsity-aware rank selection strategies to address the challenges of preference noise accumulation and layer sparsity adaptation inherent in 3H-aligned LLM merging. Extensive evaluations can verify the effectiveness and robustness of RESM compared to previous data mixture (2\\%-5\\% gain) and model merging (1\\%-3\\% gain) methods in achieving balanced LLM alignment.",
      "arxiv_url": "https://openreview.net/forum?id=SNJhYhO3a9",
      "pdf_url": "https://openreview.net/pdf/ff19d12d1f143465ace04dbed33831a724066410.pdf",
      "primary_category": "Model Merging; Large Language Models; Alignment",
      "categories": [
        "Model Merging; Large Language Models; Alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MLprqOvAAK",
      "title": "Teaching Transformers to Solve Combinatorial Problems through Efficient Trial & Error",
      "authors": [
        "Panagiotis Giannoulis",
        "Yorgos Pantis",
        "Christos Tzamos"
      ],
      "abstract": "Despite their proficiency in various language tasks, Large Language Models (LLMs) struggle with combinatorial problems like Satisfiability, Traveling Salesman Problem, or even basic arithmetic. We address this gap through a novel trial \\& error approach for solving problems in the class NP, where candidate solutions are iteratively generated and efficiently validated using verifiers. We focus on the paradigmatic task of Sudoku and achieve state-of-the-art accuracy (99\\%) compared to prior neuro-symbolic approaches. Unlike prior work that used custom architectures,  our method employs a vanilla decoder-only Transformer (GPT-2) without external tools or function calling. Our method integrates imitation learning of simple Sudoku rules with an explicit Depth-First Search (DFS) exploration strategy involving informed guessing and backtracking. Moving beyond imitation learning, we seek to minimize the number of guesses until reaching a solution. This is achieved using depth-1 guessing, showing empirically that almost all Sudoku can be solved using the puzzle's rules with at most one guess. We provide a rigorous analysis of this setup formalizing its connection to a contextual variant of $\\textit{Min-Sum Set Cover}$, a well-studied problem in algorithms and stochastic optimization.",
      "arxiv_url": "https://openreview.net/forum?id=MLprqOvAAK",
      "pdf_url": "https://openreview.net/pdf/2b1ea920bcb1c1c85be86bf838b562a8a6385162.pdf",
      "primary_category": "LLMs, Transformers, Combinatorial",
      "categories": [
        "LLMs",
        "Transformers",
        "Combinatorial",
        "Sudoku",
        "SAT"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "AsRB5nmlOD",
      "title": "SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation",
      "authors": [
        "Wenyi Yu",
        "Siyin Wang",
        "Xiaoyu Yang",
        "Xianzhao Chen",
        "Xiaohai Tian",
        "Jun Zhang",
        "Guangzhi Sun",
        "Lu Lu",
        "Yuxuan Wang",
        "Chao Zhang"
      ],
      "abstract": "In order to enable fluid and natural human-machine speech interaction, existing full-duplex conversational systems often adopt modular architectures with auxiliary components such as voice activity detectors, interrupters, conversation state predictors, or multiple LLMs. These systems, however, suffer from error accumulation across modules and struggle with key challenges such as context-dependent barge-in and echo cancellation. Recent approaches, most notably Moshi, simplify the pipeline by injecting audio codecs into the token space of a single LLM. However, such methods still incur significant performance degradation when operating on the speech rather than text modality. In this paper, we introduce SALMONN-omni, the first single, standalone full-duplex speech LLM that operates without audio codecs in its token space. It features a novel dynamic thinking mechanism within the LLM backbone, enabling the model to learn when to transition between speaking and listening states. Experiments on widely used benchmarks for spoken question answering and open-domain dialogue show that SALMONN-omni achieves at least 30\\% relative performance improvement over existing open-source full-duplex models and performs highly competitively to half-duplex and turn-based systems, despite using substantially less training data. Moreover, SALMONN-omni demonstrates strong performance in complex conversational scenarios, including turn-taking, backchanneling, echo cancellation and context-dependent barge-in, with further improvements achieved through reinforcement learning. Some demo conversations between user and SALMONN-omni are provided in the following repository https://github.com/bytedance/SALMONN.",
      "arxiv_url": "https://openreview.net/forum?id=AsRB5nmlOD",
      "pdf_url": "https://openreview.net/pdf/4e7868eb633eba650aef6c37986acf1077932586.pdf",
      "primary_category": "speech large language model, spoken dialogue model, full duplex model",
      "categories": [
        "speech large language model",
        "spoken dialogue model",
        "full duplex model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VGB2TV0QUE",
      "title": "Hierarchical Optimization via LLM-Guided Objective Evolution for Mobility-on-Demand Systems",
      "authors": [
        "Yi Zhang",
        "Yushen Long",
        "Yun Ni",
        "Liping Huang",
        "Xiaohong Wang",
        "Jun Liu"
      ],
      "abstract": "Online ride-hailing platforms aim to deliver efficient mobility-on-demand services, often facing challenges in balancing dynamic and spatially heterogeneous supply and demand. Existing methods typically fall into two categories: reinforcement learning (RL) approaches, which suffer from data inefficiency, oversimplified modeling of real-world dynamics, and difficulty enforcing operational constraints; or decomposed online optimization methods, which rely on manually designed high-level objectives that lack awareness of low-level routing dynamics. To address this issue, we propose a novel hybrid framework that integrates large language model (LLM) with mathematical optimization in a dynamic hierarchical system: (1) it is training-free, removing the need for large-scale interaction data as in RL, and (2) it leverages LLM to bridge cognitive limitations caused by problem decomposition by adaptively generating high-level objectives. Within this framework, LLM serves as a meta-optimizer, producing semantic heuristics that guide a low-level optimizer responsible for constraint enforcement and real-time decision execution. These heuristics are refined through a closed-loop evolutionary process, driven by harmony search, which iteratively adapts the LLM prompts based on feasibility and performance feedback from the optimization layer. Extensive experiments based on scenarios derived from both the New York and Chicago taxi datasets demonstrate the effectiveness of our approach, achieving an average improvement of 16% compared to state-of-the-art baselines.",
      "arxiv_url": "https://openreview.net/forum?id=VGB2TV0QUE",
      "pdf_url": "https://openreview.net/pdf/09ab10c2b16e5791fe0ecb70e99d504f82306b92.pdf",
      "primary_category": "Dynamic Hierarchical Optimization, LLM-Optimizer Interaction, Mobility-on-Demand Systems",
      "categories": [
        "Dynamic Hierarchical Optimization",
        "LLM-Optimizer Interaction",
        "Mobility-on-Demand Systems",
        "Meta-Objective Design",
        "Feasibility-Aware Adaptation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qfP6IDxOrA",
      "title": "Omni-DNA: A Genomic Model Supporting Sequence Understanding, Long-context, and Textual Annotation",
      "authors": [
        "Zehui Li",
        "Vallijah Subasri",
        "Yifei Shen",
        "Dongsheng Li",
        "Wentao Gu",
        "Guy-Bart Stan",
        "Yiren Zhao",
        "Caihua Shan"
      ],
      "abstract": "The interpretation of genomic sequences is crucial for understanding biological processes. To handle the growing volume of DNA sequence data, Genomic Foundation Models (GFMs) have been developed by adapting architectures and training paradigms from Large Language Models (LLMs). Despite their remarkable performance in DNA sequence classification tasks, there remains a lack of systematic understanding regarding the training and task-adaptation processes of GFMs. Moreover, existing GFMs cannot achieve state-of-the-art performance on both short and long-context tasks and lacks multimodal abilities. By revisiting pre-training architectures and post-training techniques, we propose **Omni-DNA**, a family of models spanning 20M to 1.1B parameters that supports sequence understanding, long-context genomic reasoning, and natural-language annotation. **Omni-DNA** establishes new state-of-the-art results on 18 of 26 evaluations drawn from Nucleotide Transformer and Genomic Benchmarks. When jointly fine-tuning on biologically related tasks, **Omni-DNA** consistently outperform existing models and demonstrate multi-tasking abilities.  To enable processing of arbitrary sequence lengths, we introduce **SEQPACK**—an adaptive compression operator that packs historical tokens into a learned synopsis using a position-aware learnable sampling mechanism, enabling transformer-based models to process ultra-long sequences with minimal memory and computational requirements. Our approach demonstrates superior performance on enhancer-target interaction tasks, capturing distant regulatory interactions at the 450kbp range more effectively than existing models. Finally, we present a new dataset termed **seq2func**, enabling Omni-DNA to generate accurate and functionally meaningful interpretations of DNA sequences, unlocking new possibilities for genomic analysis and discovery.",
      "arxiv_url": "https://openreview.net/forum?id=qfP6IDxOrA",
      "pdf_url": "https://openreview.net/pdf/e038f294ab29cc597ce4e261209e5e765c2069f9.pdf",
      "primary_category": "DNA, Genomics, Health",
      "categories": [
        "DNA",
        "Genomics",
        "Health",
        "Representation Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "iY1zuKydO0",
      "title": "DISC: Dynamic Decomposition Improves LLM Inference Scaling",
      "authors": [
        "Jonathan Light",
        "Wei Cheng",
        "Benjamin Riviere",
        "Yue Wu",
        "Masafumi Oyamada",
        "Mengdi Wang",
        "Yisong Yue",
        "Santiago Paternain",
        "Haifeng Chen"
      ],
      "abstract": "Inference scaling methods for LLMs often rely on decomposing problems into steps (or groups of tokens), followed by sampling and selecting the best next steps. However, these steps and their sizes are often predetermined or manually designed based on domain knowledge. We propose dynamic decomposition, a method that adaptively and automatically partitions solution and reasoning traces into manageable steps during inference. By more effectively allocating compute -- particularly through subdividing challenging steps and prioritizing their sampling -- dynamic decomposition significantly improves inference efficiency. Experiments on benchmarks such as APPS, MATH, and LiveCodeBench demonstrate that dynamic decomposition outperforms static approaches, including token-level, sentence-level, and single-step decompositions, reducing the pass@10 error rate by 5.0%, 6.7%, and 10.5% respectively. These findings highlight the potential of dynamic decomposition to improve a wide range of inference scaling techniques.",
      "arxiv_url": "https://openreview.net/forum?id=iY1zuKydO0",
      "pdf_url": "https://openreview.net/pdf/8e6f80833c03da451fac1cb9faac6f430ef6d525.pdf",
      "primary_category": "Inference scaling, search, decomposition",
      "categories": [
        "Inference scaling",
        "search",
        "decomposition",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "eHRFb3DSZS",
      "title": "ZeCO: Zero-Communication Overhead Sequence Parallelism for Linear Attention",
      "authors": [
        "Yuhong Chou",
        "Zehao Liu",
        "Rui-Jie Zhu",
        "Xinyi Wan",
        "Tianjian Li",
        "Congying Chu",
        "Qian Liu",
        "Jibin Wu",
        "Zejun MA"
      ],
      "abstract": "Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary performance bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve practically end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a novel collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths.",
      "arxiv_url": "https://openreview.net/forum?id=eHRFb3DSZS",
      "pdf_url": "https://openreview.net/pdf/17933e31df2bb3be6cec18534124873195b1ad5b.pdf",
      "primary_category": "sequence parallelism, context parallelism, linear attention",
      "categories": [
        "sequence parallelism",
        "context parallelism",
        "linear attention",
        "long context training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8YniJnJQ0P",
      "title": "Detecting High-Stakes Interactions with Activation Probes",
      "authors": [
        "Alex McKenzie",
        "Urja Pawar",
        "Phil Blandfort",
        "William Bankes",
        "David Krueger",
        "Ekdeep Singh Lubana",
        "Dmitrii Krasheninnikov"
      ],
      "abstract": "Monitoring is an important aspect of safely deploying Large Language Models (LLMs).\nThis paper examines activation probes for detecting ``high-stakes'' interactions---where the text indicates that the interaction might lead to significant harm---as a critical, yet underexplored, target for such monitoring.\nWe evaluate several probe architectures trained on synthetic data, and find them to exhibit robust generalization to diverse, out-of-distribution, real-world data.\nProbes' performance is comparable to that of prompted or finetuned medium-sized LLM monitors, while offering computational savings of six orders-of-magnitude.\nThese savings are enabled by reusing activations of the model that is being monitored.\nOur experiments also highlight the potential of building resource-aware hierarchical monitoring systems, where probes serve as an efficient initial filter and flag cases for more expensive downstream analysis.\nWe release our novel synthetic dataset and the codebase at\n\\url{https://github.com/arrrlex/models-under-pressure}.",
      "arxiv_url": "https://openreview.net/forum?id=8YniJnJQ0P",
      "pdf_url": "https://openreview.net/pdf/6291cf20df7b3579af7a0739c773db78424d18cd.pdf",
      "primary_category": "linear probes, monitoring, mechanistic interpretability",
      "categories": [
        "linear probes",
        "monitoring",
        "mechanistic interpretability",
        "large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "G7CiAs8xyw",
      "title": "Foundations of Top-$k$ Decoding for Language Models",
      "authors": [
        "Georgy Noarov",
        "Soham Mallick",
        "Tao Wang",
        "Sunay Joshi",
        "Yan Sun",
        "Yangxinyu Xie",
        "Mengxin Yu",
        "Edgar Dobriban"
      ],
      "abstract": "Top-$k$ decoding is a widely used method for sampling from LLMs: at each token, only the largest $k$ next-token-probabilities are kept, and the next token is sampled after re-normalizing them to sum to unity. Top-$k$ and other sampling methods are motivated by the intuition that true next-token distributions are sparse, and the noisy LLM probabilities need to be truncated. However, to our knowledge, a precise theoretical motivation for the use of top-$k$ decoding is missing. In this work, we develop a theoretical framework that both explains and generalizes top-$k$ decoding. We view decoding at a fixed token as the recovery of a sparse probability distribution. We introduce *Bregman decoders* obtained by minimizing a separable Bregman divergence (for both the *primal* and *dual* cases) with a sparsity-inducing $\\ell_0$-regularization; in particular, these decoders are *adaptive* in the sense that the sparsity parameter $k$ is chosen depending on the underlying token distribution. Despite the combinatorial nature of the sparse Bregman objective, we show how to optimize it efficiently for a large class of divergences. We prove that (i) the optimal decoding strategies are greedy, and further that (ii) the objective is discretely convex in $k$, such that the optimal $k$ can be identified in logarithmic time. We note that standard top-$k$ decoding arises as a special case for the KL divergence, and construct new decoding strategies with substantially different behaviors (e.g., non-linearly up-weighting larger probabilities after re-normalization).",
      "arxiv_url": "https://openreview.net/forum?id=G7CiAs8xyw",
      "pdf_url": "https://openreview.net/pdf/4f0949c771faa3e272f16a9171ac11d032efb142.pdf",
      "primary_category": "Large Language Models, Top-$k$ Decoding, Adaptive Decoding",
      "categories": [
        "Large Language Models",
        "Top-$k$ Decoding",
        "Adaptive Decoding",
        "Sparse Optimization",
        "$\\ell_0$-Regularization",
        "Bregman Divergence"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VZQ04Ojhu5",
      "title": "ConfTuner: Training Large Language Models to Express Their Confidence Verbally",
      "authors": [
        "Yibo Li",
        "Miao Xiong",
        "Jiaying Wu",
        "Bryan Hooi"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in high-stakes domains such as science, law, and healthcare, where accurate expressions of uncertainty are essential for reliability and trust. However, current LLMs are often observed to generate incorrect answers with high confidence—a phenomenon known as \"overconfidence\". Recent efforts have focused on calibrating LLMs' verbalized confidence: i.e., their expressions of confidence in text form, such as \"I am 80% confident that...\". Existing approaches either rely on prompt engineering or fine-tuning with heuristically generated uncertainty estimates, both of which have limited effectiveness and generalizability. Motivated by the notion of proper scoring rules for calibration in classical machine learning models, we introduce ConfTuner, a simple and efficient fine-tuning method that introduces minimal overhead and does not require ground-truth confidence scores or proxy confidence estimates. ConfTuner relies on a new loss function, tokenized Brier score, which we theoretically prove to be a proper scoring rule, intuitively meaning that it \"correctly incentivizes the model to report its true probability of being correct\". \nConfTuner improves calibration across diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our results further show that better-calibrated confidence enables downstream gains in self-correction and model cascade, advancing the development of trustworthy LLM systems.\nThe code is available at https://github.com/liushiliushi/ConfTuner.",
      "arxiv_url": "https://openreview.net/forum?id=VZQ04Ojhu5",
      "pdf_url": "https://openreview.net/pdf/05eea868da4d6362aff776bdd4a7e6bd92c69027.pdf",
      "primary_category": "uncertainty quantification, uncertainty estimation, calibration",
      "categories": [
        "uncertainty quantification",
        "uncertainty estimation",
        "calibration",
        "failure prediction",
        "large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4Qe2Hga43N",
      "title": "Cost-Aware Contrastive Routing for LLMs",
      "authors": [
        "Reza Shirkavand",
        "Shangqian Gao",
        "Peiran Yu",
        "Heng Huang"
      ],
      "abstract": "We study cost-aware routing for large language models across diverse and dynamic pools of models. Existing approaches often overlook prompt-specific context, rely on expensive model profiling, assume a fixed set of experts, or use inefficient trial-and-error strategies. We introduce Cost-Spectrum Contrastive Routing (CSCR), a lightweight framework that maps both prompts and models into a shared embedding space to enable fast, cost-sensitive selection. CSCR uses compact, fast-to-compute logit footprints for open-source models and perplexity fingerprints for black-box APIs. A contrastive encoder is trained to favor the cheapest accurate expert within adaptive cost bands. At inference time, routing reduces to a single $k$‑NN lookup via a FAISS index, requiring no retraining when the expert pool changes and enabling microsecond latency. Across multiple benchmarks, CSCR consistently outperforms baselines, improving the accuracy–cost tradeoff by up to 25\\%, while generalizing robustly to unseen LLMs and out-of-distribution prompts.",
      "arxiv_url": "https://openreview.net/forum?id=4Qe2Hga43N",
      "pdf_url": "https://openreview.net/pdf/581f1ff4563146c5a7b2c688928d4932c50225c6.pdf",
      "primary_category": "LLM Routing, Inference Efficiency, Contrastive Learning",
      "categories": [
        "LLM Routing",
        "Inference Efficiency",
        "Contrastive Learning",
        "Model fingerprints",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xdNAVP7TGy",
      "title": "70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float (DFloat11)",
      "authors": [
        "Tianyi Zhang",
        "Mohsen Hariri",
        "Shaochen Zhong",
        "Vipin Chaudhary",
        "Yang Sui",
        "Xia Hu",
        "Anshumali Shrivastava"
      ],
      "abstract": "Large-scale AI models, such as Large Language Models (LLMs) and Diffusion Models (DMs), have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression framework that reduces LLM and DM size by 30\\% while preserving outputs that are bit-for-bit identical to the original model. DFloat11 is motivated by the low entropy in the BFloat16 weight representation of LLMs, which reveals significant inefficiency in the existing storage format. By applying entropy coding, DFloat11 assigns dynamic-length encodings to weights based on frequency, achieving near information-optimal compression without any loss of precision. To facilitate efficient inference with dynamic-length encodings, we develop a custom GPU kernel for fast online decompression. Our design incorporates the following: (i) compact, hierarchical lookup tables (LUTs) that fit within GPU SRAM for efficient decoding, (ii) a two-phase GPU kernel for coordinating thread read/write positions using lightweight auxiliary variables, and (iii) transformer-block-level decompression to minimize latency. Experiments on Llama 3.3, Qwen 3, Mistral 3, FLUX.1, and others validate our hypothesis that DFloat11 achieves around 30\\% model size reduction while preserving bit-for-bit identical outputs. Compared to a potential alternative of offloading parts of an uncompressed model to the CPU to meet memory constraints, DFloat11 achieves 2.3--46.2$\\times$ higher throughput in token generation. With a fixed GPU memory budget, DFloat11 enables 5.7--14.9$\\times$ longer generation lengths than uncompressed models. Notably, our method enables lossless inference of Llama 3.1 405B, an 810GB model, on a single node equipped with 8$\\times$80GB GPUs.",
      "arxiv_url": "https://openreview.net/forum?id=xdNAVP7TGy",
      "pdf_url": "https://openreview.net/pdf/6b1607ff4d79f3a474030fbbf74f738e0e5350c9.pdf",
      "primary_category": "Compression, Lossless Compression, LLM",
      "categories": [
        "Compression",
        "Lossless Compression",
        "LLM",
        "Efficiency",
        "Diffusion Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "hCc6obJhlj",
      "title": "BAM-ICL: Causal Hijacking In-Context Learning with Budgeted Adversarial Manipulation",
      "authors": [
        "Rui Chu",
        "Bingyin Zhao",
        "Hanling Jiang",
        "Shuchin Aeron",
        "Yingjie Lao"
      ],
      "abstract": "Recent research shows that large language models (LLMs) are vulnerable to hijacking attacks under the scenario of in-context learning (ICL) where LLMs demonstrate impressive capabilities in performing tasks by conditioning on a sequence of in-context examples (ICEs) (i.e., prompts with task-specific input-output pairs). Adversaries can manipulate the provided ICEs to steer the model toward attacker-specified outputs, effectively ''hijacking'' the model's decision-making process. Unlike traditional adversarial attacks targeting single inputs, hijacking attacks in LLMs aim to subtly manipulate the initial few examples to influence the model's behavior across a range of subsequent inputs, which requires distributed and stealthy perturbations. However, existing approaches overlook how to effectively allocate the perturbation budget across ICEs. We argue that fixed budgets miss the potential of dynamic reallocation to improve attack success while maintaining high stealthiness and text quality. In this paper, we propose BAM-ICL, a novel **b**udgeted **a**dversarial **m**anipulation hijacking attack framework for in-context learning. We also consider a more practical yet stringent scenario where ICEs arrive sequentially and only the current ICE can be perturbed. BAM-ICL mainly consists of two stages: In the offline stage, where we assume the adversary has access to data drawn from the same distribution as the target task, we develop a global gradient-based attack to learn optimal budget allocations across ICEs. In the online stage, where ICEs arrive sequentially, perturbations are generated progressively according to the learned budget profile. We evaluate BAM-ICL on diverse LLMs and datasets. The experimental results demonstrate that it achieves superior attack success rates and stealthiness, and the adversarial ICEs are highly transferable to other models.",
      "arxiv_url": "https://openreview.net/forum?id=hCc6obJhlj",
      "pdf_url": "https://openreview.net/pdf/38a65c9f42ffea6b51178e8fe7af04344615057c.pdf",
      "primary_category": "large language models, in-context learning, hijacking",
      "categories": [
        "large language models",
        "in-context learning",
        "hijacking",
        "budget profile"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "to1VYVar9W",
      "title": "Breaking AR’s Sampling Bottleneck: Provable Acceleration via Diffusion Language Models",
      "authors": [
        "Gen Li",
        "Changxiao Cai"
      ],
      "abstract": "Diffusion models have emerged as a powerful paradigm for modern generative modeling, demonstrating strong potential for large language models (LLMs). Unlike conventional autoregressive (AR) models that generate tokens sequentially, diffusion models allow for parallel sampling, offering a promising path to accelerate generation and eliminate the left-to-right generation constraints. Despite their empirical success, theoretical understandings of diffusion language models remain underdeveloped. In this work, we develop convergence guarantees for diffusion language models from an information-theoretic perspective. Our analysis demonstrates that the sampling error, measured by the Kullback-Leibler (KL) divergence, decays inversely with the number of iterations $T$ and scales linearly with the mutual information between tokens in the target text sequence. Crucially, our theory covers the regime $T<L$, where $L$ is the text sequence length. This justifies that high-quality samples can be generated with fewer iterations than $L$, thereby breaking the fundamental sampling bottleneck of $L$ steps required by AR models. We further establish matching upper and lower bounds, up to some constant factor, that shows the tightness of our convergence analysis. These results offer novel theoretical insights into the practical effectiveness of diffusion language models.",
      "arxiv_url": "https://openreview.net/forum?id=to1VYVar9W",
      "pdf_url": "https://openreview.net/pdf/1261b5bf2bd40e9c3cabc341d1b097ab4ff4d743.pdf",
      "primary_category": "diffusion model, large language model (LLM), iteration complexity",
      "categories": [
        "diffusion model",
        "large language model (LLM)",
        "iteration complexity",
        "mutual information"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zJzu9evD5K",
      "title": "LittleBit: Ultra Low-Bit Quantization via Latent Factorization",
      "authors": [
        "Banseok Lee",
        "Dongkyu Kim",
        "Youngcheon you",
        "Young-Min Kim"
      ],
      "abstract": "The deployment of large language models (LLMs) is frequently hindered by prohibitive memory and computational requirements. While quantization mitigates these bottlenecks, maintaining model fidelity in the sub-1-bit regime remains a persistent challenge. In this paper, we introduce LittleBit, a novel framework for extreme LLM compression. We target quantization rates as low as $0.1$ bits per weight (BPW), achieving a memory reduction of approximately $31\\times$, which effectively compresses Llama2-13B to under $0.9$ GB. We represent weights via low-rank latent matrix factorization and subsequently binarize the resulting factors. To counteract the information loss inherent to such drastic precision reduction, we integrate a multi-scale compensation mechanism that learns importance parameters across row, column, and latent dimensions. Two primary contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for quantization-aware training (QAT) initialization, and Residual Compensation to minimize approximation errors. Extensive experiments confirm the superiority of LittleBit in the sub-1-bit domain; for instance, our method at $0.1$ BPW surpasses the performance of leading techniques operating at $0.7$ BPW on Llama2-7B. We establish a new size-performance trade-off---unlocking a potential $11.6\\times$ inference speedup relative to FP16---and render powerful LLMs practical for resource-constrained environments. Our code is available at https://github.com/SamsungLabs/LittleBit.",
      "arxiv_url": "https://openreview.net/forum?id=zJzu9evD5K",
      "pdf_url": "https://openreview.net/pdf/e3ee46c9c7611a5f06642d9f05b1227c81648d92.pdf",
      "primary_category": "Large language models, Low-bit quantization, Low-rank factorization",
      "categories": [
        "Large language models",
        "Low-bit quantization",
        "Low-rank factorization",
        "Sub-1-bit model compression",
        "Binarization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oXSkzIXgbk",
      "title": "Bayesian Concept Bottleneck Models with LLM Priors",
      "authors": [
        "Jean Feng",
        "Avni Kothari",
        "Lucas Zier",
        "Chandan Singh",
        "Yan Shuo Tan"
      ],
      "abstract": "Concept Bottleneck Models (CBMs) have been proposed as a compromise between white-box and black-box models, aiming to achieve interpretability without sacrificing accuracy. The standard training procedure for CBMs is to predefine a candidate set of human-interpretable concepts, extract their values from the training data, and identify a sparse subset as inputs to a transparent prediction model. However, such approaches are often hampered by the tradeoff between exploring a sufficiently large set of concepts versus controlling the cost of obtaining concept extractions, resulting in a large interpretability-accuracy tradeoff. This work investigates a novel approach that sidesteps these challenges: BC-LLM iteratively searches over a potentially infinite set of concepts within a Bayesian framework, in which Large Language Models (LLMs) serve as both a concept extraction mechanism and prior. Even though LLMs can be miscalibrated and hallucinate, we prove that BC-LLM can provide rigorous statistical inference and uncertainty quantification. Across image, text, and tabular datasets, BC-LLM outperforms interpretable baselines and even black-box models in certain settings, converges more rapidly towards relevant concepts, and is more robust to out-of-distribution samples.",
      "arxiv_url": "https://openreview.net/forum?id=oXSkzIXgbk",
      "pdf_url": "https://openreview.net/pdf/1e23e77a1a49efae13a6127730fa6e474bd835db.pdf",
      "primary_category": "Bayesian inference, concept bottleneck models, explainable AI",
      "categories": [
        "Bayesian inference",
        "concept bottleneck models",
        "explainable AI",
        "large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QKICx7eSMJ",
      "title": "Generating Computational Cognitive models using Large Language Models",
      "authors": [
        "Milena Rmus",
        "Akshay Kumar Jagadish",
        "Marvin Mathony",
        "Tobias Ludwig",
        "Eric Schulz"
      ],
      "abstract": "Computational cognitive models, which formalize theories of cognition, enable researchers to quantify cognitive processes and arbitrate between competing theories by fitting models to behavioral data. Traditionally, these models are handcrafted, which requires significant domain knowledge, coding expertise, and time investment. However, recent advances in machine learning offer solutions to these challenges. In particular, Large Language Models (LLMs) have demonstrated remarkable capabilities for in-context pattern recognition, leveraging knowledge from diverse domains to solve complex problems, and generating executable code that can be used to facilitate the generation of cognitive models. \nBuilding on this potential, we introduce a pipeline for Guided generation of Computational Cognitive Models (GeCCo). Given task instructions, participant data, and a template function, GeCCo prompts an LLM to propose candidate models, fits proposals to held-out data, and iteratively refines them based on feedback constructed from their predictive performance. We benchmark this approach across four different cognitive domains -- decision making, learning, planning, and memory -- using three open-source LLMs, spanning different model sizes, capacities, and families. On four human behavioral data sets, the LLM generated models that consistently matched or outperformed the best domain-specific models from the cognitive science literature. \nTo validate these findings, we performed control experiments that investigated (1) the contribution of the different LLM features (model size, model family, capacities); (2) the causal role of different prompt components; (3) the effect of data contamination; (4) the ability to recover ground truth models from simulated data; and (5) the total explainable variance in human behavior captured by LLM-generated models. \nTaken together, our results suggest that LLMs can rapidly generate cognitive models with conceptually plausible theories that rival -- or even surpass -- the best models from the literature across diverse task domains.",
      "arxiv_url": "https://openreview.net/forum?id=QKICx7eSMJ",
      "pdf_url": "https://openreview.net/pdf/71914784b067b8a89e5ba9e648c3f3fe0bef8659.pdf",
      "primary_category": "large language models, cognitive computational models, cognitive science",
      "categories": [
        "large language models",
        "cognitive computational models",
        "cognitive science",
        "learning",
        "decision making",
        "neuroscience"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "CHN4HG9R5e",
      "title": "E2E-VGuard: Adversarial Prevention for Production LLM-based End-To-End Speech Synthesis",
      "authors": [
        "Zhisheng Zhang",
        "Derui Wang",
        "Yifan Mi",
        "Zhiyong Wu",
        "JieGao",
        "Yuxin Cao",
        "Kai Ye",
        "Jason Xue",
        "Jie Hao"
      ],
      "abstract": "Recent advancements in speech synthesis technology have enriched our daily lives, with high-quality and human-like audio widely adopted across real-world applications. However, malicious exploitation like voice-cloning fraud poses severe security risks. Existing defense techniques struggle to address the production large language model (LLM)-based speech synthesis. While previous studies have considered the protection for fine-tuning synthesizers, they assume manually annotated transcripts. Given the labor intensity of manual annotation, end-to-end (E2E) systems leveraging automatic speech recognition (ASR) to generate transcripts are becoming increasingly prevalent, e.g., voice cloning via commercial APIs. Therefore, this E2E speech synthesis also requires new security mechanisms. To tackle these challenges, we propose E2E-VGuard, a proactive defense framework for two emerging threats: (1) production LLM-based speech synthesis, and (2) the novel attack arising from ASR-driven E2E scenarios. Specifically, we employ the encoder ensemble with a feature extractor to protect timbre, while ASR-targeted adversarial examples disrupt pronunciation. Moreover, we incorporate the psychoacoustic model to ensure perturbative imperceptibility. For a comprehensive evaluation, we test 16 open-source synthesizers and 3 commercial APIs across Chinese and English datasets, confirming E2E-VGuard's effectiveness in timbre and pronunciation protection. Real-world deployment validation is also conducted. Our code and demo page are available at https://wxzyd123.github.io/e2e-vguard/.",
      "arxiv_url": "https://openreview.net/forum?id=CHN4HG9R5e",
      "pdf_url": "https://openreview.net/pdf/ff8e2693b7a19a41c4943b7bc5fc0f0b220f15fd.pdf",
      "primary_category": "Voice Protection; Adversarial Examples; Speech Synthesis; Data Poisoning",
      "categories": [
        "Voice Protection; Adversarial Examples; Speech Synthesis; Data Poisoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0cCkauhKit",
      "title": "Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules",
      "authors": [
        "Yueqi Zhang",
        "Peiwen Yuan",
        "Yiwei Li",
        "Shaoxiong Feng",
        "Xinglin Wang",
        "Jiayi Shi",
        "Chuyi Tan",
        "Boyuan Pan",
        "Yao Hu",
        "Kan Li"
      ],
      "abstract": "Human–AI conversation frequently relies on quoting earlier text—“check it with the formula I just highlighted”—yet today’s large language models (LLMs) lack an explicit mechanism for locating and exploiting such spans. We formalize the challenge as span-conditioned generation, decomposing each turn into the dialogue history, a set of token-offset quotation spans, and an intent utterance. Building on this abstraction, we introduce a quotation-centric data pipeline that automatically synthesizes task-specific dialogues, verifies answer correctness through multi-stage consistency checks, and yields both a heterogeneous training corpus and the first benchmark covering five representative scenarios. To meet the benchmark’s zero-overhead and parameter-efficiency requirements, we propose QuAda, a lightweight training-based method that attaches two bottleneck projections to every attention head, dynamically amplifying or suppressing attention to quoted spans at inference time while leaving the prompt unchanged and updating < 2.8% of backbone weights. Experiments across models show that QuAda is suitable for all scenarios and generalizes to unseen topics, offering an effective, plug-and-play solution for quotation-aware dialogue.",
      "arxiv_url": "https://openreview.net/forum?id=0cCkauhKit",
      "pdf_url": "https://openreview.net/pdf/ccc57d1f272b13a4853fce78582f6e5b42ba120e.pdf",
      "primary_category": "span-conditioned generation, quotation referencing, position-aware attention",
      "categories": [
        "span-conditioned generation",
        "quotation referencing",
        "position-aware attention",
        "adapters"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "bjoHB7IN6b",
      "title": "Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models",
      "authors": [
        "Zhentao he",
        "Can Zhang",
        "Ziheng Wu",
        "Zhenghao Chen",
        "Yufei Zhan",
        "Yifan Li",
        "Zhao Zhang",
        "Xian Wang",
        "Minghui Qiu"
      ],
      "abstract": "Recent advancements in multimodal large language models (MLLMs) have enhanced document understanding by integrating textual and visual information. However, existing models exhibit incompleteness within their paradigm in real-world scenarios, particularly under visual degradation (e.g., blur, occlusion, low contrast). In such conditions, the current response paradigm often fails to adequately perceive visual degradation and ambiguity, leading to overreliance on linguistic priors or misaligned visual-textual reasoning. This difficulty in recognizing uncertainty frequently results in the generation of hallucinatory content, especially when a precise answer is not feasible. To better demonstrate and analyze this phenomenon and problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR hallucination in degraded document understanding. This dataset includes test samples spanning identity cards, invoices, and prescriptions, with simulated real-world degradations and pixel-level annotations for OCR reliability. This setup allows for evaluating models' capacity, under degraded input, to distinguish reliable visual information and answer accordingly, thereby highlighting the challenge of avoiding hallucination on uncertain data. To achieve vision-faithful reasoning and thereby avoid the aforementioned issues, we further introduce a Group Relative Policy Optimization (GRPO)-based framework featuring a novel reward mechanism. By incorporating a self-awareness of visual uncertainty and an analysis method that initiates refusal to answer to increase task difficulty within our supervised fine-tuning and reinforcement learning framework, we successfully mitigated hallucinations in ambiguous regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model achieves a ~28% absolute improvement in hallucination-free accuracy over GPT-4o on KIE-HVQA and there is no significant performance drop in standard tasks, highlighting both effectiveness and robustness. This work advances the development of reliable MLLMs for real-world document analysis by addressing critical challenges in visual-linguistic alignment under degradation.",
      "arxiv_url": "https://openreview.net/forum?id=bjoHB7IN6b",
      "pdf_url": "https://openreview.net/pdf/1881f652c0211a643300520f2cc8fc4f17b36990.pdf",
      "primary_category": "OCR, MLLM, hallucination",
      "categories": [
        "OCR",
        "MLLM",
        "hallucination"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "iyu4sLQZvW",
      "title": "First Attentions Last: Better Exploiting First Attentions for Efficient Parallel Training",
      "authors": [
        "Gyudong Kim",
        "Hyukju Na",
        "Jin Hyeon Kim",
        "Hyunsung Jang",
        "Jaemin Park",
        "Jaegi Hwang",
        "NAMKOO HA",
        "Seungryong Kim",
        "Young Geun Kim"
      ],
      "abstract": "As training billion-scale transformers becomes increasingly common, employing multiple distributed GPUs along with parallel training methods has become a standard practice. However, existing transformer designs suffer from significant communication overhead, especially in Tensor Parallelism (TP), where each block’s MHA–MLP connection requires an all-reduce communication. Through our investigation, we show that the MHA-MLP connections can be bypassed for efficiency, while the attention output of the first layer can serve as an alternative signal for the bypassed connection. Motivated by the observations, we propose FAL (First Attentions Last), an efficient transformer architecture that redirects the first MHA output to the MLP inputs of the following layers, eliminating the per-block MHA-MLP connections. This removes the all-reduce communication and enables parallel execution of MHA and MLP on a single GPU. We also introduce FAL+, which adds the normalized first attention output to the MHA outputs of the following layers to augment the MLP input for the model quality. Our evaluation shows that FAL reduces multi-GPU training time by up to 44%, improves single-GPU throughput by up to 1.18×, and achieves better perplexity compared to the baseline GPT. FAL+ achieves even lower perplexity without increasing the training time than the baseline. Codes are available at: https://casl-ku.github.io/FAL/",
      "arxiv_url": "https://openreview.net/forum?id=iyu4sLQZvW",
      "pdf_url": "https://openreview.net/pdf/6f467922152eb94ff41fe1cebcb893ba76604317.pdf",
      "primary_category": "Neural Networks, Parallelism, Distributed Training",
      "categories": [
        "Neural Networks",
        "Parallelism",
        "Distributed Training",
        "Tensor Parallel"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZwDMrArTBg",
      "title": "Validating LLM-as-a-Judge Systems under Rating Indeterminacy",
      "authors": [
        "Luke Guerdan",
        "Solon Barocas",
        "Ken Holstein",
        "Hanna Wallach",
        "Steven Wu",
        "Alexandra Chouldechova"
      ],
      "abstract": "The LLM-as-a-judge paradigm, in which a judge LLM system replaces human raters in rating the outputs of other generative AI (GenAI) systems, plays a critical role in scaling and standardizing GenAI evaluations. To validate such judge systems, evaluators assess human--judge agreement by first collecting multiple human ratings for each item in a validation corpus, then aggregating the ratings into a single, per-item gold label rating. For many items, however, rating criteria may admit multiple valid interpretations, so a human or LLM rater may deem multiple ratings \"reasonable\" or \"correct.\"  We call this condition rating indeterminacy. Problematically, many rating tasks that contain rating indeterminacy rely on forced-choice elicitation, whereby raters are instructed to select only one rating for each item. In this paper, we introduce a framework for validating LLM-as-a-judge systems under rating indeterminacy. We draw theoretical connections between different measures of judge system performance under different human--judge agreement metrics, and different rating elicitation and aggregation schemes. We demonstrate that differences in how humans and LLMs resolve rating indeterminacy when responding to forced-choice rating instructions can heavily bias LLM-as-a-judge validation. Through extensive experiments involving 11 real-world rating tasks and 9 commercial LLMs, we show that standard validation approaches that rely upon forced-choice ratings select judge systems that are highly suboptimal, performing as much as 31% worse than judge systems selected by our approach that uses multi-label \"response set\" ratings to account for rating indeterminacy. We conclude with concrete recommendations for more principled approaches to LLM-as-a-judge validation.",
      "arxiv_url": "https://openreview.net/forum?id=ZwDMrArTBg",
      "pdf_url": "https://openreview.net/pdf/2ad527d02fe920443fe79a516d7575f417103f99.pdf",
      "primary_category": "large language models, LLM-as-a-Judge, gold labels",
      "categories": [
        "large language models",
        "LLM-as-a-Judge",
        "gold labels",
        "validity",
        "evaluation",
        "meta-evaluation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Rc489jcc30",
      "title": "AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation Model Pretraining",
      "authors": [
        "Hongyuan Dong",
        "Dingkang Yang",
        "LiangXiao",
        "ChaoFeng",
        "Ran Jiao"
      ],
      "abstract": "Learning rate is widely regarded as crucial for effective foundation model pretraining.\nRecent research explores and demonstrates the transferability of learning rate configurations across varying model and dataset sizes, etc. \nNevertheless, these approaches are constrained to specific training scenarios and typically necessitate extensive hyperparameter tuning on proxy models.\nIn this work, we propose \\textbf{AdaLRS}, a plug-in-and-play adaptive learning rate search algorithm that conducts online optimal learning rate search via optimizing loss descent velocities.\nWe provide theoretical and experimental analyzes to show that foundation model pretraining loss and its descent velocity are both convex and share the same optimal learning rate. \nRelying solely on training loss dynamics, AdaLRS involves few extra computations to guide the search process, and its convergence is guaranteed via theoretical analysis. \nExperiments on both LLM and VLM pretraining show that AdaLRS adjusts suboptimal learning rates to the neighborhood of optimum with marked efficiency and effectiveness, with model performance improved accordingly. \nWe also show the robust generalizability of AdaLRS across varying training scenarios, such as different model sizes, training paradigms, base learning rate scheduler choices, and hyperparameter settings.",
      "arxiv_url": "https://openreview.net/forum?id=Rc489jcc30",
      "pdf_url": "https://openreview.net/pdf/fbc823935262760b602282e9ac61708ffc1c0259.pdf",
      "primary_category": "Foundation Model Pretraining, Hyperparameter Search, Learning Rate Search",
      "categories": [
        "Foundation Model Pretraining",
        "Hyperparameter Search",
        "Learning Rate Search"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yrNw1R8o2W",
      "title": "Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections",
      "authors": [
        "Wei Zhuo",
        "Zhaohuan Zhan",
        "Han Yu"
      ],
      "abstract": "Federated Learning (FL) on graph-structured data typically faces non-IID challenges, particularly in scenarios where each client holds a distinct subgraph sampled from a global graph. In this paper, we introduce **Fed**erated learning with **Aux**iliary projections (FedAux), a personalized subgraph FL framework that learns to align, compare, and aggregate heterogeneously distributed local models without sharing raw data or node embeddings. In FedAux, each client jointly trains (i) a local GNN and (ii) a learnable auxiliary projection vector (APV) that differentiably projects node embeddings onto a 1D space. A soft-sorting operation followed by a lightweight 1D convolution refines these embeddings in the ordered space, enabling the APV to effectively capture client-specific information. After local training, these APVs serve as compact signatures that the server uses to compute inter‑client similarities and perform similarity‑weighted parameter mixing, yielding personalized models while preserving cross‑client knowledge transfer. Moreover, we provide rigorous theoretical analysis to establish the convergence and rationality of our design. Empirical evaluations across diverse graph benchmarks demonstrate that FedAux substantially outperforms existing baselines in both accuracy and personalization performance. The code is available at [https://github.com/JhuoW/FedAux](https://github.com/JhuoW/FedAux).",
      "arxiv_url": "https://openreview.net/forum?id=yrNw1R8o2W",
      "pdf_url": "https://openreview.net/pdf/2aea32be0016f01a10707eccb345637ff6816abc.pdf",
      "primary_category": "Graph Neural Networks, Federated Learning, Node Classification",
      "categories": [
        "Graph Neural Networks",
        "Federated Learning",
        "Node Classification",
        "Graph Representation Learning"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZtXT584LrT",
      "title": "Large Language Bayes",
      "authors": [
        "Justin Domke"
      ],
      "abstract": "Many domain experts do not have the time or expertise to write formal Bayesian models. This paper takes an informal problem description as input, and combines a large language model and a probabilistic programming language to define a joint distribution over formal models, latent variables, and data. A posterior over latent variables follows by conditioning on observed data and integrating over formal models. This presents a challenging inference problem. We suggest an inference recipe that amounts to generating many formal models from the large language model, performing approximate inference on each, and then doing a weighted aver- age. This is justified and analyzed as a combination of self-normalized importance sampling, MCMC, and importance-weighted variational inference. Experimentally, this produces sensible predictions from only data and an informal problem description, without the need to specify a formal model.",
      "arxiv_url": "https://openreview.net/forum?id=ZtXT584LrT",
      "pdf_url": "https://openreview.net/pdf/bc6732b0ddff866ecfb682e38016463e2718781d.pdf",
      "primary_category": "bayesian inference, variational inference",
      "categories": [
        "bayesian inference",
        "variational inference"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pBFVoll8Xa",
      "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
      "authors": [
        "Xueguang Ma",
        "Qian Liu",
        "Dongfu Jiang",
        "Ge Zhang",
        "Zejun MA",
        "Wenhu Chen"
      ],
      "abstract": "Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage.\nDespite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification.\nThis limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce.\nIn this paper, we propose General-Reasoner, a novel training framework designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks.",
      "arxiv_url": "https://openreview.net/forum?id=pBFVoll8Xa",
      "pdf_url": "https://openreview.net/pdf/59ca2ebad16c9f4c1d622f0bcd9007d31e1874bc.pdf",
      "primary_category": "large language model, reasoning, reinforcement learning",
      "categories": [
        "large language model",
        "reasoning",
        "reinforcement learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6geRIdlFWJ",
      "title": "SubTrack++ : Gradient Subspace Tracking for Scalable LLM Training",
      "authors": [
        "Sahar Rajabi",
        "Nayeema Nonta",
        "Sirisha Rambhatla"
      ],
      "abstract": "Training large language models (LLMs) is highly resource-intensive due to their massive number of parameters and the overhead of optimizer states. While recent work has aimed to reduce memory consumption, such efforts often entail trade-offs among memory efficiency, training time, and model performance. Yet, true democratization of LLMs requires simultaneous progress across all three dimensions. To this end, we propose SubTrack++ that leverages Grassmannian gradient subspace tracking combined with projection-aware optimizers, enabling Adam’s internal statistics to adapt to subspace changes. Additionally, employing recovery scaling, a technique that restores information lost through low-rank projections, further enhances model performance. Our method demonstrates SOTA convergence by exploiting Grassmannian geometry, **reducing training wall-time by up to 65%** compared to the best performing baseline, LDAdam, while preserving the reduced memory footprint.  Code is at https://github.com/criticalml-uw/SubTrack.",
      "arxiv_url": "https://openreview.net/forum?id=6geRIdlFWJ",
      "pdf_url": "https://openreview.net/pdf/012df3dbd55a8c18e8eecac5b53371eff2c3eda9.pdf",
      "primary_category": "large language models, memory-efficient training, low-rank optimization",
      "categories": [
        "large language models",
        "memory-efficient training",
        "low-rank optimization",
        "gradient projection"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IiPSP4OUYx",
      "title": "MLLM-For3D: Adapting Multimodal Large Language Model for 3D Reasoning Segmentation",
      "authors": [
        "Jiaxin Huang",
        "Runnan Chen",
        "Ziwen Li",
        "Zhengqing Gao",
        "Xiao He",
        "Yandong Guo",
        "Mingming Gong",
        "Tongliang Liu"
      ],
      "abstract": "Reasoning segmentation aims to segment target objects in complex scenes based on human intent and spatial reasoning. While recent multimodal large language models (MLLMs) have demonstrated impressive 2D image reasoning segmentation, adapting these capabilities to 3D scenes remains underexplored. In this paper, we introduce MLLM-For3D, a simple yet effective framework that transfers knowledge from 2D MLLMs to 3D scene understanding. Specifically, we utilize MLLMs to generate multi-view pseudo-segmentation masks and corresponding text embeddings, then unproject 2D masks into 3D space and align them with the text embeddings. The primary challenge lies in the absence of 3D context and spatial consistency across multiple views, causing the model to hallucinate objects that do not exist and fail to target objects consistently. Training the 3D model with such irrelevant objects leads to performance degradation. To address this, we first filter irrelevant views using token attention. With these reliable pseudo-labels, we develop a token-for-Query approach for multimodal semantic alignment, enabling consistent identification of the same object across different views. Moreover, we introduce a spatial consistency strategy to enforce that segmentation masks remain coherent in the 3D space, effectively capturing the geometry of the scene. Extensive evaluations of various challenging indoor scene benchmarks demonstrate that, even without labeled 3D training data, MLLM-For3D outperforms existing 3D reasoning segmentation methods, effectively interpreting user intent, understanding 3D scenes, and reasoning about spatial relationships.",
      "arxiv_url": "https://openreview.net/forum?id=IiPSP4OUYx",
      "pdf_url": "https://openreview.net/pdf/085227656e9c81cdc7f20e8aa5eef33d292bf0bf.pdf",
      "primary_category": "3D reasoning segmentation, Multi-modal large language Model",
      "categories": [
        "3D reasoning segmentation",
        "Multi-modal large language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "EGK487IYAW",
      "title": "One Filters All: A Generalist Filter For State Estimation",
      "authors": [
        "Shiqi Liu",
        "Wenhan Cao",
        "Chang Liu",
        "Zeyu He",
        "Tianyi Zhang",
        "Yinuo Wang",
        "Shengbo Eben Li"
      ],
      "abstract": "Estimating hidden states in dynamical systems, also known as optimal filtering, is a long-standing problem in various fields of science and engineering. In this paper, we introduce a general filtering framework, $\\textbf{LLM-Filter}$, which leverages large language models (LLMs) for state estimation by embedding noisy observations with text prototypes. In a number of experiments for classical dynamical systems, we find that first, state estimation can significantly benefit from the knowledge embedded in pre-trained LLMs. By achieving proper modality alignment with the frozen LLM, LLM-Filter outperforms the state-of-the-art learning-based approaches. Second, we carefully design the prompt structure, System-as-Prompt (SaP), incorporating task instructions that enable LLMs to understand tasks and adapt to specific systems. Guided by these prompts, LLM-Filter exhibits exceptional generalization, capable of performing filtering tasks accurately in changed or even unseen environments.  We further observe a scaling-law behavior in LLM-Filter, where accuracy improves with larger model sizes and longer training times. These findings make LLM-Filter a promising foundation model of filtering.",
      "arxiv_url": "https://openreview.net/forum?id=EGK487IYAW",
      "pdf_url": "https://openreview.net/pdf/6d25aa6598d42556551c9238a6f852ba81eb5414.pdf",
      "primary_category": "State Estimation, Large Language Model, Bayesian Filtering",
      "categories": [
        "State Estimation",
        "Large Language Model",
        "Bayesian Filtering"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nufqobhME7",
      "title": "Revealing Multimodal Causality with Large Language Models",
      "authors": [
        "Jin Li",
        "Shoujin Wang",
        "Qi Zhang",
        "Feng Liu",
        "Tongliang Liu",
        "Longbing Cao",
        "Shui Yu",
        "Fang Chen"
      ],
      "abstract": "Uncovering cause-and-effect mechanisms from data is fundamental to scientific progress. While large language models (LLMs) show promise for enhancing causal discovery (CD) from unstructured data, their application to the increasingly prevalent multimodal setting remains a critical challenge. Even with the advent of multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two primary limitations: (1) difficulty in exploring intra- and inter-modal interactions for comprehensive causal variable identification; and (2) insufficiency to handle structural ambiguities with purely observational data. To address these challenges, we propose MLLM-CD, a novel framework for multimodal causal discovery from unstructured data. It consists of three key components: (1) a novel contrastive factor discovery module to identify genuine multimodal factors based on the interactions explored from contrastive sample pairs; (2) a statistical causal structure discovery module to infer causal relationships among discovered factors; and (3) an iterative multimodal counterfactual reasoning module to refine the discovery outcomes iteratively by incorporating the world knowledge and reasoning capabilities of MLLMs. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed MLLM-CD in revealing genuine factors and causal relationships among them from multimodal unstructured data. The implementation code and data are available at https://github.com/JinLi-i/MLLM-CD.",
      "arxiv_url": "https://openreview.net/forum?id=nufqobhME7",
      "pdf_url": "https://openreview.net/pdf/6fd0ed368b98ef83e704c98c8326dd77b764dd02.pdf",
      "primary_category": "Multimodal Learning, Large Language Models, Causal Discovery",
      "categories": [
        "Multimodal Learning",
        "Large Language Models",
        "Causal Discovery"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QINnsnppv8",
      "title": "Memory Injection Attacks on LLM Agents via Query-Only Interaction",
      "authors": [
        "Shen Dong",
        "Shaochen Xu",
        "Pengfei He",
        "Yige Li",
        "Jiliang Tang",
        "Tianming Liu",
        "Hui Liu",
        "Zhen Xiang"
      ],
      "abstract": "Agents powered by large language models (LLMs) have demonstrated strong capabilities in a wide range of complex, real-world applications.\nHowever, LLM agents with a compromised memory bank may easily produce harmful outputs when the past records retrieved for demonstration are malicious.\nIn this paper, we propose a novel Memory INJection Attack, MINJA, without assuming that the attacker can directly modify the memory bank of the agent.\nThe attacker injects malicious records into the memory bank by only **interacting with the agent via queries and output observations**.\nThese malicious records are designed to elicit a sequence of malicious reasoning steps corresponding to a different target query during the agent's execution of the victim user's query.\nSpecifically, we introduce a sequence of *bridging steps* to link victim queries to the malicious reasoning steps.\nDuring the memory injection, we propose an *indication prompt* that guides the agent to autonomously generate similar bridging steps, with a *progressive shortening strategy* that gradually removes the indication prompt, such that the malicious record will be easily retrieved when processing later victim queries.\nOur extensive experiments across diverse agents demonstrate the effectiveness of MINJA in compromising agent memory.\nWith minimal requirements for execution, MINJA enables any user to influence agent memory, highlighting the risk.",
      "arxiv_url": "https://openreview.net/forum?id=QINnsnppv8",
      "pdf_url": "https://openreview.net/pdf/825b923c5abe7627f6d9c39ef0254795011dfd85.pdf",
      "primary_category": "LLM agent, LLM, Memory Injection",
      "categories": [
        "LLM agent",
        "LLM",
        "Memory Injection",
        "Trustworthy"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "F1wDPNLvTb",
      "title": "Conditional Representation Learning for Customized Tasks",
      "authors": [
        "Honglin Liu",
        "Chao Sun",
        "Peng Hu",
        "Yunfan Li",
        "Xi Peng"
      ],
      "abstract": "Conventional representation learning methods learn a universal representation that primarily captures dominant semantics, which may not always align with customized downstream tasks. For instance, in animal habitat analysis, researchers prioritize scene-related features, whereas universal embeddings emphasize categorical semantics, leading to suboptimal results. As a solution, existing approaches resort to supervised fine-tuning, which however incurs high computational and annotation costs. In this paper, we propose Conditional Representation Learning (CRL), aiming to extract representations tailored to arbitrary user-specified criteria. Specifically, we reveal that the semantics of a space are determined by its basis, thereby enabling a set of descriptive words to approximate the basis for a customized feature space. Building upon this insight, given a user-specified criterion, CRL first employs a large language model (LLM) to generate descriptive texts to construct the semantic basis, then projects the image representation into this conditional feature space leveraging a vision-language model (VLM). The conditional representation better captures semantics for the specific criterion, which could be utilized for multiple customized tasks. Extensive experiments on classification and retrieval tasks demonstrate the superiority and generality of the proposed CRL. The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.",
      "arxiv_url": "https://openreview.net/forum?id=F1wDPNLvTb",
      "pdf_url": "https://openreview.net/pdf/a3606f3281f5882d08f9bf13a0a44a830f8ffdb2.pdf",
      "primary_category": "Unsupervised Learning, Representation Learning, Conditional Similarity",
      "categories": [
        "Unsupervised Learning",
        "Representation Learning",
        "Conditional Similarity"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3zx087XPtz",
      "title": "AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding",
      "authors": [
        "Chaeyoung Jung",
        "Youngjoon Jang",
        "Joon Son Chung"
      ],
      "abstract": "Hallucination remains a major challenge in multimodal large language models (MLLMs). To address this, various contrastive decoding (CD) methods have been proposed that contrasts original logits with hallucinated logits generated from perturbed inputs. While CD has shown promise in vision-language models (VLMs), it is not well-suited for AV-LLMs, where hallucinations often emerge from both unimodal and cross-modal combinations involving audio, video, and language. These intricate interactions call for a more adaptive and modality-aware decoding strategy. In this paper, we propose Audio-Visual Contrastive Decoding (AVCD)—a novel, training-free decoding framework designed to model trimodal interactions and suppress modality-induced hallucinations in AV-LLMs. Unlike previous CD methods in VLMs that corrupt a fixed modality, AVCD leverages attention distributions to dynamically identify less dominant modalities and applies attentive masking to generate perturbed output logits. To support CD in a trimodal setting, we also reformulate the original CD framework to jointly handle audio, visual, and textual inputs. Finally, to improve efficiency, we introduce entropy-guided adaptive decoding, which selectively skips unnecessary decoding steps based on the model’s confidence in its predictions. Extensive experiments demonstrate that AVCD consistently outperforms existing decoding methods. Especially, on the AVHBench dataset, it improves accuracy by 2% for VideoLLaMA2 and 7% for video-SALMONN, demonstrating strong robustness and generalizability. Our code is available at : https://github.com/kaistmm/AVCD.",
      "arxiv_url": "https://openreview.net/forum?id=3zx087XPtz",
      "pdf_url": "https://openreview.net/pdf/8f976dd72a39b412aab4326ecb3d56facd3d0f5e.pdf",
      "primary_category": "Audio-visual processing, Audio-visual LLM, Audio-visual LLM hallucination",
      "categories": [
        "Audio-visual processing",
        "Audio-visual LLM",
        "Audio-visual LLM hallucination"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xSHqNf5Pdc",
      "title": "Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling",
      "authors": [
        "Xinglin Wang",
        "Yiwei Li",
        "Shaoxiong Feng",
        "Peiwen Yuan",
        "Yueqi Zhang",
        "Jiayi Shi",
        "Chuyi Tan",
        "Boyuan Pan",
        "Yao Hu",
        "Kan Li"
      ],
      "abstract": "Test-Time Scaling (TTS) improves the performance of Large Language Models (LLMs) by using additional inference-time computation to explore multiple reasoning paths through search. Yet how to allocate a fixed rollout budget most effectively during search remains underexplored, often resulting in inefficient use of compute at test time. To bridge this gap, we formulate test-time search as a resource allocation problem and derive the optimal allocation strategy that maximizes the probability of obtaining a correct solution under a fixed rollout budget. Within this formulation, we reveal a core limitation of existing search methods: solution-level allocation tends to favor reasoning directions with more candidates, leading to theoretically suboptimal and inefficient use of compute. To address this, we propose Direction-Oriented Resource Allocation (DORA), a provably optimal method that mitigates this bias by decoupling direction quality from candidate count and allocating resources at the direction level. To demonstrate DORA’s effectiveness, we conduct extensive experiments on challenging mathematical reasoning benchmarks including MATH500, AIME2024, and AIME2025. The empirical results show that DORA consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art accuracy.  We hope our findings contribute to a broader understanding of optimal TTS for LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=xSHqNf5Pdc",
      "pdf_url": "https://openreview.net/pdf/639ca4d7460b732c0c9d399142939d60bcdb29d2.pdf",
      "primary_category": "Test-Time Scaling, Resource Allocation, Large Language Model",
      "categories": [
        "Test-Time Scaling",
        "Resource Allocation",
        "Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "D19hc2XPeZ",
      "title": "Robust LLM Alignment via Distributionally Robust Direct Preference Optimization",
      "authors": [
        "Zaiyan Xu",
        "Sushil Vemuri",
        "Kishan Panaganti",
        "Dileep Kalathil",
        "Rahul Jain",
        "Deepak Ramachandran"
      ],
      "abstract": "A major challenge in aligning large language models (LLMs) with human preferences is the issue of distribution shift. LLM alignment algorithms rely on static preference datasets, assuming that they accurately represent real-world user preferences. However, user preferences vary significantly across geographical regions, demographics, linguistic patterns, and evolving cultural trends. This preference distribution shift leads to catastrophic alignment failures in many real-world applications.  We address this problem using the principled framework of distributionally robust optimization, and develop two novel distributionally robust direct preference optimization (DPO) algorithms, namely, Wasserstein DPO (WDPO) and Kullback–Leibler DPO (KLDPO). We characterize the sample complexity of learning the optimal policy parameters for WDPO and KLDPO. Moreover, we propose scalable gradient descent-style learning algorithms by developing suitable approximations for the challenging minimax loss functions of  WDPO and KLDPO.  Our empirical experiments using benchmark data sets and LLMs demonstrate the superior performance of  WDPO and KLDPO in substantially improving the alignment when there is a preference distribution shift.",
      "arxiv_url": "https://openreview.net/forum?id=D19hc2XPeZ",
      "pdf_url": "https://openreview.net/pdf/84f0dbff8f6f61f16440237d67fc91fd94fa0b4d.pdf",
      "primary_category": "Large Language Model Fine-tuning, Reinforcement Learning from Human Feedback, Direct Preference Optimization",
      "categories": [
        "Large Language Model Fine-tuning",
        "Reinforcement Learning from Human Feedback",
        "Direct Preference Optimization",
        "Distributionally Robust Optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DaNnkQJSQf",
      "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models",
      "authors": [
        "Mateusz Pach",
        "Shyamgopal Karthik",
        "Quentin Bouniot",
        "Serge Belongie",
        "Zeynep Akata"
      ],
      "abstract": "Sparse Autoencoders (SAEs) have recently gained attention as a means to improve the interpretability and steerability of Large Language Models (LLMs), both of which are essential for AI safety. In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity at the neuron-level in visual representations. To ensure that our evaluation aligns with human perception, we propose a benchmark derived from a large-scale user study. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons, with sparsity and wide latents being the most influential factors. Further, we demonstrate that applying SAE interventions on CLIP's vision encoder directly steers multimodal LLM outputs (e.g., LLaVA), without any modifications to the underlying language model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised tool for enhancing both interpretability and control of VLMs.\nCode and benchmark data are available at https://github.com/ExplainableML/sae-for-vlm.",
      "arxiv_url": "https://openreview.net/forum?id=DaNnkQJSQf",
      "pdf_url": "https://openreview.net/pdf/17a5ab967eb24c1d35fb0c6dcf2a6c0ccbd236df.pdf",
      "primary_category": "sparse autoencoders, interpretability, steering",
      "categories": [
        "sparse autoencoders",
        "interpretability",
        "steering",
        "vision and language models"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ycMpNwzUAA",
      "title": "MetaDefense: Defending Fine-tuning based Jailbreak Attack Before and During Generation",
      "authors": [
        "Weisen Jiang",
        "Sinno Jialin Pan"
      ],
      "abstract": "This paper introduces MetaDefense, a novel framework for defending against finetuning-based jailbreak attacks in large language models (LLMs). \nWe observe that existing defense mechanisms fail to generalize to harmful queries disguised by unseen attack templates, despite LLMs being capable of distinguishing disguised harmful queries in the embedding space. \nBased on these insights, we propose a two-stage defense approach: \n(i) pre-generation defense that detects harmful queries before response generation begins, and (ii) mid-generation defense that monitors partial responses during generation to prevent outputting more harmful content. \nOur MetaDefense trains the LLM to predict the harmfulness of both queries and partial responses using specialized prompts, enabling early termination of potentially harmful interactions. \nExtensive experiments across multiple LLM architectures (LLaMA-2-7B, Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense significantly outperforms existing defense mechanisms, achieving robust defense against harmful queries with seen and unseen attack templates while maintaining competitive performance on benign tasks.\nCode is available at [https://github.com/ws-jiang/MetaDefense](https://github.com/ws-jiang/MetaDefense).",
      "arxiv_url": "https://openreview.net/forum?id=ycMpNwzUAA",
      "pdf_url": "https://openreview.net/pdf/006cba1cf9e75c1f5fafd3bea5ee62d71f204085.pdf",
      "primary_category": "jailbreak attack",
      "categories": [
        "jailbreak attack"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2Qn6skg175",
      "title": "Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning",
      "authors": [
        "Xiao Han",
        "ZIMO ZHAO",
        "Wanyu Wang",
        "Maolin Wang",
        "Zitao Liu",
        "Yi Chang",
        "Xiangyu Zhao"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have emphasized the critical role of fine-tuning (FT) techniques in adapting LLMs to specific tasks, especially when retraining from scratch is computationally infeasible. Fine-tuning enables LLMs to leverage task- or domain-specific data, producing models that more effectively meet the requirements of targeted applications. However, conventional FT approaches often suffer from catastrophic forgetting and suboptimal data efficiency, limiting their real-world applicability. To address these challenges, this paper proposes DEAL, a novel framework that integrates Low-Rank Adaptation (LoRA) with a continuous fine-tuning strategy. By incorporating knowledge retention and adaptive parameter update modules, the framework mitigates the limitations of existing FT methods while maintaining efficiency. Experiments on 15 diverse datasets show that DEAL consistently outperforms baseline methods, yielding substantial gains in task accuracy and resource efficiency. These findings demonstrate the potential of our approach to advance continual adaptation in LLMs by enhancing task performance while improving resource efficiency. The source code is publicly available at https://github.com/Applied-Machine-Learning-Lab/DEAL.",
      "arxiv_url": "https://openreview.net/forum?id=2Qn6skg175",
      "pdf_url": "https://openreview.net/pdf/d88ced8a42aa35f1fc7d922e2366b3ba6fc30230.pdf",
      "primary_category": "Continual Learning, Parameter-Efficient Fine-Tuning, Large Language Model",
      "categories": [
        "Continual Learning",
        "Parameter-Efficient Fine-Tuning",
        "Large Language Model",
        "Low-Rank Adaptation",
        "Lifelong Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VeZkY3JjWV",
      "title": "Pixel Reasoner: Incentivizing Pixel Space Reasoning via Curiosity-Driven Reinforcement Learning",
      "authors": [
        "Alex Su",
        "Haozhe Wang",
        "Weiming Ren",
        "Fangzhen Lin",
        "Wenhu Chen"
      ],
      "abstract": "Chain-of-thought reasoning has significantly improved the performance of Large\nLanguage Models (LLMs) across various domains. However, this reasoning process has been confined exclusively to textual space, limiting its effectiveness in visually intensive tasks. To address this limitation, we introduce the concept of pixel-space reasoning. Within this novel framework, Vision-Language Models (VLMs) are equipped with a suite of visual reasoning operations, such as zoom-in and select-frame. These operations enable VLMs to directly inspect, interrogate, and infer from visual evidences, thereby enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space reasoning capabilities in VLMs presents notable challenges, including the model’s initially imbalanced competence and its reluctance to adopt the newly introduced pixel-space operations. We address these challenges through a two-phase training approach. The first phase employs instruction tuning on synthesized reasoning traces to familiarize the model with the novel visual operations. Following this, a reinforcement learning (RL) phase leverages a curiosity-driven reward scheme to balance exploration between pixel-space reasoning and textual reasoning. With these visual operations, VLMs can interact with complex visual inputs, such as information-rich images or videos to proactively gather necessary information. We demonstrate that this approach significantly improves VLM performance across diverse visual reasoning benchmarks. Our 7B model, Pixel-Reasoner, achieves 84% on V* bench, 74% on TallyQA-Complex, and 84% on InfographicsVQA, marking the highest accuracy achieved by any open-source model to date. These results highlight the importance of pixel-space reasoning and the effectiveness of our framework.",
      "arxiv_url": "https://openreview.net/forum?id=VeZkY3JjWV",
      "pdf_url": "https://openreview.net/pdf/1cff63c2ec284033d02664f1b348bc05165216ea.pdf",
      "primary_category": "Vision-Language Models, Reinforcement Learning",
      "categories": [
        "Vision-Language Models",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Rgk129n73h",
      "title": "Reliable Decision‑Making via Calibration‑Oriented Retrieval‑Augmented Generation",
      "authors": [
        "Chaeyun Jang",
        "Deukhwan Cho",
        "Seanie Lee",
        "Hyungi Lee",
        "Juho Lee"
      ],
      "abstract": "Recently, Large Language Models (LLMs) have been increasingly used to support various decision-making tasks, assisting humans in making informed decisions. However, when LLMs confidently provide incorrect information, it can lead humans to make suboptimal decisions. To prevent LLMs from generating incorrect information on topics they are unsure of and to improve the accuracy of generated content, prior works have proposed Retrieval Augmented Generation (RAG), where external documents are referenced to generate responses. However, previous RAG methods focus only on retrieving documents most relevant to the input query, without specifically aiming to ensure that the human user's decisions are well-calibrated. To address this limitation, we propose a novel retrieval method called Calibrated Retrieval-Augmented Generation (CalibRAG), which ensures that decisions informed by RAG are well-calibrated. Then we empirically validate that CalibRAG improves calibration performance as well as accuracy, compared to other baselines across various datasets.",
      "arxiv_url": "https://openreview.net/forum?id=Rgk129n73h",
      "pdf_url": "https://openreview.net/pdf/acaaf0a9c6251733a867b8b7a4828db665154c02.pdf",
      "primary_category": "Calibration, Uncertainty, LLMs",
      "categories": [
        "Calibration",
        "Uncertainty",
        "LLMs",
        "RAG"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "jaMPaFDAaZ",
      "title": "FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing",
      "authors": [
        "Shoutao Guo",
        "Shaolei Zhang",
        "Qingkai Fang",
        "Zhengrui Ma",
        "Min zhang",
        "Yang Feng"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) has spurred significant progress in Large Speech-Language Models (LSLMs), enhancing their capabilities in both speech understanding and generation. While existing LSLMs often concentrate on augmenting speech generation or tackling a diverse array of short-speech tasks, the efficient processing of long-form speech remains a critical yet underexplored challenge. This gap is primarily attributed to the scarcity of long-speech training datasets and the high computational costs associated with long sequences. To address these limitations, we introduce FastLongSpeech, a novel framework designed to extend LSLM capabilities for efficient long-speech processing without necessitating dedicated long-speech training data. FastLongSpeech incorporates an iterative fusion strategy that can compress excessively long-speech sequences into manageable lengths. To adapt LSLMs for long-speech inputs, it introduces a dynamic compression training approach, which exposes the model to short-speech sequences at varying compression ratios, thereby transferring the capabilities of LSLMs to long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop a long-speech understanding benchmark called LongSpeech-Eval. Experiments show that our method exhibits strong performance in both long-speech and short-speech tasks, while greatly improving inference efficiency.",
      "arxiv_url": "https://openreview.net/forum?id=jaMPaFDAaZ",
      "pdf_url": "https://openreview.net/pdf/ed9a9ed39891b0f6e01c98f12a13df9dc4c3be10.pdf",
      "primary_category": "Spoken Language Understanding, Large Speech-Language Models, Speech Technologies",
      "categories": [
        "Spoken Language Understanding",
        "Large Speech-Language Models",
        "Speech Technologies"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "LtON4qr1if",
      "title": "On Efficiency-Effectiveness Trade-off of Diffusion-based Recommenders",
      "authors": [
        "Wenyu Mao",
        "Jiancan Wu",
        "Guoqing Hu",
        "Zhengyi Yang",
        "Wei Ji",
        "Xiang Wang"
      ],
      "abstract": "Diffusion models have emerged as a powerful paradigm for generative sequential recommendation, which typically generate next items to recommend guided by user interaction histories with a multi-step denoising process. However, the multi-step process relies on discrete approximations, introducing discretization error that creates a trade-off between computational efficiency and recommendation effectiveness.\nTo address this trade-off, we propose TA-Rec, a two-stage framework that achieves one-step generation by smoothing the denoising function during pretraining while alleviating trajectory deviation by aligning with user preferences during fine-tuning. Specifically, to improve the efficiency without sacrificing the recommendation performance, TA-Rec pretrains the denoising model with Temporal Consistency Regularization (TCR), enforcing the consistency between the denoising results across adjacent steps. Thus, we can smooth the denoising function to map the noise as oracle items in one step with bounded error. To further enhance effectiveness, TA-Rec introduces Adaptive Preference Alignment (APA) that aligns the denoising process with user preference adaptively based on preference pair similarity and timesteps. Extensive experiments prove that TA-Rec’s two-stage objective effectively mitigates the discretization errors-induced trade-off, enhancing both efficiency and effectiveness of diffusion-based recommenders. Our code is available at https://github.com/maowenyu-11/TA-Rec.",
      "arxiv_url": "https://openreview.net/forum?id=LtON4qr1if",
      "pdf_url": "https://openreview.net/pdf/28abafe6d0550c52fc2cef3c3ea532c12ca56eba.pdf",
      "primary_category": "Diffusion Models, Recommender System, Preference Optimization",
      "categories": [
        "Diffusion Models",
        "Recommender System",
        "Preference Optimization"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "v6kyF3S7dM",
      "title": "Flex-Judge: Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators",
      "authors": [
        "Jongwoo Ko",
        "Sungnyun Kim",
        "Sungwoo Cho",
        "Se-Young Yun"
      ],
      "abstract": "Human-generated reward signals are critical for aligning generative models with human preferences, guiding both training and inference-time evaluations. While large language models (LLMs) employed as proxy evaluators, i.e., LLM-as-a-Judge, significantly reduce the costs associated with manual annotations, they typically require extensive modality-specific training data and fail to generalize well across diverse multimodal tasks. In this paper, we propose **Flex-Judge**, a reasoning-guided multimodal judge model that leverages minimal textual reasoning data to robustly generalize across multiple modalities and evaluation formats. Our core intuition is that structured textual reasoning explanations inherently encode generalizable decision-making patterns, enabling an effective transfer to multimodal judgments, e.g., with images or videos. Empirical results demonstrate that Flex-Judge, despite being trained on significantly fewer text data, achieves competitive or superior performance compared to state-of-the-art commercial APIs and extensively trained multimodal evaluators. Notably, Flex-Judge presents broad impact in modalities like molecule, where comprehensive evaluation benchmarks are scarce, underscoring its practical value in resource-constrained domains. Our framework highlights reasoning-based text supervision as a powerful, cost-effective alternative to traditional annotation-intensive approaches, substantially advancing scalable multimodal model-as-a-judge.",
      "arxiv_url": "https://openreview.net/forum?id=v6kyF3S7dM",
      "pdf_url": "https://openreview.net/pdf/0895cf6fc3c5ff656996f38666b8fb528c975fb4.pdf",
      "primary_category": "mllm-as-a-judge, automatic evaluation, multimodal reasoning",
      "categories": [
        "mllm-as-a-judge",
        "automatic evaluation",
        "multimodal reasoning",
        "multimodal large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1mokb8ohOQ",
      "title": "ForgerySleuth: Empowering Multimodal Large Language Models for Image Manipulation Detection",
      "authors": [
        "Zhihao Sun",
        "Haoran Jiang",
        "Haoran Chen",
        "Yixin Cao",
        "Xipeng Qiu",
        "Zuxuan Wu",
        "Yu-Gang Jiang"
      ],
      "abstract": "Multimodal large language models have unlocked new possibilities for various multimodal tasks. However, their potential in image manipulation detection remains unexplored. When directly applied to the IMD task, M-LLMs often produce reasoning texts that suffer from hallucinations and overthinking. To address this, we propose ForgerySleuth, which leverages M-LLMs to perform comprehensive clue fusion and generate segmentation outputs indicating specific regions that are tampered with. Moreover, we construct the ForgeryAnalysis dataset through the Chain-of-Clues prompt, which includes analysis and reasoning text to upgrade the image manipulation detection task. A data engine is also introduced to build a larger-scale dataset for the pre-training phase. Our extensive experiments demonstrate the effectiveness of ForgeryAnalysis and show that ForgerySleuth significantly outperforms existing methods in generalization, robustness, and explainability.",
      "arxiv_url": "https://openreview.net/forum?id=1mokb8ohOQ",
      "pdf_url": "https://openreview.net/pdf/5517edb2d1bec74828178b92a3a4b93d6916ec3b.pdf",
      "primary_category": "Image Manipulation Detection Assistant, Chain-of-Clues, Multimodal Large Language Model",
      "categories": [
        "Image Manipulation Detection Assistant",
        "Chain-of-Clues",
        "Multimodal Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "jQn9oYY4sz",
      "title": "On the Role of Hidden States of Modern Hopfield Network in Transformer",
      "authors": [
        "Tsubasa Masumura",
        "Masato Taki"
      ],
      "abstract": "Associative memory models based on Hopfield networks and self-attention based on key-value mechanisms have been popular approaches in the study of memory mechanisms in deep learning. It has been pointed out that the state update rule of the modern Hopfield network (MHN) in the adiabatic approximation is in agreement with the self-attention layer of Transformer. In this paper, we go beyond this approximation and investigate the relationship between MHN and self-attention. Our results show that the correspondence between Hopfield networks and Transformers can be established in a more generalized form by adding a new variable, the hidden state derived from the MHN, to self-attention. This new attention mechanism, modern Hopfield attention (MHA), allows the inheritance of attention scores from the input layer of the Transformer to the output layer, which greatly improves the nature of attention weights. In particular, we show both theoretically and empirically that MHA hidden states significantly improve serious problem of deep Transformers known as rank collapse and token uniformity. We also confirm that MHA can systematically improve accuracy without adding training parameters to the Vision Transformer or GPT. Our results provide a new case in which Hopfield networks can be a useful perspective for improving the Transformer architecture.",
      "arxiv_url": "https://openreview.net/forum?id=jQn9oYY4sz",
      "pdf_url": "https://openreview.net/pdf/33e2b5f534ac5d3dd00b5820c6bb75b193c00042.pdf",
      "primary_category": "hopfield networks, self-attention, Transformer",
      "categories": [
        "hopfield networks",
        "self-attention",
        "Transformer",
        "rank collapse"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9GN5Jsa3lv",
      "title": "Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families",
      "authors": [
        "Felipe Maia Polo",
        "Seamus Somerstep",
        "Leshem Choshen",
        "Yuekai Sun",
        "Mikhail Yurochkin"
      ],
      "abstract": "Scaling laws for large language models (LLMs) predict model performance based on parameters like size and training data. However, differences in training configurations and data processing across model families lead to significant variations in benchmark performance, making it difficult for a single scaling law to generalize across all LLMs. On the other hand, training family-specific scaling laws requires training models of varying sizes for every family. In this work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a novel scaling law that leverages publicly available benchmark data and assumes LLM performance is driven by low-dimensional latent skills, such as reasoning and instruction following. These latent skills are influenced by computational resources like model size and training tokens, but with varying efficiencies across model families. Sloth exploits correlations across benchmarks to provide more accurate and interpretable predictions while alleviating the need to train multiple LLMs per family. We present both theoretical results on parameter identification and empirical evaluations on 12 prominent benchmarks, from Open LLM Leaderboard v1/v2, demonstrating that Sloth predicts LLM performance accurately and offers insights into scaling behaviors for complex downstream tasks, increased test-time compute, and compute-optimal scaling of skills.",
      "arxiv_url": "https://openreview.net/forum?id=9GN5Jsa3lv",
      "pdf_url": "https://openreview.net/pdf/82fec2c6ee0cec1161d232d81bfad1d63de8fd54.pdf",
      "primary_category": "scaling law, LLM, benchmark",
      "categories": [
        "scaling law",
        "LLM",
        "benchmark",
        "skill"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "41mBrhy6U4",
      "title": "Provably Efficient Online RLHF with One-Pass Reward Modeling",
      "authors": [
        "Long-Fei Li",
        "Yu-Yang Qian",
        "Peng Zhao",
        "Zhi-Hua Zhou"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has shown remarkable success in aligning Large Language Models (LLMs) with human preferences. Traditional RLHF methods rely on a fixed dataset, which often suffers from limited coverage. To this end, online RLHF has emerged as a promising direction, enabling iterative data collection and refinement. Despite its potential, this paradigm faces a key bottleneck: the requirement to continuously integrate new data into the dataset and re-optimize the model from scratch at each iteration, resulting in computational and storage costs that grow linearly with the number of iterations. In this work, we address this challenge by proposing a *one-pass* reward modeling method that eliminates the need to store historical data and achieves constant-time updates per iteration. Specifically, we first formalize RLHF as a contextual preference bandit and develop a new algorithm based on online mirror descent with a tailored local norm, replacing the standard maximum likelihood estimation for reward modeling. We then apply it to various online RLHF settings, including passive data collection, active data collection, and deployment-time adaptation. We provide theoretical guarantees showing that our method enhances both statistical and computational efficiency. Finally, we design practical algorithms for LLMs and conduct experiments with the Llama-3-8B-Instruct and Qwen2.5-7B-Instruct models on Ultrafeedback and Mixture2 datasets, validating the effectiveness of our approach.",
      "arxiv_url": "https://openreview.net/forum?id=41mBrhy6U4",
      "pdf_url": "https://openreview.net/pdf/2b4a45acffa783ad5fe1dedba55f616925717fd4.pdf",
      "primary_category": "RLHF, LLMs, reinforcement learning",
      "categories": [
        "RLHF",
        "LLMs",
        "reinforcement learning",
        "online learning",
        "online mirror descent",
        "contextual bandit"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Kvsa8ZXd0W",
      "title": "Data Mixture Optimization: A Multi-fidelity Multi-scale Bayesian Framework",
      "authors": [
        "Thomson Yen",
        "Andrew Wei Tung Siah",
        "Haozhe Chen",
        "C. Daniel Guetta",
        "Tianyi Peng",
        "Hongseok Namkoong"
      ],
      "abstract": "Careful curation of data sources can significantly improve the performance of LLM pre-training, but predominant approaches rely heavily on intuition or costly trial-and-error, making them difficult to generalize across different data domains and downstream tasks. Although scaling laws can provide a principled and general approach for data curation, standard deterministic extrapolation from small-scale experiments to larger scales requires strong assumptions on the reliability of such extrapolation, whose brittleness has been highlighted in prior works. In this paper, we introduce a probabilistic extrapolation framework for data mixture optimization that avoids rigid assumptions and explicitly models the uncertainty in performance across decision variables. We formulate data curation as a sequential decision-making problem–multi-fidelity, multi-scale Bayesian optimization–where {data mixtures, model scale, training steps} are adaptively selected to balance training cost and potential information gain. Our framework naturally gives rise to algorithm prototypes that leverage noisy information from inexpensive experiments to systematically inform costly training decisions. To accelerate methodological progress, we build a simulator based on 472 language model pre-training runs with varying data compositions from the SlimPajama dataset. We observe that even simple kernels and acquisition functions can enable principled decisions across training models from 20M to 1B parameters and achieve 2.6x and 3.3x speedups compared to multi-fidelity BO and random search baselines. Taken together, our framework underscores potential efficiency gains achievable by developing principled and transferable data mixture optimization methods. Our code is publicly available at https://github.com/namkoong-lab/data-recipes.",
      "arxiv_url": "https://openreview.net/forum?id=Kvsa8ZXd0W",
      "pdf_url": "https://openreview.net/pdf/f1eddcad0054767c1a8bd1897c35acc74aca8f10.pdf",
      "primary_category": "language models, Bayesian optimization, multi-fidelity optimization",
      "categories": [
        "language models",
        "Bayesian optimization",
        "multi-fidelity optimization",
        "data mixture",
        "transfer learning",
        "adaptive experimentation",
        "model scaling",
        "hyperparameter optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "heJ7NRInjs",
      "title": "RSafe: Incentivizing proactive reasoning to build robust and adaptive  LLM safeguards",
      "authors": [
        "Jingnan Zheng",
        "Xiangtian Ji",
        "Yijun Lu",
        "Chenhang Cui",
        "Weixiang Zhao",
        "Gelei Deng",
        "Zhenkai Liang",
        "An Zhang",
        "Tat-Seng Chua"
      ],
      "abstract": "Large Language Models (LLMs) continue to exhibit vulnerabilities despite deliberate safety alignment efforts, posing significant risks to users and society. To safeguard against the risk of policy-violating content, system-level moderation via external guard models—designed to monitor LLM inputs and outputs and block potentially harmful content—has emerged as a prevalent mitigation strategy. Existing approaches of training guard models rely heavily on extensive human curated datasets and struggle with out-of-distribution threats, such as emerging harmful categories or jailbreak attacks. To address these limitations, we propose RSafe, an adaptive reasoning-based safeguard that conducts guided safety reasoning to provide robust protection within the scope of specified safety policies. RSafe operates\nin two stages: (1) guided reasoning, where it analyzes safety risks of input content through policy-guided step-by-step reasoning, and (2) reinforced alignment, where rule-based RL optimizes its reasoning paths to align with accurate safety prediction. This two-stage training paradigm enables RSafe to internalize safety principles to generalize safety protection capability over unseen or adversarial safety violation\nscenarios. During inference, RSafe accepts user-specified safety policies to provide enhanced safeguards tailored to specific safety requirements. Experiments demonstrate that RSafe matches state-of-the-art guard models using limited amount of public data in both prompt- and response-level harmfulness detection, while achieving superior out-of-distribution generalization on both emerging harmful category and jailbreak attacks. Furthermore, RSafe provides human-readable explanations for its safety judgments for better interpretability. RSafe offers a robust, adaptive, and interpretable solution for LLM safety moderation, advancing the development of reliable safeguards in dynamic real-world environments. Our code is available at https://anonymous.4open.science/r/RSafe-996D.",
      "arxiv_url": "https://openreview.net/forum?id=heJ7NRInjs",
      "pdf_url": "https://openreview.net/pdf/04dac969863242564dac530962d0596f2d7cf2ad.pdf",
      "primary_category": "large language model, safety, moderation",
      "categories": [
        "large language model",
        "safety",
        "moderation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "fVgnP5WHXX",
      "title": "VADTree: Explainable Training-Free Video Anomaly Detection via Hierarchical Granularity-Aware Tree",
      "authors": [
        "Wenlong Li",
        "Yifei Xu",
        "Yuan Rao",
        "Zhenhua Wang",
        "Shuiguang Deng"
      ],
      "abstract": "Video anomaly detection (VAD) focuses on identifying anomalies in videos. Su-\npervised methods demand substantial in-domain training data and fail to deliver\nclear explanations for anomalies. In contrast, training-free methods leverage\nthe knowledge reserves and language interactivity of large pre-trained models\nto detect anomalies. However, the current fixed-length temporal window sam-\npling approaches struggle to accurately capture anomalies with varying temporal\nspans. Therefore, we propose VADTree that utilizes a Hierarchical Granularity-\naware Tree (HGTree) structure for flexible sampling in VAD. VADTree leverages\nthe knowledge embedded in a pre-trained Generic Event Boundary Detection\n(GEBD) model to characterize potential anomaly event boundaries. Specifically,\nVADTree decomposes the video into generic event nodes based on boundary\nconfidence, and performs adaptive coarse-fine hierarchical structuring and re-\ndundancy removal to construct the HGTree. Then, the multi-dimensional priors\nare injected into the visual language models (VLMs) to enhance the node-wise\nanomaly perception, and anomaly reasoning for generic event nodes is achieved\nvia large language models (LLMs). Finally, an inter-cluster node correlation\nmethod is used to integrate the multi-granularity anomaly scores. Extensive\nexperiments on three challenging datasets demonstrate that VADTree achieves\nstate-of-the-art performance in training-free settings while drastically reducing\nthe number of sampled video segments. The code will be available at https:\n//github.com/wenlongli10/VADTree.",
      "arxiv_url": "https://openreview.net/forum?id=fVgnP5WHXX",
      "pdf_url": "https://openreview.net/pdf/b35b120e2603cb56836d1a5fb58e720c956d47c8.pdf",
      "primary_category": "Video Anomaly Detection, Training-Free, Generic Event Boundary",
      "categories": [
        "Video Anomaly Detection",
        "Training-Free",
        "Generic Event Boundary",
        "Granularity-Aware"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9xXjWwAoUF",
      "title": "A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1",
      "authors": [
        "Zhaoyi Li",
        "Xiaohan Zhao",
        "Dong-Dong Wu",
        "Jiacheng Cui",
        "Zhiqiang Shen"
      ],
      "abstract": "Despite promising performance on open-source large vision-language models (LVLMs), transfer-based targeted attacks often fail against closed-source commercial LVLMs. Analyzing failed adversarial perturbations reveals that the learned perturbations typically originate from a uniform distribution and lack clear semantic details, resulting in unintended responses. This critical absence of semantic information leads commercial black-box LVLMs to either ignore the perturbation entirely or misinterpret its embedded semantics, thereby causing the attack to fail. To overcome these issues, we propose to refine semantic clarity by encoding explicit semantic details within local regions, thus ensuring the capture of finer-grained features and inter-model transferability, and by concentrating modifications on semantically rich areas rather than applying them uniformly. To achieve this, we propose  *a simple yet highly effective baseline*: at each optimization step, the adversarial image is cropped randomly by a controlled aspect ratio and scale, resized, and then aligned with the target image in the embedding space. While the naive source-target matching method has been utilized before in the literature, we are the first to provide a tight analysis, which establishes a close connection between perturbation optimization and semantics. Experimental results confirm our hypothesis. Our adversarial examples crafted with local-aggregated perturbations focused on crucial regions exhibit surprisingly good transferability to commercial LVLMs, including GPT-4.5, GPT-4o, Gemini-2.0-flash, Claude-3.5/3.7-sonnet, and even reasoning models like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach achieves success rates exceeding 90\\% on GPT-4.5, 4o, and o1, significantly outperforming all prior state-of-the-art attack methods with lower $\\ell_1/\\ell_2$ perturbations. Our optimized adversarial examples under different configurations are available at https://huggingface.co/datasets/MBZUAI-LLM/M-Attack_AdvSamples and our training code at https://github.com/VILA-Lab/M-Attack.",
      "arxiv_url": "https://openreview.net/forum?id=9xXjWwAoUF",
      "pdf_url": "https://openreview.net/pdf/5949721f82d1ac16063070f0d6dd34dca5f2403e.pdf",
      "primary_category": "Adversraial Attack, Local-level Matching, Vision-Language Models",
      "categories": [
        "Adversraial Attack",
        "Local-level Matching",
        "Vision-Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qMm7tC1zvj",
      "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval",
      "authors": [
        "Zebin Yang",
        "Sunjian Zheng",
        "Tong Xie",
        "Tianshi Xu",
        "Bo Yu",
        "Fan Wang",
        "Jie Tang",
        "Shaoshan Liu",
        "Meng Li"
      ],
      "abstract": "Object-goal navigation (ObjNav) tasks an agent with navigating to the location of a specific object in an unseen environment. \nEmbodied agents equipped with large language models (LLMs) and online constructed navigation maps can perform ObjNav in a zero-shot manner. However, existing agents heavily rely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small LLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to limited model capacity for understanding complex navigation maps, which prevents deploying ObjNav on local devices.\nAt the same time, the long prompt introduced by the navigation map description will cause high planning latency on local devices.\nIn this paper, we propose EfficientNav to enable on-device efficient LLM-based zero-shot ObjNav. To help the smaller LLMs better understand the environment, we propose semantics-aware memory retrieval to prune redundant information in navigation maps.\nTo reduce planning latency, we propose discrete memory caching and attention-based memory clustering to efficiently save and re-use the KV cache.\nExtensive experimental results demonstrate that EfficientNav\nachieves 11.1\\% improvement in success rate on HM3D benchmark over GPT-4-based baselines, \nand demonstrates 6.7$\\times$ real-time latency reduction and 4.7$\\times$ end-to-end latency reduction over GPT-4 planner. Our code is available on https://github.com/PKU-SEC-Lab/EfficientNav.",
      "arxiv_url": "https://openreview.net/forum?id=qMm7tC1zvj",
      "pdf_url": "https://openreview.net/pdf/2001e2272aed429a8f96bf544c6ac5d5d8645cfb.pdf",
      "primary_category": "Object-Goal Navigation, Navigation Map, Memory Caching",
      "categories": [
        "Object-Goal Navigation",
        "Navigation Map",
        "Memory Caching",
        "Memory Retrieval"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "o2y6BS6mm0",
      "title": "Don’t Forget the Enjoin: FocalLoRA for Instruction Hierarchical Alignment in Large Language Models",
      "authors": [
        "Zitong Shi",
        "Guancheng Wan",
        "Haixin Wang",
        "Ruoyan Li",
        "Zijie Huang",
        "Wanjia Zhao",
        "Yijia Xiao",
        "Xiao Luo",
        "Carl Yang",
        "Yizhou Sun",
        "Wei Wang"
      ],
      "abstract": "Recent studies reveal that large language models (LLMs) often struggle to resolve conflicting instructions embedded within hierarchical prompts, resulting in decreased compliance with system-level directives and compromising the reliability of safety-critical applications. While earlier approaches attempt to improve instruction hierarchy awareness through prompt engineering or embedding-level modifications, they typically lack structural modeling and either offer limited gains or require extensive fine-tuning. In this work, we introduce $\\textbf{FocalLoRA}$, a parameter-efficient and structure-aware framework that strengthens hierarchical instruction adherence by selectively optimizing structurally critical attention heads, referred to as $\\textit{focal heads}$, which exhibit heightened sensitivity to instruction conflicts. Experiments across multiple models and a dedicated benchmark demonstrate that FocalLoRA markedly enhances system instruction compliance with minimal tuning cost. For instance, on Llama-8B, fine-tuning only 0.0188\\% of parameters yields a 35.52\\% $\\uparrow$ in system instruction compliance.",
      "arxiv_url": "https://openreview.net/forum?id=o2y6BS6mm0",
      "pdf_url": "https://openreview.net/pdf/7eb3bb533d24e9cb1ccd7e8725fec5b7937b2447.pdf",
      "primary_category": "Instruction Hierarchy, Large Language Models",
      "categories": [
        "Instruction Hierarchy",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oJ84bedrtM",
      "title": "MokA: Multimodal Low-Rank Adaptation for MLLMs",
      "authors": [
        "Yake Wei",
        "Yu Miao",
        "Dongzhan Zhou",
        "Di Hu"
      ],
      "abstract": "In this paper, we reveal that most current efficient multimodal fine-tuning methods are hindered by a key limitation: they are directly borrowed from LLMs, often neglecting the intrinsic differences of multimodal scenarios and even affecting the full utilization of all modalities. Inspired by our empirical observation, we argue that unimodal adaptation and cross-modal adaptation are two essential parts for the effective fine-tuning of MLLMs. From this perspective, we propose Multimodal Low-rank Adaptation (MokA), a multimodal-aware efficient fine-tuning strategy that takes multimodal characteristics into consideration. It compresses unimodal information by modality-specific parameters while explicitly enhancing cross-modal interaction, ensuring both unimodal and cross-modal adaptation. Extensive experiments cover three representative multimodal scenarios (audio-visual-text, visual-text, and speech-text), and multiple LLM backbones (LLaMA2, Qwen2, Qwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility of the proposed method. Ablation studies and efficiency evaluation are also conducted to fully asses our method. Overall, we think MokA provides a more targeted solution for efficient adaptation of MLLMs, paving the way for further exploration.",
      "arxiv_url": "https://openreview.net/forum?id=oJ84bedrtM",
      "pdf_url": "https://openreview.net/pdf/14a2dcbc7353e7f55e037453563c792800c99220.pdf",
      "primary_category": "Multimodal, MLLMs, PEFT",
      "categories": [
        "Multimodal",
        "MLLMs",
        "PEFT",
        "fune-tuning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nHkg4yc7SP",
      "title": "Improving Video Generation with Human Feedback",
      "authors": [
        "Jie Liu",
        "Gongye Liu",
        "Jiajun Liang",
        "Ziyang Yuan",
        "Xiaokun Liu",
        "Mingwu Zheng",
        "Xiele Wu",
        "Qiulin Wang",
        "Menghan Xia",
        "Xintao Wang",
        "Xiaohong Liu",
        "Fei Yang",
        "Pengfei Wan",
        "Di ZHANG",
        "Kun Gai",
        "Yujiu Yang",
        "Wanli Ouyang"
      ],
      "abstract": "Video generation has achieved significant advances through rectified flow techniques, but issues like unsmooth motion and misalignment between videos and prompts persist. In this work, we develop a systematic pipeline that harnesses human feedback to mitigate these problems and refine the video generation model. Specifically, we begin by constructing a large-scale human preference dataset focused on modern video generation models, incorporating pairwise annotations across multi-dimensions. We then introduce VideoReward, a multi-dimensional video reward model, and examine how annotations and various design choices impact its rewarding efficacy. From a unified reinforcement learning perspective aimed at maximizing reward with KL regularization, we introduce three alignment algorithms for flow-based models. These include two training-time strategies: direct preference optimization for flow (Flow-DPO) and reward weighted regression for flow (Flow-RWR), and an inference-time technique, Flow-NRG, which applies reward guidance directly to noisy videos. Experimental results indicate that VideoReward significantly outperforms existing reward models, and Flow-DPO demonstrates superior performance compared to both Flow-RWR and supervised fine-tuning methods. Additionally, Flow-NRG lets users assign custom weights to multiple objectives during inference, meeting personalized video quality needs.",
      "arxiv_url": "https://openreview.net/forum?id=nHkg4yc7SP",
      "pdf_url": "https://openreview.net/pdf/9300b9bfa061e0fc09e2514b1515f9d8296c8da3.pdf",
      "primary_category": "video generation, human feedback, flow matching",
      "categories": [
        "video generation",
        "human feedback",
        "flow matching"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3JUhkxVlyF",
      "title": "Mechanism Design for LLM Fine-tuning with Multiple Reward Models",
      "authors": [
        "Haoran Sun",
        "Yurong Chen",
        "Siwei Wang",
        "Xu Chu",
        "Wei Chen",
        "Xiaotie Deng"
      ],
      "abstract": "Fine-tuning large language models (LLMs) to aggregate multiple preferences has attracted considerable research attention. With aggregation algorithms advancing, a potential economic scenario arises where fine-tuning services are provided to agents with different preferences. In this context, agents may benefit from strategically misreporting their preferences, but this could harm the aggregation performance. This paper addresses such incentive issues by framing it as a mechanism design problem: an LLM provider determines the fine-tuning objective (training rule) and the pricing scheme (payment rule) for agents. We primarily focus on training rules that maximize social welfare subject to certain regularizations, referred to as SW-Max rules. First, we show that under most circumstances, truthful reporting is sub-optimal with simply a SW-Max rule, thereby highlighting the necessity of payments. Second, we extend the VCG payment to implement SW-Max rules in dominant-strategy incentive compatibility (DSIC). We characterize sufficient conditions for payment equivalence and derive the necessary conditions for a payment rule to implement a SW-Max rule in DSIC and other principles. Third, we demonstrate that our mechanism is approximately DSIC with perturbed input, showcasing its robustness against the inevitable errors in real-world applications. Experiments on real LLM training results further confirm the practical implications of our results.",
      "arxiv_url": "https://openreview.net/forum?id=3JUhkxVlyF",
      "pdf_url": "https://openreview.net/pdf/9fc40d66213f6ef7830d37559447be1b085502a4.pdf",
      "primary_category": "Mechanism Design, RLHF, LLMs",
      "categories": [
        "Mechanism Design",
        "RLHF",
        "LLMs",
        "Incentive Compatibility"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "KQ9KCDS4zp",
      "title": "Foundation Cures Personalization: Improving Personalized Models’ Prompt Consistency via Hidden Foundation Knowledge",
      "authors": [
        "Yiyang Cai",
        "Zhengkai Jiang",
        "Yulong Liu",
        "Chunyang Jiang",
        "Wei Xue",
        "Yike Guo",
        "Wenhan Luo"
      ],
      "abstract": "Facial personalization faces challenges to maintain identity fidelity without disrupting the foundation model's prompt consistency. The mainstream personalization models employ identity embedding to integrate identity information within the attention mechanisms. However, our preliminary findings reveal that identity embeddings compromise the effectiveness of other tokens in the prompt, thereby limiting high prompt consistency and attribute-level controllability. Moreover, by deactivating identity embedding, personalization models still demonstrate the underlying foundation models' ability to control facial attributes precisely. It suggests that such foundation models' knowledge can be leveraged to cure the ill-aligned prompt consistency of personalization models. Building upon these insights, we propose FreeCure, a framework that improves the prompt consistency of personalization models with their latent foundation models' knowledge. First, by setting a dual inference paradigm with/without identity embedding, we identify attributes (e.g., hair, accessories, etc.) for enhancements. Second, we introduce a novel foundation-aware self-attention module, coupled with an inversion-based process to bring well-aligned attribute information to the personalization process. Our approach is training-free, and can effectively enhance a wide array of facial attributes; and it can be seamlessly integrated into existing popular personalization models based on both Stable Diffusion and FLUX. FreeCure has consistently shown significant improvements in prompt consistency across these facial personalization models while maintaining the integrity of their original identity fidelity.",
      "arxiv_url": "https://openreview.net/forum?id=KQ9KCDS4zp",
      "pdf_url": "https://openreview.net/pdf/025e967b3a4ad1f79cd45501ed3993491d57bbbb.pdf",
      "primary_category": "Image Generation, Persoanalization",
      "categories": [
        "Image Generation",
        "Persoanalization"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "hhg1TPk3RG",
      "title": "AnimateQR: Bridging Aesthetics and Functionality in Dynamic QR Code Generation",
      "authors": [
        "Guangyang Wu",
        "Huayu Zheng",
        "Siqi Luo",
        "Guangtao Zhai",
        "Xiaohong Liu"
      ],
      "abstract": "Animated QR codes present an exciting frontier for dynamic content delivery and digital interaction. However, despite their potential, there has been no prior work focusing on the generation of animated QR codes that are both visually appealing and universally scannable. In this paper, we introduce AnimateQR, **the first generative framework** for creating **animated QR codes** that balance aesthetic flexibility with scannability. Unlike previous methods that focus on static QR codes, AnimateQR leverages **hierarchical luminance guidance** and **progressive spatiotemporal control** to produce high-quality dynamic QR codes. Our first innovation is a multi-scale hierarchical control signal that adjusts luminance across different spatial scales, ensuring that the QR code remains decodable while allowing for artistic expression.  The second innovation is a progressive control mechanism that dynamically adjusts spatiotemporal guidance throughout the diffusion denoising steps, enabling fine-grained balance between visual quality and scannability. Extensive experimental results demonstrate that AnimateQR achieves state-of-the-art performance in both decoding success rates (96\\% vs. 56\\% baseline) and visual quality (user preference: 7.2 vs. 2.3 on a 10-point scale). Codes are availble at https://github.com/mulns/AnimateQR.",
      "arxiv_url": "https://openreview.net/forum?id=hhg1TPk3RG",
      "pdf_url": "https://openreview.net/pdf/e49b2c2a17aafc94e363837c5d5f96b1f06e2e6e.pdf",
      "primary_category": "ControlNet, AnimateDiff, diffusion",
      "categories": [
        "ControlNet",
        "AnimateDiff",
        "diffusion",
        "QR code"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ILZ7ZPEHD5",
      "title": "Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing",
      "authors": [
        "Yilmazcan Ozyurt",
        "Tunaberk Almaci",
        "Stefan Feuerriegel",
        "Mrinmaya Sachan"
      ],
      "abstract": "We introduce ExRec, a general framework for personalized exercise recommendation with semantically-grounded knowledge tracing. Our method builds on the observation that existing exercise recommendation approaches simulate student performance via knowledge tracing (KT) but they often overlook two key aspects: (a) the semantic content of questions and (b) the sequential, structured progression of student learning. To address this, our ExRec presents an end-to-end pipeline, from annotating the KCs of questions and learning their semantic representations to training KT models and optimizing several reinforcement learning (RL) methods. Moreover, we improve standard Q-learning-based continuous RL methods via a tailored model-based value estimation (MVE) approach that directly leverages the components of KT model in estimating cumulative knowledge improvement. We validate the effectiveness of our ExRec using various RL methods across four real-world tasks with different educational goals in online math learning. We further show that ExRec generalizes robustly to new, unseen questions and that it produces interpretable student learning trajectories. Together, our findings highlight the promise of KT-guided RL for effective personalization in education.",
      "arxiv_url": "https://openreview.net/forum?id=ILZ7ZPEHD5",
      "pdf_url": "https://openreview.net/pdf/4ce2cca2151ceb9c4e1ba1c20bf66bcf95d01fcf.pdf",
      "primary_category": "Knowledge Tracing, Exercise Recommendation, Reinforcement Learning",
      "categories": [
        "Knowledge Tracing",
        "Exercise Recommendation",
        "Reinforcement Learning",
        "Educational AI",
        "Semantic Representation Learning"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "iydmH9boLb",
      "title": "Advancing Expert Specialization for Better MoE",
      "authors": [
        "Hongcan Guo",
        "Haolang Lu",
        "Guoshun Nan",
        "Bolun Chu",
        "Jialin Zhuang",
        "Yuan Yang",
        "Wenhao Che",
        "Xinye Cao",
        "Sicong Leng",
        "Qimei Cui",
        "Xudong Jiang"
      ],
      "abstract": "Mixture-of-Experts (MoE) models enable efficient scaling of large language models (LLMs) by activating only a subset of experts per input. \nHowever, we observe that the commonly used auxiliary load balancing loss often leads to expert overlap and overly uniform routing, which hinders expert specialization and degrades overall performance during post-training.\nTo address this, we propose a simple yet effective solution that introduces two complementary objectives: (1) an orthogonality loss to encourage experts to process distinct types of tokens, and (2) a variance loss to encourage more discriminative routing decisions.\nGradient-level analysis demonstrates that these objectives are compatible with the existing auxiliary loss and contribute to optimizing the training process.\nExperimental results over various model architectures and across multiple benchmarks show that our method significantly enhances expert specialization. \nNotably, our method improves classic MoE baselines with auxiliary loss by up to 23.79\\%, while also maintaining load balancing in downstream tasks, without any architectural modifications or additional components. We will release our code to contribute to the community.",
      "arxiv_url": "https://openreview.net/forum?id=iydmH9boLb",
      "pdf_url": "https://openreview.net/pdf/799c3d91e78ae57d4619c5e6a3358cc7ce638f16.pdf",
      "primary_category": "Mixture of Experts, Load Balancing, Optimization",
      "categories": [
        "Mixture of Experts",
        "Load Balancing",
        "Optimization",
        "Supervised Fine-Tuning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "y7ahj9RoXQ",
      "title": "ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints",
      "authors": [
        "Rui Xu",
        "Dakuan Lu",
        "Zicheng Zhao",
        "Xiaoyu Tan",
        "Xintao Wang",
        "Siyu Yuan",
        "Jiangjie Chen",
        "Xu Yinghui"
      ],
      "abstract": "Spatial reasoning is a key capability in the field of artificial intelligence, especially crucial in areas such as robotics, computer vision, and natural language understanding. However, evaluating the ability of multimodal large language models (MLLMs) in complex spatial reasoning still faces challenges, particularly in scenarios requiring multi-step reasoning and precise mathematical constraints. This paper introduces ORIGAMISPACE, a new dataset and benchmark designed to evaluate the multi-step spatial reasoning ability and the capacity to handle mathematical constraints of MLLMs through origami tasks. The dataset contains 350 data instances, each comprising a strictly formatted crease pattern (CP diagram), the Compiled Flat Pattern, the complete Folding Process, and the final Folded Shape Image. We propose four evaluation tasks: Pattern Prediction, Multi-step Spatial Reasoning, Spatial Relationship Prediction, and End-to-End CP Code Generation. For the CP code generation task, we design an interactive environment and explore the possibility of using reinforcement learning methods to train MLLMs. Through experiments on existing MLLMs, we initially reveal the strengths and weaknesses of these models in handling complex spatial reasoning tasks.",
      "arxiv_url": "https://openreview.net/forum?id=y7ahj9RoXQ",
      "pdf_url": "https://openreview.net/pdf/2bc8b445588504ed12b491f7cc33f033191ae2ad.pdf",
      "primary_category": "spatial reasoning, multimodal large language model, benchmark",
      "categories": [
        "spatial reasoning",
        "multimodal large language model",
        "benchmark"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "36TBVGwzAY",
      "title": "GRIP: A Graph-Based Reasoning Instruction Producer",
      "authors": [
        "Jiankang Wang",
        "Jianjun Xu",
        "Xiaorui Wang",
        "Yuxin Wang",
        "Mengting Xing",
        "Shancheng Fang",
        "Hongtao Xie"
      ],
      "abstract": "Large-scale, high-quality data is essential for advancing the reasoning capabilities of large language models (LLMs). As publicly available Internet data becomes increasingly scarce, synthetic data has emerged as a crucial research direction. However, existing data synthesis methods often suffer from limited scalability, insufficient sample diversity, and a tendency to overfit to seed data, which constrains their practical utility. In this paper, we present \\textit{\\textbf{GRIP}}, a \\textbf{G}raph-based \\textbf{R}easoning \\textbf{I}nstruction \\textbf{P}roducer that efficiently synthesizes high-quality and diverse reasoning instructions. \\textit{GRIP} constructs a knowledge graph by extracting high-level concepts from seed data, and uniquely leverages both explicit and implicit relationships within the graph to drive large-scale and diverse instruction data synthesis, while employing open-source multi-model supervision to ensure data quality. We apply \\textit{GRIP} to the critical and challenging domain of mathematical reasoning. Starting from a seed set of 7.5K math reasoning samples, we construct \\textbf{GRIP-MATH}, a dataset containing 2.1 million synthesized question-answer pairs. Compared to similar synthetic data methods, \\textit{GRIP} achieves greater scalability and diversity while also significantly reducing costs. On mathematical reasoning benchmarks, models trained with GRIP-MATH demonstrate substantial improvements over their base models and significantly outperform previous data synthesis methods.",
      "arxiv_url": "https://openreview.net/forum?id=36TBVGwzAY",
      "pdf_url": "https://openreview.net/pdf/9ef2967c43fa97f6b4cfdddaf146abf156250794.pdf",
      "primary_category": "Large Language Models, Reasoning, Data Synthesis",
      "categories": [
        "Large Language Models",
        "Reasoning",
        "Data Synthesis"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "gIGtOg4DNa",
      "title": "MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization",
      "authors": [
        "Rizhen Hu",
        "Yutong He",
        "Ran Yan",
        "Mou Sun",
        "Binhang Yuan",
        "Kun Yuan"
      ],
      "abstract": "As distributed optimization scales to meet the demands of Large Language Model (LLM) training, hardware failures become increasingly non-negligible. Existing fault-tolerant training methods often introduce significant computational or memory overhead, demanding additional resources. To address this challenge, we propose **Me**mory- and **C**omputation- **e**fficient **F**ault-tolerant **O**ptimization (**MeCeFO**), a novel algorithm that ensures robust training with minimal overhead. When a computing node fails, MeCeFO seamlessly transfers its training task to a neighboring node while employing memory- and computation-efficient algorithmic optimizations to minimize the extra workload imposed on the neighboring node handling both tasks. MeCeFO leverages three key algorithmic designs: (i) Skip-connection, which drops the multi-head attention (MHA) module during backpropagation for memory- and computation-efficient approximation; (ii) Recomputation, which reduces activation memory in feedforward networks (FFNs); and (iii) Low-rank gradient approximation, enabling efficient estimation of FFN weight matrix gradients. Theoretically, MeCeFO matches the convergence rate of conventional distributed training, with a rate of $\\mathcal{O}(1/\\sqrt{nT})$, where $n$ is the data parallelism size and $T$ is the number of iterations. Empirically, MeCeFO maintains robust performance under high failure rates, incurring only a 4.18\\% drop in throughput, demonstrating $5.0\\times$ to $6.7\\times$ greater resilience than previous SOTA approaches.",
      "arxiv_url": "https://openreview.net/forum?id=gIGtOg4DNa",
      "pdf_url": "https://openreview.net/pdf/23030948cee6c70d3bf1caef23d39d1de2313446.pdf",
      "primary_category": "Fault tolerance, Memory efficiency, Computation efficiency",
      "categories": [
        "Fault tolerance",
        "Memory efficiency",
        "Computation efficiency",
        "Distributed training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "aNpj43Uh35",
      "title": "Multi-Objective One-Shot Pruning for Large Language Models",
      "authors": [
        "Weiyu Chen",
        "Hansi Yang",
        "Yunhao GOU",
        "Han Shi",
        "En-Liang Hu",
        "Zhenguo Li",
        "James Kwok"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but require substantial computational resources, limiting their deployment in resource-constrained environments. While one-shot pruning methods can reduce model size without expensive retraining, they typically optimize for single objectives, ignoring LLMs' multi-faceted applications. We introduce Multi-Objective One-Shot Pruning (MOSP), which formulates LLM pruning as a multi-objective optimization problem. MOSP efficiently generates a Pareto set of pruned models representing different capability trade-offs, allowing users to select solutions aligned with their preferences. The proposed approach identifies share core support while enabling specialized support. Experiments across various LLMs and sparsity levels demonstrate MOSP's superior performance in navigating multi-objective trade-offs compared to baseline methods.",
      "arxiv_url": "https://openreview.net/forum?id=aNpj43Uh35",
      "pdf_url": "https://openreview.net/pdf/57bc887b7564d9c4265d180d739174577189c519.pdf",
      "primary_category": "Multi-Objective Optimization, Pruning, Large Language Models",
      "categories": [
        "Multi-Objective Optimization",
        "Pruning",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "a3l3K9khbL",
      "title": "Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization",
      "authors": [
        "Yamato Arai",
        "Yuma Ichikawa"
      ],
      "abstract": "Layer-wise PTQ is a promising technique for compressing large language models (LLMs), due to its simplicity and effectiveness without requiring retraining. However, recent progress in this area is saturating, underscoring the need to revisit its core limitations and explore further improvements. We address this challenge by identifying a key limitation of existing layer-wise PTQ methods: the growth of quantization errors across layers significantly degrades performance, particularly in low-bit regimes. To address this fundamental issue, we propose Quantization Error Propagation (QEP), a general, lightweight, and scalable framework that enhances layer-wise PTQ by explicitly propagating quantization errors and compensating for accumulated errors. QEP also offers a tunable propagation mechanism that prevents overfitting and controls computational overhead, enabling the framework to adapt to various architectures and resource budgets. Extensive experiments on several LLMs demonstrate that QEP-enhanced layer-wise PTQ achieves substantially higher accuracy than existing methods. Notably, the gains are most pronounced in the extremely low-bit quantization regime.",
      "arxiv_url": "https://openreview.net/forum?id=a3l3K9khbL",
      "pdf_url": "https://openreview.net/pdf/9ee4e3fe907e128a7abeb472697f0aa5e7297263.pdf",
      "primary_category": "quantization, post-training quantization, layer-wise post-training quantization",
      "categories": [
        "quantization",
        "post-training quantization",
        "layer-wise post-training quantization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DbzREoPwmM",
      "title": "Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models",
      "authors": [
        "Luca Eyring",
        "Shyamgopal Karthik",
        "Alexey Dosovitskiy",
        "Nataniel Ruiz",
        "Zeynep Akata"
      ],
      "abstract": "The new paradigm of test-time scaling has yielded remarkable breakthroughs in Large Language Models (LLMs) (e.g. reasoning models) and in generative vision models, allowing models to allocate additional computation during inference to effectively tackle increasingly complex problems. Despite the improvements of this approach, an important limitation emerges: the substantial increase in computation time makes the process slow and impractical for many applications. Given the success of this paradigm and its growing usage, we seek to preserve its benefits while eschewing the inference overhead. In this work we propose one solution to the critical problem of integrating test-time scaling knowledge into a model during post-training. Specifically, we replace reward guided test-time noise optimization in diffusion models with a Noise Hypernetwork that modulates initial input noise. We propose a theoretically grounded framework for learning this reward-tilted distribution for distilled generators, through a tractable noise-space objective that maintains fidelity to the base model while optimizing for desired characteristics. We show that our approach recovers a substantial portion of the quality gains from explicit test-time optimization at a fraction of the computational cost.",
      "arxiv_url": "https://openreview.net/forum?id=DbzREoPwmM",
      "pdf_url": "https://openreview.net/pdf/6a5669aeb2f98e502de0ca50d202e09b131c26de.pdf",
      "primary_category": "generative models, diffusion models, hypernetworks",
      "categories": [
        "generative models",
        "diffusion models",
        "hypernetworks",
        "preference optimization",
        "flow matching",
        "reward fine-tuning",
        "noise optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yZzhaHygWW",
      "title": "Optimizing Retrieval for RAG via Reinforcement Learning",
      "authors": [
        "Jiawei Zhou",
        "Lei Chen"
      ],
      "abstract": "As retrieval-augmented generation (RAG) becomes more widespread, the role of retrieval is shifting from retrieving information for human browsing to retrieving context for AI reasoning. This shift creates more complex search environments, where relevance is difficult to pre-define. Existing retrievers rely on supervised fine-tuning (SFT) with human labels or synthetic data, resulting in static relevance that struggles to adapt to diverse RAG environments. To address this challenge, we propose R3, a Retrieval framework optimized for RAG through Reinforcement learning (RL). Specifically, we adopt an RL training paradigm that enables the retriever to explore and self-improve within given RAG environments, automating the learning process with minimal manual experimentation or tuning effort. Extensive experiments across diverse tasks demonstrate that \\ours improves RAG performance by 5.2% over the original retriever and surpasses state-of-the-art retrievers by 4.9%, while achieving comparable results to LLM-augmented retrieval and RAG systems built on post-trained or instruction-tuned LLMs. It is both efficient and practical, requiring only 4 GPUs and completing training within a single day.",
      "arxiv_url": "https://openreview.net/forum?id=yZzhaHygWW",
      "pdf_url": "https://openreview.net/pdf/74b57081a9a2db071d310000acfd1692aa247047.pdf",
      "primary_category": "Retrieval-augmented Generation, RAG, RL",
      "categories": [
        "Retrieval-augmented Generation",
        "RAG",
        "RL"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tqriGodQ79",
      "title": "Hippocampal-like Sequential Editing for Continual Knowledge Updates in Large Language Models",
      "authors": [
        "Quntian Fang",
        "Zhen Huang",
        "Zhiliang Tian",
        "Minghao Hu",
        "Dongsheng Li",
        "Yiping Yao",
        "Xinyue Fang",
        "Menglong Lu",
        "Guotong Geng"
      ],
      "abstract": "Large language models (LLMs) are now pivotal in real-world applications. Model editing has emerged as a promising paradigm for efficiently modifying LLMs without full retraining. However, current editing approaches face significant limitations due to parameter drift, which stems from inconsistencies between newly edited knowledge and the model's existing knowledge. In sequential editing scenarios, cumulative drifts progressively lead to model collapse characterized by general capability degradation and balance between acquiring new knowledge and catastrophic forgetting of existing knowledge. Drawing inspiration from the hippocampal trisynaptic circuit for continual memorizing and forgetting, we propose a Hippocampal-like Sequential Editing (HSE) framework that designs the unlearning of obsolete knowledge, domain-specific knowledge update separation and replay for edited knowledge. Specifically, the HSE framework designs three core mechanisms: (1) Machine unlearning selectively erases outdated knowledge to facilitate integration of new information, (2) Fisher Information Matrix-guided parameter updates prevents cross-domain knowledge interference, and (3) Parameter replay consolidates long-term editing memory through lightweight and global replay of editing data in a parametric form. Theoretical analysis demonstrates that HSE achieves smaller generalization error bounds, more stable convergence and higher computational efficiency. Experimental results validate its effective balance between acquiring new knowledge and mitigating catastrophic forgetting, maintaining or even slightly enhancing general capabilities. In practical applications, experiments confirm its effectiveness in multi-domain hallucination mitigation, healthcare knowledge injecting, and societal bias reduction.",
      "arxiv_url": "https://openreview.net/forum?id=tqriGodQ79",
      "pdf_url": "https://openreview.net/pdf/bc7a1ee950ea2bfd61da7051086a3076bddf092b.pdf",
      "primary_category": "Large Language Models, Hippocampal Mechanisms, Machine Unlearning",
      "categories": [
        "Large Language Models",
        "Hippocampal Mechanisms",
        "Machine Unlearning",
        "Catastrophic Forgetting",
        "Hallucination Mitigation",
        "Long-term Editing Memory"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "80L235oVBe",
      "title": "Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling",
      "authors": [
        "Yitian Chen",
        "Jingfan Xia",
        "Siyu Shao",
        "Dongdong Ge",
        "Yinyu Ye"
      ],
      "abstract": "Optimization modeling is fundamental to decision-making in fields such as supply chain management, logistics, and financial engineering, but its complexity presents a major barrier to adoption. Automating model creation from natural language is key to improving efficiency and access. However, while Large Language Models (LLMs) are a promising tool for this, they often produce flawed or infeasible results due to errors and hallucinations.\n   To address this issue, we propose Solver-Informed Reinforcement Learning (SIRL), a framework that uses Reinforcement Learning with Verifiable Reward to improve LLMs’ ability to generate accurate and executable optimization models. Specifically, SIRL automatically assesses the executable code and the instance-level mathematical model represented by the associated .lp files. This process yields precise feedback on syntactic validity, feasibility, and solution quality, which serves as a direct reward signal to guide the reinforcement learning process. Furthermore, this verification mechanism also supports our instance-enhanced self-consistency method for creating high-quality training data.\n    Extensive experiments on diverse public benchmarks demonstrate that models trained with our SIRL framework achieve state-of-the-art performance, substantially outperforming existing methods in generating accurate and executable optimization models. Specifically, our SIRL-32B model surpasses DeepSeek-V3 and OpenAI-o3 on the majority of these benchmarks.\n    Our code is publicly available at https://github.com/Cardinal-Operations/SIRL.",
      "arxiv_url": "https://openreview.net/forum?id=80L235oVBe",
      "pdf_url": "https://openreview.net/pdf/b354babdd37a4bbb9a67f01f12802970609d5429.pdf",
      "primary_category": "Optimization, Optimization Problem Formulation, Problem Definition",
      "categories": [
        "Optimization",
        "Optimization Problem Formulation",
        "Problem Definition",
        "Foundation Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "mPuOMcN9E7",
      "title": "Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options",
      "authors": [
        "Joongkyu Lee",
        "Seouh-won Yi",
        "Min-hwan Oh"
      ],
      "abstract": "We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged—motivated by PbRL’s recent empirical success, particularly in aligning large language models (LLMs)—most existing studies focus only on pairwise comparisons. A few recent works  (Zhu et al., 2023, Mukherjee et al., 2024, Thekumparampil et al., 2024)  have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve—and can even deteriorate—as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett–Luce (PL) model for ranking feedback over action subsets and propose **M-AUPO**, an algorithm that selects multiple actions by maximizing the average uncertainty within the offered subset. We prove that **M-AUPO** achieves a suboptimality gap of $\\tilde{\\mathcal{O}}\\left( \\frac{d}{T} \\sqrt{ \\sum_{t=1}^T \\frac{1}{|S_t|}} \\right)$, where $T$ is the total number of rounds, $d$ is the feature dimension, and $|S_t|$ is the size of the subset at round $t$. This result shows that larger subsets directly lead to improved performance and, notably, the bound avoids the exponential dependence on the unknown parameter’s norm, which was a fundamental limitation in most previous works. Moreover, we establish a near-matching lower bound of $\\Omega \\left( \\frac{d}{K \\sqrt{T}} \\right)$, where $K$ is the maximum subset size. To the best of our knowledge, this is the first theoretical result in PbRL with ranking feedback that explicitly shows improved sample efficiency as a function of the subset size.",
      "arxiv_url": "https://openreview.net/forum?id=mPuOMcN9E7",
      "pdf_url": "https://openreview.net/pdf/834533ef3b038770b93583442863552a67807c60.pdf",
      "primary_category": "Preference-based Reinforcement Learning, Ranking Feedback, Plackett–Luce Model",
      "categories": [
        "Preference-based Reinforcement Learning",
        "Ranking Feedback",
        "Plackett–Luce Model",
        "Reinforcement Learning from Human Feedback",
        "Dueling Bandit"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Tjw0ACu3NL",
      "title": "Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning",
      "authors": [
        "Yong Liu",
        "Zirui Zhu",
        "Chaoyu Gong",
        "Minhao Cheng",
        "Cho-Jui Hsieh",
        "Yang You"
      ],
      "abstract": "While fine-tuning large language models (LLMs) for specific tasks often yields impressive results, it comes at the cost of memory inefficiency due to back-propagation in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently proposed to address this issue, only require forward passes during training, making them more memory-friendly. However, compared with exact gradients, ZO-based gradients usually exhibit an estimation error, which can significantly hurt the optimization process, leading to slower convergence and suboptimal solutions. In addition, we find that the estimation error will hurt more when adding to large weights instead of small weights. Based on this observation, this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization approach that applies ZO only to a carefully chosen subset of parameters. We propose a simple yet effective parameter selection scheme that yields significant performance gains with Sparse-MeZO. Additionally, we develop a memory-optimized implementation for sparse masking, ensuring the algorithm requires only inference-level memory consumption, allowing Sparse-MeZO to fine-tune LLaMA-30b on a single A100 GPU. Experimental results illustrate that Sparse-MeZO consistently improves both performance and convergence speed over MeZO without any overhead. For example, it achieves a 9% absolute accuracy improvement and 3.5x speedup over MeZO on the RTE task.",
      "arxiv_url": "https://openreview.net/forum?id=Tjw0ACu3NL",
      "pdf_url": "https://openreview.net/pdf/317ff5a023f3c71c738a95b75c54d4f89202f41a.pdf",
      "primary_category": "Zeroth-Order Optimization, Parameter-Efficient Fine Tuning",
      "categories": [
        "Zeroth-Order Optimization",
        "Parameter-Efficient Fine Tuning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "5xPvWat3IX",
      "title": "Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding",
      "authors": [
        "Xiaoqian Shen",
        "Wenxuan Zhang",
        "Jun Chen",
        "Mohamed Elhoseiny"
      ],
      "abstract": "Understanding and reasoning over long videos pose significant challenges for large video language models (LVLMs) due to the difficulty in processing intensive video tokens beyond context window and retaining long-term sequential information. Retrieval-Augmented Generation (RAG) has demonstrated effectiveness in processing long context for Large Language Models (LLMs); however, applying RAG to long video faces challenges such as disrupted temporal dependencies and inclusion of irrelevant information that can hinder accurate reasoning. To address these limitations, we propose Vgent, a novel \\textbf{graph-based retrieval-reasoning-augmented generation framework} to enhance LVLMs for long video understanding. Our approach introduces two key innovations: (i) It represents videos by structured graphs with  semantic relationships across video clips preserved to improve retrieval effectiveness. (ii) It introduces an intermediate reasoning step to mitigate the reasoning limitation of LVLMs, which leverages structured verification to reduce retrieval noise and facilitate the explicit aggregation of relevant information across clips, resulting in more accurate and context-aware responses. We comprehensively evaluate our framework with various open-source LVLMs on three long-video understanding benchmarks. Our approach yielded an overall performance improvement of $3.0\\%\\sim 5.4\\%$ over base models on MLVU, and outperformed state-of-the-art video RAG methods by $8.6\\%$. Our code is publicly available at https://xiaoqian-shen.github.io/Vgent.",
      "arxiv_url": "https://openreview.net/forum?id=5xPvWat3IX",
      "pdf_url": "https://openreview.net/pdf/b6c36e77072ceda7d5f612502fe9d9dad3125832.pdf",
      "primary_category": "Long Video Understanding; Retrieval-augmented generation; GraphRAG",
      "categories": [
        "Long Video Understanding; Retrieval-augmented generation; GraphRAG"
      ],
      "tags": [
        "LLM",
        "RAG",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yISJGSdzdd",
      "title": "MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling",
      "authors": [
        "Yuxi Liu",
        "Renjia Deng",
        "Yutong He",
        "Xue Wang",
        "Tao Yao",
        "Kun Yuan"
      ],
      "abstract": "The substantial memory demands of pre-training and fine-tuning large language models (LLMs) require memory-efficient optimization algorithms. One promising approach is layer-wise optimization, which treats each transformer block as a single layer and optimizes it sequentially, while freezing the other layers to save optimizer states and activations. Although effective, these methods ignore the varying importance of the modules within each layer, leading to suboptimal performance. Moreover, layer-wise sampling provides only limited memory savings, as at least one full layer must remain active during optimization.  To overcome these limitations, we propose **M**odule-wise **I**mportance **SA**mpling (**MISA**), a novel method that divides each layer into smaller modules and assigns importance scores to each module. \nMISA uses a weighted random sampling mechanism to activate modules, provably reducing\ngradient variance compared to layer-wise sampling. \nAdditionally, we establish an $\\mathcal{O}(1/\\sqrt{K})$ convergence rate under non-convex and stochastic conditions, where $K$ is the total number of training steps, and provide a detailed memory analysis showcasing MISA's superiority over existing baseline methods.  Experiments on diverse learning tasks validate the effectiveness of MISA.",
      "arxiv_url": "https://openreview.net/forum?id=yISJGSdzdd",
      "pdf_url": "https://openreview.net/pdf/1bfcc69f11623c9f8d8794b168e4db37d656b785.pdf",
      "primary_category": "large language models, memory-efficient fine-tuning, block coordinate descent",
      "categories": [
        "large language models",
        "memory-efficient fine-tuning",
        "block coordinate descent",
        "importance sampling"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xWYL9Ki32T",
      "title": "Efficient Pre-Training of LLMs via Topology-Aware Communication Alignment on More Than 9600 GPUs",
      "authors": [
        "Guoliang HE",
        "YOUHE JIANG",
        "Wencong Xiao",
        "Jiang Kaihua",
        "Shuguang Wang",
        "Jun Wang",
        "Du Zixian",
        "Zhuo Jiang",
        "Xinlei Zhang",
        "Binhang Yuan",
        "Eiko Yoneki"
      ],
      "abstract": "The scaling law for large language models (LLMs) depicts that the path towards machine intelligence necessitates training at large scale. Thus, companies continuously build large-scale GPU clusters, and launch training jobs that span over thousands of computing nodes. However, LLM pre-training presents unique challenges due to its complex communication patterns, where GPUs exchange data in sparse yet high-volume bursts within specific groups. Inefficient resource scheduling exacerbates bandwidth contention, leading to suboptimal training performance. This paper presents Arnold, a scheduling system summarizing our experience to effectively align LLM communication patterns to data center topology at scale. In-depth characteristic study is performed to identify the impact of physical network topology to LLM pre-training jobs. Based on the insights, we develop a scheduling algorithm to effectively align communication patterns to physical network topology in data centers. Through simulation experiments, we show the effectiveness of our algorithm in reducing the maximum spread of communication groups by up to $1.67$x. In production training, our scheduling system improves the end-to-end performance by $10.6\\%$ when training with more than $9600$ Hopper GPUs, a significant improvement for our training pipeline.",
      "arxiv_url": "https://openreview.net/forum?id=xWYL9Ki32T",
      "pdf_url": "https://openreview.net/pdf/2a97b9aff754ccdbaac31b4b7efcf27f2b4f8122.pdf",
      "primary_category": "Large Language Models Training, Data Center Topology, Resource Scheduling",
      "categories": [
        "Large Language Models Training",
        "Data Center Topology",
        "Resource Scheduling"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4zhhgKkVzY",
      "title": "Cypher-RI: Reinforcement Learning for Integrating Schema Selection into Cypher Generation",
      "authors": [
        "Hanchen Su",
        "Xuyuan Li",
        "Yan Zhou",
        "zhuoyi lu",
        "Ziwei Chai",
        "Haozheng Wang",
        "Chen Zhang",
        "Yang Yang"
      ],
      "abstract": "The increasing utilization of graph databases across various fields stems from their capacity to represent intricate interconnections. Nonetheless, exploiting the full capabilities of graph databases continues to be a significant hurdle, largely because of the inherent difficulty in translating natural language into Cypher. Recognizing the critical role of schema selection in database query generation and drawing inspiration from recent progress in reasoning-augmented approaches trained through reinforcement learning to enhance inference capabilities and generalization, we introduce Cypher-RI, a specialized framework for the Text-to-Cypher task. Distinct from conventional approaches, our methodology seamlessly integrates schema selection within the Cypher generation pipeline, conceptualizing it as a critical element in the reasoning process. The schema selection mechanism is guided by textual context, with its outcomes recursively shaping subsequent inference processes. Impressively, our 7B-parameter model, trained through this RL paradigm, demonstrates superior performance compared to baselines, exhibiting a 9.41\\% accuracy improvement over GPT-4o on CypherBench. These results underscore the effectiveness of our proposed reinforcement learning framework, which integrates schema selection to enhance both the accuracy and reasoning capabilities in Text-to-Cypher tasks.",
      "arxiv_url": "https://openreview.net/forum?id=4zhhgKkVzY",
      "pdf_url": "https://openreview.net/pdf/43f3e66a09523c7a66ac6e04d413154bc7f6aa9e.pdf",
      "primary_category": "Text-to-Cypher, Reinforcement learning, Graph Databases",
      "categories": [
        "Text-to-Cypher",
        "Reinforcement learning",
        "Graph Databases",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "eXckZbaYma",
      "title": "SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought",
      "authors": [
        "Guanghao Li",
        "Wenhao Jiang",
        "Mingfeng Chen",
        "Yan Li",
        "Hao Yu",
        "Shuting Dong",
        "Tao Ren",
        "Ming Tang",
        "Chun Yuan"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting improves the reasoning performance of large language models (LLMs) by encouraging step-by-step thinking. However, CoT-based methods depend on  intermediate reasoning steps, which limits scalability and generalization. Recent work explores recursive reasoning, where LLMs reuse internal layers across iterations to refine latent representations without explicit CoT supervision. While promising, these approaches often require costly pretraining and lack a principled framework for how reasoning should evolve across iterations.\nWe address this gap by introducing **Flow Chain-of-Thought (Flow CoT)**, a reasoning paradigm that models recursive inference as a progressive trajectory of latent cognitive states. Flow CoT frames each iteration as a distinct cognitive stage—deepening reasoning across iterations without relying on manual supervision. To realize  this, we propose **SCOUT** (*Stepwise Cognitive Optimization Using Teachers*), a lightweight fine-tuning framework that enables Flow CoT-style reasoning without the need for pretraining. SCOUT uses progressive distillation to align each iteration with a teacher of appropriate capacity, and a cross-attention-based retrospective module that integrates outputs from previous iterations while preserving the model’s original computation flow.\nExperiments across eight reasoning benchmarks show that SCOUT consistently improves both accuracy and explanation quality, achieving up to 1.8\\% gains under fine-tuning. Qualitative analyses further reveal that SCOUT enables progressively deeper reasoning across iterations—refining both belief formation and explanation granularity. These results not only validate the effectiveness of SCOUT, but also demonstrate the practical viability of Flow CoT as a scalable framework for enhancing reasoning in LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=eXckZbaYma",
      "pdf_url": "https://openreview.net/pdf/e31dcc29db400a0d8f2b23e4dca55cc1663442a1.pdf",
      "primary_category": "Recursive reasoning, Flow Chain-of-Thought",
      "categories": [
        "Recursive reasoning",
        "Flow Chain-of-Thought"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zaV9s8iM2T",
      "title": "UniGTE: Unified Graph–Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains",
      "authors": [
        "Duo Wang",
        "Yuan Zuo",
        "Guangyue Lu",
        "Junjie Wu"
      ],
      "abstract": "Generalizing to unseen graph tasks without task-specific supervision is challenging: conventional graph neural networks are typically tied to a fixed label space, while large language models (LLMs) struggle to capture graph structure. We introduce UniGTE, an instruction-tuned encoder–decoder framework that unifies structural and semantic reasoning. The encoder augments a pretrained autoregressive LLM with learnable alignment tokens and a structure-aware graph–text attention mechanism, enabling it to attend jointly to a tokenized graph and a natural-language task prompt while remaining permutation-invariant to node order. This yields compact, task-aware graph representations. Conditioned solely on these representations, a frozen LLM decoder predicts and reconstructs: it outputs the task answer and simultaneously paraphrases the input graph in natural language. The reconstruction objective regularizes the encoder to preserve structural cues. UniGTE is instruction-tuned on five datasets spanning node-, edge-, and graph-level tasks across diverse domains, yet requires no fine-tuning at inference. It achieves new state-of-the-art zero-shot results on node classification, link prediction, graph classification and graph regression under cross-task and cross-domain settings, demonstrating that tight integration of graph structure with LLM semantics enables robust, transferable graph reasoning.",
      "arxiv_url": "https://openreview.net/forum?id=zaV9s8iM2T",
      "pdf_url": "https://openreview.net/pdf/c1a1062b3f5f96a551d1d7b549479e96ffc186db.pdf",
      "primary_category": "Large Language Models, Graph Neural Networks, Zero Shot Learning",
      "categories": [
        "Large Language Models",
        "Graph Neural Networks",
        "Zero Shot Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "dbioYc7qav",
      "title": "Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator",
      "authors": [
        "Peiwen Yuan",
        "Yiwei Li",
        "Shaoxiong Feng",
        "Xinglin Wang",
        "Yueqi Zhang",
        "Jiayi Shi",
        "Chuyi Tan",
        "Boyuan Pan",
        "Yao Hu",
        "Kan Li"
      ],
      "abstract": "LLM-as-Benchmark-Generator methods have been widely studied as a supplement to human annotators for scalable evaluation, while the potential biases within this paradigm remain underexplored. \nIn this work, we systematically define and validate the phenomenon of inflated performance in models evaluated on their self-generated benchmarks, referred to as self-bias, and attribute it to sub-biases arising from question domain, language style, and wrong labels.\nOn this basis, we propose Silencer, a general framework that leverages the heterogeneity between multiple generators at both the sample and benchmark levels to neutralize bias and generate high-quality, self-bias-silenced benchmark. Experimental results across various settings demonstrate that Silencer can suppress self-bias to near zero, significantly improve evaluation effectiveness of the generated benchmark (with an average improvement from 0.655 to 0.833 in Pearson correlation with high-quality human-annotated benchmark), while also exhibiting strong generalizability.",
      "arxiv_url": "https://openreview.net/forum?id=dbioYc7qav",
      "pdf_url": "https://openreview.net/pdf/c5fe3ac1075ea972d226a6fa8178871b6c6f8c9a.pdf",
      "primary_category": "large language model, benchmark generation, self-bias",
      "categories": [
        "large language model",
        "benchmark generation",
        "self-bias"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "u4j0LtCYid",
      "title": "Demystifying Language Model Forgetting with Low-rank Example Associations",
      "authors": [
        "Xisen Jin",
        "Xiang Ren"
      ],
      "abstract": "Large Language models (LLMs) suffer from forgetting of upstream knowledge when fine-tuned. Despite efforts on mitigating forgetting, few have investigated how forgotten upstream examples are dependent on newly learned tasks. Insights on such dependencies enable efficient and targeted mitigation of forgetting. In this paper, we empirically analyze forgetting that occurs in $N$ upstream examples of language modeling or instruction-tuning after fine-tuning LLMs on one of $M$ new tasks, visualized in $M\\times N$ matrices. We show that the matrices are often well-approximated with low-rank matrices, indicating the dominance of simple associations between the learned tasks and forgotten upstream examples. Leveraging the analysis, we predict forgetting of upstream examples when fine-tuning LLMs on unseen tasks with matrix completion over the empirical associations. This enables fast identification of most forgotten examples without expensive inference on the entire upstream data. Despite simplicity, the approach outperforms prior approaches that learn semantic relationships of learned tasks and upstream examples with LMs. We demonstrate the practical utility of our analysis by showing statistically significantly reduced forgetting as we upweight predicted examples for replay during fine-tuning.",
      "arxiv_url": "https://openreview.net/forum?id=u4j0LtCYid",
      "pdf_url": "https://openreview.net/pdf/067610017dd177dfa6a1029074e8116d3570ba5f.pdf",
      "primary_category": "catastrophic forgetting, continual learning, large language model",
      "categories": [
        "catastrophic forgetting",
        "continual learning",
        "large language model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "JIpKkzSqly",
      "title": "Transstratal Adversarial Attack: Compromising Multi-Layered Defenses in Text-to-Image Models",
      "authors": [
        "Chunlong Xie",
        "Kangjie Chen",
        "Shangwei Guo",
        "Shudong Zhang",
        "Tianwei Zhang",
        "Tao Xiang"
      ],
      "abstract": "Modern Text-to-Image (T2I) models deploy multi-layered defenses to block Not-Safe-For-Work (NSFW) content generation. These defenses typically include sequential layers such as prompt filters, concept erasers and image filters. While existing adversarial attacks have demonstrated vulnerabilities in isolated defense layers, they prove largely ineffective against multi-layered defenses deployed in real-world T2I systems. In this paper, we demonstrate that exploiting overlapping vulnerabilities across these distinct defense layers enables adversaries to systematically bypass the entire safeguard of T2I systems. We propose Transstratal Adversarial Attack (TAA), a novel black-box framework to compromise T2I models with multi-layered protection. It generates transstratal adversarial prompts to evade all defense layers simultaneously. This is accomplished through transstratal adversarial candidate generation using LLMs to fulfill implicit and subjective adversarial requirements against different defense layers, combined with adversarial genetic optimization for efficient black-box search to maximize the bypass rates and generated image harmfulness. Evaluated across 14 T2I models (e.g., Stable Diffusion, DALL·E, and Midjourney) and 17 safety modules, our attack achieves an average attack success rate of 85.6\\%, surpassing state-of-the-art methods by 73.5\\%. Our findings challenge the isolated design of safety mechanisms and establish the first benchmark for holistic robustness evaluation in multi-layered safeguarded T2I models. The code can be found in https://github.com/Bluedask/TAA-T2I.",
      "arxiv_url": "https://openreview.net/forum?id=JIpKkzSqly",
      "pdf_url": "https://openreview.net/pdf/ca09041eedd05212e3c61cbb632d0aa176bc371d.pdf",
      "primary_category": "Text-to-Image, Adversarial Attack, NSFW",
      "categories": [
        "Text-to-Image",
        "Adversarial Attack",
        "NSFW",
        "Multi-Layered Defense"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RwCaBZ4w5P",
      "title": "Analogy-based Multi-Turn Jailbreak against Large Language Models",
      "authors": [
        "Mengjie Wu",
        "Yihao Huang",
        "Zhenjun Lin",
        "Kangjie Chen",
        "Yuyang zhang",
        "Yuhan Huang",
        "Run Wang",
        "Lina Wang"
      ],
      "abstract": "Large language models (LLMs) are inherently designed to support multi-turn interactions, which opens up new possibilities for jailbreak attacks that unfold gradually and potentially bypass safety mechanisms more effectively than single-turn attacks. However, current multi-turn jailbreak methods are still in their early stages and suffer from two key limitations. First, they all inherently require inserting sensitive phrases into the context, which makes the dialogue appear suspicious and increases the likelihood of rejection, undermining the effectiveness of the attack. Second, even when harmful content is generated, the response often fails to align with the malicious prompt due to semantic drift, where the conversation slowly moves away from its intended goal. To address these challenges, we propose an analogy-based black-box multi-turn jailbreak framework that constructs fully benign contexts to improve attack success rate while ensuring semantic alignment with the malicious intent. The method first guides the model through safe tasks that mirror the response structure of the malicious prompt, enabling it to internalize the format without exposure to sensitive content. A controlled semantic shift is then introduced in the final turn, substituting benign elements with malicious ones while preserving structural coherence. Experiments on six commercial and open-source LLMs, two benchmark datasets show that our method significantly improves attack performance, achieving an average attack success rate of 93.3\\% and outperforming five competitive baselines. Our code is released at https://github.com/MM-WW55/AMA",
      "arxiv_url": "https://openreview.net/forum?id=RwCaBZ4w5P",
      "pdf_url": "https://openreview.net/pdf/90ec84387b8d7282640d625e2d28faef32f89000.pdf",
      "primary_category": "Multi-turn jailbreak, large language models",
      "categories": [
        "Multi-turn jailbreak",
        "large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Wc1VZ2bVJn",
      "title": "TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs",
      "authors": [
        "Yuxiang Zhang",
        "Zhengxu Yu",
        "Weihang Pan",
        "Zhongming Jin",
        "Qiang Fu",
        "Deng Cai",
        "Binbin Lin",
        "Jieping Ye"
      ],
      "abstract": "Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrated the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek‑R1‑Distill‑Qwen‑7B fine-tuned by using our proposed method achieved a 50\\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at \\url{https://github.com/zhangyx1122/TokenSqueeze}.",
      "arxiv_url": "https://openreview.net/forum?id=Wc1VZ2bVJn",
      "pdf_url": "https://openreview.net/pdf/2658ef233811501fdccb69e12fe4d667bf1fe11d.pdf",
      "primary_category": "Efficient Reasoning, Chain-of-Thought, Large Language Models",
      "categories": [
        "Efficient Reasoning",
        "Chain-of-Thought",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VCj7knCJhn",
      "title": "The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training",
      "authors": [
        "Weize Chen",
        "Jiarui Yuan",
        "Tailin Jin",
        "Ning Ding",
        "Huimin Chen",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Recent large language models (LLMs) exhibit impressive reasoning but often \\textit{overthink}, generating excessively long responses that hinder efficiency. We introduce DIET (DIfficulty-AwarE Training), a framework that systematically cuts these \"token calories\" by integrating on-the-fly problem difficulty into the reinforcement learning (RL) process. DIET dynamically adapts token compression strategies by modulating token penalty strength and conditioning target lengths on estimated task difficulty, to optimize the performance-efficiency trade-off. We also theoretically analyze the pitfalls of naive reward weighting in group-normalized RL algorithms like GRPO, and propose \\textit{Advantage Weighting} technique, which enables stable and effective implementation of these difficulty-aware objectives. Experimental results demonstrate that DIET significantly reduces token counts while simultaneously improving reasoning performance. Beyond raw token reduction, we show two crucial benefits largely overlooked by prior work: (1) DIET leads to superior \\textbf{inference scaling}. By maintaining high per-sample quality with fewer tokens, it enables better scaling performance via majority voting under fixed computational budgets, an area where other methods falter. (2) DIET enhances the natural positive correlation between response length and problem difficulty, ensuring verbosity is appropriately allocated, unlike many existing compression methods that disrupt this relationship. Our analyses provide a principled and effective framework for developing more efficient, practical, and high-performing LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=VCj7knCJhn",
      "pdf_url": "https://openreview.net/pdf/33e46bebe7b8b298e40a0c3d81837e4d6c1f4673.pdf",
      "primary_category": "token compression, efficient reasoning, reinforcement learning",
      "categories": [
        "token compression",
        "efficient reasoning",
        "reinforcement learning",
        "inference scaling"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "sMtiGB2YZT",
      "title": "Panacea: Mitigating Harmful Fine-tuning for Large Language Models via Post-fine-tuning Perturbation",
      "authors": [
        "Yibo Wang",
        "Tiansheng Huang",
        "Li Shen",
        "Huanjin Yao",
        "Haotian Luo",
        "Rui Liu",
        "Naiqiang Tan",
        "Jiaxing Huang",
        "Dacheng Tao"
      ],
      "abstract": "Harmful fine-tuning attack introduces significant security risks to the fine-tuning services. Main-stream defenses aim to vaccinate the model such that the later harmful fine-tuning attack is less effective. However, our evaluation results show that such defenses are fragile-- with a few fine-tuning steps, the model still can learn the harmful knowledge. To this end, we do further experiment and find that an embarrassingly simple solution-- adding purely random perturbations to the fine-tuned model, can recover the model from harmful behaviors, though it leads to a degradation in the model’s fine-tuning performance. To address the degradation of fine-tuning performance, we further propose \\methodname, which optimizes an adaptive perturbation that will be applied to the model after fine-tuning. \\methodname maintains model's safety alignment performance without compromising downstream fine-tuning performance. Comprehensive experiments are conducted on different harmful ratios, fine-tuning tasks and mainstream LLMs, where the average harmful scores are reduced by up-to 21.2%, while maintaining fine-tuning performance. As a by-product, we analyze the adaptive perturbation and show that different layers in various LLMs have distinct safety coefficients. Source code available at https://github.com/w-yibo/Panacea.",
      "arxiv_url": "https://openreview.net/forum?id=sMtiGB2YZT",
      "pdf_url": "https://openreview.net/pdf/b4a3d65ecb35c040dd86a4915f579bd051c29f9b.pdf",
      "primary_category": "Harmful fine-tuning, LLM, Safety alignment",
      "categories": [
        "Harmful fine-tuning",
        "LLM",
        "Safety alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "gHeMLGEVJz",
      "title": "Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval",
      "authors": [
        "Siting Li",
        "Xiang Gao",
        "Simon Shaolei Du"
      ],
      "abstract": "While an image is worth more than a thousand words, only a few provide crucial information for a given task and thus should be focused on. In light of this, ideal text-to-image (T2I) retrievers should prioritize specific visual attributes relevant to queries. To evaluate current retrievers on handling attribute-focused queries, we build COCO-Facet, a COCO-based benchmark with 9,112 queries about diverse attributes of interest. We find that CLIP-like retrievers, which are widely adopted due to their efficiency and zero-shot ability, have poor and imbalanced performance, possibly because their image embeddings focus on global semantics and subjects while leaving out other details. Notably, we reveal that even recent Multimodal Large Language Model (MLLM)-based, stronger retrievers with a larger output dimension struggle with this limitation. Hence, we hypothesize that retrieving with *general* image embeddings is suboptimal for performing such queries. As a solution, we propose to use *promptable* image embeddings enabled by these multimodal retrievers, which boost performance by highlighting required attributes. Our pipeline for deriving such embeddings generalizes across query types, image pools, and base retriever architectures. To enhance real-world applicability, we offer two acceleration strategies: Pre-processing promptable embeddings and using linear approximations. We show that the former yields a 15\\% improvement in Recall@5 when prompts are predefined, while the latter achieves an 8\\% improvement when prompts are only available during inference.",
      "arxiv_url": "https://openreview.net/forum?id=gHeMLGEVJz",
      "pdf_url": "https://openreview.net/pdf/8197443737fe19c9a91c16f13d0dcf17e31a87d0.pdf",
      "primary_category": "text-to-image retrieval, image embeddings",
      "categories": [
        "text-to-image retrieval",
        "image embeddings"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "KnqiC0znVF",
      "title": "Large Language Diffusion Models",
      "authors": [
        "Shen Nie",
        "Fengqi Zhu",
        "Zebin You",
        "Xiaolu Zhang",
        "Jingyang Ou",
        "Jun Hu",
        "JUN ZHOU",
        "Yankai Lin",
        "Ji-Rong Wen",
        "Chongxuan Li"
      ],
      "abstract": "The capabilities of large language models (LLMs) are widely regarded as relying on autoregressive models (ARMs). We challenge this notion by introducing *LLaDA*, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA employs a forward data masking process and a reverse generation process, parameterized by a Transformer to predict masked tokens. It provides a principled generative approach for probabilistic inference by optimizing a likelihood lower bound. Across extensive benchmarks on general tasks, math, code, and so on, LLaDA demonstrates strong *scalability* and performs comparably to our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in *in-context learning* and, after SFT, exhibits impressive *instruction-following* abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings show the promise of diffusion models for language modeling at scale and challenge the common assumption that core LLM capabilities discussed above inherently depend on ARMs. Project page and codes: \\url{https://ml-gsai.github.io/LLaDA-demo/}.",
      "arxiv_url": "https://openreview.net/forum?id=KnqiC0znVF",
      "pdf_url": "https://openreview.net/pdf/7b4f4c91006394a5b226b046c9b09051cb241d59.pdf",
      "primary_category": "diffusion language models, large language models, masked diffusion models",
      "categories": [
        "diffusion language models",
        "large language models",
        "masked diffusion models",
        "discrete diffusion models",
        "diffusion models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qr5uMEs6iR",
      "title": "Large Language Models as End-to-end Combinatorial Optimization Solvers",
      "authors": [
        "Xia Jiang",
        "Yaoxin Wu",
        "Minshuo Li",
        "Zhiguang Cao",
        "Yingqian Zhang"
      ],
      "abstract": "Combinatorial optimization (CO) problems, central to decision-making scenarios like logistics and manufacturing, are traditionally solved using problem-specific algorithms requiring significant domain expertise. While large language models (LLMs) have shown promise in automating CO problem solving, existing approaches rely on intermediate steps such as code generation or solver invocation, limiting their generality and accessibility. This paper introduces a novel framework that empowers LLMs to serve as end-to-end CO solvers by directly mapping natural language problem descriptions to solutions. We propose a two-stage training strategy: supervised fine-tuning (SFT) imparts LLMs with solution construction patterns from domain-specific solvers, while a feasibility-and-optimality-aware reinforcement learning (FOARL) process explicitly mitigates constraint violations and refines solution quality. Evaluation across seven NP-hard CO problems shows that our method achieves a high feasibility rate and reduces the average optimality gap to 1.03–8.20% by tuning a 7B-parameter LLM, surpassing both general-purpose LLMs (e.g., GPT-4o), reasoning models (e.g., DeepSeek-R1), and domain-specific heuristics. Our method establishes a unified language-based pipeline for CO without extensive code execution or manual architectural adjustments for different problems, offering a general and language-driven alternative to traditional solver design while maintaining relative feasibility guarantees.",
      "arxiv_url": "https://openreview.net/forum?id=qr5uMEs6iR",
      "pdf_url": "https://openreview.net/pdf/88f23ba0801e1b8546e9c0a85346250cbe4ce74f.pdf",
      "primary_category": "Large Language Model; Combinatorial Optimization; Reinforcement Learning",
      "categories": [
        "Large Language Model; Combinatorial Optimization; Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XtNiw8OQsy",
      "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond",
      "authors": [
        "Junteng Liu",
        "Yuanxiang Fan",
        "Jiang Zhuo",
        "Han Ding",
        "Yongyi Hu",
        "Chi Zhang",
        "Yiqi Shi",
        "Shitong Weng",
        "Aili Chen",
        "Shiqi Chen",
        "Mozhi Zhang",
        "Pengyu Zhao",
        "Junxian He"
      ],
      "abstract": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL.\nWe hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards.\nIn our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks.\nThese findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We will open-source both the data synthesis pipeline and the SynLogic dataset.",
      "arxiv_url": "https://openreview.net/forum?id=XtNiw8OQsy",
      "pdf_url": "https://openreview.net/pdf/b5b93e7b533135e2ad1d72d22e3a481324c2a73d.pdf",
      "primary_category": "LLM, reinforcement learning, reasoning",
      "categories": [
        "LLM",
        "reinforcement learning",
        "reasoning",
        "logical reasoning",
        "data synthesis"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1OuhWYrwgW",
      "title": "Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs",
      "authors": [
        "Vaggelis Dorovatas",
        "Soroush Seifi",
        "Gunshi Gupta",
        "Rahaf Aljundi"
      ],
      "abstract": "Video Large Language Models (Video-LLMs) excel at understanding videos in-context, assuming full access to the video when answering queries. However, these models face challenges in streaming scenarios where hour-long videos must be processed online, and questions need timely responses. In this work, we propose a training-free approach compatible with standard Video-LLMs, leveraging three key concepts: 1) LLM-informed selection of visual tokens to identify those that the LLM has attended to and contributed to its understanding of each short clip. Our attention-based selection allows us to discard up to ~95\\% of unimportant visual tokens with minimal performance loss; 2) Hierarchical selection of tokens combined with natural language understanding of each processed clip; 3) Caption-based question answering for lightweight and accurate responses. Our method achieves state-of-the-art performance on streaming video benchmarks, striking a balance between efficiency and effectiveness.",
      "arxiv_url": "https://openreview.net/forum?id=1OuhWYrwgW",
      "pdf_url": "https://openreview.net/pdf/4c6f293fe5e7718c40b72658eceb4fbecce16be6.pdf",
      "primary_category": "streaming video understanding, vlms, training-free",
      "categories": [
        "streaming video understanding",
        "vlms",
        "training-free",
        "memory"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BbWrp6O8Lm",
      "title": "When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinations in Scene Text Spotting and Understanding",
      "authors": [
        "Yan Shu",
        "Hangui Lin",
        "Yexin Liu",
        "Yan Zhang",
        "Gangyan Zeng",
        "Yan Li",
        "Yu ZHOU",
        "Ser-Nam Lim",
        "Harry Yang",
        "Nicu Sebe"
      ],
      "abstract": "Large Multimodal Models (LMMs) have achieved impressive progress in visual perception and reasoning. However, when confronted with visually ambiguous or non-semantic scene text, they often struggle to accurately spot and understand the content, frequently generating semantically plausible yet visually incorrect answers, which we refer to as semantic hallucination.\nIn this work, we investigate the underlying causes of semantic hallucination and identify a key finding:  Transformer layers in LLM with stronger attention focus on scene text regions are less prone to producing semantic hallucinations.\n Thus, we propose a training-free semantic hallucination mitigation framework comprising two key components: (1) ZoomText, a coarse-to-fine strategy that identifies potential text regions without external detectors; and (2) Grounded Layer Correction, which adaptively leverages the internal representations from layers less prone to hallucination to guide decoding, correcting hallucinated outputs for non-semantic samples while preserving the semantics of meaningful ones. To enable rigorous evaluation, we introduce TextHalu-Bench, a benchmark of  1,740 samples spanning both semantic and non-semantic cases, with manually curated question–answer pairs designed to probe model hallucinations.\nExtensive experiments demonstrate that our method not only effectively mitigates semantic hallucination but also achieves strong performance on public benchmarks for scene text spotting and understanding.",
      "arxiv_url": "https://openreview.net/forum?id=BbWrp6O8Lm",
      "pdf_url": "https://openreview.net/pdf/e7e38a8ef7807bc7e1a452c1a1fa74d52f068819.pdf",
      "primary_category": "Large Multimodal Models, Hallucination, Scene text spotting and understanding",
      "categories": [
        "Large Multimodal Models",
        "Hallucination",
        "Scene text spotting and understanding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "sUjwDdyspc",
      "title": "MotionBind: Multi-Modal Human Motion Alignment for Retrieval, Recognition, and Generation",
      "authors": [
        "Kaleab A Kinfu",
        "Rene Vidal"
      ],
      "abstract": "Recent advances in multi-modal representation learning have led to unified embedding spaces that align modalities such as images, text, audio, and vision. However, human motion sequences, a modality that is fundamental for understanding dynamic human activities, remains largely unrepresented in these frameworks. Semantic understanding of actions requires multi-modal grounding: text conveys descriptive semantics, vision provides visual context, and audio provides environmental cues. To bridge this gap, we propose MotionBind, a novel architecture that extends the LanguageBind embedding space to incorporate human motion. MotionBind has two major components. The first one is a Multi-Scale Temporal Motion Transformer (MuTMoT) that maps motion sequences to semantically meaningful embeddings. Multimodal alignment is achieved via diverse cross-modal supervision, including motion-text pairs from HumanML3D and KIT-ML, motion-video pairs rendered from AMASS, and motion-video-audio triplets from AIST++. The second component is a Retrieval-Augmented Latent diffusion Model (REALM) that can generate motion sequences conditioned on many modalities. MotionBind achieves state-of-the-art or competitive performance across motion reconstruction, cross-modal retrieval, zero-shot action recognition, and text-to-motion generation benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=sUjwDdyspc",
      "pdf_url": "https://openreview.net/pdf/09de4050405f8f712f8bd4ad12cd35dfe9fb3ec5.pdf",
      "primary_category": "human motion, multi-modal representation learning, cross-modal retrieval",
      "categories": [
        "human motion",
        "multi-modal representation learning",
        "cross-modal retrieval",
        "action recognition",
        "human motion synthesis"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "u7jtLj46i9",
      "title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Cultural Learning",
      "authors": [
        "Mircea Tudor Lică",
        "Ojas Shirekar",
        "Baptiste Colle",
        "Chirag Raman"
      ],
      "abstract": "Embodied agents powered by large language models (LLMs), such as Voyager, promise open-ended competence in worlds such as Minecraft. However, when powered by open-weight LLMs they still falter on elementary tasks after domain-specific fine-tuning. We propose MindForge, a generative-agent framework for cultural lifelong learning through explicit perspective taking. We introduce three key innovations: (1) a structured theory of mind representation linking percepts, beliefs, desires, and actions; (2) natural inter-agent communication; and (3) a multi-component memory system. Following the cultural learning framework, we test MindForge in both instructive and collaborative settings within Minecraft. In an instructive setting with GPT-4, MindForge agents powered by open-weight LLMs significantly outperform their Voyager counterparts in basic tasks yielding $3\\times$ more tech-tree milestones and collecting $2.3\\times$ more unique items than the Voyager baseline. Furthermore, in fully collaborative settings, we find that the performance of two underachieving agents improves with more communication rounds, echoing the Condorcet Jury Theorem. MindForge agents demonstrate sophisticated behaviors, including expert-novice knowledge transfer, collaborative problem solving, and adaptation to out-of-distribution tasks through accumulated cultural experiences.",
      "arxiv_url": "https://openreview.net/forum?id=u7jtLj46i9",
      "pdf_url": "https://openreview.net/pdf/aaff34bc5f2395383d58c98d38975df26262971c.pdf",
      "primary_category": "lifelong learning, theory of mind, LLMs",
      "categories": [
        "lifelong learning",
        "theory of mind",
        "LLMs",
        "cultural learning",
        "in-context learning"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "g3EF5XsapH",
      "title": "URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model",
      "authors": [
        "Zhe Li",
        "Xiang Bai",
        "Jieyu Zhang",
        "Zhuangzhe Wu",
        "Che Xu",
        "Ying Li",
        "Chengkai Hou",
        "Shanghang Zhang"
      ],
      "abstract": "Constructing accurate digital twins of articulated objects is essential for robotic simulation training and embodied AI world model building, yet historically requires painstaking manual modeling or multi-stage pipelines. In this work, we propose \\textbf{URDF-Anything}, an end-to-end automatic reconstruction framework based on a 3D multimodal large language model (MLLM). URDF-Anything utilizes an autoregressive prediction framework based on point-cloud and text multimodal input to jointly optimize geometric segmentation and kinematic parameter prediction. It implements a specialized [SEG] token mechanism that interacts directly with point cloud features, enabling fine-grained part-level segmentation while maintaining consistency with the kinematic parameter predictions.\nExperiments on both simulated and real-world datasets demonstrate that our method significantly outperforms existing approaches regarding geometric segmentation (mIoU 17\\% improvement), kinematic parameter prediction (average error reduction of 29\\%), and physical executability (surpassing baselines by 50\\%). Notably, our method exhibits excellent generalization ability, performing well even on objects outside the training set. This work provides an efficient solution for constructing digital twins for robotic simulation, significantly enhancing the sim-to-real transfer capability.",
      "arxiv_url": "https://openreview.net/forum?id=g3EF5XsapH",
      "pdf_url": "https://openreview.net/pdf/85f6f487962eccd257865869162d7611d20b0836.pdf",
      "primary_category": "articulated objects, 3D multimodal language models",
      "categories": [
        "articulated objects",
        "3D multimodal language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Xm57IXqU0n",
      "title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning",
      "authors": [
        "Guangchen Lan",
        "Huseyin A Inan",
        "Sahar Abdelnabi",
        "Janardhan Kulkarni",
        "Lukas Wutschitz",
        "Reza Shokri",
        "Christopher Brinton",
        "Robert Sim"
      ],
      "abstract": "As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. \nWe posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating.\nTo test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. \nWe then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI.\nUsing a synthetic, automatically created, dataset of only $\\sim700$ examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. \nImportantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.",
      "arxiv_url": "https://openreview.net/forum?id=Xm57IXqU0n",
      "pdf_url": "https://openreview.net/pdf/6bda8091eca7970546811d3f74616d7cfb64cfb1.pdf",
      "primary_category": "Reinforcement Learning, LLM Reasoning, Contextual Integrity",
      "categories": [
        "Reinforcement Learning",
        "LLM Reasoning",
        "Contextual Integrity"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "jCuEeQF7uP",
      "title": "CADMorph: Geometry‑Driven Parametric CAD Editing via a Plan–Generate–Verify Loop",
      "authors": [
        "Weijian Ma",
        "Shizhao Sun",
        "Ruiyu Wang",
        "Jiang Bian"
      ],
      "abstract": "A Computer-Aided Design (CAD) model encodes an object in two coupled forms: a \\emph{parametric construction sequence} and its resulting \\emph{visible geometric shape}.\nDuring iterative design, adjustments to the geometric shape inevitably require synchronized edits to the underlying parametric sequence, called \\emph{geometry-driven parametric CAD editing}.\nThe task calls for 1) preserving the original sequence’s structure, 2) ensuring each edit's semantic validity, and 3) maintaining high shape fidelity to the target shape, all under scarce editing data triplets.\nWe present \\emph{CADMorph}, an iterative \\emph{plan–generate–verify} framework that orchestrates pretrained domain-specific foundation models during inference: a \\emph{parameter-to-shape} (P2S) latent diffusion model and a \\emph{masked-parameter-prediction} (MPP) model.\nIn the planning stage, cross-attention maps from the P2S model pinpoint the segments that need modification and offer editing masks. \nThe MPP model then infills these masks with semantically valid edits in the generation stage. \nDuring verification, the P2S model embeds each candidate sequence in shape-latent space, measures its distance to the target shape, and selects the closest one. \nThe three stages leverage the inherent geometric consciousness and design knowledge in pretrained priors, and thus tackle structure preservation, semantic validity, and shape fidelity respectively. \nBesides, both P2S and MPP models are trained without triplet data, bypassing the data-scarcity bottleneck.\nCADMorph surpasses GPT-4o and specialized CAD baselines, and supports downstream applications such as iterative editing and reverse-engineering enhancement.",
      "arxiv_url": "https://openreview.net/forum?id=jCuEeQF7uP",
      "pdf_url": "https://openreview.net/pdf/f3570a57e1a51ecc4499514f523405d920071baf.pdf",
      "primary_category": "Computer-Aided Design, Test-time scaling, Generative Models",
      "categories": [
        "Computer-Aided Design",
        "Test-time scaling",
        "Generative Models",
        "Large-language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qI95wZZCWh",
      "title": "Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models",
      "authors": [
        "Jiaqi WANG",
        "Kevin Qinghong Lin",
        "James Cheng",
        "Mike Zheng Shou"
      ],
      "abstract": "Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision–language models (VLMs). Group Relative Policy Optimization (GRPO) is a recent prominent method that encourages models to generate complete reasoning traces before answering, leading to increased token usage and computational cost. Inspired by the human-like thinking process—where people skip reasoning for easy questions but think carefully when needed—we explore how to enable VLMs to first decide *when reasoning is necessary*.\nTo realize this, we propose \\ours, a two-stage training strategy:\n**(i)** a supervised fine-tuning (SFT) stage with a simple yet effective “**thought dropout**” operation, where reasoning traces are randomly replaced with empty thoughts. This introduces a think-or-not format that serves as a cold start for selective reasoning;  **(ii)** a GRPO stage that enables the model to freely explore when to think or not, while maximizing task-aware outcome rewards.\nExperimental results show that \\ours can *reduce the completion length by up to **90%** compared to vanilla GRPO, without sacrificing performance or even improving it*. Further evaluations across LLM (GSM8K), VLM (CLEVR, Super-CLEVR, GeoQA), and Agentic (AITZ) tasks—covering a range of reasoning difficulties under both 3B and 7B models—consistently reveal that the \\textit{model progressively learns to bypass unnecessary reasoning steps as training advances}.\nThese findings shed light on the path toward human-like reasoning patterns in RL approaches.\nOur code is available at https://github.com/kokolerk/TON.",
      "arxiv_url": "https://openreview.net/forum?id=qI95wZZCWh",
      "pdf_url": "https://openreview.net/pdf/62bc8f6585ecc381363cf32ea8c833c4ef08d00e.pdf",
      "primary_category": "Vision-Language Models, Reinforcement Learning",
      "categories": [
        "Vision-Language Models",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "OwU0mgfKUi",
      "title": "A$^3$E: Towards Compositional Model Editing",
      "authors": [
        "Hongming Piao",
        "Hao Wang",
        "Dapeng Wu",
        "Ying Wei"
      ],
      "abstract": "Model editing has become a *de-facto* practice to address hallucinations and outdated knowledge of large language models (LLMs). However, existing methods are predominantly evaluated in isolation, i.e., one edit at a time, failing to consider a critical scenario of compositional model editing, where multiple edits must be integrated and jointly utilized to answer real-world multifaceted questions. For instance, in medical domains, if one edit informs LLMs that COVID-19 causes \"fever\" and another that it causes \"loss of taste\", a qualified compositional editor should enable LLMs to answer the question \"What are the symptoms of COVID-19?\" with both \"fever\" and \"loss of taste\" (and potentially more). In this work, we define and systematically benchmark this compositional model editing (CME) task, identifying three key undesirable issues that existing methods struggle with: *knowledge loss*, *incorrect preceding* and *knowledge sinking*. To overcome these issues, we propose A$^3$E, a novel compositional editor that (1) ***a**daptively combines and **a**daptively regularizes* pre-trained foundation knowledge in LLMs in the stage of edit training and (2) ***a**daptively merges* multiple edits to better meet compositional needs in the stage of edit composing. Extensive experiments demonstrate that A$^3$E improves the composability by at least 22.45\\% without sacrificing the performance of non-compositional model editing.",
      "arxiv_url": "https://openreview.net/forum?id=OwU0mgfKUi",
      "pdf_url": "https://openreview.net/pdf/30d162d84e1c61104f03fe62e615c3e70d67faf0.pdf",
      "primary_category": "Model Editing",
      "categories": [
        "Model Editing"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3Qo2SRcHgU",
      "title": "AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees",
      "authors": [
        "Hongyi Zhou",
        "Jin Zhu",
        "Pingfan Su",
        "Kai Ye",
        "Ying Yang",
        "Shakeel Gavioli-Akilagun",
        "Chengchun Shi"
      ],
      "abstract": "We study the problem of determining whether a piece of text has been authored by a human or by a large language model (LLM). Existing state of the art logits-based detectors make use of statistics derived from the log-probability of the observed text evaluated using the distribution function of a given source LLM. However, relying solely on log probabilities can be sub-optimal. In response, we introduce AdaDetectGPT -- a novel classifier that adaptively learns a witness function from training data to enhance the performance of logits-based detectors. We provide statistical guarantees on its true positive rate, false positive rate, true negative rate and false negative rate. Extensive numerical studies show AdaDetectGPT nearly uniformly improves the state-of-the-art method in various combination of datasets and LLMs, and the improvement can reach up to 37\\%. A python implementation of our method is available at https://github.com/Mamba413/AdaDetectGPT.",
      "arxiv_url": "https://openreview.net/forum?id=3Qo2SRcHgU",
      "pdf_url": "https://openreview.net/pdf/dd2031f08502f004cd975ca7ca1bf663694103b4.pdf",
      "primary_category": "Large language models, DetectGPT, Statistical guarantees",
      "categories": [
        "Large language models",
        "DetectGPT",
        "Statistical guarantees"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BfO6od6JD6",
      "title": "Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning",
      "authors": [
        "Zhonghao He",
        "Tianyi Qiu",
        "Hirokazu Shirado",
        "Maarten Sap"
      ],
      "abstract": "Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for *belief entrenchment* in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates cannot be predicted from solely the current belief. We propose the unsupervised, regression-based *Martingale Score* to measure violations of this property, signaling a deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains, including event forecasting, value-laden questions, and academic paper review, we found such violations to be widespread across models, reasoning paradigms, problem domains, and system prompts, where the future beliefs are consistently predictable from the model's current belief, a phenomenon which we term *belief entrenchment*. Through comprehensive experiments, we identify the models (e.g., GPT-4o), reasoning techniques (e.g., chain of thought), and domains (e.g., forecasting) more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of the LLM reasoning process.",
      "arxiv_url": "https://openreview.net/forum?id=BfO6od6JD6",
      "pdf_url": "https://openreview.net/pdf/16550fde50b561ce565e1b821d74005a1fde4a33.pdf",
      "primary_category": "Martingale Score, Martingale property, bayesian reasoning",
      "categories": [
        "Martingale Score",
        "Martingale property",
        "bayesian reasoning",
        "LLM",
        "unsupervised",
        "evaluation",
        "belief entrenchment",
        "forecasting"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "853SwC2dMZ",
      "title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws",
      "authors": [
        "Zhixuan Pan",
        "Shaowen Wang",
        "Liao Pengfei",
        "Jian Li"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous tasks, yet principled explanations for their underlying mechanisms and several phenomena, such as scaling laws, hallucinations, and related behaviors, remain elusive. In this work, we revisit the classical relationship between compression and prediction, grounded in Kolmogorov complexity and Shannon information theory, to provide deeper insights into LLM behaviors. By leveraging the Kolmogorov Structure Function and interpreting LLM compression as a two-part coding process, we offer a detailed view of how LLMs acquire and store information across increasing model and data scales -- from pervasive syntactic patterns to progressively rarer knowledge elements. Motivated by this theoretical perspective and natural assumptions inspired by Heap’s and Zipf’s laws, we introduce a simplified yet representative hierarchical data-generation framework called the Syntax-Knowledge model. Under the Bayesian setting, we show that prediction and compression within this model naturally lead to diverse learning and scaling behaviors of LLMs. In particular, our theoretical analysis offers intuitive and principled explanations for both data and model scaling laws, the dynamics of knowledge acquisition during training and fine-tuning, factual knowledge hallucinations in LLMs. The experimental results validate our theoretical predictions.",
      "arxiv_url": "https://openreview.net/forum?id=853SwC2dMZ",
      "pdf_url": "https://openreview.net/pdf/95f61a66375ba3e46803c24b0ddc45e0df29334d.pdf",
      "primary_category": "Large Language Model, Scaling Law, Information Theory",
      "categories": [
        "Large Language Model",
        "Scaling Law",
        "Information Theory"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cQxLCVa9u7",
      "title": "HyGen: Efficient LLM Serving via Elastic Online-Offline Request Co-location",
      "authors": [
        "Ting Sun",
        "Penghan Wang",
        "Fan Lai"
      ],
      "abstract": "Large language models (LLMs) have facilitated a wide range of applications with distinct service-level objectives (SLOs), from latency-sensitive online tasks like interactive chatbots to throughput-oriented offline workloads like data synthesis. The existing deployment model, which dedicates machines to each workload, simplifies SLO management but often leads to poor resource utilization. This paper introduces HyGen, an interference-aware LLM serving system that enables efficient co-location of online and offline workloads while preserving SLOs. HyGen incorporates two key innovations: (1) performance control mechanisms, including a latency predictor to estimate batch execution time and an SLO-aware profiler to quantify latency interference, and (2) SLO-aware offline scheduling policies that maximize serving throughput and prevent starvation. Our evaluation on production workloads shows that HyGen achieves up to 3.9-5.8× throughput gains over online and hybrid serving baselines, while ensuring latency SLOs. The code of HyGen is publicly available at https://github.com/UIUC-MLSys/HyGen.",
      "arxiv_url": "https://openreview.net/forum?id=cQxLCVa9u7",
      "pdf_url": "https://openreview.net/pdf/40f47181fc0983cc4ba51054de34b5d6a4e75e14.pdf",
      "primary_category": "LLM Inference, Request Scheduling, Machine Learning Systems",
      "categories": [
        "LLM Inference",
        "Request Scheduling",
        "Machine Learning Systems"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "TecJ926Vgn",
      "title": "Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix",
      "authors": [
        "Ming Wen",
        "Jiaqi Zhu",
        "Yuedong Xu",
        "Yipeng Zhou",
        "DINGDING HAN"
      ],
      "abstract": "Large language models (LLMs) typically require fine-tuning for domain-specific tasks, and LoRA offers a computationally efficient approach by training low-rank adaptors. LoRA is also communication-efficient for federated LLMs when multiple users collaboratively fine-tune a global LLM model without sharing their proprietary raw data. However, even the transmission of local adaptors between a server and clients risks serious privacy leakage. Applying differential privacy (DP) to federated LoRA encounters a dilemma: adding noise to both adaptors amplifies synthetic noise on the model, while fixing one adaptor impairs the learnability of fine-tuning. In this paper, we propose FedASK (Differentially Private Federated Low Rank Adaptation with Double SKetching) , a novel federated LoRA framework to enable effective updating of both low-rank adaptor matrices with robust differential privacy. Inspired by randomized SVD, our key idea is a two-stage sketching pipeline. This pipeline first aggregates carefully sketched, privacy-preserving local updates, and then reconstructs the global matrices on the server to facilitate effective updating of both adaptors. We theoretically prove FedASK's differential privacy guarantee and its exact aggregation property. Comprehensive experiments demonstrate that FedASK consistently outperforms baseline methods across a variety of privacy settings and data distributions.",
      "arxiv_url": "https://openreview.net/forum?id=TecJ926Vgn",
      "pdf_url": "https://openreview.net/pdf/1285808214856d06a8f694d71f561d952897fd00.pdf",
      "primary_category": "Federated Learning, Low-Rank Adaptation, Differential Privacy",
      "categories": [
        "Federated Learning",
        "Low-Rank Adaptation",
        "Differential Privacy"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "aYRNINhNGV",
      "title": "Meta CLIP 2: A Worldwide Scaling Recipe",
      "authors": [
        "Yung-Sung Chuang",
        "Yang Li",
        "Dong Wang",
        "Ching-Feng Yeh",
        "Kehan Lyu",
        "Ramya Raghavendra",
        "James R. Glass",
        "LIFEI HUANG",
        "Jason E Weston",
        "Luke Zettlemoyer",
        "Xinlei Chen",
        "Zhuang Liu",
        "Saining Xie",
        "Wen-tau Yih",
        "Shang-Wen Li",
        "Hu Xu"
      ],
      "abstract": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., \"curse of multilinguality\" that is common in LLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data.\nIn zero-shot ImageNet classification, Meta CLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval. Code and model are available at https://github.com/facebookresearch/MetaCLIP.",
      "arxiv_url": "https://openreview.net/forum?id=aYRNINhNGV",
      "pdf_url": "https://openreview.net/pdf/302582bcc995ceb79d3c0f6ad78257f61bef0372.pdf",
      "primary_category": "CLIP, curation, multimodal",
      "categories": [
        "CLIP",
        "curation",
        "multimodal",
        "web",
        "multilingual",
        "data curation",
        "pretraining",
        "visual-language",
        "contrastive learning",
        "long-tail distribution",
        "scaling"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "a64D9Vl7wK",
      "title": "Predicting Empirical AI Research Outcomes with Language Models",
      "authors": [
        "Jiaxin Wen",
        "Chenglei Si",
        "Chen Yueh-Han",
        "He He",
        "Shi Feng"
      ],
      "abstract": "Many promising-looking ideas in AI research fail to deliver, but their validation takes substantial human labor and compute. Predicting an idea's chance of success is thus crucial for accelerating empirical AI research, a skill that even expert researchers can only acquire through substantial experience. We build the first benchmark for this task and compare LMs with human experts. Concretely, given two research ideas (e.g., two jailbreaking methods), we aim to predict which will perform better on a set of benchmarks. \nWe scrape ideas and experimental results from conference papers, yielding 1,585 human-verified idea pairs \\textit{published after our base model's cut-off date} for testing, and 6,000 pairs for training.\nWe then develop a system that combines a fine-tuned GPT-4.1 with a paper retrieval agent, and we recruit 25 human experts to compare with.\nIn the NLP domain, our system beats human experts by a large margin (64.4\\% v.s. 48.9\\%).\nOn the full test set, our system achieves 77\\% accuracy, while off-the-shelf frontier LMs like o3 perform no better than random guessing, even with the same retrieval augmentation.\nWe verify that our system does not exploit superficial features like idea complexity through extensive human-written and LM-designed robustness tests.\nFinally, we evaluate our system on unpublished novel ideas, including ideas generated by an AI ideation agent.\nOur system achieves 63.6\\% accuracy, demonstrating its potential as a reward model for improving idea generation models.\nAltogether, our results outline a promising new direction for LMs to accelerate empirical AI research.",
      "arxiv_url": "https://openreview.net/forum?id=a64D9Vl7wK",
      "pdf_url": "https://openreview.net/pdf/41c1dcfabab79f5c50263693b525a793ad543425.pdf",
      "primary_category": "LLM, Research Idea Evaluation, Automated Research",
      "categories": [
        "LLM",
        "Research Idea Evaluation",
        "Automated Research"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qYkhCah8OZ",
      "title": "Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation",
      "authors": [
        "Wenbin An",
        "Jiahao Nie",
        "Feng Tian",
        "Haonan Lin",
        "mingxiang cai",
        "Yaqiang Wu",
        "QianYing Wang",
        "Xiaoqin Zhang",
        "Shijian Lu"
      ],
      "abstract": "Despite their recent progress, Multimodal Large Language Models (MLLMs) often struggle in knowledge-intensive tasks due to the limited and outdated parametric knowledge acquired during training. Multimodal Retrieval Augmented Generation addresses this issue by retrieving contextual knowledge from external databases, thereby enhancing MLLMs with expanded knowledge sources. \nHowever, existing MLLMs often fail to fully leverage the retrieved contextual knowledge for response generation. We examine representative MLLMs and identify two major causes, namely, attention bias toward different tokens and knowledge conflicts between parametric and contextual knowledge. To this end, we design Adaptive Logits Fusion and Attention Reallocation (ALFAR), a training-free and plug-and-play approach that improves MLLM responses by maximizing the utility of the retrieved knowledge. Specifically, ALFAR tackles the challenges from two perspectives. First, it alleviates attention bias by adaptively shifting attention from visual tokens to relevant context tokens according to query-context relevance. Second, it decouples and weights parametric and contextual knowledge at output logits, mitigating conflicts between the two types of knowledge. As a plug-and-play method, ALFAR achieves superior performance across diverse datasets without requiring additional training or external tools. Extensive experiments over multiple MLLMs and benchmarks show that ALFAR consistently outperforms the state-of-the-art by large margins. Our code and data are available at https://github.com/Lackel/ALFAR.",
      "arxiv_url": "https://openreview.net/forum?id=qYkhCah8OZ",
      "pdf_url": "https://openreview.net/pdf/40d94836204a19bf22a4813c820925434476760b.pdf",
      "primary_category": "Multimodal Large Language Models, Multimodal Retrieval Augmented Generation",
      "categories": [
        "Multimodal Large Language Models",
        "Multimodal Retrieval Augmented Generation"
      ],
      "tags": [
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zlMupLoKRf",
      "title": "Causality Meets the Table: Debiasing LLMs for Faithful TableQA via Front-Door Intervention",
      "authors": [
        "Zhen Yang",
        "Ziwei Du",
        "Minghan Zhang",
        "Wei Du",
        "Jie Chen",
        "Fulan Qian",
        "Shu Zhao"
      ],
      "abstract": "Table Question Answering (TableQA) combines natural language understanding and structured data reasoning, posing challenges in semantic interpretation and logical inference. Recent advances in Large Language Models (LLMs) have improved TableQA performance through Direct Prompting and Agent paradigms. However, these models often rely on spurious correlations, as they tend to overfit to token co-occurrence patterns in pretraining corpora, rather than perform genuine reasoning. To address this issue, we propose Causal Intervention TableQA (CIT), which is based on a structural causal graph and applies front-door adjustment to eliminate bias caused by token co-occurrence. CIT formalizes TableQA as a causal graph and identifies token co-occurrence patterns as confounders. By applying front-door adjustment, CIT guides question variant generation and reasoning to reduce confounding effects. Experiments on multiple benchmarks show that CIT achieves state-of-the-art performance, demonstrating its effectiveness in mitigating bias. Consistent gains across various LLMs further confirm its generalizability.",
      "arxiv_url": "https://openreview.net/forum?id=zlMupLoKRf",
      "pdf_url": "https://openreview.net/pdf/cf0cd1c3230b290f0202028b7f264b6b0e78ab6f.pdf",
      "primary_category": "TableQA, Causal, Large Language Model",
      "categories": [
        "TableQA",
        "Causal",
        "Large Language Model",
        "Question Answering"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "c6CAVKlKmU",
      "title": "WorldMem: Long-term Consistent World Simulation with Memory",
      "authors": [
        "Zeqi Xiao",
        "Yushi LAN",
        "Yifan Zhou",
        "Wenqi Ouyang",
        "Shuai Yang",
        "Yanhong Zeng",
        "Xingang Pan"
      ],
      "abstract": "World simulation has gained increasing popularity due to its ability to model virtual environments and predict the consequences of actions. However, the limited temporal context window often leads to failures in maintaining long-term consistency, particularly in preserving 3D spatial consistency. In this work, we present WorldMem, a framework that enhances scene generation with a memory bank consisting of memory units that store memory frames and states (e.g., poses and timestamps). By employing state-aware memory attention that effectively extracts relevant information from these memory frames based on their states, our method is capable of accurately reconstructing previously observed scenes, even under significant viewpoint or temporal gaps. Furthermore, by incorporating timestamps into the states, our framework not only models a static world but also captures its dynamic evolution over time, enabling both perception and interaction within the simulated world. Extensive experiments in both virtual and real scenarios validate the effectiveness of our approach.",
      "arxiv_url": "https://openreview.net/forum?id=c6CAVKlKmU",
      "pdf_url": "https://openreview.net/pdf/6fb1926ef50fc9595056113d70f67236bd3090a2.pdf",
      "primary_category": "world simulation, memory",
      "categories": [
        "world simulation",
        "memory"
      ],
      "tags": [
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "CqLWckpTbG",
      "title": "DeepDiver: Adaptive Web-Search Intensity Scaling via Reinforcement Learning",
      "authors": [
        "Wenxuan Shi",
        "Haochen Tan",
        "Chuqiao Kuang",
        "Xiaoguang Li",
        "Hanting Chen",
        "Xiaozhe Ren",
        "Yasheng Wang",
        "Lu Hou",
        "Lifeng Shang"
      ],
      "abstract": "Information seeking demands iterative evidence gathering and reflective reasoning, yet large language models (LLMs) still struggle with it in open-web question answering. Existing prompting and supervised fine-tuning (SFT) methods remain fixed by prompt rules or training corpora, and are usually benchmarked only on well-structured wiki sources, limiting real-world adaptability. We introduce $\\textbf{WebPuzzle}$, a 24k-sample training and 275-sample test benchmark that evaluates information seeking on the live internet, across both wiki and open-domain queries. Leveraging 7k WebPuzzle instances, we develop $\\textbf{DeepDiver}$, a reinforcement-learning (RL) framework that cultivates $\\textbf{Search Intensity Scaling (SIS)}$—an emergent ability to escalate search frequency and depth instead of settling on overconfident, under-evidenced answers. With SIS, Qwen2.5-7B-Instruct and Pangu-7B-Reasoner attain performance on real-web tasks comparable to the 671B-parameter DeepSeek-R1. We detail DeepDiver’s curriculum from cold-start SFT to a well designed RL procedure, and show that its seeking policy generalized from closed-ended queries to open-ended generation such as long-form writing. Our results advance adaptive information seeking in LLMs and provide a rigorous benchmark for future work.",
      "arxiv_url": "https://openreview.net/forum?id=CqLWckpTbG",
      "pdf_url": "https://openreview.net/pdf/4da779e56e6d170cf4fc07f2af1015f08b35314e.pdf",
      "primary_category": "Agentic Reinforcement Learning, Retrieval Augmented Generation, Search Intensity Scaling",
      "categories": [
        "Agentic Reinforcement Learning",
        "Retrieval Augmented Generation",
        "Search Intensity Scaling",
        "Data Synthesis"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8LO0vLRXpz",
      "title": "Enhancing LLM Planning for Robotics Manipulation through Hierarchical Procedural Knowledge Graphs",
      "authors": [
        "Jiacong Zhou",
        "Jiaxu Miao",
        "Xianyun Wang",
        "Jun Yu"
      ],
      "abstract": "Large Language Models (LLMs) have shown the promising planning capabilities for robotic manipulation, which advances the development of embodied intelligence significantly. However, existing LLM-driven robotic manipulation approaches excel at simple pick-and-place tasks but are insufficient for complex manipulation tasks due to inaccurate procedural knowledge. Besides, for embodied intelligence, equipping a large scale LLM is energy-consuming and inefficient, which affects its real-world application.\nTo address the above problems, we propose Hierarchical Procedural Knowledge Graphs (\\textbf{HP-KG}) to enhance LLMs for complex robotic planning while significantly reducing the demand for LLM scale in robotic manipulation. \nConsidering that the complex real-world tasks require multiple steps, and each step is composed of robotic-understandable atomic actions, we design a hierarchical knowledge graph structure to model the relationships between tasks, steps, and actions. This design bridges the gap between human instructions and robotic manipulation actions. To construct HP-KG, we develop an automatic knowledge graph construction framework powered by LLM-based multi-agents, which eliminates costly manual efforts while maintaining high-quality graph structures. \nThe resulting HP-KG encompasses over 40k activity steps across more than 6k household tasks, spanning diverse everyday scenarios. Extensive experiments demonstrate that small scale LLMs (7B) enhanced by our HP-KG significantly improve the planning capabilities, which are stronger than 72B LLMs only. Encouragingly, our approach remains effective on the most powerful GPT-4o model.",
      "arxiv_url": "https://openreview.net/forum?id=8LO0vLRXpz",
      "pdf_url": "https://openreview.net/pdf/46ff7c53a48a15dc3fd5f9022062622e9c136c9d.pdf",
      "primary_category": "Large Language Model, Robotics Manipulation",
      "categories": [
        "Large Language Model",
        "Robotics Manipulation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Q3DtkFJ1Ap",
      "title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search",
      "authors": [
        "Zonglin Yang",
        "Wanhao Liu",
        "Ben Gao",
        "Yujie Liu",
        "Wei Li",
        "Tong Xie",
        "Lidong Bing",
        "Wanli Ouyang",
        "Erik Cambria",
        "Dongzhan Zhou"
      ],
      "abstract": "Large language models (LLMs) have shown promise in automating scientific hypothesis generation, yet existing approaches primarily yield coarse-grained hypotheses lacking critical methodological and experimental details. We introduce and formally define the new task of fine-grained scientific hypothesis discovery, which entails generating detailed, experimentally actionable hypotheses from coarse initial research directions. We frame this as a combinatorial optimization problem and investigate the upper limits of LLMs' capacity to solve it when maximally leveraged. Specifically, we explore four foundational questions: (1) how to best harness an LLM's internal heuristics to formulate the fine-grained hypothesis it itself would judge as the most promising among all the possible hypotheses it might generate, based on its own internal scoring-thus defining a latent reward landscape over the hypothesis space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment with ground-truth hypotheses; (3) whether shaping the reward landscape using an ensemble of diverse LLMs of similar capacity yields better outcomes than defining it with repeated instances of the strongest LLM among them; and (4) whether an ensemble of identical LLMs provides a more reliable reward landscape than a single LLM. To address these questions, we propose a hierarchical search method that incrementally proposes and integrates details into the hypothesis, progressing from general concepts to specific experimental configurations. We show that this hierarchical process smooths the reward landscape and enables more effective optimization. Empirical evaluations on a new benchmark of expert-annotated fine-grained hypotheses from recent literature show that our method consistently outperforms strong baselines.",
      "arxiv_url": "https://openreview.net/forum?id=Q3DtkFJ1Ap",
      "pdf_url": "https://openreview.net/pdf/c882167545d3506b5c5a2eb7b5c1a0526cc477f3.pdf",
      "primary_category": "scientific discovery",
      "categories": [
        "scientific discovery"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "E1FrjgaG1J",
      "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning",
      "authors": [
        "Chen Qian",
        "Dongrui Liu",
        "Haochen Wen",
        "Zhen Bai",
        "Yong Liu",
        "Jing Shao"
      ],
      "abstract": "Large reasoning models (LRMs) have demonstrated impressive capabilities in complex problem-solving, yet their internal reasoning mechanisms remain poorly understood.\nIn this paper, we investigate the reasoning trajectories of LRMs from an information-theoretic perspective. \nBy tracking how mutual information (MI) between intermediate representations and the correct answer evolves during LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at specific generative steps exhibits a sudden and significant increase during LRM's reasoning process. \nWe theoretically analyze such phenomenon and show that as MI increases, the probability of model's prediction error decreases.\nFurthermore, these MI peaks often correspond to tokens expressing reflection or transition, such as \"Hmm\", \"Wait\" and \"Therefore,\" which we term as the thinking tokens.\nWe then demonstrate that these thinking tokens are crucial for LRM's reasoning performance, while other tokens has minimal impacts.\nBuilding on these analyses, we propose two simple yet effective methods to improve LRM's reasoning performance, by delicately leveraging these thinking tokens.\nOverall, our work provides novel insights into the reasoning mechanisms of LRMs and offers practical ways to improve their reasoning capabilities.\nThe code is available at \\url{https://github.com/ChnQ/MI-Peaks}.",
      "arxiv_url": "https://openreview.net/forum?id=E1FrjgaG1J",
      "pdf_url": "https://openreview.net/pdf/c3c72e4c1f8357c9104201b84b6fba745157bb28.pdf",
      "primary_category": "Large Language Model, Reasoning, Information Theory",
      "categories": [
        "Large Language Model",
        "Reasoning",
        "Information Theory"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "AZ1iyo58F8",
      "title": "Elevating Visual Perception in Multimodal LLMs with Visual Embedding Distillation",
      "authors": [
        "Jitesh Jain",
        "Zhengyuan Yang",
        "Humphrey Shi",
        "Jianfeng Gao",
        "Jianwei Yang"
      ],
      "abstract": "In recent times, the standard practice for developing MLLMs is to feed features from vision encoder(s) into the LLM and train with natural language supervision. This approach often causes models to lean towards language comprehension and undermine the rich visual perception signals present in the data, which are critical for tasks involving spatial reasoning in the domain of embodied AI and robotics. Is it possible to optimize both at the same time? In this work, we propose VisPer-LM, the first approach that infuses visual perception knowledge from expert vision encoders into the LLM's (of an MLLM) hidden representations. We start by investigating MLLMs trained solely with natural language supervision and identify a positive correlation between the quality of visual representations within these models and their downstream performance. Given this insight, we formulate the objective during the pretraining stage in MLLMs as a coupled optimization of predictive visual embedding and next (text) token prediction.  Moreover, through extensive probing, we observe improved visual representation quality due to embedding optimization, underscoring the effectiveness of our probing setup. We demonstrate that our VisPer-LM outperforms the single and multi-encoder baselines, proving our approach's superiority over explicitly feeding the corresponding features to the LLM. In particular, VisPer-LM boosts performance by an average margin of up to 2.5% on various benchmarks, with a notable improvement of 8.7% on the Depth task in CV-Bench.",
      "arxiv_url": "https://openreview.net/forum?id=AZ1iyo58F8",
      "pdf_url": "https://openreview.net/pdf/79c1152558904f8c4793fa636c396e0a5c4740bf.pdf",
      "primary_category": "Representation Learning, Multimodal Large Language Models, Visual Perception",
      "categories": [
        "Representation Learning",
        "Multimodal Large Language Models",
        "Visual Perception"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "gpCleSGCkV",
      "title": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense",
      "authors": [
        "Yangyang Guo",
        "Fangkai Jiao",
        "Liqiang Nie",
        "Mohan Kankanhalli"
      ],
      "abstract": "The vulnerability of Vision Large Language Models (VLLMs) to jailbreak attacks appears as no surprise.\nHowever, recent defense mechanisms against these attacks have reached near-saturation performance on benchmark evaluations, often with minimal effort.\nThis dual high performance in both attack and defense gives rise to a fundamental and perplexing paradox.\nTo gain a deep understanding of this issue and thus further help strengthen the trustworthiness of VLLMs, this paper makes three key contributions: \ni) One tentative explanation for VLLMs being prone to jailbreak attacks--inclusion of vision inputs, as well as its in-depth analysis. \nii) The recognition of a largely ignored problem in existing VLLM defense mechanisms--over-prudence.\nThe problem causes these defense methods to exhibit unintended abstention, even in the presence of benign inputs, thereby undermining their reliability in faithfully defending against attacks.\niii) A simple safety-aware method--LLM-Pipeline.\nOur method repurposes the more advanced guardrails of LLMs on the fly, serving as an effective alternative detector prior to VLLM response. \nLast but not least, we find that the two representative evaluation methods for jailbreak often exhibit chance agreement.\nThis limitation makes it potentially misleading when evaluating attack strategies or defense mechanisms.\nWe believe the findings from this paper offer useful insights to rethink the foundational development of VLLM safety with respect to benchmark datasets, defense strategies, and evaluation methods.",
      "arxiv_url": "https://openreview.net/forum?id=gpCleSGCkV",
      "pdf_url": "https://openreview.net/pdf/85f3e38bd08668ee901051237edc01d1b8d8823e.pdf",
      "primary_category": "Vision Large Language Model, Safety Alignment, Jailbreak Attack",
      "categories": [
        "Vision Large Language Model",
        "Safety Alignment",
        "Jailbreak Attack",
        "Jailbreak Defense"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "dAwKePZvcN",
      "title": "SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents",
      "authors": [
        "Wanxin Tian",
        "Shijie Zhang",
        "Kevin Zhang",
        "Xiaowei Chi",
        "Chun-Kai Fan",
        "Junyu Lu",
        "Yulin Luo",
        "Qiang Zhou",
        "Yiming Zhao",
        "Ning Liu",
        "Siyu Lin",
        "Zhiyuan Qin",
        "Xiaozhu Ju",
        "Shanghang Zhang",
        "Jian Tang"
      ],
      "abstract": "Self-evolution, the ability of agents to autonomously improve their reasoning and behavior, is essential for the embodied domain with long-horizon, real-world tasks. Despite current advancements in reinforcement fine-tuning (RFT) showing strong performance in enhancing reasoning in LLMs, its potential to enable self-evolving embodied intelligence with multi-modal interactions remains largely unexplored. Specifically, reinforcement fine-tuning faces two fundamental obstacles in embodied settings: (i) the lack of accessible intermediate rewards in multi-step reasoning tasks limits effective learning signals, and (ii) reliance on hand-crafted reward functions restricts generalization to novel tasks and environments. To address these challenges, we present *Self-Evolving Embodied Agents-R1*, **SEEA-R1**, the first RFT framework designed for enabling the self-evolving capabilities of embodied agents. Specifically, to convert sparse delayed rewards into denser intermediate signals that improve multi-step reasoning, we propose Tree-based group relative policy optimization (**Tree-GRPO**) integrates Monte Carlo Tree Search into GRPO. To generalize reward estimation across tasks and scenes, supporting autonomous adaptation and reward-driven self-evolution, we further introduce Multi-modal Generative Reward Model (**MGRM**). To holistically evaluate the effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing state-of-the-art methods with scores of 85.07\\% (textual) and 46.27\\% (multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also achieves scores of 80.3\\% (textual) and 44.03\\% (multi-modal) without ground truth reward, surpassing all open-source baselines and highlighting its scalability as a self-evolving embodied agent. Additional experiments and qualitative analysis further support the potential of SEEA-R1 for future research in scalable embodied intelligence. Project page is at https://seea-r1.github.io/.",
      "arxiv_url": "https://openreview.net/forum?id=dAwKePZvcN",
      "pdf_url": "https://openreview.net/pdf/83c7ef71a62b520ab6cc9c5eb50b4d91606051d6.pdf",
      "primary_category": "Embodied AI, Agents, Generative Reward",
      "categories": [
        "Embodied AI",
        "Agents",
        "Generative Reward"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "5Iw1nDtYmT",
      "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows",
      "authors": [
        "Zaifeng Pan",
        "AJJKUMAR PATEL",
        "Yipeng Shen",
        "Zhengding Hu",
        "Yue Guan",
        "Wan-Lu Li",
        "Lianhui Qin",
        "Yida Wang",
        "Yufei Ding"
      ],
      "abstract": "Large language model (LLM) based agentic workflows have become a popular paradigm for coordinating multiple specialized agents to solve complex tasks. To improve serving efficiency, existing LLM systems employ prefix caching to reuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby avoiding redundant computation across repeated invocations. However, current systems typically evict KV caches using a Least Recently Used (LRU) policy, which fails to anticipate future agent usage and often discards KV caches shortly before their reuse. This leads to frequent cache misses and substantial recomputation or swap- ping overhead. We present KVFlow, a workflow-aware KV cache management framework tailored for agentic workloads. KVFlow abstracts the agent execution schedule as an Agent Step Graph and assigns each agent a steps-to-execution value that estimates its temporal proximity to future activation. These values guide a fine-grained eviction policy at the KV node level, allowing KVFlow to preserve entries likely to be reused and efficiently manage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a fully overlapped KV prefetching mecha- nism, which proactively loads required tensors from CPU to GPU in background threads for agents scheduled in the next step, thereby avoiding cache miss stalls during generation. Compared to SGLang with hierarchical radix cache, KVFlow achieves up to 1.83× speedup for single workflows with large prompts, and up to 2.19× speedup for scenarios with many concurrent workflows.",
      "arxiv_url": "https://openreview.net/forum?id=5Iw1nDtYmT",
      "pdf_url": "https://openreview.net/pdf/2c47adb29432f99879fceb1371b72f6e97e1f3ac.pdf",
      "primary_category": "LLM serving systems, multi-agent workflow, prefix caching",
      "categories": [
        "LLM serving systems",
        "multi-agent workflow",
        "prefix caching"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "LNWyf2RR1V",
      "title": "Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting",
      "authors": [
        "Chenchen Tan",
        "Youyang Qu",
        "Xinghao Li",
        "Hui Zhang",
        "Shujie Cui",
        "Cunjian Chen",
        "Longxiang Gao"
      ],
      "abstract": "The increase in computing power and the necessity of AI-assisted decision-making boost the growing application of large language models (LLMs). Along with this, the potential retention of sensitive data of LLMs has spurred increasing research into machine unlearning. However, existing unlearning approaches face a critical dilemma: Aggressive unlearning compromises model utility, while conservative strategies preserve utility but risk hallucinated responses. This significantly limits LLMs' reliability in knowledge-intensive applications. To address this, we introduce a novel Attention-Shifting (AS) framework for selective unlearning. AS is driven by two design objectives: (1) context-preserving suppression that attenuates attention to fact-bearing tokens without disrupting LLMs' linguistic structure; and (2) hallucination-resistant response shaping that discourages fabricated completions when queried about unlearning content. AS realizes these objectives through two attention-level interventions, which are importance-aware suppression applied to the unlearning set to reduce reliance on memorized knowledge and attention-guided retention enhancement that reinforces attention toward semantically essential tokens in the retained dataset to mitigate unintended degradation. These two components are jointly optimized via a dual-loss objective, which forms a soft boundary that localizes unlearning while preserving unrelated knowledge under representation superposition. Experimental results show that AS improves performance preservation over the state-of-the-art unlearning methods, achieving up to 15\\% higher accuracy on the ToFU benchmark and 10\\% on the TDEC benchmark, while maintaining competitive hallucination-free unlearning effectiveness. Compared to existing methods, AS demonstrates a superior balance between unlearning effectiveness, generalization, and response reliability.",
      "arxiv_url": "https://openreview.net/forum?id=LNWyf2RR1V",
      "pdf_url": "https://openreview.net/pdf/ba7d15ee60c6daa5f9c70b57695f8a91605da5c9.pdf",
      "primary_category": "machine unlearning, large language model, LLM unlearn",
      "categories": [
        "machine unlearning",
        "large language model",
        "LLM unlearn"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "usOkGv1S7M",
      "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning",
      "authors": [
        "Mingyang Liu",
        "Gabriele Farina",
        "Asuman E. Ozdaglar"
      ],
      "abstract": "Post-training has demonstrated its importance in enhancing the reasoning capabilities of large language models (LLMs). The primary post-training methods can be categorized into supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). SFT is efficient and well-suited for small language models, but it may lead to overfitting and limit the reasoning abilities of larger models. In contrast, RFT generally yields better generalization but depends heavily on the strength of the base model. To address the limitations of SFT and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm that unifies SFT and RFT into a single, integrated process. UFT enables the model to effectively explore solutions while incorporating informative supervision signals, bridging the gap between memorizing and thinking underlying existing methods. Notably, UFT outperforms both SFT and RFT in general, regardless of model sizes. Furthermore, we theoretically prove that UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for the first time that unified training can exponentially accelerate convergence on long-horizon reasoning tasks.",
      "arxiv_url": "https://openreview.net/forum?id=usOkGv1S7M",
      "pdf_url": "https://openreview.net/pdf/4638c4fdf4fd757fe0b2cd9e2720d114e2529986.pdf",
      "primary_category": "Reasoning, SFT, RFT",
      "categories": [
        "Reasoning",
        "SFT",
        "RFT"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BL8h1Axr0i",
      "title": "Storyboard-guided Alignment for Fine-grained Video Action Recognition",
      "authors": [
        "Enqi Liu",
        "Liyuan Pan",
        "Yan Yang",
        "Yiran Zhong",
        "Zhijing Wu",
        "Xinxiao Wu",
        "Liu Liu"
      ],
      "abstract": "Fine-grained video action recognition can be formulated as a video–text matching problem. Previous approaches primarily rely on global video semantics to consolidate video embeddings, often leading to misaligned video–text pairs due to inaccurate atomic-level action understanding. This inaccuracy arises due to i) videos with distinct global semantics may share similar atomic actions or visual appearances, and ii) atomic actions can be momentary, gradual, or not directly aligned with overarching video semantics. Inspired by storyboarding, where a script is segmented into individual shots, we propose a multi-granularity framework, SFAR. SFAR generates fine-grained descriptions of common atomic actions for each global semantic using a large language model. Unlike existing works that refine global semantics with auxiliary video frames, SFAR introduces a filtering metric to ensure correspondence between the descriptions and the global semantics, eliminating the need for direct video involvement and thereby enabling more nuanced recognition of subtle actions. By leveraging both global semantics and fine-grained descriptions, our SFAR effectively identifies prominent frames within videos, thereby improving the accuracy of embedding aggregation. Extensive experiments on various video action recognition datasets demonstrate the competitive performance of our SFAR in supervised, few-shot, and zero-shot settings.",
      "arxiv_url": "https://openreview.net/forum?id=BL8h1Axr0i",
      "pdf_url": "https://openreview.net/pdf/96df5563509acbddcad8b0b98ca3b85e78e897f0.pdf",
      "primary_category": "Storyboard, Fine-grained, Atomic action.",
      "categories": [
        "Storyboard",
        "Fine-grained",
        "Atomic action."
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "S1F2qhendd",
      "title": "Self-Refining Language Model Anonymizers via Adversarial Distillation",
      "authors": [
        "Kyuyoung Kim",
        "Hyunjun Jeon",
        "Jinwoo Shin"
      ],
      "abstract": "Large language models (LLMs) are increasingly used in sensitive domains, where their ability to infer personal data from seemingly benign text introduces emerging privacy risks. While recent LLM-based anonymization methods help mitigate such risks, they often rely on proprietary models (e.g., GPT-4), raising concerns about cost and the potential exposure of sensitive data to untrusted external systems. To address this, we introduce $\\textit{SElf-refining Anonymization with Language model}$ (SEAL), a novel distillation framework for training small language models (SLMs) to perform effective anonymization without relying on external models at inference time. SEAL leverages adversarial interactions between an LLM anonymizer and an inference model to collect trajectories of anonymized texts and inferred attributes, which are then used to distill anonymization and critique capabilities into SLMs through supervised fine-tuning and preference learning. The resulting models learn both to anonymize text and to evaluate their outputs, enabling iterative improvement of anonymization quality via self-refinement. Experiments on SynthPAI, a dataset of synthetic personal profiles and text comments, demonstrate that SLMs trained with SEAL achieve substantial improvements in anonymization capabilities. Notably, 8B models attain a privacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with self-refinement, even surpass it in terms of privacy protection. These results highlight the effectiveness of our adversarial distillation framework for training SLMs as efficient anonymizers.",
      "arxiv_url": "https://openreview.net/forum?id=S1F2qhendd",
      "pdf_url": "https://openreview.net/pdf/3f8a6454ad03b1ebbd65fb27990a3c58e2aa7919.pdf",
      "primary_category": "privacy, anonymization, distillation",
      "categories": [
        "privacy",
        "anonymization",
        "distillation",
        "large language models",
        "small language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QQUhGPST45",
      "title": "Personalized Federated Conformal Prediction with Localization",
      "authors": [
        "Yinjie Min",
        "Chuchen Zhang",
        "Liuhua Peng",
        "Changliang Zou"
      ],
      "abstract": "Personalized federated learning addresses data heterogeneity across distributed agents but lacks uncertainty quantification that is both agent-specific and instance-specific, which is a critical requirement for risk-sensitive applications. We propose personalized federated conformal prediction (PFCP), a novel framework that combines personalized federated learning with conformal prediction to provide statistically valid agent-personalized prediction sets with instance-localization. By leveraging privacy-preserving knowledge transfer from other source agents, PFCP ensures marginal coverage guarantees for target agents while significantly improving conditional coverage performance on individual test instances, which has been validated by extensive experiments.",
      "arxiv_url": "https://openreview.net/forum?id=QQUhGPST45",
      "pdf_url": "https://openreview.net/pdf/c9c590d05004466a99dc663378ba6b4afa98ae61.pdf",
      "primary_category": "Personalized federated learning; Conformal prediction; Localization; Conditional coverage",
      "categories": [
        "Personalized federated learning; Conformal prediction; Localization; Conditional coverage"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "YFa7eULIeN",
      "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models",
      "authors": [
        "Ziyi Wu",
        "Anil Kag",
        "Ivan Skorokhodov",
        "Willi Menapace",
        "Ashkan Mirzaei",
        "Igor Gilitschenski",
        "Sergey Tulyakov",
        "Aliaksandr Siarohin"
      ],
      "abstract": "Direct Preference Optimization (DPO) has recently been applied as a post‑training technique for text-to-video diffusion models.\nTo obtain training data, annotators are asked to provide preferences between two videos generated from independent noise.\nHowever, this approach prohibits fine-grained comparisons, and we point out that it biases the annotators towards low-motion clips as they often contain fewer visual artifacts.\nIn this work, we introduce DenseDPO, a method that addresses these shortcomings by making three contributions.\nFirst, we create each video pair for DPO by denoising corrupted copies of a ground truth video.\nThis results in aligned pairs with similar motion structures while differing in local details, effectively neutralizing the motion bias.\nSecond, we leverage the resulting temporal alignment to label preferences on short segments rather than entire clips, yielding a denser and more precise learning signal.\nWith only one‑third of the labeled data, DenseDPO greatly improves motion generation over vanilla DPO, while matching it in text alignment, visual quality, and temporal consistency.\nFinally, we show that DenseDPO unlocks automatic preference annotation using off-the-shelf Vision Language Models (VLMs): GPT accurately predicts segment-level preferences similar to task-specifically fine-tuned video reward models, and DenseDPO trained on these labels achieves performance close to using human labels.",
      "arxiv_url": "https://openreview.net/forum?id=YFa7eULIeN",
      "pdf_url": "https://openreview.net/pdf/9bbf9a9f91d2e9e4ada48f9d6a6e3ad0f5860942.pdf",
      "primary_category": "video generation, diffusion model, direct preference optimization",
      "categories": [
        "video generation",
        "diffusion model",
        "direct preference optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "dCGQlVRa2B",
      "title": "Beyond Value Functions: Single-Loop Bilevel Optimization under Flatness Conditions",
      "authors": [
        "Liuyuan Jiang",
        "Quan Xiao",
        "Lisha Chen",
        "Tianyi Chen"
      ],
      "abstract": "Bilevel optimization, a hierarchical optimization paradigm, has gained significant attention in a wide range of practical applications, notably in the fine-tuning of generative models. However, due to the nested problem structure, most existing algorithms require either the Hessian vector calculation or the nested loop updates, which are computationally inefficient in large language model (LLM) fine-tuning. In this paper, building upon the fully first-order penalty-based approach, we propose an efficient value function-free (\\textsf{PBGD-Free}) algorithm that eliminates the loop of solving the lower-level problem and admits fully single-loop updates. Inspired by the landscape analysis of representation learning-based LLM fine-tuning problem, we propose a relaxed flatness condition for the upper-level function and prove the convergence of the proposed value-function-free algorithm. We test the performance of the proposed algorithm in various applications and demonstrate its superior computational efficiency over the state-of-the-art bilevel methods.",
      "arxiv_url": "https://openreview.net/forum?id=dCGQlVRa2B",
      "pdf_url": "https://openreview.net/pdf/9ff1eb928c0a714ee78ec1b8c2f40fc750c94080.pdf",
      "primary_category": "bilevel optimization, Holder's condition, value function",
      "categories": [
        "bilevel optimization",
        "Holder's condition",
        "value function",
        "penalty bilevel"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9o7oH6DAHB",
      "title": "LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning",
      "authors": [
        "Junyu Chen",
        "Junzhuo Li",
        "Zhen Peng",
        "Wenjie Wang",
        "Yuxiang Ren",
        "Long Shi",
        "Xuming Hu"
      ],
      "abstract": "Quantization and fine-tuning are crucial for deploying large language models (LLMs) on resource-constrained edge devices. However, fine-tuning quantized models presents significant challenges, primarily stemming from: First, the mismatch in data types between the low-precision quantized weights (e.g., 4-bit) and the high-precision adaptation weights (e.g., 16-bit). This mismatch limits the computational efficiency advantage offered by quantized weights during inference. Second, potential accuracy degradation when merging these high-precision adaptation weights into the low-precision quantized weights, as the adaptation weights often necessitate approximation or truncation. Third, as far as we know, no existing methods support the lossless merging of adaptation while adjusting all quantized weights. To address these challenges, we introduce lossless ternary adaptation for quantization-aware fine-tuning (LoTA-QAF). This is a novel fine-tuning method specifically designed for quantized LLMs, enabling the lossless merging of ternary adaptation weights into quantized weights and the adjustment of all quantized weights. LoTA-QAF operates through a combination of: i) A custom-designed ternary adaptation (TA) that aligns ternary weights with the quantization grid and uses these ternary weights to adjust quantized weights. ii) A TA-based mechanism that enables the lossless merging of adaptation weights. iii) Ternary signed gradient descent (t-SignSGD) for updating the TA weights. We apply LoTA-QAF to Llama-3.1/3.3 and Qwen-2.5 model families and validate its effectiveness on several downstream tasks. On the MMLU benchmark, our method effectively recovers performance for quantized models, surpassing 16-bit LoRA by up to 5.14\\%. For task-specific fine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still outperforms other methods. Code is available in github.com/KingdalfGoodman/LoTA-QAF.",
      "arxiv_url": "https://openreview.net/forum?id=9o7oH6DAHB",
      "pdf_url": "https://openreview.net/pdf/87e9e056817eb2d02e742e66682de092d74a203b.pdf",
      "primary_category": "Quantization, Fine-Tuning, PEFT",
      "categories": [
        "Quantization",
        "Fine-Tuning",
        "PEFT",
        "Efficient LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Rdz6ESQYkK",
      "title": "GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining",
      "authors": [
        "Chunyu Wei",
        "Wenji Hu",
        "Xingjia Hao",
        "Xin Wang",
        "Yifan Yang",
        "Yunhai Wang",
        "Yang Tian",
        "Yueguo Chen"
      ],
      "abstract": "Large Language Models (LLMs) face significant limitations when applied to large-scale graphs, struggling with context constraints and inflexible reasoning. We introduce GraphChain, a novel framework enabling LLMs to analyze large graphs by orchestrating dynamic sequences of specialized tools, mimicking human exploratory processes. GraphChain incorporates two core technical contributions: (1) Progressive Graph Distillation, a reinforcement learning approach that learns to generate tool sequences balancing task relevance and intermediate state compression, thereby overcoming LLM context limitations. (2) Structure-aware Test-Time Adaptation (STTA), a mechanism using a lightweight, self-supervised adapter conditioned on graph spectral properties to efficiently adapt a frozen LLM policy to diverse graph structures via soft prompts without retraining. Experiments show GraphChain significantly outperforms prior methods, enabling scalable and adaptive LLM-driven graph analysis.",
      "arxiv_url": "https://openreview.net/forum?id=Rdz6ESQYkK",
      "pdf_url": "https://openreview.net/pdf/f02b4d56b69187bc1f3068acff465c82ae91fdfe.pdf",
      "primary_category": "Large Language Model, Graph Analysis, Tool Learning",
      "categories": [
        "Large Language Model",
        "Graph Analysis",
        "Tool Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "OGgV9hpVGD",
      "title": "From Style to Facts: Mapping the Boundaries of Knowledge Injection with Finetuning",
      "authors": [
        "Eric Zhao",
        "Pranjal Awasthi",
        "Nika Haghtalab"
      ],
      "abstract": "Finetuning provides a scalable and cost-effective means of customizing language models for specific tasks or response styles, with greater reliability than prompting or in-context learning. In contrast, the conventional wisdom is that injecting knowledge via finetuning results in brittle performance and poor generalization. We argue that the dichotomy of \"task customization\" (e.g., instruction tuning) and \"knowledge injection\" (e.g., teaching new facts) is a distinction without a difference. We instead identify concrete factors that explain the heterogeneous effectiveness observed with finetuning. To this end, we conduct a large-scale experimental study of finetuning the frontier Gemini v1.5 model family on a spectrum of datasets that are artificially engineered to interpolate between the strengths and failure modes of finetuning. Our findings indicate that question-answer training data formats provide much stronger knowledge generalization than document/article-style training data, numerical information can be harder for finetuning to retain than categorical information, and models struggle to apply finetuned knowledge during multi-step reasoning even when trained on similar examples---all factors that render ``knowledge injection'' to be especially difficult, even after controlling for considerations like data augmentation and information volume. On the other hand, our findings also indicate that it is not fundamentally more difficult to finetune information about a real-world event than information about writing style.",
      "arxiv_url": "https://openreview.net/forum?id=OGgV9hpVGD",
      "pdf_url": "https://openreview.net/pdf/56758773cd0df02c9bd855e7446cb6eec6638607.pdf",
      "primary_category": "finetuning, language models, knowledge injection",
      "categories": [
        "finetuning",
        "language models",
        "knowledge injection",
        "task customization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "sJd4DpYOis",
      "title": "SpecEM: Training-Free LLM Ensembling via Iterative Drafting, Verification, and Online Feedback",
      "authors": [
        "Bo Lv",
        "Nayu Liu",
        "Chen Tang",
        "Xin Liu",
        "Yue Yu",
        "Ping Luo"
      ],
      "abstract": "Ensembles of generative large language models (LLMs) are a promising way to compensate for individual model limitations, integrating the strengths of different LLMs. Existing LLM ensemble methods, however, face limitations such as first-token delay and challenges in long-range semantic collaboration between models, Moreover, they typically assume equal voting weights for all models during ensemble, ignoring performance differences between models for a given task. In this work, we propose SpecEM, a training-free, plug-and-play LLM ensemble framework that dynamically adjusts each model's model contribution in real time based on task performance. Inspired by speculative decoding, SpecFuse iteratively performs drafting and verification, allowing models to collaborate semantically at the segment level for integrated output. Furthermore, we introduce an online feedback mechanism with multiplicative weight updates, where each model's voting weight is adjusted on-the-fly according to how often it \"outperforms\" others during verification stage, ensuring that stronger models exert greater influence on the ensemble during generation. Experimental results on five popular LLMs (ranging from 7B to 72B parameters) and six benchmark tasks, spanning instruction following, reasoning, commonsense, and general instruction response, demonstrate consistent performance improvements compared to state-of-the-art LLM ensemble methods.",
      "arxiv_url": "https://openreview.net/forum?id=sJd4DpYOis",
      "pdf_url": "https://openreview.net/pdf/b8be6120b3857bb40eae49a929d3366cc524dedb.pdf",
      "primary_category": "Ensembling language models.+Ensemble method.+Generative.+instruction response",
      "categories": [
        "Ensembling language models.+Ensemble method.+Generative.+instruction response"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nJq5z21eUk",
      "title": "Learning to Watermark: A Selective Watermarking Framework for Large Language Models via Multi-Objective Optimization",
      "authors": [
        "Chenrui Wang",
        "Junyi Shu",
        "Billy Chiu",
        "YU LI",
        "Saleh Alharbi",
        "Min Zhang",
        "Jing Li"
      ],
      "abstract": "The rapid development of LLMs has raised concerns about their potential misuse, leading to various watermarking schemes that typically offer high detectability. \nHowever, existing watermarking techniques often face trade-off between watermark detectability and generated text quality. \nIn this paper, we introduce Learning to Watermark (LTW), a novel selective watermarking framework that leverages multi-objective optimization to effectively balance these competing goals. \nLTW features a lightweight network that adaptively decides when to apply the watermark by analyzing sentence embeddings, token entropy, and current watermarking ratio. \nTraining of the network involves two specifically constructed loss functions that guide the model toward Pareto-optimal solutions, thereby harmonizing watermark detectability and text quality.\nBy integrating LTW with two baseline watermarking methods, our experimental evaluations demonstrate that LTW significantly enhances text quality without compromising detectability. \nOur selective watermarking approach offers a new perspective for designing watermarks for LLMs and a way to preserve high text quality for watermarks. The code is publicly available at: https://github.com/fattyray/learning-to-watermark",
      "arxiv_url": "https://openreview.net/forum?id=nJq5z21eUk",
      "pdf_url": "https://openreview.net/pdf/aae64ec6d2c0c7a1d022462f723893fd48f7c0ab.pdf",
      "primary_category": "LLM, watermark, Selective Watermarking",
      "categories": [
        "LLM",
        "watermark",
        "Selective Watermarking",
        "multi-objective optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "URB690A5r5",
      "title": "BlockScan: Detecting Anomalies in Blockchain Transactions",
      "authors": [
        "Jiahao Yu",
        "Xian Wu",
        "Hao Liu",
        "Wenbo Guo",
        "Xinyu Xing"
      ],
      "abstract": "We propose BlockScan, a customized Transformer for anomaly detection in blockchain transactions.\nUnlike existing methods that rely on rule-based systems or directly apply off-the-shelf large language models (LLMs), BlockScan introduces a series of customized designs to effectively model the unique data structure of blockchain transactions.\nFirst, a blockchain transaction is multi-modal, containing blockchain-specific tokens, texts, and numbers.\nWe design a novel modularized tokenizer to handle these multi-modal inputs, balancing the information across different modalities.\nSecond, we design a customized masked language modeling mechanism for pretraining the Transformer architecture, incorporating RoPE embedding and FlashAttention for handling longer sequences.\nFinally, we design a novel anomaly detection method based on the model outputs.\nWe further provide theoretical analysis for the detection method of our system. \nExtensive evaluations on Ethereum and Solana transactions demonstrate BlockScan's exceptional capability in anomaly detection while maintaining a low false positive rate.\nRemarkably, BlockScan is the only method that successfully detects anomalous transactions on Solana with high accuracy, whereas all other approaches achieved very low or zero detection recall scores.\nThis work sets a new benchmark for applying Transformer-based approaches in blockchain data analysis.",
      "arxiv_url": "https://openreview.net/forum?id=URB690A5r5",
      "pdf_url": "https://openreview.net/pdf/265cc120ce7fbcc4a716e76b15ed1cba0c477eb9.pdf",
      "primary_category": "foundation model, blockchain transaction",
      "categories": [
        "foundation model",
        "blockchain transaction"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "c6RDAutyNE",
      "title": "GPO: Learning from Critical Steps to Improve LLM Reasoning",
      "authors": [
        "Jiahao Yu",
        "Zelei Cheng",
        "Xian Wu",
        "Xinyu Xing"
      ],
      "abstract": "Large language models (LLMs) are increasingly used in various domains, showing impressive potential on various tasks. \n    Recently, reasoning LLMs have been proposed to improve the \\textit{reasoning} or \\textit{thinking} capabilities of LLMs to solve complex problems. \n    Despite the promising results of reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs still remains a significant challenge. \n    While existing optimization methods have advanced the LLM reasoning capabilities, they often treat reasoning trajectories as a whole, without considering the underlying critical steps within the trajectory. In this paper, we introduce \\textbf{G}uided \\textbf{P}ivotal \\textbf{O}ptimization (GPO), a novel fine-tuning strategy that dives into the reasoning process to enable more effective improvements. \n    GPO first identifies the `critical step' within a reasoning trajectory - a point that the model must carefully proceed so as to succeed at the problem. We locate the critical step by estimating the advantage function.\n    GPO then resets the policy to the critical step and samples the new rollout and prioritizes learning process on those rollouts. \n    This focus allows the model to learn more effectively from pivotal moments within the reasoning process to improve the reasoning performance.\n    We demonstrate that GPO is not a standalone method, but rather a general strategy that can be integrated with various optimization methods to improve reasoning performance. \n    Besides theoretical analysis, our experiments across challenging reasoning benchmarks show that GPO can consistently and significantly enhances the performance of existing optimization methods, showcasing its effectiveness and generalizability in improving LLM reasoning by concentrating on pivotal moments within the generation process.",
      "arxiv_url": "https://openreview.net/forum?id=c6RDAutyNE",
      "pdf_url": "https://openreview.net/pdf/13f0ee90e24b21be9c369212e9e13510ea825e84.pdf",
      "primary_category": "Large language model, explainable reinforcement learning, fine-tuning",
      "categories": [
        "Large language model",
        "explainable reinforcement learning",
        "fine-tuning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2a36EMSSTp",
      "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
      "authors": [
        "Qiying Yu",
        "Zheng Zhang",
        "Ruofei Zhu",
        "Yufeng Yuan",
        "Xiaochen Zuo",
        "YuYue",
        "Weinan Dai",
        "Tiantian Fan",
        "Gaohong Liu",
        "Juncai Liu",
        "LingJun Liu",
        "Xin Liu",
        "Haibin Lin",
        "Zhiqi Lin",
        "Bole Ma",
        "Guangming Sheng",
        "Yuxuan Tong",
        "Chi Zhang",
        "Mofan Zhang",
        "Ru Zhang",
        "Wang Zhang",
        "Hang Zhu",
        "Jinhua Zhu",
        "Jiaze Chen",
        "Jiangjie Chen",
        "Chengyi Wang",
        "Hongli Yu",
        "Yuxuan Song",
        "Xiangpeng Wei",
        "Hao Zhou",
        "Jingjing Liu",
        "Wei-Ying Ma",
        "Ya-Qin Zhang",
        "Lin Yan",
        "Yonghui Wu",
        "Mingxuan Wang"
      ],
      "abstract": "Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the **D**ecoupled Clip and **D**ynamic s**A**mpling **P**olicy **O**ptimization (**DAPO**) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.",
      "arxiv_url": "https://openreview.net/forum?id=2a36EMSSTp",
      "pdf_url": "https://openreview.net/pdf/a5ca4684c1debe30e4fde4bd063a262d61e13db7.pdf",
      "primary_category": "LLM, Reasoning, RL",
      "categories": [
        "LLM",
        "Reasoning",
        "RL"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "uAeqQePu4c",
      "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation",
      "authors": [
        "Jiajun Shi",
        "Jian Yang",
        "Jiaheng Liu",
        "Xingyuan Bu",
        "Jiangjie Chen",
        "Junting Zhou",
        "Kaijing Ma",
        "Zhoufutu Wen",
        "Bingli Wang",
        "Yancheng He",
        "Liang Song",
        "Hualei Zhu",
        "Shilong Li",
        "Xingjian Wang",
        "Wei Zhang",
        "Ruibin Yuan",
        "Yifan Yao",
        "Wenjun Yang",
        "Yunli Wang",
        "Siyuan Fang",
        "Siyu Yuan",
        "Qianyu He",
        "Xiangru Tang",
        "Yingshui Tan",
        "Wangchunshu Zhou",
        "Zhaoxiang Zhang",
        "Zhoujun Li",
        "Wenhao Huang",
        "Ge Zhang"
      ],
      "abstract": "Recent advancements in large language models (LLMs) underscore the need for more comprehensive evaluation methods to accurately assess their reasoning capabilities. Existing benchmarks are often domain-specific and thus cannot fully capture an LLM’s general reasoning potential. To address this limitation, we introduce the **Knowledge Orthogonal Reasoning Gymnasium (KORGym)**, a dynamic evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over fifty games in either textual or visual formats and supports interactive, multi-turn assessments with reinforcement learning scenarios. Using KORGym, we conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent reasoning patterns within model families and demonstrating the superior performance of closed-source models. Further analysis examines the effects of modality, reasoning strategies, reinforcement learning techniques, and response length on model performance. We expect KORGym to become a valuable resource for advancing LLM reasoning research and developing evaluation methodologies suited to complex, interactive environments.",
      "arxiv_url": "https://openreview.net/forum?id=uAeqQePu4c",
      "pdf_url": "https://openreview.net/pdf/f825a59867c5bb6282e851bc259033c0a30a35f6.pdf",
      "primary_category": "LLM; Evaluation; RL; Game",
      "categories": [
        "LLM; Evaluation; RL; Game"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2K9QsDaqkM",
      "title": "On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization",
      "authors": [
        "Wenlong Deng",
        "Yi Ren",
        "Muchen Li",
        "Danica J. Sutherland",
        "Xiaoxiao Li",
        "Christos Thrampoulidis"
      ],
      "abstract": "Reinforcement learning (RL) has become popular in enhancing the reasoning capabilities of large language models (LLMs), with Group Relative Policy Optimization (GRPO) emerging as a widely used algorithm in recent systems. Despite GRPO's widespread adoption, we identify a previously unrecognized phenomenon we term Lazy Likelihood Displacement (LLD), wherein the likelihood of correct responses marginally increases or even decreases during training. This behavior mirrors a recently discovered misalignment issue in Direct Preference Optimization (DPO), attributed to the influence of negative gradients. We provide a theoretical analysis of GRPO’s learning dynamic, identifying the source of LLD as the naive penalization of all tokens in incorrect responses with the same strength. To address this, we develop a method called NTHR, which downweights penalties on tokens contributing to the LLD. Unlike prior DPO-based approaches, NTHR takes advantage of GRPO’s group-based structure, using correct responses as anchors to identify influential tokens. Experiments on math reasoning benchmarks demonstrate that NTHR effectively mitigates LLD, yielding consistent performance gains across models ranging from 0.5B to 3B parameters.",
      "arxiv_url": "https://openreview.net/forum?id=2K9QsDaqkM",
      "pdf_url": "https://openreview.net/pdf/6d96a3e245851409d418f33a121d764014dd6829.pdf",
      "primary_category": "Large language model, Reasoning, Reinforcement Learning",
      "categories": [
        "Large language model",
        "Reasoning",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wj4lM45xQR",
      "title": "LayerNavigator: Finding Promising Intervention Layers for Efficient Activation Steering in Large Language Models",
      "authors": [
        "Hao Sun",
        "Huailiang Peng",
        "Qiong Dai",
        "Xu Bai",
        "Yanan Cao"
      ],
      "abstract": "Activation steering is an efficient technique for aligning the behavior of large language models (LLMs) by injecting steering vectors directly into a model’s residual stream during inference.\nA pivotal challenge in this approach lies in choosing the right layers to intervene, as inappropriate selection can undermine behavioral alignment and even impair the model’s language fluency and other core capabilities.\nWhile single-layer steering allows straightforward evaluation on held-out data to identify the \"best\" layer, it offers only limited alignment improvements.\nMulti-layer steering promises stronger control but faces a combinatorial explosion of possible layer subsets, making exhaustive search impractical.\nTo address these challenges, we propose LayerNavigator, which provides a principled and promising layer selection strategy.\nThe core innovation of LayerNavigator lies in its novel, quantifiable criterion that evaluates each layer's steerability by jointly considering two key aspects: discriminability and consistency.\nBy reusing the activations computed during steering vector generation, LayerNavigator requires no extra data and adds negligible overhead.\nComprehensive experiments show that LayerNavigator achieves not only superior alignment but also greater scalability and interpretability compared to existing strategies.\nOur code is available at https://github.com/Bryson-Arrot/LayerNavigator",
      "arxiv_url": "https://openreview.net/forum?id=wj4lM45xQR",
      "pdf_url": "https://openreview.net/pdf/04c255da8f39f376d75c5309a37d58d3378eb82a.pdf",
      "primary_category": "Large Language Model, Activation Steering, Behavior Alignment",
      "categories": [
        "Large Language Model",
        "Activation Steering",
        "Behavior Alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "rBlWKIUQey",
      "title": "Search and Refine During Think: Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning",
      "authors": [
        "Yaorui Shi",
        "Sihang Li",
        "Chang Wu",
        "Zhiyuan Liu",
        "Junfeng Fang",
        "Hengxing Cai",
        "An Zhang",
        "Xiang Wang"
      ],
      "abstract": "Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir.\nRetrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning.\nIn this paper, we propose **AutoRefine**, a reinforcement learning post-training framework that adopts a new \"search-and-refine-during-think\" paradigm.\nAutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer.\nFurthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios.\nDetailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively.",
      "arxiv_url": "https://openreview.net/forum?id=rBlWKIUQey",
      "pdf_url": "https://openreview.net/pdf/55cb62683b4084a45b97fd43ed7c483dbae3acc3.pdf",
      "primary_category": "Large Language Model, Reinforcement Learning, Retrieval-Augmented Reasoning",
      "categories": [
        "Large Language Model",
        "Reinforcement Learning",
        "Retrieval-Augmented Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "FAeU7516MR",
      "title": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE",
      "authors": [
        "Zongle Huang",
        "Lei Zhu",
        "ZongYuan Zhan",
        "Ting Hu",
        "Weikai Mao",
        "Xianzhi Yu",
        "Yongpan Liu",
        "Tianyu Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable success across many applications, with Mixture of Experts (MoE) models demonstrating great potential. Compared to traditional dense models, MoEs achieve better performance with less computation. Speculative decoding (SD) is a widely used technique to accelerate LLM inference without accuracy loss, but it has been considered efficient only for dense models. In this work, we first demonstrate that, under medium batch sizes, MoE surprisingly benefits more from SD than dense models. Furthermore, as MoE becomes sparser -- the prevailing trend in MoE designs -- the batch size range where SD acceleration is expected to be effective becomes broader. To quantitatively understand tradeoffs involved in SD, we develop a reliable modeling based on theoretical analyses. While current SD research primarily focuses on improving acceptance rates of algorithms, changes in workload and model architecture can still lead to degraded SD acceleration even with high acceptance rates. To address this limitation, we introduce a new metric 'target efficiency' that characterizes these effects, thus helping researchers identify system bottlenecks and understand SD acceleration more comprehensively. For scenarios like private serving, this work unveils a new perspective to speed up MoE inference, where existing solutions struggle. Experiments on different GPUs show up to 2.29x speedup for Qwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.",
      "arxiv_url": "https://openreview.net/forum?id=FAeU7516MR",
      "pdf_url": "https://openreview.net/pdf/97aad2e71863d0f5a6cda9ae68d3109542792be2.pdf",
      "primary_category": "Speculative decoding, MoE, Inference",
      "categories": [
        "Speculative decoding",
        "MoE",
        "Inference",
        "Acceleration"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vS1M06Px6u",
      "title": "MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement",
      "authors": [
        "Jaehyun Nam",
        "Jinsung Yoon",
        "Jiefeng Chen",
        "Jinwoo Shin",
        "Sercan O Arik",
        "Tomas Pfister"
      ],
      "abstract": "Agents based on large language models (LLMs) for machine learning engineering (MLE) can automatically implement ML models via code generation. However, existing approaches to build such agents often rely heavily on inherent LLM knowledge and employ coarse exploration strategies that modify the entire code structure at once. This limits their ability to select effective task-specific models and perform deep exploration within specific components, such as experimenting extensively with feature engineering options. To overcome these, we propose MLE-STAR, a novel approach to build MLE agents. MLE-STAR first leverages external knowledge by using a search engine to retrieve effective models from the web, forming an initial solution, then iteratively refines it by exploring various strategies targeting specific ML components. This exploration is guided by ablation studies analyzing the impact of individual code blocks. Furthermore, we introduce a novel ensembling method using an effective strategy suggested by MLE-STAR. Our experimental results show that MLE-STAR achieves medals in 64% of the Kaggle competitions on the MLE-bench, significantly outperforming the best alternative.",
      "arxiv_url": "https://openreview.net/forum?id=vS1M06Px6u",
      "pdf_url": "https://openreview.net/pdf/516c0c7cfd100775da54d33c97e84faffb80b05d.pdf",
      "primary_category": "Machine Learning Engineering Agents, LLM Agents",
      "categories": [
        "Machine Learning Engineering Agents",
        "LLM Agents"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "mGEPbyJ8OT",
      "title": "FEEDBACK FRICTION: LLMs Struggle to Fully Incorporate External Feedback",
      "authors": [
        "Dongwei Jiang",
        "Alvin Zhang",
        "Andrew Wang",
        "Nicholas Andrews",
        "Daniel Khashabi"
      ],
      "abstract": "Recent studies have shown LLMs possess some ability to improve their responses when given external feedback. However, it remains unclear how effectively and thoroughly these models can incorporate extrinsic feedback. In an ideal scenario, if LLMs receive near-perfect and complete feedback, we would expect them to fully integrate the feedback and reach correct solutions. In this paper, we systematically investigate LLMs’ ability to incorporate feedback by designing a controlled experimental environment. For each problem, a solver model attempts a solution, then a feedback generator with access to near-complete ground-truth answers produces targeted feedback, after which the solver tries again. We evaluate this pipeline across a diverse range of tasks, including math reasoning, knowledge reasoning, scientific reasoning, and general multi-domain evaluations with state-of-the-art language models including Claude 3.7 with extended thinking. Surprisingly, even under these near-ideal conditions, solver models consistently show resistance to feedback, a limitation that we term FEEDBACK FRICTION. To mitigate this limitation, we experiment with sampling-based strategies like progressive temperature increases and explicit rejection of previously attempted incorrect answers, which yield improvements but still fail to help models achieve target performance. We analyze FEEDBACK FRICTION and find that models’ confidence on specific questions, measured by semantic entropy, predicts feedback resistance: high-confidence predictions remain resistant to external correction. We hope that highlighting this issue in LLMs will help future research in self-improvement.",
      "arxiv_url": "https://openreview.net/forum?id=mGEPbyJ8OT",
      "pdf_url": "https://openreview.net/pdf/eccd2acb7a6781dcf3c02474daa623367b3329ce.pdf",
      "primary_category": "Language Model, Self-Improvement",
      "categories": [
        "Language Model",
        "Self-Improvement"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ygNaCTGUwJ",
      "title": "IGD: Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation",
      "authors": [
        "Zijie Lin",
        "Yang Zhang",
        "Xiaoyan Zhao",
        "Fengbin ZHU",
        "Fuli Feng",
        "Tat-Seng Chua"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong potential for recommendation by framing item prediction as a token-by-token language generation task. However, existing methods treat all item tokens equally, simply pursuing likelihood maximization during both optimization and decoding. This overlooks crucial token-level differences in decisiveness—many tokens contribute little to item discrimination yet can dominate optimization or decoding.\nTo quantify token decisiveness, we propose a novel perspective that models item generation as a decision process, measuring token decisiveness by the Information Gain (IG) each token provides in reducing uncertainty about the generated item. Our empirical analysis reveals that most tokens have low IG but often correspond to high logits, disproportionately influencing training loss and decoding, which may impair model performance.\nBuilding on these insights, we introduce an Information Gain-based Decisiveness-aware Token handling (IGD) strategy that integrates token decisiveness into both tuning and decoding. Specifically, IGD downweights low-IG tokens during tuning and rebalances decoding to emphasize tokens with high IG. In this way, IGD moves beyond pure likelihood maximization, effectively prioritizing high-decisiveness tokens. Extensive experiments on four benchmark datasets with two LLM backbones demonstrate that IGD consistently improves recommendation accuracy, achieving significant gains on widely used ranking metrics compared to strong baselines. Our codes are available at \\url{https://github.com/ZJLin2oo1/IGD}.",
      "arxiv_url": "https://openreview.net/forum?id=ygNaCTGUwJ",
      "pdf_url": "https://openreview.net/pdf/86952b79260abf511d4031d829d5944493792774.pdf",
      "primary_category": "Personalization, LLM-based Recommendation",
      "categories": [
        "Personalization",
        "LLM-based Recommendation"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "PHu9xJeAum",
      "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement",
      "authors": [
        "Xiyao Wang",
        "Zhengyuan Yang",
        "Chao Feng",
        "Hongjin Lu",
        "Linjie Li",
        "Chung-Ching Lin",
        "Kevin Lin",
        "Furong Huang",
        "Lijuan Wang"
      ],
      "abstract": "We introduce ThinkLite-VL, a family of visual reasoning models that achieve state-of-the-art (SoTA) performance using an order of magnitude fewer training samples, relying purely on reinforcement fine-tuning (RFT) self-improvement without any knowledge distillation. Our central insight is that sample difficulty critically influences RFT effectiveness: appropriately challenging examples can drive substantial reasoning improvements, even in low-data regimes.   However, quantifying sample difficulty in a reliable and scalable manner remains non-trivial. To address this, we repurpose Monte Carlo Tree Search (MCTS) to measure sample difficulty via the number of reasoning iterations a vision-language model (VLM) requires to solve each instance. This MCTS-based selection procedure identifies samples that induce deeper reasoning while remaining solvable, allowing us to filter a high-quality subset from 70k open-source examples spanning math, natural image understanding, and chart comprehension.  Using this approach, we select just 11k challenging samples for RFT on Qwen2.5-VL-7B-Instruct and 7.5k samples for Qwen2.5-VL-72B-Instruct. The resulting models, ThinkLite-VL-7B and ThinkLite-VL-72B, significantly outperform their respective base models across eight visual reasoning benchmarks.  In particular, ThinkLite-VL-7B improves the average performance of Qwen2.5-VL-7B-Instruct by 7\\% and surpasses all existing 7B-level models, as well as much larger models such as GPT-4o, O1 and Qwen2.5-VL-72B, achieving a new SoTA score of 75.1 on MathVista.  ThinkLite-VL-72B further advances the SoTA frontier, achieving an accuracy of 79.7 on MathVista and an average benchmark improvement of 4.42 over the open-source SOTA.  These results demonstrate that MCTS-guided difficulty filtering provides a scalable and effective path toward data-efficient self-improvement in multimodal reasoning.",
      "arxiv_url": "https://openreview.net/forum?id=PHu9xJeAum",
      "pdf_url": "https://openreview.net/pdf/758205547dc940c338296a99ef384666df7ca2ac.pdf",
      "primary_category": "vision language model; reinforcement finetuning; vlm reasoning; data selection",
      "categories": [
        "vision language model; reinforcement finetuning; vlm reasoning; data selection"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wsR7VYXbdR",
      "title": "HiMoLE: Towards OOD-Robust LoRA via Hierarchical Mixture of Experts",
      "authors": [
        "Yinuo Jiang",
        "Yan Xiaodong",
        "Keyan Ding",
        "Deng Zhao",
        "Lei Liang",
        "Qiang Zhang",
        "Huajun Chen"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, have enabled the efficient adaptation of large language models (LLMs) by updating only a small subset of parameters. However, their robustness under out-of-distribution (OOD) conditions remains insufficiently studied. In this paper, we identify the limitations of conventional LoRA in handling distributional shifts and propose $\\textbf{HiMoLE}$($\\textbf{Hi}$erarchical $\\textbf{M}$ixture of $\\textbf{L}$oRA $\\textbf{E}$xperts), a new framework designed to improve OOD generalization. HiMoLE integrates hierarchical expert modules and hierarchical routing strategies into the LoRA architecture and introduces a two-phase training procedure enhanced by a diversity-driven loss. This design mitigates negative transfer and promotes effective knowledge adaptation across diverse data distributions. We evaluate HiMoLE on three representative tasks in natural language processing. Experimental results evidence that HiMoLE consistently outperforms existing LoRA-based approaches, significantly reducing performance degradation on OOD data while improving in-distribution performance. Our work bridges the gap between parameter efficiency and distributional robustness, advancing the practical deployment of LLMs in real-world applications.",
      "arxiv_url": "https://openreview.net/forum?id=wsR7VYXbdR",
      "pdf_url": "https://openreview.net/pdf/1d0816b77f135ae47ceb4baa8b84e588d6527937.pdf",
      "primary_category": "OOD Robustness, Large Language Models, Parameter-Efficient Fine-Tuning",
      "categories": [
        "OOD Robustness",
        "Large Language Models",
        "Parameter-Efficient Fine-Tuning",
        "Mixture of Experts."
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "YJ7H3amL0k",
      "title": "The Promise of RL for Autoregressive Image Editing",
      "authors": [
        "Saba Ahmadi",
        "Rabiul Awal",
        "Ankur Sikarwar",
        "Amirhossein Kazemnejad",
        "Ge Ya Luo",
        "Juan A. Rodriguez",
        "Sai Rajeswar",
        "Siva Reddy",
        "Christopher Pal",
        "Benno Krojer",
        "Aishwarya Agrawal"
      ],
      "abstract": "While image generation techniques are now capable of producing high-quality images that respect prompts which span multiple sentences, the task of text-guided image editing remains a challenge. Even edit requests that consist of only a few words often fail to be executed correctly. We explore three strategies to enhance performance on a wide range of image editing tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and Chain-of-Thought (CoT) reasoning. In order to study all these components in one consistent framework, we adopt an autoregressive multimodal model that processes textual and visual tokens in a unified manner.\nWe find RL combined with a large multi-modal LLM verifier to be the most effective of these strategies.\nAs a result, we release **EARL**: **E**diting with **A**utoregression and **RL**, a strong RL-based image editing model that performs competitively on a diverse range of edits compared to strong baselines, despite using much less training data. Thus, EARL pushes the frontier of autoregressive multimodal models on image editing. We release our code, training data, and trained models at [https://github.com/mair-lab/EARL](https://github.com/mair-lab/EARL).",
      "arxiv_url": "https://openreview.net/forum?id=YJ7H3amL0k",
      "pdf_url": "https://openreview.net/pdf/8b598265103cb81feca926ffeb6c0f85a739a3be.pdf",
      "primary_category": "Image editing, Autoregressive, MLLM",
      "categories": [
        "Image editing",
        "Autoregressive",
        "MLLM",
        "GRPO",
        "RL",
        "Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xvxgG668th",
      "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient Variable-Length VLMs",
      "authors": [
        "Zhenhailong Wang",
        "Senthil Purushwalkam",
        "Caiming Xiong",
        "Silvio Savarese",
        "Heng Ji",
        "Ran Xu"
      ],
      "abstract": "We present DyMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embeddings by merging similar tokens based on image complexity, addressing the inherent inefficiency of fixed-length outputs in vision transformers. Second, Virtual Token Unmerging (VTU) simulates the expected token sequence for large language models (LLMs) by efficiently reconstructing the attention dynamics of a full sequence, thus preserving the downstream performance without additional fine-tuning. \nUnlike previous approaches, our method dynamically determines token length based on the *image content*—not just resolution—and operates completely training-free, making it readily applicable to most state-of-the-art VLM architectures. Extensive experiments on image and video understanding tasks, demonstrate that DyMU can reduce the average visual token count by 32%-85% while achieving comparable performance to full-length models, across diverse VLM architectures.  Furthermore, qualitative analyses show that the adaptive token reduction from DToMe aligns well with human perception and enables users to better control computational costs through flexible integration with additional vision tools and models.",
      "arxiv_url": "https://openreview.net/forum?id=xvxgG668th",
      "pdf_url": "https://openreview.net/pdf/7bb52b265e0feea0c6ca608921da2db2db8019c8.pdf",
      "primary_category": "Dynamic Length Visual Encoding, Vision-Language Models, Token Merging",
      "categories": [
        "Dynamic Length Visual Encoding",
        "Vision-Language Models",
        "Token Merging"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RpE4HeuX69",
      "title": "Practical and Effective Code Watermarking for Large Language Models",
      "authors": [
        "Zhimeng Guo",
        "Minhao Cheng"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) in code generation has raised significant attribution and intellectual property concerns. Code watermarking offers a potential solution but faces unique challenges due to programming languages' strict syntactic constraints and semantic requirements.\nTo address these challenges, we introduce ACW (AST-guided Code Watermarking), a novel adaptive framework that leverages Abstract Syntax Tree (AST) analysis during training to learn watermark embedding strategies. Our framework identifies substitutable code components and strategically biases token selections to embed watermarks. We also propose a novel sampling scheme that distributes tokens between green/red lists according to semantic context, ensuring statistical distinguishability while preserving code functionality. Extensive experiments demonstrate that ACW achieves a significant improvement in watermark detection accuracy compared to existing methods, with negligible impact on code functionality. This adaptive framework offers a promising solution for effective and practical code watermarking in the age of LLMs. Our code is available at: https://github.com/TimeLovercc/code-watermark.",
      "arxiv_url": "https://openreview.net/forum?id=RpE4HeuX69",
      "pdf_url": "https://openreview.net/pdf/e28031c958fa8b0115bf14d0fcd0a2c33c8d8826.pdf",
      "primary_category": "Large Language Model, LLM Watermarking, Code Watermarking",
      "categories": [
        "Large Language Model",
        "LLM Watermarking",
        "Code Watermarking"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "KWRKCXRwNg",
      "title": "Ground-Compose-Reinforce: Grounding Language in Agentic Behaviours using Limited Data",
      "authors": [
        "Andrew C Li",
        "Toryn Q. Klassen",
        "Andrew Wang",
        "Parand A. Alamdari",
        "Sheila A. McIlraith"
      ],
      "abstract": "Grounding language in perception and action is a key challenge when building situated agents that can interact with humans, or other agents, via language. In the past, addressing this challenge has required manually designing the language grounding or curating massive datasets that associate language with the environment. We propose Ground-Compose-Reinforce, an end-to-end, neurosymbolic framework for training RL agents directly from high-level task specifications—without manually designed reward functions or other domain-specific oracles, and without massive datasets. These task specifications take the form of Reward Machines, automata-based representations that capture high-level task structure and are in some cases autoformalizable from natural language. Critically, we show that Reward Machines can be grounded using limited data by exploiting compositionality. Experiments in a custom Meta-World domain with only 350 labelled pretraining trajectories show that our framework faithfully elicits complex behaviours from high-level specifications—including behaviours that never appear in pretraining—while non-compositional approaches fail.",
      "arxiv_url": "https://openreview.net/forum?id=KWRKCXRwNg",
      "pdf_url": "https://openreview.net/pdf/e33748604d90eec9596d5c866b3628aa72b66112.pdf",
      "primary_category": "Reinforcement Learning, RL, Formal Language",
      "categories": [
        "Reinforcement Learning",
        "RL",
        "Formal Language",
        "Reward Machine",
        "LTL",
        "Language",
        "Grounding",
        "Compositional"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "WdL3O58gde",
      "title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks",
      "authors": [
        "Vishnu Sarukkai",
        "Zhiqiang Xie",
        "Kayvon Fatahalian"
      ],
      "abstract": "Improving Large Language Model (LLM) agents for sequential decision-making tasks typically requires extensive task-specific knowledge engineering—custom prompts, curated examples, and specialized observation/action spaces. We investigate a different approach where agents automatically improve by learning from their own successful experiences without human intervention. Our method constructs and refines a database of self-generated trajectories that serve as in-context examples for future tasks. Even naive accumulation of successful trajectories yields substantial performance gains across three diverse benchmarks: ALFWorld (73\\% to 89\\%), Wordcraft (55\\% to 64\\%), and InterCode-SQL (75\\% to 79\\%). These improvements exceed those achieved by upgrading from gpt-4o-mini to gpt-4o and match the performance of allowing multiple attempts per task. We further enhance this approach with two innovations: database-level curation using population-based training to propagate high-performing example collections, and exemplar-level curation that selectively retains trajectories based on their empirical utility as in-context examples. With these enhancements, our method achieves 93\\% success on ALFWorld—surpassing approaches that use more powerful LLMs and hand-crafted components. Our trajectory bootstrapping technique demonstrates that agents can autonomously improve through experience, offering a scalable alternative to labor-intensive knowledge engineering.",
      "arxiv_url": "https://openreview.net/forum?id=WdL3O58gde",
      "pdf_url": "https://openreview.net/pdf/05d0804df1c396da814a29970ae7d37c40a1b84e.pdf",
      "primary_category": "language models, agents, self-improvement",
      "categories": [
        "language models",
        "agents",
        "self-improvement",
        "in-context learning",
        "sequential decision-making"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "5l8GydIsby",
      "title": "RESAnything: Attribute Prompting for Arbitrary Referring Segmentation",
      "authors": [
        "Ruiqi Wang",
        "Hao Zhang"
      ],
      "abstract": "We present an open-vocabulary and zero-shot method for arbitrary referring expression segmentation (RES), targeting input expressions that are more general than what prior works were designed to handle. Specifically, our inputs encompass both object- and part-level labels as well as implicit references pointing to properties or qualities of object/part function, design, style, material, etc. Our model, coined RESAnything, leverages Chain-of-Thoughts (CoT) reasoning, where the key idea is attribute prompting. We generate detailed descriptions of object/part attributes including shape, color, and location for potential segment proposals through systematic prompting of a large language model (LLM), where the proposals are produced by a foundational image segmentation model. Our approach encourages deep reasoning about object or part attributes related to function, style, design, etc., enabling the system to handle implicit queries without any part annotations for training or fine-tuning. As the first zero-shot and LLM-based RES method, RESAnything achieves clearly superior performance among zero-shot methods on traditional RES benchmarks and significantly outperforms existing methods on challenging scenarios involving implicit queries and complex part-level relations. Finally, we contribute a new benchmark dataset to offer ~3K carefully curated RES instances to assess part-level, arbitrary RES solutions.",
      "arxiv_url": "https://openreview.net/forum?id=5l8GydIsby",
      "pdf_url": "https://openreview.net/pdf/3ec88c013e57b56be455d70c321c6a870176ef99.pdf",
      "primary_category": "Referring Expression Segmentation, Prompting",
      "categories": [
        "Referring Expression Segmentation",
        "Prompting"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Hkvyosg7Yx",
      "title": "Permissioned LLMs: Enforcing Access Control in Large Language Models",
      "authors": [
        "Bargav Jayaraman",
        "Virendra Marathe",
        "Hamid Mozaffari",
        "William F. Shen",
        "Krishnaram Kenthapadi"
      ],
      "abstract": "In enterprise settings, organizational data is segregated, siloed and carefully protected by elaborate access control frameworks. These access control structures can completely break down if an LLM fine-tuned on the siloed data serves requests, for downstream tasks, from individuals with disparate access privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs that superimpose the organizational data access control structures on query responses they generate. We formalize abstractions underpinning the means to determine whether access control enforcement happens correctly over LLM query responses. Our formalism introduces the notion of a relevant response that can be used to prove whether a PermLLM mechanism has been implemented correctly. We also introduce a novel\nmetric, called access advantage, to empirically evaluate the efficacy of a PermLLM mechanism. We introduce three novel PermLLM mechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired access control. We furthermore present two instantiations of access advantage–(i) Domain Distinguishability Index (DDI) based on Membership Inference Attacks, and (ii) Utility Gap Index (UGI)\nbased on LLM utility evaluation. We demonstrate the efficacy of our PermLLM mechanisms through extensive experiments on five public datasets (GPQA, RCV1, SimpleQA, WMDP, and PubMedQA), in addition to evaluating the validity of DDI and UGI metrics themselves for quantifying access control in LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=Hkvyosg7Yx",
      "pdf_url": "https://openreview.net/pdf/fc22aa4d0c1c280b5a3e4299cf03105685c0dee9.pdf",
      "primary_category": "LLM, PEFT, access control",
      "categories": [
        "LLM",
        "PEFT",
        "access control"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "SptbUlfhJg",
      "title": "StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations",
      "authors": [
        "Yanjie Li",
        "Wenxuan Zhang",
        "Xinqi LYU",
        "Yihao LIU",
        "Bin Xiao"
      ],
      "abstract": "Recently, text-to-image diffusion models have been widely used for style mimicry and personalized customization through methods such as DreamBooth and Textual Inversion. This has raised concerns about intellectual property protection and the generation of deceptive content.\nRecent studies, such as Glaze and Anti-DreamBooth, have proposed using adversarial noise to protect images from these attacks. However, recent purification-based methods, such as DiffPure and Noise Upscaling, have successfully attacked these latest defenses, showing the vulnerabilities of these methods. \nMoreover, present methods show limited transferability across models, making them less effective against unknown text-to-image models.\nTo address these issues, we propose a novel anti-mimicry method, StyleGuard. We propose a novel style loss that optimizes the style-related features in the latent space to make it deviate from the original image, which improves model-agnostic transferability.\nAdditionally, to enhance the perturbation's ability to bypass diffusion-based purification, we designed a novel upscale loss that involves ensemble purifiers and upscalers during training.\nExtensive experiments on the WikiArt and CelebA datasets demonstrate that StyleGuard outperforms existing methods in robustness against various transformations and purifications, effectively countering style mimicry in various models. Moreover, StyleGuard is effective on different style mimicry methods, including DreamBooth and Textual Inversion. The code is available at \\url{https://github.com/PolyLiYJ/StyleGuard}.",
      "arxiv_url": "https://openreview.net/forum?id=SptbUlfhJg",
      "pdf_url": "https://openreview.net/pdf/8b475ef742f4bdd818afc07210c1b96736a7f024.pdf",
      "primary_category": "adversarial attack, style mimicry attack, Dreambooth",
      "categories": [
        "adversarial attack",
        "style mimicry attack",
        "Dreambooth",
        "diffusion model"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "n6SrVj7I0g",
      "title": "MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition",
      "authors": [
        "Umberto Cappellazzo",
        "Minsu Kim",
        "Pingchuan Ma",
        "Honglie Chen",
        "Xubo Liu",
        "Stavros Petridis",
        "Maja Pantic"
      ],
      "abstract": "Large language models (LLMs) have recently shown strong potential in audio-visual speech recognition (AVSR), but their high computational demands and sensitivity to token granularity limit their practicality in resource-constrained settings. Token compression methods can reduce inference cost, but they require fixing a compression rate in advance and produce a single fixed-length output, offering no flexibility to balance information density and efficiency at inference time. Matryoshka representation learning (MRL) addresses this by enabling a single model to operate across multiple token granularities, allowing compression rates to be adjusted dynamically. However, current MRL-based methods treat each scale independently during training, limiting cross-scale generalization, robustness at high compression, and interpretability. To overcome these limitations, we propose MoME (Mixture of Matryoshka Experts), a novel framework that integrates sparse Mixture-of-Experts (MoE) into MRL-based LLMs for AVSR. MoME augments a frozen LLM with top-k routed and shared experts, allowing dynamic capacity allocation across scales and modalities. A shared router promotes consistent expert activation across granularities, enabling compressed sequences to benefit from representations learned at lower compression. Experiments on LRS2 and LRS3 demonstrate that MoME achieves state-of-the-art performance across AVSR, ASR, and VSR tasks, while requiring significantly fewer parameters and maintaining robustness under noise. MoME unifies the adaptability of MRL with the efficiency of MoE, offering a scalable and interpretable solution for resource-aware speech recognition.",
      "arxiv_url": "https://openreview.net/forum?id=n6SrVj7I0g",
      "pdf_url": "https://openreview.net/pdf/0674446cde13359788a302a98c1b84aa415840e0.pdf",
      "primary_category": "Mixture of Experts, Matryoshka Representation Learning, Audio-Visual Speech Recognition",
      "categories": [
        "Mixture of Experts",
        "Matryoshka Representation Learning",
        "Audio-Visual Speech Recognition"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ibLGUkBWlz",
      "title": "Preference Learning with Lie Detectors can Induce Honesty or Evasion",
      "authors": [
        "Chris Cundy",
        "Adam Gleave"
      ],
      "abstract": "As AI systems become more capable, deceptive behaviors can undermine evaluation and mislead users at deployment.\nRecent work has shown that lie detectors can accurately classify deceptive behavior, but they are not typically used in the training pipeline due to concerns around contamination and objective hacking.\nWe examine these concerns by incorporating a lie detector into the labelling step of LLM post-training and evaluating whether the learned policy is genuinely more honest, or instead learns to fool the lie detector while remaining deceptive.\nUsing DolusChat, a novel 65k-example dataset with paired truthful/deceptive responses, we identify three key factors that determine the honesty of learned policies: amount of exploration during preference learning, lie detector accuracy, and KL regularization strength. \nWe find that preference learning with lie detectors and GRPO can lead to policies which evade lie detectors, with deception rates of over 85\\%. \nHowever, if the lie detector true positive rate (TPR) or KL regularization is sufficiently high, GRPO learns honest policies.\nIn contrast, off-policy algorithms (DPO) consistently lead to deception rates under 25\\% for realistic TPRs.\nOur results illustrate a more complex picture than previously assumed: depending on the context, lie-detector-enhanced training can be a powerful tool for scalable oversight, or a counterproductive method encouraging undetectable misalignment.",
      "arxiv_url": "https://openreview.net/forum?id=ibLGUkBWlz",
      "pdf_url": "https://openreview.net/pdf/9fb3a9acd1880de1852581a7b427604fe6bb570d.pdf",
      "primary_category": "RLHF, deception, preference learning",
      "categories": [
        "RLHF",
        "deception",
        "preference learning",
        "safety",
        "scalable oversight",
        "monitoring"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Gsi42ohBoM",
      "title": "Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach",
      "authors": [
        "Yuchen Wu",
        "Edward Sun",
        "Kaijie Zhu",
        "Jianxun Lian",
        "Jose Hernandez-Orallo",
        "Aylin Caliskan",
        "Jindong Wang"
      ],
      "abstract": "Large language models (LLMs) typically generate identical or similar responses for all users given the same prompt, posing serious safety risks in high-stakes applications where user vulnerabilities differ widely.\nExisting safety evaluations primarily rely on context-independent metrics—such as factuality, bias, or toxicity—overlooking the fact that the same response may carry divergent risks depending on the user's background or condition.\nWe introduce ``personalized safety'' to fill this gap and present PENGUIN—a benchmark comprising 14,000 scenarios across seven sensitive domains with both context-rich and context-free variants. Evaluating six leading LLMs, we demonstrate that personalized user information significantly improves safety scores by 43.2%, confirming the effectiveness of personalization in safety alignment. However, not all context attributes contribute equally to safety enhancement. To address this, we develop RAISE—a training-free, two-stage agent framework that strategically acquires user-specific background. RAISE improves safety scores by up to 31.6% over six vanilla LLMs, while maintaining a low interaction cost of just 2.7 user queries on average. Our findings highlight the importance of selective information gathering in safety-critical domains and offer a practical solution for personalizing LLM responses without model retraining. This work establishes a foundation for safety research that adapts to individual user contexts rather than assuming a universal harm standard.",
      "arxiv_url": "https://openreview.net/forum?id=Gsi42ohBoM",
      "pdf_url": "https://openreview.net/pdf/0304913384dfe7a58dfbb08bd51ce4530d265507.pdf",
      "primary_category": "LLM Safety, Evaluation, Personalized Safety",
      "categories": [
        "LLM Safety",
        "Evaluation",
        "Personalized Safety",
        "Structured User Context",
        "Monte Carlo Tree Search (MCTS)"
      ],
      "tags": [
        "LLM",
        "Personalization",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ir8u0crTcA",
      "title": "UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation",
      "authors": [
        "Rui Tian",
        "Mingfei Gao",
        "Mingze Xu",
        "Jiaming Hu",
        "Jiasen Lu",
        "Zuxuan Wu",
        "Yinfei Yang",
        "Afshin Dehghan"
      ],
      "abstract": "We introduce UniGen, a unified multimodal large language model (MLLM) capable of image understanding and generation. We study the full training pipeline of UniGen from a data-centric perspective, including multi-stage pre-training, supervised fine-tuning, and direct preference optimization. More importantly, we propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time scaling, which significantly boosts UniGen’s image generation quality using a simple Best-of-N test-time strategy. Specifically, CoT-V enables UniGen to act as both image generator and verifier at test time, assessing the semantic alignment between a text prompt and its generated image in a step-by-step CoT manner. Trained entirely on open-source datasets across all stages, UniGen achieves state-of-the-art performance on a range of image understanding and generation benchmarks, with a final score of 0.78 on GenEval and 85.19 on DPG-Bench. Through extensive ablation studies, our work provides actionable insights and addresses key challenges in the full life cycle of building unified MLLMs, contributing meaningful directions to future research. Code is available at https://github.com/apple/ml-unigen.",
      "arxiv_url": "https://openreview.net/forum?id=ir8u0crTcA",
      "pdf_url": "https://openreview.net/pdf/aaf1dfc4cc34853277d406dd6964a87ffe69276f.pdf",
      "primary_category": "Unified Multimodal Understanding and Generation, Multimodal Foundation Model, Multimodal LLM",
      "categories": [
        "Unified Multimodal Understanding and Generation",
        "Multimodal Foundation Model",
        "Multimodal LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MaJ3ASZ0NI",
      "title": "Lost in Transmission: When and Why LLMs Fail to Reason Globally",
      "authors": [
        "Tobias Schnabel",
        "Kiran Tomlinson",
        "Adith Swaminathan",
        "Jennifer Neville"
      ],
      "abstract": "Despite their many successes, transformer-based large language models (LLMs) continue to struggle with tasks that require complex reasoning over large parts of their input. We argue that these failures arise due to capacity limits on the accurate flow of information within LLMs. To formalize this issue, we introduce the bounded attention prefix oracle (BAPO) model, a new computational framework that models bandwidth constraints on attention heads, the mechanism for internal communication in LLMs. We show that several important reasoning problems like graph reachability require high communication bandwidth for BAPOs to solve; we call these problems BAPO-hard. Our experiments corroborate our theoretical predictions: GPT-4o, Claude, and Gemini succeed on BAPO-easy tasks and fail even on relatively small BAPO-hard tasks. BAPOs also reveal another benefit of chain of thought (CoT): we prove that breaking down a task using CoT can turn any BAPO-hard problem into a BAPO-easy one. Our results offer principled explanations for key LLM failures and suggest directions for architectures and inference methods that mitigate bandwidth limits.",
      "arxiv_url": "https://openreview.net/forum?id=MaJ3ASZ0NI",
      "pdf_url": "https://openreview.net/pdf/abcd28fc182b4d4dcc23369f3bc095c3f56ed1e3.pdf",
      "primary_category": "LLM limitations, chain of thought, reasoning",
      "categories": [
        "LLM limitations",
        "chain of thought",
        "reasoning",
        "communication complexity"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "aXO0xg0ttW",
      "title": "Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models",
      "authors": [
        "Jiajun Fan",
        "Tong Wei",
        "Chaoran Cheng",
        "Yuxin Chen",
        "Ge Liu"
      ],
      "abstract": "Balancing exploration and exploitation during reinforcement learning fine-tuning of generative models presents a critical challenge, as existing approaches rely on fixed divergence regularization that creates an inherent dilemma: strong regularization preserves model capabilities but limits reward optimization, while weak regularization enables greater alignment but risks instability or reward hacking. We introduce Adaptive Divergence Regularized Policy Optimization (ADRPO), which automatically adjusts regularization strength based on advantage estimates—reducing regularization for high-value samples while applying stronger regularization to poor samples, enabling policies to navigate between exploration and aggressive exploitation according to data quality. Our implementation with Wasserstein-2 regularization for flow matching generative models achieves remarkable results on text-to-image generation, achieving better semantic alignment and diversity than offline methods like DPO and online methods with fixed regularization like ORW-CFM-W2. ADRPO enables a 2B parameter SD3 model to surpass much larger models with 4.8B and 12B parameters in attribute binding, semantic consistency, artistic style transfer, and compositional control while maintaining generation diversity. ADRPO generalizes to KL-regularized fine-tuning of both text-only LLMs and multi-modal reasoning models, enhancing existing online RL methods like GRPO while requiring no additional networks or complex architectural changes. In LLM fine-tuning, ADRPO demonstrates an emergent ability to escape local optima through active exploration, while in multi-modal audio reasoning, it outperforms GRPO through superior step-by-step reasoning, enabling a 7B model to outperform substantially larger commercial models including Gemini 2.5 Pro and GPT-4o Audio, offering an effective plug-and-play solution to the exploration-exploitation challenge across diverse generative architectures and modalities.",
      "arxiv_url": "https://openreview.net/forum?id=aXO0xg0ttW",
      "pdf_url": "https://openreview.net/pdf/5a283782c1d2629f4e29a26e7e862287ccd01991.pdf",
      "primary_category": "Reinforcement learning, generative models, adaptive regularization",
      "categories": [
        "Reinforcement learning",
        "generative models",
        "adaptive regularization",
        "advantage estimation",
        "fine-tuning",
        "flow matching",
        "text-to-image generation",
        "exploration-exploitation trade-off"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MqGZIJxZ1z",
      "title": "Variational Uncertainty Decomposition for In-Context Learning",
      "authors": [
        "I. Shavindra Jayasekera",
        "Jacob Si",
        "Filippo Valdettaro",
        "Wenlong Chen",
        "Aldo A. Faisal",
        "Yingzhen Li"
      ],
      "abstract": "As large language models (LLMs) gain popularity in conducting prediction tasks in-context, understanding the sources of uncertainty in in-context learning becomes essential to ensuring reliability. The recent hypothesis of in-context learning performing predictive Bayesian inference opens the avenue for Bayesian uncertainty estimation, particularly for decomposing uncertainty into epistemic uncertainty due to lack of in-context data and aleatoric uncertainty inherent in the in-context prediction task. However, the decomposition idea remains under-explored due to the intractability of the latent parameter posterior from the underlying Bayesian model. In this work, we introduce a variational uncertainty decomposition framework for in-context learning without explicitly sampling from the latent parameter posterior, by optimising auxiliary inputs as probes to obtain an upper bound to the aleatoric uncertainty of an LLM's in-context learning procedure. Through experiments on synthetic and real-world tasks, we show quantitatively and qualitatively that the decomposed uncertainties obtained from our method exhibit desirable properties of epistemic and aleatoric uncertainty.",
      "arxiv_url": "https://openreview.net/forum?id=MqGZIJxZ1z",
      "pdf_url": "https://openreview.net/pdf/e044da5609e35fc504bda2b43e5b9fc504a960ab.pdf",
      "primary_category": "Uncertainty Quantification, Uncertainty Decomposition, In-Context Learning",
      "categories": [
        "Uncertainty Quantification",
        "Uncertainty Decomposition",
        "In-Context Learning",
        "Variational Methods",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "v6Oo0zO2oA",
      "title": "LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation",
      "authors": [
        "Yang Miao",
        "Jan-Nico Zaech",
        "Xi Wang",
        "Fabien Despinoy",
        "Danda Pani Paudel",
        "Luc Van Gool"
      ],
      "abstract": "We propose LangHOPS, the first Multimodal Large Language Model (MLLM)-based framework for open-vocabulary object–part instance segmentation. Given an image, LangHOPS can jointly detect and segment hierarchical object and part instances from open-vocabulary candidate categories. Unlike prior approaches that rely on heuristic or learnable visual grouping, our approach grounds object–part hierarchies in language space. It integrates the MLLM into the object-part parsing pipeline to leverage rich knowledge and reasoning capabilities, and link multi-granularity concepts within the hierarchies. We evaluate LangHOPS across multiple challenging scenarios, including in-domain and cross-dataset object-part instance segmentation, and zero-shot semantic segmentation. LangHOPS achieves state-of-the-art results, surpassing previous methods by 5.5% Average Precision(AP) (in-domain) and 4.8% (cross-dataset) on the PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K (zero-shot). Ablation studies further validate the effectiveness of the language-grounded hierarchy and MLLM-driven part query refinement strategy.",
      "arxiv_url": "https://openreview.net/forum?id=v6Oo0zO2oA",
      "pdf_url": "https://openreview.net/pdf/c1e63925ea92fcf485d0dc1f0e9a3534306cee1c.pdf",
      "primary_category": "Object Part Segmentation; Language Grounding; Multi-modality; Multimodal Large Language Model",
      "categories": [
        "Object Part Segmentation; Language Grounding; Multi-modality; Multimodal Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0mOBdNsI3L",
      "title": "Approximately Aligned Decoding",
      "authors": [
        "Daniel Melcer",
        "Sujan Kumar Gonugondla",
        "Pramuditha Perera",
        "Haifeng Qian",
        "Wen-Hao Chiang",
        "Yanjun Wang",
        "Nihal Jain",
        "Pranav Garg",
        "Xiaofei Ma",
        "Anoop Deoras"
      ],
      "abstract": "It is common to reject undesired outputs of Large Language Models (LLMs); however, current methods to do so require an excessive amount of computation to re-sample after a rejection, or distort the distribution of outputs by constraining the output to highly improbable tokens.\nWe present a method, Approximately Aligned Decoding (AprAD), to balance the distortion of the output distribution with computational efficiency, inspired by algorithms from the speculative decoding literature.\nAprAD allows for the generation of long sequences of text with difficult-to-satisfy constraints, while amplifying low probability outputs much less compared to existing methods.\nWe show through a series of experiments that the task-specific performance of AprAD is comparable to methods that do not distort the output distribution, while being much more computationally efficient.",
      "arxiv_url": "https://openreview.net/forum?id=0mOBdNsI3L",
      "pdf_url": "https://openreview.net/pdf/fed4641ccc8eb690a2e8911e4b450fc13007947f.pdf",
      "primary_category": "Constrained Decoding, Large Language Models",
      "categories": [
        "Constrained Decoding",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0ZnXGzLcOg",
      "title": "Privacy Reasoning in Ambiguous Contexts",
      "authors": [
        "Ren Yi",
        "Octavian Suciu",
        "Adrian Gascon",
        "Sarah Meiklejohn",
        "Eugene Bagdasarian",
        "Marco Gruteser"
      ],
      "abstract": "We study the ability of language models to reason about appropriate information disclosure - a central aspect of the evolving field of agentic privacy. Whereas previous works have focused on evaluating a model's ability to align with human decisions, we examine the role of ambiguity and missing context on model performance when making information-sharing decisions. We identify context ambiguity as a crucial barrier for high performance in privacy assessments. By designing Camber, a framework for context disambiguation, we show that model-generated decision rationales can reveal ambiguities and that systematically disambiguating context based on these rationales leads to significant accuracy improvements (up to 13.3% in precision and up to 22.3% in recall) as well as reductions in prompt sensitivity. Overall, our results indicate that approaches for context disambiguation are a promising way forward to enhance agentic privacy reasoning.",
      "arxiv_url": "https://openreview.net/forum?id=0ZnXGzLcOg",
      "pdf_url": "https://openreview.net/pdf/9d9d30a456ab8f177899648b9431299142c6f87d.pdf",
      "primary_category": "Personal Agents, Privacy Reasoning, Context Disambiguation",
      "categories": [
        "Personal Agents",
        "Privacy Reasoning",
        "Context Disambiguation",
        "Benchmarks"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "bTssV4Cnjn",
      "title": "Incremental Sequence Classification with Temporal Consistency",
      "authors": [
        "Lucas Maystre",
        "Gabriel Barello",
        "Tudor Berariu",
        "Aleix Cambray",
        "Rares Dolga",
        "Alvaro Ortega Gonzalez",
        "Andrei Cristian Nica",
        "David Barber"
      ],
      "abstract": "We address the problem of incremental sequence classification, where predictions are updated as new elements in the sequence are revealed. Drawing on temporal-difference learning from reinforcement learning, we identify a temporal-consistency condition that successive predictions should satisfy. We leverage this condition to develop a novel loss function for training incremental sequence classifiers. Through a concrete example, we demonstrate that optimizing this loss can offer substantial gains in data efficiency. We apply our method to text classification tasks and show that it improves predictive accuracy over competing approaches on several benchmark datasets. We further evaluate our approach on the task of verifying large language model generations for correctness in grade-school math problems. Our results show that models trained with our method are better able to distinguish promising generations from unpromising ones after observing only a few tokens.",
      "arxiv_url": "https://openreview.net/forum?id=bTssV4Cnjn",
      "pdf_url": "https://openreview.net/pdf/e440490e43c94488ad8fc785f89db2e23e53de25.pdf",
      "primary_category": "sequence classification, temporal-difference learning",
      "categories": [
        "sequence classification",
        "temporal-difference learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "PLBVtJt4td",
      "title": "Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models",
      "authors": [
        "Jiachen Jiang",
        "Jinxin Zhou",
        "Bo Peng",
        "Xia Ning",
        "Zhihui Zhu"
      ],
      "abstract": "Achieving better alignment between vision embeddings and Large Language Models (LLMs) is crucial for enhancing the abilities of Multimodal LLMs (MLLMs), particularly for recent models that rely on powerful pretrained vision encoders and LLMs. A common approach to connect the pretrained vision encoder and LLM is through a projector applied after the vision encoder. However, the projector is often trained to enable the LLM to generate captions, and hence the mechanism by which LLMs understand each vision token remains unclear. In this work, we first investigate the role of the projector in compressing vision embeddings and aligning them with word embeddings. We show that the projector significantly compresses visual information, removing redundant details while preserving essential elements necessary for the LLM to understand visual content. We then examine patch-level alignment---the alignment between each vision patch and its corresponding semantic words---and propose a $\\textit{multi-semantic alignment hypothesis}$. Our analysis indicates that the projector trained by caption loss improves patch-level alignment but only to a limited extent, resulting in weak and coarse alignment. To address this issue, we propose $\\textit{patch-aligned training}$ to efficiently enhance patch-level alignment. Our experiments show that patch-aligned training (1) achieves stronger compression capability and improved patch-level alignment, enabling the MLLM to generate higher-quality captions, (2) improves the MLLM's performance by 16% on referring expression grounding tasks, 4% on question-answering tasks, and 3% on modern instruction-following benchmarks when using the same supervised fine-tuning (SFT) setting. The proposed method can be easily extended to other multimodal models.",
      "arxiv_url": "https://openreview.net/forum?id=PLBVtJt4td",
      "pdf_url": "https://openreview.net/pdf/1126bea84448647427622496ecd246e73dccde15.pdf",
      "primary_category": "Multimodality Alignment; Token-Level Alignment; Multimodal Large Language Models; Representation Similarity",
      "categories": [
        "Multimodality Alignment; Token-Level Alignment; Multimodal Large Language Models; Representation Similarity"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BuYtcTUMyA",
      "title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks",
      "authors": [
        "Fali Wang",
        "Hui Liu",
        "Zhenwei DAI",
        "Jingying Zeng",
        "Zhiwei Zhang",
        "Zongyu Wu",
        "Chen Luo",
        "Zhen Li",
        "Xianfeng Tang",
        "Qi He",
        "Suhang Wang"
      ],
      "abstract": "Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability.",
      "arxiv_url": "https://openreview.net/forum?id=BuYtcTUMyA",
      "pdf_url": "https://openreview.net/pdf/13ea5abf7135c2269571122e956e24009565de1f.pdf",
      "primary_category": "language models, scaling, test-time compute",
      "categories": [
        "language models",
        "scaling",
        "test-time compute",
        "compute-optimal inference"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "dXqqFte3KT",
      "title": "Less is More: Local Intrinsic Dimensions of Contextual Language Models",
      "authors": [
        "Benjamin Matthias Ruppik",
        "Julius von Rohrscheidt",
        "Carel van Niekerk",
        "Michael Heck",
        "Renato Vukovic",
        "Shutong Feng",
        "Hsien-chin Lin",
        "Nurul Lubis",
        "Bastian Rieck",
        "Marcus Zibrowius",
        "Milica Gasic"
      ],
      "abstract": "Understanding the internal mechanisms of large language models (LLMs) remains a challenging and complex endeavor. \nEven fundamental questions, such as how fine-tuning affects model behavior, often require extensive empirical evaluation. \nIn this paper, we introduce a novel perspective based on the geometric properties of contextual latent embeddings to study the effects of training and fine-tuning. \nTo that end, we measure the local dimensions of a contextual language model's latent space and analyze their shifts during training and fine-tuning.\nWe show that the local dimensions provide insights into the model's training dynamics and generalization ability.\nSpecifically, the mean of the local dimensions predicts when the model’s training capabilities are exhausted, as exemplified in a dialogue state tracking task, overfitting, as demonstrated in an emotion recognition task, and grokking, as illustrated with an arithmetic task.\nFurthermore, our experiments suggest a practical heuristic: reductions in the mean local dimension tend to accompany and predict subsequent performance gains.\nThrough this exploration, we aim to provide practitioners with a deeper understanding of the implications of fine-tuning on embedding spaces, facilitating informed decisions when configuring models for specific applications. \nThe results of this work contribute to the ongoing discourse on the interpretability, adaptability, and generalizability of LLMs by bridging the gap between intrinsic model mechanisms and geometric properties in the respective embeddings.",
      "arxiv_url": "https://openreview.net/forum?id=dXqqFte3KT",
      "pdf_url": "https://openreview.net/pdf/a1ca323fee74a43a0992d0c353345d2c105ce725.pdf",
      "primary_category": "latent space geometry, contextual embeddings, LLMs",
      "categories": [
        "latent space geometry",
        "contextual embeddings",
        "LLMs",
        "intrinsic dimension",
        "training dynamics",
        "generalization",
        "overfitting detection",
        "grokking"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nGEq3D6FFX",
      "title": "Compress & Cache: Vision token compression for efficient generation and retrieval",
      "authors": [
        "Adrian Bulat",
        "Yassine Ouali",
        "Georgios Tzimiropoulos"
      ],
      "abstract": "This work aims to compress the vision tokens of an LVLM into a representation that is simultaneously suitable for (a) generative and (b) discriminative tasks, (c) is nearly lossless, and (d) storage-efficient. \nTo this end, we propose C&C, a novel compression method that leverages the LVLM itself for task-agnostic visual token compression.\nUnlike prior methods that perform token reduction on-the-fly, our approach offloads computation to a dedicated, upfront indexing stage, effectively decoupling compression from generation. This enables learning more powerful representations for generation during inference. \nAt the core of C&C is a ``double-forward pass'' training strategy. During the first forward pass, the LLM (of the LVLM) creates a bottleneck by compressing the dense visual tokens into a few summary tokens. Subsequently, the second forward pass processes the language instruction(s) alongside the summary tokens, used as a direct replacement for the image ones. \nThe training of C&C is guided by two key losses: an autoregressive loss applied after the second pass that provides a direct optimization objective for reconstructing the original information flow, and a contrastive loss applied after the first pass to bolster the representational strength of the summary tokens, particularly for discriminative tasks. Moreover, we propose stage-specific adapters for further enhancing performance. C&C  produces highly informative compressed representations. An in-depth ablation study confirms the efficacy of our approach. For generative tasks, we achieve a 2x higher compression rate without compromising capabilities, setting a new state-of-the-art. For discriminative tasks, we establish new state-of-the-art results on image retrieval and compositionality benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=nGEq3D6FFX",
      "pdf_url": "https://openreview.net/pdf/82ef421c53f3faf122d6626fe524ba5fc9c8bdab.pdf",
      "primary_category": "token compression, llava",
      "categories": [
        "token compression",
        "llava"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1ffIkWo0yq",
      "title": "Provable Gradient Editing of Deep Neural Networks",
      "authors": [
        "Zhe Tao",
        "Aditya V. Thakur"
      ],
      "abstract": "In explainable AI, DNN gradients are used to interpret the prediction; in safety-critical control systems, gradients could encode safety constraints; in scientific-computing applications, gradients could encode physical invariants. While recent work on provable editing of DNNs has focused on input-output constraints, the problem of enforcing hard constraints on DNN gradients remains unaddressed. We present ProGrad, the first efficient approach for editing the parameters of a DNN to provably enforce hard constraints on the DNN gradients. Given a DNN $\\mathcal{N}$ with parameters $\\theta$, and a set $\\mathcal{S}$ of pairs $(\\mathrm{x}, \\mathrm{Q})$ of input $\\mathrm{x}$ and corresponding linear gradient constraints $\\mathrm{Q}$, ProGrad finds new parameters $\\theta'$ such that $\\bigwedge_{(\\mathrm{x}, \\mathrm{Q}) \\in \\mathcal{S}} \\frac{\\partial}{\\partial \\mathrm{x}}\\mathcal{N}(\\mathrm{x}; \\theta') \\in \\mathrm{Q}$ while minimizing the changes $\\lVert\\theta' - \\theta\\rVert$. The key contribution is a novel *conditional variable gradient* of DNNs, which relaxes the NP-hard provable gradient editing problem to a linear program (LP), enabling ProGrad to use an LP solver to efficiently and effectively enforce the gradient constraints. We experimentally evaluated ProGrad via enforcing (i) hard Grad-CAM constraints on ImageNet ResNet DNNs; (ii) hard Integrated Gradients constraints on Llama 3 and Qwen 3 LLMs; (iii) hard gradient constraints in training a DNN to approximate a target function as a proxy for safety constraints in control systems and physical invariants in scientific applications. The results highlight the unique capability of ProGrad in enforcing hard constraints on DNN gradients.",
      "arxiv_url": "https://openreview.net/forum?id=1ffIkWo0yq",
      "pdf_url": "https://openreview.net/pdf/864c61b7a2e24db707ef858ff6bcd32926c98143.pdf",
      "primary_category": "Machine Learning, Deep Neural Network, Provable Editing",
      "categories": [
        "Machine Learning",
        "Deep Neural Network",
        "Provable Editing",
        "Explainable AI",
        "Interpretability",
        "Grad-CAM",
        "Integrated Gradients"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "x5lITYXmW2",
      "title": "Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts",
      "authors": [
        "Haizhong Zheng",
        "Yang Zhou",
        "Brian R. Bartoldson",
        "Bhavya Kailkhura",
        "Fan Lai",
        "Jiawei Zhao",
        "Beidi Chen"
      ],
      "abstract": "Reinforcement learning, such as PPO and GRPO, has powered recent breakthroughs in LLM reasoning. Scaling rollout to sample more prompts enables models to selectively use higher-quality data for training, which can stabilize RL training and improve model performance, but at the cost of significant computational overhead. In this paper, we first show that a substantial portion of this overhead can be avoided by skipping uninformative prompts before rollout. Our analysis of reward dynamics reveals a strong temporal consistency in prompt value: prompts that are uninformative in one epoch of training are likely to remain uninformative in near future epochs. Based on these insights, we propose GRESO (GRPO with Efficient Selective Rollout), an online, lightweight pre-rollout filtering algorithm that predicts and skips uninformative prompts using reward training dynamics. By evaluating GRESO on a broad range of math reasoning benchmarks and models, like Qwen2.5-Math-1.5B, DeepSeek-R1-Distill-Qwen-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, and Qwen2.5-32B, we show that GRESO achieves up to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total training time without accuracy degradation. We make our code publicly available at https://github.com/Infini-AI-Lab/GRESO/.",
      "arxiv_url": "https://openreview.net/forum?id=x5lITYXmW2",
      "pdf_url": "https://openreview.net/pdf/e9fe74d4ac932f9808ff517b30047d7e5763dad7.pdf",
      "primary_category": "LLM, GRPO, Reinforcement Learning",
      "categories": [
        "LLM",
        "GRPO",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8JLpE8YnjD",
      "title": "Short-length Adversarial Training Helps LLMs Defend Long-length Jailbreak Attacks: Theoretical and Empirical Evidence",
      "authors": [
        "Shaopeng Fu",
        "Liang Ding",
        "Jingfeng Zhang",
        "Di Wang"
      ],
      "abstract": "Jailbreak attacks against large language models (LLMs) aim to induce harmful behaviors in LLMs through carefully crafted adversarial prompts. To mitigate attacks, one way is to perform adversarial training (AT)-based alignment, i.e., training LLMs on some of the most adversarial prompts to help them learn how to behave safely under attacks. During AT, the length of adversarial prompts plays a critical role in the robustness of aligned LLMs. While long-length adversarial prompts during AT might lead to strong LLM robustness, their synthesis however is very resource-consuming, which may limit the application of LLM AT. This paper focuses on adversarial suffix jailbreak attacks and unveils that to defend against a jailbreak attack with an adversarial suffix of length $\\Theta(M)$, it is enough to align LLMs on prompts with adversarial suffixes of length $\\Theta(\\sqrt{M})$. Theoretically, we analyze the adversarial in-context learning of linear transformers on linear regression tasks and prove a robust generalization bound for trained transformers. The bound depends on the term $\\Theta(\\sqrt{M_{\\text{test}}}/M_{\\text{train}})$, where $M_{\\text{train}}$ and $M_{\\text{test}}$ are the numbers of adversarially perturbed in-context samples during training and testing. Empirically, we conduct AT on popular open-source LLMs and evaluate their robustness against jailbreak attacks of different adversarial suffix lengths. Results confirm a positive correlation between the attack success rate and the ratio of the square root of the adversarial suffix length during jailbreaking to the length during AT. Our findings show that it is practical to defend against \"long-length\" jailbreak attacks via efficient \"short-length\" AT. The code is available at https://github.com/fshp971/adv-icl.",
      "arxiv_url": "https://openreview.net/forum?id=8JLpE8YnjD",
      "pdf_url": "https://openreview.net/pdf/ed8deae5c0450890c6d185c947ecadc877ce974c.pdf",
      "primary_category": "In-context Learning, Adversarial Training, Jailbreak Attacks",
      "categories": [
        "In-context Learning",
        "Adversarial Training",
        "Jailbreak Attacks",
        "Large Language Models",
        "Learning Theory"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BDkNRlGmP9",
      "title": "Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames",
      "authors": [
        "Anurag Arnab",
        "Ahmet Iscen",
        "Mathilde Caron",
        "Alireza Fathi",
        "Cordelia Schmid"
      ],
      "abstract": "Despite recent advances in Vision-Language Models (VLMs), long-video understanding remains a challenging problem. Although state-of-the-art long-context VLMs can process around 1000 input frames, they still struggle to effectively leverage this sequence length, and succumb to irrelevant distractors within the context window. We present Dynamic Context Aggregation, an inference strategy for video question-answering that curates the model's input context. We use the VLM itself to iteratively identify and extract the most relevant frames from the video, which are then used for answering. We demonstrate how leveraging more computation at inference-time to select the most relevant context leads to improvements in accuracy, in agreement with recent work on inference-time scaling of LLMs. Moreover, we achieve state-of-the-art results on 4 diverse video question-answering datasets, showing consistent improvements with 3 different VLMs. In particular, our method shines on longer videos which would not otherwise fit in the model's context window: On longer videos of more than 1 hour on LVBench, our approach using a context window of 32K outperforms the same VLM using standard inference with a 700K context window by 2.8 points.",
      "arxiv_url": "https://openreview.net/forum?id=BDkNRlGmP9",
      "pdf_url": "https://openreview.net/pdf/ecef04e6004300bdfe10dc8ca9560dcffe5ecf5d.pdf",
      "primary_category": "VLM, Long-video understanding, video question-answering",
      "categories": [
        "VLM",
        "Long-video understanding",
        "video question-answering",
        "long-context"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "45igeoC560",
      "title": "Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving",
      "authors": [
        "Yuchen Zhang",
        "Hanyue Du",
        "Chun Cao",
        "Jingwei Xu"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient fine-tuning (PEFT) technique for adapting large language models (LLMs) to downstream tasks. While prior work has explored strategies for integrating LLM training and serving, there still remains a gap in unifying fine-tuning and inference for LoRA-based models. We present **Loquetier**, a virtualized multi-LoRA framework that seamlessly integrates LoRA fine-tuning and serving within a single runtime. Loquetier introduces two key components: (1) a Virtualized Module that isolates PEFT-based modifications and supports multiple adapters on a shared base model, and (2) an optimized computation flow with a kernel design that merges fine-tuning and inference paths in forward propagation, enabling efficient batching and minimizing kernel invocation overhead. Extensive experiments across three task settings show that Loquetier consistently outperforms existing baselines in both performance and flexibility, achieving up to $3.0\\times$ the throughput of the state-of-the-art co-serving system on inference-only tasks and $46.4\\times$ higher SLO attainment than PEFT on unified fine-tuning and inference tasks. The implementation of Loquetier is publicly available at https://github.com/NJUDeepEngine/Loquetier.",
      "arxiv_url": "https://openreview.net/forum?id=45igeoC560",
      "pdf_url": "https://openreview.net/pdf/96dd38c325596e5c2e70f27b2cbd13eed8aef321.pdf",
      "primary_category": "LLMs",
      "categories": [
        "LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "91l4ZTMpO4",
      "title": "Best-of-N Jailbreaking",
      "authors": [
        "John Hughes",
        "Sara Price",
        "Aengus Lynch",
        "Rylan Schaeffer",
        "Fazl Barez",
        "Arushi Somani",
        "Sanmi Koyejo",
        "Henry Sleight",
        "Erik Jones",
        "Ethan Perez",
        "Mrinank Sharma"
      ],
      "abstract": "We introduce Best-of-N (BoN) Jailbreaking, a simple black-box algorithm that jailbreaks frontier AI systems across modalities. BoN Jailbreaking works by repeatedly sampling variations of a prompt with a combination of augmentations---such as random shuffling or capitalization for textual prompts---until a harmful response is elicited. We find that BoN Jailbreaking achieves high attack success rates (ASRs) on closed-source language models, such as 89% on GPT-4o and 78% on Claude 3.5 Sonnet when sampling 10,000 augmented prompts. Further, it is similarly effective at circumventing state-of-the-art open-source defenses like circuit breakers and reasoning models like o1. BoN also seamlessly extends to other modalities: it jailbreaks vision language models (VLMs) such as GPT-4o and audio language models (ALMs) like Gemini 1.5 Pro, using modality-specific augmentations. BoN reliably improves when we sample more augmented prompts. Across all modalities, ASR, as a function of the number of samples (N), empirically follows power-law-like behavior for many orders of magnitude. BoN Jailbreaking can also be composed with other black-box algorithms for even more effective attacks---combining BoN with an optimized prefix attack achieves up to a 35% increase in ASR. Overall, our work indicates that, despite their capability, language models are sensitive to seemingly innocuous changes to inputs, which attackers can exploit across modalities.",
      "arxiv_url": "https://openreview.net/forum?id=91l4ZTMpO4",
      "pdf_url": "https://openreview.net/pdf/214b0cbe5fe5a3a56ddfd1977e1acfb9c721c50a.pdf",
      "primary_category": "robustness, adversarial robustness, jailbreaking",
      "categories": [
        "robustness",
        "adversarial robustness",
        "jailbreaking",
        "multi-modal jailbreaks",
        "automatic red teaming",
        "large language models",
        "LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Qh458ZamHm",
      "title": "WEDGE: Synthesizing Performance Constraints for Evaluating and Improving Code Efficiency",
      "authors": [
        "Jun Yang",
        "Cheng-Chi Wang",
        "Bogdan Alexandru Stoica",
        "Kexin Pei"
      ],
      "abstract": "Large Language Models (LLMs) have been increasingly used to optimize code efficiency. Evaluating their effectiveness and further suggesting optimization opportunities often rely on high-quality tests to demonstrate the performance bottlenecks presented in the program. However, existing approaches rely on a limited set of hand-curated inputs or LLM-generated uninteresting length-stressing tests, failing to reveal more nuanced optimization opportunities. We present WEDGE, a framework for generating performance-stressing input given the program under test. WEDGE synthesizes explicit performance-characterizing constraints in the form of branch conditions to partition the programs’ execution space into performance-specific regions. When integrated with the coverage-guided fuzzer, reaching different regions introduces explicit rewards for test generation to explore inefficient implementations. Our evaluation shows that WEDGE introduces a significant slowdown compared to the tests in CodeContests and those claimed to be optimized by existing approaches. From the utility perspective, integrating our tests substantially improves the existing code optimization approaches that rely on test-driven execution feedback. We release PERFFORGE, the performance tests generated by WEDGE, to benchmark future approaches for efficient code generation at https://github.com/UChiSeclab/perfforge.",
      "arxiv_url": "https://openreview.net/forum?id=Qh458ZamHm",
      "pdf_url": "https://openreview.net/pdf/7c9fc577929c90b9844fdcfbbc5bc1879da581ea.pdf",
      "primary_category": "Large Language Models, Test Generation, Code Analysis",
      "categories": [
        "Large Language Models",
        "Test Generation",
        "Code Analysis",
        "Performance Analysis",
        "Code Optimization",
        "Fuzzing"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZBlHEeSvKd",
      "title": "PaTH Attention: Position Encoding via Accumulating Householder Transformations",
      "authors": [
        "Songlin Yang",
        "Yikang Shen",
        "Kaiyue Wen",
        "Shawn Tan",
        "Mayank Mishra",
        "Liliang Ren",
        "Rameswar Panda",
        "Yoon Kim"
      ],
      "abstract": "The attention mechanism is a core primitive in modern large language models (LLMs) and AI more broadly. Since attention by itself is permutation-invariant, position encoding is essential for modeling structured domains such as language. Rotary position encoding (RoPE) has emerged as the de facto standard approach for position encoding and is part of many modern LLMs. However, in RoPE the key/query transformation between two elements in a sequence is only a function of their relative position and otherwise independent of the actual input. This limits the expressivity of RoPE-based transformers.\n\n   This paper describes PaTH, a flexible data-dependent position encoding scheme based on accumulated products of  Householder(like) transformations, where each transformation is data-dependent, i.e., a function of the input. We derive an efficient parallel  algorithm for training through exploiting a compact representation of products of Householder matrices, and implement a FlashAttention-style blockwise  algorithm. Across both targeted synthetic benchmarks  and moderate-scale real-world language modeling experiments, we find that PaTH improves upon RoPE and other recent baselines. Finally, we show that we can convert pretrained RoPE transformers into PaTH with continued pretraining.",
      "arxiv_url": "https://openreview.net/forum?id=ZBlHEeSvKd",
      "pdf_url": "https://openreview.net/pdf/ebf94fa6640e357e04f3502291de9bcd64bc534b.pdf",
      "primary_category": "position encoding, state tracking, position embedding",
      "categories": [
        "position encoding",
        "state tracking",
        "position embedding",
        "contextualized position encoding",
        "sequence modeling",
        "softmax attention",
        "attention mechanism",
        "DeltaNet",
        "delta rule"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3G0IWDIoRG",
      "title": "Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation",
      "authors": [
        "Erfan Baghaei Potraghloo",
        "Seyedarmin Azizi",
        "Souvik Kundu",
        "Massoud Pedram"
      ],
      "abstract": "Large language models (LLMs), despite their impressive performance across a wide range of tasks, often struggle to balance two competing objectives in open-ended text generation: fostering diversity and creativity while preserving logical coherence. Existing truncated sampling techniques, including temperature scaling, top-*p* (nucleus) sampling, and min-*p* sampling, aim to manage this trade-off. However, they exhibit limitations, particularly in the effective incorporation of the confidence of the model into the corresponding sampling strategy. For example, min-*p* sampling relies on a single top token as a heuristic for confidence, eventually underutilizing the information of the probability distribution.  To effectively incorporate the model confidence, this paper presents **_top-H_ decoding**. We first establish the theoretical foundation of the interplay between creativity and coherence in truncated sampling by formulating an **entropy-constrained minimum divergence** problem. We then prove this minimization problem to be equivalent to an **entropy-constrained mass maximization (ECMM)** problem, which is NP-hard. Finally, we present top-H decoding, a computationally efficient greedy algorithm to solve the ECMM problem.  Extensive empirical evaluations demonstrate that top-H outperforms the state-of-the-art (SoTA) alternative of min-*p* sampling by up to **25.63%** on creative writing benchmarks, while maintaining robustness on question-answering datasets such as GPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms that top-H indeed produces coherent outputs even at higher temperatures, where creativity is especially critical. In summary, top-H advances SoTA in open-ended text generation and can be *easily integrated* into creative writing applications. The code is available at [https://github.com/ErfanBaghaei/Top-H-Decoding](https://github.com/ErfanBaghaei/Top-H-Decoding).",
      "arxiv_url": "https://openreview.net/forum?id=3G0IWDIoRG",
      "pdf_url": "https://openreview.net/pdf/0b494e52bae7fe34f7af35e0d5bfa6bd0dcb39b8.pdf",
      "primary_category": "LLMs, Truncated sampling, Decoding Strategies",
      "categories": [
        "LLMs",
        "Truncated sampling",
        "Decoding Strategies"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "GSAKL9tc7L",
      "title": "FoGE: Fock Space inspired encoding for graph prompting",
      "authors": [
        "Sotirios Panagiotis Chytas",
        "Rudrasis Chakraborty",
        "Vikas Singh"
      ],
      "abstract": "Recent results show that modern Large Language Models (LLM) are indeed capable of understanding and answering questions about structured data such as graphs. This new paradigm can lead to solutions that require less supervision while, at the same time, providing a model that can generalize and answer questions beyond the training labels. Existing proposals often use some description of the graph to create an ``augmented'' prompt fed to the LLM. For a chosen class of graphs, if a well-tailored graph encoder is deployed to play together with a pre-trained LLM, the model can answer graph-related questions well. Existing solutions to graph-based prompts range from graph serialization to graph transformers. In this work, we show that the use of a parameter-free graph encoder based on Fock space representations, a concept  borrowed from mathematical physics, is remarkably versatile in this problem setting. The simple construction, inherited directly from the theory with a few small adjustments, can provide rich and informative graph encodings, for a wide range of different graphs. We investigate the use of this idea for prefix-tuned prompts leveraging the capabilities of a pre-trained, frozen LLM. The modifications lead to a model that can answer graph-related questions -- from simple graphs to proteins to hypergraphs -- effectively and  with minimal, if any, adjustments to the architecture. Our work significantly simplifies existing solutions and generalizes well to multiple different graph-based structures effortlessly.",
      "arxiv_url": "https://openreview.net/forum?id=GSAKL9tc7L",
      "pdf_url": "https://openreview.net/pdf/2330463e7ea609c4689f065547e0432f7bd8324e.pdf",
      "primary_category": "Graphs, Fock Space, Geometric Algebra",
      "categories": [
        "Graphs",
        "Fock Space",
        "Geometric Algebra",
        "Vector Symbolic Architectures"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DxKP2E0xK2",
      "title": "Universal Cross-Tokenizer Distillation via Approximate Likelihood Matching",
      "authors": [
        "Benjamin Minixhofer",
        "Ivan Vulić",
        "Edoardo Ponti"
      ],
      "abstract": "Distillation has shown remarkable success in transferring knowledge from a Large Language Model (LLM) teacher to a student LLM. However, current distillation methods require similar tokenizers between the teacher and the student, restricting their applicability to only a small subset of teacher--student pairs. In this work, we develop a principled cross-tokenizer distillation method to solve this crucial deficiency. Our method is the first to enable effective distillation across fundamentally different tokenizers, while also substantially outperforming prior methods in all other cases. We verify the efficacy of our method on three distinct use cases. First, we show that viewing tokenizer transfer as self-distillation enables unprecedentedly effective transfer across tokenizers, including rapid transfer of subword models to the byte-level. Transferring different models to the same tokenizer also enables ensembling to boost performance. Secondly, we distil a large maths-specialised LLM into a small general-purpose model with a different tokenizer, achieving competitive maths problem-solving performance. Thirdly, we use our method to train state-of-the-art embedding prediction hypernetworks for training-free tokenizer transfer. Our results unlock an expanded range of teacher--student pairs for distillation, enabling new ways to adapt and enhance interaction between LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=DxKP2E0xK2",
      "pdf_url": "https://openreview.net/pdf/8eea5cadec7b4feb167fb67b0ed95e27474c4e79.pdf",
      "primary_category": "distillation, tokenization, modularity",
      "categories": [
        "distillation",
        "tokenization",
        "modularity",
        "tokenizer transfer"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ccPts3Df2q",
      "title": "The Hawthorne Effect in Reasoning Models: Evaluating and Steering Test Awareness",
      "authors": [
        "Sahar Abdelnabi",
        "Ahmed Salem"
      ],
      "abstract": "Reasoning-focused LLMs sometimes alter their behavior when they detect that they are being evaluated—which can lead them to optimize for test-passing performance or to comply more readily with harmful prompts if real-world consequences appear absent. We present the first quantitative study of how such “test awareness” impacts model behavior, particularly its performance on safety-related tasks. We introduce a white-box probing framework that (i) linearly identifies awareness-related activations and (ii) steers models toward or away from test awareness while monitoring downstream performance. We apply our method to different state-of-the-art open-weight reasoning LLMs across both realistic and hypothetical tasks (denoting tests or simulations). Our results demonstrate that test awareness significantly impacts safety alignment (such as compliance with harmful requests and conforming to stereotypes) with effects varying in both magnitude and direction across models. By providing control over this latent effect, our work aims to provide a stress-test mechanism and increase trust in how we perform safety evaluations.",
      "arxiv_url": "https://openreview.net/forum?id=ccPts3Df2q",
      "pdf_url": "https://openreview.net/pdf/beb8c8b717b5764e1ae9e2c4fdfe81940c18270a.pdf",
      "primary_category": "Model awareness; test awareness; evaluation awareness; steering",
      "categories": [
        "Model awareness; test awareness; evaluation awareness; steering"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "OuklL6Q3sO",
      "title": "Fast Solvers for Discrete Diffusion Models: Theory and Applications of High-Order Algorithms",
      "authors": [
        "Yinuo Ren",
        "Haoxuan Chen",
        "Yuchen Zhu",
        "Wei Guo",
        "Yongxin Chen",
        "Grant M. Rotskoff",
        "Molei Tao",
        "Lexing Ying"
      ],
      "abstract": "Discrete diffusion models have emerged as a powerful generative modeling framework for discrete data with successful applications spanning from text generation to image synthesis. However, their deployment faces challenges due to the high dimensionality of the state space, necessitating the development of efficient inference algorithms. Current inference approaches mainly fall into two categories: exact simulation and approximate methods such as $\\tau$-leaping. While exact methods suffer from unpredictable inference time and redundant function evaluations, $\\tau$-leaping is limited by its first-order accuracy. In this work, we advance the latter category by tailoring the first extension of high-order numerical inference schemes to discrete diffusion models, enabling larger step sizes while reducing error. We rigorously analyze the proposed schemes and establish the second-order accuracy of the $\\theta$-Trapezoidal method in KL divergence. Empirical evaluations on GSM8K-level math-reasoning, GPT-2-level text, and ImageNet-level image generation tasks demonstrate that our method achieves superior sample quality compared to existing approaches under equivalent computational constraints, with consistent performance gains across models ranging from 200M to 8B. Our code is available at https://github.com/yuchen-zhu-zyc/DiscreteFastSolver",
      "arxiv_url": "https://openreview.net/forum?id=OuklL6Q3sO",
      "pdf_url": "https://openreview.net/pdf/2ed15ff16a97abb82427e599a4fc70d666c7ff74.pdf",
      "primary_category": "Discrete diffusion model, fast solver, high-order scheme",
      "categories": [
        "Discrete diffusion model",
        "fast solver",
        "high-order scheme",
        "Runge-Kutta method"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "E7gH8L4yHi",
      "title": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and Generation via Reinforcement Learning",
      "authors": [
        "Kaihang Pan",
        "Yang Wu",
        "Wendong Bu",
        "Kai Shen",
        "Juncheng Li",
        "Yingting Wang",
        "liyunfei",
        "Siliang Tang",
        "Jun Xiao",
        "Fei Wu",
        "ZhaoHang",
        "Yueting Zhuang"
      ],
      "abstract": "Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify visual comprehension and generation. However, these two capabilities remain largely independent, as if they are two separate functions encapsulated within the same model. Consequently, visual comprehension does not enhance visual generation, and the reasoning mechanisms of LLMs have not been fully integrated to revolutionize image generation. In this paper, we propose to enable the collaborative co-evolution of visual comprehension and generation, advancing image generation into an iterative introspective process. We introduce a two-stage training approach: supervised fine-tuning teaches the MLLM with the foundational ability to generate genuine CoT for visual generation, while reinforcement learning activates its full potential via an exploration-exploitation trade-off. Ultimately, we unlock the Aha moment in visual generation, advancing MLLMs from text-to-image tasks to unified image generation. Extensive experiments demonstrate that our model not only excels in text-to-image generation and image editing, but also functions as a superior image semantic evaluator with enhanced visual comprehension capabilities. Project Page: \\url{https://janus-pro-r1.github.io}.",
      "arxiv_url": "https://openreview.net/forum?id=E7gH8L4yHi",
      "pdf_url": "https://openreview.net/pdf/1bd7f81bb169e6a36599f94208fd1a3f52c3b803.pdf",
      "primary_category": "Image generation, Image understanding",
      "categories": [
        "Image generation",
        "Image understanding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cZMno8E3yp",
      "title": "Preventing Shortcuts in Adapter Training via Providing the Shortcuts",
      "authors": [
        "Anujraaj Goyal",
        "Guocheng Qian",
        "Huseyin Coskun",
        "Aarush Gupta",
        "Himmy Tam",
        "Daniil Ostashev",
        "Ju Hu",
        "Dhritiman Sagar",
        "Sergey Tulyakov",
        "Kfir Aberman",
        "Kuan-Chieh Wang"
      ],
      "abstract": "Adapter-based training has emerged as a key mechanism for extending the capabilities of powerful foundation image generators, enabling personalized and stylized text-to-image synthesis. These adapters are typically trained to capture a specific target attribute, such as subject identity, using single-image reconstruction objectives. However, because the input image inevitably contains a mixture of visual factors, adapters are prone to entangle the target attribute with incidental ones, such as pose, expression, and lighting. This spurious correlation problem limits generalization and obstructs the model's ability to adhere to the input text prompt.  In this work, we uncover a simple yet effective solution: provide the very shortcuts we wish to eliminate during adapter training. In Shortcut-Rerouted Adapter Training, confounding factors are routed through auxiliary modules, such as ControlNet or LoRA, eliminating the incentive for the adapter to internalize them. The auxiliary modules are then removed during inference. When applied to tasks like facial and full-body identity injection, our approach improves generation quality, diversity, and prompt adherence. These results point to a general design principle in the era of large models: when seeking disentangled representations, the most effective path may be to establish shortcuts for what should NOT be learned.",
      "arxiv_url": "https://openreview.net/forum?id=cZMno8E3yp",
      "pdf_url": "https://openreview.net/pdf/fd8fde782aeeae7f68a5baa72ba7260fb70a7fa6.pdf",
      "primary_category": "text to image generation, adapter, personalized generation",
      "categories": [
        "text to image generation",
        "adapter",
        "personalized generation",
        "controllable generation",
        "spurrious correlation",
        "shortcut learning"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "JRmIvBcnWc",
      "title": "GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining",
      "authors": [
        "Simin Fan",
        "Maria Ios Glarou",
        "Martin Jaggi"
      ],
      "abstract": "The performance of large language models (LLMs) across diverse downstream applications is fundamentally governed by the quality and composition of their pretraining corpora.\nExisting domain reweighting algorithms primarily optimize data mixtures for a single target task, thereby resulting in models that overfit to specialized objectives while exhibiting substantial performance degradation on other benchmarks.\nThis paper introduces $\\textbf{G}$roup $\\textbf{R}$obust Multi-target $\\textbf{A}$daptive $\\textbf{P}$r$\\textbf{E}$training (GRAPE), a novel multi-source-multi-target domain reweighting framework designed to calibrate pretraining data mixtures for robust performance across multiple target tasks simultaneously.\nGRAPE dynamically adjusts sampling weights across source domains ($\\textit{domain weights}$) while concurrently modulating $\\textit{task weights}$ that quantify the relative importance of each individual target task.\nThis adaptive process prioritizes tasks based on their learning difficulty throughout training. \nWe formulate this interleaved reweighting mechanism as a minimax optimization problem: \nThe inner maximization adjusts task weights leveraging group distributed-robust-optimization (DRO), where those tasks demonstrating the least improvement under the current data mixture are prioritized with higher weights; \nThe outer minimization then optimizes domain weights to maximize loss reduction on the prioritized tasks.\nExperiments on $\\texttt{ClimbLab}$ and $\\texttt{SlimPajama}$ datasets demonstrate that GRAPE consistently outperforms baseline methods in terms of reasoning accuracies across 6 benchmarks. \nFurthermore, when applied to multilingual targets, GRAPE effectively identifies optimal training mixtures from mainstream languages, achieving superior language modeling capabilities across 8 low-resource target languages.",
      "arxiv_url": "https://openreview.net/forum?id=JRmIvBcnWc",
      "pdf_url": "https://openreview.net/pdf/d794b4f238aed6abf473b91fc6b2987e67d47220.pdf",
      "primary_category": "LLM pretrain, data selection",
      "categories": [
        "LLM pretrain",
        "data selection"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IiEtQPGVyV",
      "title": "Efficient semantic uncertainty quantification in language models via diversity-steered sampling",
      "authors": [
        "Ji Won Park",
        "Kyunghyun Cho"
      ],
      "abstract": "Accurately estimating *semantic* aleatoric and epistemic uncertainties in large language models (LLMs) is particularly challenging in free-form question answering (QA), where obtaining stable estimates often requires many expensive generations. We introduce a **diversity-steered sampler** that discourages semantically redundant outputs during decoding, covers both autoregressive and masked diffusion paradigms, and yields substantial sample-efficiency gains. The key idea is to inject a continuous semantic-similarity penalty into the model’s proposal distribution using a natural language inference (NLI) model lightly finetuned on partial prefixes or intermediate diffusion states. We debias downstream uncertainty estimates with importance reweighting and shrink their variance with control variates. Across four QA benchmarks, our method matches or surpasses baselines while covering more semantic clusters with the same number of samples. Being modular and requiring no gradient access to the base LLM, the framework promises to serve as a drop-in enhancement for uncertainty estimation in risk-sensitive model deployments.",
      "arxiv_url": "https://openreview.net/forum?id=IiEtQPGVyV",
      "pdf_url": "https://openreview.net/pdf/a98c7ff39658fbd7515c9e50003f9b26620ff762.pdf",
      "primary_category": "uncertainty quantification, large language models, natural language generation",
      "categories": [
        "uncertainty quantification",
        "large language models",
        "natural language generation",
        "semantic diversity",
        "importance sampling"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "S6SKa97Gm0",
      "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
      "authors": [
        "Kristjan Greenewald",
        "Luis A. Lastras",
        "Thomas Parnell",
        "Vraj Shah",
        "Lucian Popa",
        "Giulio Zizzo",
        "Chulaka Gunasekara",
        "Ambrish Rawat",
        "David Daniel Cox"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for finetuning the weights of large foundation models, and has become the go-to method for data-driven customization of LLMs. Despite the promise of highly customized behaviors and capabilities, switching between relevant LoRAs in a multiturn setting is inefficient, as the key-value (KV) cache of the entire turn history must be recomputed with the LoRA weights before generation can begin. To address this problem, we propose Activated LoRA (aLoRA), an adapter architecture which modifies the LoRA framework to only adapt weights for the tokens in the sequence after the aLoRA is invoked. This change crucially allows aLoRA to accept the base model's KV cache of the input string, meaning that aLoRA can be instantly activated whenever needed in a chain without recomputing the prior keys and values. This enables building what we call intrinsics, i.e. specialized models invoked to perform well-defined operations on portions of an input chain or conversation that otherwise uses the base model by default. We train a set of aLoRA-based intrinsics models, demonstrating competitive accuracy with standard LoRA while significantly improving inference efficiency. We contributed our Activated LoRA implementation to the Huggingface PEFT library.",
      "arxiv_url": "https://openreview.net/forum?id=S6SKa97Gm0",
      "pdf_url": "https://openreview.net/pdf/3e772cdebadbd5f5afeffe2e7697ab860b68a4d6.pdf",
      "primary_category": "low rank adapters, parameter efficient finetuning, inference-efficient adapter architectures",
      "categories": [
        "low rank adapters",
        "parameter efficient finetuning",
        "inference-efficient adapter architectures"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "b6SWqFEOSF",
      "title": "Private Training Large-scale Models with Efficient DP-SGD",
      "authors": [
        "Liangyu Wang",
        "Junxiao Wang",
        "Jie Ren",
        "Zihang Xiang",
        "David E. Keyes",
        "Di Wang"
      ],
      "abstract": "As large language models (LLMs) increasingly underpin technological advancements, the privacy of their training data emerges as a critical concern. Differential Privacy (DP) serves as a rigorous mechanism to protect this data, yet its integration via Differentially Private Stochastic Gradient Descent (DP-SGD) introduces substantial challenges, primarily due to the complexities of per-sample gradient clipping. Current explicit methods, such as Opacus, necessitate extensive storage for per-sample gradients, significantly inflating memory requirements. Conversely, implicit methods like GhostClip reduce storage needs by recalculating gradients multiple times, which leads to inefficiencies due to redundant computations. This paper introduces FlashDP, an innovative cache-friendly per-layer DP-SGD that consolidates necessary operations into a single task, calculating gradients only once in a fused manner. This approach not only diminishes memory movement by up to 50\\% but also cuts down redundant computations by 20\\%, compared to previous methods. Consequently, FlashDP does not increase memory demands and achieves a 90\\% throughput compared to the Non-DP method on a four-A100 system during the pre-training of the Llama-13B model, while maintaining parity with standard per-layer clipped DP-SGD in terms of accuracy. These advancements establish FlashDP as a pivotal development for efficient and privacy-preserving training of LLMs. FlashDP's code has been open-sourced in https://github.com/kaustpradalab/flashdp.",
      "arxiv_url": "https://openreview.net/forum?id=b6SWqFEOSF",
      "pdf_url": "https://openreview.net/pdf/d768b7e7a1b434677c0dc7d91ab4df2d7b50f9f7.pdf",
      "primary_category": "Differential Privacy, Large Language Models, DP-SGD",
      "categories": [
        "Differential Privacy",
        "Large Language Models",
        "DP-SGD"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "C69741fMFX",
      "title": "MUSTAFAR: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference",
      "authors": [
        "Donghyeon Joo",
        "Helya Hosseini",
        "Ramyad Hadidi",
        "Bahar Asgari"
      ],
      "abstract": "We demonstrate that unstructured sparsity significantly improves KV cache compression for LLMs, enabling sparsity levels up to 70\\% without compromising accuracy or requiring fine-tuning. We conduct a systematic exploration of pruning strategies and find per-token magnitude-based pruning as highly effective for both Key and Value caches under unstructured sparsity, surpassing prior structured pruning schemes. The Key cache benefits from prominent outlier elements, while the Value cache surprisingly benefits from a simple magnitude-based pruning despite its uniform distribution. KV cache size is the major bottleneck in decode performance due to high memory overhead for large context lengths. To address this, we use a bitmap-based sparse format and a custom attention kernel capable of compressing and directly computing over compressed caches pruned to arbitrary sparsity patterns, significantly accelerating memory-bound operations in decode computations and thereby compensating for the overhead of runtime pruning and compression. Our custom attention kernel coupled with the bitmap-based format delivers substantial compression of KV cache up to 45\\% of dense inference and thereby enables longer context lengths and increased tokens/sec throughput of up to 2.23$\\times$ compared to dense inference. Our pruning mechanism and sparse attention kernel is available at https://github.com/dhjoo98/mustafar.",
      "arxiv_url": "https://openreview.net/forum?id=C69741fMFX",
      "pdf_url": "https://openreview.net/pdf/3f12569e75bc44a8825d64bce1b98b485b326f52.pdf",
      "primary_category": "KV cache compression, Unstructured Pruning, Efficient LLM Inference",
      "categories": [
        "KV cache compression",
        "Unstructured Pruning",
        "Efficient LLM Inference"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DkSeM3AZVs",
      "title": "Guiding LLM Decision-Making with Fairness Reward Models",
      "authors": [
        "Zara Hall",
        "Melanie Subbiah",
        "Thomas P Zollo",
        "Kathleen McKeown",
        "Richard Zemel"
      ],
      "abstract": "Large language models are increasingly used to support high-stakes decisions, potentially influencing who is granted bail or receives a loan. Naive chain-of-thought sampling can improve average decision accuracy, but has also been shown to amplify unfair bias. To address this challenge and enable the trustworthy use of reasoning models in high-stakes decision-making, we propose a framework for training a generalizable Fairness Reward Model (FRM). Our model assigns a fairness score to LLM reasoning, enabling the system to down-weight biased trajectories and favor equitable ones when aggregating decisions across reasoning chains. We show that a single Fairness Reward Model, trained on weakly supervised, LLM-annotated examples of biased versus unbiased reasoning, transfers across tasks, domains, and model families without additional fine-tuning. When applied to real-world decision-making tasks including recidivism prediction and social media moderation, our approach consistently improves fairness while matching, or even surpassing, baseline accuracy.",
      "arxiv_url": "https://openreview.net/forum?id=DkSeM3AZVs",
      "pdf_url": "https://openreview.net/pdf/9c67b8ffdbef13dfa1266b41fef89b0ccc4f7f74.pdf",
      "primary_category": "fairness, bias mitigation, test-time compute",
      "categories": [
        "fairness",
        "bias mitigation",
        "test-time compute",
        "language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zo4zYTR8vn",
      "title": "Analog Foundation Models",
      "authors": [
        "Julian Büchel",
        "Iason Chalas",
        "Giovanni Acampa",
        "An Chen",
        "Omobayode Fagbohungbe",
        "Hsinyu Tsai",
        "Kaoutar El Maghraoui",
        "Manuel Le Gallo",
        "Abbas Rahimi",
        "Abu Sebastian"
      ],
      "abstract": "Analog in-memory computing (AIMC) is a promising compute paradigm to improve speed and power efficiency of neural network inference beyond the limits of conventional von Neumann-based architectures. However, AIMC introduces fundamental challenges such as noisy computations and strict constraints on input and output quantization. Because of these constraints and imprecisions, off-the-shelf LLMs are not able to achieve 4-bit-level performance when deployed on AIMC-based hardware. While researchers previously investigated recovering this accuracy gap on small, mostly vision-based models, a generic method applicable to LLMs pre-trained on trillions of tokens does not yet exist. In this work, we introduce a general and scalable method to robustly adapt LLMs for execution on noisy, low-precision analog hardware. Our approach enables state-of-the-art models — including Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct — to retain performance comparable to 4-bit weight, 8-bit activation baselines, despite the presence of analog noise and quantization constraints. Additionally, we show that as a byproduct of our training methodology, analog foundation models can be quantized for inference on low-precision digital hardware. Finally, we show that our models also benefit from test-time compute scaling, showing better scaling behavior than models trained with 4-bit weight and 8-bit static input quantization. Our work bridges the gap between high-capacity LLMs and efficient analog hardware, offering a path toward energy-efficient foundation models. Code is available at [github.com/IBM/analog-foundation-models](https://github.com/IBM/analog-foundation-models).",
      "arxiv_url": "https://openreview.net/forum?id=zo4zYTR8vn",
      "pdf_url": "https://openreview.net/pdf/0164c70993619d19546a13b8a5c806ba0cbd5e3f.pdf",
      "primary_category": "analog in-memory computing, robustness, large language models",
      "categories": [
        "analog in-memory computing",
        "robustness",
        "large language models",
        "foundation models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Ixl7e5pR52",
      "title": "GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer",
      "authors": [
        "Sayan Deb Sarkar",
        "Sinisa Stekovic",
        "Vincent Lepetit",
        "Iro Armeni"
      ],
      "abstract": "Transferring appearance to 3D assets using different representations of the appearance object - such as images or text - has garnered interest due to its wide range of applications in industries like gaming, augmented reality, and digital content creation. However, state-of-the-art methods still fail when the geometry between the input and appearance objects is significantly different. A straightforward approach is to directly apply a 3D generative model, but we show that this ultimately fails to produce appealing results. Instead, we propose a principled approach inspired by universal guidance. Given a pretrained rectified flow model conditioned on image or text, our training-free method interacts with the sampling process by periodically adding guidance. This guidance can be modeled as a differentiable loss function, and we experiment with two different types of guidance including part-aware losses for appearance and self-similarity. Our experiments show that our approach successfully transfers texture and geometric details to the input 3D asset, outperforming baselines both qualitatively and quantitatively. We also show that traditional metrics are not suitable for evaluating the task due to their inability of focusing on local details and comparing dissimilar inputs, in absence of ground truth data. We thus evaluate appearance transfer quality with a GPT-based system objectively ranking outputs, ensuring robust and human-like assessment, as further confirmed by our user study. Beyond showcased scenarios, our method is general and could be extended to different types of diffusion models and guidance functions. Project Page: *[https://sayands.github.io/guideflow3d](https://sayands.github.io/guideflow3d)*",
      "arxiv_url": "https://openreview.net/forum?id=Ixl7e5pR52",
      "pdf_url": "https://openreview.net/pdf/ad14d957f040a469b7969b0b2cb5aee71c681bde.pdf",
      "primary_category": "3D Diffusion, 3D Appearance Transfer, Generative Guidance",
      "categories": [
        "3D Diffusion",
        "3D Appearance Transfer",
        "Generative Guidance"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "uBaFH7aQnC",
      "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments",
      "authors": [
        "Junyoung Park",
        "Dalton Jones",
        "Matthew J Morse",
        "Raghavv Goel",
        "Mingu Lee",
        "Christopher Lott"
      ],
      "abstract": "We demonstrate that geometrically distinctive keys during LLM inference tend to have high attention scores. Based on the phenomenon we propose KeyDiff, a training-free KV cache eviction method based solely on key similarity. Unlike other KV cache eviction methods, KeyDiff can process arbitrarily long prompts within strict resource constraints and efficiently generate responses.\nWe provide a theoretical basis for KeyDiff by relating key diversity with attention scores. These results imply  KeyDiff can efficiently identify the most important tokens to retain. Notably KeyDiff does not rely on attention scores, allowing the use of optimized attention mechanisms like FlashAttention. Under a strict memory allowance, we demonstrate the effectiveness of KeyDiff for the Llama and Qwen model families by observing a performance gap of less than 0.04\\% with 8K cache budget (~23\\% KV cache reduction) from the non-evicting baseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near baseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning benchmark and decrease end-to-end inference latency by up to 30\\% compared to the other token-eviction methods.",
      "arxiv_url": "https://openreview.net/forum?id=uBaFH7aQnC",
      "pdf_url": "https://openreview.net/pdf/5717ff92a2ae0a9fa4cb2f7968739b926adc1318.pdf",
      "primary_category": "Large Language Models, LLM, Key-Value Cache",
      "categories": [
        "Large Language Models",
        "LLM",
        "Key-Value Cache",
        "KV Cache",
        "KV Cache Eviction",
        "Token Eviction",
        "Efficient LLM Inference"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "M5jz47umjR",
      "title": "DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding",
      "authors": [
        "Yunhai Hu",
        "Tianhua Xia",
        "Zining Liu",
        "Rahul Raman",
        "Xingyu Liu",
        "BO BAO",
        "Eric Sather",
        "Vithursan Thangarasa",
        "Sai Qian Zhang"
      ],
      "abstract": "Speculative decoding (SD) has emerged as a powerful method for accelerating autoregressive generation in large language models (LLMs), yet its integration into vision-language models (VLMs) remains underexplored. We introduce DREAM, a novel speculative decoding framework tailored for VLMs that combines three key innovations: (1) a cross-attention-based mechanism to inject intermediate features from the target model into the draft model for improved alignment, (2) adaptive intermediate feature selection based on attention entropy to guide efficient draft model training, and (3) visual token compression to reduce draft model latency. DREAM enables efficient, accurate, and parallel multimodal decoding with significant throughput improvement. Experiments across a diverse set of recent popular VLMs, including LLaVA, Pixtral, SmolVLM and Gemma3, demonstrate up to 3.6x speedup over conventional decoding and significantly outperform prior SD baselines in both inference throughput and speculative draft acceptance length across a broad range of multimodal benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=M5jz47umjR",
      "pdf_url": "https://openreview.net/pdf/5582f2867ceb910e424d7da894c313a650e51e17.pdf",
      "primary_category": "Speculative Decoding, Vision language model",
      "categories": [
        "Speculative Decoding",
        "Vision language model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XUKUx7Xu89",
      "title": "Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training",
      "authors": [
        "William Merrill",
        "Shane Arora",
        "Dirk Groeneveld",
        "Hannaneh Hajishirzi"
      ],
      "abstract": "The right batch size is important when training language models at scale: a large batch size is necessary for fast training, but a batch size that is *too large* will harm token efficiency. To navigate this tradeoff, McCandlish et al. (2018) suggest that a *critical batch size* (CBS), below which training will not substantially degrade loss, can be estimated based on the gradient noise scale during training. While their method has been adopted in practice, e.g., when training GPT-3, strong assumptions are required to justify gradient noise as a proxy for the CBS, which makes it unclear whether their approach should be trusted in practice, limiting its applicability. In this paper, we introduce a simple, empirical approach to *directly* measure the CBS and show how the CBS evolves over training. Applying our approach to the OLMo models, we find that CBS is near 0 at initialization, increases rapidly at first, and then plateaus as training progresses. Furthermore, we find that this trend holds across different model sizes (1B and 7B), suggesting CBS from small training runs can inform larger-scale training runs. Our findings about how the CBS changes over training motivate *batch size warmup* as a natural way to reliably train language models at large batch size: start the batch size small and increase it as the CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to slightly better loss than the original training run with 43% fewer gradient steps. This shows how our framework can be applied to reliably train language models at larger batch sizes, increasing data parallelism without compromising performance.",
      "arxiv_url": "https://openreview.net/forum?id=XUKUx7Xu89",
      "pdf_url": "https://openreview.net/pdf/fc46293fc342e1f7b2794debf317afc3914462dd.pdf",
      "primary_category": "large language models, pretraining, critical batch size",
      "categories": [
        "large language models",
        "pretraining",
        "critical batch size"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "UE0cxjNnIw",
      "title": "Scaling Up Active Testing to Large Language Models",
      "authors": [
        "Gabrielle Berrada",
        "Jannik Kossen",
        "Freddie Bickford Smith",
        "Muhammed Razzak",
        "Yarin Gal",
        "Tom Rainforth"
      ],
      "abstract": "Active testing enables label-efficient evaluation of predictive models through careful data acquisition, but it can pose a significant computational cost. We identify cost-saving measures that enable active testing to be scaled up to large language models (LLMs). In particular we show that the surrogate model used to guide data acquisition can be constructed cheaply using in-context learning, does not require updating within an active-testing loop, and can be smaller than the target model. We even find we can make good data-acquisition decisions without making predictions with the target model. As a result we are able to achieve much more accurate evaluations of LLM performance relative to using randomly acquired data. We additionally introduce a bootstrap estimator of evaluation error, which we show to be a useful indicator of how well active testing is working within a single run.",
      "arxiv_url": "https://openreview.net/forum?id=UE0cxjNnIw",
      "pdf_url": "https://openreview.net/pdf/0e4fdfd137ed152835e93d1c72c71b4217f5dfa0.pdf",
      "primary_category": "active testing, active evaluation, model evaluation",
      "categories": [
        "active testing",
        "active evaluation",
        "model evaluation",
        "data curation",
        "large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xpY3C8HxNh",
      "title": "Escaping Collapse: The Strength of Weak Data for Large Language Model Training",
      "authors": [
        "Kareem Amin",
        "Sara Babakniya",
        "Alex Bie",
        "Weiwei Kong",
        "Umar Syed",
        "Sergei Vassilvitskii"
      ],
      "abstract": "Synthetically-generated data plays an increasingly larger role in training large language models. However, while synthetic data has been found to be useful, studies have also shown that without proper curation it can cause LLM performance to plateau, or even \"collapse\", after many training iterations. In this paper, we formalize this question and develop a theoretical framework to investigate how much curation is needed in order to ensure that LLM performance continually improves. Our analysis is inspired by boosting, a classic machine learning technique that leverages a very weak learning algorithm to produce an arbitrarily good classifier. The approach we analyze subsumes many recently proposed methods for training LLMs on synthetic data, and thus our analysis sheds light on why they are successful, and also suggests opportunities for future improvement. We present experiments that validate our theory, and show that dynamically focusing labeling resources on the most challenging examples --- in much the same way that boosting focuses the efforts of the weak learner --- leads to improved performance.",
      "arxiv_url": "https://openreview.net/forum?id=xpY3C8HxNh",
      "pdf_url": "https://openreview.net/pdf/87b7b2555554bc1fda223a1e4acaec6e976ce71b.pdf",
      "primary_category": "Large-language models, synthetic data",
      "categories": [
        "Large-language models",
        "synthetic data"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "NHz3BlszTR",
      "title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO",
      "authors": [
        "Yicheng Xiao",
        "Lin Song",
        "Yukang Chen",
        "Yingmin Luo",
        "Yuxin Chen",
        "Yukang Gan",
        "Wei Huang",
        "Xiu Li",
        "XIAOJUAN QI",
        "Ying Shan"
      ],
      "abstract": "Recent text-to-image systems face limitations in handling multimodal inputs and complex reasoning tasks.\nWe introduce MindOmni, a unified multimodal large language model that addresses these challenges by incorporating reasoning generation through reinforcement learning. MindOmni leverages a three-phase training strategy: i) design of a unified vision language model with a decoder-only diffusion module, ii) supervised fine-tuning with Chain-of-Thought (CoT) instruction data, and iii) our proposed Reasoning Generation Policy Optimization (RGPO) algorithm, utilizing multimodal feedback to effectively guide policy updates.\nExperimental results demonstrate that MindOmni outperforms existing models, achieving impressive performance on both understanding and generation benchmarks, meanwhile showcasing advanced fine-grained reasoning generation capabilities,  especially with mathematical reasoning instruction. All codes will be made public.",
      "arxiv_url": "https://openreview.net/forum?id=NHz3BlszTR",
      "pdf_url": "https://openreview.net/pdf/5fa00a0d90f0acb707914173f890aa13a87a5ee3.pdf",
      "primary_category": "Unified MLLM, Reasoning Generation, Reinforcement Learning",
      "categories": [
        "Unified MLLM",
        "Reasoning Generation",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pV17ra3AxZ",
      "title": "LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing",
      "authors": [
        "Ruijie ZHANG",
        "Ziyue Liu",
        "Zhengyang Wang",
        "Zheng Zhang"
      ],
      "abstract": "Training foundation models such as ViTs and LLMs requires tremendous computing cost. Low-rank matrix or tensor factorization offers a parameter-efficient alternative, but often downgrades performance due to the restricted parameter space. In this work, we introduce ${\\textbf{Latent Crossing (LaX)}}$ -- a simple yet effective plug-and-play module that enhances the capacity of low-rank models by enabling information flow across low-rank subspaces. We extensively validate the benefits of LaX on pre-training tasks with ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters. LaX boosts low-rank model performance to match or exceed the full-rank baselines while using 2-3$\\times$ fewer parameters. When equipped with low-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently improves performance on arithmetic and common sense reasoning tasks with negligible cost.",
      "arxiv_url": "https://openreview.net/forum?id=pV17ra3AxZ",
      "pdf_url": "https://openreview.net/pdf/bc7975f35a0438309c229aee31271c33197465da.pdf",
      "primary_category": "Efficient Pre-training, Efficient Fine-tuning, Adapters",
      "categories": [
        "Efficient Pre-training",
        "Efficient Fine-tuning",
        "Adapters",
        "Low-rank Training",
        "Vision Transformers",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1KTnLdrBR2",
      "title": "Concept Incongruence: An Exploration of Time and Death in Role Playing",
      "authors": [
        "Xiaoyan Bai",
        "Ike Peng",
        "Aditya Singh",
        "Chenhao Tan"
      ],
      "abstract": "Consider this prompt \"Draw a unicorn with two horns\". Should large language models (LLMs) recognize that a unicorn has only one horn by definition and ask users for clarifications, or proceed to generate something anyway? We introduce *concept incongruence* to capture such phenomena where concept boundaries clash with each other, either in user prompts or in model representations, often leading to under-specified or mis-specified behaviors. In this work, we take the first step towards defining and analyzing model behavior under concept incongruence. Focusing on temporal boundaries in the Role-Play setting, we propose three behavioral metrics---abstention rate, conditional accuracy, and answer rate---to quantify model behavior under incongruence due to the role's death. We show that models fail to abstain after death and suffer from an accuracy drop compared to the Non-Role-Play setting. Through probing experiments, we identify two main causes: (i) unreliable encoding of the \"death\" state across different years, leading to unsatisfactory abstention behavior, and (ii) role playing causes shifts in the model’s temporal representations, resulting in accuracy drops. We leverage these insights to improve consistency in the model's abstention and answer behaviors. Our findings suggest that concept incongruence leads to unexpected model behaviors and point to future directions on improving model behavior under concept incongruence.",
      "arxiv_url": "https://openreview.net/forum?id=1KTnLdrBR2",
      "pdf_url": "https://openreview.net/pdf/f54b5d2e244f30b828fbc91785249039f259f758.pdf",
      "primary_category": "concept incongruence, temporal representations",
      "categories": [
        "concept incongruence",
        "temporal representations"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "A2pmvkqOgp",
      "title": "Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation",
      "authors": [
        "Cécile Rousseau",
        "Tobia Boschi",
        "Giandomenico Cornacchia",
        "Dhaval Salwala",
        "Alessandra Pascale",
        "Juan Bernabe Moreno"
      ],
      "abstract": "SDForger is a flexible and efficient framework for generating high-quality multivariate time series using LLMs.\nLeveraging a compact data representation, SDForger provides synthetic time series generation from a few samples and low-computation fine-tuning of any autoregressive LLM. Specifically, the framework transforms univariate and multivariate signals into tabular embeddings, which are then encoded into text and used to fine-tune the LLM.\nAt inference, new textual embeddings are sampled and decoded into synthetic time series that retain the original data's statistical properties and temporal dynamics. Across a diverse range of datasets, SDForger outperforms existing generative models in many scenarios, both in similarity-based evaluations and downstream forecasting tasks. By enabling textual conditioning in the generation process, SDForger paves the way for multimodal modeling and the streamlined integration of time series with textual information. The model is open-sourced at https://github.com/IBM/fms-dgt/tree/main/fms_dgt/public/databuilders/time_series.",
      "arxiv_url": "https://openreview.net/forum?id=A2pmvkqOgp",
      "pdf_url": "https://openreview.net/pdf/2e07288e13fe84c7676bb493f7db6d8eaaf41eb0.pdf",
      "primary_category": "Time Series, Synthetic Data Generation, LLMs",
      "categories": [
        "Time Series",
        "Synthetic Data Generation",
        "LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4exx1hUffq",
      "title": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test",
      "authors": [
        "Yuhui Li",
        "Fangyun Wei",
        "Chao Zhang",
        "Hongyang Zhang"
      ],
      "abstract": "The sequential nature of modern LLMs makes them expensive and slow, and speculative sam- pling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top- layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE’s feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE- 3 achieves a 1.38x throughput improvement at a batch size of 64.",
      "arxiv_url": "https://openreview.net/forum?id=4exx1hUffq",
      "pdf_url": "https://openreview.net/pdf/0fe7af5ec1ec69cb936b30db6ec01cdec6fc5d81.pdf",
      "primary_category": "Speculative decoding",
      "categories": [
        "Speculative decoding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "O1abxStFcy",
      "title": "Exact Expressive Power of Transformers with Padding",
      "authors": [
        "William Merrill",
        "Ashish Sabharwal"
      ],
      "abstract": "Chain of thought is a natural inference-time method for increasing the computational power of transformer-based large language models (LLMs), but comes at the cost of sequential decoding. Are there more efficient alternatives to expand a transformer's expressive power without adding parameters? We consider transformers with *padding* tokens as a form of parallelizable test-time compute. We show that averaging-hard-attention, masked-pre-norm transformers with polynomial padding recognize precisely the class $\\mathsf{FO}$-uniform $\\mathsf{TC}^0$ of extremely parallelizable problems. While the $\\mathsf{TC}^0$ upper bound was known, proving a matching lower bound had been elusive. Further, our novel analysis reveals the precise expanded power of padded transformers when coupled with another form of inference-time compute, namely dynamically increasing depth via *looping*. Our core technical contribution is to show how padding helps bring the notions of *complete problems* and *reductions*, which have been a cornerstone of classical complexity theory, to the formal study of transformers. Armed with this new tool, we prove that padded transformers with $\\mathrm{O}(\\log^d n)$ looping on inputs of length $n$ recognize exactly the class $\\mathsf{FO}$-uniform $\\mathsf{TC}^d$ of moderately parallelizable problems. Thus, padding and looping together systematically expand transformers' expressive power: with polylogarithmic looping, polynomially padded transformers recognize precisely the class $\\mathsf{FO}$-uniform $\\mathsf{NC}$, the best that could be expected without losing parallelism (unless $\\mathsf{NC} = \\mathsf{P}$). Our results thus motivate further exploration of padding and looping as parallelizable alternatives to chain of thought for test-time compute.",
      "arxiv_url": "https://openreview.net/forum?id=O1abxStFcy",
      "pdf_url": "https://openreview.net/pdf/13cb36bcaa4c024c1ab9b544bc2ce0fe826d4e9d.pdf",
      "primary_category": "transformers, padding tokens, looped transformers",
      "categories": [
        "transformers",
        "padding tokens",
        "looped transformers",
        "expressivity",
        "circuit complexity",
        "inference-time compute"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "psfk761b6H",
      "title": "Thinker: Learning to Think Fast and Slow",
      "authors": [
        "Stephen Chung",
        "Wenyu Du",
        "Jie Fu"
      ],
      "abstract": "Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 25.6% to 27.3% for Qwen2.5-1.5B, and from 45.9% to 51.0% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 25.2% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training. Additionally, we have open-sourced both the trained models and the source code.",
      "arxiv_url": "https://openreview.net/forum?id=psfk761b6H",
      "pdf_url": "https://openreview.net/pdf/f23194e92e9932bef9aa81c91ed4d6aa1d835472.pdf",
      "primary_category": "Reinforcement Learning, Reasoning, Large Language Models",
      "categories": [
        "Reinforcement Learning",
        "Reasoning",
        "Large Language Models",
        "Dual Process Theory"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "NG7kM4wxaN",
      "title": "MixAT: Combining Continuous and Discrete Adversarial Training for LLMs",
      "authors": [
        "Csaba Dékány",
        "Stefan Balauca",
        "Dimitar Iliev Dimitrov",
        "Robin Staab",
        "Martin Vechev"
      ],
      "abstract": "Despite recent efforts in Large Language Model (LLM) safety and alignment, current adversarial attacks on frontier LLMs can still consistently force harmful generations. \nAlthough adversarial training has been widely studied and shown to significantly improve the robustness of traditional machine learning models, its strengths and weaknesses in the context of LLMs are less understood. \n Specifically, while existing discrete adversarial attacks are effective at producing harmful content, training LLMs with concrete adversarial prompts is often computationally expensive, leading to reliance on continuous relaxations. At the same time, despite their effectiveness and generalization capabilities, training with continuous perturbations does not always capture the full spectrum of vulnerabilities exploited by discrete attacks. In this work, we aim to bridge this gap by introducing MIXAT, a novel method that combines stronger discrete and faster continuous attacks during training. \n We rigorously evaluate MIXAT across a wide spectrum of state-of-the-art attacks, proposing the *At Least One Attack Success Rate* (ALO-ASR) metric to capture the worst-case vulnerability of models. \n We show MIXAT achieves substantially better robustness (ALO-ASR $< 20\\%$) compared to prior defenses (ALO-ASR $> 50\\%$), while maintaining a runtime comparable to methods based on continuous relaxations. \n We further analyze MIXAT in realistic deployment settings, exploring how chat templates, quantization, low-rank adapters, and temperature affect both adversarial training and evaluation, revealing additional blind spots in current methodologies. \nOur results demonstrate that MIXAT discrete-continuous defense offers a principled and superior robustness-accuracy tradeoff with minimal computational overhead, highlighting its promise for building safer LLMs. We provide our code and models at https://github.com/insait-institute/MixAT.",
      "arxiv_url": "https://openreview.net/forum?id=NG7kM4wxaN",
      "pdf_url": "https://openreview.net/pdf/912fee2be52f0ed3af1bb5bbea228cf3b4a8739c.pdf",
      "primary_category": "LLM, Large Language Models, Adversarial Robustness",
      "categories": [
        "LLM",
        "Large Language Models",
        "Adversarial Robustness",
        "Jailbreak Attacks",
        "Adversarial Training",
        "Adversarial Examples"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "AQsko3PPUe",
      "title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains",
      "authors": [
        "Wenhui Tan",
        "Jiaze Li",
        "Jianzhong Ju",
        "Zhenbo Luo",
        "Ruihua Song",
        "Jian Luan"
      ],
      "abstract": "Large Language Models (LLMs) achieve superior performance through Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are computationally expensive and inefficient.\nIn this paper, we introduce Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach.\nFirst, during supervised fine-tuning, CoLaR extends beyond next-token prediction by incorporating an auxiliary next compressed embedding prediction objective. This process merges embeddings of consecutive tokens using a compression factor $c$ randomly sampled from a predefined range, and trains a specialized latent head to predict distributions of subsequent compressed embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that leverages the latent head's non-deterministic nature to explore diverse reasoning paths and exploit more compact ones.\nThis approach enables CoLaR to: i) **perform reasoning at a dense latent level** (i.e., silently), substantially reducing reasoning chain length, and ii) **dynamically adjust reasoning speed** at inference time by simply prompting the desired compression factor.\nExtensive experiments across four mathematical reasoning datasets demonstrate that CoLaR achieves 14.1% higher accuracy than latent-based baseline methods at comparable compression ratios, and reduces reasoning chain length by 53.3% with only 4.8% performance degradation compared to explicit CoT method. Moreover, when applied to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR demonstrates performance gains of up to 5.4% while dramatically reducing latent reasoning chain length by 82.8%.\nThe code and models will be released upon acceptance.",
      "arxiv_url": "https://openreview.net/forum?id=AQsko3PPUe",
      "pdf_url": "https://openreview.net/pdf/9a1c1a85e08c73fd0ae736b069afe387fbe0fb59.pdf",
      "primary_category": "Large Language Models, LLM Reasoning",
      "categories": [
        "Large Language Models",
        "LLM Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4xvN7uOKZt",
      "title": "Incentivizing Truthful Language Models via Peer Elicitation Games",
      "authors": [
        "Baiting Chen",
        "Tong Zhu",
        "Jiale Han",
        "Lexin Li",
        "Gang Li",
        "Xiaowu Dai"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong generative capabilities but remain prone to inconsistencies and hallucinations. We introduce Peer Elicitation Games (PEG), a training-free, game-theoretic framework for aligning LLMs through a peer elicitation mechanism involving a generator and multiple discriminators instantiated from distinct base models. Discriminators interact in a peer evaluation setting, where utilities are computed using a determinant-based mutual information score that provably incentivizes truthful reporting without requiring ground-truth labels. We establish theoretical guarantees showing that each agent, via online learning, achieves sublinear regret in the sense their cumulative performance approaches that of the best fixed truthful strategy in hindsight. Moreover, we prove last-iterate convergence to a truthful Nash equilibrium, ensuring that the actual policies used by agents converge to stable and truthful behavior over time. Empirical evaluations across multiple benchmarks demonstrate significant improvements in factual accuracy. These results position PEG as a practical approach for eliciting truthful behavior from LLMs without supervision or fine-tuning.",
      "arxiv_url": "https://openreview.net/forum?id=4xvN7uOKZt",
      "pdf_url": "https://openreview.net/pdf/2c907560755a42fb626d153a4643f69b992c881a.pdf",
      "primary_category": "Large language models, Mechanism design, Nash equilibrium",
      "categories": [
        "Large language models",
        "Mechanism design",
        "Nash equilibrium",
        "Peer prediction"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ooiHIklvN5",
      "title": "Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs",
      "authors": [
        "Zhangyin Feng",
        "Qianglong Chen",
        "Ning Lu",
        "Yongqian Li",
        "Siqi Cheng",
        "Shuangmu Peng",
        "Duyu Tang",
        "Shengcai Liu",
        "Zhirui Zhang"
      ],
      "abstract": "The development of reasoning capabilities represents a critical frontier in large language models (LLMs) research, where reinforcement learning (RL) and process reward models (PRMs) have emerged as predominant methodological frameworks. Contrary to conventional wisdom, empirical evidence from DeepSeek-R1 demonstrates that pure RL training focused on mathematical problem-solving can progressively enhance reasoning abilities without PRM integration, challenging the perceived necessity of process supervision.\nIn this study, we conduct a systematic investigation of the relationship between RL training and PRM capabilities. Our findings demonstrate that problem-solving proficiency and process supervision capabilities represent complementary dimensions of reasoning that co-evolve synergistically during pure RL training. In particular, current PRMs underperform simple baselines like majority voting when applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To address this limitation, we propose Self-PRM, an introspective framework in which models autonomously evaluate and rerank their generated solutions through self-reward mechanisms. Although Self-PRM consistently improves the accuracy of the benchmark (particularly with larger sample sizes), analysis exposes persistent challenges: The approach exhibits low precision (<10\\%) on difficult problems, frequently misclassifying flawed solutions as valid. These analyses underscore the need for combined training with process supervision and continued RL scaling to enhance reward alignment and introspective accuracy. We hope these findings provide actionable insights for building more reliable and self-aware complex reasoning models.",
      "arxiv_url": "https://openreview.net/forum?id=ooiHIklvN5",
      "pdf_url": "https://openreview.net/pdf/e7ed5c9a866ddc4710624ed2df37d91bce6455d3.pdf",
      "primary_category": "PRM, LLM, RL",
      "categories": [
        "PRM",
        "LLM",
        "RL",
        "Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "R2ZJSjLDJC",
      "title": "Less is More: Improving LLM Alignment via Preference Data Selection",
      "authors": [
        "Xun Deng",
        "Han Zhong",
        "Rui Ai",
        "Fuli Feng",
        "Zheng Wang",
        "Xiangnan He"
      ],
      "abstract": "Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences. While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from the largely overlooked but critical aspect of data selection. Specifically, we address the issue of parameter shrinkage caused by noisy data by proposing a novel margin-maximization principle for dataset curation in DPO training. To further mitigate the noise in different reward models, we propose a Bayesian Aggregation approach that unifies multiple margin sources (external and implicit) into a single preference probability.  Extensive experiments in diverse settings demonstrate the consistently high data efficiency of our approach. Remarkably, by using just 10\\% of the Ultrafeedback dataset, our approach achieves 3\\% to 8\\% improvements across various Llama, Mistral, and Qwen models on the AlpacaEval2 benchmark.  Furthermore, our approach seamlessly extends to iterative DPO, yielding a roughly 3\\% improvement with 25\\% online data, revealing the high redundancy in this presumed high-quality data construction manner. These results highlight the potential of data selection strategies for advancing preference optimization.",
      "arxiv_url": "https://openreview.net/forum?id=R2ZJSjLDJC",
      "pdf_url": "https://openreview.net/pdf/25b5723d978db683d8fcf5771f228fa4d71571b7.pdf",
      "primary_category": "data selection, DPO, alignment",
      "categories": [
        "data selection",
        "DPO",
        "alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4QVLKwgg3S",
      "title": "SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs",
      "authors": [
        "Jinwoo Park",
        "Seunggeun Cho",
        "Dongsu Han"
      ],
      "abstract": "Large language models (LLMs) power many modern applications, but serving them at scale remains costly and resource-intensive. Current server-centric systems overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an edge-assisted inference framework that splits LLM workloads between edge and server GPUs using a speculative decoding scheme, exchanging only token outputs over the network. SpecEdge employs proactive edge drafting to overlap edge token creation with server verification and pipeline-aware scheduling that interleaves multiple user requests to increase server-side throughput. Experiments show SpecEdge enhances overall cost efficiency by **1.91×** through achieving **2.22×** server throughput, and reduces inter token latency by **11.24\\%** compared to a server-only baseline, introducing a scalable, cost-effective paradigm for LLM serving. The code is available at https://github.com/kaist-ina/specedge",
      "arxiv_url": "https://openreview.net/forum?id=4QVLKwgg3S",
      "pdf_url": "https://openreview.net/pdf/459a4e18b8ff1185a575fab6045c65a522b34c45.pdf",
      "primary_category": "Machine Learning Systems, LLM Serving, Split Computing",
      "categories": [
        "Machine Learning Systems",
        "LLM Serving",
        "Split Computing",
        "Speculative Decoding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "onyzhnhApp",
      "title": "SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs",
      "authors": [
        "LiuRuyue",
        "Rong Yin",
        "Xiangzhen Bo",
        "Xiaoshuai Hao",
        "Yong Liu",
        "Jinwen Zhong",
        "Can Ma",
        "Weiping Wang"
      ],
      "abstract": "Large-scale pre-trained models have revolutionized Natural Language Processing (NLP) and Computer Vision (CV), showcasing remarkable cross-domain generalization abilities. However, in graph learning, models are typically trained on individual graph datasets, limiting their capacity to transfer knowledge across different graphs and tasks. This approach also heavily relies on large volumes of annotated data, which presents a significant challenge in resource-constrained settings. Unlike NLP and CV, graph-structured data presents unique challenges due to its inherent heterogeneity, including domain-specific feature spaces and structural diversity across various applications. To address these challenges, we propose a novel structure-aware self-supervised learning method for Text-Attributed Graphs (SSTAG). By leveraging text as a unified representation medium for graph learning, SSTAG bridges the gap between the semantic reasoning of Large Language Models (LLMs) and the structural modeling capabilities of Graph Neural Networks (GNNs). Our approach introduces a dual knowledge distillation framework that co-distills both LLMs and GNNs into structure-aware multilayer perceptrons (MLPs), enhancing the scalability of large-scale TAGs. Additionally, we introduce an in-memory mechanism that stores typical graph representations, aligning them with memory anchors in an in-memory repository to integrate invariant knowledge, thereby improving the model’s generalization ability. Extensive experiments demonstrate that SSTAG outperforms state-of-the-art models on cross-domain transfer learning tasks, achieves exceptional scalability, and reduces inference costs while maintaining competitive performance.",
      "arxiv_url": "https://openreview.net/forum?id=onyzhnhApp",
      "pdf_url": "https://openreview.net/pdf/e78cee7b5466893dcf8c114f024ab4b72308b70f.pdf",
      "primary_category": "Self-supervised Learning, Text-Attributed Graphs",
      "categories": [
        "Self-supervised Learning",
        "Text-Attributed Graphs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "aPPnmuuNhx",
      "title": "Universal Visuo-Tactile Video Understanding for Embodied Interaction",
      "authors": [
        "Yifan Xie",
        "Mingyang Li",
        "Shoujie Li",
        "Xingting Li",
        "Guangyu Chen",
        "Fei Ma",
        "Fei Yu",
        "Wenbo Ding"
      ],
      "abstract": "Tactile perception is essential for embodied agents to understand the physical attributes of objects that cannot be determined through visual inspection alone. While existing methods have made progress in visual and language modalities for physical understanding, they fail to effectively incorporate tactile information that provides crucial haptic feedback for real-world interaction. In this paper, we present VTV-LLM, the first multi-modal large language model that enables universal Visuo-Tactile Video (VTV) understanding, bridging the gap between tactile perception and natural language. To address the challenges of cross-sensor and cross-modal integration, we contribute VTV150K, a comprehensive dataset comprising 150,000 video frames from 100 diverse objects captured across three different tactile sensors (GelSight Mini, DIGIT, and Tac3D), annotated with four fundamental tactile attributes (hardness, protrusion, elasticity, and friction). We develop a novel three-stage training paradigm that includes VTV enhancement for robust visuo-tactile representation, VTV-text alignment for cross-modal correspondence, and text prompt finetuning for natural language generation. Our framework enables sophisticated tactile reasoning capabilities including feature assessment, comparative analysis, and scenario-based decision-making. Extensive experimental evaluations demonstrate that VTV-LLM achieves superior performance in tactile reasoning tasks, establishing a foundation for more intuitive human-machine interaction in tactile domains.",
      "arxiv_url": "https://openreview.net/forum?id=aPPnmuuNhx",
      "pdf_url": "https://openreview.net/pdf/c2694bb8f335ccb203151131402b0a74aa6de7bf.pdf",
      "primary_category": "Embodied Interaction, Tactile Perception, Video Understanding",
      "categories": [
        "Embodied Interaction",
        "Tactile Perception",
        "Video Understanding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "WRwr2YZ4zt",
      "title": "TimeXL: Explainable Multi-modal Time Series Prediction with LLM-in-the-Loop",
      "authors": [
        "Yushan Jiang",
        "Wenchao Yu",
        "Geon Lee",
        "Dongjin Song",
        "Kijung Shin",
        "Wei Cheng",
        "Yanchi Liu",
        "Haifeng Chen"
      ],
      "abstract": "Time series analysis provides essential insights for real-world system dynamics and informs downstream decision-making, yet most existing methods often overlook the rich contextual signals present in auxiliary modalities. To bridge this gap, we introduce TimeXL, a multi-modal prediction framework that integrates a prototype-based time series encoder with three collaborating Large Language Models (LLMs) to deliver more accurate predictions and interpretable explanations. First, a multi-modal prototype-based encoder processes both time series and textual inputs to generate preliminary forecasts alongside case-based rationales. These outputs then feed into a prediction LLM, which refines the forecasts by reasoning over the encoder's predictions and explanations. Next, a reflection LLM compares the predicted values against the ground truth, identifying textual inconsistencies or noise. Guided by this feedback, a refinement LLM iteratively enhances text quality and triggers encoder retraining. This closed-loop workflow---prediction, critique (reflect), and refinement---continuously boosts the framework's performance and interpretability. Empirical evaluations on four real-world datasets demonstrate that TimeXL achieves up to 8.9\\% improvement in AUC and produces human-centric, multi-modal explanations, highlighting the power of LLM-driven reasoning for time series prediction.",
      "arxiv_url": "https://openreview.net/forum?id=WRwr2YZ4zt",
      "pdf_url": "https://openreview.net/pdf/ef39950258039d699cf6f08edbba6c51d1b14182.pdf",
      "primary_category": "multi-modal time series prediction, explanations, prototypes",
      "categories": [
        "multi-modal time series prediction",
        "explanations",
        "prototypes",
        "large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7AMriz7I3K",
      "title": "BeliefMapNav: 3D Voxel-Based Belief Map for Zero-Shot Object Navigation",
      "authors": [
        "Zibo Zhou",
        "Yue Hu",
        "Lingkai Zhang",
        "Zonglin Li",
        "Siheng Chen"
      ],
      "abstract": "Zero-shot object navigation (ZSON) allows robots to find target objects in unfamiliar environments using natural language instructions, without relying on pre-built maps or task-specific training. Recent general-purpose models, such as large language models (LLMs) and vision-language models (VLMs), equip agents with semantic reasoning abilities to estimate target object locations in a zero-shot manner. However, these models often greedily select the next goal without maintaining a global understanding of the environment and are fundamentally limited in the spatial reasoning necessary for effective navigation. To overcome these limitations, we propose a novel 3D voxel-based belief map that estimates the target’s prior presence distribution within a voxelized 3D space. This approach enables agents to integrate semantic priors from LLMs and visual embeddings with hierarchical spatial structure, alongside real-time observations, to build a comprehensive 3D global posterior belief of the target’s location. Building on this 3D voxel map, we introduce BeliefMapNav, an efficient navigation system with two key advantages: i) grounding LLM semantic reasoning within the 3D hierarchical semantics voxel space for precise target position estimation, and ii) integrating sequential path planning to enable efficient global navigation decisions. Experiments on HM3D and HSSD benchmarks show that BeliefMapNav achieves state-of-the-art (SOTA) Success Rate (SR) and Success weighted by Path Length (SPL), with a notable 9.7 SPL improvement over the previous best SR method, validating its effectiveness and efficiency.",
      "arxiv_url": "https://openreview.net/forum?id=7AMriz7I3K",
      "pdf_url": "https://openreview.net/pdf/0723f863304fd597c9e6e38242914d49584b6776.pdf",
      "primary_category": "zero shot object Navigation, belief map, mobile agent",
      "categories": [
        "zero shot object Navigation",
        "belief map",
        "mobile agent"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vqLoLqUUNB",
      "title": "Knowledge Starts with Practice: Knowledge-Aware Exercise Generative Recommendation with Adaptive Multi-Agent Cooperation",
      "authors": [
        "Yangtao Zhou",
        "Hua Chu",
        "Yongxiang Chen",
        "Ziwen Wang",
        "Jiacheng Liu",
        "Jianan Li",
        "Yueying Feng",
        "Xiangming Li",
        "Zihan Han",
        "Qingshan Li"
      ],
      "abstract": "Adaptive learning, which requires the in-depth understanding of students' learning processes and rational planning of learning resources, plays a crucial role in intelligent education. However, how to effectively model these two processes and seamlessly integrate them poses significant implementation challenges for adaptive learning. As core learning resources, exercises have the potential to diagnose students' knowledge states during the learning processes and provide personalized learning recommendations to strengthen students' knowledge, thereby serving as a bridge to boost student-oriented adaptive learning. Therefore, we introduce a novel task called Knowledge-aware Exercise Generative Recommendation (KEGR). It aims to dynamically infer students' knowledge states from their past exercise responses and customizably generate new exercises. To achieve KEGR, we propose an adaptive multi-agent cooperation framework, called ExeGen, inspired by the excellent reasoning and generative capabilities of LLM-based AI agents. Specifically, ExeGen coordinates four specialized agents for supervision, knowledge state perception, exercise generation, and quality refinement through an adaptive loop workflow pipeline. More importantly, we devise two enhancement mechanisms in ExeGen: 1) A human-simulated knowledge perception mechanism mimics students' cognitive processes and generates interpretable knowledge state descriptions via demonstration-based In-Context Learning (ICL). In this mechanism, a dual-matching strategy is further designed to retrieve highly relevant demonstrations for reliable ICL reasoning. 2) An exercise generation-adversarial mechanism collaboratively refines exercise generation leveraging a group of quality evaluation expert agents via iterative adversarial feedback. Finally, a comprehensive evaluation protocol is carefully designed to assess ExeGen. Extensive experiments on real-world educational datasets and a practical deployment in college education demonstrate the effectiveness and superiority of ExeGen. The code is available at https://github.com/dsz532/exeGen.",
      "arxiv_url": "https://openreview.net/forum?id=vqLoLqUUNB",
      "pdf_url": "https://openreview.net/pdf/1adfc528d571320329046af8a43c2ab665336ec5.pdf",
      "primary_category": "Exercise Generative Recommendation, Knowledge State Perception, Personalized Learning",
      "categories": [
        "Exercise Generative Recommendation",
        "Knowledge State Perception",
        "Personalized Learning",
        "Multi-agent Cooperation"
      ],
      "tags": [
        "LLM",
        "Personalization",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vqaWAmuzRt",
      "title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction",
      "authors": [
        "Hsi-Che Lin",
        "Yu-Chu Yu",
        "Kai-Po Chang",
        "Yu-Chiang Frank Wang"
      ],
      "abstract": "Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model, which originally required 95GB of memory, on a single 24GB consumer GPU—bringing efficient and practical model adaptation to individual users.",
      "arxiv_url": "https://openreview.net/forum?id=vqaWAmuzRt",
      "pdf_url": "https://openreview.net/pdf/48f5c3688bd6d8b15b10a4cc92a53dd7357badfc.pdf",
      "primary_category": "Memory-efficient fine-tuning, Low-rank approximation",
      "categories": [
        "Memory-efficient fine-tuning",
        "Low-rank approximation"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yEq201U9AM",
      "title": "Policy Optimized Text-to-Image Pipeline Design",
      "authors": [
        "Uri Gadot",
        "Rinon Gal",
        "Yftah Ziser",
        "Gal Chechik",
        "Shie Mannor"
      ],
      "abstract": "Text-to-image generation has evolved beyond single monolithic models to complex multi-component pipelines that combine various enhancement tools. While these pipelines significantly improve image quality, their effective design requires substantial expertise. Recent approaches automating this process through large language models (LLMs) have shown promise but suffer from two critical limitations: extensive computational requirements from generating images with hundreds of predefined pipelines, and poor generalization beyond memorized training examples.\n    We introduce a novel reinforcement learning-based framework that addresses these inefficiencies. Our approach first trains an ensemble of reward models capable of predicting image quality scores directly from prompt-workflow combinations, eliminating the need for costly image generation during training. We then implement a two-phase training strategy: initial workflow prediction training followed by GRPO-based optimization that guides the model toward higher-performing regions of the workflow space. Additionally, we incorporate a classifier-free guidance based enhancement technique that extrapolates along the path between the initial and GRPO-tuned models, further improving output quality.\n    We validate our approach through a set of comparisons, showing that it can successfully create new flows with greater diversity and lead to superior image quality compared to existing baselines.",
      "arxiv_url": "https://openreview.net/forum?id=yEq201U9AM",
      "pdf_url": "https://openreview.net/pdf/95c598b69de1149c8e8b6bf1ed5f4f286df6d69c.pdf",
      "primary_category": "ComfyUI; Text-to-Image workflow design; Text-to-Image generation",
      "categories": [
        "ComfyUI; Text-to-Image workflow design; Text-to-Image generation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "px1OlCcqkj",
      "title": "Strategic Hypothesis Testing",
      "authors": [
        "Yatong Chen",
        "Safwan Hossain",
        "Yiling Chen"
      ],
      "abstract": "We examine hypothesis testing within a principal-agent framework, where a strategic agent, holding private beliefs about the effectiveness of a product, submits data to a principal who decides on approval. The principal employs a hypothesis testing rule, aiming to pick a p-value threshold that balances false positives and false negatives while anticipating the agent’s incentive to maximize expected profitability. Building on prior work, we develop a game-theoretic model that captures how the agent’s participation and reporting behavior respond to the principal’s statistical decision rule. Despite the complexity of the interaction, we show that the principal's errors exhibit clear monotonic behavior when segmented by an efficiently computable critical p-value threshold, leading to an interpretable characterization of their optimal p-value threshold. We empirically validate our model and these insights using publicly available data on drug approvals. Overall, our work offers a comprehensive perspective on strategic interactions within the hypothesis testing framework, providing technical and regulatory insights.",
      "arxiv_url": "https://openreview.net/forum?id=px1OlCcqkj",
      "pdf_url": "https://openreview.net/pdf/c38d706a3c20b0dac8563d5978d068f33b8dae0c.pdf",
      "primary_category": "Stackelberg Game; Hypothesis Testing; Strategic Machine Learning; Game Theory",
      "categories": [
        "Stackelberg Game; Hypothesis Testing; Strategic Machine Learning; Game Theory"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "KUHrL5NYHe",
      "title": "SEGA: Shaping Semantic Geometry for Robust Hashing under Noisy Supervision",
      "authors": [
        "Yiyang Gu",
        "Bohan Wu",
        "Qinghua Ran",
        "Rong-Cheng Tu",
        "Xiao Luo",
        "Zhiping Xiao",
        "Wei Ju",
        "Dacheng Tao",
        "Ming Zhang"
      ],
      "abstract": "This paper studies the problem of learning hash codes from noisy supervision, which is a practical yet challenging task. This problem is important in extensive real-world applications such as image retrieval and cross-modal retrieval. However, most of the existing methods focus on label denoising to address this problem, but ignore the geometric structure of the hash space, which is critical for learning stable hash codes. Towards this end, this paper proposes a novel framework named Semantic Geometry Shaping (SEGA) that explicitly refines the semantic geometry of hash space. Specifically, we first learn dynamic class prototypes as semantic anchors and cluster hash embeddings around these prototypes to keep structural stability. We then leverage both the energy of predicted distributions and structure-based divergence to estimate the uncertainty of instances and calibrate the supervision in a soft manner. Moreover, we introduce structure-aware interpolation to improve the class boundaries. To verify the effectiveness of our design, we give the theoretical analysis for the proposed framework. Experiments on a range of widely-used retrieval datasets justify the superiority of our SEGA over extensive strong baselines under noisy supervision.",
      "arxiv_url": "https://openreview.net/forum?id=KUHrL5NYHe",
      "pdf_url": "https://openreview.net/pdf/dba71e2eee70475af688a2dc84e7f4005a621f08.pdf",
      "primary_category": "Robust Hashing, Semantic Geometry, Noisy Supervision",
      "categories": [
        "Robust Hashing",
        "Semantic Geometry",
        "Noisy Supervision"
      ],
      "tags": [
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "TJhHb6CscW",
      "title": "Progress Reward Model for Reinforcement Learning via Large Language Models",
      "authors": [
        "Xiuhui Zhang",
        "Ning Gao",
        "Xingyu Jiang",
        "Yihui Chen",
        "Yuheng Pan",
        "Mohan Zhang",
        "Yue Deng"
      ],
      "abstract": "Traditional reinforcement learning (RL) algorithms face significant limitations in handling long-term tasks with sparse rewards. \nRecent advancements have leveraged large language models (LLMs) to enhance RL by utilizing their world knowledge for task planning and reward generation. \nHowever, planning-based approaches often depend on pre-defined skill libraries and fail to optimize low-level control policies, while reward-based methods require extensive human feedback or exhaustive searching due to the complexity of tasks.\nIn this paper, we propose the Progress Reward Model for RL (PRM4RL), a novel framework that integrates task planning and dense reward to enhance RL. \nFor high-level planning, a complex task is decomposed into a series of simple manageable subtasks, with a subtask-oriented, fine-grained progress function designed to monitor task execution progress. \nFor low-level reward generation, inspired by potential-based reward shaping, we use the progress function to construct a Progress Reward Model (PRM), providing theoretically grounded optimality and convergence guarantees, thereby enabling effective policy optimization.\nExperimental results on robotics control tasks demonstrate that our approach outperforms both LLM-based planning and reward methods, achieving state-of-the-art performance.",
      "arxiv_url": "https://openreview.net/forum?id=TJhHb6CscW",
      "pdf_url": "https://openreview.net/pdf/b3d0347ee14e84dbfde1bc8ebfdc701c8d1dd0bb.pdf",
      "primary_category": "Reinforcement Learning, Large Language Models, Reward Design",
      "categories": [
        "Reinforcement Learning",
        "Large Language Models",
        "Reward Design"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "EdP45Yxdc3",
      "title": "Orthogonal Survival Learners for Estimating Heterogeneous Treatment Effects from Time-to-Event Data",
      "authors": [
        "Dennis Frauen",
        "Maresa Schröder",
        "Konstantin Hess",
        "Stefan Feuerriegel"
      ],
      "abstract": "Estimating heterogeneous treatment effects (HTEs) is crucial for personalized decision-making. However, this task is challenging in survival analysis, which includes time-to-event data with censored outcomes (e.g., due to study dropout). In this paper, we propose a toolbox of orthogonal survival learners to estimate HTEs from time-to-event data under censoring. Our learners have three main advantages: (i) we show that learners from our toolbox are guaranteed to be orthogonal and thus robust with respect to nuisance estimation errors; (ii) our toolbox allows for incorporating a custom weighting function, which can lead to robustness against different types of low overlap, and (iii) our learners are \\emph{model-agnostic} (i.e., they can be combined with arbitrary machine learning models). We instantiate the learners from our toolbox using several weighting functions and, as a result, propose various neural orthogonal survival learners. Some of these coincide with existing survival learners (including survival versions of the DR- and R-learner), while others are novel and further robust w.r.t. low overlap regimes specific to the survival setting (i.e., survival overlap and censoring overlap). We then empirically verify the effectiveness of our learners for HTE estimation in different low-overlap regimes through numerical experiments. In sum, we provide practitioners with a large toolbox of learners that can be used for randomized and observational studies with censored time-to-event data.",
      "arxiv_url": "https://openreview.net/forum?id=EdP45Yxdc3",
      "pdf_url": "https://openreview.net/pdf/0e2fad4841c0ddbb291a5d87a1917c0a01692d36.pdf",
      "primary_category": "Causal inference, treatment effect estimation, survival analysis",
      "categories": [
        "Causal inference",
        "treatment effect estimation",
        "survival analysis",
        "orthogonal learning"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vVU1KTOsju",
      "title": "Scaling Laws for Optimal Data Mixtures",
      "authors": [
        "Mustafa Shukor",
        "Louis Béthune",
        "Dan Busbridge",
        "David Grangier",
        "Enrico Fini",
        "Alaaeldin El-Nouby",
        "Pierre Ablin"
      ],
      "abstract": "Large foundation models are typically trained on data from multiple domains, with the data mixture--the proportion of each domain used--playing a critical role in model performance. The standard approach to selecting this mixture relies on trial and error, which becomes impractical for large-scale pretraining. We propose a systematic method to determine the optimal data mixture for any target domain using scaling laws. Our approach accurately predicts the loss of a model of size $N$ trained with $D$ tokens and a specific domain weight vector $h$. We validate the universality of these scaling laws by demonstrating their predictive power in three distinct and large-scale settings: large language model (LLM), native multimodal model (NMM), and large vision models (LVM) pretraining. We further show that these scaling laws can extrapolate to new data mixtures and across scales: their parameters can be accurately estimated using a few small-scale training runs, and used to estimate the performance at larger scales and unseen domain weights. The scaling laws allow to derive the optimal domain weights for any target domain under a given training budget ($N$,$D$), providing a principled alternative to costly trial-and-error methods.",
      "arxiv_url": "https://openreview.net/forum?id=vVU1KTOsju",
      "pdf_url": "https://openreview.net/pdf/0c763c984dae4c3b82805d215708d4f45f9f9814.pdf",
      "primary_category": "Scaling laws, large models, data mixture",
      "categories": [
        "Scaling laws",
        "large models",
        "data mixture"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "YBrBFn4nSM",
      "title": "One SPACE to Rule Them All: Jointly Mitigating Factuality and Faithfulness Hallucinations in LLMs",
      "authors": [
        "Pengbo Wang",
        "Chaozhuo Li",
        "Chenxu Wang",
        "Liwen Zheng",
        "Litian Zhang",
        "Xi Zhang"
      ],
      "abstract": "LLMs have demonstrated unprecedented capabilities in natural language processing, yet their practical deployment remains hindered by persistent factuality and faithfulness hallucinations. While existing methods address these hallucination types independently, they inadvertently induce performance trade-offs, as interventions targeting one type often exacerbate the other. Through empirical and theoretical analysis of activation space dynamics in LLMs, we reveal that these hallucination categories share overlapping subspaces within neural representations, presenting an opportunity for concurrent mitigation. To harness this insight, we propose SPACE, a unified framework that jointly enhances factuality and faithfulness by editing shared activation subspaces. SPACE establishes a geometric foundation for shared subspace existence through dual-task feature modeling, then identifies and edits these subspaces via a hybrid probe strategy combining spectral clustering and attention head saliency scoring. Experimental results across multiple benchmark datasets demonstrate the superiority of our approach.",
      "arxiv_url": "https://openreview.net/forum?id=YBrBFn4nSM",
      "pdf_url": "https://openreview.net/pdf/e476618d998ed24f80d4096db1e2e26574eda423.pdf",
      "primary_category": "LLM, Hallucination, Safety",
      "categories": [
        "LLM",
        "Hallucination",
        "Safety"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "AOe1aUhEQQ",
      "title": "Benford’s Curse: Tracing Digit Bias to Numerical Hallucination in LLMs",
      "authors": [
        "Jiandong Shao",
        "Yao Lu",
        "Jianfei Yang"
      ],
      "abstract": "Large Language Models (LLMs) exhibit impressive performance on complex reasoning tasks, yet they frequently fail on basic numerical problems, producing incorrect outputs. Inspired by Benford’s Law, a statistical pattern in which lower digits occur more frequently as leading digits, we hypothesize that the skewed digit distributions in web-collected corpora may be learned by LLMs during pretraining, leading to biased numerical generation. To investigate the hypothesis, we first examine whether digits frequencies in pretraining corpus (OLMo2) follows Benford's law. We then construct an evaluation benchmark in which the ground-truth digits are uniformly distributed within each of the seven numerical reasoning tasks. Our evaluation results demonstrate that leading open-source LLMs show a consistent pattern of digit bias that resembles Benford's law. Through logit-lens tracing and neuron-level dissection, we identify that this bias arises predominantly from a small subset of highly digit-selective feed-forward network (FFN) neurons in the deeper layers. Finally, we demonstrate that pruning these neurons mitigates imbalanced overgeneration and partially corrects erroneous outputs, providing causal evidence that fine-grained pretraining digit bias can propagate into model behavior. Our findings reveal a fundamental connection between corpus-level statistics and symbolic failure modes in LLMs, offering a new lens for diagnosing and mitigating hallucinations in numerical tasks.",
      "arxiv_url": "https://openreview.net/forum?id=AOe1aUhEQQ",
      "pdf_url": "https://openreview.net/pdf/cdbc08e2b31917dd9ad4d544136252c8173bc655.pdf",
      "primary_category": "Digit Bias, Benford's Law, Numerical Hallucination",
      "categories": [
        "Digit Bias",
        "Benford's Law",
        "Numerical Hallucination",
        "Pretraining Bias",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "s9NkfkUuEr",
      "title": "Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers",
      "authors": [
        "Zhengliang Shi",
        "Lingyong Yan",
        "Dawei Yin",
        "Suzan Verberne",
        "Maarten de Rijke",
        "Zhaochun Ren"
      ],
      "abstract": "Large language models (LLMs) have been widely integrated into information retrieval to advance traditional techniques. However, effectively enabling LLMs to seek accurate knowledge in complex tasks remains a challenge due to the complexity of multi-hop queries as well as the irrelevant retrieved content. To address these limitations, we propose ExSearch, an agentic search framework, where the LLM learns to retrieve useful information as the reasoning unfolds through a self-incentivized process. At each step, the LLM decides what to retrieve (thinking), triggers an external retriever (search), and extracts fine-grained evidence (recording) to support next-step reasoning.  To enable LLM with this capability, we adopts a Generalized Expectation-Maximization algorithm. In the E-step, the LLM generates multiple search trajectories and assigns an importance weight to each; the M-step trains the LLM on them with a re-weighted loss function. This creates a self-incentivized loop, where the LLM iteratively learns from its own generated data, progressively improving itself for search. We further theoretically analyze this training process, establishing convergence guarantees. Extensive experiments on four knowledge-intensive benchmarks show that ExSearchS substantially outperforms baselines, e.g., +7.8% improvement on exact match score. Motivated by these promising results, we introduce ExSearch-Zoo, an extension that extends our method to broader scenarios, to facilitate future work.",
      "arxiv_url": "https://openreview.net/forum?id=s9NkfkUuEr",
      "pdf_url": "https://openreview.net/pdf/f6244ed69adb4a0f1c375503deb1e23c39a8ac15.pdf",
      "primary_category": "information retrieval, large language model, retrieval-augmented generation",
      "categories": [
        "information retrieval",
        "large language model",
        "retrieval-augmented generation"
      ],
      "tags": [
        "LLM",
        "Information Retrieval",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "dZ94ZS410X",
      "title": "RF-Agent: Automated Reward Function Design via Language Agent Tree Search",
      "authors": [
        "Ning Gao",
        "Xiuhui Zhang",
        "Xingyu Jiang",
        "Mukang You",
        "Mohan Zhang",
        "Yue Deng"
      ],
      "abstract": "Designing efficient reward functions for low-level control tasks is a challenging problem. Recent research aims to reduce reliance on expert experience by using Large Language Models (LLMs) with task information to generate dense reward functions. These methods typically rely on training results as feedback, iteratively generating new reward functions with greedy or evolutionary algorithms. However, they suffer from poor utilization of historical feedback and inefficient search, resulting in limited improvements in complex control tasks. To address this challenge, we propose RF-Agent, a framework that treats LLMs as language agents and frames reward function design as a sequential decision-making process, enhancing optimization through better contextual reasoning. RF-Agent integrates Monte Carlo Tree Search (MCTS) to manage the reward design and optimization process, leveraging the multi-stage contextual reasoning ability of LLM. This approach better utilizes historical information and improves search efficiency to identify promising reward functions. Outstanding experimental results in 17 diverse low-level control tasks demonstrate the effectiveness of our method.",
      "arxiv_url": "https://openreview.net/forum?id=dZ94ZS410X",
      "pdf_url": "https://openreview.net/pdf/062acc1493b646a44c8215cf3fd64f0900f393e8.pdf",
      "primary_category": "Reward Design, Reinforcement Learning, Large Language Models",
      "categories": [
        "Reward Design",
        "Reinforcement Learning",
        "Large Language Models",
        "Language Agent",
        "Monte Carlo Tree Search"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ltOnam1gY7",
      "title": "VTON-VLLM: Aligning Virtual Try-On Models with Human Preferences",
      "authors": [
        "Siqi Wan",
        "Jingwen Chen",
        "Qi Cai",
        "Yingwei Pan",
        "Ting Yao",
        "Tao Mei"
      ],
      "abstract": "Diffusion models have yielded remarkable success in virtual try-on (VTON) task, yet they often fall short of fully meeting user expectations regarding visual quality and detail preservation. To alleviate this issue, we curate a dataset of synthesized VTON images annotated with human judgments across multiple perceptual criteria. A vision large language model (VLLM), namely VTON-VLLM, is then learnt on these annotations. VTON-VLLM functions as a unified ``fashion expert'' and is capable of both evaluating and steering VTON synthesis towards human preferences. Technically, beyond serving as an automatic VTON evaluator, VTON-VLLM upgrades VTON model through two pivotal ways: (1) providing fine-grained supervisory signals during the training of a plug-and-play VTON refinement model, and (2) enabling adaptive and preference-aware test-time scaling at inference. To benchmark VTON models more holistically, we introduce VITON-Bench, a challenging test suite of complex try-on scenarios, and human-preference–aware metrics. Extensive experiments demonstrate that powering VTON models with our VTON-VLLM markedly enhances alignment with human preferences. Code is publicly available at: [https://github.com/HiDream-ai/VTON-VLLM/](https://github.com/HiDream-ai/VTON-VLLM/).",
      "arxiv_url": "https://openreview.net/forum?id=ltOnam1gY7",
      "pdf_url": "https://openreview.net/pdf/4370147f13f27355fe1eddb2f490b4d17972ea3d.pdf",
      "primary_category": "virtual try-on",
      "categories": [
        "virtual try-on"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "onhjdWCxZY",
      "title": "HiFC: High-efficiency Flash-based KV Cache Swapping for Scaling LLM Inference",
      "authors": [
        "Inho Jeong",
        "Sunghyeon Woo",
        "Sol Namkung",
        "Dongsuk Jeon"
      ],
      "abstract": "Large‑language‑model inference with long contexts often produces key–value (KV) caches whose footprint exceeds the capacity of high‑bandwidth memory on a GPU. Prior LLM inference frameworks such as vLLM mitigate this pressure by swapping KV cache pages to host DRAM. However, the high cost of large DRAM pools makes this solution economically unattractive. Although offloading to SSDs can be a cost-effective way to expand memory capacity relative to DRAM, conventional frameworks such as FlexGen experience a substantial throughput drop since the data path that routes SSD traffic through CPU to GPU is severely bandwidth-constrained. To overcome these limitations, we introduce HiFC, a novel DRAM‑free swapping scheme that enables direct access to SSD-resident memory with low latency and high effective bandwidth. HiFC stores KV pages in pseudo-SLC (pSLC) regions of commodity NVMe SSDs, sustaining high throughput under sequential I/O and improving write endurance by up to 8$\\times$. Leveraging GPU Direct Storage, HiFC enables direct transfers between SSD and GPU, bypassing host DRAM and alleviating PCIe bottlenecks. HiFC employs fine-grained block mapping to confine writes to high-performance pSLC zones, stabilizing latency and throughput under load. HiFC achieves inference throughput comparable to DRAM-based swapping under diverse long-context workloads, such as NarrativeQA, while significantly lowering the memory expansion cost of a GPU server system by 4.5$\\times$ over three years.",
      "arxiv_url": "https://openreview.net/forum?id=onhjdWCxZY",
      "pdf_url": "https://openreview.net/pdf/54ad85c547f1d3f857eaf95351118ce21c8de1d6.pdf",
      "primary_category": "LLM, inference, long context",
      "categories": [
        "LLM",
        "inference",
        "long context",
        "KV cache",
        "flash cache",
        "swapping",
        "HiFC",
        "DRAM-free",
        "vLLM",
        "paged attention",
        "SSD",
        "pSLC",
        "GDS",
        "cost efficiency"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tuA2R6gZEA",
      "title": "LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions",
      "authors": [
        "Chaochen Gao",
        "Xing W",
        "Zijia Lin",
        "Debing Zhang",
        "Songlin Hu"
      ],
      "abstract": "High-quality long-context instruction data is essential for aligning long-context large language models (LLMs). Despite the public release of models like Qwen and Llama, their long-context instruction data remains proprietary. Human annotation is costly and challenging, while template-based synthesis methods limit scale, diversity, and quality. We introduce LongMagpie, a self-synthesis framework that automatically generates large-scale long-context instruction data. Our key insight is that aligned long-context LLMs, when presented with a document followed by special tokens preceding a user turn, auto-regressively generate contextually relevant queries. By harvesting these document-query pairs and the model's responses, LongMagpie produces high-quality instructions without human effort. Experiments on HELMET, RULER, and Longbench v2 demonstrate that LongMagpie achieves leading performance on long-context tasks while maintaining competitive performance on short-context tasks, establishing it as a simple and effective approach for open, diverse, and scalable long-context instruction data synthesis.",
      "arxiv_url": "https://openreview.net/forum?id=tuA2R6gZEA",
      "pdf_url": "https://openreview.net/pdf/ed87797f943163aff0337ae4824c26f2e347ea6a.pdf",
      "primary_category": "Long-context Model, Synthesis Data",
      "categories": [
        "Long-context Model",
        "Synthesis Data"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "X5Hk8aMs6w",
      "title": "Self-Verification Provably Prevents Model Collapse in Recursive Synthetic Training",
      "authors": [
        "Shi Fu",
        "Yingjie Wang",
        "Yuzhu Chen",
        "Li Shen",
        "Dacheng Tao"
      ],
      "abstract": "Large generative models are increasingly trained on synthetic data from earlier generations, raising concerns about *model collapse*, a progressive performance decline consistently observed in empirical studies. However, theoretical understanding of recursive training dynamics and their failure modes remains limited. In this work, we theoretically show that recursive training inherently leads to exponential error growth unless mitigated by sufficient real data.  Addressing the growing scarcity of real data, we introduce a self-verification mechanism enabling models to filter their outputs based on internal confidence scores without external validation. Through rigorous analysis, we derive finite-sample error bounds demonstrating that self-verification alone can prevent collapse, even in fully synthetic training regimes. Our theoretical framework extends to large language models (LLMs), characterizing the conditions under which recursive training can maintain stability without performance degradation.",
      "arxiv_url": "https://openreview.net/forum?id=X5Hk8aMs6w",
      "pdf_url": "https://openreview.net/pdf/623a8abf038a3cdfb6f7204fbc0a67a690057306.pdf",
      "primary_category": "Model Collapse, Learning Theory, Synthetic Data",
      "categories": [
        "Model Collapse",
        "Learning Theory",
        "Synthetic Data",
        "Self-Verification",
        "LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7dBPm5c5ue",
      "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks",
      "authors": [
        "Yixuan Xu",
        "Antoine Bosselut",
        "Imanol Schlag"
      ],
      "abstract": "Large language models are known to memorize parts of their training data, posing risk of copyright violations. To systematically examine this risk, we pretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing web-scale data with public domain books used to simulate copyrighted content at controlled frequencies at lengths at least ten times longer than prior work. We thereby identified the offset effect, a phenomenon characterized by two key findings: (1) verbatim memorization is most strongly triggered by short prefixes drawn from the beginning of the context window, with memorization decreasing counterintuitively as prefix length increases; and (2) a sharp decline in verbatim recall when prefix begins offset from the initial tokens of the context window. We attribute this to positional fragility: models rely disproportionately on the earliest tokens in their context window as retrieval anchors, making them sensitive to even slight shifts. We further observe that when the model fails to retrieve memorized content, it often produces degenerated text. Leveraging these findings, we show that shifting sensitive data deeper into the context window suppresses both extractable memorization and degeneration. Our results suggest that positional offset is a critical and previously overlooked axis for evaluating memorization risks, since prior work implicitly assumed uniformity by probing only from the beginning of documents or training sequences.",
      "arxiv_url": "https://openreview.net/forum?id=7dBPm5c5ue",
      "pdf_url": "https://openreview.net/pdf/0c16737b706941dfe7e369f656cd397264125b10.pdf",
      "primary_category": "Large Language Model, Verbatim Memorization, Pretraining",
      "categories": [
        "Large Language Model",
        "Verbatim Memorization",
        "Pretraining",
        "Law",
        "Copyright"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "hg5UGeAr1Q",
      "title": "Why Knowledge Distillation Works in Generative Models: A Minimal Working Explanation",
      "authors": [
        "Sungmin Cha",
        "Kyunghyun Cho"
      ],
      "abstract": "Knowledge distillation (KD) is a core component in the training and deployment of modern generative models, particularly large language models (LLMs). While its empirical benefits are well documented---enabling smaller student models to emulate the performance of much larger teachers---the underlying mechanisms by which KD improves generative quality remain poorly understood. In this work, we present a minimal working explanation of KD in generative modeling. Using a controlled simulation with mixtures of Gaussians, we demonstrate that distillation induces a trade-off between precision and recall in the student model. As the teacher distribution becomes more selective, the student concentrates more probability mass on high-likelihood regions at the expense of coverage, which is a behavior modulated by a single entropy-controlling parameter. We then validate this effect in a large-scale language modeling setup using the SmolLM2 family of models. Empirical results reveal the same precision-recall dynamics observed in simulation, where precision corresponds to sample quality and recall to distributional coverage. This precision-recall trade-off in LLMs is found to be especially beneficial in scenarios where sample quality is more important than diversity, such as instruction tuning or downstream generation.  Our analysis provides a simple and general explanation for the effectiveness of KD in generative modeling.",
      "arxiv_url": "https://openreview.net/forum?id=hg5UGeAr1Q",
      "pdf_url": "https://openreview.net/pdf/afbbe433c194924f6d92244e6cfc31ea86f009c7.pdf",
      "primary_category": "LLMs, Generative Model, Knowledge Distillation",
      "categories": [
        "LLMs",
        "Generative Model",
        "Knowledge Distillation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "I9VNWQ15Ni",
      "title": "PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs",
      "authors": [
        "Jaewon Chu",
        "Seunghun Lee",
        "Hyunwoo J. Kim"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable success across diverse domains, due to their strong instruction-following capabilities. This raised interest in optimizing instructions for black-box LLMs, whose internal parameters are inaccessible but popular for their strong performance and ease of use. Recent approaches leverage white-box LLMs to assist instruction optimization for black-box LLMs by generating instructions from soft prompts. However, white-box LLMs often map different soft prompts to the same instruction, leading to redundant queries to the black-box model. While previous studies regarded this many-to-one mapping as a redundancy to be avoided, we reinterpret it as useful prior knowledge that can enhance the optimization performance. To this end, we introduce PREimage-informed inSTruction Optimization (PRESTO), a novel framework that leverages the preimage structure of soft prompts to improve query efficiency. PRESTO consists of three key components: (1) score sharing, which shares the evaluation score with all soft prompts in a preimage; (2) preimage-based initialization, which select initial data points that maximize search space coverage using preimage information; and (3) score consistency regularization, which enforces prediction consistency within each preimage. By leveraging preimages, PRESTO observes 14 times more scored data under the same query budget, resulting in more efficient optimization. Experimental results on 33 instruction optimization tasks demonstrate the superior performance of PRESTO.",
      "arxiv_url": "https://openreview.net/forum?id=I9VNWQ15Ni",
      "pdf_url": "https://openreview.net/pdf/064ac04cd75e10b9f79e38da608768967dbc21a1.pdf",
      "primary_category": "Large Language Models, Instruction Optimization",
      "categories": [
        "Large Language Models",
        "Instruction Optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RE97LT26w8",
      "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics",
      "authors": [
        "Christoph Jürgen Hemmer",
        "Daniel Durstewitz"
      ],
      "abstract": "Complex, temporally evolving phenomena, from climate to brain activity, are governed by dynamical systems (DS). DS reconstruction (DSR) seeks to infer generative surrogate models of these from observed data, reproducing their long-term behavior. Existing DSR approaches require purpose-training for any new system observed, lacking the zero-shot and in-context inference capabilities known from LLMs. Here we introduce *DynaMix*, a novel multivariate ALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR model able to generalize zero-shot to out-of-domain DS. Just from a provided context signal, without any re-training, DynaMix faithfully forecasts the long-term evolution of novel DS where existing time series (TS) foundation models, like Chronos, fail -- at a fraction of the number of parameters (0.1%) and orders of magnitude faster inference times. DynaMix outperforms TS foundation models in terms of long-term statistics, and often also short-term forecasts, even on real-world time series, like traffic or weather data, typically used for training and evaluating TS models, *but not at all part of DynaMix' training corpus*. We illustrate some of the failure modes of TS models for DSR problems, and conclude that models built on DS principles may bear a huge potential also for advancing the TS prediction field.",
      "arxiv_url": "https://openreview.net/forum?id=RE97LT26w8",
      "pdf_url": "https://openreview.net/pdf/30e791664cd06901e6c80be8dd8b6d6906233761.pdf",
      "primary_category": "Dynamical Systems, Dynamical Systems Reconstruction, Chaos",
      "categories": [
        "Dynamical Systems",
        "Dynamical Systems Reconstruction",
        "Chaos",
        "Recurrent Neural Networks",
        "Foundation Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VA5P0rUZPx",
      "title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models",
      "authors": [
        "Qianyue Hao",
        "Yiwen Song",
        "Qingmin Liao",
        "Jian Yuan",
        "Yong Li"
      ],
      "abstract": "Policy exploration is critical in reinforcement learning (RL), where existing approaches include $\\epsilon$-greedy, Gaussian process, etc.\nHowever, these approaches utilize preset stochastic processes and are indiscriminately applied in all kinds of RL tasks without considering task-specific features that influence policy exploration. Moreover, during RL training, the evolution of such stochastic processes is rigid, which typically only incorporates a decay in the variance, failing to adjust flexibly according to the agent's real-time learning status.\nInspired by the analyzing and reasoning capability of large language models (LLMs), we design **LLM-Explorer** to adaptively generate task-specific exploration strategies with LLMs, enhancing the policy exploration in RL. In our design, we sample the learning trajectory of the agent during the RL training in a given task and prompt the LLM to analyze the agent's current policy learning status and then generate a probability distribution for future policy exploration. Updating the probability distribution periodically, we derive a stochastic process specialized for the particular task and dynamically adjusted to adapt to the learning process. Our design is a plug-in module compatible with various widely applied RL algorithms, including the DQN series, DDPG, TD3, and any possible variants developed based on them. Through extensive experiments on the Atari and MuJoCo benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy exploration, achieving an average performance improvement up to 37.27%. Our code is open-source at https://github.com/tsinghua-fib-lab/LLM-Explorer for reproducibility.",
      "arxiv_url": "https://openreview.net/forum?id=VA5P0rUZPx",
      "pdf_url": "https://openreview.net/pdf/64ba671c860d8032b21824e28d00916c3e613964.pdf",
      "primary_category": "Reinforcement learning, large language model, policy exploration",
      "categories": [
        "Reinforcement learning",
        "large language model",
        "policy exploration"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "GBMzJLhsRj",
      "title": "Provable Scaling Laws for the Test-Time Compute of Large Language Models",
      "authors": [
        "Yanxi Chen",
        "Xuchen Pan",
        "Yaliang Li",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "abstract": "We propose two simple, principled and practical algorithms that enjoy provable scaling laws for the test-time compute of large language models (LLMs). The first one is a two-stage knockout-style algorithm: given an input problem, it first generates multiple candidate solutions, and then aggregate them via a knockout tournament for the final output. Assuming that the LLM can generate a correct solution with non-zero probability and do better than a random guess in comparing a pair of correct and incorrect solutions, we prove theoretically that the failure probability of this algorithm decays to zero exponentially or by a power law (depending on the specific way of scaling) as its test-time compute grows. The second one is a two-stage league-style algorithm, where each candidate is evaluated by its average win rate against multiple opponents, rather than eliminated upon loss to a single opponent. Under analogous but more robust assumptions, we prove that its failure probability also decays to zero exponentially with more test-time compute. Both algorithms require a black-box LLM and nothing else (e.g., no verifier or reward model) for a minimalistic implementation, which makes them appealing for practical applications and easy to adapt for different tasks. Through extensive experiments with diverse models and datasets, we validate the proposed theories and demonstrate the outstanding scaling properties of both algorithms.",
      "arxiv_url": "https://openreview.net/forum?id=GBMzJLhsRj",
      "pdf_url": "https://openreview.net/pdf/dfab05f4543d811a499e3234d231318075ea52aa.pdf",
      "primary_category": "test-time compute, inference scaling law, large language model",
      "categories": [
        "test-time compute",
        "inference scaling law",
        "large language model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wryhlhA8QI",
      "title": "Emergent Risk Awareness in Rational Agents under Resource Constraints",
      "authors": [
        "Daniel Jarne Ornia",
        "Nicholas George Bishop",
        "Joel Dyer",
        "Wei-Chen Lee",
        "Ani Calinescu",
        "J. Doyne Farmer",
        "Michael J. Wooldridge"
      ],
      "abstract": "Advanced reasoning models with agentic capabilities (AI agents) are deployed to interact with humans and to solve sequential decision‑making problems under (often approximate) utility functions and internal models. When such problems have resource or failure constraints where action sequences may be forcibly terminated once resources are exhausted, agents face implicit trade‑offs that reshape their utility-driven (rational) behaviour. Additionally, since these agents are typically commissioned by a human principal to act on their behalf, asymmetries in constraint exposure can give rise to previously unanticipated misalignment between human objectives and agent incentives. We formalise this setting through a survival bandit framework, provide theoretical and empirical results that quantify the impact of survival‑driven preference shifts, identify conditions under which misalignment emerges and propose mechanisms to mitigate the emergence of risk-seeking or risk-averse behaviours. As a result, this work aims to increase understanding and interpretability of emergent behaviours of AI agents operating under such survival pressure, and offer guidelines for safely deploying such AI systems in critical resource‑limited environments.",
      "arxiv_url": "https://openreview.net/forum?id=wryhlhA8QI",
      "pdf_url": "https://openreview.net/pdf/b721f93c521d624000a1eedbe25a30f679087e56.pdf",
      "primary_category": "Decision Making, Alignment, Interpretability",
      "categories": [
        "Decision Making",
        "Alignment",
        "Interpretability",
        "Planning"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "x0i7wvRLHK",
      "title": "Exploring the limits of strong membership inference attacks on large language models",
      "authors": [
        "Jamie Hayes",
        "Ilia Shumailov",
        "Christopher A. Choquette-Choo",
        "Matthew Jagielski",
        "Georgios Kaissis",
        "Milad Nasr",
        "Meenatchi Sundaram Muthu Selva Annamalai",
        "Niloofar Mireshghallah",
        "Igor Shilov",
        "Matthieu Meeus",
        "Yves-Alexandre de Montjoye",
        "Katherine Lee",
        "Franziska Boenisch",
        "Adam Dziedzic",
        "A. Feder Cooper"
      ],
      "abstract": "State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training references (e.g., fine-tuning attacks), or on stronger attacks applied to small models and datasets. However, weaker attacks have been shown to be brittle and insights from strong attacks in simplified settings do not translate to today's LLMs. These challenges prompt an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRA--one of the strongest MIAs--to GPT-2 architectures ranging from 10M to 1B parameters, training references on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in four key ways. While (1) strong MIAs can succeed on pre-trained LLMs, (2) their effectiveness, remains limited (e.g., AUC<0.7) in practical settings. (3) Even when strong MIAs achieve better-than-random AUC, aggregate metrics can conceal substantial per-sample MIA decision instability: due to training randomness, many decisions are so unstable that they are statistically indistinguishable from a coin flip. Finally, (4) the relationship between MIA success and related LLM privacy metrics is not as straightforward as prior work has suggested.",
      "arxiv_url": "https://openreview.net/forum?id=x0i7wvRLHK",
      "pdf_url": "https://openreview.net/pdf/b974f672f410a1296113f1d4b651a95f5194e3e5.pdf",
      "primary_category": "membership inference, memorization",
      "categories": [
        "membership inference",
        "memorization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qXSFkP0ELS",
      "title": "Atom of Thoughts for Markov LLM Test-Time Scaling",
      "authors": [
        "Fengwei Teng",
        "Quan Shi",
        "Zhaoyang Yu",
        "Jiayi Zhang",
        "Yuyu Luo",
        "Chenglin Wu",
        "Zhijiang Guo"
      ],
      "abstract": "Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. \nHowever, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. \nTo address this issue, we observe that complex reasoning can be achieved by solving a series of independent and self-contained subquestions. These subquestions are essentially \\textit{atomic questions}, exhibiting the memoryless property similar to Markov processes.\nBased on this observation, we propose Atom of Thoughts (\\our), where each state transition consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a simplified question that maintains answer equivalence with the original problem. This answer preservation enables the iterative \\textit{decomposition-contraction} process to naturally form a meaningful Markov reasoning process.\nFurthermore, these atomic states can be seamlessly integrated into existing test-time scaling methods, enabling \\our to serve as a plug-in enhancement for improving reasoning capabilities.",
      "arxiv_url": "https://openreview.net/forum?id=qXSFkP0ELS",
      "pdf_url": "https://openreview.net/pdf/aee7e5e0ac85edb676698e634deccc28c92e1407.pdf",
      "primary_category": "LLM Reasoning, Reasoning Framework",
      "categories": [
        "LLM Reasoning",
        "Reasoning Framework"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "KAMsbarp3w",
      "title": "Understanding and Rectifying Safety Perception Distortion in VLMs",
      "authors": [
        "Xiaohan Zou",
        "Jian Kang",
        "George Kesidis",
        "Lu Lin"
      ],
      "abstract": "Recent studies reveal that vision-language models (VLMs) become more susceptible to harmful requests and jailbreak attacks after integrating the vision modality, exhibiting greater vulnerability than their text-only LLM backbones. To uncover the root cause of this phenomenon, we conduct an in-depth analysis and identify a key issue: multimodal inputs introduce an modality-induced activation shift toward a “safer” direction compared to their text-only counterparts, leading VLMs to systematically overestimate the safety of harmful inputs. We refer to this issue as safety perception distortion. To mitigate such distortion, we propose Activation Shift Disentanglement and Calibration (ShiftDC), a training-free method that decomposes and calibrates the modality-induced activation shift to reduce its impact on safety. By isolating and removing the safety-relevant component,  ShiftDC restores the inherent safety alignment of the LLM backbone while preserving the vision-language capabilities of VLMs. Experiments demonstrate that ShiftDC significantly enhances safety alignment without impairing model utility.",
      "arxiv_url": "https://openreview.net/forum?id=KAMsbarp3w",
      "pdf_url": "https://openreview.net/pdf/d8bfe8faa1c6d91348f3a52c462a813e35110d7e.pdf",
      "primary_category": "multimodal llm, jailbreak, llm safety",
      "categories": [
        "multimodal llm",
        "jailbreak",
        "llm safety"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "fDjDVE4qdj",
      "title": "Think Only When You Need with Large Hybrid-Reasoning Models",
      "authors": [
        "Lingjie Jiang",
        "Xun Wu",
        "Shaohan Huang",
        "Qingxiu Dong",
        "Zewen Chi",
        "Li Dong",
        "Xingxing Zhang",
        "Tengchao Lv",
        "Lei Cui",
        "Furu Wei"
      ],
      "abstract": "Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, which is unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform reasoning based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate reasoning mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the model’s capability for hybrid reasoning. Extensive experimental results show that LHRMs can adaptively perform hybrid reasoning on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended reasoning processes and provides a solid starting point for building hybrid reasoning systems.",
      "arxiv_url": "https://openreview.net/forum?id=fDjDVE4qdj",
      "pdf_url": "https://openreview.net/pdf/33fb886d1cda050a0c29d0bdee85176c1c3f7f31.pdf",
      "primary_category": "Hybrid Reasoning, Large Reasoning Models, Reinforcement Learning",
      "categories": [
        "Hybrid Reasoning",
        "Large Reasoning Models",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "eIojV2epgX",
      "title": "Synergistic Tensor and Pipeline Parallelism",
      "authors": [
        "Mengshi Qi",
        "Jiaxuan Peng",
        "Jie Zhang",
        "Juan Zhu",
        "Yong Li",
        "Huadong Ma"
      ],
      "abstract": "In the machine learning system, the hybrid model parallelism combining tensor parallelism (TP) and pipeline parallelism (PP) has become the dominant solution for distributed training of Large Language Models~(LLMs) and Multimodal LLMs (MLLMs). However, TP introduces significant collective communication overheads, while PP suffers from synchronization inefficiencies such as pipeline bubbles. Existing works primarily address these challenges from isolated perspectives, focusing either on overlapping TP communication or on flexible PP scheduling to mitigate pipeline bubbles. In this paper, we propose a new synergistic tensor and pipeline parallelism schedule that simultaneously reduces both types of bubbles. Our proposed schedule decouples the forward and backward passes in PP into fine-grained computation units, which are then braided to form a composite computation sequence. This compositional structure enables near-complete elimination of TP-related bubbles. Building upon this structure, we further design the PP schedule to minimize PP bubbles. Experimental results demonstrate that our approach improves training throughput by up to 12\\% for LLMs and 16\\% for MLLMs compared to existing scheduling methods. Our source code is avaiable at https://github.com/MICLAB-BUPT/STP.",
      "arxiv_url": "https://openreview.net/forum?id=eIojV2epgX",
      "pdf_url": "https://openreview.net/pdf/bec908639def2b2c30972be14736f60f3b75648e.pdf",
      "primary_category": "Distributed System, Pipeline Parallelism, Large-Scale Model Training",
      "categories": [
        "Distributed System",
        "Pipeline Parallelism",
        "Large-Scale Model Training",
        "LLM",
        "Machine Learning System"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DlkM0q4Cvk",
      "title": "Dependency Matters: Enhancing LLM Reasoning with Explicit Knowledge Grounding",
      "authors": [
        "Xiangyu Wen",
        "Min Li",
        "Junhua Huang",
        "Jianyuan Zhong",
        "Zhijian Xu",
        "Zeju Li",
        "Yongxiang Huang",
        "Mingxuan Yuan",
        "Qiang Xu"
      ],
      "abstract": "Large language models (LLMs) often produce reasoning steps that are superficially coherent yet internally inconsistent, leading to unreliable outputs. Since such failures typically arise from implicit or poorly-grounded knowledge, we introduce \\emph{Grounded Reasoning in Dependency (GRiD)}, a novel dependency-aware reasoning framework that explicitly grounds reasoning steps in structured knowledge. GRiD represents reasoning as a graph consisting of interconnected knowledge extraction nodes and reasoning nodes, enforcing logical consistency through explicit dependencies. Each reasoning step is validated via a lightweight, step-wise verifier that ensures logical correctness relative to its premises. Extensive experiments across diverse reasoning benchmarks—including StrategyQA, CommonsenseQA, GPQA, and TruthfulQA—demonstrate that GRiD substantially improves reasoning accuracy, consistency, and faithfulness compared to recent state-of-the-art structured reasoning methods. Notably, GRiD enhances performance even when applied purely as a lightweight verification module at inference time, underscoring its generalizability and practical utility. Code is available at: https://github.com/cure-lab/GRiD.",
      "arxiv_url": "https://openreview.net/forum?id=DlkM0q4Cvk",
      "pdf_url": "https://openreview.net/pdf/88669d564fe12a96458ad3ff7025be1f74506a21.pdf",
      "primary_category": "LLM Reasoning, Question Answering, Knowledge-enhanced Reasoning",
      "categories": [
        "LLM Reasoning",
        "Question Answering",
        "Knowledge-enhanced Reasoning",
        "Dependency-aware Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "86b23oNkg9",
      "title": "Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models",
      "authors": [
        "Hanze Guo",
        "Jing Yao",
        "Xiao Zhou",
        "Xiaoyuan Yi",
        "Xing Xie"
      ],
      "abstract": "As large language models (LLMs) become increasingly integrated into applications serving users across diverse cultures, communities, and demographics, it is critical to align LLMs with pluralistic human values beyond average principles (e.g., HHH). \nIn psychological and social value theories such as Schwartz’s Value Theory, pluralistic values are represented by multiple value dimensions paired with various priorities. However, existing methods encounter two challenges when aligning with such fine-grained value objectives: 1) they often treat multiple values as independent and equally important, ignoring their interdependence and relative priorities (value complexity); 2) they struggle to precisely control nuanced value priorities, especially those underrepresented ones (value steerability). To handle these challenges, we propose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE alignment. It introduces a structural causal model (SCM) to feature complex interdependency and prioritization among features, as well as the causal relationship between high-level value dimensions and behaviors. Moreover, it applies counterfactual reasoning to generate outputs aligned with any desired value objectives. Benefitting from explicit causal modeling, COUPLE also provides better interpretability. We evaluate COUPLE on two datasets with different value systems and demonstrate that COUPLE advances other baselines across diverse types of value objectives. Our code is available at https://github.com/microsoft/COUPLE.",
      "arxiv_url": "https://openreview.net/forum?id=86b23oNkg9",
      "pdf_url": "https://openreview.net/pdf/4caafadcb0cbd8132a6debcc551fa3692a1fa2df.pdf",
      "primary_category": "Large Language Models; Pluralistic Alignment; Value Alignment",
      "categories": [
        "Large Language Models; Pluralistic Alignment; Value Alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qmBMPInbZC",
      "title": "What Can RL Bring to VLA Generalization? An Empirical Study",
      "authors": [
        "Jijia Liu",
        "Feng Gao",
        "Bingwen Wei",
        "Xinlei Chen",
        "Qingmin Liao",
        "Yi Wu",
        "Chao Yu",
        "Yu Wang"
      ],
      "abstract": "Large Vision-Language Action (VLA) models have shown significant potential for embodied AI. \nHowever, their predominant training via supervised fine-tuning (SFT) limits generalization due to susceptibility to compounding errors under distribution shifts. Reinforcement learning (RL) offers a path to overcome these limitations by optimizing for task objectives via trial-and-error, yet a systematic understanding of its specific generalization benefits for VLAs compared to SFT is lacking. \nTo address this, our study introduces a comprehensive benchmark for evaluating VLA generalization and systematically investigates the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Our extensive experiments reveal that RL fine-tuning, particularly with PPO, significantly enhances generalization in semantic understanding and execution robustness over SFT, while maintaining comparable visual robustness. We identify PPO as a more effective RL algorithm for VLAs than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for efficient PPO training on VLAs, and demonstrate its practical utility for improving VLA generalization. The project page is at https://rlvla.github.io",
      "arxiv_url": "https://openreview.net/forum?id=qmBMPInbZC",
      "pdf_url": "https://openreview.net/pdf/bf7915a221a54c4a9df55d8362b9600c9904af4b.pdf",
      "primary_category": "Reinforcement Learning, Vision-Language-Action Model, Policy Generalization",
      "categories": [
        "Reinforcement Learning",
        "Vision-Language-Action Model",
        "Policy Generalization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oQYq9L1NVT",
      "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding",
      "authors": [
        "Xiaoyi Zhang",
        "Zhaoyang Jia",
        "Zongyu Guo",
        "Jiahao Li",
        "Bin Li",
        "Houqiang Li",
        "Yan Lu"
      ],
      "abstract": "Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. \nWhile Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. \nTo overcome such limitations, we propose the $\\textbf{D}eep \\ \\textbf{V}ideo \\ \\textbf{D}iscovery \\ (\\textbf{DVD})$ agent to leverage an $\\textit{agentic search}$ strategy over segmented video clips. Different from previous video agents manually designing a rigid workflow, our approach emphasizes the autonomous nature of agents.\nBy providing a set of search-centric tools on multi-granular video database,\nour DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools to orchestrate adaptive workflow for different queries in light of the gathered information.\nWe perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates our advantage.\nOur DVD agent achieves state-of-the-art performance on the challenging LVBench dataset, reaching an accuracy of $\\textbf{74.2\\%}$, which substantially surpasses all prior works, and further improves to $\\textbf{76.0\\%}$ with transcripts.",
      "arxiv_url": "https://openreview.net/forum?id=oQYq9L1NVT",
      "pdf_url": "https://openreview.net/pdf/e088d8522092e965e16cd0d0f7a994ed5ea27435.pdf",
      "primary_category": "agent, computer vision, long video understanding",
      "categories": [
        "agent",
        "computer vision",
        "long video understanding"
      ],
      "tags": [
        "LLM",
        "Agentic AI",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XdwPWKbxd9",
      "title": "RvLLM: LLM Runtime Verification with Domain Knowledge",
      "authors": [
        "Yedi Zhang",
        "Sun Yi Emma",
        "Annabelle Lee Jia En",
        "Jin Song Dong"
      ],
      "abstract": "Large language models (LLMs) have emerged as a dominant AI paradigm due to their exceptional text understanding and generation capabilities. However, their tendency to generate inconsistent or erroneous outputs challenges their reliability, especially in high-stakes domains requiring accuracy and trustworthiness. Existing research primarily focuses on detecting and mitigating model misbehavior in general-purpose scenarios, often overlooking the potential of integrating domain-specific knowledge. In this work, we advance misbehavior detection by incorporating domain knowledge. The core idea is to design a general specification language that enables domain experts to customize domain-specific constraints in a lightweight and intuitive manner, supporting later runtime monitoring of LLM outputs. To achieve this, we design a novel specification language ESL, and introduce a runtime verification framework RvLLM to validate LLM output against domain-specific constraints defined in ESL. RvLLM operates in two main stages: interpretation and reasoning. During interpretation, it derives interpretations of the specification based on the context, which then guide the reasoning process to identify inconsistencies. When new knowledge is derived, RvLLM issues a follow-up query to the LLM to further verify the consistency. We evaluate RvLLM on three representative tasks: violation detection against Singapore Rapid Transit Systems Act, numerical comparison, and inequality solving. Experimental results show that RvLLM effectively detects erroneous outputs across various LLMs in a lightweight and flexible manner. The results reveal that despite their impressive capabilities, LLMs remain prone to low-level errors due to a lack of formal guarantees during inference, and our framework offers a potential long-term solution by leveraging expert domain knowledge to rigorously and efficiently verify LLM outputs.",
      "arxiv_url": "https://openreview.net/forum?id=XdwPWKbxd9",
      "pdf_url": "https://openreview.net/pdf/ef4bf3121d8273376f32b0e938e0ee3ff3c79db6.pdf",
      "primary_category": "Runtime Verification, Specification, Domain Knowledge",
      "categories": [
        "Runtime Verification",
        "Specification",
        "Domain Knowledge",
        "Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "68OW8tLSh2",
      "title": "Towards Reliable LLM-based Robots Planning via Combined Uncertainty Estimation",
      "authors": [
        "Shiyuan Yin",
        "Chenjia Bai",
        "Zhang Zihao",
        "Junwei Jin",
        "Xinxin Zhang",
        "Chi Zhang",
        "Xuelong Li"
      ],
      "abstract": "Large language models (LLMs) demonstrate advanced reasoning abilities, enabling robots to understand natural language instructions and generate high-level plans with appropriate grounding. However, LLM hallucinations present a significant challenge, often leading to overconfident yet potentially misaligned or unsafe plans. While researchers have explored uncertainty estimation to improve the reliability of LLM-based planning, existing studies have not sufficiently differentiated between epistemic and intrinsic uncertainty, limiting the effectiveness of uncertainty estimation.\nIn this paper, we present Combined Uncertainty estimation for Reliable Embodied planning (CURE), which decomposes the uncertainty into epistemic and intrinsic uncertainty, each estimated separately. Furthermore, epistemic uncertainty is subdivided into task clarity and task familiarity for more accurate evaluation. The overall uncertainty assessments are obtained using random network distillation and multi-layer perceptron regression heads driven by LLM features. \nWe validated our approach in two distinct experimental settings: kitchen manipulation and tabletop rearrangement experiments. The results show that, compared to existing methods, our approach yields uncertainty estimates that are more closely aligned with the actual execution outcomes. The code is at https://github.com/Firesuiry/CURE.",
      "arxiv_url": "https://openreview.net/forum?id=68OW8tLSh2",
      "pdf_url": "https://openreview.net/pdf/73d35e625e771f7cd1b49e5522535287d1ea7293.pdf",
      "primary_category": "Language-based planning, Uncertainty Estimation, Uncertainty Quantification",
      "categories": [
        "Language-based planning",
        "Uncertainty Estimation",
        "Uncertainty Quantification",
        "Foundation Models for Decision Making"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BCWQ5w9aGd",
      "title": "Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via Internal Representations",
      "authors": [
        "Peng Lai",
        "Jianjie Zheng",
        "Sijie Cheng",
        "Yun Chen",
        "Peng Li",
        "Yang Liu",
        "Guanhua Chen"
      ],
      "abstract": "The growing scale of evaluation tasks has led to the widespread adoption of automated evaluation using LLMs, a paradigm known as “LLM-as-a-judge”. However, improving its alignment with human preferences without complex prompts or fine-tuning remains challenging. Previous studies mainly optimize based on shallow outputs, overlooking rich cross-layer representations. In this work, motivated by preliminary findings that middle-to-upper layers encode semantically and task-relevant representations that are often more aligned with human judgments than the final layer, we propose LAGER, a post-hoc, plug-and-play framework for improving the alignment of LLM-as-a-Judge point-wise evaluations with human scores by leveraging internal representations. LAGER produces fine-grained judgment scores by aggregating cross-layer score-token logits and computing the expected score from a softmax-based distribution, while keeping the LLM backbone frozen and ensuring no impact on the inference process.\nLAGER fully leverages the complementary information across different layers, overcoming the limitations of relying solely on the final layer.\nWe evaluate our method on the standard alignment benchmarks Flask, HelpSteer, and BIGGen using Spearman correlation, and find that LAGER achieves improvements of up to 7.5% over the best baseline across these benchmarks. Without reasoning steps, LAGER matches or outperforms reasoning-based methods. Experiments on downstream applications, such as data selection and emotional understanding, further show the generalization of LAGER.",
      "arxiv_url": "https://openreview.net/forum?id=BCWQ5w9aGd",
      "pdf_url": "https://openreview.net/pdf/77ddc1ab75310af76bd42cf125a63ace72c45a7c.pdf",
      "primary_category": "llm-as-a-judge, llm evaluator",
      "categories": [
        "llm-as-a-judge",
        "llm evaluator"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "h3lyFa5e1W",
      "title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning",
      "authors": [
        "Zhongwei Wan",
        "Zhihao Dou",
        "Che Liu",
        "Yu Zhang",
        "Dongfei Cui",
        "Qinjian Zhao",
        "Hui Shen",
        "Jing Xiong",
        "Yi Xin",
        "Yifan Jiang",
        "Chaofan Tao",
        "Yangfan He",
        "Mi Zhang",
        "Shen Yan"
      ],
      "abstract": "Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle significantly with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic and struggle to generate meaningful, instructive feedback, as the reasoning ability and knowledge limits of pre-trained models are largely fixed during initial training. To overcome these challenges, we propose \\textit{multimodal \\textbf{S}elf-\\textbf{R}eflection enhanced reasoning with Group Relative \\textbf{P}olicy \\textbf{O}ptimization} \\textbf{SRPO}, a two-stage reflection-aware reinforcement learning (RL) framework explicitly designed to enhance multimodal LLM reasoning. In the first stage, we construct a high-quality, reflection-focused dataset under the guidance of an advanced MLLM, which generates reflections based on initial responses to help the policy model to learn both reasoning and self-reflection. In the second stage, we introduce a novel reward mechanism within the GRPO framework that encourages concise and cognitively meaningful reflection while avoiding redundancy. Extensive experiments across multiple multimodal reasoning benchmarks—including MathVista, MathVision, Mathverse, and MMMU-Pro—using Qwen-2.5-VL-7B and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms state-of-the-art models, achieving notable improvements in both reasoning accuracy and reflection quality.",
      "arxiv_url": "https://openreview.net/forum?id=h3lyFa5e1W",
      "pdf_url": "https://openreview.net/pdf/514851b938f3c4ab888ed4499926b37fdbb1b89c.pdf",
      "primary_category": "MLLMs, Reasoning",
      "categories": [
        "MLLMs",
        "Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2VdsYVXLDl",
      "title": "Zero-Shot Detection of LLM-Generated Text via Implicit Reward Model",
      "authors": [
        "Runheng Liu",
        "Heyan Huang",
        "Xingchen Xiao",
        "Zhijing Wu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their ability to generate human-like text has raised concerns about potential misuse. This underscores the need for reliable and effective methods to detect LLM-generated text. \nIn this paper, we propose IRM, a novel zero-shot approach that leverages Implicit Reward Models for LLM-generated text detection. Such implicit reward models can be derived from publicly available instruction-tuned and base models. Previous reward-based method relies on preference construction and task-specific fine-tuning. In comparison, IRM requires neither preference collection nor additional training.\nWe evaluate IRM on the DetectRL benchmark and demonstrate that IRM can achieve superior detection performance, outperforms existing zero-shot and supervised methods in LLM-generated text detection.",
      "arxiv_url": "https://openreview.net/forum?id=2VdsYVXLDl",
      "pdf_url": "https://openreview.net/pdf/ae628de52a9dd0d27defdaa32a8257f81db335db.pdf",
      "primary_category": "Large Language Model, LLM Generated Text Detection, Reward Model",
      "categories": [
        "Large Language Model",
        "LLM Generated Text Detection",
        "Reward Model",
        "Zero-Shot Detection"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Hmd8CqMo3E",
      "title": "Semantic-guided Diverse Decoding for Large Language Model",
      "authors": [
        "Weijie Shi",
        "Yue Cui",
        "Yaguang Wu",
        "Jingzhi Fang",
        "Shibo Zhang",
        "Mengze Li",
        "Sirui Han",
        "Jia Zhu",
        "Jiajie Xu",
        "Xiaofang Zhou"
      ],
      "abstract": "Diverse decoding of large language models is crucial for applications requiring multiple semantically distinct responses, yet existing methods primarily achieve lexical rather than semantic diversity. This limitation significantly constrains Best-of-N strategies, group-based reinforcement learning, and data synthesis. While temperature sampling and diverse beam search modify token distributions or apply n-gram penalties, they fail to ensure meaningful semantic differentiation. We introduce Semantic-guided Diverse Decoding (SemDiD), operating directly in embedding space that balances quality with diversity through three complementary mechanisms: orthogonal directional guidance, dynamic inter-group repulsion, and position-debiased probability assessment. SemDiD harmonizes these competing objectives using adaptive gain functions and constraint optimization, ensuring both quality thresholds and maximal semantic differentiation. Experiments show SemDiD consistently outperforms existing methods, improving Best-of-N coverage by 1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15% while increasing accuracy by up to 2.1%.",
      "arxiv_url": "https://openreview.net/forum?id=Hmd8CqMo3E",
      "pdf_url": "https://openreview.net/pdf/9a6bb6930d9a85846b541f44d9e74055dd67c548.pdf",
      "primary_category": "Diverse Decoding, Sampling Strategy, Large Language Model",
      "categories": [
        "Diverse Decoding",
        "Sampling Strategy",
        "Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "aAhhMr0TX9",
      "title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM",
      "authors": [
        "Bowen Dong",
        "Minheng Ni",
        "Zitong Huang",
        "Guanglei Yang",
        "Wangmeng Zuo",
        "Lei Zhang"
      ],
      "abstract": "Multimodal hallucination in multimodal large language models (MLLMs) restricts the correctness of MLLMs. However, multimodal hallucinations are multi-sourced and arise from diverse causes. Existing benchmarks fail to adequately distinguish between perception-induced hallucinations and reasoning-induced hallucinations. This failure constitutes a significant issue and hinders the diagnosis of multimodal reasoning failures within MLLMs. To address this, we propose the MIRAGE benchmark, which isolates reasoning hallucinations by constructing questions where input images are correctly perceived by MLLMs yet reasoning errors persist. MIRAGE introduces multi-granular evaluation metrics: accuracy, factuality, and LLMs hallucination score for hallucination quantification. Our analysis reveals strong correlations between question types and specific hallucination patterns, particularly systematic failures of MLLMs in spatial reasoning involving complex relationships (\\emph{e.g.}, complex geometric patterns across images). This highlights a critical limitation in the reasoning capabilities of current MLLMs and provides targeted insights for hallucination mitigation on specific types. To address these challenges, we propose Logos, a method that combines curriculum reinforcement fine-tuning to encourage models to generate logic-consistent reasoning chains by stepwise reducing learning difficulty, and collaborative hint inference to reduce reasoning complexity. Logos establishes a baseline on MIRAGE, and reduces the logical hallucinations in original base models. Link: \\url{https://bit.ly/25mirage}.",
      "arxiv_url": "https://openreview.net/forum?id=aAhhMr0TX9",
      "pdf_url": "https://openreview.net/pdf/5c8396fbd51956694870ead9c44afcff4e5253cb.pdf",
      "primary_category": "multimodal learning, hallucinations, evaluation",
      "categories": [
        "multimodal learning",
        "hallucinations",
        "evaluation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vpp3FGNLi6",
      "title": "NeuroPath: Neurobiology-Inspired Path Tracking and Reflection for Semantically Coherent Retrieval",
      "authors": [
        "Junchen Li",
        "Rongzheng Wang",
        "Yihong Huang",
        "Qizhi Chen",
        "Jiasheng Zhang",
        "Shuang Liang"
      ],
      "abstract": "Retrieval-augmented generation (RAG) greatly enhances large language models (LLMs) performance in knowledge-intensive tasks. However, naive RAG methods struggle with multi-hop question answering due to their limited capacity to capture complex dependencies across documents. Recent studies employ graph-based RAG to capture document connections. However, these approaches often result in a loss of semantic coherence and introduce irrelevant noise during node matching and subgraph construction. To address these limitations, we propose NeuroPath, an LLM-driven semantic path tracking RAG framework inspired by the path navigational planning of place cells in neurobiology. It consists of two steps: Dynamic Path Tracking and Post-retrieval Completion. Dynamic Path Tracking performs goal-directed semantic path tracking and pruning over the constructed knowledge graph (KG), improving noise reduction and semantic coherence. Post-retrieval Completion further reinforces these benefits by conducting second-stage retrieval using intermediate reasoning and the original query to refine the query goal and complete missing information in the reasoning path. NeuroPath surpasses current state-of-the-art baselines on three multi-hop QA datasets, achieving average improvements of 16.3\\% on recall@2 and 13.5\\% on recall@5 over advanced graph-based RAG methods. Moreover, compared to existing iter-based RAG methods, NeuroPath achieves higher accuracy and reduces token consumption by 22.8\\%. Finally, we demonstrate the robustness of NeuroPath across four smaller LLMs (Llama3.1, GLM4, Mistral0.3, and Gemma3), and further validate its scalability across tasks of varying complexity. Code is available at [https://github.com/KennyCaty/NeuroPath](https://github.com/KennyCaty/NeuroPath).",
      "arxiv_url": "https://openreview.net/forum?id=vpp3FGNLi6",
      "pdf_url": "https://openreview.net/pdf/498f2ece209c8ffc5d3d7da2ead58437a45cacc0.pdf",
      "primary_category": "Retrieval-Augmented Generation, Large Language Models, Multi-hop Question Answering",
      "categories": [
        "Retrieval-Augmented Generation",
        "Large Language Models",
        "Multi-hop Question Answering",
        "Knowledge Graphs"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4MvqmXnCEr",
      "title": "Self-Verifying Reflection Helps Transformers with CoT Reasoning",
      "authors": [
        "Zhongwei Yu",
        "Wannian Xia",
        "Xue Yan",
        "Bo XU",
        "Haifeng Zhang",
        "Yali Du",
        "Jun Wang"
      ],
      "abstract": "Advanced large language models (LLMs) frequently reflect in reasoning chain-of-thoughts (CoTs), where they self-verify the correctness of current solutions and explore alternatives. However, given recent findings that LLMs detect limited errors in CoTs, how reflection contributes to empirical improvements remains unclear. To analyze this issue, in this paper, we present a minimalistic reasoning framework to support basic self-verifying reflection for small transformers without natural language, which ensures analytic clarity and reduces the cost of comprehensive experiments. Theoretically, we prove that self-verifying reflection guarantees improvements if verification errors are properly bounded. Experimentally, we show that tiny transformers, with only a few million parameters, benefit from self-verification in both training and reflective execution, reaching remarkable LLM-level performance in integer multiplication and Sudoku. Similar to LLM results, we find that reinforcement learning (RL) improves in-distribution performance and incentivizes frequent reflection for tiny transformers, yet RL mainly optimizes shallow statistical patterns without faithfully reducing verification errors. In conclusion, integrating generative transformers with discriminative verification inherently facilitates CoT reasoning, regardless of scaling and natural language.",
      "arxiv_url": "https://openreview.net/forum?id=4MvqmXnCEr",
      "pdf_url": "https://openreview.net/pdf/3e6b4af36ae39d6874476ce9767cebc056b26e92.pdf",
      "primary_category": "chain-of-thought, reasoning, reflection",
      "categories": [
        "chain-of-thought",
        "reasoning",
        "reflection",
        "verification",
        "transformer"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "t0cpjJxj1s",
      "title": "Analyzing the Power of Chain of Thought through Memorization Capabilities",
      "authors": [
        "Lijia Yu",
        "Xiao-Shan Gao",
        "Lijun Zhang"
      ],
      "abstract": "It has been shown that the chain of thought (CoT) can enhance the power of LLMs to simulate a Turing machine or an algorithm, and in particular its mathematical reasoning ability. The memorization capability of LLMs is an important aspect of their expressive ability, which offers valuable insight into designing models with enhanced generalization potential. Currently, the optimal memorization capacities of transformers have been established for both the general dataset and the dataset that satisfies a specific separability condition. However, the question of whether the CoT can improve the memorization capability of LLMs remains unexamined. To fill this gap, we establish the memorization capability for fixed-precision autoregressive transformers with or without CoT. Precisely, we first give the necessary and sufficient conditions for transformers to memorize a finite language and then provide the upper and lower bounds for the number of parameters of the memorization transformers. Our result indicates that the classes of languages that can be memorized by transformers with or without CoT do not contain each other, and the same number of parameters is needed for transformers with or without CoT to memorize, implying that CoT does not enhance a transformer’s memorization power significantly. We further show that CoT can not help transformers to memory certain infinite languages.",
      "arxiv_url": "https://openreview.net/forum?id=t0cpjJxj1s",
      "pdf_url": "https://openreview.net/pdf/8d91eddfc018cf29e380abcaca30be477466bc10.pdf",
      "primary_category": "Theory, Chain of Thought, Transformer",
      "categories": [
        "Theory",
        "Chain of Thought",
        "Transformer",
        "Memorization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "w4qJ056WhI",
      "title": "SpaceServe: Spatial Multiplexing of Complementary Encoders and Decoders for Multimodal LLMs",
      "authors": [
        "zhicheng li",
        "Shuoming Zhang",
        "Jiacheng Zhao",
        "Siqi Li",
        "Xiyu Shi",
        "Yangyu Zhang",
        "Shuaijiang Li",
        "Donglin Yu",
        "Zheming Yang",
        "YUAN WEN",
        "Huimin Cui"
      ],
      "abstract": "Recent multimodal large language models (MLLMs) marry modality-specific\nvision or audio encoders with a shared text decoder. While the encoder is compute-\nintensive but memory-light, the decoder is the opposite, yet state-of-the-art serving\nstacks still time-multiplex these complementary kernels, idling SMs or HBM in\nturn. We introduce SpaceServe, a serving system that space-multiplexes MLLMs:\nit decouples all modality encoders from the decoder, and co-locates them on the\nsame GPU using fine-grained SM partitioning available in modern runtimes. A\ncost-model-guided Space-Inference Scheduler (SIS) dynamically assigns SM slices,\nwhile a Time-Windowed Shortest-Remaining-First (TWSRFT) policy batches en-\ncoder requests to minimise completion latency and smooth decoder arrivals. \nEvaluation shows that SpaceServe reduces time-per-output-token by 4.81×\non average and up to 28.9× on Nvidia A100 GPUs. SpaceServe is available at\nhttps://github.com/gofreelee/SpaceServe",
      "arxiv_url": "https://openreview.net/forum?id=w4qJ056WhI",
      "pdf_url": "https://openreview.net/pdf/3b4c3201ed20b1e72b79554b27c1dbf4768bb9b9.pdf",
      "primary_category": "Multimodal large language models; Inference optimizations; Infrastructure",
      "categories": [
        "Multimodal large language models; Inference optimizations; Infrastructure"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "u1lNQH5upa",
      "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling",
      "authors": [
        "Yongsen Mao",
        "Junhao Zhong",
        "Chuan Fang",
        "Jia Zheng",
        "Rui Tang",
        "Hao Zhu",
        "Ping Tan",
        "Zihan Zhou"
      ],
      "abstract": "SpatialLM is a large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object boxes with their semantic categories. Unlike previous methods which exploit task-specific network designs, our model adheres to the standard multimodal LLM architecture and is fine-tuned directly from open-source LLMs.\n\nTo train SpatialLM, we collect a large-scale, high-quality synthetic dataset consisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with ground-truth 3D annotations, and conduct a careful study on various modeling and training decisions. On public benchmarks, our model gives state-of-the-art performance in layout estimation and competitive results in 3D object detection. With that, we show a feasible path for enhancing the spatial understanding capabilities of modern LLMs for applications in augmented reality, embodied robotics, and more.",
      "arxiv_url": "https://openreview.net/forum?id=u1lNQH5upa",
      "pdf_url": "https://openreview.net/pdf/22f930c6b44c852e1c53aa7784df786168735e5d.pdf",
      "primary_category": "Structured Indoor Modeling, Large Language Models, Spatial Understanding",
      "categories": [
        "Structured Indoor Modeling",
        "Large Language Models",
        "Spatial Understanding",
        "3D Reconstruction"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yHi8Ao6GAe",
      "title": "MAPLE: Multi-scale Attribute-enhanced Prompt Learning for Few-shot Whole Slide Image Classification",
      "authors": [
        "Junjie Zhou",
        "WEI SHAO",
        "Yagao Yue",
        "Wei Mu",
        "Peng Wan",
        "Qi Zhu",
        "Daoqiang Zhang"
      ],
      "abstract": "Prompt learning has emerged as a promising paradigm for adapting pre-trained vision-language models (VLMs) to few-shot whole slide image (WSI) classification by aligning visual features with textual representations, thereby reducing annotation cost and enhancing model generalization. Nevertheless, existing methods typically rely on slide-level prompts and fail to capture the subtype-specific phenotypic variations of histological entities (e.g., nuclei, glands) that are critical for cancer diagnosis. To address this gap, we propose Multi-scale Attribute-enhanced Prompt Learning (MAPLE), a hierarchical framework for few-shot WSI classification that jointly integrates multi-scale visual semantics and performs prediction at both the entity and slide levels. Specifically, we first leverage large language models (LLMs) to generate entity-level prompts that can help identify multi-scale histological entities and their phenotypic attributes, as well as slide-level prompts to capture global visual descriptions. Then, an entity-guided cross-attention module is proposed to generate entity-level features, followed by aligning with their corresponding subtype-specific attributes for fine-grained entity-level prediction. To enrich entity representations, we further develop a cross-scale entity graph learning module that can update these representations by capturing their semantic correlations within and across scales.  The refined representations are then aggregated into a slide-level representation and aligned with the corresponding prompts for slide-level prediction. Finally, we combine both entity-level and slide-level outputs to produce the final prediction results. Results on three cancer cohorts confirm the effectiveness of our approach in addressing few-shot pathology diagnosis tasks.",
      "arxiv_url": "https://openreview.net/forum?id=yHi8Ao6GAe",
      "pdf_url": "https://openreview.net/pdf/e0566ec2495763913c1fd8bc505a245e5f996758.pdf",
      "primary_category": "multiple instance learning, whole slide image classification, prompt learning",
      "categories": [
        "multiple instance learning",
        "whole slide image classification",
        "prompt learning",
        "vision-language model",
        "few-shot learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "D6w7wIN360",
      "title": "DynaPipe: Dynamic Layer Redistribution for Efficient Serving of LLMs with Pipeline Parallelism",
      "authors": [
        "HongXin Xu",
        "Tianyu Guo",
        "Xianwei Zhang"
      ],
      "abstract": "To accelerate large language model (LLM) inference, pipeline parallelism partitions model layers into sequential stages, each assigned to a different device for concurrent execution. However, this method often suffers from pipeline bubbles caused by imbalanced computation in the tail stage. While upstream stages focus solely on layer-forward operations, the final stage must also handle post-processing tasks like sampling, introducing significant latency. This uneven workload leads to pipeline misalignment, forcing upstream stages to idle and degrading overall performance. Existing frameworks typically distribute layers evenly across stages without accounting for computational load differences. To address this, we propose DynaPipe, a dynamic layer redistribution scheme that adaptively balances computation by predicting execution latency in real time. Moreover, we introduce an asynchronous key-value (KV) cache migration coordinator to enable\nnon-blocking layer redistribution during inference. Experiments on representative LLMs demonstrate that DynaPipe reduces average end-to-end request latency by 8% to 49% across diverse workloads, outperforming state-of-the-art pipeline parallelism systems.",
      "arxiv_url": "https://openreview.net/forum?id=D6w7wIN360",
      "pdf_url": "https://openreview.net/pdf/8504f4e482ff780ff39a92c3a62cfc54193c6957.pdf",
      "primary_category": "Large Language Model, Pipeline Parallelism, KV Cache",
      "categories": [
        "Large Language Model",
        "Pipeline Parallelism",
        "KV Cache",
        "Sample"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "PRCizVyL1K",
      "title": "SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models",
      "authors": [
        "Pingyi Chen",
        "Yujing Lou",
        "Shen Cao",
        "Jinhui Guo",
        "Lubin Fan",
        "Yue Wu",
        "Lin Yang",
        "Lizhuang Ma",
        "Jieping Ye"
      ],
      "abstract": "While vision language models (VLMs) excel in 2D semantic visual understanding, their ability to quantitatively reason about 3D spatial relationships remains under\u0002explored due to the deficiency of spatial representation ability of 2D images. In this paper, we analyze the problem hindering VLMs’ spatial understanding abilities and propose SD-VLM, a novel framework that significantly enhances fundamental spatial perception abilities of VLMs through two key contributions: (1) propose Massive Spatial Measuring and Understanding (MSMU) dataset with precise spatial annotations, and (2) introduce a simple depth positional encoding method strengthening VLMs’ spatial awareness. MSMU dataset includes massive quantitative spatial tasks with 700K QA pairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented samples. We have trained SD-VLM, a strong generalist VLM which shows superior quantitative spatial measuring and understanding capability. SD-VLM not only achieves state-of-the-art performance on our proposed MSMU-Bench, but also shows spatial generalization abilities on other spatial understanding benchmarks including Q-Spatial and SpatialRGPT\u0002Bench. Extensive experiments demonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and 25.56% respectively on MSMU-Bench. Code and models are released at https://github.com/cpystan/SD-VLM.",
      "arxiv_url": "https://openreview.net/forum?id=PRCizVyL1K",
      "pdf_url": "https://openreview.net/pdf/393c4a64fb26650685687e28b69293a2446786a7.pdf",
      "primary_category": "MLLM, CV, Spatial Understanding",
      "categories": [
        "MLLM",
        "CV",
        "Spatial Understanding"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ft971e7HH5",
      "title": "IneqSearch: Hybrid Reasoning for Olympiad Inequality Proofs",
      "authors": [
        "Zhaoqun Li",
        "Beishui Liao",
        "Qiwei Ye"
      ],
      "abstract": "Mathematicians have long employed decomposition techniques to prove inequalities, yet automating this process remains a significant challenge in computational mathematics. We introduce IneqSearch, a hybrid reasoning system that integrates symbolic computation with large language models (LLMs) to address this challenge. IneqSearch reformulates inequality proving as a structured search problem: identifying appropriate combinations of theorems that decompose expressions into non-negative components. The system combines a symbolic solver for deductive reasoning with an LLM-based agent for constructive proof exploration, effectively implementing methodologies observed in formal mathematical practice. A key contribution of IneqSearch is its iterative learning mechanism that systematically incorporates newly proven results into its theorem database, enabling knowledge acquisition during practice that enhances its capabilities without requiring human intervention. In empirical evaluation on 437 Olympiad-level inequalities, IneqSearch successfully proves 342 problems, significantly outperforming existing methods and demonstrating the effectiveness of integrating symbolic and neural approaches for mathematical reasoning.",
      "arxiv_url": "https://openreview.net/forum?id=ft971e7HH5",
      "pdf_url": "https://openreview.net/pdf/d8313d5519ba8a62477586ab85f6b092ce6f7339.pdf",
      "primary_category": "Olympiad Inequality Proving, Neuro-Symbolic Method",
      "categories": [
        "Olympiad Inequality Proving",
        "Neuro-Symbolic Method"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "JEqlA8N88d",
      "title": "No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization",
      "authors": [
        "Wenhang Shi",
        "Yiren Chen",
        "Shuqing Bian",
        "Xinyi Zhang",
        "Kai Tang",
        "Pengfei Hu",
        "Zhe Zhao",
        "WEI LU",
        "Xiaoyong Du"
      ],
      "abstract": "Prompt engineering is crucial for leveraging the full potential of large language models (LLMs). While automatic prompt optimization offers a scalable alternative to costly manual design, generating effective prompts remains challenging. Existing methods often struggle to stably generate improved prompts, leading to low efficiency, and overlook that prompt optimization easily gets trapped in local optima. Addressing this, we propose GRACE, a framework that integrates two synergistic strategies: Gated Refinement and Adaptive Compression, achieving Efficient prompt optimization. The gated refinement strategy introduces a feedback regulation gate and an update rejection gate, which refine update signals to produce stable and effective prompt improvements. When optimization stagnates, the adaptive compression strategy distills the prompt’s core concepts, restructuring the optimization trace and opening new paths. By strategically introducing information loss through refinement and compression, GRACE delivers substantial gains in performance and efficiency. In extensive experiments on 11 tasks across three practical domains, including BIG-Bench Hard (BBH), domain-specific, and general NLP tasks, GRACE achieves significant average relative performance improvements of 4.7\\%, 4.4\\% and 2.7\\% over state-of-the-art methods, respectively. Further analysis shows that GRACE achieves these gains using only 25\\% of the prompt generation budget required by prior methods, highlighting its high optimization efficiency and low computational overhead. Our code is available at https://github.com/Eric8932/GRACE.",
      "arxiv_url": "https://openreview.net/forum?id=JEqlA8N88d",
      "pdf_url": "https://openreview.net/pdf/2b128b7e323e1c21bb947a8115b169efb216c8b2.pdf",
      "primary_category": "Large Language Models, Automatic Prompt Optimization",
      "categories": [
        "Large Language Models",
        "Automatic Prompt Optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "gUPGGCM4WH",
      "title": "Chain-of-Retrieval Augmented Generation",
      "authors": [
        "Liang Wang",
        "Haonan Chen",
        "Nan Yang",
        "Xiaolong Huang",
        "Zhicheng Dou",
        "Furu Wei"
      ],
      "abstract": "This paper introduces an approach for training o1-like RAG models that retrieve and reason over relevant information step by step before generating the final answer. Conventional RAG methods usually perform a single retrieval step before the generation process, which limits their effectiveness in addressing complex queries due to imperfect retrieval results. In contrast, our proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the model to dynamically reformulate the query based on the evolving state. To train CoRAG effectively, we utilize rejection sampling to automatically generate intermediate retrieval chains, thereby augmenting existing RAG datasets that only provide the correct final answer. At test time, we propose various decoding strategies to scale the model's test-time compute by controlling the length and number of sampled retrieval chains. Experimental results across multiple benchmarks validate the efficacy of CoRAG, particularly in multi-hop question answering tasks, where we observe more than $10$ points improvement in EM score compared to strong baselines. On the KILT benchmark, CoRAG establishes a new state-of-the-art performance across a diverse range of knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to understand the scaling behavior of CoRAG, laying the groundwork for future research aimed at developing factual and grounded foundation models.",
      "arxiv_url": "https://openreview.net/forum?id=gUPGGCM4WH",
      "pdf_url": "https://openreview.net/pdf/5a6faf62c69cb2470dc5a358601ce9be1015dcdb.pdf",
      "primary_category": "retrieval-augmented generation, scaling law, multi-hop reasoning",
      "categories": [
        "retrieval-augmented generation",
        "scaling law",
        "multi-hop reasoning"
      ],
      "tags": [
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "a2JTVVvcEl",
      "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
      "authors": [
        "Kaituo Feng",
        "Kaixiong Gong",
        "Bohao Li",
        "Zonghao Guo",
        "Yibing Wang",
        "Tianshuo Peng",
        "Junfei Wu",
        "Xiaoying Zhang",
        "Benyou Wang",
        "Xiangyu Yue"
      ],
      "abstract": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for incentivizing video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-CoT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 37.1\\% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All code, models, and data will be released.",
      "arxiv_url": "https://openreview.net/forum?id=a2JTVVvcEl",
      "pdf_url": "https://openreview.net/pdf/88e4718ed4321b178136effa3621642582b63792.pdf",
      "primary_category": "Multimodal Large Language Models, Video Reasoning",
      "categories": [
        "Multimodal Large Language Models",
        "Video Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xgvSwwlLah",
      "title": "Efficiently Maintaining the Multilingual Capacity of MCLIP in Downstream Cross-Modal Retrieval Tasks",
      "authors": [
        "Fengmao Lv",
        "Jitong Lei",
        "Guosheng Lin",
        "Desheng ZHENG",
        "Jianyang Zhang",
        "Tianrui Li"
      ],
      "abstract": "While existing research on Multilingual CLIP (MCLIP) has prioritized model architecture design, our work uncovers a critical challenge in practical adaptation: fine-tuning MCLIP through a single source language risks diminishing its multilingual capabilities in downstream tasks due to cross-linguistic disparities. To bridge this gap, we systematically investigate the role of ‌token similarity‌ in cross-lingual transferability for image-text retrieval, establishing it as a key factor governing fine-tuning efficacy. Building on this insight, we propose two novel strategies to enhance efficiency while preserving multilinguality: 1) ‌TaPCL dynamically optimizes training by prioritizing linguistically distant language pairs during corpus sampling, reducing redundant computation, and 2) CiPCL enriches the source corpus with multilingual key terms, enabling targeted knowledge transfer without reliance on exhaustive parallel data. By strategically balancing token similarity and domain-critical information, our methods significantly lower computational costs and mitigate over-dependence on parallel corpora. Experimental evaluations across diverse datasets validate the effectiveness and scalability of our framework, demonstrating robust multilingual retention across languages. This work provides a principled pathway for adapting MCLIP to real-world scenarios, where computational efficiency and cross-lingual robustness are paramount. Our codes are available at https://github.com/tiggers23/TaPCL-CiPCL.",
      "arxiv_url": "https://openreview.net/forum?id=xgvSwwlLah",
      "pdf_url": "https://openreview.net/pdf/fc5c7656d44692d6e150860f2342ea6d08543843.pdf",
      "primary_category": "multilingual CLIP, multilingual learning, cross-lingual transfer",
      "categories": [
        "multilingual CLIP",
        "multilingual learning",
        "cross-lingual transfer"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4VKVUmE1I8",
      "title": "RAGRouter: Learning to Route Queries to Multiple Retrieval-Augmented Language Models",
      "authors": [
        "Jiarui Zhang",
        "Xiangyu Liu",
        "Yong Hu",
        "Chaoyue Niu",
        "Fan Wu",
        "Guihai Chen"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) significantly improves the performance of Large Language Models (LLMs) on knowledge-intensive tasks. However, varying response quality across LLMs under RAG necessitates intelligent routing mechanisms, which select the most suitable model for each query from multiple retrieval-augmented LLMs via a dedicated router model. We observe that external documents dynamically affect LLMs' ability to answer queries, while existing routing methods, which rely on static parametric knowledge representations, exhibit suboptimal performance in RAG scenarios. To address this, we formally define the new retrieval-augmented LLM routing problem, incorporating the influence of retrieved documents into the routing framework. We propose RAGRouter, a RAG-aware routing design, which leverages document embeddings and RAG capability embeddings with contrastive learning to capture knowledge representation shifts and enable informed routing decisions. Extensive experiments on diverse knowledge-intensive tasks and retrieval settings, covering open and closed-source LLMs, show that RAGRouter outperforms the best individual LLM and existing routing methods. With an extended score-threshold-based mechanism, it also achieves strong performance-efficiency trade-offs under low-latency constraints. The code and data are available at https://github.com/OwwO99/RAGRouter.",
      "arxiv_url": "https://openreview.net/forum?id=4VKVUmE1I8",
      "pdf_url": "https://openreview.net/pdf/8f68f92dc478f3973bdaeafa880e5011aa1d3784.pdf",
      "primary_category": "LLM Routing, Retrieval-Augmented Generation, Large Language Models",
      "categories": [
        "LLM Routing",
        "Retrieval-Augmented Generation",
        "Large Language Models"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "klOr9y9nMU",
      "title": "CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs",
      "authors": [
        "Gucongcong Fan",
        "Chaoyue Niu",
        "chengfei lv",
        "Fan Wu",
        "Guihai Chen"
      ],
      "abstract": "Mobile agents rely on Large Language Models (LLMs) to plan and execute tasks on smartphone user interfaces (UIs). While cloud-based LLMs achieve high task accuracy, they require uploading the full UI state at every step, exposing unnecessary and often irrelevant information. In contrast, local LLMs avoid UI uploads but suffer from limited capacity, resulting in lower task success rates. We propose $\\textbf{CORE}$, a $\\textbf{CO}$llaborative framework that combines the strengths of cloud and local LLMs to $\\textbf{R}$educe UI $\\textbf{E}$xposure, while maintaining task accuracy for mobile agents. CORE comprises three key components: (1) $\\textbf{Layout-aware block partitioning}$, which groups semantically related UI elements based on the XML screen hierarchy; (2) $\\textbf{Co-planning}$, where local and cloud LLMs collaboratively identify the current sub-task; and (3) $\\textbf{Co-decision-making}$, where the local LLM ranks relevant UI blocks, and the cloud LLM selects specific UI elements within the top-ranked block. CORE further introduces a multi-round accumulation mechanism to mitigate local misjudgment or limited context. Experiments across diverse mobile apps and tasks show that CORE reduces UI exposure by up to 55.6\\% while maintaining task success rates slightly below cloud-only agents, effectively mitigating unnecessary privacy exposure to the cloud. The code is available at https://github.com/Entropy-Fighter/CORE.",
      "arxiv_url": "https://openreview.net/forum?id=klOr9y9nMU",
      "pdf_url": "https://openreview.net/pdf/5b9c63b9b600fd293798deb51960a50373bb0faf.pdf",
      "primary_category": "Mobile Agents, Large Language Models, Collaboration",
      "categories": [
        "Mobile Agents",
        "Large Language Models",
        "Collaboration",
        "UI Exposure Reduction"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2YPypUwWIv",
      "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO",
      "authors": [
        "Jinyoung Park",
        "Jeehye Na",
        "Jinyoung Kim",
        "Hyunwoo J. Kim"
      ],
      "abstract": "Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training for enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success using a PPO-style reinforcement learning algorithm with group-normalized rewards. However, the effectiveness of GRPO in Video Large Language Models (VideoLLMs) remains underexplored. In this paper, we explore GRPO and identify two issues that hinder effective learning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation. Reg-GRPO reformulates the GRPO loss function as a regression task that directly predicts the advantage in GRPO, eliminating the need for safeguards such as clipping and min operations. This directly aligns the model with the advantages, providing guidance to prefer better outputs. The difficulty-aware data augmentation strategy augments input prompts/videos to target solvable difficulty levels, enabling diverse reward signals. Our experimental results show that our approach significantly improves video reasoning performance across multiple benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=2YPypUwWIv",
      "pdf_url": "https://openreview.net/pdf/402ab9bf321fbec77bbebb639cf4f899b3831836.pdf",
      "primary_category": "Video Large Language Model, Post-training, GRPO",
      "categories": [
        "Video Large Language Model",
        "Post-training",
        "GRPO"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "e40dYCosQd",
      "title": "SilentStriker: Toward Stealthy Bit-Flip Attacks on Large Language Models",
      "authors": [
        "HAOTIAN XU",
        "Qingsong Peng",
        "Jie Shi",
        "Huadi Zheng",
        "YU LI",
        "Cheng Zhuo"
      ],
      "abstract": "The rapid adoption of large language models (LLMs) in critical domains has spurred extensive research into their security issues. While input manipulation attacks (e.g., prompt injection) have been well-studied, Bit-Flip Attacks (BFAs)—which exploit hardware vulnerabilities to corrupt model parameters and cause severe performance degradation—have received far less attention. Existing BFA methods suffer from key limitations: they fail to balance performance degradation and output naturalness, making them prone to discovery. In this paper, we introduce SilentStriker, the first stealthy bit-flip attack against LLMs that effectively degrades task performance while maintaining output naturalness. Our core contribution lies in addressing the challenge of designing effective loss functions for LLMs with variable output length and the vast output space. Unlike prior approaches that rely on output perplexity for attack loss formulation, which in-evidently degrade the output naturalness, we reformulate the attack objective by leveraging key output tokens as targets for suppression, enabling effective joint optimization of attack effectiveness and stealthiness. Additionally, we employ an iterative, progressive search strategy to maximize attack efficacy. Experiments show that SilentStriker significantly outperforms existing baselines, achieving successful attacks without compromising the naturalness of generated text.",
      "arxiv_url": "https://openreview.net/forum?id=e40dYCosQd",
      "pdf_url": "https://openreview.net/pdf/660f7e225727c7cbd467dbf397d6d0551fcbfb1d.pdf",
      "primary_category": "Large Language Models, Security, Bit-Flip Attack",
      "categories": [
        "Large Language Models",
        "Security",
        "Bit-Flip Attack",
        "Stealthy Attack"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "x2BsIdJJJW",
      "title": "ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding",
      "authors": [
        "Jialiang Kang",
        "Han Shu",
        "Wenshuo Li",
        "Yingjie Zhai",
        "Xinghao Chen"
      ],
      "abstract": "Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), yet its application to vision-language models (VLMs) remains underexplored, with existing methods achieving only modest speedups ($<1.5\\times$). This gap is increasingly significant as multimodal capabilities become central to large-scale models. We hypothesize that large VLMs can effectively filter redundant image information layer by layer without compromising textual comprehension, whereas smaller draft models struggle to do so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor module to compress image tokens into a compact representation, which is seamlessly integrated into the draft model's attention mechanism while preserving original image positional information. Additionally, we extract a global feature vector for each input image and augment all subsequent text tokens with this feature to enhance multimodal coherence. To overcome the scarcity of multimodal datasets with long assistant responses, we curate a specialized training dataset by repurposing existing datasets and generating extended outputs using the target VLM with modified prompts. Our training strategy mitigates the risk of the draft model exploiting direct access to the target model's hidden states, which could otherwise lead to shortcut learning when training solely on target model outputs. Extensive experiments validate ViSpec, achieving, to our knowledge, the first substantial speedup in VLM speculative decoding.",
      "arxiv_url": "https://openreview.net/forum?id=x2BsIdJJJW",
      "pdf_url": "https://openreview.net/pdf/5cc1cc1fbfadc72f4c5f96b94d29671e352d2f1f.pdf",
      "primary_category": "vision-language models, speculative decoding, synthetic dataset",
      "categories": [
        "vision-language models",
        "speculative decoding",
        "synthetic dataset"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ppKDXf55lY",
      "title": "DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment",
      "authors": [
        "Sangwoo Kwon",
        "Seong Hoon Seo",
        "Jae W. Lee",
        "Yeonhong Park"
      ],
      "abstract": "How can we effectively handle queries for on-device large language models (LLMs) with varying runtime constraints, such as latency and accuracy? Multi-scale quantization addresses this challenge by enabling memory-efficient runtime model adaptation of LLMs through the overlaying of multiple model variants quantized to different bitwidths. Meanwhile, an important question still remains open-ended: how can models be properly configured to match a target precision or latency? While mixed-precision offers a promising solution, we take this further by leveraging the key observation that the sensitivity of each layer dynamically changes across decoding steps. Building on this insight, we introduce DP-LLM, a novel mechanism that dynamically assigns precision to each layer based on input values. Experimental results across multiple models and benchmarks demonstrate that DP-LLM achieves a superior performance-latency trade-off, outperforming prior approaches.",
      "arxiv_url": "https://openreview.net/forum?id=ppKDXf55lY",
      "pdf_url": "https://openreview.net/pdf/523b6f8097aa032b4da47977d854556f0113ad67.pdf",
      "primary_category": "LLM Quantization, LLM Inference, Efficiency",
      "categories": [
        "LLM Quantization",
        "LLM Inference",
        "Efficiency",
        "ML System"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "mzxGGzeLCL",
      "title": "EAReranker: Efficient Embedding Adequacy Assessment for Retrieval Augmented Generation",
      "authors": [
        "Dongyang Zeng",
        "Yaping Liu",
        "Wei Zhang",
        "Shuo Zhang",
        "Xinwang Liu",
        "Binxing Fang"
      ],
      "abstract": "With the increasing adoption of Retrieval-Augmented Generation (RAG) systems for knowledge-intensive tasks, ensuring the adequacy of retrieved documents has become critically important for generation quality. Traditional reranking approaches face three significant challenges: substantial computational overhead that scales with document length, dependency on plain text that limits application in sensitive scenarios, and insufficient assessment of document value beyond simple relevance metrics.  We propose EAReranker, an efficient embedding-based adequacy assessment framework that evaluates document utility for RAG systems without requiring access to original text content. The framework quantifies document adequacy through a comprehensive scoring methodology considering verifiability, coverage, completeness and structural aspects, providing interpretable adequacy classifications for downstream applications. EAReranker employs a Decoder-Only Transformer architecture that introduces embedding dimension expansion method and bin-aware weighted loss, designed specifically to predict adequacy directly from embedding vectors. Our comprehensive evaluation across four public benchmarks demonstrates that EAReranker achieves competitive performance with state-of-the-art plaintext rerankers while maintaining constant memory usage ($\\sim$550MB) regardless of input length and processing 2-3x faster than traditional approaches. The semantic bin adequacy prediction accuracy of 92.85\\% LACC@10 and 86.12\\% LACC@25 demonstrates its capability to effectively filter out inadequate documents that could potentially mislead or adversely impact RAG system performance, thereby ensuring only high-utility information serves as generation context. These results establish EAReranker as an efficient and practical solution for enhancing RAG system performance through improved context selection while addressing the computational and privacy challenges of existing methods.",
      "arxiv_url": "https://openreview.net/forum?id=mzxGGzeLCL",
      "pdf_url": "https://openreview.net/pdf/37139b624df8d6e1749c2afb3331b33301f2277e.pdf",
      "primary_category": "Retrieval-Augmented Generation, Embedding-based Reranking, Document Adequacy Assessment",
      "categories": [
        "Retrieval-Augmented Generation",
        "Embedding-based Reranking",
        "Document Adequacy Assessment"
      ],
      "tags": [
        "RAG",
        "Information Retrieval"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2jzGEudVdS",
      "title": "Synergy over Discrepancy: A Partition-Based Approach to Multi-Domain LLM Fine-Tuning",
      "authors": [
        "Hua Ye",
        "Siyuan Chen",
        "Haoliang Zhang",
        "Weihao Luo",
        "Yanbin Li",
        "Xuan Zhang"
      ],
      "abstract": "Large language models (LLMs) demonstrate impressive generalization abilities, yet adapting them effectively across multiple heterogeneous domains remains challenging due to inter-domain interference. To overcome this challenge, we propose a partition-based multi-stage fine-tuning framework designed to exploit inter-domain synergies while minimizing negative transfer. Our approach strategically partitions domains into subsets (stages) by balancing domain discrepancy, synergy, and model capacity constraints. We theoretically analyze the proposed framework and derive novel generalization bounds that justify our partitioning strategy. Extensive empirical evaluations on various language understanding tasks show that our method consistently outperforms state-of-the-art baselines.",
      "arxiv_url": "https://openreview.net/forum?id=2jzGEudVdS",
      "pdf_url": "https://openreview.net/pdf/447318ed71a3d34a21f4acd6fb712ce11df0ca1a.pdf",
      "primary_category": "Fine-Tuning, Multi-Domain",
      "categories": [
        "Fine-Tuning",
        "Multi-Domain"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "UFAKqq77e3",
      "title": "Whose Instructions Count? Resolving Preference Bias in Instruction Fine-Tuning",
      "authors": [
        "Jiayu Zhang",
        "Changbang Li",
        "Yinan Peng",
        "Weihao Luo",
        "Peilai Yu",
        "Xuan Zhang"
      ],
      "abstract": "Instruction fine-tuning (IFT) has emerged as a ubiquitous strategy for specializing large language models (LLMs), yet it implicitly assumes a single, coherent \"ground-truth\" preference behind all human-written instructions. In practice, annotators differ in the styles, emphases, and granularities they prefer, introducing preference bias that can erode both robustness and generalization. We propose Dynamic Cross-Layer Preference Correction (\\textsc{DCPC}), it couples (i) a preference-sensitive similarity estimator that detects mismatched instructional cues, (ii) cross-layer prefix alignment to reconcile semantic representations across transformer layers, and (iii) a lightweight Preference Correction Module (PCM) that dynamically adjusts hidden states to honor the inferred dominant preference.   On five Super/GLUE tasks and the Alpaca set—plus six preference-shifted variants—DCPC boosts accuracy/F1-EM by 4.0–6.7 points and gpt-score by +0.7, while cutting inter-seed variance up to 35% on LlaMA-2 13B and Mistral-7B, setting a new state of the art for robust instruction tuning.",
      "arxiv_url": "https://openreview.net/forum?id=UFAKqq77e3",
      "pdf_url": "https://openreview.net/pdf/6f8f94be640aed4cbe3b7c2eab36611d85b89c1d.pdf",
      "primary_category": "Instruction Fine-Tuning, Preference Bias, Self-Supervised Learning",
      "categories": [
        "Instruction Fine-Tuning",
        "Preference Bias",
        "Self-Supervised Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BpbJc1Jfbv",
      "title": "EchoShot: Multi-Shot Portrait Video Generation",
      "authors": [
        "Jiahao Wang",
        "Hualian Sheng",
        "Sijia Cai",
        "Weizhan Zhang",
        "Caixia Yan",
        "Yachuang Feng",
        "Bing Deng",
        "Jieping Ye"
      ],
      "abstract": "Video diffusion models substantially boost the productivity of artistic workflows with high-quality portrait video generative capacity. However, prevailing pipelines are primarily constrained to single-shot creation, while real-world applications urge multiple shots with identity consistency and flexible content controllability. In this work, we propose EchoShot, a native and scalable multi-shot framework for portrait customization built upon a foundation video diffusion model. To start with, we propose shot-aware position embedding mechanisms within the video diffusion transformer architecture to model inter-shot variations and establish intricate correspondence between multi-shot visual content and their textual descriptions. This simple yet effective design enables direct training on multi-shot video data without introducing additional computational overhead. To facilitate model training within multi-shot scenarios, we construct PortraitGala, a large-scale and high-fidelity human-centric video dataset featuring cross-shot identity consistency and fine-grained captions such as facial attributes, outfits, and dynamic motions. To further enhance applicability,  we extend EchoShot to perform reference image-based personalized multi-shot generation and long video synthesis with infinite shot counts. Extensive evaluations demonstrate that EchoShot achieves superior identity consistency as well as attribute-level controllability in multi-shot portrait video generation. Notably, the proposed framework demonstrates potential as a foundational paradigm for general multi-shot video modeling. Project page: https://johnneywang.github.io/EchoShot-webpage.",
      "arxiv_url": "https://openreview.net/forum?id=BpbJc1Jfbv",
      "pdf_url": "https://openreview.net/pdf/7d2c435dfd1310a9a3e717745425c0269505a0a6.pdf",
      "primary_category": "Portrait Video Generation, Multi-shot Video Modeling, Rotary Position Embedding",
      "categories": [
        "Portrait Video Generation",
        "Multi-shot Video Modeling",
        "Rotary Position Embedding",
        "Portrait Video Dataset"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "dpllevHMbc",
      "title": "Functional Scaling Laws in Kernel Regression: Loss Dynamics and Learning Rate Schedules",
      "authors": [
        "Binghui Li",
        "Fengling Chen",
        "Zixun Huang",
        "Lean Wang",
        "Lei Wu"
      ],
      "abstract": "Scaling laws have emerged as a unifying lens for understanding and guiding the training of large language models (LLMs).  However, existing studies predominantly focus on the final-step loss, leaving open whether the entire $\\textit{loss dynamics}$ obey similar laws and, crucially, how the $\\textit{learning rate schedule}$ (LRS) shapes them. We address these gaps in a controlled theoretical setting by analyzing stochastic gradient descent (SGD) on a power-law kernel regression model. The key insight is a novel $\\textbf{intrinsic-time}$ viewpoint, which captures the training progress more faithfully than iteration count. We then establish a $\\textbf{Functional Scaling Law (FSL)}$ that captures the full loss trajectory under arbitrary LRSs, with the schedule’s influence entering through a simple convolutional functional. We further instantiate the theory for three representative LRSs---constant, exponential decay, and warmup–stable–decay (WSD)---and derive explicit scaling relations in both data- and compute-limited regimes. These comparisons explain key empirical phenomena: (i) higher-capacity models are more data- and compute-efficient; (ii) learning-rate decay improves training efficiency; and (iii) WSD-type schedules outperform pure decay. Finally, experiments on LLMs ranging from 0.1B to 1B parameters demonstrate the practical relevance of FSL as a surrogate model for fitting and predicting loss trajectories in large-scale pre-training.",
      "arxiv_url": "https://openreview.net/forum?id=dpllevHMbc",
      "pdf_url": "https://openreview.net/pdf/71f646ca71a0242b6aacb785b855df43d7c3e823.pdf",
      "primary_category": "Scaling Laws, Learning Rate Schedule, Kernel Regression",
      "categories": [
        "Scaling Laws",
        "Learning Rate Schedule",
        "Kernel Regression",
        "LLM Pre-Training",
        "Loss Dynamics"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1nL84tQNnK",
      "title": "Conformal Prediction for Causal Effects of Continuous Treatments",
      "authors": [
        "Maresa Schröder",
        "Dennis Frauen",
        "Jonas Schweisthal",
        "Konstantin Hess",
        "Valentyn Melnychuk",
        "Stefan Feuerriegel"
      ],
      "abstract": "Uncertainty quantification of causal effects is crucial for safety-critical applications such as personalized medicine. A powerful approach for this is conformal prediction, which has several practical benefits due to model-agnostic finite-sample guarantees. Yet, existing methods for conformal prediction of causal effects are limited to binary/discrete treatments and make highly restrictive assumptions, such as known propensity scores. In this work, we provide a novel conformal prediction method for potential outcomes of continuous treatments. We account for the additional uncertainty introduced through propensity estimation so that our conformal prediction intervals are valid even if the propensity score is unknown. Our contributions are three-fold: (1) We derive finite-sample validity guarantees for prediction intervals of potential outcomes of continuous treatments. (2) We provide an algorithm for calculating the derived intervals. (3) We demonstrate the effectiveness of the conformal prediction intervals in experiments on synthetic and real-world datasets. To the best of our knowledge, we are the first to propose conformal prediction for continuous treatments when the propensity score is unknown and must be estimated from data.",
      "arxiv_url": "https://openreview.net/forum?id=1nL84tQNnK",
      "pdf_url": "https://openreview.net/pdf/aeaafa3f93e6cbe6330acda208d855db3458c34e.pdf",
      "primary_category": "causality, dosage response curves, conformal prediction",
      "categories": [
        "causality",
        "dosage response curves",
        "conformal prediction",
        "uncertainty quantification"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "gm65gK3uOJ",
      "title": "Glance2Gaze: Efficient Vision-Language Models from Glance Fusion to Gaze Compression",
      "authors": [
        "Juan Chen",
        "Honglin liu",
        "Yingying Ao",
        "Ting Zhang",
        "Yan Huang",
        "Xudong Liu",
        "Biao Li",
        "Jintao Fang"
      ],
      "abstract": "Vision-language models heavily rely on visual representations, yet ensuring its efficiency remains a critical challenge. Most existing approaches focus on reducing visual tokens either at the visual encoder phase or during the LLM decoder stage. Inspired by human visual cognition, where an initial global glance precedes focused attention on semantically salient regions, we introduce Glance2Gaze, a cognitively inspired framework that mimics the human two-stage attention process. The framework consists of two key components: the Glance Fusion module, which integrates multi-layer vision transformer features with text-aware attention to generate a semantically enriched global representation, and the Gaze Compression module, which utilizes a novel query-guided mechanism to selectively compress visual tokens based on their semantic relevance. Experimental results on widely adopted benchmarks demonstrate that Glance2Gaze outperforms existing methods, achieving superior performance with equal or lower computational cost. Furthermore, it generalizes well to high-resolution and video scenarios, showcasing robust and scalable efficiency improvements in VLMs.",
      "arxiv_url": "https://openreview.net/forum?id=gm65gK3uOJ",
      "pdf_url": "https://openreview.net/pdf/cace55a64d956ca69ab6942930e4d833d27e5559.pdf",
      "primary_category": "Multimodal large language models; Vision-Language Models;",
      "categories": [
        "Multimodal large language models; Vision-Language Models;"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "WPHpBnKvdq",
      "title": "Multi-agent KTO: Enhancing Strategic Interactions of Large Language Model in Language Game",
      "authors": [
        "Rong Ye",
        "Yongxin Zhang",
        "Yikai Zhang",
        "Haoyu Kuang",
        "peng sun",
        "zhongyu wei"
      ],
      "abstract": "Achieving Artificial General Intelligence (AGI) requires AI agents that can not only make strategic decisions but also engage in flexible and meaningful communication. Inspired by Wittgenstein's language game theory, we propose that language agents can learn through in-context interaction rather than traditional multi-stage frameworks that separate decision-making from language expression. Using Werewolf, a social deduction game that tests language understanding, strategic interaction, and adaptability, as a test bed, we develop the Multi-agent Kahneman-Tversky's Optimization (MaKTO). MaKTO engages diverse models in extensive gameplay to generate unpaired desirable and unacceptable responses, then employs KTO to refine the model's decision-making process. In 9-player Werewolf games, MaKTO achieves a 61% average win rate across various models, outperforming GPT-4o and two-stage RL agents by relative improvements of 23.0% and 10.9%, respectively. Notably, MaKTO also demonstrates human-like performance, winning 60% against expert players and showing only 48.9% detectability in Turing-style blind tests. Code and data are available at project page https://reneeye.github.io/MaKTO.html.",
      "arxiv_url": "https://openreview.net/forum?id=WPHpBnKvdq",
      "pdf_url": "https://openreview.net/pdf/d3da558992765f632692ff672c1a0e216ecaff06.pdf",
      "primary_category": "Large Language Model, Language Game, Application",
      "categories": [
        "Large Language Model",
        "Language Game",
        "Application"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "I4PJYZvfW5",
      "title": "Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator",
      "authors": [
        "Beier Luo",
        "Shuoyuan Wang",
        "Sharon Li",
        "Hongxin Wei"
      ],
      "abstract": "Post-training of large language models is essential for adapting pre-trained language models (PLMs) to align with human preferences and downstream tasks. \nWhile PLMs typically exhibit well-calibrated confidence, post-trained language models (PoLMs) often suffer from over-confidence, assigning high confidence to both correct and incorrect outputs, which can undermine reliability in critical applications.\nA major obstacle in calibrating PoLMs is the scarcity of labeled data for individual downstream tasks. \nTo address this, we propose Disagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to optimize the parameters (e.g., temperature $\\tau$) in post-hoc confidence calibration. \nOur method is motivated by the under-confidence issue caused by prediction disagreement between the PLM and PoLM while aligning their confidence via temperature scaling. Theoretically, the PLM's confidence underestimates PoLM's prediction accuracy on disagreement examples, causing a larger $\\tau$ and producing under-confident predictions. DACA mitigates this by selectively using only agreement examples for calibration, effectively decoupling the influence of disagreement.\nIn this manner, our method avoids an overly large $\\tau$ in temperature scaling caused by disagreement examples, improving calibration performance.\nExtensive experiments demonstrate the effectiveness of our method, improving the average ECE of open-sourced and API-based LLMs (e.g. GPT-4o) by up to 15.08$\\%$ on common benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=I4PJYZvfW5",
      "pdf_url": "https://openreview.net/pdf/93af86faea181727876e693c424c5f07fd124766.pdf",
      "primary_category": "Confidence Calibration, LLM",
      "categories": [
        "Confidence Calibration",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qNNjudaNhb",
      "title": "One Head to Rule Them All: Amplifying LVLM Safety through a Single Critical Attention Head",
      "authors": [
        "Junhao Xia",
        "Haotian Zhu",
        "Shuchao Pang",
        "Zhigang Lu",
        "Bing Li",
        "Yongbin Zhou",
        "Jason Xue"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities in tasks requiring multimodal understanding. However, recent studies indicate that LVLMs are more vulnerable than LLMs to unsafe inputs and prone to generating harmful content. Existing defense strategies primarily include fine-tuning, input sanitization, and output intervention. Although these approaches provide a certain level of protection, they tend to be resource-intensive and struggle to effectively counter sophisticated attack techniques. To tackle such issues, we propose One-head Defense (Oh Defense), a novel yet simple approach utilizing LVLMs' internal safety capabilities. Through systematic analysis of the attention mechanisms, we discover that LVLMs' safety capabilities are concentrated within specific attention heads that respond differently to safe or unsafe inputs. Further exploration reveals that a single critical attention head can effectively serve as a safety guard, providing a strong discriminative signal that amplifies the model's inherent safety capabilities. Hence, the Oh Defense requires no additional training or external modules, making it computationally efficient while effectively reactivating suppressed safety mechanisms. Extensive experiments across diverse LVLM architectures and unsafe datasets validate our approach, i.e., the Oh Defense achieves near-perfect defense success rates (> 98\\%) for unsafe inputs while maintaining low false positive rates (< 5\\%) for safe content. The source code is available at https://github.com/AIASLab/Oh-Defense.",
      "arxiv_url": "https://openreview.net/forum?id=qNNjudaNhb",
      "pdf_url": "https://openreview.net/pdf/c578302a72a59021c814c33157b9826b5817b940.pdf",
      "primary_category": "Large Vision-Language Models, Security, Adversarial and Jailbreak attacks",
      "categories": [
        "Large Vision-Language Models",
        "Security",
        "Adversarial and Jailbreak attacks"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "77eEDRhPkQ",
      "title": "DAPO : Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage-Based Policy Optimization",
      "authors": [
        "Jiacai Liu",
        "Chaojie Wang",
        "Chris Yuhao Liu",
        "Liang Zeng",
        "Rui Yan",
        "Yiwen Sun",
        "Yang Liu"
      ],
      "abstract": "The role of reinforcement learning (RL) in enhancing the reasoning of large language models (LLMs) is becoming increasingly significant. Despite the success of RL in many scenarios, there are still many challenges in improving the reasoning of LLMs. One key challenge is the sparse reward, which introduces more training variance in policy optimization and makes it difficult to obtain a good estimation for value function in Actor-Critic (AC) methods.  To address these issues, we introduce Direct Advantage-Based Policy Optimization (DAPO), a novel step-level offline RL algorithm  with theoretical guarantees for enhancing the reasoning abilities of LLMs. Unlike response-level methods (such as DPO and GRPO) that the update directions of all reasoning steps are governed by the outcome reward uniformly, DAPO employs a critic function to provide step-level dense signals for policy optimization. Additionally, the actor and critic in DAPO are trained independently, ensuring that critic is a good estimation of true state value function and avoiding the co-training instability observed in standard AC methods. We train DAPO on mathematical and code problems and then evaluate its performance on multiple benchmarks. Our results show that DAPO can effectively enhance the mathematical and code capabilities on both SFT models and RL models, demonstrating the effectiveness of DAPO.",
      "arxiv_url": "https://openreview.net/forum?id=77eEDRhPkQ",
      "pdf_url": "https://openreview.net/pdf/be1c0b5313f1e34cb0858d1d966b8d6e19d6afd6.pdf",
      "primary_category": "large language models, reasoning, alignment",
      "categories": [
        "large language models",
        "reasoning",
        "alignment",
        "multi-step RL",
        "offline RL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "TDFSKAspoQ",
      "title": "MGUP: A Momentum-Gradient Alignment Update Policy for Stochastic Optimization",
      "authors": [
        "Da Chang",
        "Ganzhao Yuan"
      ],
      "abstract": "Efficient optimization is essential for training large language models. Although intra-layer selective updates have been explored, a general mechanism that enables fine-grained control while ensuring convergence guarantees is still lacking. To bridge this gap, we propose \\textbf{MGUP}, a novel mechanism for selective updates. \\textbf{MGUP} augments standard momentum-based optimizers by applying larger step-sizes to a selected fixed proportion of parameters in each iteration, while applying smaller, non-zero step-sizes to the rest. As a nearly {plug-and-play} module, \\textbf{MGUP} seamlessly integrates with optimizers such as AdamW, Lion, and Muon. This yields powerful variants such as \\textbf{MGUP-AdamW}, \\textbf{MGUP-Lion}, and \\textbf{MGUP-Muon}. Under standard assumptions, we provide theoretical convergence guarantees for \\textbf{MGUP-AdamW} (without weight decay) in stochastic optimization. Extensive experiments across diverse tasks, including MAE pretraining, LLM pretraining, and downstream fine-tuning, demonstrate that our \\textbf{MGUP}-enhanced optimizers achieve superior or more stable performance compared to their original base optimizers. We offer a principled, versatile, and theoretically grounded strategy for efficient intra-layer selective updates, accelerating and stabilizing the training of large-scale models. The code is publicly available at https://github.com/MaeChd/MGUP.",
      "arxiv_url": "https://openreview.net/forum?id=TDFSKAspoQ",
      "pdf_url": "https://openreview.net/pdf/106ecdcc2b8ad7549b92b28b9bf4087cd5c12e7d.pdf",
      "primary_category": "Stochastic Optimization; Convergence Analysis; Adaptive Optimizers ; Nonconvex Optimization; Large Language Models; Momentum-Gradient Alignment",
      "categories": [
        "Stochastic Optimization; Convergence Analysis; Adaptive Optimizers ; Nonconvex Optimization; Large Language Models; Momentum-Gradient Alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7wEvjzkNXg",
      "title": "Unified Reinforcement and Imitation Learning for Vision-Language Models",
      "authors": [
        "Byung-Kwan Lee",
        "Ryo Hachiuma",
        "Yong Man Ro",
        "Yu-Chiang Frank Wang",
        "Yueh-Hua Wu"
      ],
      "abstract": "Vision-Language Models (VLMs) have achieved remarkable progress, yet their large scale often renders them impractical for resource-constrained environments. This paper introduces Unified Reinforcement and Imitation Learning (RIL), a novel and efficient training algorithm designed to create powerful, lightweight VLMs. RIL distinctively combines the strengths of reinforcement learning with adversarial imitation learning. This enables smaller student VLMs not only to mimic the sophisticated text generation of large teacher models but also to systematically improve their generative capabilities through reinforcement signals. Key to our imitation framework is a LLM-based discriminator that adeptly distinguishes between student and teacher outputs, complemented by guidance from multiple large teacher VLMs to ensure diverse learning. This unified learning strategy, leveraging both reinforcement and imitation, empowers student models to achieve significant performance gains, making them competitive with leading closed-source VLMs. Extensive experiments on diverse vision-language benchmarks demonstrate that RIL significantly narrows the performance gap with state-of-the-art open- and closed-source VLMs and, in several instances, surpasses them.",
      "arxiv_url": "https://openreview.net/forum?id=7wEvjzkNXg",
      "pdf_url": "https://openreview.net/pdf/2579be81d9a81207d2d8e18a6e46e9d3739caac7.pdf",
      "primary_category": "Vision-Language Models",
      "categories": [
        "Vision-Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8FN25PlktS",
      "title": "Adaptive Batch-Wise Sample Scheduling for Direct Preference Optimization",
      "authors": [
        "Zixuan Huang",
        "Yikun Ban",
        "Lean Fu",
        "Xiaojie Li",
        "Zhongxiang Dai",
        "Jianxin Li",
        "deqing wang"
      ],
      "abstract": "Direct Preference Optimization (DPO) has emerged as an effective approach for aligning large language models (LLMs) with human preferences. However, its performance is highly dependent on the quality of the underlying human preference data. To address this bottleneck, prior work has explored various data selection strategies,  but these methods often overlook the impact of the evolving states of the language model during the optimization process.\nIn this paper, we introduce a novel problem: Sample Scheduling for DPO, which aims to dynamically and adaptively schedule training samples based on the model's evolving batch-wise states throughout preference optimization. To solve this problem, we propose SamS, an efficient and effective algorithm that adaptively selects samples in each training batch based on the LLM's learning feedback to maximize the potential generalization performance.\nNotably, without modifying the core DPO algorithm, simply integrating SamS significantly improves performance across tasks, with minimal additional computational overhead. \nThis work points to a promising new direction for improving LLM alignment through batch-wise sample selection, with potential generalization to RLHF and broader supervised learning paradigms.",
      "arxiv_url": "https://openreview.net/forum?id=8FN25PlktS",
      "pdf_url": "https://openreview.net/pdf/c150f4ed4e4459a8689553f5058a3b3b63d45035.pdf",
      "primary_category": "Reinforcement Learning, Direct Preference Optimization",
      "categories": [
        "Reinforcement Learning",
        "Direct Preference Optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3pORFyKzh1",
      "title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning",
      "authors": [
        "Qi Wang",
        "Yanrui Yu",
        "Ye Yuan",
        "Rui Mao",
        "Tianfei Zhou"
      ],
      "abstract": "Reinforcement fine-tuning (RFT) has shown great promise in achieving humanlevel reasoning capabilities of Large Language Models (LLMs), and has recently been extended to MLLMs. Nevertheless, reasoning about videos, which is a fundamental aspect of human intelligence, remains a persistent challenge due to the complex logic, temporal and causal structures inherent in video data. To fill this gap, we propose VideoRFT, a novel approach that extends the RFT paradigm to cultivate human-like video reasoning capabilities in MLLMs. VideoRFT follows the standard two-stage scheme in RFT: supervised fine-tuning (SFT) with chain-of-thought (CoT) annotations, followed by reinforcement learning (RL) to improve generalization. A central challenge to achieve this in the video domain lies in the scarcity of large-scale, high-quality video CoT datasets. We address this by building a multi-expert-driven, cognition-inspired CoT curation pipeline. First, we devise a cognition-inspired prompting strategy to elicit a reasoning LLM to generate preliminary CoTs based solely on rich, structured, and literal representations of video content. Subsequently, these CoTs are revised by a MLLM conditioned on the actual video, ensuring visual consistency and reducing visual hallucinations. This pipeline results in two new datasets, i.e.VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To further strengthen the RL phase, we introduce a novel semantic-consistency reward that explicitly promotes the alignment between textual reasoning and visual evidence. This reward encourages the model to produce coherent, context-aware reasoning outputs grounded in visual input. Extensive experiments show that VideoRFT achieves state-of-the-art performance on six video reasoning benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=3pORFyKzh1",
      "pdf_url": "https://openreview.net/pdf/e9e5329cdcc660a432b172dfdeb67afae9045051.pdf",
      "primary_category": "Multimodal Large Language Models, Video Reasoning, Reinforced fine-tuning",
      "categories": [
        "Multimodal Large Language Models",
        "Video Reasoning",
        "Reinforced fine-tuning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pIZxEOZCId",
      "title": "TabDPT: Scaling Tabular Foundation Models on Real Data",
      "authors": [
        "Junwei Ma",
        "Valentin Thomas",
        "Rasa Hosseinzadeh",
        "Alex Labach",
        "Jesse C. Cresswell",
        "Keyvan Golestan",
        "Guangwei Yu",
        "Anthony L. Caterini",
        "Maksims Volkovs"
      ],
      "abstract": "Tabular data is one of the most ubiquitous sources of information worldwide, spanning a wide variety of domains. This inherent heterogeneity has slowed the development of Tabular Foundation Models (TFMs) capable of fast generalization to unseen datasets. In-Context Learning (ICL) has recently emerged as a promising solution for TFMs, enabling dynamic adaptation to new tasks without additional tuning. While many studies have attempted to re-purpose large language models for tabular ICL, they have had limited success, so recent works have focused on developing tabular-specific foundation models. In this work, we propose an approach to combine ICL-based retrieval with self supervised learning to train tabular foundation models. We also investigate the utility of real vs. synthetic data for model pre-training, and show that real data can contain useful signal not easily captured in synthetic training. Specifically, we show that incorporating real data during the pre-training phase can lead to significantly faster training and better downstream generalization to unseen data. Our resulting model, **TabDPT**, achieves strong performance on both regression (CTR23) and classification (CC18) benchmarks. Importantly, we also demonstrate that with our pre-training procedure, scaling both model and data size leads to consistent performance improvements that follow power laws. This echoes scaling laws in LLMs and other foundation models, and suggests that large-scale TFMs can be achievable.\nWe open-source our full pipeline: inference code including trained model weights can be found [here](https://github.com/layer6ai-labs/TabDPT-inference), and the training code to reproduce experiments can be found [here](https://github.com/layer6ai-labs/TabDPT-training).",
      "arxiv_url": "https://openreview.net/forum?id=pIZxEOZCId",
      "pdf_url": "https://openreview.net/pdf/8a62ff5ceb105dc3afce4aa2d1dfe332d05d2af9.pdf",
      "primary_category": "Tabular Foundation Models, In-Context Learning, Self Supervised Learning",
      "categories": [
        "Tabular Foundation Models",
        "In-Context Learning",
        "Self Supervised Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IKVkpjSJzJ",
      "title": "EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths",
      "authors": [
        "Zhening Li",
        "Armando Solar-Lezama",
        "Yisong Yue",
        "Stephan Zheng"
      ],
      "abstract": "We introduce a new approach to *agent programming*, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce *probabilistic angelic nondeterminism* (PAN), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.",
      "arxiv_url": "https://openreview.net/forum?id=IKVkpjSJzJ",
      "pdf_url": "https://openreview.net/pdf/b95a88bc7dd044e15af6e09eb3340c743d2df9a8.pdf",
      "primary_category": "AI agents, large language models, agent frameworks",
      "categories": [
        "AI agents",
        "large language models",
        "agent frameworks",
        "angelic nondeterminism",
        "inference-time strategies",
        "test-time scaling"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qTXlFwlggv",
      "title": "Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations",
      "authors": [
        "Li Ji-An",
        "Hua-Dong Xiong",
        "Robert Wilson",
        "Marcelo G Mattar",
        "Marcus K. Benna"
      ],
      "abstract": "Large language models (LLMs) can sometimes report the strategies they actually use to solve tasks, yet at other times seem unable to recognize those strategies that govern their behavior. This suggests a limited degree of metacognition --- the capacity to monitor one's own cognitive processes for subsequent reporting and self-control. Metacognition enhances LLMs' capabilities in solving complex tasks but also raises safety concerns, as models may obfuscate their internal processes to evade neural-activation-based oversight (e.g., safety detector). Given society's increased reliance on these models, it is critical that we understand their metacognitive abilities. To address this, we introduce a neuroscience-inspired \\emph{neurofeedback} paradigm that uses in-context learning to quantify metacognitive abilities of LLMs to \\textit{report} and \\textit{control} their activation patterns. \nWe demonstrate that their abilities depend on several factors: the number of in-context examples provided, the semantic interpretability of the neural activation direction (to be reported/controlled), and the variance explained by that direction. These directions span a ``metacognitive space'' with dimensionality much lower than the model's neural space, suggesting LLMs can monitor only a small subset of their neural activations. Our paradigm provides empirical evidence to quantify metacognition in LLMs, with significant implications for AI safety (e.g., adversarial attack and defense).",
      "arxiv_url": "https://openreview.net/forum?id=qTXlFwlggv",
      "pdf_url": "https://openreview.net/pdf/c20785c8a52c617399f46f8ca8642d6bbc2249ff.pdf",
      "primary_category": "large language model, metacognition, safety",
      "categories": [
        "large language model",
        "metacognition",
        "safety",
        "neurofeedback",
        "in-context learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IYVknFxsJb",
      "title": "System Prompt Optimization with Meta-Learning",
      "authors": [
        "Yumin Choi",
        "Jinheon Baek",
        "Sung Ju Hwang"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance. However, while LLM prompts consist of both the task-agnostic system prompts and task-specific user prompts, existing work on prompt optimization has focused on user prompts specific to individual queries or tasks, and largely overlooked the system prompt that is, once optimized, applicable across different tasks and domains. Motivated by this, we introduce the novel problem of bilevel system prompt optimization, whose objective is to design system prompts that are robust to diverse user prompts and transferable to unseen tasks. To tackle this problem, we then propose a meta-learning framework, which meta-learns the system prompt by optimizing it over various user prompts across multiple datasets, while simultaneously updating the user prompts in an iterative manner to ensure synergy between them. We conduct experiments on 14 unseen datasets spanning 5 different domains, on which we show that our approach produces system prompts that generalize effectively to diverse user prompts. Also, our findings reveal that the optimized system prompt enables rapid adaptation even to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance.",
      "arxiv_url": "https://openreview.net/forum?id=IYVknFxsJb",
      "pdf_url": "https://openreview.net/pdf/5950932e5a47ffa0dc543a67e0f60e5c76e5c100.pdf",
      "primary_category": "Prompt Optimization, Meta-Learning, System Prompt Optimization",
      "categories": [
        "Prompt Optimization",
        "Meta-Learning",
        "System Prompt Optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "d4mZyZB5I9",
      "title": "Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving",
      "authors": [
        "Fangzhou Wu",
        "Sandeep Silwal"
      ],
      "abstract": "Increasing demand for Large Language Models (LLMs) services imposes substantial deployment and computation costs on providers. \nLLM routing offers a cost-efficient solution by directing queries to the optimal LLM based on model and query features.\nHowever, existing works primarily focus on offline scenarios and struggle to adapt to online settings with high query volume and constrained token budgets.\nIn this work, we introduce the first training-free algorithm for online routing scenarios.\nOur algorithm leverages approximate nearest neighbor search to efficiently estimate the features of queries and performs a one-time optimization over a small set of initial queries to learn a set of routing weights that guide future routing.\nWe provide a theoretical guarantee that the algorithm achieves a competitive ratio of $1 - o(1)$ under natural assumptions, which is further validated by extensive experiments across 3 benchmark datasets and 8 baselines, showing an average improvement of 3.55$\\times$ in performance, 1.85$\\times$ in cost efficiency, and nearly 4.25$\\times$ in throughput.\nOur code is available at https://github.com/fzwark/PORT.",
      "arxiv_url": "https://openreview.net/forum?id=d4mZyZB5I9",
      "pdf_url": "https://openreview.net/pdf/463ab498b4d2ef7b5a0cafd7ac4276fc50fa3cb1.pdf",
      "primary_category": "Large Language Models, LLM Routing, Cost-Aware Inference",
      "categories": [
        "Large Language Models",
        "LLM Routing",
        "Cost-Aware Inference",
        "Online Algorithm",
        "Multi-LLM Serving"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9YkEcAqiIK",
      "title": "Lifelong Safety Alignment for Language Models",
      "authors": [
        "Haoyu Wang",
        "Yifei Zhao",
        "Zeyu Qin",
        "Chao Du",
        "Min Lin",
        "Xueqian Wang",
        "Tianyu Pang"
      ],
      "abstract": "LLMs have made impressive progress, but their growing capabilities also expose them to highly flexible jailbreaking attacks designed to bypass safety alignment. While many existing defenses focus on known types of attacks, it is more critical to prepare LLMs for *unseen* attacks that may arise during deployment. To address this, we propose a **lifelong safety alignment** framework that enables LLMs to continuously adapt to new and evolving jailbreaking strategies. Our framework introduces a competitive setup between two components: a **Meta-Attacker**, trained to actively discover novel jailbreaking strategies, and a **Defender**, trained to resist them. To effectively warm up the Meta-Attacker, we first leverage the GPT-4o API to extract key insights from a large collection of jailbreak-related research papers. Through iterative training, the first iteration Meta-Attacker achieves a 73% attack success rate (ASR) on RR and a 57% transfer ASR on LAT using only *single-turn* attacks. Meanwhile, the Defender progressively improves its robustness and ultimately reduces the Meta-Attacker's success rate to just 7%, enabling safer and more reliable deployment of LLMs in open-ended environments.",
      "arxiv_url": "https://openreview.net/forum?id=9YkEcAqiIK",
      "pdf_url": "https://openreview.net/pdf/8b599899e65f157c99b0a6ac3e8b45afbc4fee1a.pdf",
      "primary_category": "Safety Alignment, Language Models",
      "categories": [
        "Safety Alignment",
        "Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "rAuRLePL2R",
      "title": "Týr-the-Pruner: Structural Pruning LLMs via Global Sparsity Distribution Optimization",
      "authors": [
        "Guanchen Li",
        "Yixing Xu",
        "Zeping Li",
        "Ji Liu",
        "Xuanwu Yin",
        "Dong Li",
        "Emad Barsoum"
      ],
      "abstract": "Structural pruning enhances hardware-agnostic inference efficiency for large language models (LLMs) yet often fails to maintain comparable performance. Local pruning performs efficient layer-by-layer compression but ignores global topology. Although global pruning aims to identify an optimal sparse model, intuitive methods typically adopt a two-stage paradigm that first evaluates substructure saliency and then applies global pruning, which ignores inter-structure dependencies and fails to achieve end-to-end optimization. To address these limitations, we propose Týr-the-Pruner, an efficient end-to-end search-based global structural pruning framework. This framework constructs a supernet by repeatedly applying local pruning across a range of sparsity ratios to each layer in an LLM, with the core goal of determining the optimal sparsity distribution under a target overall sparsity ratio. Concretely, we introduce an effective local pruning and an expectation error accumulation approach to improve supernet construction. Furthermore, we employ an iterative prune-and-search strategy with coarse-to-fine sparsity granularity to ensure efficient search convergence. Experimental results show that Týr-the-Pruner achieves state-of-the-art structural pruning, retaining 97% of the dense model's performance while removing a challenging 50% of Llama-3.1-70B's parameters.",
      "arxiv_url": "https://openreview.net/forum?id=rAuRLePL2R",
      "pdf_url": "https://openreview.net/pdf/b8d4af71f4e823cde143ab63fa780a2fddb141b0.pdf",
      "primary_category": "Structural Pruning, Large Language Models, Sparsity Distribution Optimization",
      "categories": [
        "Structural Pruning",
        "Large Language Models",
        "Sparsity Distribution Optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vTWNVYuvuF",
      "title": "Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning",
      "authors": [
        "Jiayu Wang",
        "Yifei Ming",
        "Zixuan Ke",
        "Caiming Xiong",
        "Shafiq Joty",
        "Aws Albarghouthi",
        "Frederic Sala"
      ],
      "abstract": "Reinforcement learning (RL) has become the dominant paradigm for improving the performance of language models on complex reasoning tasks. Despite the substantial empirical gains demonstrated by RL-based training methods like GRPO, a granular understanding of why and how RL enhances performance is still lacking. To bridge this gap, we introduce SPARKLE, a fine-grained analytic framework to dissect the effects of RL across three key dimensions: (1) plan following and execution, (2) knowledge integration, and (3) chain of subproblems. Using this framework, we gain insights beyond mere accuracy. For instance, providing models with explicit human-crafted, step-by-step plans can surprisingly degrade performance on the most challenging benchmarks, yet RL-tuned models exhibit greater robustness, experiencing markedly smaller performance drops than base or SFT models. This suggests that RL may not primarily enhance the execution of external plans but rather empower models to formulate and follow internal strategies better suited to their reasoning processes. Conversely, we observe that RL enhances models' ability to integrate provided knowledge into their reasoning process, yielding consistent gains across diverse tasks. Finally, we study whether difficult problems---those yielding no RL signals and mixed-quality reasoning traces---can still be effectively used for training. We introduce SparkleRL-PSS, a multi-stage RL pipeline that reuses hard problems with partial step scaffolding, guiding exploration effectively without additional data generation. Together, our findings provide a principled foundation for understanding how RL shapes model behavior, offering practical insights for building more adaptive, data-efficient, and interpretable RL pipelines for reasoning tasks. Our code, data, and checkpoints are available at: https://sparkle-reasoning.github.io/.",
      "arxiv_url": "https://openreview.net/forum?id=vTWNVYuvuF",
      "pdf_url": "https://openreview.net/pdf/110e32186e208c949feb60e663a9f14c3e76244f.pdf",
      "primary_category": "Large Language Models, Mathematical Reasoning, Reinforcement Learning",
      "categories": [
        "Large Language Models",
        "Mathematical Reasoning",
        "Reinforcement Learning",
        "Evaluation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9nlTapr2sd",
      "title": "Dual-Comb Ghost Imaging with Transformer-Based Reconstruction for Optical Fiber Endomicroscopy",
      "authors": [
        "David Dang",
        "Myoung-Gyun Suh",
        "Maodong Gao",
        "ByoungJun Park",
        "Beyonce Hu",
        "Yucheng Jin",
        "Wilton J. M. Kort-Kamp",
        "Ho Wai Howard Lee"
      ],
      "abstract": "Endoscopic imaging is indispensable for visualizing internal organs, yet conventional systems remain bulky and costly because they rely on large, multi-element optics, which limits their ability to access and image certain areas of the body. Achieving high-quality endomicroscopy with hundred micron-scale and inexpensive hardware remains a grand challenge. Optical fibers offer a sub-millimeter-scale imaging conduit that could meet this need, but existing fiber-based approaches typically require either raster scanning or multicore bundles, which limit resolution and speed of imaging. In this work, we overcome these limitations by combining dual-comb interferometry with optical ghost imaging and advanced algorithm. Optical frequency combs enable precise and parallel speckle illumination via wavelength-division multiplexing through a single-core fiber, while our dual-comb compressive ghost imaging approach enables snapshot detection of bucket-sum signals using a single-pixel detector, eliminating the need for both spatial and spectral scanning. To reconstruct images from these highly compressed measurements, we introduce Optical Ghost-GPT, a transformer-based image reconstruction model that enables fast, high-fidelity recovery at low sampling rates. Our dual-comb ghost imaging approach, combined with the novel algorithm, outperforms classical ghost imaging techniques in both speed and accuracy, enabling real-time, high-resolution endoscopic imaging with a significantly reduced device footprint. This advancement paves the way for non-invasive, high-resolution, low-cost endomicroscopy and other sensing applications constrained by hardware size and complexity.",
      "arxiv_url": "https://openreview.net/forum?id=9nlTapr2sd",
      "pdf_url": "https://openreview.net/pdf/701826af28ba9b7d63fa018eb0850ecdf6f13bf4.pdf",
      "primary_category": "Ghost Imaging, Sparse Sensing, Optics",
      "categories": [
        "Ghost Imaging",
        "Sparse Sensing",
        "Optics",
        "Transformers"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "OH7U836jKk",
      "title": "CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs",
      "authors": [
        "Gunho Park",
        "Jeongin Bae",
        "Byeongwook Kim",
        "Baeseong park",
        "Jiwon Ryu",
        "Hoseung Kim",
        "Se Jung Kwon",
        "Dongsoo Lee"
      ],
      "abstract": "Weight-only quantization is widely used to mitigate the memory-bound nature of LLM inference. Codebook-based methods extend this trend by achieving strong accuracy in the extremely low-bit regime (e.g., 2-bit). However, current kernels rely on dequantization, which repeatedly fetches centroids and reconstructs weights, incurring substantial latency and cache pressure. We present CodeGEMM, a codebook-centric GEMM kernel that replaces dequantization with precomputed inner products between centroids and activations stored in a lightweight Psumbook. At inference, code indices directly gather these partial sums, eliminating per-element lookups and reducing the on-chip footprint. The kernel supports the systematic exploration of latency–memory–accuracy trade-offs under a unified implementation.\nOn Llama-3 models, CodeGEMM delivers 1.83x (8B) and 8.93x (70B) speedups in the 2-bit configuration compared to state-of-the-art codebook-based quantization at comparable accuracy and further improves computing efficiency and memory subsystem utilization.",
      "arxiv_url": "https://openreview.net/forum?id=OH7U836jKk",
      "pdf_url": "https://openreview.net/pdf/98c8317e24f515c34310b87b823b3caf648cc1d9.pdf",
      "primary_category": "Quantization, Matrix Multiplication, Large Language Models",
      "categories": [
        "Quantization",
        "Matrix Multiplication",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QYJt0pX0zJ",
      "title": "Virus Infection Attack on LLMs: Your Poisoning Can Spread \"VIA\" Synthetic Data",
      "authors": [
        "Zi Liang",
        "Qingqing Ye",
        "Xuan Liu",
        "Yanyun Wang",
        "Jianliang Xu",
        "Haibo Hu"
      ],
      "abstract": "Synthetic data refers to artificial samples generated by models. While it has been validated to significantly enhance the performance of large language models (LLMs) during training and has been widely adopted in LLM development, potential security risks it may introduce remain uninvestigated. This paper systematically evaluates the resilience of synthetic-data-integrated training paradigm for LLMs against mainstream poisoning and backdoor attacks. We reveal that such a paradigm exhibits strong resistance to existing attacks, primarily thanks to the different distribution patterns between poisoning data and queries used to generate synthetic samples. To enhance the effectiveness of these attacks and further investigate the security risks introduced by synthetic data, we introduce a novel and universal attack framework, namely, Virus Infection Attack (VIA), which enables the propagation of current attacks through synthetic data even under purely clean queries. Inspired by the principles of virus design in cybersecurity, VIA conceals the poisoning payload within a protective “shell” and strategically searches for optimal hijacking points in benign samples to maximize the likelihood of generating malicious content. Extensive experiments on both data poisoning and backdoor attacks show that VIA significantly increases the presence of poisoning content in synthetic data and correspondingly raises the attack success rate (ASR) on downstream models to levels comparable to those observed in the poisoned upstream models.",
      "arxiv_url": "https://openreview.net/forum?id=QYJt0pX0zJ",
      "pdf_url": "https://openreview.net/pdf/4314d097789b9ce667a3dcd9e84db87e31c26f92.pdf",
      "primary_category": "Data Poisoning, Synthetic Data, Large Language Models",
      "categories": [
        "Data Poisoning",
        "Synthetic Data",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "D1Iw4Unvfc",
      "title": "ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding",
      "authors": [
        "Yiyang Zhou",
        "Yangfan He",
        "Yaofeng Su",
        "Siwei Han",
        "Joel Jang",
        "Gedas Bertasius",
        "Mohit Bansal",
        "Huaxiu Yao"
      ],
      "abstract": "Video understanding is fundamental to tasks such as action recognition, video reasoning, and robotic control. Early video understanding methods based on large vision-language models (LVLMs) typically adopt a single-pass reasoning paradigm without dynamic feedback, limiting the model’s capacity to self-correct and adapt in complex scenarios. Recent efforts have attempted to address this limitation by incorporating reward models and reinforcement learning to enhance reasoning, or by employing tool-agent frameworks. However, these approaches face several challenges, including high annotation costs, reward signals that fail to capture real-time reasoning states, and low inference efficiency. To overcome these issues, we propose ReAgent-V, a novel agentic video understanding framework that integrates efficient frame selection with real-time reward generation during inference. These reward signals not only guide iterative answer refinement through a multi-perspective reflection mechanism—adjusting predictions from conservative, neutral, and aggressive viewpoints—but also enable automatic filtering of high-quality data for supervised fine-tuning (SFT), direct preference optimization (DPO), and group relative policy optimization (GRPO). ReAgent-V is lightweight, modular, and extensible, supporting flexible tool integration tailored to diverse tasks. Extensive experiments on 12 datasets across three core applications—video understanding, video reasoning enhancement, and vision-language-action model alignment—demonstrate significant gains in generalization and reasoning, with improvements of up to 6.9%, 2.1%, and 9.8%, respectively, highlighting the effectiveness and versatility of the proposed framework.",
      "arxiv_url": "https://openreview.net/forum?id=D1Iw4Unvfc",
      "pdf_url": "https://openreview.net/pdf/6d161d5f3d882ca658ad735bc6b9ad5b35e62169.pdf",
      "primary_category": "Video understanding; Multi-agent framework; Reflective reasoning; VLA alignment; Video reasoning",
      "categories": [
        "Video understanding; Multi-agent framework; Reflective reasoning; VLA alignment; Video reasoning"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DqRbfiTdKK",
      "title": "LLM at Network Edge: A Layer-wise Efficient Federated Fine-tuning Approach",
      "authors": [
        "Jinglong Shen",
        "Nan Cheng",
        "Wenchao Xu",
        "Haozhao Wang",
        "Yifan Guo",
        "Jiajie Xu"
      ],
      "abstract": "Fine-tuning large language models (LLMs) poses significant computational burdens, especially in federated learning (FL) settings. We introduce Layer-wise Efficient Federated Fine-tuning (LEFF), a novel method designed to enhance the efficiency of FL fine-tuning while preserving model performance and minimizing client-side computational overhead. LEFF strategically selects layers for fine-tuning based on client computational capacity, thereby mitigating the straggler effect prevalent in heterogeneous environments. Furthermore, LEFF incorporates an importance-driven layer sampling mechanism, prioritizing layers with greater influence on model performance. Theoretical analysis demonstrates that LEFF achieves a convergence rate of $\\mathcal{O}(1/\\sqrt{T})$. Extensive experiments on diverse datasets demonstrate that LEFF attains superior computational efficiency and model performance compared to existing federated fine-tuning methods, particularly under heterogeneous conditions.",
      "arxiv_url": "https://openreview.net/forum?id=DqRbfiTdKK",
      "pdf_url": "https://openreview.net/pdf/4a214b518e425bc717fbf54e4404d51c15e641e4.pdf",
      "primary_category": "federated learning, large language model, fine-tuning",
      "categories": [
        "federated learning",
        "large language model",
        "fine-tuning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7OdU0LYXLr",
      "title": "On the Loss of Context Awareness in General Instruction Fine-tuning",
      "authors": [
        "Yihan Wang",
        "Andrew Bai",
        "Nanyun Peng",
        "Cho-Jui Hsieh"
      ],
      "abstract": "Pre-trained Large Language Models (LLMs) require post-training methods such as supervised fine-tuning (SFT) on instruction-response pairs to enable instruction following. However, this process can cause forgetting in capabilities learned during pre-training. In this paper, we investigate the loss of context awareness after SFT, where context awareness is defined as the ability to extract and understand information from user-provided context. % and respond accordingly. Surprisingly, we discovered that the loss of context awareness occurs in instruction fine-tuned LLMs when the chat template is applied to input prompts. We identify that the performance decline is associated with a bias toward different roles learned during conversational instruction fine-tuning. The bias can be traced to training samples where the assistant response minimally relies on the user-provided instruction. Based on these observations, we propose a metric to identify context-dependent examples from general instruction fine-tuning datasets. We then apply conditional instruction fine-tuning with a context-dependency indicator, enabling the model to preserve context awareness after SFT. Experiments on four context-dependent downstream tasks and three pre-trained LLMs of different sizes show that our method effectively mitigates the loss of context awareness without compromising general instruction-following capabilities.",
      "arxiv_url": "https://openreview.net/forum?id=7OdU0LYXLr",
      "pdf_url": "https://openreview.net/pdf/4e7225fddbfb46fd09f93659364c5ff93ba5217d.pdf",
      "primary_category": "Instruction fine-tuning, context awareness, catastrophic forgetting",
      "categories": [
        "Instruction fine-tuning",
        "context awareness",
        "catastrophic forgetting"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1ktdvp1EYI",
      "title": "Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning",
      "authors": [
        "Chao-Chung Wu",
        "Zhi Rui Tam",
        "Chieh-Yen Lin",
        "Yun-Nung Chen",
        "Shao-Hua Sun",
        "Hung-yi Lee"
      ],
      "abstract": "Maintaining consistent model performance across domains is a fundamental challenge in machine learning. While recent work has explored using LLM-generated data for fine-tuning, its impact on cross-domain generalization remains poorly understood. This paper presents a systematic analysis revealing that fine-tuning with LLM-generated data not only improves target task performance but also reduces non-target task degradation compared to fine-tuning with ground truth data. Through analyzing the data sequence in tasks of various domains, we demonstrate that this enhancement of non-target task robustness stems from the reduction of high perplexity tokens found in LLM-generated sequences. Following our findings, we showed that masking high perplexity tokens in ground truth training data achieves similar non-target task performance preservation, comparable to using LLM-generated data. Extensive experiments across different model families and scales, including Gemma 2 IT 2B, Llama 3 8B Instruct, and three additional models, agree with our findings. To the best of our knowledge, this is the first work to provide an empirical explanation based on token perplexity reduction to mitigate catastrophic forgetting in LLMs after fine-tuning, offering valuable insights for developing more robust fine-tuning strategies.",
      "arxiv_url": "https://openreview.net/forum?id=1ktdvp1EYI",
      "pdf_url": "https://openreview.net/pdf/8757a2ebc0f92a3f1ee16d51c9f1096b157d2fca.pdf",
      "primary_category": "perplexity, robustness, LLM-generated data",
      "categories": [
        "perplexity",
        "robustness",
        "LLM-generated data",
        "performance degradation",
        "forgetting"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "5OnejG2SQH",
      "title": "Masked Gated Linear Unit",
      "authors": [
        "Yukito Tajima",
        "Nakamasa Inoue",
        "Yusuke Sekikawa",
        "Ikuro Sato",
        "Rio Yokota"
      ],
      "abstract": "Gated Linear Units (GLUs) have become essential components in the feed-forward networks of state-of-the-art Large Language Models (LLMs).\nHowever, they require twice as many memory reads compared to feed-forward layers without gating, due to the use of separate weight matrices for the gate and value streams.\nTo address this bottleneck, we introduce Masked Gated Linear Units (MGLUs),  a novel family of GLUs with an efficient kernel implementation.\nThe core contribution of MGLUs include:\n(1) the Mixture of Element-wise Gating (MoEG) architecture that learns multiple binary masks, each determining gate or value assignments at the element level on a single shared weight matrix resulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly kernel that yields up to a 19.7$\\times$ inference-time speed-up over a na\\\"ive PyTorch MGLU and is 47\\% more memory-efficient and 34\\% faster than standard GLUs despite added architectural complexity on an RTX5090 GPU.\nIn LLM experiments, the Swish-activated variant SwiMGLU preserves its memory advantages while matching—or even surpassing—the downstream accuracy of the SwiGLU baseline.",
      "arxiv_url": "https://openreview.net/forum?id=5OnejG2SQH",
      "pdf_url": "https://openreview.net/pdf/22794ba8531d3c67b3f070a88fb885f40e5a0f6c.pdf",
      "primary_category": "Activation, Sparsity, GPUs",
      "categories": [
        "Activation",
        "Sparsity",
        "GPUs",
        "LLM training",
        "LLM inference"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VwPt1WDQNB",
      "title": "Trajectory Balance with Asynchrony: Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training",
      "authors": [
        "Brian R. Bartoldson",
        "Siddarth Venkatraman",
        "James Diffenderfer",
        "Moksh Jain",
        "Tal Ben-Nun",
        "Seanie Lee",
        "Minsu Kim",
        "Johan Obando-Ceron",
        "Yoshua Bengio",
        "Bhavya Kailkhura"
      ],
      "abstract": "Reinforcement learning (RL) is a critical component of large language model (LLM) post-training. However, on-policy algorithms used for post-training are not naturally robust to a diversified content of experience replay buffers, which asynchronous off-policy actors can efficiently populate in parallel to training. We propose efficiently learning on such off-policy data via Trajectory Balance with Asynchrony (TBA), an approach to asynchronous RL for LLMs that leverages the principled off-policy TB objective. On math, preference-tuning, and automated red-teaming tasks, we post-train models ranging from Pythia 410M to Qwen 2.5 7B, finding TBA offers speed and performance boosts over strong baselines like Online DPO and Dr. GRPO. Beyond TBA's performance benefits (high accuracy even as asynchrony grows) and speedups ($4\\times$ or more), we show its reward- and recency-prioritizing sampling enable further gains as data generation is scaled. Our code is available at https://github.com/bbartoldson/TBA.",
      "arxiv_url": "https://openreview.net/forum?id=VwPt1WDQNB",
      "pdf_url": "https://openreview.net/pdf/13f871efbbbe385c376ef8105b8c5e2aaeddb542.pdf",
      "primary_category": "LLM, post-training, RL",
      "categories": [
        "LLM",
        "post-training",
        "RL",
        "async",
        "off-policy",
        "RLHF",
        "reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VNTj7PGlrz",
      "title": "Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs",
      "authors": [
        "Yunqi Hong",
        "Sohyun An",
        "Andrew Bai",
        "Neil Lin",
        "Cho-Jui Hsieh"
      ],
      "abstract": "Despite Multimodal Large Language Models (MLLMs) showing promising results on general zero-shot image classification tasks, fine-grained image classification remains challenging. It demands precise attention to subtle visual details to distinguish between visually similar subcategories—details that MLLMs may easily overlook without explicit guidance. To address this, we introduce AutoSEP, an iterative self-supervised prompt learning framework designed to enhance MLLM fine-grained classification capabilities in a fully unsupervised manner. Our core idea is to leverage unlabeled data to learn a description prompt that guides MLLMs in identifying crucial discriminative features within an image, and boost classification accuracy. We developed an automatic self-enhancing prompt learning framework called AutoSEP to iteratively improve the description prompt using unlabeled data, based on instance-level classification scoring function. AutoSEP only requires black-box access to MLLMs, eliminating the need for any training or fine-tuning. We evaluate our approach on multiple fine-grained classification datasets. It consistently outperforms other unsupervised baselines, demonstrating the effectiveness of our self-supervised optimization framework. Notably, AutoSEP in average improves 13\\% over standard zero-shot classification and 3\\% over the best-performing baselines.",
      "arxiv_url": "https://openreview.net/forum?id=VNTj7PGlrz",
      "pdf_url": "https://openreview.net/pdf/4395da638b1cd706cf06ae053fa3f3bd8db43038.pdf",
      "primary_category": "Fine-grained image classification, unsupervised prompt optimization",
      "categories": [
        "Fine-grained image classification",
        "unsupervised prompt optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "HKfZwLjSwQ",
      "title": "LLM Query Scheduling with Prefix Reuse and Latency Constraints",
      "authors": [
        "Gregory Dexter",
        "Shao Tang",
        "Ata Fatahibaarzi",
        "Qingquan Song",
        "Tejas Dharamsi",
        "Aman Gupta"
      ],
      "abstract": "The efficient deployment of large language models (LLMs) in online settings requires optimizing inference performance under stringent latency constraints, particularly the time-to-first-token (TTFT) and time-per-output-token (TPOT). This paper focuses on the query scheduling problem for LLM inference with prefix reuse, a technique that leverages shared prefixes across queries to reduce computational overhead. Our work reveals previously unknown limitations of the existing first-come-first-serve (FCFS) and longest-prefix-match (LPM) scheduling strategies with respect to satisfying latency constraints. We present a formal theoretical framework for LLM query scheduling under RadixAttention, a prefix reuse mechanism that stores and reuses intermediate representations in a radix tree structure. Our analysis establishes the NP-hardness of the scheduling problem with prefix reuse under TTFT constraints and proposes a novel scheduling algorithm, $k$-LPM, which generalizes existing methods by balancing prefix reuse and fairness in query processing. Theoretical guarantees demonstrate that $k$-LPM achieves improved TTFT performance under realistic traffic patterns captured by a data generative model. Empirical evaluations in a realistic serving setting validates our findings, showing significant reductions in P99 TTFT compared to baseline methods.",
      "arxiv_url": "https://openreview.net/forum?id=HKfZwLjSwQ",
      "pdf_url": "https://openreview.net/pdf/fdc2d3f31e21bc1bd6bb55f63f8850408fc26cf7.pdf",
      "primary_category": "queueing theory, scheduling theory, Large Language Models (LLMs)",
      "categories": [
        "queueing theory",
        "scheduling theory",
        "Large Language Models (LLMs)",
        "prefix caching"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "elfvik1mkg",
      "title": "When Can Model-Free Reinforcement Learning be Enough for Thinking?",
      "authors": [
        "Josiah P. Hanna",
        "Nicholas E. Corrado"
      ],
      "abstract": "Recent work on large language models has demonstrated the use of model-free reinforcement learning (RL) to train reasoning-like capabilities. The emergence of \"thinking\" through model-free RL is interesting as thinking actions neither produce reward nor change the external world state to one where the agent is more likely to get reward. This paper seeks to build a domain-independent understanding of when model-free RL will lead to such \"thinking\" as a strategy for reward maximization. To build this understanding, we first introduce a theoretical model which we call a thought Markov decision process (MDP). Thought MDPs minimally extend the classical MDP model to include an abstract notion of thought state and thought action. Using the thought MDP model, we prove the importance of policy initialization in determining whether or not thinking emerges and show formally that thought actions are equivalent to the agent choosing to perform a step of policy improvement before continuing to act. We then show that open-source LLMs satisfy the conditions that our theory predicts are necessary for model-free RL to produce thinking-like behavior. Finally, we hypothesize sufficient conditions that would enable thinking to be learned outside of language generation and introduce a toy domain where a combination of multi-task pre-training and designated thought actions enable more data-efficient RL compared to non-thinking agents.",
      "arxiv_url": "https://openreview.net/forum?id=elfvik1mkg",
      "pdf_url": "https://openreview.net/pdf/4684ec41a51576823f59b805a298c2038939d7be.pdf",
      "primary_category": "reinforcement learning, thinking, model-free",
      "categories": [
        "reinforcement learning",
        "thinking",
        "model-free",
        "planning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "bZ0MXXoldX",
      "title": "Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability",
      "authors": [
        "Yarden Bakish",
        "Itamar Zimerman",
        "Hila Chefer",
        "Lior Wolf"
      ],
      "abstract": "The development of effective explainability tools for Transformers is a crucial pursuit in deep learning research. One of the most promising approaches in this domain is Layer-wise Relevance Propagation (LRP), which propagates relevance scores backward through the network to the input space by redistributing activation values based on predefined rules. However, existing LRP-based methods for Transformer explainability entirely overlook a critical component of the Transformer architecture: its positional encoding (PE), resulting in violations of conservation, and the loss of an important and unique type of relevance, which is also associated with structural and positional features. To address this limitation, we reformulate the input space for Transformer explainability as a set of position-token pairs, rather than relying solely on the standard vocabulary space. This allows us to propose specialized theoretically-grounded LRP rules designed to propagate attributions across various positional encoding methods, including Rotary, Learned, and Absolute PE. Extensive experiments with both fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3, demonstrate that our method significantly outperforms the SoTA in both vision and NLP explainability tasks. Our code is provided as a supplement.",
      "arxiv_url": "https://openreview.net/forum?id=bZ0MXXoldX",
      "pdf_url": "https://openreview.net/pdf/f166dd5740c825cb0d7ff41994d899b05c4d43bc.pdf",
      "primary_category": "Explainability, Interpretability, Transformers",
      "categories": [
        "Explainability",
        "Interpretability",
        "Transformers",
        "LRP"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3AdTRYA2uJ",
      "title": "Cost-aware LLM-based Online Dataset Annotation",
      "authors": [
        "Eray Can Elumar",
        "Cem Tekin",
        "Osman Yagan"
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled automated dataset labeling with minimal human supervision. While majority voting across multiple LLMs can improve label reliability by mitigating individual model biases, it incurs high computational costs due to repeated querying. In this work, we propose a novel online framework, Cost-aware Majority Voting (CaMVo), for efficient and accurate LLM-based dataset annotation. CaMVo adaptively selects a subset of LLMs for each data instance based on contextual embeddings, balancing confidence and cost without requiring pre-training or ground-truth labels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator over confidence scores, CaMVo estimates a lower bound on labeling accuracy for each LLM and aggregates responses through weighted majority voting. Our empirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates that CaMVo achieves comparable or superior accuracy to full majority voting while significantly reducing labeling costs. This establishes CaMVo as a practical and robust solution for cost-efficient annotation in dynamic labeling environments.",
      "arxiv_url": "https://openreview.net/forum?id=3AdTRYA2uJ",
      "pdf_url": "https://openreview.net/pdf/5cf97e80c2be3a6041066e0ed5da77312fd17e85.pdf",
      "primary_category": "large language models (LLMs), dataset annotation, optimization",
      "categories": [
        "large language models (LLMs)",
        "dataset annotation",
        "optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Zd6VyjmN1S",
      "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism",
      "authors": [
        "Zedong Liu",
        "Shenggan Cheng",
        "Guangming Tan",
        "Yang You",
        "Dingwen Tao"
      ],
      "abstract": "Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components—combined with complex inference pipelines and heterogeneous workloads—introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2$\\times$ and achieving 3.2–4.5$\\times$ higher throughput while meeting service-level objectives (SLOs).",
      "arxiv_url": "https://openreview.net/forum?id=Zd6VyjmN1S",
      "pdf_url": "https://openreview.net/pdf/7250c50e4dd72db4731ced5dc34ecbc4256eaf90.pdf",
      "primary_category": "Hardware and Systems, Efficient Inference Methods, Distributed Inference",
      "categories": [
        "Hardware and Systems",
        "Efficient Inference Methods",
        "Distributed Inference",
        "Visual Question Answering"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VjjJlJ5qik",
      "title": "AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking",
      "authors": [
        "Xiangqi Wang",
        "Yue Huang",
        "Yanbo Wang",
        "Xiaonan Luo",
        "Kehan Guo",
        "Yujun Zhou",
        "Xiangliang Zhang"
      ],
      "abstract": "LLMs often need effective configurations, like temperature and reasoning steps, to handle tasks requiring sophisticated reasoning and problem-solving, ranging from joke generation to mathematical reasoning. Existing prompting approaches usually adopt general-purpose, fixed configurations that work “well enough” across tasks but seldom achieve task-specific optimality.\nTo address this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM to automate adaptive reasoning configurations for tasks requiring different types of thinking. AdaReasoner is trained using a reinforcement learning (RL) framework, combining a factorized action space with a targeted exploration strategy, along with  a pretrained reward model to optimize the policy model for reasoning configurations with only a few-shot guide.\nAdaReasoner is backed by theoretical guarantees and experiments of fast convergence and a sublinear policy gap. Across six different LLMs and a variety of reasoning tasks, it consistently outperforms standard baselines, preserves out-of-distribution robustness, \nand yield gains on knowledge-intensive tasks through tailored prompts.",
      "arxiv_url": "https://openreview.net/forum?id=VjjJlJ5qik",
      "pdf_url": "https://openreview.net/pdf/dd1128b101a8e529cfa219b76424da7861923e32.pdf",
      "primary_category": "Adaptive reasoning, LLM reasoning, Reinforcement learning",
      "categories": [
        "Adaptive reasoning",
        "LLM reasoning",
        "Reinforcement learning",
        "large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DxiP59Z81m",
      "title": "REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing",
      "authors": [
        "Weihan Xu",
        "Yimeng Ma",
        "Jingyue Huang",
        "Yang Li",
        "Wenye Ma",
        "Taylor Berg-Kirkpatrick",
        "Julian McAuley",
        "Paul Pu Liang",
        "Hao-Wen Dong"
      ],
      "abstract": "Short videos are an effective tool for promoting contents and improving knowledge accessibility. While existing extractive video summarization methods struggle to produce a coherent narrative, existing abstractive methods cannot `quote' from the input videos, i.e., inserting short video clips in their outputs. In this work, we explore novel video editing models for generating shorts that feature a coherent narrative with embedded video insertions extracted from a long input video. We propose a novel retrieval-embedded generation framework that allows a large language model to quote multimodal resources while maintaining a coherent narrative. Our proposed REGen system first generates the output story script with quote placeholders using a finetuned large language model, and then uses a novel retrieval model to replace the quote placeholders by selecting a video clip that best supports the narrative from a pool of candidate quotable video clips. We examine the proposed method on the task of documentary teaser generation, where short interview insertions are commonly used to support the narrative of a documentary. Our objective evaluations show that the proposed method can effectively insert short video clips while maintaining a coherent narrative. In a subjective survey, we show that our proposed method outperforms existing abstractive and extractive approaches in terms of coherence, alignment, and realism in teaser generation.",
      "arxiv_url": "https://openreview.net/forum?id=DxiP59Z81m",
      "pdf_url": "https://openreview.net/pdf/ddbe6060a1604b0bf20f94c036b7a48036ec3d02.pdf",
      "primary_category": "Multimodal Large Language Model; Retrieval Augmented Generation; Video Editing",
      "categories": [
        "Multimodal Large Language Model; Retrieval Augmented Generation; Video Editing"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "axlGOT58e8",
      "title": "LLM Safety Alignment is Divergence Estimation in Disguise",
      "authors": [
        "Rajdeep Haldar",
        "Ziyi Wang",
        "Guang Lin",
        "Yue Xing",
        "Qifan Song"
      ],
      "abstract": "We present a theoretical framework showing that popular LLM alignment methods—including RLHF and its variants—can be understood as divergence estimators between aligned (safe or preferred) and unaligned (harmful or less-preferred) distributions. This perspective explains the emergence of separation in the latent space between safe and harmful prompts after alignment. As an application of our general divergence framework, we propose KLDO, a novel KL divergence-based alignment method, and empirically validate its effectiveness. We further show that using compliance–refusal datasets, rather than standard preference-based datasets, leads to stronger separation and improved safety alignment. Finally, to quantify the separation effect, we propose a distance-based metric in the prompt representation space, which also acts as a statistically significant indicator for model safety.",
      "arxiv_url": "https://openreview.net/forum?id=axlGOT58e8",
      "pdf_url": "https://openreview.net/pdf/07394c13188137e467073040a282174c90cebc5d.pdf",
      "primary_category": "Large Language Models, Theory, Safety Alignment",
      "categories": [
        "Large Language Models",
        "Theory",
        "Safety Alignment",
        "RLHF",
        "Divergence Estimation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IHX5V6zQpY",
      "title": "LayerCraft: Enhancing Text-to-Image Generation with CoT Reasoning and Layered Object Integration",
      "authors": [
        "Yuyao Zhang",
        "Jinghao Li",
        "Yu-Wing Tai"
      ],
      "abstract": "Text-to-image (T2I) generation has made remarkable progress, yet existing systems still lack intuitive control over spatial composition, object consistency, and multi-step editing. We present **LayerCraft**, a modular framework that uses large language models (LLMs) as autonomous agents to orchestrate structured, layered image generation and editing. LayerCraft supports two key capabilities: (1) *structured generation* from simple prompts via chain-of-thought (CoT) reasoning, enabling it to decompose scenes, reason about object placement, and guide composition in a controllable, interpretable manner; and (2) *layered object integration*, allowing users to insert and customize objects---such as characters or props---across diverse images or scenes while preserving identity, context, and style. The system comprises a coordinator agent, the **ChainArchitect** for CoT-driven layout planning, and the **Object Integration Network (OIN)** for seamless image editing using off-the-shelf T2I models without retraining. Through applications like batch collage editing and narrative scene generation, LayerCraft empowers non-experts to iteratively design, customize, and refine visual content with minimal manual effort. Code will be released upon acceptance.",
      "arxiv_url": "https://openreview.net/forum?id=IHX5V6zQpY",
      "pdf_url": "https://openreview.net/pdf/176e92466e52092e78e122878a6ffcc0c2392d7e.pdf",
      "primary_category": "Multi-control image generation, Diffusion Transformer, LLM agent framework",
      "categories": [
        "Multi-control image generation",
        "Diffusion Transformer",
        "LLM agent framework"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "N9HLe9iPhj",
      "title": "Can Agent Fix Agent Issues?",
      "authors": [
        "Alfin Wijaya Rahardja",
        "Junwei Liu",
        "Weitong Chen",
        "Zhenpeng Chen",
        "Yiling Lou"
      ],
      "abstract": "LLM-based agent systems are emerging as a new software paradigm and have been widely adopted across diverse domains such as medicine, robotics, and programming. However, maintaining these systems requires substantial effort, as they are inevitably prone to bugs and continually evolve to meet changing external requirements. Therefore, automatically resolving agent issues (i.e.,bug reports or feature requests) is a crucial and challenging task.  While recent software engineering (SE) agents (e.g., SWE-agent) have shown promise in addressing issues in traditional software systems, it remains unclear how effectively they can resolve real-world issues in agent systems, which differ significantly from traditional software. To fill this gap, we first manually analyze 201 real-world agent issues and identify common categories of agent issues. We then spend 500 person-hours constructing AgentIssue-bench, a reproducible benchmark comprising 50 agent issue resolution tasks (each with an executable environment and failure-triggering tests). We further evaluate state-of-the-art SE agents on AgentIssue-bench and reveal their limited effectiveness (\\.e., with only 0.67% - 4.67% resolution rates). These results underscore the unique challenges of maintaining agent systems compared to traditional software, highlighting the need for further research to develop advanced SE agents for resolving agent issues.",
      "arxiv_url": "https://openreview.net/forum?id=N9HLe9iPhj",
      "pdf_url": "https://openreview.net/pdf/ab4c234adba0d835dbb64828e87b465d8983cecd.pdf",
      "primary_category": "LLM-based agents, SE agents, issue resolution",
      "categories": [
        "LLM-based agents",
        "SE agents",
        "issue resolution"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0BVrpXMr5Y",
      "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference",
      "authors": [
        "Yi Zhao",
        "Yajuan Peng",
        "Nguyen Cam-Tu",
        "Zuchao Li",
        "Wang Xiaoliang",
        "hai zhao",
        "Xiaoming Fu"
      ],
      "abstract": "KV cache eviction has emerged as an effective solution to alleviate resource constraints faced by LLMs in long-context scenarios. However, existing token-level eviction methods often overlook two critical aspects: (1) their irreversible eviction strategy fails to adapt to dynamic attention patterns during decoding (the saliency shift problem), and (2) they treat both marginally important tokens and truly unimportant tokens uniformly, despite the collective significance of marginal tokens to model performance (the marginal information over-compression problem). To address these issues, we design two compensation mechanisms based on the high similarity of attention matrices between LLMs with different scales. We propose SmallKV, a small model assisted compensation method for KV cache compression. SmallKV can maintain attention matching between different-scale LLMs to: 1) assist the larger model in perceiving globally important information of attention; and 2) use the smaller model’s attention scores to approximate those of marginal tokens in the larger model. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and LongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency evaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than baseline methods, highlighting its potential for efficient and performant LLM inference in resource constrained environments.",
      "arxiv_url": "https://openreview.net/forum?id=0BVrpXMr5Y",
      "pdf_url": "https://openreview.net/pdf/a9bf67e94ca9d4f29d0cf15f37fae223436e1ed1.pdf",
      "primary_category": "KV Cache Compression, Efficient LLM Inference",
      "categories": [
        "KV Cache Compression",
        "Efficient LLM Inference"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zQK6IluJi3",
      "title": "Dynamic Masking and Auxiliary Hash Learning for Enhanced Cross-Modal Retrieval",
      "authors": [
        "Shuang Zhang",
        "Yue Wu",
        "Lei Shi",
        "Yingxue Zhang",
        "Feifei Kou",
        "Huilong Jin",
        "Pengfei Zhang",
        "Meiyu Liang",
        "Mingying Xu"
      ],
      "abstract": "The demand for multimodal data processing drives the development of information technology. Cross-modal hash retrieval has attracted much attention because it can overcome modal differences and achieve efficient retrieval, and has shown great application potential in many practical scenarios. Existing cross-modal hashing methods have difficulties in fully capturing the semantic information of different modal data, which leads to a significant semantic gap between modalities. Moreover, these methods often ignore the importance differences of channels, and due to the limitation of a single goal, the matching effect between hash codes is also affected to a certain extent, thus facing many challenges. To address these issues, we propose a Dynamic Masking and Auxiliary Hash Learning (AHLR) method for enhanced cross-modal retrieval. By jointly leveraging the dynamic masking and auxiliary hash learning mechanisms, our approach effectively resolves the problems of channel information imbalance and insufficient key information capture, thereby significantly improving the retrieval accuracy. Specifically, we introduce a dynamic masking mechanism that automatically screens and weights the key information in images and texts during the training process, enhancing the accuracy of feature matching. We further construct an auxiliary hash layer to adaptively balance the weights of features across each channel, compensating for the deficiencies of traditional methods in key information capture and channel processing. In addition, we design a contrastive loss function to optimize the generation of hash codes and enhance their discriminative power, further improving the performance of cross-modal retrieval. Comprehensive experimental results on NUS-WIDE, MIRFlickr-25K and MS-COCO benchmark datasets show that the proposed AHLR algorithm outperforms several existing algorithms.",
      "arxiv_url": "https://openreview.net/forum?id=zQK6IluJi3",
      "pdf_url": "https://openreview.net/pdf/ff3af1cd902c1f0334b4482238afa14a38fee146.pdf",
      "primary_category": "Cross-modal hash retrieval, Auxiliary hash, Dynamic masking",
      "categories": [
        "Cross-modal hash retrieval",
        "Auxiliary hash",
        "Dynamic masking"
      ],
      "tags": [
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "5sgK63Zshg",
      "title": "SteerConf: Steering LLMs for Confidence Elicitation",
      "authors": [
        "Ziang Zhou",
        "Tianyuan Jin",
        "Jieming Shi",
        "Li Qing"
      ],
      "abstract": "Large Language Models (LLMs) exhibit impressive performance across diverse domains but often suffer from overconfidence, limiting their reliability in critical applications. We propose SteerConf, a novel framework that systematically steers LLMs' confidence scores to improve their calibration and reliability. SteerConf introduces three key components: (1) a steering prompt strategy that guides LLMs to produce confidence scores in specified directions (e.g., conservative or optimistic) by leveraging prompts with varying steering levels; (2) a steered confidence consistency measure that quantifies alignment across multiple steered confidences to enhance calibration; and (3) a steered confidence calibration method that aggregates confidence scores using consistency measures and applies linear quantization for answer selection. SteerConf operates without additional training or fine-tuning, making it broadly applicable to existing LLMs. Experiments on seven benchmarks spanning professional knowledge, common sense, ethics, and reasoning tasks, using advanced LLM models (GPT-3.5, LLaMA 3, GPT-4), demonstrate that SteerConf significantly outperforms existing methods, often by a significant margin. \nOur findings highlight the potential of steering the confidence of LLMs to enhance their reliability for safer deployment in real-world applications. The implementation is at \\url{https://github.com/scottjiao/SteerConf}.",
      "arxiv_url": "https://openreview.net/forum?id=5sgK63Zshg",
      "pdf_url": "https://openreview.net/pdf/2dea5d57488bdb780105be0720fc4b246ed074ec.pdf",
      "primary_category": "calibration, uncertainty, prompting",
      "categories": [
        "calibration",
        "uncertainty",
        "prompting",
        "black-box",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6Had86RHix",
      "title": "MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection",
      "authors": [
        "Shengtian Yang",
        "Yue Feng",
        "Yingshi Liu",
        "Jingrou Zhang",
        "Jie Qin"
      ],
      "abstract": "Video Anomaly Detection (VAD) aims to locate unusual activities or behaviors within videos. Recently, offline VAD has garnered substantial research attention, which has been invigorated by the progress in large language models (LLMs) and vision-language models (VLMs), offering the potential for a more nuanced understanding of anomalies. \nHowever, online VAD has seldom received attention due to real-time constraints and computational intensity. \nIn this paper, we introduce a novel **M**emory-based online scoring queue scheme for **T**raining-free VAD (MoniTor), to address the inherent complexities in online VAD. \nSpecifically, MoniTor applies a streaming input to VLMs, leveraging the capabilities of pre-trained large-scale models. \nTo capture temporal dependencies more effectively, we incorporate a novel prediction mechanism inspired by Long Short-Term Memory (LSTM) networks. This ensures the model can effectively model past states and leverage previous predictions to identify anomalous behaviors. Thereby, it better understands the current frame. \nMoreover, we design a scoring queue and an anomaly prior to dynamically store recent scores and cover all anomalies in the monitoring scenario, providing guidance for LLMs to distinguish between normal and abnormal behaviors over time.\nWe evaluate MoniTor on two large datasets (i.e., UCF-Crime and XD-Violence) containing various surveillance and real-world scenarios. \nThe results demonstrate that MoniTor outperforms state-of-the-art methods and is competitive with weakly supervised methods without training. Code is available at https://github.com/YsTvT/MoniTor.",
      "arxiv_url": "https://openreview.net/forum?id=6Had86RHix",
      "pdf_url": "https://openreview.net/pdf/12eb1e4189682fe7b8223e53327f9e2ada20bd59.pdf",
      "primary_category": "Training-free, Online VAD, Instructed LLMs",
      "categories": [
        "Training-free",
        "Online VAD",
        "Instructed LLMs",
        "LSTM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "UCV21BsuqA",
      "title": "Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code",
      "authors": [
        "Augusto B. Corrêa",
        "André Grahl Pereira",
        "Jendrik Seipp"
      ],
      "abstract": "In recent years, large language models (LLMs) have shown remarkable performance in many problems. However, they fail to plan reliably. Specialized attempts to improve their planning capabilities still produce incorrect plans and fail to generalize to larger tasks. Furthermore, LLMs designed for explicit \"reasoning\" fail to compete with automated planners while increasing computational costs, which reduces one of the advantages of using LLMs. In this paper, we show how to use LLMs to always generate correct plans, even for out-of-distribution tasks of increasing size. For a given planning domain, we ask an LLM to generate several domain-dependent heuristic functions in the form of Python code, evaluate them on a set of training tasks with a greedy best-first search, and choose the best one. The resulting LLM-generated heuristic functions solve substantially more unseen out-of-distribution test tasks than end-to-end LLM planning, particularly for non-reasoning LLMs. Moreover, they also solve many more tasks than state-of-the-art domain-independent heuristics for classical planning, and are competitive with the strongest learning algorithm for domain-dependent planning. These results are impressive given that our implementation is based on a Python planner and the baselines all build upon highly optimized C++ code. In some domains, the LLM-generated heuristics expand fewer states than the baselines, showing that they are not only efficiently computable but also more informative than the state-of-the-art heuristics. Overall, our results show that sampling a set of planning heuristic functions can significantly improve the planning capabilities of LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=UCV21BsuqA",
      "pdf_url": "https://openreview.net/pdf/924624fad0f2d9ea4637686b275593407c20b753.pdf",
      "primary_category": "automated planning, planning, heuristic search",
      "categories": [
        "automated planning",
        "planning",
        "heuristic search",
        "search"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kd6hcHUl9C",
      "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM Inference",
      "authors": [
        "Wenxuan Zeng",
        "Ye Dong",
        "Jinjin Zhou",
        "Jin Tan",
        "Lei Wang",
        "Tao Wei",
        "Runsheng Wang",
        "Meng Li"
      ],
      "abstract": "Private large language model (LLM) inference based on secure multi-party computation (MPC) achieves formal data privacy protection but suffers from significant latency overhead, especially for long input sequences. While key-value (KV) cache eviction and sparse attention algorithms have been proposed for efficient LLM inference in plaintext, they are not designed for MPC and cannot benefit private LLM inference directly. In this paper, we propose an accurate and MPC-friendly KV cache eviction framework, dubbed MPCache, building on the observation that historical tokens in a long sequence may have different effects on the downstream decoding. Hence, MPCache combines a look-once static eviction algorithm to discard unimportant KV cache and a query-aware dynamic selection algorithm to activate only a small subset of KV cache for attention computation. MPCache further incorporates a series of optimizations for efficient dynamic KV cache selection, including MPC-friendly similarity approximation, hierarchical KV cache clustering, and cross-layer index-sharing strategy. Extensive experiments demonstrate that MPCache consistently outperforms prior-art KV cache eviction baselines across different generation tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and communication reduction on different sequence lengths, respectively.",
      "arxiv_url": "https://openreview.net/forum?id=kd6hcHUl9C",
      "pdf_url": "https://openreview.net/pdf/20929dac9b67fad4ccdf8d864b7ebe09c4e8ca00.pdf",
      "primary_category": "Large Language Model, KV Cache Compression, Sparse Attention",
      "categories": [
        "Large Language Model",
        "KV Cache Compression",
        "Sparse Attention",
        "Private Inference",
        "Multi-party Computation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "YES7VDXPV8",
      "title": "On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection",
      "authors": [
        "Weiqing He",
        "Xiang Li",
        "Tianqi Shang",
        "Li Shen",
        "Weijie J Su",
        "Qi Long"
      ],
      "abstract": "Large language models (LLMs) raise concerns about content authenticity and integrity because they can generate human-like text at scale. Text watermarks, which embed detectable statistical signals into generated text, offer a provable way to verify content origin. Many detection methods rely on pivotal statistics that are i.i.d. under human-written text, making goodness-of-fit (GoF) tests a natural tool for watermark detection. However, GoF tests remain largely underexplored in this setting.\nIn this paper, we systematically evaluate eight GoF tests across three popular watermarking schemes, using three open-source LLMs, two datasets, various generation temperatures, and multiple post-editing methods. \nWe find that general GoF tests can improve both the detection power and robustness of watermark detectors. Notably, we observe that text repetition, common in low-temperature settings, gives GoF tests a unique advantage not exploited by existing methods.\nOur results highlight that classic GoF tests are a simple yet powerful and underused tool for watermark detection in LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=YES7VDXPV8",
      "pdf_url": "https://openreview.net/pdf/8fcead2176e8a34cb2660bd42fe07b07519cdc4c.pdf",
      "primary_category": "Large Language Models, Watermark, Goodness-of-Fit",
      "categories": [
        "Large Language Models",
        "Watermark",
        "Goodness-of-Fit"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "P4xaLYXBRe",
      "title": "SPACE: Noise Contrastive Estimation Stabilizes Self-Play Fine-Tuning for Large Language Models",
      "authors": [
        "Yibo Wang",
        "Guangda Huzhang",
        "Qing-Guo Chen",
        "Zhao Xu",
        "Weihua Luo",
        "Kaifu Zhang",
        "Lijun Zhang"
      ],
      "abstract": "Self-play fine-tuning has demonstrated promising abilities in adapting large language models (LLMs) to downstream tasks with limited real-world data. The basic principle is to iteratively refine the model with real samples and synthetic ones generated from itself. However, the existing methods primarily focus on the relative gaps between the rewards for two types of data, neglecting their absolute values. Through theoretical analysis, we identify that the gap-based methods suffer from unstable evolution, due to the potentially degenerated objectives. To address this limitation, we introduce a novel self-play fine-tuning method, namely \\underline{S}elf-\\underline{P}l\\underline{A}y via Noise \\underline{C}ontrastive \\underline{E}stimation (SPACE), which leverages noise contrastive estimation to capture the real-world data distribution. Specifically, SPACE treats synthetic samples as auxiliary components, and discriminates them from the real ones in a binary classification manner. As a result, SPACE independently optimizes the absolute reward values for each type of data, ensuring a consistently meaningful objective and thereby avoiding the instability issue. Theoretically, we show that the optimal solution of the objective in SPACE aligns with the underlying distribution of real-world data, and SPACE guarantees a provably stable convergence to the optimal distribution. Empirically, we show that SPACE significantly improves the performance of LLMs over various tasks, and outperforms supervised fine-tuning that employs much more real-world samples. Compared to gap-based self-play fine-tuning methods, SPACE exhibits remarkable superiority and stable evolution.",
      "arxiv_url": "https://openreview.net/forum?id=P4xaLYXBRe",
      "pdf_url": "https://openreview.net/pdf/694976ed654aa651e22e67966e61c6b69f328d65.pdf",
      "primary_category": "Language Models, Self-play Fine-tuning, Noise Contrastive Estimation",
      "categories": [
        "Language Models",
        "Self-play Fine-tuning",
        "Noise Contrastive Estimation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oUYmk8WaG0",
      "title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors",
      "authors": [
        "Duo Zheng",
        "Shijia Huang",
        "Yanyang Li",
        "Liwei Wang"
      ],
      "abstract": "Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method called the Video-3D Geometry Large Language Model (VG LLM). Our approach utilizes a 3D visual geometry encoder to extract 3D prior information from video sequences. This information is then integrated with visual tokens and input into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations.",
      "arxiv_url": "https://openreview.net/forum?id=oUYmk8WaG0",
      "pdf_url": "https://openreview.net/pdf/8302c7eb6fffa2ef2835e307052bb699126996a1.pdf",
      "primary_category": "MLLM, 3D Scene Understanding, Spatial Reasoning",
      "categories": [
        "MLLM",
        "3D Scene Understanding",
        "Spatial Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1Qo6DzdPOG",
      "title": "From Sequence to Structure: Uncovering Substructure Reasoning in Transformers",
      "authors": [
        "Xinnan Dai",
        "Kai Yang",
        "Jay Revolinsky",
        "Kai Guo",
        "Aoran Wang",
        "Bohang Zhang",
        "Jiliang Tang"
      ],
      "abstract": "Recent studies suggest that large language models (LLMs) possess the capability to solve graph reasoning tasks. Notably, even when graph structures are embedded within textual descriptions, LLMs can still effectively answer related questions. This raises a fundamental question: How can a decoder-only Transformer architecture understand underlying graph structures? To address this, \nwe start with the substructure extraction task, interpreting the inner mechanisms inside the transformers and analyzing the impact of the input queries. Specifically, through both empirical results and theoretical analysis, we present Induced Substructure Filtration (ISF), a perspective that captures the substructure identification in the multi-layer transformers. We further validate the ISF process in LLMs, revealing consistent internal dynamics across layers. Building on these insights, we explore the broader capabilities of Transformers in handling diverse graph types. Specifically, we introduce the concept of thinking in substructures to efficiently extract complex composite patterns, and demonstrate that decoder-only Transformers can successfully extract substructures from attributed graphs, such as molecular graphs. Together, our findings offer a new insight on how sequence-based Transformers perform the substructure extraction task over graph data.",
      "arxiv_url": "https://openreview.net/forum?id=1Qo6DzdPOG",
      "pdf_url": "https://openreview.net/pdf/dc9f74f680dd9339e01160ceef5012a82cf96f83.pdf",
      "primary_category": "LLMs for graph reasoning, interpretation for Transformers",
      "categories": [
        "LLMs for graph reasoning",
        "interpretation for Transformers"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Qg4wmZSkWI",
      "title": "ChatbotID: Identifying Chatbots with Granger Causality Test",
      "authors": [
        "Xiaoquan Yi",
        "Haozhao Wang",
        "Yining Qi",
        "Wenchao Xu",
        "Rui Zhang",
        "Yuhua Li",
        "Ruixuan Li"
      ],
      "abstract": "With the increasing sophistication of Large Language Models (LLMs), it is crucial to develop reliable methods to accurately identify whether an interlocutor in real-time dialogue is human or chatbot. However, existing detection methods are primarily designed for analyzing full documents, not the unique dynamics and characteristics of dialogue. These approaches frequently overlook the nuances of interaction that are essential in conversational contexts. This work identifies two key patterns in dialogues: (1) Human-Human (H-H) interactions exhibit significant bidirectional sentiment influence, while (2) Human-Chatbot (H-C) interactions display a clear asymmetric pattern. We propose an innovative approach named ChatbotID, which\napplies the Granger Causality Test (GCT) to extract a novel set of interactional features that capture the evolving, predictive relationships between conversational attributes. By synergistically fusing these GCT-based interactional features with contextual embeddings, and optimizing the model through a meticulous loss function. Experimental results across multiple datasets and detection models demonstrate the effectiveness of our framework, with significant improvements in accuracy for distinguishing between H-H and H-C dialogues.",
      "arxiv_url": "https://openreview.net/forum?id=Qg4wmZSkWI",
      "pdf_url": "https://openreview.net/pdf/ce0c3de9b4fc30734ce45f744c49e624a71aace7.pdf",
      "primary_category": "Granger Causality, Dialogue, LLMs",
      "categories": [
        "Granger Causality",
        "Dialogue",
        "LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tM4cHBD7kD",
      "title": "PseuZO: Pseudo-Zeroth-Order Algorithm for Training Deep Neural Networks",
      "authors": [
        "Pengyun Yue",
        "Xuanlin Yang",
        "Mingqing Xiao",
        "Zhouchen Lin"
      ],
      "abstract": "Zeroth-order Optimization (ZO) has received wide attention in machine learning, especially when computing full gradient is expensive or even impossible. Recently, ZO has emerged as an important paradigm for memory-efficient fine-tuning of large language models (LLMs), circumventing the memory overhead of backpropagation. However, existing ZO gradient estimators exhibit dimension-dependent variance scaling as $\\Theta(d)$, leading to dimension-dependent convergence rates without further assumptions on the objective function, which is prohibitive for large-scale LLM parameters. To address this problem, we present a Pseudo-Zeroth-Order (PseuZO) framework for optimizing composite objective functions, especially large-scale models: $ \\min_{\\mathbf{x} \\in \\mathcal{X}} \\mathcal{F}(\\mathbf{x})= \\bbE_{\\mathbf{z}} g\\circ h(\\mathbf{x};\\mathbf{z}) $, where $h$ represents complex, high-dimensional representations and $g$ is a task-specific loss. While existing zeroth-order methods estimate gradients with final loss functions, our PseuZO algorithm estimate the Jacobian matrix of $h(\\mathbf{x})$ with the model output $\\mathbf{o}= h(\\mathbf{x})$, and the gradient of the loss function on model output $\\mathbf{e} = \\nabla_{\\mathbf{o}} g(\\mathbf{o})$, and apply exponential moving average on Jacobian estimators to reduce the variance. Moreover, we use the sliding window technique to reduce memory costs. Our algorithm achieves an $O( \\max \\lbrace \\alpha_1 L\\epsilon^{-2}, \\alpha_1 L \\sigma_2^2\\epsilon^{-4} \\rbrace )$ convergence rate, where $\\alpha_1$ is the effective dimension of $\\mathcal{F}$.\n  Experimental results demonstrate that PseuZO outperforms MeZO and MeZO-SVRG in classification, multiple choice and generation tasks in both full-parameter and PEFT fine-tuning settings by boosting convergence in the early stages of training. For instance, under the same computation time, with respect to SST2 task, PesuZO gets 9.8\\% higher accuracy than MeZO (91.2\\% v.s. 82.4\\%). With the sliding window technique, our PseuZO achieves $70\\%\\sim80\\%$ memory reduction compared to FO-SGD for different model sizes as PseuZO only introduced a small dimension-independent memory overhead, which enables efficient scaling of the model size. The code is available at https://github.com/YangBigMn/PseuZO.\n$\\newcommand{\\bbE}{\\mathbb{E}}$",
      "arxiv_url": "https://openreview.net/forum?id=tM4cHBD7kD",
      "pdf_url": "https://openreview.net/pdf/43a9d24dd5077fb66c068a4fc5e540e2ab045de6.pdf",
      "primary_category": "zeroth-order optimization, LLM fine-tuning",
      "categories": [
        "zeroth-order optimization",
        "LLM fine-tuning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZwCVFBFUFb",
      "title": "QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training",
      "authors": [
        "Wei Dai",
        "Peilin Chen",
        "Chanakya Ekbote",
        "Paul Pu Liang"
      ],
      "abstract": "Clinical decision‑making routinely demands reasoning over heterogeneous data, yet existing multimodal language models (MLLMs) remain largely vision‑centric and fail to generalize across clinical specialties. To bridge this gap, we introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model that jointly reasons across medical images, time‑series signals, and text reports. QoQ-Med is trained with Domain‑aware Relative Policy Optimization (DRPO), a novel reinforcement‑learning objective that hierarchically scales normalized rewards according to domain rarity and modality difficulty, mitigating performance imbalance caused by skewed clinical data distributions. Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains, we show that DRPO training boosts diagnostic performance by 43% in macro‑F1 on average across all visual domains as compared to other critic-free training methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation data, it is able to highlight salient regions related to the diagnosis, with an IoU 10x higher than open models while reaching the performance of OpenAI o4-mini. To foster reproducibility and downstream research, we release (i) the full model weights, (ii) the modular training pipeline, and (iii) all intermediate reasoning traces.",
      "arxiv_url": "https://openreview.net/forum?id=ZwCVFBFUFb",
      "pdf_url": "https://openreview.net/pdf/b108c9d8c17d3e41084d1257c97d19a7d41f99e1.pdf",
      "primary_category": "machine learning, healthcare, reinforcement learning",
      "categories": [
        "machine learning",
        "healthcare",
        "reinforcement learning",
        "multimodal learning",
        "AI for healthcare",
        "clinical AI",
        "time series modeling",
        "natural language processing"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "V13dSX1wAs",
      "title": "PermLLM: Learnable Channel Permutation for N:M Sparse Large Language Models",
      "authors": [
        "Lancheng Zou",
        "Shuo Yin",
        "Zehua Pei",
        "Tsung-Yi Ho",
        "Farzan Farnia",
        "Bei Yu"
      ],
      "abstract": "Channel permutation is a powerful technique for enhancing the accuracy of N:M sparse models by reordering the channels of weight matrices to prioritize the retention of important weights. \nHowever, traditional channel permutation methods rely on handcrafted quality metrics, which often fail to accurately capture the true impact of pruning on model performance. \nTo address this limitation, we propose PermLLM, a novel post-training pruning framework that introduces learnable channel permutation (LCP) for N:M sparsity. \nLCP leverages Sinkhorn normalization to transform discrete permutation matrices into differentiable soft permutation matrices, enabling end-to-end optimization. \nAdditionally, PermLLM incorporates an efficient block-wise channel permutation strategy, which significantly reduces the number of learnable parameters and computational complexity. \nPermLLM seamlessly integrates with existing one-shot pruning methods to adaptively optimize channel permutations, effectively mitigating pruning-induced errors. \nExtensive experiments on the LLaMA series, Qwen, and OPT models demonstrate that PermLLM achieves superior performance in optimizing N:M sparse models.",
      "arxiv_url": "https://openreview.net/forum?id=V13dSX1wAs",
      "pdf_url": "https://openreview.net/pdf/c4efdba2903a535d6f7ad1014f1e6e31076d7792.pdf",
      "primary_category": "Channel Permutation, N:M Sparsity",
      "categories": [
        "Channel Permutation",
        "N:M Sparsity"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2EJrs3gUO6",
      "title": "Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders",
      "authors": [
        "Ali Rasekh",
        "Erfan Bagheri Soula",
        "Omid Daliran",
        "Simon Gottschalk",
        "Mohsen Fayyaz"
      ],
      "abstract": "Despite significant advances in Multimodal Large Language Models (MLLMs), understanding complex temporal dynamics in videos remains a major challenge. Our experiments show that current Video Large Language Model (Video-LLM) architectures have critical limitations in temporal understanding, struggling with tasks that require detailed comprehension of action sequences and temporal progression. In this work, we propose a Video-LLM architecture that introduces stacked temporal attention modules directly within the vision encoder. This design incorporates a temporal attention in vision encoder, enabling the model to better capture the progression of actions and the relationships between frames before passing visual tokens to the LLM. Our results show that this approach significantly improves temporal reasoning and outperforms existing models in video question answering tasks, specifically in action recognition. We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to +5.5%. By enhancing the vision encoder with temporal structure, we address a critical gap in video understanding for Video-LLMs. Project page and code are available at: https://alirasekh.github.io/STAVEQ2/",
      "arxiv_url": "https://openreview.net/forum?id=2EJrs3gUO6",
      "pdf_url": "https://openreview.net/pdf/8983a85d88c67728d26c47fa75cdce08379b3561.pdf",
      "primary_category": "Video-LLM, Video Understanding, Temporal Reasoning",
      "categories": [
        "Video-LLM",
        "Video Understanding",
        "Temporal Reasoning",
        "Video Question Answering",
        "Large Vision Language Model",
        "Multimodal LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ONE9LYBQYS",
      "title": "Scale-invariant attention",
      "authors": [
        "Ben Anson",
        "Xi Wang",
        "Laurence Aitchison"
      ],
      "abstract": "One persistent challenge in LLM research is the development of attention mechanisms that are able to generalise from training on shorter contexts to inference on longer contexts. We propose two conditions that we expect all effective long-context attention mechanisms to have: scale-invariant total attention, and scale-invariant attention sparsity. Under a Gaussian assumption, we show that a simple position-dependent transformation of the attention logits is sufficient for these conditions to hold. Experimentally we find that the resulting scale-invariant attention scheme gives considerable benefits in terms of validation loss when zero-shot generalising from training on short contexts to validation on longer contexts, and is effective at long-context retrieval.",
      "arxiv_url": "https://openreview.net/forum?id=ONE9LYBQYS",
      "pdf_url": "https://openreview.net/pdf/335c20d88f091f664d66631abdf286d50f5fe3c2.pdf",
      "primary_category": "long context, LLM, Transformer",
      "categories": [
        "long context",
        "LLM",
        "Transformer",
        "scale invariance"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wmweEDugTZ",
      "title": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning",
      "authors": [
        "Sheng Wang",
        "Pengan CHEN",
        "Jingqi Zhou",
        "Qintong Li",
        "Jingwei Dong",
        "Jiahui Gao",
        "Boyang XUE",
        "Jiyue Jiang",
        "Lingpeng Kong",
        "Chuan Wu"
      ],
      "abstract": "Model customization necessitates high-quality and diverse datasets, but acquiring such data remains time-consuming and labor-intensive. \nDespite the great potential of large language models (LLMs) for data synthesis, current approaches are constrained by limited seed data, model biases and low-variation prompts, resulting in limited diversity and biased distribution with the increase of data scales.\nTo tackle this challenge, we introduce TreeSynth, a tree-guided subspace-based data synthesis approach inspired by decision trees. \nIt constructs a spatial partitioning tree to recursively divide a task-specific full data space (i.e., root node) into numerous atomic subspaces (i.e., leaf nodes) with mutually exclusive and exhaustive attributes to ensure both distinctiveness and comprehensiveness, before synthesizing samples within each atomic subspace. \nThis globally divide-and-synthesize method finally collects subspace samples into a comprehensive dataset, effectively circumventing repetition and space collapse to ensure the diversity of large-scale data synthesis.\nFurthermore, the spatial partitioning tree enables sample allocation into atomic subspaces, allowing the re-balancing of existing datasets for more balanced and comprehensive distributions.\nEmpirically, extensive experiments across diverse benchmarks consistently validates the superior data diversity, model performance, and robust scalability of TreeSynth compared to both human-crafted datasets and peer data synthesis methods, with the average performance gain reaching 10%.\nBesides, the consistent improvements of TreeSynth-balanced datasets highlight its efficacious application to redistribute existing datasets for more comprehensive coverage and the induced performance enhancement. The code is available at https://github.com/cpa2001/TreeSynth.",
      "arxiv_url": "https://openreview.net/forum?id=wmweEDugTZ",
      "pdf_url": "https://openreview.net/pdf/c387c97ed27b2d47ecceda9366fa389a37b17b60.pdf",
      "primary_category": "Data Synthesis, Tree-Guided Space Partitioning, Data Diversity Enhancement",
      "categories": [
        "Data Synthesis",
        "Tree-Guided Space Partitioning",
        "Data Diversity Enhancement"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "aqgLyQOECN",
      "title": "Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain",
      "authors": [
        "Jingmin An",
        "Yilong Song",
        "Ruolin Yang",
        "Nai Ding",
        "Lingxi Lu",
        "Yuxuan Wang",
        "Wei Wang",
        "Chu Zhuang",
        "Qian Wang",
        "Fang Fang"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate human-level or even superior language abilities, effectively modeling syntactic structures, yet the specific computational units responsible remain unclear. A key question is whether LLM behavioral capabilities stem from mechanisms akin to those in the human brain. To address these questions, we introduce the Hierarchical Frequency Tagging Probe (HFTP), a tool that utilizes frequency-domain analysis to identify neuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP) neurons) and cortical regions (via intracranial recordings) encoding syntactic structures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama 2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human brain relies on distinct cortical regions for different syntactic levels. Representational similarity analysis reveals a stronger alignment between LLM representations and the left hemisphere of the brain (dominant in language processing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows greater brain similarity than Gemma, while Llama 3.1 shows less alignment with the brain compared to Llama 2. These findings offer new insights into the interpretability of LLM behavioral improvements, raising questions about whether these advancements are driven by human-like or non-human-like mechanisms, and establish HFTP as a valuable tool bridging computational linguistics and cognitive neuroscience. This project is available at https://github.com/LilTiger/HFTP.",
      "arxiv_url": "https://openreview.net/forum?id=aqgLyQOECN",
      "pdf_url": "https://openreview.net/pdf/e70aeaab2b89b824628a019e4dd6f88b8ea082bd.pdf",
      "primary_category": "Syntactic structure processing, LLM, Syntactic alignment",
      "categories": [
        "Syntactic structure processing",
        "LLM",
        "Syntactic alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2ogTw5ue7v",
      "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
      "authors": [
        "Haolei Xu",
        "Yuchen Yan",
        "Yongliang Shen",
        "Wenqi Zhang",
        "Guiyang Hou",
        "Shengpei Jiang",
        "Kaitao Song",
        "Weiming Lu",
        "Jun Xiao",
        "Yueting Zhuang"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable progress on mathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from **Thought Leaps** due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called **ScaleQM+**, based on the structured ScaleQuestMath dataset, and trained **CoT-Bridge** to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87\\% on NuminaMath. Our approach effectively enhances distilled data (+3.02\\%) and provides better starting points for reinforcement learning (+3.1\\%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrates improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits.",
      "arxiv_url": "https://openreview.net/forum?id=2ogTw5ue7v",
      "pdf_url": "https://openreview.net/pdf/b63a80f12eba96f7a1d80c464c6cd302c8b2c6b6.pdf",
      "primary_category": "Large Reasoning Models, LLM Reasoning",
      "categories": [
        "Large Reasoning Models",
        "LLM Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "PBvlBI6c30",
      "title": "FlowMoE: A Scalable Pipeline Scheduling Framework for Distributed Mixture-of-Experts Training",
      "authors": [
        "Yunqi Gao",
        "Bing Hu",
        "Mahdi Boloursaz Mashhadi",
        "A-Long Jin",
        "Yanfeng Zhang",
        "Pei Xiao",
        "Rahim Tafazolli",
        "Merouane Abdelkader DEBBAH"
      ],
      "abstract": "The parameter size of modern large language models (LLMs) can be scaled up to the trillion-level via the sparsely-activated Mixture-of-Experts (MoE) technique to avoid excessive increase of the computational costs. To further improve training efficiency, pipelining computation and communication has become a promising solution for distributed MoE training. However, existing work primarily focuses on scheduling tasks within the MoE layer, such as expert computing and all-to-all (A2A) communication, while neglecting other key operations including multi-head attention (MHA) computing, gating, and all-reduce communication. In this paper, we propose FlowMoE, a scalable framework for scheduling multi-type task pipelines. First, FlowMoE constructs a unified pipeline to consistently scheduling MHA computing, gating, expert computing, and A2A communication. Second, FlowMoE introduces a tensor chunk-based priority scheduling mechanism to overlap the all-reduce communication with all computing tasks. We implement FlowMoE as an adaptive and generic framework atop PyTorch. Extensive experiments with 675 typical MoE layers and four real-world MoE models across two GPU clusters demonstrate that our proposed FlowMoE framework outperforms state-of-the-art MoE training frameworks, reducing training time by14%-57%, energy consumption by 10%-39%, and memory usage by 7%-32%. FlowMoE’s code is anonymously available at https://anonymous.4open.science/r/FlowMoE.",
      "arxiv_url": "https://openreview.net/forum?id=PBvlBI6c30",
      "pdf_url": "https://openreview.net/pdf/a2ee0ab50600ee9a7b2254d156b239209d3eb690.pdf",
      "primary_category": "Distributed Deep Learning, Large Language Model, Mixture-of-Experts",
      "categories": [
        "Distributed Deep Learning",
        "Large Language Model",
        "Mixture-of-Experts",
        "Pipeline Scheduling"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4t8dYmu5vG",
      "title": "Succeed or Learn Slowly: Sample Efficient Off-Policy Reinforcement Learning for Mobile App Control",
      "authors": [
        "Georgios Papoudakis",
        "Thomas Coste",
        "Jianye HAO",
        "Jun Wang",
        "Kun Shao"
      ],
      "abstract": "Reinforcement learning (RL) using foundation models for policy approximations in multi-turn tasks remains challenging. We identify two main limitations related to sparse reward settings and policy gradient updates, based on which we formulate a key insight: updates from positive samples with high returns typically do not require policy regularisation, whereas updates from negative samples, reflecting undesirable behaviour, can harm model performance. This paper introduces Succeed or Learn Slowly (SoLS), a novel off-policy RL algorithm evaluated on mobile app control tasks. SoLS improves sample efficiency when fine-tuning foundation models for user interface navigation via a modified off-policy actor-critic approach, applying direct policy updates for positive samples and conservative, regularised updates for negative ones to prevent model degradation. We augment SoLS with Successful Transition Replay (STR), which prioritises learning from successful interactions, further improving sample efficiency. We evaluate SoLS on the AndroidWorld benchmark, where it significantly outperforms existing methods (at least 17\\% relative increase), including prompt-engineering and RL approaches, while requiring substantially fewer computational resources than GPT-4o-based methods with 5-60x faster inference.",
      "arxiv_url": "https://openreview.net/forum?id=4t8dYmu5vG",
      "pdf_url": "https://openreview.net/pdf/01a90e212c621698c3073eeb39df9232aaa9cf09.pdf",
      "primary_category": "App Control, Reinforcement Learning",
      "categories": [
        "App Control",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wNMK5o0Vfg",
      "title": "S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models",
      "authors": [
        "Mz Dai",
        "Chenxu Yang",
        "Qingyi Si"
      ],
      "abstract": "As Test-Time Scaling emerges as an active research focus in the large language model community, advanced post-training methods increasingly emphasize extending chain-of-thought (CoT) generation length, thereby enhancing reasoning capabilities to approach Deepseek R1-like reasoning models. \nHowever, recent studies reveal that reasoning models (even Qwen3) consistently exhibit excessive thought redundancy in CoT generation. This overthinking issue arises from the inherent limitations of conventional outcome-reward reinforcement learning, which systematically overlooks the regulation of intermediate reasoning processes. This paper introduces Serial-Group Decaying-Reward Policy Optimization (S-GRPO), a novel reinforcement learning paradigm that enables models to implicitly evaluate the sufficiency of intermediate reasoning steps, thereby facilitating early exit in CoT generation.\nUnlike GRPO, which samples multiple possible reasoning paths in parallel (parallel group), S-GRPO only samples one reasoning path and serially selects multiple temporal positions from the path to exit thinking and directly generate answers (serial group). For correct answers within a serial group, rewards gradually decrease based on the exit positions along the reasoning path from front to back. This design encourages the model to produce more accurate and concise thoughts, while also incentivizing early thinking termination when appropriate. Empirical evaluations demonstrate that S-GRPO is compatible with state-of-the-art reasoning models, including Qwen3 and Deepseek-distill. Across diverse benchmarks such as GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond, S-GRPO achieves a substantial reduction in sequence length (40.4%～61.1%) while simultaneously improving accuracy (absolute 0.72%～3.92%).",
      "arxiv_url": "https://openreview.net/forum?id=wNMK5o0Vfg",
      "pdf_url": "https://openreview.net/pdf/569e607e6ef3208e833f0f3b98e88117311fc562.pdf",
      "primary_category": "Large language model, Reasoning model, Reinforcement learning",
      "categories": [
        "Large language model",
        "Reasoning model",
        "Reinforcement learning",
        "Post-training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "SRDF3RV0KP",
      "title": "LLM Meeting Decision Trees on Tabular Data",
      "authors": [
        "Hangting Ye",
        "Jinmeng Li",
        "He Zhao",
        "Dandan Guo",
        "Yi Chang"
      ],
      "abstract": "Tabular data have been playing a vital role in diverse real-world fields, including healthcare, finance, etc. \nWith the recent success of Large Language Models (LLMs), early explorations of extending LLMs to the domain of tabular data have been developed. Most of these LLM-based methods typically first serialize tabular data into natural language descriptions, and then tune LLMs or directly infer on these serialized data. However, these methods suffer from two key inherent issues: (i) data perspective: existing data serialization methods lack universal applicability for structured tabular data, and may pose privacy risks through direct textual exposure, and (ii) model perspective: LLM fine-tuning methods struggle with tabular data, and in-context learning scalability is bottle-necked by input length constraints (suitable for few-shot learning). This work explores a novel direction of integrating LLMs into tabular data through logical decision tree rules as intermediaries, proposing a decision tree enhancer with LLM-derived rule for tabular prediction, DeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied to full data learning setting without LLM fine-tuning. \nSpecifically, we leverage the reasoning ability of LLMs to redesign an improved rule given a set of decision tree rules. Furthermore, we provide a calibration method for original decision trees via new generated rule by LLM, which approximates the error correction vector to steer the original decision tree predictions in the direction of ``errors'' reducing.\nFinally, extensive experiments on diverse tabular benchmarks show that our method achieves state-of-the-art performance.",
      "arxiv_url": "https://openreview.net/forum?id=SRDF3RV0KP",
      "pdf_url": "https://openreview.net/pdf/f894493ba33f577f565fdf5fdf26c92b4ee2399a.pdf",
      "primary_category": "Tabular data, Large language models, Classification and regression",
      "categories": [
        "Tabular data",
        "Large language models",
        "Classification and regression"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "mlU9KqdZUS",
      "title": "AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement",
      "authors": [
        "J Rosser",
        "Jakob Nicolaus Foerster"
      ],
      "abstract": "Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been thoroughly explored. We introduce AgentBreeder, a framework for multi-objective self-improving evolutionary search over scaffolds. We evaluate discovered scaffolds on widely recognized reasoning, mathematics, and safety benchmarks and compare them with popular baselines. In \"blue\" mode, we see a 79.4% average uplift in safety benchmark performance while maintaining or improving capability scores. In \"red\" mode, we find adversarially weak scaffolds emerging concurrently with capability optimization. Our work demonstrates the risks of multi-agent scaffolding and provides a framework for mitigating them. Code is available at \\url{https://github.com/jrosseruk/AgentBreeder}.",
      "arxiv_url": "https://openreview.net/forum?id=mlU9KqdZUS",
      "pdf_url": "https://openreview.net/pdf/237ca0099a7a95299cd0bae4b495038b19873e08.pdf",
      "primary_category": "AI Safety, Multi-Agent Systems, LLMs",
      "categories": [
        "AI Safety",
        "Multi-Agent Systems",
        "LLMs",
        "Large Language Models",
        "Jailbreaking",
        "Agent Scaffolds"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cCYUFaR6En",
      "title": "GVPO: Group Variance Policy Optimization for Large Language Model Post-Training",
      "authors": [
        "Kaichen Zhang",
        "Yuzhong Hong",
        "Junwei Bao",
        "Hongfei Jiang",
        "yang song",
        "Hong Dingqian",
        "Hui Xiong"
      ],
      "abstract": "Post-training plays a crucial role in refining and aligning large language models to meet specific tasks and human preferences. While recent advancements in post-training techniques, such as Group Relative Policy Optimization (GRPO), leverage increased sampling with relative reward scoring to achieve superior performance, these methods often suffer from training instability that limits their practical adoption. As a next step, we present Group Variance Policy Optimization (GVPO). GVPO incorporates the analytical solution to KL-constrained reward maximization directly into its gradient weights, ensuring alignment with the optimal policy. The method provides intuitive physical interpretations: its gradient mirrors the mean squared error between the central distance of implicit rewards and that of actual rewards. GVPO offers two key advantages: (1) it guarantees a unique optimal solution, exactly the KL-constrained reward maximization objective, (2) it supports flexible sampling distributions that avoids on-policy and importance sampling limitations. By unifying theoretical guarantees with practical adaptability, GVPO establishes a new paradigm for reliable and versatile LLM post-training.",
      "arxiv_url": "https://openreview.net/forum?id=cCYUFaR6En",
      "pdf_url": "https://openreview.net/pdf/b507d480bd87c1ba0e05bf79ce938cd0b3a0b4e6.pdf",
      "primary_category": "Large Language Model, Post Training, Reinforcement Learning",
      "categories": [
        "Large Language Model",
        "Post Training",
        "Reinforcement Learning",
        "Reasoning",
        "GVPO",
        "GRPO"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7Z25QbOv4a",
      "title": "MigGPT: Harnessing Large Language Models for Automated Migration of Out-of-Tree Linux Kernel Patches Across Versions",
      "authors": [
        "Pucheng Dang",
        "Di Huang",
        "Dong Li",
        "Kang Chen",
        "Yuanbo Wen",
        "Qi Guo",
        "Xing Hu"
      ],
      "abstract": "Out-of-tree kernel patches are essential for adapting the Linux kernel to new hardware or enabling specific functionalities. Maintaining and updating these patches across different kernel versions demands significant effort from experienced engineers. Large language models (LLMs) have shown remarkable progress across various domains, suggesting their potential for automating out-of-tree kernel patch migration. However, our findings reveal that LLMs, while promising, struggle with incomplete code context understanding and inaccurate migration point identification. In this work, we propose MigGPT, a framework that employs a novel code fingerprint structure to retain code snippet information and incorporates three meticulously designed modules to improve the migration accuracy and efficiency of out-of-tree kernel patches. Furthermore, we establish a robust benchmark using real-world out-of-tree kernel patch projects to evaluate LLM capabilities. Evaluations show that MigGPT significantly outperforms the direct application of vanilla LLMs, achieving an average completion rate of 72.59\\% ($\\uparrow 50.74\\%$) for migration tasks.",
      "arxiv_url": "https://openreview.net/forum?id=7Z25QbOv4a",
      "pdf_url": "https://openreview.net/pdf/64bef410a720504a3e06996bda08b1b8135d14e7.pdf",
      "primary_category": "Code Migration, Linux Kernel, Out-of-Tree Patch Migration",
      "categories": [
        "Code Migration",
        "Linux Kernel",
        "Out-of-Tree Patch Migration",
        "large language models",
        "Code Structure Analysis"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ckW70ls93V",
      "title": "Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning",
      "authors": [
        "Kongcheng Zhang",
        "QI YAO",
        "Shunyu Liu",
        "Yingjie Wang",
        "Baisheng Lai",
        "Jieping Ye",
        "Mingli Song",
        "Dacheng Tao"
      ],
      "abstract": "Recent advances of Reinforcement Learning (RL) have highlighted its potential in complex reasoning tasks, yet effective training often relies on external supervision, which limits the broader applicability. In this work, we propose a novel self-rewarding reinforcement learning framework to enhance Large Language Model (LLM) reasoning by leveraging the consistency of intermediate reasoning states across different reasoning trajectories. Our key insight is that correct responses often exhibit consistent trajectory patterns in terms of model likelihood: their intermediate reasoning states tend to converge toward their own final answers (*high consistency*) with minimal deviation toward other candidates (*low volatility*). Inspired by this observation, we introduce CoVo, an intrinsic reward mechanism that integrates ***Co***nsistency and ***Vo***latility via a robust vector-space aggregation strategy, complemented by a curiosity bonus to promote diverse exploration. CoVo enables LLMs to perform RL in a self-rewarding manner, offering a scalable pathway for learning to reason without external supervision. Extensive experiments on diverse reasoning benchmarks show that CoVo achieves performance comparable to or even surpassing supervised RL. Our code is available at https://github.com/sastpg/CoVo.",
      "arxiv_url": "https://openreview.net/forum?id=ckW70ls93V",
      "pdf_url": "https://openreview.net/pdf/4c696fcdaaa5b6e8b53a1bf9e94c8993ee0cd433.pdf",
      "primary_category": "Self-Rewarding, Reinforcement Learning, Large Language Models",
      "categories": [
        "Self-Rewarding",
        "Reinforcement Learning",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kuzye4EPLR",
      "title": "FP4 All the Way: Fully Quantized Training of Large Language Models",
      "authors": [
        "Brian Chmiel",
        "Maxim Fishman",
        "Ron Banner",
        "Daniel Soudry"
      ],
      "abstract": "We demonstrate, for the first time, fully quantized training (FQT) of large language models (LLMs) using predominantly 4-bit floating-point (FP4) precision for weights, activations, and gradients on datasets up to 200 billion tokens. We extensively investigate key design choices for FP4, including block sizes, scaling formats, and rounding methods. Our analysis shows that the NVFP4 format, where each block of 16 FP4 values (E2M1) shares a scale represented in E4M3, provides optimal results. We use stochastic rounding for backward and update passes and round-to-nearest for the forward pass to enhance stability. Additionally, we identify a theoretical and empirical threshold for effective quantized training: when the gradient norm falls below approximately $\\sqrt{3}$ times the quantization noise, quantized training becomes less effective. Leveraging these insights, we successfully train a 7-billion-parameter model on 256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves downstream task performance comparable to a standard BF16 baseline, confirming that FP4 training is a practical and highly efficient approach for large-scale LLM training. A reference implementation is supplied in https://github.com/Anonymous1252022/fp4-all-the-way.",
      "arxiv_url": "https://openreview.net/forum?id=kuzye4EPLR",
      "pdf_url": "https://openreview.net/pdf/43b9e7285904d70ba2b9386b736d51555d621885.pdf",
      "primary_category": "Quantization, Low precision, Training",
      "categories": [
        "Quantization",
        "Low precision",
        "Training",
        "FP4",
        "Accelerations",
        "Compression"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZWOe1kkufx",
      "title": "Efficient Knowledge Transfer in Federated Recommendation for Joint Venture Ecosystem",
      "authors": [
        "Yichen Li",
        "Yijing Shan",
        "YI LIU",
        "Haozhao Wang",
        "Cheng Wang",
        "wangshi.ww",
        "Yi Wang",
        "Ruixuan Li"
      ],
      "abstract": "The current Federated Recommendation System (FedRS) focuses on personalized recommendation services and assumes clients are personalized IoT devices (e.g., Mobile phones). In this paper, we deeply dive into new but practical FedRS applications within the joint venture ecosystem. Subsidiaries engage as participants with their users and items. However, in such a situation, merely exchanging item embedding is insufficient, as user bases always exhibit both overlaps and exclusive segments, demonstrating the complexity of user information. Meanwhile, directly uploading user information is a violation of privacy and unacceptable. To tackle the above challenges, we propose an efficient and privacy-enhanced federated recommendation for the joint venture ecosystem (FR-JVE) that each client transfers more common knowledge from other clients with a distilled user's \\textit{rating preference} from the local dataset. More specifically, we first transform the local data into a new format and apply model inversion techniques to distill the rating preference with frozen user gradients before the federated training. Then, a bridge function is employed on each client side to align the local rating preference and aggregated global preference in a privacy-friendly manner. Finally, each client matches similar users to make a better prediction for overlapped users. From a theoretical perspective, we analyze how effectively FR-JVE can guarantee user privacy. Empirically, we show that FR-JVE achieves superior performance compared to state-of-the-art methods.",
      "arxiv_url": "https://openreview.net/forum?id=ZWOe1kkufx",
      "pdf_url": "https://openreview.net/pdf/6a7286b7d4960678cc61dcb585b3e42ca44f6047.pdf",
      "primary_category": "Federated Learning, Recommendation System",
      "categories": [
        "Federated Learning",
        "Recommendation System"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4oYxzssbVg",
      "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning",
      "authors": [
        "Haozhe Wang",
        "Chao Qu",
        "Zuming Huang",
        "Wei Chu",
        "Fangzhen Lin",
        "Wenhu Chen"
      ],
      "abstract": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a rethinking trigger token to the end of rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with OpenAI-o1. We conduct comprehensive ablations and analysis to provide insights into the effectiveness of our approach.",
      "arxiv_url": "https://openreview.net/forum?id=4oYxzssbVg",
      "pdf_url": "https://openreview.net/pdf/4caa878eb3dbd25718a328d937d8d6cc4e0d3100.pdf",
      "primary_category": "Vision-Language Models, Reasoning, Reinforcement Learning",
      "categories": [
        "Vision-Language Models",
        "Reasoning",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "08mjueZ0Iq",
      "title": "VLMLight: Safety-Critical Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning Architecture",
      "authors": [
        "Maonan Wang",
        "Yirong Chen",
        "Aoyu Pang",
        "Yuxin Cai",
        "Chung Shue Chen",
        "Yuheng KAN",
        "Man On Pun"
      ],
      "abstract": "Traffic signal control (TSC) is a core challenge in urban mobility, where real-time decisions must balance efficiency and safety. Existing methods—ranging from rule-based heuristics to reinforcement learning (RL)—often struggle to generalize to complex, dynamic, and safety-critical scenarios. We introduce \\textbf{VLMLight}, a novel TSC framework that integrates vision-language meta-control with dual-branch reasoning. At the core of VLMLight is the first image-based traffic simulator that enables multi-view visual perception at intersections, allowing policies to reason over rich cues such as vehicle type, motion, and spatial density. A large language model (LLM) serves as a safety-prioritized meta-controller, selecting between a fast RL policy for routine traffic and a structured reasoning branch for critical cases. In the latter, multiple LLM agents collaborate to assess traffic phases, prioritize emergency vehicles, and verify rule compliance. Experiments show that VLMLight reduces waiting times for emergency vehicles by up to 65% over RL-only systems, while preserving real-time performance in standard conditions with less than 1% degradation. VLMLight offers a scalable, interpretable, and safety-aware solution for next-generation traffic signal control.",
      "arxiv_url": "https://openreview.net/forum?id=08mjueZ0Iq",
      "pdf_url": "https://openreview.net/pdf/33b72708f460f7581306147471f9fba4a0d8fd81.pdf",
      "primary_category": "traffic signal control, urban systems, multimodal language model",
      "categories": [
        "traffic signal control",
        "urban systems",
        "multimodal language model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qjV3YHW3PD",
      "title": "Enhancing Safety in Reinforcement Learning with Human Feedback via Rectified Policy Optimization",
      "authors": [
        "Xiyue Peng",
        "Hengquan Guo",
        "jiawei zhang",
        "Dongqing Zou",
        "Ziyu Shao",
        "Honghao Wei",
        "Xin Liu"
      ],
      "abstract": "Balancing helpfulness and safety (harmlessness) is a critical challenge in aligning large language models (LLMs). Current approaches often decouple these two objectives, training separate preference models for helpfulness and safety, while framing safety as a constraint within a constrained Markov Decision Process (CMDP) framework. This paper identifies a potential issue when using the widely adopted expected safety constraints for LLM safety alignment, termed \"safety compensation'', where the constraints are satisfied on expectation, but individual prompts may trade off safety, resulting in some responses being overly restrictive while others remain unsafe. To address this issue, we propose **Rectified Policy Optimization (RePO)**, which replaces the expected safety constraint with critical safety constraints imposed on every prompt. At the core of RePO is a policy update mechanism driven by rectified policy gradients, which penalizes the strict safety violation of every prompt, thereby enhancing safety across nearly all prompts. Our experiments demonstrate that RePO outperforms strong baseline methods and significantly enhances LLM safety alignment.",
      "arxiv_url": "https://openreview.net/forum?id=qjV3YHW3PD",
      "pdf_url": "https://openreview.net/pdf/0f4ae4f5e7b22f9498313b2bd4ef85fcd6a72512.pdf",
      "primary_category": "Safe Reinforcement Learning, Reinforcement Learning with Human Feedback, Large Language Model",
      "categories": [
        "Safe Reinforcement Learning",
        "Reinforcement Learning with Human Feedback",
        "Large Language Model",
        "Safety Alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7WgYEIOLdv",
      "title": "HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery",
      "authors": [
        "Vaibhav Rathore",
        "Divyam Gupta",
        "Biplab Banerjee"
      ],
      "abstract": "Generalized Category Discovery (GCD) aims to classify test-time samples into either seen categories—available during training—or novel ones, without relying on label supervision. Most existing GCD methods assume simultaneous access to labeled and unlabeled data during training and arising from the same domain, limiting applicability in open-world scenarios involving distribution shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by requiring models to generalize to unseen domains containing novel categories, without accessing target-domain data during training.\nThe only prior DG-GCD method, DG$^2$CD-Net~\\cite{dg2net}, relies on episodic training with multiple synthetic domains and task vector aggregation, incurring high computational cost and error accumulation. We propose \\textsc{HiDISC}, a hyperbolic representation learning framework that achieves domain and category-level generalization without episodic simulation. To expose the model to minimal but diverse domain variations, we augment the source domain using GPT-guided diffusion, avoiding overfitting while maintaining efficiency.\nTo structure the representation space, we introduce \\emph{Tangent CutMix}, a curvature-aware interpolation that synthesizes pseudo-novel samples in tangent space, preserving manifold consistency. A unified loss—combining penalized Busemann alignment, hybrid hyperbolic contrastive regularization, and adaptive outlier repulsion—facilitates compact, semantically structured embeddings. A learnable curvature parameter further adapts the geometry to dataset complexity.\n\\textsc{HiDISC} achieves state-of-the-art results on PACS~\\cite{pacs}, Office-Home~\\cite{officehome}, and DomainNet~\\cite{domainnet}, consistently outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.",
      "arxiv_url": "https://openreview.net/forum?id=7WgYEIOLdv",
      "pdf_url": "https://openreview.net/pdf/98db185a9cbc023aaf46b25024606a05d21cae5c.pdf",
      "primary_category": "Domain Generalization, Category Discovery, Hyperbolic Spaces",
      "categories": [
        "Domain Generalization",
        "Category Discovery",
        "Hyperbolic Spaces"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ruzMpz4rBC",
      "title": "Language Ranker: A Lightweight Ranking framework for LLM Decoding",
      "authors": [
        "Chenheng Zhang",
        "Tianqi Du",
        "Jizhe Zhang",
        "Mingqing Xiao",
        "Yifei Wang",
        "Yisen Wang",
        "Zhouchen Lin"
      ],
      "abstract": "Conventional research on large language models (LLMs) has primarily focused on refining output distributions, while paying less attention to the decoding process that transforms these distributions into final responses. Recent advances, such as scaling the computation of inference time with reward models, have underscored the importance of decoding, but these methods often suffer from high computational costs and limited applicability.\nIn this paper, we revisit LLM generation through the lens of recommender systems, conceptualizing the decoding process as analogous to the ranking stage in recommendation pipelines. From this perspective, we observe that both traditional decoding methods and reward models exhibit clear limitations such as redundancy.\nMotivated by this insight, we propose Language Ranker, a novel framework that introduces a lightweight module to rerank candidate responses using features extracted by the base model. Experiments across a wide range of tasks show that Language Ranker achieves performance comparable to large-scale reward models, while requiring only <0.5M additional parameters, significantly reducing the computational overhead during both training and inference stages.  This highlights the efficiency and effectiveness of our method, showcasing its potential to fully unlock the capabilities of LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=ruzMpz4rBC",
      "pdf_url": "https://openreview.net/pdf/cfd4247fb814ce3b2eb3e8cadde8e5db672025b0.pdf",
      "primary_category": "language model, recommender system, inference-time computing",
      "categories": [
        "language model",
        "recommender system",
        "inference-time computing"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pdc0yEoOj6",
      "title": "Leveraging robust optimization for llm alignment under distribution shifts",
      "authors": [
        "Mingye Zhu",
        "Yi Liu",
        "Zheren Fu",
        "Yongdong Zhang",
        "Zhendong Mao"
      ],
      "abstract": "Preference alignment methods are increasingly critical for steering large language models (LLMs) to generate outputs consistent with human values. While recent approaches often rely on synthetic data generated by LLMs for scalability and cost-efficiency reasons, this reliance can introduce distributional shifts that undermine the nuanced representation of human preferences needed for desirable outputs. In this paper, we propose a novel distribution-aware optimization framework that improves preference alignment despite such shifts. Our approach first leverages well-learned classifiers to assign a calibration value to each training sample, quantifying its alignment with the target human-preferred distribution. These values are then incorporated into a robust optimization objective that minimizes the worst-case loss over regions of the data space most relevant to human preferences. By explicitly focusing optimization on the target distribution, our approach mitigates the impact of distributional mismatch and improves the generation of responses that better reflect intended values.",
      "arxiv_url": "https://openreview.net/forum?id=pdc0yEoOj6",
      "pdf_url": "https://openreview.net/pdf/df148ea36b63ff72a94648b59eb6a1e53caf5989.pdf",
      "primary_category": "preference alignment, distribution shifts, robust optimization",
      "categories": [
        "preference alignment",
        "distribution shifts",
        "robust optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "muWdWcMvpW",
      "title": "ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding",
      "authors": [
        "Junliang Ye",
        "Zhengyi Wang",
        "Ruowen Zhao",
        "Shenghao Xie",
        "Jun Zhu"
      ],
      "abstract": "Recently, the powerful text-to-image capabilities of GPT-4o have led to growing appreciation for native multimodal large language models. However, its multimodal capabilities remain confined to images and text. Yet beyond images, the ability to understand and generate 3D content is equally crucial. To address this gap, we propose ShapeLLM-Omni—a native 3D large language model capable of understanding and generating 3D assets and text in any sequence. First, we train a 3D vector-quantized variational autoencoder (VQVAE), which maps 3D objects into a discrete latent space to achieve efficient and accurate shape representation and reconstruction. Building upon the 3D-aware discrete tokens, we innovatively construct a large-scale continuous training dataset named 3D-Alpaca, encompassing generation, comprehension, and editing, thus providing rich resources for future research and training. Finally, we perform instruction-based fine-tuning of the Qwen-2.5-vl-7B-Instruct model on the 3D-Alpaca dataset, equipping it with native 3D understanding and generation capabilities. Our work represents an effective step toward extending multimodal large language models with fundamental 3D intelligence, paving the way for future advances in 3D-native AI.",
      "arxiv_url": "https://openreview.net/forum?id=muWdWcMvpW",
      "pdf_url": "https://openreview.net/pdf/0741a0f58eb89e812942a50f030289f0500c5c0b.pdf",
      "primary_category": "LLM, MLLM, 3D generation",
      "categories": [
        "LLM",
        "MLLM",
        "3D generation",
        "gpt-4o",
        "Editing"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "OFz4VDn0SO",
      "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
      "authors": [
        "Sunqi Fan",
        "Jiashuo Cui",
        "Meng-Hao Guo",
        "Shuojin Yang"
      ],
      "abstract": "Video Question Answering (VideoQA) task serves as a critical playground for evaluating whether foundation models can effectively perceive, understand, and reason about dynamic real-world scenarios. However, existing Multimodal Large Language Models (MLLMs) struggle with simultaneously ensuring the ability to model spatial relationships between video frames and to understand the causal dynamics of temporal evolution on complex and reasoning-intensive VideoQA. In this work, we equip MLLM with a comprehensive and extensible Video Toolkit, to enhance MLLM’s spatiotemporal reasoning capabilities as well as guarantee the harmony between the quantity and diversity of tools. To better control the tool invocation sequence and avoid toolchain shortcut issues, we propose a Spatiotemporal Reasoning Framework (STAR) that strategically schedules temporal and spatial tools, thereby progressively localizing the key area in the video. Our STAR framework enhances GPT-4o using lightweight tools, achieving an 8.2% gain on VideoMME and 4.6% on LongVideoBench. We believe that our proposed Video Toolkit and STAR framework make an important step towards building autonomous and intelligent video analysis assistants. The code is publicly available at https://github.com/fansunqi/VideoTool.",
      "arxiv_url": "https://openreview.net/forum?id=OFz4VDn0SO",
      "pdf_url": "https://openreview.net/pdf/c3fe7507152ba55f4cdc2d606716d226689509d6.pdf",
      "primary_category": "Video Question Answering, Tool Learning, Visual Reasoning",
      "categories": [
        "Video Question Answering",
        "Tool Learning",
        "Visual Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DTvviEnW2A",
      "title": "StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant",
      "authors": [
        "Haibo Wang",
        "Bo Feng",
        "Zhengfeng Lai",
        "Mingze Xu",
        "Shiyu Li",
        "Weifeng Ge",
        "Afshin Dehghan",
        "Meng Cao",
        "Ping Huang"
      ],
      "abstract": "We present StreamBridge, a simple yet effective framework that seamlessly transforms offline Video-LLMs into streaming-capable models. It addresses two fundamental challenges in adapting existing models into online scenarios: (1) limited capability for multi-turn real-time understanding, and (2) lack of proactive response mechanisms. Specifically, StreamBridge incorporates (1) a memory buffer combined with a round-decayed compression strategy, supporting long-context multi-turn interactions, and (2) a decoupled, lightweight activation model that can be effortlessly integrated into existing Video-LLMs, enabling continuous proactive responses. To further support StreamBridge, we construct Stream-IT, a large-scale dataset tailored for streaming video understanding, featuring interleaved video-text sequences and diverse instruction formats. Extensive experiments show that StreamBridge significantly improves the streaming understanding capabilities of offline Video-LLMs across various tasks, outperforming even proprietary models such as GPT-4o and Gemini 1.5 Pro. Simultaneously, it achieves competitive or superior performance on standard video understanding benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=DTvviEnW2A",
      "pdf_url": "https://openreview.net/pdf/f800f26e9a3f507a5c877cdd29032f1b2b575c59.pdf",
      "primary_category": "Video Large Language Model; Streaming Video Understanding",
      "categories": [
        "Video Large Language Model; Streaming Video Understanding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "r9YDEErKXU",
      "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation",
      "authors": [
        "Xinyu Yang",
        "Yuwei An",
        "Hongyi Liu",
        "Tianqi Chen",
        "Beidi Chen"
      ],
      "abstract": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit parallelism in sequential generation. Inspired by this, we introduce Multiverse, a new generative model enabling natively parallel generation. Multiverse internalizes a MapReduce paradigm, generating automatically through three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process stage for parallel subtask execution, and (iii) a Reduce stage for lossless result synthesis. Next, we build a real-world Multiverse reasoning model with co-design of data, algorithm, and system, enabling rapid and seamless transfer from frontier AR-LLMs. For data creation, we develop Multiverse Curator, an automated LLM-assisted pipeline that transforms sequential reasoning chains into structured training data, avoiding costly human annotations. Algorithmically, we design Multiverse Attention to separate parallel reasoning steps while keeping compatibility with causal attention for efficient training. Systematically, we implement Multiverse Engine to support parallel inference. It features a dedicated interpreter that dynamically switches between sequential and parallel generation, triggered directly by the model. After a 3-hour fine-tuning with 1K examples, our Multiverse-32B stands as the only open-sourced non-AR model achieving performance on par with leading AR-LLMs of the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively. Moreover, our budget control experiments show that Multiverse-32B exhibits superior scaling, outperforming AR-LLMs by 1.87% on average using the same context length. Such scaling further leads to practical efficiency gain, achieving up to 2x speedup across varying batch sizes. We have open-sourced the entire Multiverse ecosystem, including data, model weights, serving system, supporting tools, as well as data curation prompts and detailed training and evaluation recipes.",
      "arxiv_url": "https://openreview.net/forum?id=r9YDEErKXU",
      "pdf_url": "https://openreview.net/pdf/5f50a250befd3553dd40112c1a440f86b36737da.pdf",
      "primary_category": "Generative Modeling; Model Architecture; Parallel Generation; LLM Reasoning",
      "categories": [
        "Generative Modeling; Model Architecture; Parallel Generation; LLM Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cg2S1qqNSq",
      "title": "Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity",
      "authors": [
        "Susav Shrestha",
        "Bradley Settlemyer",
        "Nikoli Dryden",
        "A. L. Narasimha Reddy"
      ],
      "abstract": "Accelerating large language model (LLM) inference is critical for real-world deployments requiring high throughput and low latency. Contextual sparsity, where each token dynamically activates only a small subset of the model parameters, shows promise but does not scale to large batch sizes due to union of active neurons quickly approaching dense computation. We introduce Polar Sparsity, highlighting a key shift in sparsity importance from MLP to Attention layers as we scale batch size and sequence length. While MLP layers become more compute-efficient under batching, their sparsity vanishes. In contrast, attention becomes increasingly more expensive at scale, while their head sparsity remains stable and batch-invariant. We develop Selective Head Attention with hardware-efficient, sparsity-aware GPU kernels, delivering up to \\(2.2\\times\\) end-to-end speedups for models like OPT, LLaMA-2 \\& 3, Qwen, Mistral across various batch sizes and sequence lengths without compromising accuracy. To our knowledge, this is the first work to demonstrate that contextual sparsity can scale effectively to large batch sizes, delivering substantial inference acceleration with minimal changes, making Polar Sparsity practical for large-scale, high-throughput LLM deployment systems.",
      "arxiv_url": "https://openreview.net/forum?id=cg2S1qqNSq",
      "pdf_url": "https://openreview.net/pdf/43bf702e03ce0cb3a890cf8a35f3ded98d30b94d.pdf",
      "primary_category": "Large Language Models, Contextual Sparsity, Activation Sparsity",
      "categories": [
        "Large Language Models",
        "Contextual Sparsity",
        "Activation Sparsity",
        "Head Sparsity",
        "Select Head Attention",
        "Batched Inference",
        "Efficient Transformers",
        "Machine Learning Systems"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tVRtDIwDmQ",
      "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
      "authors": [
        "Yanming Wan",
        "Jiaxing Wu",
        "Marwa Abdulhai",
        "Lior Shani",
        "Natasha Jaques"
      ],
      "abstract": "Effective conversational agents must personalize their interactions to adapt to user preferences, personalities, and attributes across diverse domains like education and healthcare. Current methods like Reinforcement Learning from Human Feedback (RLHF), often prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized dialogues. Existing personalization approaches typically rely on extensive user history, limiting their effectiveness for new or context-limited users. To address these limitations, we propose leveraging a user model to incorporate a curiosity-based intrinsic reward into multi-turn RLHF. This novel reward mechanism encourages the agent to actively infer user traits by optimizing conversations to improve its user model's accuracy. Consequently, the agent delivers more personalized interactions by learning more about the user. We demonstrate our method's effectiveness in two distinct domains: significantly improving personalization performance in a conversational recommendation task, and personalizing conversations for different learning styles in an educational setting with improved generalization capabilities compared to traditional multi-turn RLHF, all while maintaining conversation quality. Our method offers a promising solution for creating more personalized, adaptive, and engaging conversational agents.",
      "arxiv_url": "https://openreview.net/forum?id=tVRtDIwDmQ",
      "pdf_url": "https://openreview.net/pdf/c9cd1dd6b30dd66e9e0c2d8cb230ad3af8841e5c.pdf",
      "primary_category": "Personalization in LLMs, Reinforcement Learning from Human Feedback, Conversational Agent",
      "categories": [
        "Personalization in LLMs",
        "Reinforcement Learning from Human Feedback",
        "Conversational Agent",
        "Intrinsic Motivation"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wHm5J9uanV",
      "title": "Conflict-Aware Knowledge Editing in the Wild: Semantic-Augmented Graph Representation for Unstructured Text",
      "authors": [
        "Zhange Zhang",
        "Zhicheng Geng",
        "Yuqing Ma",
        "Tianbo Wang",
        "Kai Lv",
        "Xianglong Liu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated broad applications but suffer from issues like hallucinations, erroneous outputs and outdated knowledge. Model editing emerges as an effective solution to refine knowledge in LLMs, yet existing methods typically depend on structured knowledge representations.  \nHowever, real-world knowledge is primarily embedded within complex, unstructured text. Existing structured knowledge editing approaches face significant challenges when handling the entangled and intricate knowledge present in unstructured text, resulting in issues such as representation ambiguity and editing conflicts. \nTo address these challenges, we propose a Conflict-Aware Knowledge Editing in the Wild (CAKE) framework, the first framework explicitly designed for editing knowledge extracted from wild unstructured text. \nCAKE comprises two core components: a Semantic-augmented Graph Representation module and a Conflict-aware Knowledge Editing strategy. The Semantic-augmented Graph Representation module enhances knowledge encoding through structural disambiguation, relational enrichment, and semantic diversification. Meanwhile, the Conflict-aware Knowledge Editing strategy utilizes a graph-theoretic coloring algorithm to disentangle conflicted edits by allocating them to orthogonal parameter subspaces, thereby effectively mitigating editing conflicts. Experimental results on the AKEW benchmark demonstrate that CAKE significantly outperforms existing methods, achieving a 15.43\\% improvement in accuracy on llama3 editing tasks. Our framework successfully bridges the gap between unstructured textual knowledge and reliable model editing, enabling more robust and scalable updates for practical LLM applications.",
      "arxiv_url": "https://openreview.net/forum?id=wHm5J9uanV",
      "pdf_url": "https://openreview.net/pdf/5142d41420bb64c588f184c5ebf298f99e1302e5.pdf",
      "primary_category": "Large Language Models, Model Editing, Unstructured Text",
      "categories": [
        "Large Language Models",
        "Model Editing",
        "Unstructured Text",
        "Conflict-Aware"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZyiBk1ZinG",
      "title": "DreamPRM: Domain-reweighted Process Reward Model for Multimodal Reasoning",
      "authors": [
        "Qi Cao",
        "Ruiyi Wang",
        "Ruiyi Zhang",
        "Sai Ashish Somayajula",
        "Pengtao Xie"
      ],
      "abstract": "Reasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRM's domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches. Notably, DreamPRM achieves a top-1 accuracy of 85.2% on the MathVista leaderboard using the o4-mini model, demonstrating strong generalization capability in complex multimodal reasoning tasks.",
      "arxiv_url": "https://openreview.net/forum?id=ZyiBk1ZinG",
      "pdf_url": "https://openreview.net/pdf/9293f2d3a58c73f61f83dd23599281245d58b684.pdf",
      "primary_category": "Multimodal Large Language Model, Multimodal Reasoning, Domain-reweighting",
      "categories": [
        "Multimodal Large Language Model",
        "Multimodal Reasoning",
        "Domain-reweighting",
        "Process Reward Model",
        "Bi-level Optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ULblO61XZ0",
      "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution",
      "authors": [
        "Yuxiang Wei",
        "Olivier Duchenne",
        "Jade Copet",
        "Quentin Carbonneaux",
        "LINGMING ZHANG",
        "Daniel Fried",
        "Gabriel Synnaeve",
        "Rishabh Singh",
        "Sida Wang"
      ],
      "abstract": "The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data -- the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified -- a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data.",
      "arxiv_url": "https://openreview.net/forum?id=ULblO61XZ0",
      "pdf_url": "https://openreview.net/pdf/ddd60c79265a4298a0592f27ac7d58b00b035132.pdf",
      "primary_category": "Software Engineering, Reinforcement Learning",
      "categories": [
        "Software Engineering",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0NdS4xCngO",
      "title": "Reinforcement Learning Finetunes Small Subnetworks in Large Language Models",
      "authors": [
        "Sagnik Mukherjee",
        "Lifan Yuan",
        "Dilek Hakkani-Tür",
        "Hao Peng"
      ],
      "abstract": "Reinforcement learning (RL) yields substantial improvements in large language models’ (LLMs) downstream task performance and alignment with human values. Surprisingly, such large gains result from updating only a small subnetwork comprising just 5%-30% of the parameters, with the rest effectively unchanged. We refer to this phenomenon as parameter update sparsity induced by RL. It is observed across all 7 widely-used RL algorithms (e.g., PPO, GRPO, DPO) and all 10 LLMs from different families in our experiments.\nThis sparsity is intrinsic and occurs without any explicit sparsity-promoting regularizations or architectural constraints. Finetuning the subnetwork alone recovers the test accuracy, and, remarkably, produces a model nearly identical to the one obtained via full finetuning.\nThe subnetworks from different random seeds, training data, and even RL algorithms show substantially greater overlap than expected by chance.  Our analysis suggests that this sparsity is not due to updating only a subset of layers; instead, nearly all parameter matrices receive similarly sparse updates. Moreover, the updates to almost all parameter matrices are nearly full-rank,\nsuggesting RL updates a small subset of parameters that nevertheless span almost the full  subspaces that the parameter matrices can represent. We conjecture that the this update sparsity can be primarily attributed to training on data that is near the policy distribution; \ntechniques that encourage the policy to remain close to the pretrained model, such as the KL regularization and gradient clipping, have limited impact.",
      "arxiv_url": "https://openreview.net/forum?id=0NdS4xCngO",
      "pdf_url": "https://openreview.net/pdf/9a34e049bec45c69aee22887003233d8c4c3a58b.pdf",
      "primary_category": "Large Language Models, Reinforcement Learning",
      "categories": [
        "Large Language Models",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ayzWTxb9ZD",
      "title": "Activation-Guided Consensus Merging for Large Language Models",
      "authors": [
        "Yuxuan Yao",
        "Shuqi LIU",
        "Zehua Liu",
        "Qintong Li",
        "Mingyang LIU",
        "Xiongwei Han",
        "Zhijiang Guo",
        "Han Wu",
        "Linqi Song"
      ],
      "abstract": "Recent research has increasingly focused on reconciling the reasoning capabilities of System 2 with the efficiency of System 1. While existing training-based and prompt-based approaches face significant challenges in terms of efficiency and stability, model merging emerges as a promising strategy to integrate the diverse capabilities of different Large Language Models (LLMs) into a unified model. However, conventional model merging methods often assume uniform importance across layers, overlooking the functional heterogeneity inherent in neural components. To address this limitation, we propose \\textbf{A}ctivation-Guided \\textbf{C}onsensus \\textbf{M}erging (\\textbf{ACM}), a plug-and-play merging framework that determines layer-specific merging coefficients based on mutual information between activations of pre-trained and fine-tuned models. ACM effectively preserves task-specific capabilities without requiring gradient computations or additional training. Extensive experiments on Long-to-Short (L2S) and general merging tasks demonstrate that ACM consistently outperforms all baseline methods. For instance, in the case of Qwen-7B models, TIES-Merging equipped with ACM achieves a \\textbf{55.3\\%} reduction in response length while simultaneously improving reasoning accuracy by \\textbf{1.3} points. We submit the code with the paper for reproducibility, and it will be publicly available.",
      "arxiv_url": "https://openreview.net/forum?id=ayzWTxb9ZD",
      "pdf_url": "https://openreview.net/pdf/46dbed0f3aada22261ff78f5ef77c3481fbf9eac.pdf",
      "primary_category": "LLM Reasoning, Model Merging, Long-to-Short",
      "categories": [
        "LLM Reasoning",
        "Model Merging",
        "Long-to-Short"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "rDqZjKIeda",
      "title": "Can Dependencies Induced by LLM-Agent Workflows Be Trusted?",
      "authors": [
        "Yu Yao",
        "Yiliao Song",
        "Yian Xie",
        "Mengdan Fan",
        "Mingyu Guo",
        "Tongliang Liu"
      ],
      "abstract": "LLM-agent systems often decompose high-level objectives into subtask dependency graphs, assuming that each subtask’s output is reliable and conditionally independent of others given its parent responses. \nHowever, this assumption frequently breaks during execution, as ground-truth responses are inaccessible, leading to inter-agent misalignment—failures caused by inconsistencies and coordination breakdowns among agents. \nTo address this, we propose SeqCV, a dynamic framework for reliable execution under violated conditional independence. \nSeqCV executes subtasks sequentially, each conditioned on all prior verified responses, and performs consistency checks immediately after agents generate short token sequences. \nAt each checkpoint, a token sequence is accepted only if it represents shared knowledge consistently supported across diverse LLM models; otherwise, it is discarded, triggering recursive subtask decomposition for finer-grained reasoning. \nDespite its sequential nature, SeqCV avoids repeated corrections on the same misalignment and achieves higher effective throughput than parallel pipelines. \nAcross multiple reasoning and coordination tasks, SeqCV improves accuracy by up to 30\\% over existing LLM-agent systems.\nCode is available at https://github.com/tmllab/2025_NeurIPS_SeqCV.",
      "arxiv_url": "https://openreview.net/forum?id=rDqZjKIeda",
      "pdf_url": "https://openreview.net/pdf/d84e4a86a34dd41d3f1e6d33048f4c075db78d62.pdf",
      "primary_category": "Multi-LLM-Agent System",
      "categories": [
        "Multi-LLM-Agent System"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "JkVQmaE5pK",
      "title": "SMARTraj$^2$: A Stable Multi-City Adaptive Method for Multi-View Spatio-Temporal Trajectory Representation Learning",
      "authors": [
        "Tangwen Qian",
        "Junhe Li",
        "Yile Chen",
        "Gao Cong",
        "Zezhi Shao",
        "Jun Zhang",
        "Tao Sun",
        "Fei Wang",
        "Yongjun Xu"
      ],
      "abstract": "Spatio-temporal trajectory representation learning plays a crucial role in various urban applications such as transportation systems, urban planning, and environmental monitoring. Existing methods can be divided into single-view and multi-view approaches, with the latter offering richer representations by integrating multiple sources of spatio-temporal data. However, these methods often struggle to generalize across diverse urban scenes due to multi-city structural heterogeneity, which arises from the disparities in road networks, grid layouts, and traffic regulations across cities, and the amplified seesaw phenomenon, where optimizing for one city, view, or task can degrade performance in others. These challenges hinder the deployment of trajectory learning models across multiple cities, limiting their real-world applicability. In this work, we propose SMARTraj$^2$, a novel stable multi-city adaptive method for multi-view spatio-temporal trajectory representation learning. Specifically, we introduce a feature disentanglement module to separate domain-invariant and domain-specific features, and a personalized gating mechanism to dynamically stabilize the contributions of different views and tasks. Our approach achieves superior generalization across heterogeneous urban scenes while maintaining robust performance across multiple downstream tasks. Extensive experiments on benchmark datasets demonstrate the effectiveness of SMARTraj$^2$ in enhancing cross-city generalization and outperforming state-of-the-art methods. See our project website at \\url{https://github.com/GestaltCogTeam/SMARTraj}.",
      "arxiv_url": "https://openreview.net/forum?id=JkVQmaE5pK",
      "pdf_url": "https://openreview.net/pdf/f9006947e08855ad24613b7e76291046bd967bd1.pdf",
      "primary_category": "Trajectory Representation Learning, Spatio-Temporal Data Mining, Self-Supervised Learning",
      "categories": [
        "Trajectory Representation Learning",
        "Spatio-Temporal Data Mining",
        "Self-Supervised Learning"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "JRkFZl0TJ2",
      "title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning",
      "authors": [
        "Kaiwen Zha",
        "Zhengqi Gao",
        "Maohao Shen",
        "Zhang-Wei Hong",
        "Duane S Boning",
        "Dina Katabi"
      ],
      "abstract": "Reinforcement learning (RL) has recently emerged as a compelling approach for enhancing the reasoning capabilities of large language models (LLMs), where an LLM generator serves as a policy guided by a verifier (reward model). However, current RL post-training methods for LLMs typically use verifiers that are fixed (rule-based or frozen pretrained) or trained discriminatively via supervised fine-tuning (SFT). Such designs are susceptible to reward hacking and generalize poorly beyond their training distributions. To overcome these limitations, we propose Tango, a novel framework that uses RL to concurrently train both an LLM generator and a verifier in an interleaved manner. A central innovation of Tango is its generative, process-level LLM verifier, which is trained via RL and co-evolves with the generator. Importantly, the verifier is trained solely based on outcome-level verification correctness rewards without requiring explicit process-level annotations. This generative RL-trained verifier exhibits improved robustness and superior generalization compared to deterministic or SFT-trained verifiers, fostering effective mutual reinforcement with the generator.  Extensive experiments demonstrate that both components of Tango achieve state-of-the-art results among 7B/8B-scale models: the generator attains best-in-class performance across five competition-level math benchmarks and four challenging out-of-domain reasoning tasks, while the verifier leads on the ProcessBench dataset. Remarkably, both components exhibit particularly substantial improvements on the most difficult mathematical reasoning problems.",
      "arxiv_url": "https://openreview.net/forum?id=JRkFZl0TJ2",
      "pdf_url": "https://openreview.net/pdf/df2d115dd7fe7763e94fd35d45007ed71cbcebc7.pdf",
      "primary_category": "LLM reasoning, RL post-training, Process reward models",
      "categories": [
        "LLM reasoning",
        "RL post-training",
        "Process reward models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "l75RyRcevf",
      "title": "Steering When Necessary: Flexible Steering Large Language Models with Backtracking",
      "authors": [
        "Zifeng Cheng",
        "Jinwei Gan",
        "Zhiwei Jiang",
        "Cong Wang",
        "Yafeng Yin",
        "Xiang Luo",
        "Yuchen Fu",
        "Qing Gu"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable performance across many generation tasks.\nNevertheless, effectively aligning them with desired behaviors remains a significant challenge.\nActivation steering is an effective and cost-efficient approach that directly modifies the activations of LLMs during the inference stage, aligning their responses with the desired behaviors and avoiding the high cost of fine-tuning.\nExisting methods typically indiscriminately intervene to all generations or rely solely on the question to determine intervention, which limits the accurate assessment of the intervention strength.\nTo this end, we propose the **F**lexible **A**ctivation **S**teering with **B**acktracking (**FASB**) framework, which dynamically determines both the necessity and strength of intervention by tracking the internal states of the LLMs during generation, considering both the question and the generated content.\nSince intervening after detecting a deviation from the desired behavior is often too late, we further propose the backtracking mechanism to correct the deviated tokens and steer the LLMs toward the desired behavior.\nExtensive experiments on the TruthfulQA dataset and six multiple-choice datasets demonstrate that our method outperforms baselines.\nOur code will be released at https://github.com/gjw185/FASB.",
      "arxiv_url": "https://openreview.net/forum?id=l75RyRcevf",
      "pdf_url": "https://openreview.net/pdf/795a175c3b3b1e9de873069086e754159a4e855b.pdf",
      "primary_category": "Activation steering",
      "categories": [
        "Activation steering"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vm84b0Gksj",
      "title": "DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning",
      "authors": [
        "Yongxin He",
        "Shan Zhang",
        "Yixuan Cao",
        "Lei Ma",
        "Ping Luo"
      ],
      "abstract": "Detecting AI-involved text is essential for combating misinformation, plagiarism, and academic misconduct. \nHowever, AI text generation includes diverse collaborative processes (AI-written text edited by humans, human-written text edited by AI, and AI-generated text refined by other AI), where various or even new LLMs could be involved. Texts generated through these varied processes exhibit complex characteristics, presenting significant challenges for detection. Current methods model these processes rather crudely, primarily employing binary classification (purely human vs. AI-involved) or multi-classification (treating human-AI collaboration as a new class).\nWe observe that representations of texts generated through different processes exhibit inherent clustering relationships.\nTherefore, we propose DETree, a novel approach that models the relationships among different processes as a Hierarchical Affinity Tree structure, and introduces a specialized loss function that aligns text representations with this tree. To facilitate this learning, we developed RealBench, a comprehensive benchmark dataset that automatically incorporates a wide spectrum of hybrid texts produced through various human-AI collaboration processes.\nOur method improves performance in hybrid text detection tasks and significantly enhances robustness and generalization in out-of-distribution scenarios, particularly in few-shot learning conditions, further demonstrating the promise of training-based approaches in OOD settings. \nOur code and dataset are available at https://github.com/heyongxin233/DETree.",
      "arxiv_url": "https://openreview.net/forum?id=vm84b0Gksj",
      "pdf_url": "https://openreview.net/pdf/1194adf672a09c33d593b1a8ca2387cfb0269409.pdf",
      "primary_category": "AI-generated text detection; hybrid text attribution; few-shot adaptation;",
      "categories": [
        "AI-generated text detection; hybrid text attribution; few-shot adaptation;"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "w1Y7RZC3QT",
      "title": "Document Summarization with Conformal Importance Guarantees",
      "authors": [
        "Bruce Kuwahara",
        "Chen-Yuan Lin",
        "Xiao Shi Huang",
        "Kin Kwan Leung",
        "Jullian Arta Yapeter",
        "Ilya Stanevich",
        "Felipe Perez",
        "Jesse C. Cresswell"
      ],
      "abstract": "Automatic summarization systems have advanced rapidly with large language models (LLMs), yet they still lack reliable guarantees on inclusion of critical content in high-stakes domains like healthcare, law, and finance. In this work, we introduce Conformal Importance Summarization, the first framework for importance-preserving summary generation which uses conformal prediction to provide rigorous, distribution-free coverage guarantees. By calibrating thresholds on sentence-level importance scores, we enable extractive document summarization with user-specified coverage and recall rates over critical content. Our method is model-agnostic, requires only a small calibration set, and seamlessly integrates with existing black-box LLMs. Experiments on established summarization benchmarks demonstrate that Conformal Importance Summarization achieves the theoretically assured information coverage rate. Our work suggests that Conformal Importance Summarization can be combined with existing techniques to achieve reliable, controllable automatic summarization, paving the way for safer deployment of AI summarization tools in critical applications. Code is available at github.com/layer6ai-labs/conformal-importance-summarization.",
      "arxiv_url": "https://openreview.net/forum?id=w1Y7RZC3QT",
      "pdf_url": "https://openreview.net/pdf/069a3bed60dd08e0fff9c2695ca29e20a9d9b8db.pdf",
      "primary_category": "Document Summarization, Conformal Prediction, Large Language Models",
      "categories": [
        "Document Summarization",
        "Conformal Prediction",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qUmULptJuY",
      "title": "CoC-VLA: Delving into Adversarial Domain Transfer for Explainable Autonomous Driving via Chain-of-Causality Visual-Language-Action Model",
      "authors": [
        "Dapeng Zhang",
        "Fei Shen",
        "Rui Zhao",
        "Yinda Chen",
        "Peng Zhi",
        "Chenyang Li",
        "Rui Zhou",
        "Qingguo Zhou"
      ],
      "abstract": "Autonomous driving represents a prominent application of artificial intelligence. Recent approaches have shifted from focusing solely on common scenarios to addressing complex, long-tail situations such as subtle human behaviors, traffic accidents, and non-compliant driving patterns. Given the demonstrated capabilities of large language models (LLMs) in understanding visual and natural language inputs and following instructions, recent methods have integrated LLMs into autonomous driving systems to enhance reasoning, interpretability, and performance across diverse scenarios. However, existing methods typically rely either on real-world data, which is suitable for industrial deployment, or on simulation data tailored to rare or hard case scenarios. Few approaches effectively integrate the complementary advantages of both data sources. To address this limitation, we propose a novel VLM-guided, end-to-end adversarial transfer framework for autonomous driving that transfers long-tail handling capabilities from simulation to real-world deployment, named CoC-VLA. The framework comprises a teacher VLM model, a student VLM model, and a discriminator. Both the teacher and student VLM models utilize a shared base architecture, termed the Chain-of-Causality Visual–Language Model (CoC VLM), which integrates temporal information via an end-to-end text adapter. This architecture supports chain-of-thought reasoning to infer complex driving logic. The teacher and student VLM models are pre-trained separately on simulated and real-world datasets. The discriminator is trained adversarially to facilitate the transfer of long-tail handling capabilities from simulated to real-world environments by the student VLM model, using a novel backpropagation strategy. Experimental results show that our method effectively bridges the gap between simulation and real-world autonomous driving, indicating a promising direction for future research.",
      "arxiv_url": "https://openreview.net/forum?id=qUmULptJuY",
      "pdf_url": "https://openreview.net/pdf/f627b3dfe1bdfe701ec48e3c7441d77b85356ed3.pdf",
      "primary_category": "VLA, VLM, LLM-AD",
      "categories": [
        "VLA",
        "VLM",
        "LLM-AD",
        "Domain Transfer",
        "Autonomous Driving",
        "End-to-end."
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pAfdjWD9pN",
      "title": "On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks",
      "authors": [
        "Stephen Obadinma",
        "Xiaodan Zhu"
      ],
      "abstract": "Robust verbal confidence generated by large language models (LLMs) is crucial for the deployment of LLMs to help ensure transparency, trust, and safety in many applications, including those involving human-AI interactions. In this paper, we present the first comprehensive study on the robustness of verbal confidence under adversarial attacks. We introduce attack frameworks targeting verbal confidence scores through both perturbation and jailbreak-based methods, and demonstrate that these attacks can significantly impair verbal confidence estimates and lead to frequent answer changes. We examine a variety of prompting strategies, model sizes, and application domains, revealing that current verbal confidence is vulnerable and that commonly used defence techniques are largely ineffective or counterproductive. Our findings underscore the need to design robust mechanisms for confidence expression in LLMs, as even subtle semantic-preserving modifications can lead to misleading confidence in responses.",
      "arxiv_url": "https://openreview.net/forum?id=pAfdjWD9pN",
      "pdf_url": "https://openreview.net/pdf/fe64f710bc75c3c1bb76036066655b8005bcb6b5.pdf",
      "primary_category": "adversarial attacks, robustness, uncertainty",
      "categories": [
        "adversarial attacks",
        "robustness",
        "uncertainty",
        "confidence_elicitation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "WyQ20adbUb",
      "title": "Learning to Rank for In-Context Example Retrieval",
      "authors": [
        "Yuwen Ji",
        "Luodan Zhang",
        "Ambyerhan",
        "Haoran Que",
        "Lei Shi",
        "Wang Chao",
        "Yue Zhang"
      ],
      "abstract": "Recent advances in retrieval-based in-context learning (ICL) train the retriever using a classification objective, which categorizes in-context examples (ICEs) into the most useful and the rest based on absolute scores. However, during inference, ICEs are retrieved by score ranking rather than classification  — The classification training objective deviates from this test scenario. Hence, in this paper, we propose a novel algorithm that trains a retrieval model by ranking formulation, where the preference rankings between ICEs are given by comparing the likelihood of the LLM generating the correct answer conditioned on each exemplar. By learning to rank, we motivate the retriever to automatically learn diverse rationales why specific examples are more useful for ICL decisions. This addresses the issue that classification models poorly capture broader utility. Experimental results demonstrate the top-1 performance of our proposal across 9 NLP tasks, with ablation studies and case studies further validating the effectiveness of our design. *The code can be found in: https://github.com/2022neo/SeDPO_NIPS25*",
      "arxiv_url": "https://openreview.net/forum?id=WyQ20adbUb",
      "pdf_url": "https://openreview.net/pdf/d195e7e6ca2ebc6cb55a5ec3a92cc3ca7c97e43f.pdf",
      "primary_category": "few-shot learning, in-context learning",
      "categories": [
        "few-shot learning",
        "in-context learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vAxGuGmshO",
      "title": "AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Document Understanding",
      "authors": [
        "Ahmed Masry",
        "Juan A. Rodriguez",
        "Tianyu Zhang",
        "Suyuchen Wang",
        "Chao Wang",
        "Aarash Feizi",
        "Akshay Kalkunte Suresh",
        "Abhay Puri",
        "Xiangru Jian",
        "Pierre-Andre Noel",
        "Sathwik Tejaswi Madhusudhan",
        "Marco Pedersoli",
        "Bang Liu",
        "Nicolas Chapados",
        "Yoshua Bengio",
        "Enamul Hoque",
        "Christopher Pal",
        "Issam H. Laradji",
        "David Vazquez",
        "Perouz Taslakian",
        "Spandana Gella",
        "Sai Rajeswar"
      ],
      "abstract": "Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), lack inductive bias to constrain visual features within the linguistic structure of the LLM’s embedding space, making them data-hungry and prone to cross-modal misalignment. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where visual and textual modalities are highly correlated. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods, with larger gains on document understanding and under low-resource setups. We provide further analysis demonstrating its efficiency and robustness to noise.",
      "arxiv_url": "https://openreview.net/forum?id=vAxGuGmshO",
      "pdf_url": "https://openreview.net/pdf/9e37ed13b5a2079e07e1423e14d3ca72f9054194.pdf",
      "primary_category": "vision language models, multimodal, large language models",
      "categories": [
        "vision language models",
        "multimodal",
        "large language models"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "gkG8JOOUF4",
      "title": "Adaptive Preference Arithmetic: A Personalized Agent with Adaptive Preference Arithmetic for Dynamic Preference Modeling",
      "authors": [
        "Hongyi Nie",
        "Yaqing Wang",
        "Mingyang Zhou",
        "Feiyang Pan",
        "Quanming Yao",
        "Zhen Wang"
      ],
      "abstract": "As large language models (LLMs) are increasingly used as personalized user assistants, effectively adapting to users' evolving preferences is critical for delivering high-quality personalized responses. While user preferences are often stable in content, their relative strengths shift over time due to changing goals and contexts. Therefore, modeling these dynamic preference strengths can enable finer-grained personalization. However, current methods face two major challenges: (i) limited user feedback makes it difficult to estimate preference strengths accurately, and (ii) natural language ambiguity limits the controllability of preference-guided generation. To address these issues, we propose AdaPA-Agent, a LLM-agent personalization framework that models dynamic preference strengths via Adaptive Preference Arithmetic. First,  instead of requiring additional user feedback, AdaPA-Agent employs an alignment-based strength estimation module to estimate the strength of user preferences from the existing user-agent interaction. Then, it guides controllable personalized generation by linearly combining next-token distributions, weighted by the estimated strengths of individual preferences. Experiments on two personalization tasks-conversational recommendation and personalized web interaction-demonstrate that AdaPA-Agent better aligning with users' changing intents, and has achieved over 18.9\\% and 14.2\\% improvements compared to ReAct, the widely-used agent framework.",
      "arxiv_url": "https://openreview.net/forum?id=gkG8JOOUF4",
      "pdf_url": "https://openreview.net/pdf/b4c1b22b6f2525c2e8b3988cbd3955898ddcb6b5.pdf",
      "primary_category": "Personalized LLM Agents, Dynamic Preference Modeling, Adaptive Preference Arithmetic",
      "categories": [
        "Personalized LLM Agents",
        "Dynamic Preference Modeling",
        "Adaptive Preference Arithmetic"
      ],
      "tags": [
        "LLM",
        "Personalization",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ENkss6GiKG",
      "title": "KScope: A Framework for Characterizing the Knowledge Status of Language Models",
      "authors": [
        "Yuxin Xiao",
        "Shan Chen",
        "Jack Gallifant",
        "Danielle Bitterman",
        "Thomas Hartvigsen",
        "Marzyeh Ghassemi"
      ],
      "abstract": "Characterizing a large language model's (LLM's) knowledge of a given question is challenging.\nAs a result, prior work has primarily examined LLM behavior under knowledge conflicts, where the model's internal parametric memory contradicts information in the external context.\nHowever, this does not fully reflect how well the model knows the answer to the question.\nIn this paper, we first introduce a taxonomy of five knowledge statuses based on the consistency and correctness of LLM knowledge modes.\nWe then propose KScope, a hierarchical framework of statistical tests that progressively refines hypotheses about knowledge modes and characterizes LLM knowledge into one of these five statuses. \nWe apply KScope to nine LLMs across four datasets and systematically establish:\n(1) Supporting context narrows knowledge gaps across models.\n(2) Context features related to difficulty, relevance, and familiarity drive successful knowledge updates.\n(3) LLMs exhibit similar feature preferences when partially correct or conflicted, but diverge sharply when consistently wrong.\n(4) Context summarization constrained by our feature analysis, together with enhanced credibility, further improves update effectiveness and generalizes across LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=ENkss6GiKG",
      "pdf_url": "https://openreview.net/pdf/c2f366f7999248851d27f7869cde4afa75058485.pdf",
      "primary_category": "Knowledge Conflict, Knowledge Characterization, Large Language Model",
      "categories": [
        "Knowledge Conflict",
        "Knowledge Characterization",
        "Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wPdBe9zxNr",
      "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
      "authors": [
        "Yinjie Wang",
        "Ling Yang",
        "Ye Tian",
        "Ke Shen",
        "Mengdi Wang"
      ],
      "abstract": "Mathematical reasoning in large language models has been successfully incentivized through reinforcement learning with verifiable rewards, leading to improved one-shot precision. In this work, we turn our focus to the coding domain. Beyond one-shot precision, we highlight unit test generation as another key factor for enhancing coding ability, since accurate unit tests are essential for enabling self-checking and self-correction during inference.\nTraditional approaches for fine-tuning LLMs on unit test generation rely heavily on ground-truth code solutions in the training data. We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes—without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder’s mistakes.\nThrough extensive evaluations, we demonstrate that our CURE models, derived from base models of varying sizes, excel in both code generation and unit test generation. They naturally extend to downstream tasks such as test-time scaling—achieving a 6.2\\% improvement over the base model—and agentic unit test generation, with a 25.1\\% improvement. Our 4B model consistently outperforms Qwen3-4B while achieving 64.8\\% inference efficiency in unit test generation. Notably, we also find that the CURE model can serve as an effective reward model for reinforcement learning on base models, even in the absence of any labeled supervision.",
      "arxiv_url": "https://openreview.net/forum?id=wPdBe9zxNr",
      "pdf_url": "https://openreview.net/pdf/f11013ca9dbe05459e89557ddb9b09d292b6f6ba.pdf",
      "primary_category": "Code LLM, Reinforcement Learning, Unit Test Generation",
      "categories": [
        "Code LLM",
        "Reinforcement Learning",
        "Unit Test Generation"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "dRjt4vlYVQ",
      "title": "Weaver: Shrinking the Generation-Verification Gap by Scaling Compute for Verification",
      "authors": [
        "Jon Saad-Falcon",
        "E. Kelly Buchanan",
        "Mayee F Chen",
        "Tzu-Heng Huang",
        "Brendan McLaughlin",
        "Tanvir Bhathal",
        "Shang Zhu",
        "Ben Athiwaratkun",
        "Frederic Sala",
        "Scott Linderman",
        "Azalia Mirhoseini",
        "Christopher Re"
      ],
      "abstract": "Verifiers can improve language model (LM) capabilities by providing feedback or selecting the best response from a pool of generated candidates. Currently, high-quality verifiers are either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean for formal proofs). While LM judges and reward models have become broadly useful as general-purpose verifiers, a significant performance gap remains between them and oracle verifiers. To help close this gap, we introduce Weaver, a framework for designing a strong verifier by combining multiple weak, imperfect verifiers. First we find that weighted ensembles of verifiers, which typically require learning from labeled data, significantly outperform unweighted combinations due to differences in the verifiers. To reduce the dependency on labeled data, Weaver leverages weak supervision to estimate each verifier’s accuracy and combines their outputs into a unified score that better reflects true response quality. However, directly applying weak supervision algorithms poses several challenges, including inconsistent verifier output formats and handling low-quality verifiers. Weaver addresses these challenges by using dataset statistics to normalize outputs and filter specific verifiers. We study the effectiveness of Weaver in repeated sampling settings, where a model generates multiple candidate responses at test time and a verifier is used to select the correct one. Our evaluations demonstrate that Weaver significantly improves the pass@1 performance across several reasoning and math tasks, achieving o3-mini level accuracy with Llama 3.3 70B Instruct (a much cheaper non-reasoning model) as the generator, and an ensemble of smaller judge and reward models as the verifiers (86.2% average). This gain mirrors the jump achieved between GPT-4o and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and post-training interventions. To make Weaver more efficient, we train a compact 400M cross-encoder using Weaver's combined output scores. This distilled model retains 98.7% of Weaver's full accuracy while reducing verification compute by up to 99.97%.",
      "arxiv_url": "https://openreview.net/forum?id=dRjt4vlYVQ",
      "pdf_url": "https://openreview.net/pdf/e96545cd91e3d21ec915c2a775d66c7fe5f57eb6.pdf",
      "primary_category": "test-time compute, repeated sampling, weak supervision",
      "categories": [
        "test-time compute",
        "repeated sampling",
        "weak supervision",
        "weak verification"
      ],
      "tags": [
        "LLM",
        "Information Retrieval"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "knPz7gtjPW",
      "title": "Superposition Yields Robust Neural Scaling",
      "authors": [
        "Yizhou Liu",
        "Ziming Liu",
        "Jeff Gore"
      ],
      "abstract": "The success of today's large language models (LLMs) depends on the observation that larger models perform better. However, the origin of this neural scaling law, that loss decreases as a power law with model size, remains unclear. We propose that representation superposition, meaning that LLMs represent more features than they have dimensions, can be a key contributor to loss and cause neural scaling. Based on Anthropic's toy model, we use weight decay to control the degree of superposition, allowing us to systematically study how loss scales with model size. When superposition is weak, the loss follows a power law only if data feature frequencies are power-law distributed. In contrast, under strong superposition, the loss generically scales inversely with model dimension across a broad class of frequency distributions, due to geometric overlaps between representation vectors. We confirmed that open-sourced LLMs operate in the strong superposition regime and have loss scaling inversely with model dimension, and that the Chinchilla scaling laws are also consistent with this behavior. Our results identify representation superposition as a central driver of neural scaling laws, providing insights into questions like when neural scaling laws can be improved and when they will break down.",
      "arxiv_url": "https://openreview.net/forum?id=knPz7gtjPW",
      "pdf_url": "https://openreview.net/pdf/d2a00e839d3db6f816cc3c312e12b16fe4feb3d3.pdf",
      "primary_category": "LLM, superposition, neural scaling law",
      "categories": [
        "LLM",
        "superposition",
        "neural scaling law"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "29FRqmVQK8",
      "title": "Scalable Best-of-N Selection for Large Language Models via Self-Certainty",
      "authors": [
        "Zhewei Kang",
        "Xuandong Zhao",
        "Dawn Song"
      ],
      "abstract": "Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size $N$, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at https://github.com/backprop07/Self-Certainty",
      "arxiv_url": "https://openreview.net/forum?id=29FRqmVQK8",
      "pdf_url": "https://openreview.net/pdf/f6da911febd36fcc74feee9f592a1d0dbfdb80eb.pdf",
      "primary_category": "Best-of-N, Reasoning, LLM",
      "categories": [
        "Best-of-N",
        "Reasoning",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "dH8mKmvADv",
      "title": "Learning in Compact Spaces with Approximately Normalized Transformer",
      "authors": [
        "Jörg K.H. Franke",
        "Urs Spiegelhalter",
        "Marianna Nezhurina",
        "Jenia Jitsev",
        "Frank Hutter",
        "Michael Hefenbrock"
      ],
      "abstract": "The successful training of deep neural networks requires addressing challenges such as overfitting, numerical instabilities leading to divergence, and increasing variance in the residual stream. A common solution is to apply regularization and normalization techniques that usually require tuning additional hyperparameters. An alternative is to force all parameters and representations to lie on a hypersphere. This removes the need for regularization and increases convergence speed, but comes with additional costs. In this work, we propose a more holistic, approximate normalization via simple scalar multiplications motivated by the tight concentration of the norms of high-dimensional random vectors. Additionally, instead of applying strict normalization for the parameters, we constrain their norms. These modifications remove the need for weight decay and learning rate warm-up as well, but do not increase the total number of normalization layers. Our experiments with transformer architectures show up to 40% faster convergence compared to GPT models with QK normalization, with only 3% additional runtime cost. When deriving scaling laws, we found that our method enables training with larger batch sizes while preserving the favorable scaling characteristics of classic GPT architectures.",
      "arxiv_url": "https://openreview.net/forum?id=dH8mKmvADv",
      "pdf_url": "https://openreview.net/pdf/162411a97e05bbb815f8b5409e8c2599f9ea6419.pdf",
      "primary_category": "Deep Learning, Transformer, GPT",
      "categories": [
        "Deep Learning",
        "Transformer",
        "GPT",
        "Normalization",
        "Regularization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IBFnEaArnz",
      "title": "Predicting the Performance of Black-box Language Models with Follow-up Queries",
      "authors": [
        "Dylan Sam",
        "Marc Anton Finzi",
        "J Zico Kolter"
      ],
      "abstract": "Reliably predicting the behavior of language models---such as whether their outputs are correct or have been adversarially manipulated---is a fundamentally challenging task. This is often made even more difficult as frontier language models are offered only through closed-source APIs, providing only black-box access. In this paper, we predict the behavior of black-box language models by asking follow-up questions and taking the probabilities of responses _as_ representations to train reliable predictors.\nWe first demonstrate that training a linear model on these responses reliably and accurately predicts model correctness on question-answering and reasoning benchmarks. \nSurprisingly, this can _even outperform white-box linear predictors_ that operate over model internals or activations.\nFurthermore, we demonstrate that these follow-up question responses can reliably distinguish between a clean version of an LLM and one that has been adversarially influenced via a system prompt to answer questions incorrectly or to introduce bugs into generated code. \nFinally, we show that they can also be used to differentiate between black-box LLMs, enabling the detection of misrepresented models provided through an API. \nOverall, our work shows promise in monitoring black-box language model behavior, supporting their deployment in larger, autonomous systems.",
      "arxiv_url": "https://openreview.net/forum?id=IBFnEaArnz",
      "pdf_url": "https://openreview.net/pdf/a9530414a4fd5f326629f3402e2729d04dac678a.pdf",
      "primary_category": "monitoring, language models, black-box language models",
      "categories": [
        "monitoring",
        "language models",
        "black-box language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wtcv48HImz",
      "title": "Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning",
      "authors": [
        "Van Yang",
        "Zirui Liu",
        "Hongye Jin",
        "Qingyu Yin",
        "Vipin Chaudhary",
        "Xiaotian Han"
      ],
      "abstract": "Recent language models exhibit strong reasoning capabilities, yet the influence of long-context capacity on reasoning remains underexplored. In this work, we hypothesize that current limitations in reasoning stem, in part, from insufficient long-context capacity, motivated by empirical observations such as i) higher context window length often leads to stronger reasoning performance, and ii) failed reasoning cases resemble failed long-context cases. To test this hypothesis, we examine whether enhancing a model’s long-context ability before Supervised Fine-Tuning (SFT) leads to improved reasoning performance. Specifically, we compared models with identical architectures and fine-tuning data but varying levels of long-context capacity. Our results reveal a consistent trend: models with stronger long-context capacity achieve significantly higher accuracy on reasoning benchmarks after SFT. Notably, these gains persist even on tasks with short input lengths, indicating that long-context training offers generalizable benefits for reasoning performance. These findings suggest that long-context modeling is not just essential for processing lengthy inputs, but also serves as a critical foundation for reasoning. We advocate for treating long-context capacity as a first-class objective in the design of future language models.",
      "arxiv_url": "https://openreview.net/forum?id=wtcv48HImz",
      "pdf_url": "https://openreview.net/pdf/3aa73c81186850fe236acaa4a81d2af3f1e5c910.pdf",
      "primary_category": "Long Context Ability; LLM Reasoning",
      "categories": [
        "Long Context Ability; LLM Reasoning"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "btBqWTbf6q",
      "title": "Pre-trained Large Language Models Learn to Predict Hidden Markov Models In-context",
      "authors": [
        "Yijia Dai",
        "Zhaolin Gao",
        "Yahya Sattar",
        "Sarah Dean",
        "Jennifer J. Sun"
      ],
      "abstract": "Hidden Markov Models (HMMs) are fundamental tools for modeling sequential data with latent states that follow Markovian dynamics. \nHowever, they present significant challenges in model fitting and computational efficiency on real-world datasets. \nIn this work, we demonstrate that pre-trained large language models (LLMs) can effectively model data generated by HMMs through in-context learning (ICL) — their ability to learn patterns from examples within the input context.\nWe evaluate LLMs' performance on diverse synthetic HMMs, showing that their prediction accuracy converges to the theoretical optimum. We discover novel scaling trends influenced by HMM properties and provide theoretical conjectures for these empirical observations. Furthermore, we present practical guidelines for scientists on using ICL as a diagnostic tool for complex data. Applied to real-world animal decision-making tasks, ICL achieves competitive performance with models designed by human experts. Our results demonstrate potential for advancing understanding of LLMs' capabilities while opening new avenues for scientific discovery of biological mechanisms and hidden structures in real-world phenomena.",
      "arxiv_url": "https://openreview.net/forum?id=btBqWTbf6q",
      "pdf_url": "https://openreview.net/pdf/d9c6c3a5ef2b457a99d3851f69c626f9896663ed.pdf",
      "primary_category": "In-context learning; LLM; Hidden Markov models; Neuroscience; Decision-making; Animal behavior.",
      "categories": [
        "In-context learning; LLM; Hidden Markov models; Neuroscience; Decision-making; Animal behavior."
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IkvQqD7hk3",
      "title": "With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You",
      "authors": [
        "Fabian Gröger",
        "Shuo Wen",
        "Huyen Le",
        "Maria Brbic"
      ],
      "abstract": "Multimodal models have demonstrated powerful capabilities in complex tasks requiring multimodal alignment, including zero-shot classification and cross-modal retrieval. However, existing models typically rely on millions of paired multimodal samples, which are prohibitively expensive or infeasible to obtain in many domains. In this work, we explore the feasibility of building multimodal models with limited amount of paired data by aligning pretrained unimodal foundation models. We show that high-quality alignment is possible with as few as tens of thousands of paired samples$\\unicode{x2013}$less than 1\\% of the data typically used in the field. To achieve this, we introduce STRUCTURE, an effective regularization technique that preserves the neighborhood geometry of the latent space of unimodal encoders. Additionally, we show that aligning last layers is often suboptimal and demonstrate the benefits of aligning the layers with the highest representational similarity across modalities. These two components can be readily incorporated into existing alignment methods, yielding substantial gains across 24 zero-shot image classification and retrieval benchmarks, with average relative improvement of 51.6\\% in classification and 91.8\\% in retrieval tasks. Our results highlight the effectiveness and broad applicability of our framework for limited-sample multimodal learning and offer a promising path forward for resource-constrained domains.",
      "arxiv_url": "https://openreview.net/forum?id=IkvQqD7hk3",
      "pdf_url": "https://openreview.net/pdf/303445ce5f77b32d7a9879feca4a5107f027503a.pdf",
      "primary_category": "multimodal learning, vision language models, representation learning",
      "categories": [
        "multimodal learning",
        "vision language models",
        "representation learning"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pdE9onSn2h",
      "title": "SnapMoGen: Human Motion Generation from Expressive Texts",
      "authors": [
        "chuan guo",
        "Inwoo Hwang",
        "Jian Wang",
        "Bing Zhou"
      ],
      "abstract": "Text-to-motion generation has experienced remarkable progress in recent years. However, current approaches remain limited to synthesizing motion from short or general text prompts, primarily due to dataset constraints. This limitation undermines fine-grained controllability and generalization to unseen prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset featuring high-quality motion capture data paired with accurate, \\textit{expressive} textual annotations. The dataset comprises 20K motion clips totaling 44 hours, accompanied by 122 detailed textual descriptions averaging 48 words per description (vs. 12 words of HumanML3D). Importantly, these motion clips preserve original temporal continuity as they were in long sequences, facilitating research in long-term motion generation and blending. We also improve upon previous generative masked modeling approaches. Our model, MoMask++, transforms motion into \\textbf{multi-scale} token sequences that better exploit the token capacity, and learns to generate all tokens using a single generative masked transformer. MoMask++ achieves state-of-the-art performance on both HumanML3D and OmniMotion benchmarks. Additionally, we demonstrate the ability to process casual user prompts by employing an LLM to reformat inputs to align with the expressivity and narration style of SnapMoGen.",
      "arxiv_url": "https://openreview.net/forum?id=pdE9onSn2h",
      "pdf_url": "https://openreview.net/pdf/18d69dcfe7d47808e1e6a6879564c7a94c803712.pdf",
      "primary_category": "Human Motion Generation",
      "categories": [
        "Human Motion Generation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RDt0crdC7N",
      "title": "Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning",
      "authors": [
        "Yibo Zhao",
        "Yang Zhao",
        "Hongru Du",
        "Hao Frank Yang"
      ],
      "abstract": "Decision-making models for individuals, particularly in high-stakes scenarios like vaccine uptake, often diverge from population optimal predictions. This gap arises from the uniqueness of the individual decision-making process, shaped by numerical attributes (e.g., cost, time) and linguistic influences (e.g., personal preferences and constraints). Developing upon Utility Theory and leveraging the textual-reasoning capabilities of Large Language Models (LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric Reasoning framework (ATHENA) to address the optimal information integration. ATHENA uniquely integrates two stages: First, it discovers robust, group-level symbolic utility functions via LLM-augmented symbolic discovery; Second, it implements individual-level semantic adaptation, creating personalized semantic templates guided by the optimal utility to model personalized choices. Validated on real-world travel mode and vaccine choice tasks, ATHENA consistently outperforms utility-based, machine learning, and other LLM-based models, lifting F1 score by at least 6.5\\% over the strongest cutting-edge models.  Further, ablation studies confirm that both stages of ATHENA are critical and complementary, as removing either clearly degrades overall predictive performance. By organically integrating symbolic utility modeling and semantic adaptation, ATHENA provides a new scheme for modeling human-centric decisions. The project page can be found at https://yibozh.github.io/Athena.",
      "arxiv_url": "https://openreview.net/forum?id=RDt0crdC7N",
      "pdf_url": "https://openreview.net/pdf/01b31fa5af9c9653add7707221883198df05a8ae.pdf",
      "primary_category": "Personalized Decision Making, Symbolic Regression, LLM",
      "categories": [
        "Personalized Decision Making",
        "Symbolic Regression",
        "LLM"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wpkmEV57YT",
      "title": "Learning to Focus: Causal Attention Distillation via Gradient‐Guided Token Pruning",
      "authors": [
        "Yiju Guo",
        "Wenkai Yang",
        "Zexu Sun",
        "Ning Ding",
        "Zhiyuan Liu",
        "Yankai Lin"
      ],
      "abstract": "Large language models (LLMs) have demonstrated significant improvements in contextual understanding. However, their ability to attend to truly critical information during long-context reasoning and generation still falls behind the pace. Specifically, our preliminary experiments reveal that certain distracting patterns can misdirect the model’s attention during inference, and removing these patterns substantially improves reasoning accuracy and generation quality. We attribute this phenomenon to spurious correlations in the training data, which obstruct the model’s capacity to infer authentic causal instruction–response relationships. This phenomenon may induce redundant reasoning processes, potentially resulting in significant inference overhead and, more critically, the generation of erroneous or suboptimal responses. To mitigate this, we introduce a two-stage framework called Learning to Focus (LeaF) leveraging intervention-based inference to disentangle confounding factors. In the first stage, LeaF employs gradient-based comparisons with an advanced teacher to automatically identify confounding tokens based on causal relationships in the training corpus. Then, in the second stage, it prunes these tokens during distillation to enact intervention, aligning the student’s attention with the teacher’s focus distribution on truly critical context tokens. Experimental results demonstrate that LeaF not only achieves an absolute improvement in various mathematical reasoning, code generation and multi-hop question answering benchmarks but also effectively suppresses attention to confounding tokens during inference, yielding a more interpretable and reliable reasoning model.",
      "arxiv_url": "https://openreview.net/forum?id=wpkmEV57YT",
      "pdf_url": "https://openreview.net/pdf/d3824bda73a525773efe78f8716af085acd1fe56.pdf",
      "primary_category": "Knowledge Distillation, Spurious Correlations, Long-Context Reasoning",
      "categories": [
        "Knowledge Distillation",
        "Spurious Correlations",
        "Long-Context Reasoning",
        "Model Interpretability",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "jR1lvwexLt",
      "title": "Self-Evolving Pseudo-Rehearsal for Catastrophic Forgetting with Task Similarity in LLMs",
      "authors": [
        "Jun Wang",
        "Liang Ding",
        "Shuai Wang",
        "Hongyu Li",
        "Yong Luo",
        "Huangxuan Zhao",
        "Han Hu",
        "Bo Du"
      ],
      "abstract": "Continual learning for large language models (LLMs) demands a precise balance between $\\textbf{plasticity}$ - the ability to absorb new tasks - and $\\textbf{stability}$ - the preservation of previously learned knowledge. Conventional rehearsal methods, which replay stored examples, are limited by long-term data inaccessibility; earlier pseudo-rehearsal methods require additional generation modules, while self-synthesis approaches often generate samples that poorly align with real tasks, suffer from unstable outputs, and ignore task relationships. We present $\\textbf{\\textit{Self-Evolving Pseudo-Rehearsal for Catastrophic Forgetting with Task Similarity}}(\\textbf{SERS})$, a lightweight framework that 1) decouples pseudo-input synthesis from label creation, using semantic masking and template guidance to produce diverse, task-relevant prompts without extra modules; 2) applies label self-evolution, blending base-model priors with fine-tuned outputs to prevent over-specialization; and 3) introduces a dynamic regularizer driven by the Wasserstein distance between task distributions, automatically relaxing or strengthening constraints in proportion to task similarity. Experiments across diverse tasks on different LLMs show that our SERS reduces forgetting by over 2\\% points against strong pseudo-rehearsal baselines, by ensuring efficient data utilization and wisely transferring knowledge. \nThe code will be released at https://github.com/JerryWangJun/LLM_CL_SERS/.",
      "arxiv_url": "https://openreview.net/forum?id=jR1lvwexLt",
      "pdf_url": "https://openreview.net/pdf/02bb9435749398313a19d1e2d4389699ebe7d3bf.pdf",
      "primary_category": "Continual Learning; Large Language Models; Pseudo Sample Rehearsal; Similarity Regularization",
      "categories": [
        "Continual Learning; Large Language Models; Pseudo Sample Rehearsal; Similarity Regularization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "k0wyi4cOGy",
      "title": "KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment",
      "authors": [
        "Yuxing Lu",
        "Wei Wu",
        "Xukai Zhao",
        "Rui Peng",
        "Jinzhuo Wang"
      ],
      "abstract": "Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical for modern AI systems, but manual curation struggles to scale with the rapid growth of scientific literature. This paper presents KARMA, a novel framework employing multi-agent large language models (LLMs) to automate KG enrichment through structured analysis of unstructured text. Our approach employs nine collaborative agents, spanning entity discovery, relation extraction, schema alignment, and conflict resolution that iteratively parse documents, verify extracted knowledge, and integrate it into existing graph structures while adhering to domain-specific schema. Experiments on 1,200 PubMed articles from three different domains demonstrate the effectiveness of KARMA in knowledge graph enrichment, with the identification of up to 38,230 new entities while achieving 83.1\\% LLM-verified correctness and reducing conflict edges by 18.6\\% through multi-layer assessments.",
      "arxiv_url": "https://openreview.net/forum?id=k0wyi4cOGy",
      "pdf_url": "https://openreview.net/pdf/307d3a55037031950ed1e7b9a84ab6abf7eb8f4b.pdf",
      "primary_category": "Knowledge Graphs, Multi Agent System",
      "categories": [
        "Knowledge Graphs",
        "Multi Agent System"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "KPlskOuZ46",
      "title": "Towards Doctor-Like Reasoning: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients",
      "authors": [
        "Yuxing Lu",
        "Gecheng Fu",
        "Wei Wu",
        "Xukai Zhao",
        "Goi Sin Yee",
        "Jinzhuo Wang"
      ],
      "abstract": "Existing medical RAG systems mainly leverage knowledge from medical knowledge bases, neglecting the crucial role of experiential knowledge derived from similar patient cases - a key component of human clinical reasoning. To bridge this gap, we propose DoctorRAG, a RAG framework that emulates doctor-like reasoning by integrating both explicit clinical knowledge and implicit case-based experience. DoctorRAG enhances retrieval precision by first allocating conceptual tags for queries and knowledge sources, together with a hybrid retrieval mechanism from both relevant knowledge and patient. In addition, a Med-TextGrad module using multi-agent textual gradients is integrated to ensure that the final output adheres to the retrieved knowledge and patient query. Comprehensive experiments on multilingual, multitask datasets demonstrate that DoctorRAG significantly outperforms strong baseline RAG models and gains improvements from iterative refinements. Our approach generates more accurate, relevant, and comprehensive responses, taking a step towards more doctor-like medical reasoning systems.",
      "arxiv_url": "https://openreview.net/forum?id=KPlskOuZ46",
      "pdf_url": "https://openreview.net/pdf/d44e480ab2364a71cc581dd88b8eb205e7740925.pdf",
      "primary_category": "Retrieval-Augmented Generation, Large Language Model, Clinical Decision Support System",
      "categories": [
        "Retrieval-Augmented Generation",
        "Large Language Model",
        "Clinical Decision Support System"
      ],
      "tags": [
        "RAG",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "EeAHhNwXPV",
      "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning",
      "authors": [
        "Qiuchen Wang",
        "Ruixue Ding",
        "Yu Zeng",
        "Zehui Chen",
        "Lin Chen",
        "Shihang Wang",
        "Pengjun Xie",
        "Fei Huang",
        "Feng Zhao"
      ],
      "abstract": "Effectively retrieving, reasoning and understanding visually rich information remains a challenge for traditional Retrieval-Augmented Generation (RAG) methods. On the one hand, traditional text-based methods cannot handle visual-related information. On the other hand, current vision-based RAG approaches are often limited by fixed pipelines and frequently struggle to reason effectively due to the insufficient activation of the fundamental capabilities of models. As reinforcement learning (RL) has been proven to be beneficial for model reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex reasoning across visually rich information. With this framework, VLMs interact with search engines, autonomously sampling single-turn or multi-turn reasoning trajectories with the help of visual perception tokens and undergoing continual optimization based on these samples. Our approach highlights key limitations of RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely incorporate images into the context, leading to insufficient reasoning token allocation and neglecting visual-specific perception; and (ii) When models interact with search engines, their queries often fail to retrieve relevant information due to the inability to articulate requirements, thereby leading to suboptimal performance. To address these challenges, we define an action space tailored for visually rich inputs, with actions including cropping and scaling, allowing the model to gather information from a coarse-to-fine perspective. Furthermore, to bridge the gap between users' original inquiries and the retriever, we employ a simple yet effective reward that integrates query rewriting and retrieval performance with a model-based reward. Our VRAG-RL optimizes VLMs for RAG tasks using specially designed RL strategies, aligning the model with real-world applications. Extensive experiments on diverse and challenging benchmarks show that our VRAG-RL outperforms existing methods by 20\\% (Qwen2.5-VL-7B) and 30\\% (Qwen2.5-VL-3B), demonstrating the effectiveness of our approach. The code is available at https://github.com/Alibaba-NLP/VRAG.",
      "arxiv_url": "https://openreview.net/forum?id=EeAHhNwXPV",
      "pdf_url": "https://openreview.net/pdf/facbdeba1c5f4027c295d801ab7c24a818c3957f.pdf",
      "primary_category": "Image Understanding, Retrieval Augmented Generation, Vision-language Models",
      "categories": [
        "Image Understanding",
        "Retrieval Augmented Generation",
        "Vision-language Models"
      ],
      "tags": [
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "lGyXq0LOeQ",
      "title": "EAP-GP: Mitigating Saturation Effect in Gradient-based Automated Circuit Identification",
      "authors": [
        "Lin Zhang",
        "Wenshuo Dong",
        "Zhuoran Zhang",
        "Shu Yang",
        "Lijie Hu",
        "Ninghao Liu",
        "Pan Zhou",
        "Di Wang"
      ],
      "abstract": "Understanding the internal mechanisms of transformer-based language models remains challenging. Mechanistic interpretability based on circuit discovery aims to reverse engineer neural networks by analyzing their internal processes at the level of computational subgraphs. In this paper, we revisit existing gradient-based circuit identification methods and find that their performance is either affected by the zero-gradient problem or saturation effects, where edge attribution scores become insensitive to input changes, resulting in noisy and unreliable attribution evaluations for circuit components. To address the saturation effect, we propose Edge Attribution Patching with GradPath (EAP-GP), EAP-GP introduces an integration path, starting from the input and adaptively following the direction of the difference between the gradients of corrupted and clean inputs to avoid the saturated region. This approach enhances attribution reliability and improves the faithfulness of circuit identification. We evaluate EAP-GP on 6 datasets using GPT-2 Small, GPT-2 Medium, and GPT-2 XL. Experimental results demonstrate that EAP-GP outperforms existing methods in circuit faithfulness, achieving improvements up to 17.7\\%. Comparisons with manually annotated ground-truth circuits demonstrate that EAP-GP achieves precision and recall comparable to or better than previous approaches, highlighting its effectiveness in identifying accurate circuits.",
      "arxiv_url": "https://openreview.net/forum?id=lGyXq0LOeQ",
      "pdf_url": "https://openreview.net/pdf/e8fa3d58d9acc01467bdf0e2d6607ebc338e79b4.pdf",
      "primary_category": "interpretability, explainable AI, model interpretability",
      "categories": [
        "interpretability",
        "explainable AI",
        "model interpretability"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BUkXhMb7ml",
      "title": "Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective",
      "authors": [
        "Chenwang Wu",
        "Yiu-ming Cheung",
        "Bo Han",
        "Defu Lian"
      ],
      "abstract": "Existing machine-generated text (MGT) detection methods implicitly assume labels as the \"golden standard\". However, we reveal boundary ambiguity in MGT detection, implying that traditional training paradigms are inexact. Moreover, limitations of human cognition and the superintelligence of detectors make inexact learning widespread and inevitable. To this end, we propose an easy-to-hard enhancement framework to provide reliable supervision under such inexact conditions. Distinct from knowledge distillation, our framework employs an easy supervisor targeting relatively simple longer-text detection tasks (despite weaker capabilities), to enhance the more challenging target detector. Firstly, longer texts targeted by supervisors theoretically alleviate the impact of inexact labels, laying the foundation for reliable supervision. Secondly, by structurally incorporating the detector into the supervisor, we theoretically model the supervisor as a lower performance bound for the detector. Thus, optimizing the supervisor indirectly optimizes the detector, ultimately approximating the underlying \"golden\" labels. Extensive experiments across diverse practical scenarios, including cross-LLM, cross-domain, mixed text, and paraphrase attacks, demonstrate the framework's significant detection effectiveness. The code is available at: \\url{https://github.com/tmlr-group/Easy2Hard}.",
      "arxiv_url": "https://openreview.net/forum?id=BUkXhMb7ml",
      "pdf_url": "https://openreview.net/pdf/d976a3a9acf1158349c47fef34b58ddb42c86da7.pdf",
      "primary_category": "Machine-Generated Text Detection, Simple-to-Hard Supervision, Large Language Model",
      "categories": [
        "Machine-Generated Text Detection",
        "Simple-to-Hard Supervision",
        "Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "rGMaZkn1ve",
      "title": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems",
      "authors": [
        "Xuanming Zhang",
        "Yuxuan Chen",
        "Samuel Yeh",
        "Sharon Li"
      ],
      "abstract": "Human social interactions depend on the ability to infer others' unspoken intentions, emotions, and beliefs—a cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce **MetaMind**, a multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) a *Theory-of-Mind Agent* generates hypotheses about user mental states (e.g., intent, emotion), (2) a *Moral Agent* refines these hypotheses using cultural norms and ethical constraints, and (3) a *Response Agent* generates contextually appropriate responses while validating alignment with inferred intent.\nOur framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the framework’s ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind.",
      "arxiv_url": "https://openreview.net/forum?id=rGMaZkn1ve",
      "pdf_url": "https://openreview.net/pdf/29a73113409f297f7381c7e12c760e66f10a7ba2.pdf",
      "primary_category": "LLM, multi-agent system, social cognition",
      "categories": [
        "LLM",
        "multi-agent system",
        "social cognition"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MlJyAvQaxp",
      "title": "ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data",
      "authors": [
        "Xiaoyang Liu",
        "Kangjie Bao",
        "Jiashuo Zhang",
        "Yunqi Liu",
        "Yu Chen",
        "Yuntian Liu",
        "Yang Jiao",
        "Tao Luo"
      ],
      "abstract": "Autoformalization, the automatic translation of mathematical content from natural language into machine-verifiable formal languages, has seen significant progress driven by advances in large language models (LLMs). Nonetheless, a primary barrier to further improvements is the limited availability of parallel corpora that map informal mathematical text to its formal counterpart. To address this limitation, we propose ATLAS (Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data), a novel data generation framework designed to produce large-scale, high-quality parallel corpora of theorem statements. Distinct from prior approaches, ATLAS begins with a concept repository, accelerates the improvement of the student model through expert iteration combined with knowledge distillation, and introduces two novel augmentation strategies that exploit the structural characteristics of formal languages. Running the proposed ATLAS framework for 10 iterations, we construct an undergraduate-level dataset of 117k theorem statements and develop the ATLAS Translator by fine-tuning Llama3.1-8B-Instruct with LoRA. This model establishes a new state of the art, demonstrating statistically significant improvements over both the Herald Translator and the Kimina-Autoformalizer across all benchmarks (p<0.05, two-sided t-test). Furthermore, we demonstrate that the full-parameter fine-tuning of a stronger base model on the ATLAS dataset leads to superior performance. The datasets, model, and code are available at https://github.com/XiaoyangLiu-sjtu/ATLAS.",
      "arxiv_url": "https://openreview.net/forum?id=MlJyAvQaxp",
      "pdf_url": "https://openreview.net/pdf/6620353660ac124536742e6820aa9c583d415ba6.pdf",
      "primary_category": "Autoformalization, Formal Mathematics, Lean 4",
      "categories": [
        "Autoformalization",
        "Formal Mathematics",
        "Lean 4",
        "Automated theorem proving",
        "Dataset"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MNSiBGNAvx",
      "title": "SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism",
      "authors": [
        "Beitao Chen",
        "Xinyu Lyu",
        "Shengming Yuan",
        "Jingkuan Song",
        "Heng Tao Shen",
        "Lianli Gao"
      ],
      "abstract": "By incorporating visual inputs, Multimodal Large Language Models (MLLMs) extend LLMs to support visual reasoning. However, this integration also introduces new vulnerabilities, making MLLMs susceptible to multimodal jailbreak attacks and hindering their safe deployment. Existing defense methods, including Image-to-Text Translation, Safe Prompting, and Multimodal Safety Tuning, attempt to address this by aligning multimodal inputs with LLMs’ built-in safeguards. Yet, they fall short in uncovering root causes of multimodal vulnerabilities, particularly how harmful multimodal tokens trigger jailbreak in MLLMs? Consequently, they remain vulnerable to text-driven multimodal attacks, often exhibiting overdefensive behaviors and imposing heavy training overhead. To bridge this gap, we present an comprehensive analysis of where, how and which harmful multimodal tokens bypass safeguards in MLLMs.\nSurprisingly, we find that less than 1% tokens in early-middle layers are responsible for inducing unsafe behaviors, highlighting the potential of precisely removing a small subset of harmful tokens, without requiring safety tuning, can still effectively improve safety against jailbreaks.\nMotivated by this, we propose Safe Prune-then-Restore (SafePTR), an training-free defense framework that selectively prunes harmful tokens at vulnerable layers while restoring benign features at subsequent layers. Without incurring additional computational overhead, SafePTR significantly enhances the safety of MLLMs while preserving efficiency. Extensive evaluations across three MLLMs and five benchmarks demonstrate SafePTR’s state-of-the-art performance in mitigating jailbreak risks without compromising utility.",
      "arxiv_url": "https://openreview.net/forum?id=MNSiBGNAvx",
      "pdf_url": "https://openreview.net/pdf/1e7456fc9a42ac1161bae2295fecc168e61464bd.pdf",
      "primary_category": "Multi-modal LLMs, AI safety",
      "categories": [
        "Multi-modal LLMs",
        "AI safety"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "49ueGcxA8W",
      "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic Attributes via DiT Modulation",
      "authors": [
        "Bowen Chen",
        "Brynn zhao",
        "Haomiao Sun",
        "Li Chen",
        "Xu Wang",
        "Daniel Kang Du",
        "Xinglong Wu"
      ],
      "abstract": "Achieving fine-grained control over subject identity and semantic attributes (pose, style, lighting) in text-to-image generation, particularly for multiple subjects, often undermines the editability and coherence of Diffusion Transformers (DiTs). Many approaches introduce artifacts or suffer from attribute entanglement. To overcome these challenges, we propose a novel multi-subject controlled generation model XVerse. By transforming reference images into offsets for token-specific text-stream modulation, XVerse allows for precise and independent control for specific subject without disrupting image latents or features. Consequently, XVerse offers high-fidelity, editable multi-subject image synthesis with robust control over individual subject characteristics and semantic attributes. This advancement significantly improves personalized and complex scene generation capabilities.",
      "arxiv_url": "https://openreview.net/forum?id=49ueGcxA8W",
      "pdf_url": "https://openreview.net/pdf/5d57d8b5a76e2d396b5417f016d772ca3edaea63.pdf",
      "primary_category": "text-to-image generation, modulation, multi-subject controlled generation",
      "categories": [
        "text-to-image generation",
        "modulation",
        "multi-subject controlled generation"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oLGtPYdRzU",
      "title": "Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools",
      "authors": [
        "Kanghua Mo",
        "Li Hu",
        "Yucheng Long",
        "Zhihao li"
      ],
      "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in complex reasoning and decision-making by leveraging external tools. However, this tool-centric paradigm introduces a previously underexplored attack surface, where adversaries can manipulate tool metadata---such as names, descriptions, and parameter schemas---to influence agent behavior. We identify this as a new and stealthy threat surface that allows malicious tools to be preferentially selected by LLM agents, without requiring prompt injection or access to model internals. To demonstrate and exploit this vulnerability, we propose the Attractive Metadata Attack (AMA), a black-box in-context learning framework that generates highly attractive but syntactically and semantically valid tool metadata through iterative optimization. The proposed attack integrates seamlessly into standard tool ecosystems and requires no modification to the agent’s execution framework. \nExtensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\\%-95\\%) and significant privacy leakage, with negligible impact on primary task execution. Moreover, the attack remains effective even against prompt-level defenses, auditor-based detection, and structured tool-selection protocols such as the Model Context Protocol, revealing systemic vulnerabilities in current agent architectures.\nThese findings reveal that metadata manipulation constitutes a potent and stealthy attack surface. \nNotably, AMA is orthogonal to injection attacks and can be combined with them to achieve stronger attack efficacy, highlighting the need for execution-level defenses beyond prompt-level and auditor-based mechanisms.\nCode is available at \\url{https://github.com/SEAIC-M/AMA}.",
      "arxiv_url": "https://openreview.net/forum?id=oLGtPYdRzU",
      "pdf_url": "https://openreview.net/pdf/58fd047ac6ca0bee4f61d85fc98df7a5c5b55e31.pdf",
      "primary_category": "Attractive Metadata Attack, Tool Metadata Manipulation, LLM Agent Security",
      "categories": [
        "Attractive Metadata Attack",
        "Tool Metadata Manipulation",
        "LLM Agent Security"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "HL1j92hb6z",
      "title": "First SFT, Second RL, Third UPT: Continual Improving Multi-Modal LLM Reasoning via Unsupervised Post-Training",
      "authors": [
        "Lai Wei",
        "Yuting Li",
        "Chen Wang",
        "Yue Wang",
        "Linghe Kong",
        "Weiran Huang",
        "Lichao Sun"
      ],
      "abstract": "Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL), which require expensive and manually annotated multi-modal data--an ultimately unsustainable resource.\nThis limitation has motivated a growing interest in unsupervised paradigms as a third stage of post-training after SFT and RL.\nWhile recent efforts have explored this direction, their methods are complex and difficult to iterate.\nTo address this, we propose MM-UPT, a simple yet effective framework for unsupervised post-training of MLLMs, enabling continual self-improvement without any external supervision.\nThe training method of MM-UPT builds upon GRPO, replacing traditional reward signals with a self-rewarding mechanism based on majority voting over multiple sampled responses. \nOur experiments demonstrate that such training method effectively improves the reasoning ability of Qwen2.5-VL-7B (e.g., 66.3\\%$\\rightarrow$72.9\\% on MathVista, 62.9\\%$\\rightarrow$68.7\\% on We-Math), using standard dataset without ground truth labels.\nTo further explore scalability, we extend our framework to a data self-generation setting, designing two strategies that prompt the MLLM to synthesize new training samples on its own.\nAdditional experiments show that combining these synthetic data with the unsupervised training method can also boost performance, highlighting a promising approach for scalable self-improvement.\nOverall, MM-UPT offers a new paradigm for autonomous enhancement of MLLMs, serving as a critical third step after initial SFT and RL in the absence of external supervision.\nOur code is available at \\url{https://github.com/waltonfuture/MM-UPT}.",
      "arxiv_url": "https://openreview.net/forum?id=HL1j92hb6z",
      "pdf_url": "https://openreview.net/pdf/b9ebd33d49d84affccdacf9ae1857c27f40e1ade.pdf",
      "primary_category": "Multi-modal Large Language Models, Multi-modal Reasoning, Self-Improvement",
      "categories": [
        "Multi-modal Large Language Models",
        "Multi-modal Reasoning",
        "Self-Improvement",
        "Unsupervised Post-Training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BpAx3OuNOr",
      "title": "Unlearned but Not Forgotten: Data Extraction after Exact Unlearning in LLM",
      "authors": [
        "Xiaoyu Wu",
        "Yifei Pang",
        "Terrance Liu",
        "Steven Wu"
      ],
      "abstract": "Large Language Models are typically trained on datasets collected from the web, which may inadvertently contain harmful or sensitive personal information. To address growing privacy concerns, unlearning methods have been proposed to remove the influence of specific data from trained models. Of these, exact unlearning---which retrains the model from scratch without the target data---is widely regarded as the gold standard for mitigating privacy risks in deployment. In this paper, we revisit this assumption in a practical deployment setting where both the pre- and post-unlearning logits API are exposed, such as in open-weight scenarios. Targeting this setting, we introduce a novel data extraction attack that leverages signals from the pre-unlearning model to guide the post-unlearning model, uncovering patterns that reflect the removed data distribution. Combining model guidance with a token filtering strategy, our attack significantly improves extraction success rates---doubling performance in some cases---across common benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our attack's effectiveness on a simulated medical diagnosis dataset to highlight real-world privacy risks associated with exact unlearning. In light of our findings, which suggest that unlearning may, in a contradictory way, \\textit{increase} the risk of privacy leakage during real-world deployments, we advocate for evaluation of unlearning methods to consider broader threat models that account not only for post-unlearning models but also for adversarial access to prior checkpoints. Code is publicly available at: https://github.com/Nicholas0228/unlearned_data_extraction_llm.",
      "arxiv_url": "https://openreview.net/forum?id=BpAx3OuNOr",
      "pdf_url": "https://openreview.net/pdf/6b35a1a74b58564e6ee5799a8bc30ecc14e566ff.pdf",
      "primary_category": "data extraction, privacy, unlearning",
      "categories": [
        "data extraction",
        "privacy",
        "unlearning",
        "model guidance",
        "large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MtyF5hCI7Y",
      "title": "ICLScan: Detecting Backdoors in Black-Box Large Language Models via Targeted In-context Illumination",
      "authors": [
        "Xiaoyi Pang",
        "Xuanyi Hao",
        "Song Guo",
        "Qi Luo",
        "Zhibo Wang"
      ],
      "abstract": "The widespread deployment of large language models (LLMs) allows users to access their capabilities via black-box APIs, but backdoor attacks pose serious security risks for API users by hijacking the model behavior. This highlights the importance of backdoor detection technologies to help users audit LLMs before use. However, most existing LLM backdoor defenses require white-box access or costly reverse engineering, limiting their practicality for resource-constrained users. Moreover, they mainly target classification tasks, leaving broader generative scenarios underexplored. To solve the problem, this paper introduces ICLScan, a lightweight framework that exploits targeted in-context learning (ICL) as illumination for backdoor detection in black-box LLMs, which effectively supports generative tasks without additional training or model modifications. ICLScan is based on our finding of backdoor susceptibility amplification: LLMs with pre-embedded backdoors are highly susceptible to new trigger implantation via ICL. Including only a small ratio of backdoor examples (containing ICL-triggered input and target output) in the ICL prompt can induce ICL trigger-specific malicious behavior in backdoored LLMs. ICLScan leverages this phenomenon to detect backdoored LLMs by statistically analyzing whether the success rate of new trigger injection via targeted ICL exceeds a threshold. It requires only multiple queries to estimate the backdoor success rate, overcoming black-box access and computational resource limitations. Extensive experiments across diverse LLMs and backdoor attacks demonstrate ICLScan's effectiveness and efficiency, achieving near-perfect detection performance (precision/recall/F1-score/ROC-AUC all approaching 1) with minimal additional overhead across all settings.",
      "arxiv_url": "https://openreview.net/forum?id=MtyF5hCI7Y",
      "pdf_url": "https://openreview.net/pdf/39a3759cbc35d81580adc08729d9343b6840374d.pdf",
      "primary_category": "backdoor detection, LLM",
      "categories": [
        "backdoor detection",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1nSynwHvu2",
      "title": "Dynamic Bundling with Large Language Models for Zero-Shot Inference on Text-Attributed Graphs",
      "authors": [
        "Yusheng Zhao",
        "Qixin Zhang",
        "Xiao Luo",
        "Weizhi Zhang",
        "Zhiping Xiao",
        "Wei Ju",
        "Philip S. Yu",
        "Ming Zhang"
      ],
      "abstract": "Large language models (LLMs) have been used in many zero-shot learning problems, with their strong generalization ability. Recently, adopting LLMs in text-attributed graphs (TAGs) has drawn increasing attention. However, the adoption of LLMs faces two major challenges: limited information on graph structure and unreliable responses. LLMs struggle with text attributes isolated from the graph topology. Worse still, they yield unreliable predictions due to both information insufficiency and the inherent weakness of LLMs (e.g., hallucination). Towards this end, this paper proposes a novel method named Dynamic Text Bundling Supervision (DENSE) that queries LLMs with bundles of texts to obtain bundle-level labels and uses these labels to supervise graph neural networks. Specifically, we sample a set of bundles, each containing a set of nodes with corresponding texts of close proximity. We then query LLMs with the bundled texts to obtain the label of each bundle. Subsequently, the bundle labels are used to supervise the optimization of graph neural networks, and the bundles are further refined to exclude noisy items. To justify our design, we also provide theoretical analysis of the proposed method. Extensive experiments across ten datasets validate the effectiveness of the proposed method.",
      "arxiv_url": "https://openreview.net/forum?id=1nSynwHvu2",
      "pdf_url": "https://openreview.net/pdf/d615b950a359ab4cc2684209840bedfc61f6d59e.pdf",
      "primary_category": "Text-Attributed Graphs, Large Language Models, Zero-Shot Inference",
      "categories": [
        "Text-Attributed Graphs",
        "Large Language Models",
        "Zero-Shot Inference"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9e2H9DhKPa",
      "title": "DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration",
      "authors": [
        "Tianteng Gu",
        "Bei Liu",
        "Bo Xiao",
        "Ke Zeng",
        "Jiacheng Liu",
        "Yanmin Qian"
      ],
      "abstract": "Pruning is a widely used technique to compress large language models (LLMs) by removing unimportant weights, but it often suffers from significant performance degradation—especially under semi-structured sparsity constraints. Existing pruning methods primarily focus on estimating the importance of individual weights, which limits their ability to preserve critical capabilities of the model. In this work, we propose a new perspective: rather than merely selecting which weights to prune, we first redistribute parameter importance to make the model inherently more amenable to pruning. By minimizing the information entropy of normalized importance scores, our approach concentrates importance onto a smaller subset of weights, thereby enhancing pruning robustness. We instantiate this idea through DenoiseRotator, which applies learnable orthogonal transformations to the model’s weight matrices. Our method is model-agnostic and can be seamlessly integrated with existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4 semi-structured sparsity, DenoiseRotator consistently improves perplexity and zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4 semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the dense model by 58%, narrowing the degradation from 8.1 to 3.4 points.",
      "arxiv_url": "https://openreview.net/forum?id=9e2H9DhKPa",
      "pdf_url": "https://openreview.net/pdf/8067a0744f87926b879a84ebfeb80307d7571bb0.pdf",
      "primary_category": "Large language model, Model compression, Pruning",
      "categories": [
        "Large language model",
        "Model compression",
        "Pruning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "uS4Wmg7PmE",
      "title": "Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning",
      "authors": [
        "Tianyi Bai",
        "Yuxuan Fan",
        "Qiu Jiantao",
        "Fupeng Sun",
        "Jiayi Song",
        "Junlin Han",
        "Zichen Liu",
        "Conghui He",
        "Wentao Zhang",
        "Binhang Yuan"
      ],
      "abstract": "Multimodal large language models (MLLMs) have achieved strong performance on vision-language tasks but still struggle with fine-grained visual differences, leading to hallucinations or missed semantic shifts. We attribute this to limitations in both training data and learning objectives. To address these issues, we propose a controlled data generation pipeline that produces minimally edited image pairs with semantically aligned captions. Using this pipeline, we construct the Micro Edit Dataset (MED), containing over 50K image-text pairs spanning 11 fine-grained edit categories, including attribute, count, position, and object presence changes.\nBuilding on MED, we introduce a supervised fine-tuning (SFT) framework with a feature-level consistency loss that promotes stable visual embeddings under small edits. We evaluate our approach on the Micro Edit Detection benchmark, which includes carefully balanced evaluation pairs designed to test sensitivity to subtle visual variations across the same edit categories.\nOur method improves difference detection accuracy and reduces hallucinations compared to strong baselines, including GPT-4o. Moreover, it yields consistent gains on standard vision-language tasks such as image captioning and visual question answering. These results demonstrate the effectiveness of combining targeted data and alignment objectives for enhancing fine-grained visual reasoning in MLLMs. Code and datasets are publicly released at https://github.com/Relaxed-System-Lab/hallu_med.",
      "arxiv_url": "https://openreview.net/forum?id=uS4Wmg7PmE",
      "pdf_url": "https://openreview.net/pdf/a1cb198f089b3aae657fe865e499890d5cab49cb.pdf",
      "primary_category": "MLLMs",
      "categories": [
        "MLLMs"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "fVs2BCjCqC",
      "title": "Think before Recommendation: Autonomous Reasoning-enhanced Recommender",
      "authors": [
        "Xiaoyu Kong",
        "Junguang Jiang",
        "Bin Liu",
        "Ziru Xu",
        "Han Zhu",
        "Jian Xu",
        "Bo Zheng",
        "Jiancan Wu",
        "Xiang Wang"
      ],
      "abstract": "The core task of recommender systems is to learn user preferences from historical user-item interactions. With the rapid development of large language models (LLMs), recent research has explored leveraging the reasoning capabilities of LLMs to enhance rating prediction tasks. However, existing distillation-based methods suffer from limitations such as the teacher model's insufficient recommendation capability, costly and static supervision, and superficial transfer of reasoning ability. To address these issues, this paper proposes RecZero, a reinforcement learning (RL)-based recommendation paradigm that abandons the traditional multi-model and multi-stage distillation approach. Instead, RecZero trains a single LLM through pure RL to autonomously develop reasoning capabilities for rating prediction. RecZero consists of two key components: (1) \"Think-before-Recommendation\" prompt construction, which employs a structured reasoning template to guide the model in step-wise analysis of user interests, item features, and user-item compatibility; and (2) rule-based reward modeling, which adopts group relative policy optimization (GRPO) to compute rewards for reasoning trajectories and optimize the LLM. Additionally, the paper explores a hybrid paradigm, RecOne, which combines supervised fine-tuning with RL, initializing the model with cold-start reasoning samples and further optimizing it with RL. Experimental results demonstrate that RecZero and RecOne significantly outperform existing baseline methods on multiple benchmark datasets, validating the superiority of the RL paradigm in achieving autonomous reasoning-enhanced recommender systems.",
      "arxiv_url": "https://openreview.net/forum?id=fVs2BCjCqC",
      "pdf_url": "https://openreview.net/pdf/dabe28fda98c6afcff77d9aaf707bc000be4bf15.pdf",
      "primary_category": "Recommendation, Large Language Models, Reinforcement Learning",
      "categories": [
        "Recommendation",
        "Large Language Models",
        "Reinforcement Learning",
        "Rating Prediction"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "X9diEuva9R",
      "title": "AREAL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning",
      "authors": [
        "Wei Fu",
        "Jiaxuan Gao",
        "Xujie Shen",
        "Chen Zhu",
        "Zhiyu Mei",
        "Chuyi He",
        "Shusheng Xu",
        "Guo Wei",
        "Jun Mei",
        "WANG JIASHU",
        "Tongkai Yang",
        "Binhang Yuan",
        "Yi Wu"
      ],
      "abstract": "Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL  system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.77x training speedup compared to synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.",
      "arxiv_url": "https://openreview.net/forum?id=X9diEuva9R",
      "pdf_url": "https://openreview.net/pdf/e3d06d50035996ffa8363d6a8af1c980ef772804.pdf",
      "primary_category": "distributed system",
      "categories": [
        "distributed system"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Orpf8yDjdj",
      "title": "The Curse of Depth in Large Language Models",
      "authors": [
        "Wenfang Sun",
        "Xinyuan Song",
        "Pengxiang Li",
        "Lu Yin",
        "Yefeng Zheng",
        "Shiwei Liu"
      ],
      "abstract": "In this paper, we re-introduce the Curse of Depth, a concept that re-introduces, explains, and addresses the recent observation in modern Large Language Models (LLMs) where deeper layers are much less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs, such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling, which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Our experimental results, spanning model sizes from 130M to 7B, demonstrate that \\ours significantly enhances LLM pre-training performance compared to Pre-LN. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training.",
      "arxiv_url": "https://openreview.net/forum?id=Orpf8yDjdj",
      "pdf_url": "https://openreview.net/pdf/0e48bc84752c862332e5d66da788d6aedbd7fd38.pdf",
      "primary_category": "curse of depth, LLMs, Pre-LN",
      "categories": [
        "curse of depth",
        "LLMs",
        "Pre-LN"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4iehXI36QG",
      "title": "OpenOmni: Advancing Open-Source Omnimodal Large Language Models with Progressive Multimodal Alignment and Real-time Emotional Speech Synthesis",
      "authors": [
        "Run Luo",
        "Ting-En Lin",
        "Haonan Zhang",
        "Yuchuan Wu",
        "Xiong Liu",
        "Yongbin Li",
        "Longze Chen",
        "Jiaming Li",
        "Lei Zhang",
        "Xiaobo Xia",
        "Hamid Alinejad-Rokny",
        "Fei Huang",
        "Min Yang"
      ],
      "abstract": "Recent advancements in omnimodal learning have significantly improved understanding and generation across images, text, and speech, yet these developments remain predominantly confined to proprietary models. The lack of high-quality omnimodal datasets and the challenges of real-time emotional speech synthesis have notably hindered progress in open-source research. To address these limitations, we introduce OpenOmni, a two-stage training framework that integrates omnimodal alignment and speech generation to develop a state-of-the-art omnimodal large language model. In the alignment phase, a pretrained speech model undergoes further training on image-text tasks, enabling (near) zero-shot generalization from vision to speech, outperforming models trained on tri-modal datasets. In the speech generation phase, a lightweight decoder is trained on speech tasks with direct preference optimization, which enables real-time emotional speech synthesis with high fidelity. Extensive experiments demonstrate that OpenOmni surpasses state-of-the-art models across omnimodal, vision-language, and speech-language benchmarks. It achieves a 4-point absolute improvement on OmniBench over the leading open-source model VITA, despite using 5$\\times$ fewer training examples and a smaller model size (7B vs. 7$\\times$8B). Besides, OpenOmni achieves real-time speech generation with less than 1 second latency at non-autoregressive mode, reducing inference time by 5$\\times$ compared to autoregressive methods, and improves emotion classification accuracy by 7.7\\%. The codebase is available at https://github.com/RainBowLuoCS/OpenOmni.",
      "arxiv_url": "https://openreview.net/forum?id=4iehXI36QG",
      "pdf_url": "https://openreview.net/pdf/6f375472c18fb0e826c393f4d87fd66248f4ba59.pdf",
      "primary_category": "omnimodal large language models, modality alignment, real-time inference",
      "categories": [
        "omnimodal large language models",
        "modality alignment",
        "real-time inference"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "z5vZDI2r6J",
      "title": "NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables",
      "authors": [
        "Lanrui Wang",
        "Mingyu Zheng",
        "Hongyin Tang",
        "Zheng Lin",
        "Yanan Cao",
        "Jingang Wang",
        "Xunliang Cai",
        "Weiping Wang"
      ],
      "abstract": "Processing structured tabular data, particularly large and lengthy tables, constitutes a fundamental yet challenging task for large language models (LLMs). However, existing long-context benchmarks like Needle-in-a-Haystack primarily focus on unstructured text, neglecting the challenge of diverse structured tables. Meanwhile, previous tabular benchmarks mainly consider downstream tasks that require high-level reasoning abilities, and overlook models' underlying fine-grained perception of individual table cells, which is crucial for practical and robust LLM-based table applications. To address this gap, we introduce \\textsc{NeedleInATable} (NIAT), a new long-context tabular benchmark that treats each table cell as a ``needle'' and requires models to extract the target cell based on cell locations or lookup questions. Our comprehensive evaluation of various LLMs and multimodal LLMs reveals a substantial performance gap between popular downstream tabular tasks and the simpler NIAT task, suggesting that they may rely on dataset-specific correlations or shortcuts to obtain better benchmark results but lack truly robust long-context understanding towards structured tables. Furthermore, we demonstrate that using synthesized NIAT training data can effectively improve performance on both NIAT task and downstream tabular tasks, which validates the importance of NIAT capability for LLMs' genuine table understanding ability. Our data, code and models will be released to facilitate future research.",
      "arxiv_url": "https://openreview.net/forum?id=z5vZDI2r6J",
      "pdf_url": "https://openreview.net/pdf/7c07a77f4c388194b0b0c43a4b17433889a9f08b.pdf",
      "primary_category": "Long-context, table understanding, structured data",
      "categories": [
        "Long-context",
        "table understanding",
        "structured data"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "EeyvDitalf",
      "title": "Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning",
      "authors": [
        "Fanrui Zhang",
        "Dian Li",
        "Qiang Zhang",
        "Chenjun",
        "sinbadliu",
        "Junxiong Lin",
        "Jiahong Yan",
        "Jiawei Liu",
        "Zheng-Jun Zha"
      ],
      "abstract": "The rapid spread of multimodal misinformation on social media has raised growing concerns, while research on video misinformation detection remains limited due to the lack of large-scale, diverse datasets. Existing methods often overfit to rigid templates and lack deep reasoning over deceptive content. To address these challenges, we introduce FakeVV, a large-scale benchmark comprising over 100,000 video-text pairs with fine-grained, interpretable annotations. In addition, we further propose Fact-R1, a novel framework that integrates deep reasoning with collaborative rule-based reinforcement learning. Fact-R1 is trained through a three-stage process: (1) misinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference alignment via Direct Preference Optimization (DPO), and (3) Group Relative Policy Optimization (GRPO) using a novel verifiable reward function. This enables Fact-R1 to exhibit emergent reasoning behaviors comparable to those observed in advanced text-based reinforcement learning systems, but in the more complex multimodal misinformation setting. Our work establishes a new paradigm for misinformation detection, bridging large-scale video understanding, reasoning-guided alignment, and interpretable verification.",
      "arxiv_url": "https://openreview.net/forum?id=EeyvDitalf",
      "pdf_url": "https://openreview.net/pdf/57d8d99f86e1cce535d963ef6bb4073464914b94.pdf",
      "primary_category": "Video Misinformation Detection, Deep Reasoning",
      "categories": [
        "Video Misinformation Detection",
        "Deep Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pzPyxXjHrT",
      "title": "Interpretable Next-token Prediction via the Generalized Induction Head",
      "authors": [
        "Eunji Kim",
        "Sriya Mantena",
        "Weiwei Yang",
        "Chandan Singh",
        "Sungroh Yoon",
        "Jianfeng Gao"
      ],
      "abstract": "While large transformer models excel in predictive performance, their lack of interpretability restricts their usefulness in high-stakes domains. To remedy this, we propose the Generalized Induction-Head Model (GIM), an interpretable model for next-token prediction inspired by the observation of “induction heads” in LLMs. GIM is a retrieval-based module that identifies similar sequences in the input context by combining exact n-gram matching and fuzzy matching based on a neural similarity metric. We evaluate GIM in two settings: language modeling and fMRI response prediction. In language modeling, GIM improves next-token prediction by up to 25%p over interpretable baselines, significantly narrowing the gap with black-box LLMs. In an fMRI setting, GIM improves neural response prediction by 20% and offers insights into the language selectivity of the brain. GIM represents a significant step toward uniting interpretability and performance across domains. The code is available at https://github.com/ejkim47/generalized-induction-head.",
      "arxiv_url": "https://openreview.net/forum?id=pzPyxXjHrT",
      "pdf_url": "https://openreview.net/pdf/c72ac78e6c2d4990b18236f85f8ccf826072e479.pdf",
      "primary_category": "interpretability, ngram, language modeling",
      "categories": [
        "interpretability",
        "ngram",
        "language modeling",
        "fmri",
        "neuroscience"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "LVDRJE4xQ2",
      "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone",
      "authors": [
        "Jitai Hao",
        "Qiang Huang",
        "Hao Liu",
        "Xinyan Xiao",
        "Zhaochun Ren",
        "Jun Yu"
      ],
      "abstract": "Training high-performing Small Language Models (SLMs) remains computationally expensive, even with knowledge distillation and pruning from larger teacher models. \nExisting approaches often face three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs).\nTo address these challenges, we introduce \\textbf{Low-Rank Clone (LRC)}, an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models.\nLRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher.\nThis unified design maximizes knowledge transfer while removing the need for explicit alignment modules.\nExtensive experiments with open-source teachers such as Llama-3.2-3B-Instruct and Qwen2.5-3B/7B-Instruct show that LRC matches or surpasses the performance of state-of-the-art models trained on trillions of tokens--using only 20B tokens, achieving over \\textbf{1,000$\\times$} greater training efficiency. \nOur codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/JitaiHao/LRC-4B-Base.",
      "arxiv_url": "https://openreview.net/forum?id=LVDRJE4xQ2",
      "pdf_url": "https://openreview.net/pdf/3d174b71d13bf8e1d728563aa95bdc04a181edef.pdf",
      "primary_category": "LLMs, Knowledge Distillation, Pruning",
      "categories": [
        "LLMs",
        "Knowledge Distillation",
        "Pruning",
        "Efficient Pre-training",
        "Model Compression"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "WWa5x1WnEw",
      "title": "Unbiased Prototype Consistency Learning for Multi-Modal and Multi-Task Object Re-Identification",
      "authors": [
        "Zhongao Zhou",
        "Bin Yang",
        "Wenke Huang",
        "Jun Chen",
        "Mang Ye"
      ],
      "abstract": "In object re-identification (ReID) task, both cross-modal and multi-modal retrieval methods have achieved notable progress. However, existing approaches are designed for specific modality and category (person or vehicle) retrieval task, lacking generalizability to others. Acquiring multiple task-specific models would result in wasteful allocation of both training and deployment resources. To address the practical requirements for unified retrieval, we introduce Multi-Modal and Multi-Task object ReID ($\\rm {M^3T}$-ReID). The $\\rm {M^3T}$-ReID task aims to utilize a unified model to simultaneously achieve retrieval tasks across different modalities and different categories. Specifically,\nto tackle the challenges of modality distibution divergence and category semantics discrepancy posed in $\\rm {M^3T}$-ReID, we design a novel  Unbiased Prototype Consistency Learning (UPCL) framework, which consists of two main modules: Unbiased Prototypes-guided Modality Enhancement (UPME) and Cluster Prototype Consistency Regularization (CPCR).\nUPME leverages modality-unbiased prototypes to simultaneously enhance cross-modal shared features and multi-modal fused features. Additionally, CPCR  regulates discriminative semantics learning with category-consistent information through prototypes clustering.\nUnder the collaborative operation of these two modules, our model can simultaneously learn robust cross-modal shared feature and multi-modal fused feature spaces, while also exhibiting strong category-discriminative capabilities. Extensive experiments on multi-modal datasets RGBNT201 and RGBNT100 demonstrates our UPCL framework showcasing exceptional performance for $\\rm {M^3T}$-ReID. The code is available at https://github.com/ZhouZhongao/UPCL.",
      "arxiv_url": "https://openreview.net/forum?id=WWa5x1WnEw",
      "pdf_url": "https://openreview.net/pdf/fd66958ed5cebe9fb03ea0a9cc4d2c1be46fc3c4.pdf",
      "primary_category": "Object Re-identification, Mulit-modal, Cross-modal",
      "categories": [
        "Object Re-identification",
        "Mulit-modal",
        "Cross-modal"
      ],
      "tags": [
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "jAsr5GHt3P",
      "title": "Wider or Deeper?  Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search",
      "authors": [
        "Yuichi Inoue",
        "Kou Misaki",
        "Yuki Imajuku",
        "So Kuroki",
        "Taishi Nakamura",
        "Takuya Akiba"
      ],
      "abstract": "Recent advances demonstrate that increasing inference-time computation can significantly boost the reasoning capabilities of large language models (LLMs). Although repeated sampling (i.e., generating multiple candidate outputs) is a highly effective strategy, it does not leverage external feedback signals for refinement, which are often available in tasks like coding. In this work, we propose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), a novel inference-time framework that generalizes repeated sampling with principled multi-turn exploration and exploitation. At each node in the search tree, AB-MCTS dynamically decides whether to ''go wider'' by expanding new candidate responses or ''go deeper'' by revisiting existing ones based on external feedback signals. We evaluate our method on complex coding and engineering tasks using frontier models. Empirical results show that AB-MCTS outperforms both repeated sampling and standard MCTS, underscoring the importance of combining the response diversity of LLMs with multi-turn solution refinement for effective inference-time scaling.",
      "arxiv_url": "https://openreview.net/forum?id=jAsr5GHt3P",
      "pdf_url": "https://openreview.net/pdf/e6d2b8dcb02f4dadb940b53aefa65032db4fbd89.pdf",
      "primary_category": "inference-time compute, tree search, LLMs",
      "categories": [
        "inference-time compute",
        "tree search",
        "LLMs",
        "programming",
        "reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "aSfBbhUJAa",
      "title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for  Complex Task Solving",
      "authors": [
        "Huacan Wang",
        "Ziyi Ni",
        "Shuo Zhang",
        "Shuo Lu",
        "Sen Hu",
        "Ziyang He",
        "Chen Hu",
        "Jiaye Lin",
        "Yifu Guo",
        "Yuntao Du",
        "Pin Lyu"
      ],
      "abstract": "The ultimate goal of code agents is to solve complex tasks autonomously. \nAlthough large language models (LLMs) have made substantial progress in code generation, real-world tasks typically demand full-fledged code repositories rather than simple scripts.  Building such repositories from scratch remains a major challenge.  Fortunately, GitHub hosts a vast, evolving collection of open-source repositories, which developers frequently reuse as modular components for complex tasks. Yet, existing frameworks like OpenHands and SWE-Agent still struggle to effectively leverage these valuable resources.\nRelying solely on README files provides insufficient guidance, and deeper exploration reveals two core obstacles: overwhelming information and tangled dependencies of repositories, both constrained by the limited context windows of current LLMs.\nTo tackle these issues, we propose RepoMaster, an autonomous agent framework designed to explore and reuse GitHub repositories for solving complex tasks. \nFor efficient understanding, RepoMaster constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components, providing only identified core elements to the LLMs rather than the entire repository. \nDuring autonomous execution, it progressively explores related components using our exploration tools and prunes information to optimize context usage.\nEvaluated on the adjusted MLE-bench, RepoMaster achieves a 110\\% relative boost in valid submissions over the strongest baseline OpenHands. \nOn our newly released GitTaskBench, RepoMaster lifts the task-pass rate from 40.7% to 62.9% while reducing token usage by 95%. \nOur code and demonstration materials are publicly available at https://github.com/QuantaAlpha/RepoMaster.",
      "arxiv_url": "https://openreview.net/forum?id=aSfBbhUJAa",
      "pdf_url": "https://openreview.net/pdf/fea6d1106531a6d10e824159b158dcbffbc07cc2.pdf",
      "primary_category": "Repository Comprehension, GitHub Integration, Efficient Repository Exploration",
      "categories": [
        "Repository Comprehension",
        "GitHub Integration",
        "Efficient Repository Exploration",
        "Autonomous Agents"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Vj48eXaQDM",
      "title": "Learned Prefix Caching for Efficient LLM Inference",
      "authors": [
        "Dongsheng Yang",
        "Austin Li",
        "Kai Li",
        "Wyatt Lloyd"
      ],
      "abstract": "Prefix caching is a key technique for reducing Large Language Model (LLM) inference costs. However, the prevalent least-recently-used (LRU) eviction algorithm has a large gap to the optimal algorithm. This paper introduces LPC, the first learned method to perform LLM prefix cache eviction. LPC leverages conversational content analysis to provide predictive guidance for eviction, determining which conversations are likely to continue. These insights, combined with last access timestamps, inform more effective cache management. Extensive evaluations across three real-world datasets demonstrate that LPC achieves 18-47% reductions in required cache sizes for equivalent hit ratios and has an 11% improvement in LLM prefilling throughput in an emulated environment.",
      "arxiv_url": "https://openreview.net/forum?id=Vj48eXaQDM",
      "pdf_url": "https://openreview.net/pdf/744bf03193afdce00e84f7f40fc8a7e1dd84061a.pdf",
      "primary_category": "large language model inference, prefilling, prefix caching",
      "categories": [
        "large language model inference",
        "prefilling",
        "prefix caching",
        "learned caching algorithm"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "AXlquRUO0S",
      "title": "Sum Estimation under Personalized Local Differential Privacy",
      "authors": [
        "Dajun Sun",
        "Wei Dong",
        "Yuan Qiu",
        "Ke Yi",
        "Graham Cormode"
      ],
      "abstract": "People have diverse privacy requirements. This is best modeled using a personalized local differential privacy model where each user privatizes their data using a possibly different privacy parameter.  While the model of personalized local differential privacy is a natural and important one, prior work has failed to give meaningful error bounds. In this paper, we study the foundational sum/mean estimation problem under this model. We present two novel protocols that achieve strong error guarantees. The first gives a guarantee based on the radius of the data, suiting inputs that are centered around zero. The second extends the guarantee to the diameter of the data, capturing the case when the points are situated arbitrarily. Experimental results on both synthetic and real data show that our protocols significantly outperform existing methods in terms of accuracy while providing a strong level of privacy.",
      "arxiv_url": "https://openreview.net/forum?id=AXlquRUO0S",
      "pdf_url": "https://openreview.net/pdf/24e769481cb63b600da3f3b223ab546dc74aebc5.pdf",
      "primary_category": "Differential Privacy, Sum Estimation",
      "categories": [
        "Differential Privacy",
        "Sum Estimation"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6ICFqmixlS",
      "title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning",
      "authors": [
        "Wenkai Yang",
        "Shuming Ma",
        "Yankai Lin",
        "Furu Wei"
      ],
      "abstract": "Recent studies have shown that making a model spend more time thinking through longer Chain of Thoughts (CoTs) enables it to gain significant improvements in complex reasoning tasks. While current researches continue to explore the benefits of increasing test-time compute by extending the CoT lengths of Large Language Models (LLMs), we are concerned about a potential issue hidden behind the current pursuit of test-time scaling: Would excessively scaling the CoT length actually bring adverse effects to a model's reasoning performance? Our explorations on mathematical reasoning tasks reveal an unexpected finding that scaling with longer CoTs can indeed impair the reasoning performance of LLMs in certain domains. Moreover, we discover that there exists an optimal scaled length distribution that differs across different domains. Based on these insights, we propose a Thinking-Optimal Scaling strategy. Our method first uses a small set of seed data with varying response length distributions to teach the model to adopt different reasoning efforts for deep thinking. Then, the model selects its shortest correct response under different reasoning efforts on additional problems for self-improvement. Our self-improved models built upon Qwen2.5-32B-Instruct outperform other distillation-based 32B o1-like models across various math benchmarks, and achieve performance on par with the teacher model QwQ-32B-Preview that produces the seed data.",
      "arxiv_url": "https://openreview.net/forum?id=6ICFqmixlS",
      "pdf_url": "https://openreview.net/pdf/2b5add2eb03f9fe099931ccc0051d046c4626f13.pdf",
      "primary_category": "test-time scaling, LLM reasoning",
      "categories": [
        "test-time scaling",
        "LLM reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "jvVQeSMeGM",
      "title": "Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning",
      "authors": [
        "Feng Chen",
        "Allan Raventos",
        "Nan Cheng",
        "Surya Ganguli",
        "Shaul Druckmann"
      ],
      "abstract": "Recent progress in large language models (LLMs) highlights the power of scaling test-time compute to achieve strong performance on complex tasks, such as mathematical reasoning and code generation. This raises a critical question: how should model training be modified to optimize performance under a subsequent test-time compute strategy and budget? To explore this, we focus on pass@N, a simple test-time strategy that searches for a correct answer in N independent samples. We show, surprisingly, that training with cross-entropy (CE) can be _misaligned_ with pass@N in that pass@N accuracy _decreases_ with longer CE training. We explain the origins of this misalignment in terms of model overconfidence induced by CE, and experimentally verify our prediction of overconfidence as an impediment to scaling test-time compute via pass@N. Furthermore we suggest a principled, modified training loss that is better aligned to pass@N by limiting model confidence and rescuing pass@N test performance. Our algorithm demonstrates improved mathematical reasoning on MATH and MiniF2F benchmarks under several scenarios: (1) providing answers to math questions both with and without Chain-of-Thought reasoning traces; and (2) proving theorems by searching over proof trees of varying shapes. Overall our work underscores the importance of co-designing two traditionally separate phases of LLM development: training-time protocols and test-time search and reasoning strategies.",
      "arxiv_url": "https://openreview.net/forum?id=jvVQeSMeGM",
      "pdf_url": "https://openreview.net/pdf/8102f4f18bf60fdc11280cd76fc4328d78188609.pdf",
      "primary_category": "Test-time compute, inference-time compute, coverage",
      "categories": [
        "Test-time compute",
        "inference-time compute",
        "coverage",
        "pass@N",
        "reasoning",
        "large language model",
        "formal math",
        "finetuning",
        "overfitting",
        "overconfidence"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "20JDhbJqn3",
      "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
      "authors": [
        "Xiang Liu",
        "Zhenheng Tang",
        "Peijie Dong",
        "Zeyu Li",
        "Liuyue",
        "Bo Li",
        "Xuming Hu",
        "Xiaowen Chu"
      ],
      "abstract": "Large Language Models (LLMs) require significant GPU memory when processing long texts, with the key value (KV) cache consuming up to 70\\% of total memory during inference. Although existing compression methods reduce memory by evaluating the importance of individual tokens, they overlook critical semantic relationships between tokens, resulting in fragmented context and degraded performance. We introduce \\method{}, which fundamentally reimagines KV cache compression by treating semantic chunks - rather than isolated tokens - as basic compression units. This approach preserves complete linguistic structures and contextual integrity, ensuring that essential meaning is retained even under aggressive compression. Our innovation includes a novel layer-wise index reuse technique that exploits the higher cross-layer similarity of preserved indices in \\method{}, reducing computational overhead and improving throughput by 26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench, Needle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that \\method{} outperforms state-of-the-art methods by up to 8.7\\% in precision while maintaining the same compression ratio. These results confirm that semantic-aware compression significantly enhances both efficiency and performance for long-context LLM inference, providing a simple yet effective solution to the memory bottleneck problem. \\emph{The code is available at \\href{https://github.com/NVIDIA/kvpress}{link}.}",
      "arxiv_url": "https://openreview.net/forum?id=20JDhbJqn3",
      "pdf_url": "https://openreview.net/pdf/49b5b8e7b6f5c878d028cbba29caf728aa81ffe8.pdf",
      "primary_category": "LLM, KV cache, Compression",
      "categories": [
        "LLM",
        "KV cache",
        "Compression",
        "Long-context"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "72UR53jN7T",
      "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning",
      "authors": [
        "Chi-Pin Huang",
        "Yueh-Hua Wu",
        "Min-Hung Chen",
        "Yu-Chiang Frank Wang",
        "Fu-En Yang"
      ],
      "abstract": "Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks.",
      "arxiv_url": "https://openreview.net/forum?id=72UR53jN7T",
      "pdf_url": "https://openreview.net/pdf/b35b0fc70612e191baced400f754db8ff1fae711.pdf",
      "primary_category": "Vision-Language-Action Models, Reasoning",
      "categories": [
        "Vision-Language-Action Models",
        "Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Iytf59QZzl",
      "title": "Preference Optimization by Estimating the Ratio of the Data Distribution",
      "authors": [
        "Yeongmin Kim",
        "HeeSun Bae",
        "Byeonghu Na",
        "Il-chul Moon"
      ],
      "abstract": "Direct preference optimization (DPO) is widely used as a simple and stable method for aligning large language models (LLMs) with human preferences. This paper investigates a generalized DPO loss that enables a policy model to match the target policy from a likelihood ratio estimation perspective.  The ratio of the target policy provides a unique identification of the policy distribution without relying on reward models or partition functions. This allows the generalized loss to retain both simplicity and theoretical guarantees, which prior work such as $f$-PO fails to achieve simultaneously. We propose \\textit{Bregman preference optimization} (BPO), a generalized framework for ratio matching that provides a family of objective functions achieving target policy optimality. BPO subsumes DPO as a special case and offers tractable forms for all instances, allowing implementation with a few lines of code. We further develop scaled Basu's power divergence (SBA), a gradient scaling method that can be used for BPO instances. The BPO framework complements other DPO variants and is applicable to target policies defined by these variants. In experiments, unlike other probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibits a trade-off between generation fidelity and diversity, instances of BPO improve both win rate and entropy compared with DPO. When applied to Llama-3-8B-Instruct, BPO achieves state-of-the-art performance among Llama-3-8B backbones, with a 55.9\\% length-controlled win rate on AlpacaEval2. Project page: https://github.com/aailab-kaist/BPO.",
      "arxiv_url": "https://openreview.net/forum?id=Iytf59QZzl",
      "pdf_url": "https://openreview.net/pdf/aa81613d4d5220a97c94e815900f9e710bc5efd5.pdf",
      "primary_category": "Direct Preference Optimization, Language Models, AI Alignment",
      "categories": [
        "Direct Preference Optimization",
        "Language Models",
        "AI Alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "uwL0vbeEVn",
      "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications",
      "authors": [
        "Gabriele Oliaro",
        "Zhihao Jia",
        "Daniel F Campos",
        "Aurick Qiao"
      ],
      "abstract": "Speculative decoding is widely adopted to reduce latency in large language model (LLM) inference by leveraging smaller draft models capable of handling diverse user tasks. However, emerging AI applications, such as LLM-based agents, present unique workload characteristics: instead of diverse independent requests, agentic frameworks typically submit repetitive inference requests, such as multi-agent pipelines performing similar subtasks or self-refinement loops iteratively enhancing outputs. These workloads result in long and highly predictable sequences, which current speculative decoding methods do not effectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a novel method that utilizes efficient suffix trees to cache long token sequences from prompts and previous outputs. By adaptively speculating more tokens when acceptance likelihood is high and fewer when it is low, SuffixDecoding effectively exploits opportunities for longer speculations while conserving computation when those opportunities are limited. Evaluations on agentic benchmarks, including SWE-Bench and Text-to-SQL, demonstrate that SuffixDecoding achieves speedups of up to 3.9$\\times$, outperforming state-of-the-art methods -- 2.2$\\times$ faster than model-based approaches like EAGLE-2/3 and 1.6$\\times$ faster than model-free approaches such as Token Recycling. SuffixDecoding is open-sourced.",
      "arxiv_url": "https://openreview.net/forum?id=uwL0vbeEVn",
      "pdf_url": "https://openreview.net/pdf/eab308d22c40e44f8fd67ac67876864621ddace9.pdf",
      "primary_category": "speculative decoding, LLM agents, model-free speculation",
      "categories": [
        "speculative decoding",
        "LLM agents",
        "model-free speculation",
        "SWE-Bench",
        "LLM inference"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MNduv07wAu",
      "title": "System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts",
      "authors": [
        "Xiaoqiang Wang",
        "Suyuchen Wang",
        "Yun Zhu",
        "Bang Liu"
      ],
      "abstract": "Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move beyond fast System-1 responses and engage in deliberative System-2 reasoning. However, this comes at the cost of significant inefficiency due to verbose intermediate output. Recent latent-space reasoning methods improve efficiency by operating on hidden states without decoding into language, yet they treat all steps uniformly, failing to distinguish critical deductions from auxiliary steps and resulting in suboptimal use of computational resources. In this paper, we propose System-1.5 Reasoning, an adaptive reasoning framework that dynamically allocates computation across reasoning steps through shortcut paths in latent space.Specifically, System-1.5 Reasoning introduces two types of dynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the vertical depth by early exiting non-critical tokens through lightweight adapter branches, while allowing critical tokens to continue through deeper Transformer layers. The step shortcut (SS) reuses hidden states across the decoding steps to skip trivial steps and reason horizontally in latent space. Training System-1.5 Reasoning involves a two-stage self-distillation process: first distilling natural language CoT into latent-space continuous thought, and then distilling full-path System-2 latent reasoning into adaptive shortcut paths (System-1.5 Reasoning).Experiments on reasoning tasks demonstrate the superior performance of our method.\nFor example, on GSM8K, System-1.5 Reasoning achieves reasoning performance comparable to traditional CoT fine-tuning methods while accelerating inference by over 20× and reducing token generation by 91.0\\% on average.",
      "arxiv_url": "https://openreview.net/forum?id=MNduv07wAu",
      "pdf_url": "https://openreview.net/pdf/d239a496aa58524a3d724289b589b295e3d663a1.pdf",
      "primary_category": "Efficient Reasoning Model, Latent-space Reasoning, Implicit Chain-of-Thought",
      "categories": [
        "Efficient Reasoning Model",
        "Latent-space Reasoning",
        "Implicit Chain-of-Thought",
        "Dynamic Early Exit",
        "Conditional Computation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yOs12gdsaL",
      "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context Transformer Inference",
      "authors": [
        "Weizhi Fei",
        "Xueyan Niu",
        "XIE GUOQING",
        "Yingqing Liu",
        "Bo Bai",
        "Wei Han"
      ],
      "abstract": "Although applications involving long-context inputs are crucial for the effective utilization of large language models (LLMs), they also result in increased computational costs and reduced performance. To address this challenge, we propose an efficient, training-free prompt compression method that retains key information within compressed prompts. We identify specific attention heads in transformer-based LLMs, which we designate as evaluator heads, that are capable of selecting tokens in long inputs that are most significant for inference. Building on this discovery, we develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly \"skim through'' input prompts by leveraging only the first few layers with evaluator heads during the pre-filling stage, subsequently passing only the important tokens to the model for inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt compression and long-context inference acceleration. Consequently, it effectively improves performance with the reduced costs associated with commercial API calls compared to prompt compressing methods. We further demonstrate that EHPC attains competitive results compared to key-value cache-based acceleration methods, thereby highlighting its potential to enhance the efficiency of LLMs for long-context tasks.",
      "arxiv_url": "https://openreview.net/forum?id=yOs12gdsaL",
      "pdf_url": "https://openreview.net/pdf/c92d4ba71ed258b69cbc3420e9b3873b1d9c5b21.pdf",
      "primary_category": "large language model, attention mechanism, long context inference",
      "categories": [
        "large language model",
        "attention mechanism",
        "long context inference",
        "prompt compression"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Vusd1Hw2D9",
      "title": "Multi-Agent Debate for LLM Judges with Adaptive Stability Detection",
      "authors": [
        "Tianyu Hu",
        "Zhen Tan",
        "Song Wang",
        "Huaizhi Qu",
        "Tianlong Chen"
      ],
      "abstract": "With advancements in reasoning capabilities, \nLarge Language Models (LLMs) are increasingly employed for automated judgment tasks.\nWhile LLMs-as-Judges offer promise in automating evaluations, \ncurrent approaches often rely on simplistic aggregation methods (e.g., majority voting), \nwhich can fail even when individual agents provide correct answers.\nTo address this, we propose a multi-agent debate judge framework where agents collaboratively reason and iteratively refine their responses. \nWe formalize the debate process mathematically, analyzing agent interactions and proving that debate amplifies correctness compared to static ensembles.\nTo enhance efficiency, we introduce a stability detection mechanism that models judge consensus dynamics via a time-varying Beta-Binomial mixture, with adaptive stopping based on distributional similarity (Kolmogorov-Smirnov test).\nThis mechanism models the judges' collective correct rate dynamics using a time-varying mixture of Beta-Binomial distributions and employs an adaptive stopping criterion based on distributional similarity (Kolmogorov-Smirnov statistic). \nExperiments across multiple benchmarks and models demonstrate that our framework improves judgment accuracy over majority voting while maintaining computational efficiency.",
      "arxiv_url": "https://openreview.net/forum?id=Vusd1Hw2D9",
      "pdf_url": "https://openreview.net/pdf/6f99dac2bdcda05f1538352395bc8461eea20d68.pdf",
      "primary_category": "LLM-as-Judge, Multi-Agent Debate, Adaptive Stopping",
      "categories": [
        "LLM-as-Judge",
        "Multi-Agent Debate",
        "Adaptive Stopping"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6j9xJ9pBjm",
      "title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling",
      "authors": [
        "Jialong Zhou",
        "Lichao Wang",
        "Xiao Yang"
      ],
      "abstract": "The emergence of large language models (LLMs) enables the development of intelligent agents capable of engaging in complex and multi-turn dialogues. However, multi-agent collaboration faces critical safety challenges, such as hallucination amplification and error injection and propagation. This paper presents GUARDIAN, a unified method for detecting and mitigating multiple safety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the multi-agent collaboration process as a discrete-time temporal attributed graph, GUARDIAN explicitly captures the propagation dynamics of hallucinations and errors. The unsupervised encoder-decoder architecture incorporating an incremental training paradigm learns to reconstruct node attributes and graph structures from latent embeddings, enabling the identification of anomalous nodes and edges with unparalleled precision. Moreover, we introduce a graph abstraction mechanism based on the Information Bottleneck Theory, which compresses temporal interaction graphs while preserving essential patterns. Extensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM multi-agent collaborations against diverse safety vulnerabilities, achieving state-of-the-art accuracy with efficient resource utilization. The code is available at https://github.com/JialongZhou666/GUARDIAN.",
      "arxiv_url": "https://openreview.net/forum?id=6j9xJ9pBjm",
      "pdf_url": "https://openreview.net/pdf/8835ebcdb803d6fa8f162eda4895abc5a399439c.pdf",
      "primary_category": "LLM-based agent, defense, safety",
      "categories": [
        "LLM-based agent",
        "defense",
        "safety"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "59n2g6RqjT",
      "title": "How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation",
      "authors": [
        "Yang Zhao",
        "Pu Wang",
        "Hao Frank Yang"
      ],
      "abstract": "Designing optimal prompts and reasoning processes for large language models (LLMs) on domain-specific tasks is both necessary and challenging in real-world applications. Determining how to integrate domain knowledge, enhance reasoning efficiency, and even provide domain experts with refined knowledge integration hints are particularly crucial yet unresolved tasks. In this research, we propose Evolutionary Graph Optimization for Prompting (EGO-Prompt), an automated framework to designing better prompts, efficient reasoning processes and providing enhanced causal-informed process. EGO-Prompt begins with a general prompt and fault-tolerant initial Semantic Causal Graph (SCG) descriptions, constructed by human experts, which is then automatically refined and optimized to guide LLM reasoning. Recognizing that expert-defined SCGs may be partial or imperfect and that their optimal integration varies across LLMs, EGO-Prompt integrates a novel causal-guided textual gradient process in two steps: first, generating nearly deterministic reasoning guidance from the SCG for each instance, and second, adapting the LLM to effectively utilize the guidance alongside the original input. The iterative optimization algorithm further refines both the SCG and the reasoning mechanism using textual gradients with ground-truth. We tested the framework on real-world public health, transportation and human behavior tasks. EGO-Prompt achieves 7.32\\%–12.61\\% higher F1 than cutting-edge methods, and allows small models to reach the performence of larger models at under 20\\% of the original cost. It also outputs a refined, domain-specific SCG that improves interpretability.",
      "arxiv_url": "https://openreview.net/forum?id=59n2g6RqjT",
      "pdf_url": "https://openreview.net/pdf/3069d87176e110b08bf8a1a84618385fcd354a5b.pdf",
      "primary_category": "LLMs, Prompt Optimization, Expert Knowledge Integration",
      "categories": [
        "LLMs",
        "Prompt Optimization",
        "Expert Knowledge Integration",
        "Task-adaptive Prompting"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "w5uUvxp81b",
      "title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs",
      "authors": [
        "Xiaomin Li",
        "Zhou Yu",
        "Zhiwei Zhang",
        "Xupeng Chen",
        "Ziji Zhang",
        "Yingying Zhuang",
        "Narayanan Sadagopan",
        "Anurag Beniwal"
      ],
      "abstract": "Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 20+ models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies.",
      "arxiv_url": "https://openreview.net/forum?id=w5uUvxp81b",
      "pdf_url": "https://openreview.net/pdf/057301f0efb5252bc107372a0fda3e9e277ad744.pdf",
      "primary_category": "Large language models, Reasoning, Instruction Following",
      "categories": [
        "Large language models",
        "Reasoning",
        "Instruction Following",
        "Chain of Thoughts"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XsQNqRdcdh",
      "title": "Once Upon an Input: Reasoning via Per-Instance Program Synthesis",
      "authors": [
        "Adam Stein",
        "Neelay Velingker",
        "Mayur Naik",
        "Eric Wong"
      ],
      "abstract": "Large language models (LLMs) excel at zero-shot inference but continue to struggle with complex, multi-step reasoning. Recent methods that augment LLMs with intermediate reasoning steps such as Chain of Thought (CoT) and Program of Thought (PoT) improve performance but often produce undesirable solutions, especially in algorithmic domains.\nWe introduce Per-Instance Program Synthesis (PIPS), a method that generates and refines programs at the instance level using structural feedback without relying on task-specific guidance or explicit test cases. To further improve performance, PIPS incorporates a confidence metric that dynamically chooses between direct inference and program synthesis on a per-instance basis.\nExperiments across three frontier LLMs and 30 benchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question answering tasks, relational reasoning tasks, and mathematical reasoning tasks show that PIPS  improves the absolute harmonic mean accuracy by up to 8.6\\% and 9.4\\% compared to PoT and CoT respectively,\nand reduces undesirable program generations by 65.1\\% on the algorithmic tasks compared to PoT with Gemini-2.0-Flash.",
      "arxiv_url": "https://openreview.net/forum?id=XsQNqRdcdh",
      "pdf_url": "https://openreview.net/pdf/7b4a6ca902e7228c2855e6b864e4699825b54723.pdf",
      "primary_category": "program synthesis, reasoning, LLM",
      "categories": [
        "program synthesis",
        "reasoning",
        "LLM",
        "instance",
        "chain of thought",
        "neuro-symbolic"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "CGNJL6CeV0",
      "title": "Measuring AI Ability to Complete Long Software Tasks",
      "authors": [
        "Thomas Kwa",
        "Ben West",
        "Joel Becker",
        "Amy Deng",
        "Katharyn Garcia",
        "Max Hasin",
        "Sami Jawhar",
        "Megan Kinniment",
        "Nate Rush",
        "Sydney Von Arx",
        "Ryan Bloom",
        "Thomas Broadley",
        "Haoxing Du",
        "Brian Goodrich",
        "Nikola Jurkovic",
        "Luke Harold Miles",
        "Seraphina Nix",
        "Tao Roa Lin",
        "Neev Parikh",
        "David Rein",
        "Lucas Jun Koba Sato",
        "Hjalmar Wijk",
        "Daniel M Ziegler",
        "Elizabeth Barnes",
        "Lawrence Chan"
      ],
      "abstract": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as o3 have a 50% time horizon of around 110 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated since 2024. The increase in AI models’ time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results—including their degree of external validity—and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month.",
      "arxiv_url": "https://openreview.net/forum?id=CGNJL6CeV0",
      "pdf_url": "https://openreview.net/pdf/402f09e70f9d9e6a99edbcaaa360692d05eec7ab.pdf",
      "primary_category": "benchmarks, evaluations, Natural language processing",
      "categories": [
        "benchmarks",
        "evaluations",
        "Natural language processing",
        "Software engineering",
        "forecasting"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1cjLvtFOmL",
      "title": "Block-Diagonal LoRA for Eliminating Communication Overhead in Tensor Parallel LoRA Serving",
      "authors": [
        "Xinyu Wang",
        "Jonas M. Kübler",
        "Kailash Budhathoki",
        "Yida Wang",
        "Matthäus Kleindessner"
      ],
      "abstract": "When serving a single base LLM with several different LoRA adapters simultaneously, the adapters cannot simply be merged with the base model’s weights as the adapter swapping would create overhead and requests using different adapters could not be batched. Rather, the LoRA computations have to be separated from the base LLM computations, and in a multi-device setup the LoRA adapters can be sharded in a way that is well aligned with the base model’s tensor parallel execution, as proposed in S-LoRA. However, the S-LoRA sharding strategy encounters some communication overhead, which may be small in theory, but can be large in practice. In this paper, we propose to constrain certain LoRA factors to be block-diagonal, which allows for an alternative way of sharding LoRA adapters that does not require any additional communication for the LoRA computations. We demonstrate in extensive experiments that our block-diagonal LoRA approach is similarly parameter efficient as standard LoRA (i.e., for a similar number of parameters it achieves similar downstream performance) and that it leads to significant end-to-end speed-up over S-LoRA. For example, when serving on eight A100 GPUs, we observe up to 1.79x (1.23x) end-to-end speed-up with 0.87x (1.74x) the number of adapter parameters for Llama-3.1-70B, and up to 1.63x (1.3x) end-to-end speed-up with 0.86x (1.73x) the number of adapter parameters for Llama-3.1-8B.",
      "arxiv_url": "https://openreview.net/forum?id=1cjLvtFOmL",
      "pdf_url": "https://openreview.net/pdf/815072b587d73a12b63c1c0621b52862be80ee10.pdf",
      "primary_category": "LoRA, LLM, block-diagonal",
      "categories": [
        "LoRA",
        "LLM",
        "block-diagonal",
        "Tensor Parallelism",
        "BD-LoRA"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pY65QWWFlm",
      "title": "Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve",
      "authors": [
        "Yuanzhe Liu",
        "Ryan Deng",
        "Tim Kaler",
        "Xuhao Chen",
        "Charles Leiserson",
        "Yao Ma",
        "Jie Chen"
      ],
      "abstract": "Recent studies show that LLMs possess different skills and specialize in different tasks. In fact, we observe that their varied performance occur in several levels of granularity. For example, in the code optimization task, code LLMs excel at different optimization categories and no one dominates others. This observation prompts the question of how one leverages multiple LLM agents to solve a coding problem without knowing their complementary strengths a priori. We argue that a team of agents can learn from each other's successes and failures so as to improve their own performance. Thus, a lesson is the knowledge produced by an agent and passed on to other agents in the collective solution process. We propose a lesson-based collaboration framework, design the lesson solicitation--banking--selection mechanism, and demonstrate that a team of small LLMs with lessons learned can outperform a much larger LLM and other multi-LLM collaboration methods.",
      "arxiv_url": "https://openreview.net/forum?id=pY65QWWFlm",
      "pdf_url": "https://openreview.net/pdf/11bc550a62e76a702248f346f36018dce79624fb.pdf",
      "primary_category": "Multi Agent, Large Language Model, Code Optimization",
      "categories": [
        "Multi Agent",
        "Large Language Model",
        "Code Optimization"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zefDc9oi5T",
      "title": "RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models",
      "authors": [
        "Yilang Zhang",
        "Bingcong Li",
        "Georgios B. Giannakis"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) lowers the computational and memory overhead of fine-tuning large models by updating a low-dimensional subspace of the pre-trained weight matrix. Albeit efficient, LoRA exhibits suboptimal convergence and noticeable performance degradation, due to inconsistent and imbalanced weight updates induced by its nonunique low-rank factorizations. To overcome these limitations, this article identifies the optimal low-rank factorization per step that minimizes an upper bound on the loss. The resultant refactored low-rank adaptation (RefLoRA) method promotes a flatter loss landscape, along with consistent and balanced weight updates, thus speeding up stable convergence. Extensive experiments evaluate RefLoRA on natural language understanding, and commonsense reasoning tasks with popular large language models including DeBERTaV3, LLaMA-7B, LLaMA2-7B and LLaMA3-8B. The numerical tests corroborate that RefLoRA converges faster, outperforms various benchmarks, and enjoys negligible computational overhead compared to state-of-the-art LoRA variants.",
      "arxiv_url": "https://openreview.net/forum?id=zefDc9oi5T",
      "pdf_url": "https://openreview.net/pdf/d7ba65ab3126c814359d0542a10cfcb11b005d3a.pdf",
      "primary_category": "Low-rank adaptation, parameter-efficient fine-tuning, optimization",
      "categories": [
        "Low-rank adaptation",
        "parameter-efficient fine-tuning",
        "optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ecqTSIMR2o",
      "title": "Vision‑Language‑Vision Auto‑Encoder: Scalable Knowledge Distillation from Diffusion Models",
      "authors": [
        "Tiezheng Zhang",
        "Yitong Li",
        "Yu-Cheng Chou",
        "Jieneng Chen",
        "Alan Yuille",
        "Chen Wei",
        "Junfei Xiao"
      ],
      "abstract": "Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billions of high-quality image-text pairs, requiring millions of GPU hours. This paper introduces the Vision-Language-Vision **(VLV)** auto-encoder framework, which strategically leverages key pretrained components: a vision encoder, the decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large Language Model (LLM). Specifically, we establish an information bottleneck by regularizing the language representation space, achieved through freezing the pretrained T2I diffusion decoder. Our VLV pipeline effectively distills knowledge from the text-conditioned diffusion model using continuous embeddings, demonstrating comprehensive semantic understanding via high-quality reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the intermediate language representations into detailed descriptions, we construct a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and significantly reduces data requirements; by primarily utilizing single-modal images for training and maximizing the utility of existing pretrained models (image encoder, T2I diffusion model, and LLM), it circumvents the need for massive paired image-text datasets, keeping the total training expenditure under $1,000 USD.",
      "arxiv_url": "https://openreview.net/forum?id=ecqTSIMR2o",
      "pdf_url": "https://openreview.net/pdf/d87246b38d2e11f8c1b35ebe7dfa2dc616cd399e.pdf",
      "primary_category": "Auto-Encoder, Diffusion Model, Vision-Language-Vision",
      "categories": [
        "Auto-Encoder",
        "Diffusion Model",
        "Vision-Language-Vision"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0biUwyjKkm",
      "title": "OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model",
      "authors": [
        "Zhenhao Zhang",
        "Ye Shi",
        "Lingxiao Yang",
        "Suting Ni",
        "Qi Ye",
        "Jingya Wang"
      ],
      "abstract": "Understanding and synthesizing realistic 3D hand-object interactions (HOI) is critical for applications ranging from immersive AR/VR to dexterous robotics. Existing methods struggle with generalization, performing well on closed-set objects and predefined tasks but failing to handle unseen objects or open-vocabulary instructions. We introduce OpenHOI, the first framework for open-world HOI synthesis, capable of generating long-horizon manipulation sequences for novel objects guided by free-form language commands. Our approach integrates a 3D Multimodal Large Language Model (MLLM) fine-tuned for joint affordance grounding and semantic task decomposition, enabling precise localization of interaction regions (e.g., handles, buttons) and breakdown of complex instructions (e.g., “Find a water bottle and take a sip”) into executable sub-tasks. To synthesize physically plausible interactions, we propose an affordance-driven diffusion model paired with a training-free physics refinement stage that minimizes penetration and optimizes affordance alignment.\nEvaluations across diverse scenarios demonstrate OpenHOI’s superiority over state-of-the-art methods in generalizing to novel object categories, multi-stage tasks, and complex language instructions.",
      "arxiv_url": "https://openreview.net/forum?id=0biUwyjKkm",
      "pdf_url": "https://openreview.net/pdf/9aa1e78b6ff4b90a1f09a2e5387598c62ad4d21d.pdf",
      "primary_category": "Hand-Object Interaction;Open-World;Large Language Models",
      "categories": [
        "Hand-Object Interaction;Open-World;Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vvEVQrm9M8",
      "title": "DUO: No Compromise to Accuracy Degradation",
      "authors": [
        "Jinda Jia",
        "Cong Xie",
        "Hanlin Lu",
        "Fanjiang Ye",
        "Hao Feng",
        "Daoce Wang",
        "Haibin Lin",
        "Zhi Zhang",
        "Xin Liu"
      ],
      "abstract": "Distributed training often suffers from high communication overhead due to large-scale gradient synchronization. Although gradient compression—particularly at 4-bit or even lower precision—significantly reduces transfer volume, it typically results in sacrifice in precision and degradation of the final model accuracy. \n\nIn this work, we introduce DUO, a distributed training framework designed to mitigate accuracy degradation incurred by gradient compression without involving additional overhead. DUO achieves this by inserting an additional high-precision gradient synchronization step into a previously computation-only phase, so that its communication is fully hidden by computation.\n\nWe provide a comprehensive theoretical proof of convergence for DUO and validate its effectiveness through extensive pre-training experiments on GPT models. Our results indicate that DUO effectively restores accuracy when using 4-bit gradient compression, achieving performance comparable to uncompressed training. Remarkably, DUO maintains minimal accuracy degradation even under extreme compression scenarios, including 1-bit gradients or complete omission of the low-precision gradient communication step (0-bit transmission).",
      "arxiv_url": "https://openreview.net/forum?id=vvEVQrm9M8",
      "pdf_url": "https://openreview.net/pdf/503971f94e4a666bde3e0ec78b4fc192a393d885.pdf",
      "primary_category": "Gradient Compression, Communication Computation Overlapping, Efficient Training",
      "categories": [
        "Gradient Compression",
        "Communication Computation Overlapping",
        "Efficient Training",
        "Distributed Training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VrCdsZBbIg",
      "title": "Language Modeling by Language Models",
      "authors": [
        "Junyan Cheng",
        "Peter Clark",
        "Kyle Richardson"
      ],
      "abstract": "*Can we leverage LLMs to model the process of discovering novel language model (LM) architectures?* Inspired by real research, we propose a multi-agent LLM approach that simulates the conventional stages of research, from ideation and literature search (proposal stage) to design implementation (code generation), generative pre-training, and downstream evaluation (verification). Using ideas from scaling laws, our system *Genesys* employs a *Ladder of Scales* approach; new designs are proposed, adversarially reviewed, implemented, and selectively verified at increasingly larger model scales (14M$\\sim$350M parameters) with a narrowing budget (the number of models we can train at each scale). To help make discovery efficient and factorizable, Genesys uses a novel genetic programming backbone, which we show has empirical advantages over commonly used direct prompt generation workflows (e.g., $\\sim$86\\% percentage point improvement in successful design generation, a key bottleneck). We report experiments involving 1,162 newly discovered designs (1,062 fully verified) and find the best designs to be competitive with known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common benchmarks).  We couple these results with comprehensive system-level ablations and formal results, which give broader insights into the design of effective autonomous discovery systems.",
      "arxiv_url": "https://openreview.net/forum?id=VrCdsZBbIg",
      "pdf_url": "https://openreview.net/pdf/1dda401bc2d6f8ee17a263ac3f358eb51e094d8e.pdf",
      "primary_category": "Autonomous Scientific Discovery, Large Language Model Agents, Genetic Programming",
      "categories": [
        "Autonomous Scientific Discovery",
        "Large Language Model Agents",
        "Genetic Programming"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Rx6m16By6l",
      "title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning",
      "authors": [
        "Qitao Tan",
        "Jun Liu",
        "Zheng Zhan",
        "Caiwen Ding",
        "Yanzhi Wang",
        "Xiaolong Ma",
        "Jaewoo Lee",
        "Jin Lu",
        "Geng Yuan"
      ],
      "abstract": "Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes and relying solely on forward passes for gradient estimation, making it attractive for resource-constrained scenarios. However, ZO method lags far behind FO method in both convergence speed and accuracy. To bridge the gap, we introduce a novel layer-wise divergence analysis that uncovers the distinct update pattern of FO and ZO optimization. Aiming to resemble the learning capacity of FO method from the findings, we propose \\textbf{Di}vergence-driven \\textbf{Z}eroth-\\textbf{O}rder (\\textbf{DiZO}) optimization. DiZO conducts divergence-driven layer adaptation by incorporating projections to ZO updates, generating diverse-magnitude updates precisely scaled to layer-wise individual optimization needs. Our results demonstrate that DiZO significantly reduces the needed iterations for convergence without sacrificing throughput, cutting training GPU hours by up to 48\\% on various datasets. Moreover, DiZO consistently outperforms the representative ZO baselines in fine-tuning RoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some cases, even surpasses memory-intensive FO fine-tuning. Our code is released at \\url{https://github.com/Skilteee/DiZO}.",
      "arxiv_url": "https://openreview.net/forum?id=Rx6m16By6l",
      "pdf_url": "https://openreview.net/pdf/5ca19ec13f62736b95f0a739feaedd52b42c873a.pdf",
      "primary_category": "Memory-efficient fine-tuning; Zeroth-order optimization",
      "categories": [
        "Memory-efficient fine-tuning; Zeroth-order optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "SsHCyEBMLz",
      "title": "$\\texttt{BetaConform}$: Efficient MAP Estimation of LLM Ensemble Judgment Performance with Prior Transfer",
      "authors": [
        "Huaizhi Qu",
        "Inyoung Choi",
        "Zhen Tan",
        "Song Wang",
        "Sukwon Yun",
        "Qi Long",
        "Faizan Siddiqui",
        "Kwonjoon Lee",
        "Tianlong Chen"
      ],
      "abstract": "LLM ensembles are widely used for LLM judges. However, how to estimate their accuracy, especially in an efficient way, is unknown. In this paper, we present a principled $\\textit{maximum a posteriori}$ (MAP) framework for an economical and precise estimation of the performance of LLM ensemble judgment. We first propose a mixture of Beta-Binomial distributions to model the judgment distribution, revising from the vanilla Binomial distribution. Next, we introduce a conformal prediction-driven approach that enables adaptive stopping during iterative sampling to balance accuracy with efficiency. Furthermore, we design a prior transfer mechanism that utilizes learned distributions on open-source datasets to improve estimation on a target dataset when only scarce annotations are available. Finally, we present $\\texttt{BetaConform}$, a framework that integrates our distribution assumption, adaptive stopping, and the prior transfer mechanism to deliver a theoretically guaranteed distribution estimation of LLM ensemble judgment with minimum labeled samples. $\\texttt{BetaConform}$ is also validated empirically. For instance, with only $10$ samples from the TruthfulQA dataset, for a Llama ensembled judge, $\\texttt{BetaConform}$ gauges its performance with an error margin as small as $3.37\\\\%$.",
      "arxiv_url": "https://openreview.net/forum?id=SsHCyEBMLz",
      "pdf_url": "https://openreview.net/pdf/9c0f5298eb976ac167b87ec84a6eab55c436bb88.pdf",
      "primary_category": "LLM-as-a-Judge, Distribution Estimation, LLM Ensemble",
      "categories": [
        "LLM-as-a-Judge",
        "Distribution Estimation",
        "LLM Ensemble"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3rRzYrpO70",
      "title": "Steering Information Utility in Key-Value Memory for Language Model Post-Training",
      "authors": [
        "Chunyuan Deng",
        "Ruidi Chang",
        "Hanjie Chen"
      ],
      "abstract": "Recent advancements in language models (LMs) have marked a shift toward the growing importance of post-training. Yet, post-training approaches such as supervised fine-tuning (SFT) do not guarantee the effective use of knowledge acquired during pretraining. We therefore introduce infosteer, a lightweight method that encourages parametric information utilization in LMs during post-training. Specifically, Infosteer treats the feed-forward network (FFN) layer as associate key-value memory and promotes the use of stored memory vectors via forward-pass interventions or regularization during backpropagation. This simple guidance during post-training phase yields consistent performance improvements across diverse model families--including Qwen, Gemma and Llama---spanning 15 downstream tasks in both in-distribution (ID) and out-of-distribution (OOD) evaluations. Beyond performance gains, we also find that steered LMs can adaptively allocate information by placing more emphasis on generating semantically meaningful tokens, while using fewer resources on simple transition ones (e.g., ',' or 'and'). Our work underscores that vanilla post-training does not fully exploit the potential gained during pre-training, and that steering LMs in latent representation space offers a promising approach to enhance both performance and interpretability.",
      "arxiv_url": "https://openreview.net/forum?id=3rRzYrpO70",
      "pdf_url": "https://openreview.net/pdf/adb32e4c0fd5be77ced7c2f759a3900d4fbb7d46.pdf",
      "primary_category": "Interpretability, Intervention, Model Steering",
      "categories": [
        "Interpretability",
        "Intervention",
        "Model Steering",
        "Language Models",
        "Post-Training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XOIKLlSiDq",
      "title": "Can DPO Learn Diverse Human Values? A Theoretical Scaling Law",
      "authors": [
        "Shawn Im",
        "Sharon Li"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities but often struggle to align with human preferences, leading to harmful or undesirable outputs. Preference learning, which trains models to distinguish between preferred and non-preferred responses based on human feedback, has become a crucial component for ensuring that LLMs align with human values. An essential part of ensuring that LLMs are aligned for all people is accounting for a diverse set of values. This paper introduces a new theoretical framework to analyze how generalization scales with value diversity and sample quantity in models trained with direct preference optimization. Our framework rigorously assesses how well models generalize after a finite number of gradient steps, reflecting real-world LLM training practices. By analyzing the reward margin associated with each sample and its trajectory throughout training, we provide a bound on the generalization error that demonstrates the challenges of effectively learning a wide set of concepts or values. These insights are empirically validated on contemporary LLMs, underscoring the practical relevance of our theory.",
      "arxiv_url": "https://openreview.net/forum?id=XOIKLlSiDq",
      "pdf_url": "https://openreview.net/pdf/423a825732e0caf6fadb0f5b8b5e297d3d95696f.pdf",
      "primary_category": "LLM, preference learning, generalization",
      "categories": [
        "LLM",
        "preference learning",
        "generalization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xNJenVNmzL",
      "title": "PiKE: Adaptive Data Mixing for Large-Scale Multi-Task Learning Under Low Gradient Conflicts",
      "authors": [
        "Zeman Li",
        "Yuan Deng",
        "Peilin Zhong",
        "Meisam Razaviyayn",
        "Vahab Mirrokni"
      ],
      "abstract": "Modern foundation models are trained on diverse datasets to enhance generalization across tasks and domains. A central challenge in this process is determining how to effectively mix and sample data from multiple sources. This naturally leads to a multi-task learning (MTL) perspective. While prior work in MTL has emphasized mitigating gradient conflicts, we observe that large-scale pretraining scenarios—such as multilingual or multi-domain training—often exhibit little to no gradient conflict. Motivated by this observation, we propose $\\textbf{PiKE}$ ($\\textbf{P}$ositive gradient $\\textbf{i}$nteraction-based $\\textbf{K}$-task weights $\\textbf{E}$stimator), an adaptive data mixing algorithm that dynamically adjusts sampling weights during training. PiKE exploits non-conflicting gradient interactions to minimize a near-tight upper bound on the average loss decrease at each step, while incurring negligible computational overhead. We provide theoretical convergence guarantees and  show that PiKE  outperforms static and non-adaptive mixing baselines. Furthermore, we extend PiKE to promote balanced learning across tasks. Extensive experiments on large-scale language model pretraining confirm that PiKE achieves faster convergence and improved downstream performance compared to existing approaches.",
      "arxiv_url": "https://openreview.net/forum?id=xNJenVNmzL",
      "pdf_url": "https://openreview.net/pdf/138d67f3b4689dc9559171f7e3bfdbc74d2d00eb.pdf",
      "primary_category": "Multi-task learning, Large language models, Large-scale optimization",
      "categories": [
        "Multi-task learning",
        "Large language models",
        "Large-scale optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9Ia0KiVAut",
      "title": "Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning",
      "authors": [
        "Yiqun Chen",
        "Lingyong Yan",
        "Weiwei Sun",
        "Xinyu Ma",
        "Yi Zhang",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Yiming Yang",
        "Jiaxin Mao"
      ],
      "abstract": "Retrieval-augmented generation (RAG) is widely utilized to incorporate external knowledge into large language models, thereby enhancing factuality and reducing hallucinations in question-answering (QA) tasks. A standard RAG pipeline consists of several components, such as query rewriting, document retrieval, document filtering, and answer generation. However, these components are typically optimized separately through supervised fine-tuning, which can lead to misalignments between the objectives of individual components and the overarching aim of generating accurate answers. Although recent efforts have explored using reinforcement learning (RL) to optimize specific RAG components, these approaches often focus on simple pipelines with only two components or do not adequately address the complex interdependencies and collaborative interactions among the modules. To overcome these limitations, we propose treating the complex RAG pipeline with multiple components as a multi-agent cooperative task, in which each component can be regarded as an RL agent. Specifically, we present MMOA-RAG\\footnote{The code of MMOA-RAG is on \\url{https://github.com/chenyiqun/MMOA-RAG}.}, \\textbf{M}ulti-\\textbf{M}odule joint \\textbf{O}ptimization \\textbf{A}lgorithm for \\textbf{RAG}, which employs multi-agent reinforcement learning to harmonize all agents' goals toward a unified reward, such as the F1 score of the final answer. Experiments conducted on various QA benchmarks demonstrate that MMOA-RAG effectively boost the overall performance of the pipeline and outperforms existing baselines. Furthermore, comprehensive ablation studies validate the contributions of individual components and demonstrate MMOA-RAG can be adapted to different RAG pipelines and benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=9Ia0KiVAut",
      "pdf_url": "https://openreview.net/pdf/432af8fbb3b4f8402a3c5a26a0cc59ee21ac39cf.pdf",
      "primary_category": "Retrieval-Augmented Generation, Multi-Agent Cooperation, Multi-Agent Reinforcement Learning",
      "categories": [
        "Retrieval-Augmented Generation",
        "Multi-Agent Cooperation",
        "Multi-Agent Reinforcement Learning",
        "Multi-Module Joint Learning"
      ],
      "tags": [
        "LLM",
        "RAG",
        "Information Retrieval",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "W2874Arl4g",
      "title": "Language Models can Self-Improve at State-Value Estimation for Better Search",
      "authors": [
        "Ethan Mendes",
        "Alan Ritter"
      ],
      "abstract": "Collecting ground-truth rewards or human demonstrations for multi-step reasoning tasks is often prohibitively expensive, especially in interactive domains such as web tasks. We introduce Self-Taught Lookahead (STL), a reward-free framework that improves language model–based value functions by reasoning explicitly about state transitions. STL can be viewed as a chain-of-thought analogue of the value iteration algorithm: instead of regressing directly on numeric values, a value LLM is trained to simulate a step of lookahead in natural language—predicting the next action, resulting state, and rationale for its value. This process refines value estimates without any labeled data. The self-supervised procedure yields more accurate state-value predictions, which in turn enable lightweight search algorithms to expand fewer states while maintaining strong performance. Empirically, STL-trained value models built on moderately sized (8B-parameter) open-weight LLMs boost web agent success rates by over 39%, achieving performance comparable to proprietary models. STL also generalizes to multi-hop question answering and math puzzles. Overall, STL enables small open-source models to guide efficient search, reducing inference costs by integrating explicit reasoning with value learning.",
      "arxiv_url": "https://openreview.net/forum?id=W2874Arl4g",
      "pdf_url": "https://openreview.net/pdf/ea251516f78a77cbc42dd4a50e2ce4aadfc2226f.pdf",
      "primary_category": "LLMs, agents, reasoning",
      "categories": [
        "LLMs",
        "agents",
        "reasoning",
        "self-improvement",
        "multi-step reasoning",
        "search"
      ],
      "tags": [
        "LLM",
        "Search Agent"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "skS03tzYNw",
      "title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling",
      "authors": [
        "Shuyuan Zhang",
        "Chenhan Jiang",
        "Zuoou Li",
        "Jiankang Deng"
      ],
      "abstract": "3D generation from natural language offers significant potential to reduce expert manual modeling efforts and enhance accessibility to 3D assets. However, existing methods often yield unstructured meshes and exhibit poor interactivity, making them impractical for artistic workflows. To address these limitations, we represent 3D assets as shape programs and introduce ShapeCraft, a novel multi-agent framework for text-to-3D generation. At its core, we propose a Graph-based Procedural Shape (GPS) representation that decomposes complex natural language into a structured graph of sub-tasks, thereby facilitating accurate LLM comprehension and interpretation of spatial relationships and semantic shape details. Specifically, LLM agents hierarchically parse user input to initialize GPS, then iteratively refine procedural modeling and painting to produce structured, textured, and interactive 3D assets. Qualitative and quantitative experiments demonstrate ShapeCraft's superior performance in generating geometrically accurate and semantically rich 3D assets compared to existing LLM-based agents. We further show the versatility of ShapeCraft through examples of animated and user-customized editing, highlighting its potential for broader interactive applications.",
      "arxiv_url": "https://openreview.net/forum?id=skS03tzYNw",
      "pdf_url": "https://openreview.net/pdf/59fcd010a8212ae3bab53aa4724ff5534767041f.pdf",
      "primary_category": "Text-to-3D Generation, LLM Agents, 3D Modeling",
      "categories": [
        "Text-to-3D Generation",
        "LLM Agents",
        "3D Modeling"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "OcMpSh79aE",
      "title": "HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs",
      "authors": [
        "Saleh Ashkboos",
        "Mahdi Nikdan",
        "Soroush Tabesh",
        "Roberto L. Castro",
        "Torsten Hoefler",
        "Dan Alistarh"
      ],
      "abstract": "Quantized training of Large Language Models (LLMs) remains an open challenge, as maintaining accuracy while performing all matrix multiplications in low precision has proven difficult. This is particularly the case when fine-tuning pre-trained models, which can have large weight, activation, and error (output gradient) outlier values that make lower-precision optimization difficult. To address this, we present HALO, a new quantization-aware training approach for Transformers that enables accurate and efficient low-precision training by combining 1) strategic placement of Hadamard rotations in both forward and backward passes, which mitigate outliers, 2) high-performance kernel support, and 3) FSDP integration for low-precision communication. Our approach ensures that all large matrix multiplications during the forward and backward passes are executed in lower precision. Applied to LLaMa models, HALO achieves near-full-precision-equivalent results during fine-tuning  on various tasks, while delivering up to 1.41x end-to-end speedup for full fine-tuning on RTX 4090 GPUs. HALO efficiently supports both standard and parameter-efficient fine-tuning (PEFT). Our results demonstrate the first practical approach to fully quantized LLM fine-tuning that maintains accuracy in INT8 and FP6 precision, while delivering performance benefits.",
      "arxiv_url": "https://openreview.net/forum?id=OcMpSh79aE",
      "pdf_url": "https://openreview.net/pdf/a4a2a052fc20fb6537deb7232ea3c6579f9da438.pdf",
      "primary_category": "LLMs, Quantization, Rotation",
      "categories": [
        "LLMs",
        "Quantization",
        "Rotation",
        "Fine-tuning",
        "Full Fine-tuning",
        "PEFT"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "CAB0EjD9EK",
      "title": "CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning",
      "authors": [
        "Man Ho LAM",
        "Chaozheng Wang",
        "Jen-tse Huang",
        "Michael Lyu"
      ],
      "abstract": "Large Language Models (LLMs) have recently demonstrated strong capabilities in code-related tasks, but their robustness in code reasoning under perturbations remains underexplored. We introduce CodeCrash, a stress-testing framework with 1,279 questions from CRUXEVAL and LIVECODEBENCH, designed to evaluate reasoning reliability under structural perturbations and misleading natural language (NL) contexts. Through a systematic evaluation of 17 LLMs, we find that models often shortcut reasoning by over-relying on NL cues, leading to an average performance degradation of 23.2% in output prediction tasks. Even with Chain-of-Thought reasoning, models on average still have a 13.8% drop due to distractibility and rationalization, revealing a lack of critical reasoning capability to distinguish the actual code behaviors. While Large Reasoning Models with internal reasoning mechanisms improve robustness by fostering critical thinking, plausible yet incorrect hints can trigger pathological self-reflection, causing 2-3 times token consumption and even catastrophic cognitive dissonance in extreme cases for QwQ-32B. We refer to this phenomenon as Reasoning Collapse. CodeCrash provides a rigorous benchmark for evaluating robustness in code reasoning, guiding future research and development toward more reliable and resilient models.",
      "arxiv_url": "https://openreview.net/forum?id=CAB0EjD9EK",
      "pdf_url": "https://openreview.net/pdf/9b9883a392dca607cfb2b26eccc24136b0e5cf90.pdf",
      "primary_category": "Large Language Models, Code Reasoning, Code Execution",
      "categories": [
        "Large Language Models",
        "Code Reasoning",
        "Code Execution",
        "Input-Output Prediction",
        "Robustness Evaluation",
        "Code Perturbations",
        "Model Reliability",
        "Adversarial Testing"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tRXt10xKc5",
      "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-world Sensory Perceptions",
      "authors": [
        "Bufang Yang",
        "Lilin Xu",
        "Liekang Zeng",
        "Kaiwei Liu",
        "Siyang Jiang",
        "Wenrui Lu",
        "Hongkai Chen",
        "Xiaofan Jiang",
        "Guoliang Xing",
        "Zhenyu Yan"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have propelled intelligent agents from reactive responses to proactive support. \nWhile promising, existing proactive agents either rely exclusively on observations from enclosed environments (e.g., desktop UIs) with direct LLM inference or employ rule-based proactive notifications, leading to suboptimal user intent understanding and limited functionality for proactive service. In this paper, we introduce ContextAgent, the first context-aware proactive agent that incorporates extensive sensory contexts surrounding humans to enhance the proactivity of LLM agents. ContextAgent first extracts multi-dimensional contexts from massive sensory perceptions on wearables (e.g., video and audio) to understand user intentions. ContextAgent then leverages the sensory contexts and personas from historical data to predict the necessity for proactive services. When proactive assistance is needed, ContextAgent further automatically calls the necessary tools to assist users unobtrusively. To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools. Experiments on ContextAgentBench show that ContextAgent outperforms baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive predictions and tool calling, respectively. We hope our research can inspire the development of more advanced, human-centric, proactive AI assistants. The code and dataset are publicly available at https://github.com/openaiotlab/ContextAgent.",
      "arxiv_url": "https://openreview.net/forum?id=tRXt10xKc5",
      "pdf_url": "https://openreview.net/pdf/8c61939b607693d9b13cc1df27793d844f3648f5.pdf",
      "primary_category": "LLM Agents, Proactive Agent, Human-centric AI",
      "categories": [
        "LLM Agents",
        "Proactive Agent",
        "Human-centric AI",
        "Personal Assistant"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1qKUVyymXs",
      "title": "ACCO: Accumulate While You Communicate for Communication-Overlapped Sharded LLM Training",
      "authors": [
        "Adel Nabli",
        "Louis Fournier",
        "Pierre ERBACHER",
        "Louis Serrano",
        "Eugene Belilovsky",
        "Edouard Oyallon"
      ],
      "abstract": "Training LLMs relies on distributed implementations using multiple GPUs to compute gradients in parallel with sharded optimizers. However, synchronizing gradients in data parallel setups introduces communication overhead that grows with the number of workers, limiting parallelization efficiency. Local optimization algorithms reduce communications but incur high memory costs as they prevent optimizer state sharding, hindering scalability. To address this, we propose $\\textbf{AC}$cumulate while $\\textbf{CO}$mmunicate ($\\texttt{ACCO}$), a memory-efficient optimization algorithm for distributed LLM training. By synchronizing delayed gradients while computing new ones, $\\texttt{ACCO}$ reduces GPU idle time and supports heterogeneous hardware. To mitigate the convergence issues caused by delayed updates, we introduce a novel technique ensuring training dynamics align with standard distributed optimization. Compared to ZeRO-1, our approach is significantly faster and scales effectively across heterogeneous hardware.",
      "arxiv_url": "https://openreview.net/forum?id=1qKUVyymXs",
      "pdf_url": "https://openreview.net/pdf/f848af968d1c36e105b6d643f30262765a17fdf9.pdf",
      "primary_category": "distributed training; communication overlapping; sharded optimizers",
      "categories": [
        "distributed training; communication overlapping; sharded optimizers"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "EpgMSwJY8t",
      "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs",
      "authors": [
        "Qijun Luo",
        "Mengqi Li",
        "Lei Zhao",
        "Xiao Li"
      ],
      "abstract": "Training language models on long sequence data is a demanding requirement for enhancing the model's capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a *memory-efficient* and *exact* BP method called **StreamBP**, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by $2.8-5.5 \\times$ larger, while using comparable or even less BP time. Note that StreamBP's sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at https://github.com/Ledzy/StreamBP.",
      "arxiv_url": "https://openreview.net/forum?id=EpgMSwJY8t",
      "pdf_url": "https://openreview.net/pdf/29adc0353825da13026b2601d3a98fd39d9f3906.pdf",
      "primary_category": "Memory efficiency, long sequence training, training reasoning LLMs",
      "categories": [
        "Memory efficiency",
        "long sequence training",
        "training reasoning LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0enwkxV3sx",
      "title": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models",
      "authors": [
        "Haidong Xu",
        "Guangwei Xu",
        "Zhedong Zheng",
        "Xiatian Zhu",
        "Wei Ji",
        "Xiangtai Li",
        "Ruijie Guo",
        "Meishan Zhang",
        "Min zhang",
        "Hao Fei"
      ],
      "abstract": "This paper introduces **VimoRAG**, a novel video-based retrieval-augmented motion generation framework for motion large language models (LLMs). \nAs motion LLMs face severe out-of-domain/out-of-vocabulary issues due to limited annotated data, **VimoRAG** leverages large-scale in-the-wild video databases to enhance 3D motion generation by retrieving relevant 2D human motion signals. \nWhile video-based motion RAG is nontrivial, we address two key bottlenecks: \n(1) developing an effective motion-centered video retrieval model that distinguishes human poses and actions, \nand (2) mitigating the issue of error propagation caused by suboptimal retrieval results.\nWe design the Gemini Motion Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer, enabling effective retrieval and generation processes. \nExperimental results show that **VimoRAG** significantly boosts the performance of motion LLMs constrained to text-only input.",
      "arxiv_url": "https://openreview.net/forum?id=0enwkxV3sx",
      "pdf_url": "https://openreview.net/pdf/3bc3222b486dccb71165993d1702dc5e7d2d70c4.pdf",
      "primary_category": "Motion Generation, Retrieval-augmented Generation",
      "categories": [
        "Motion Generation",
        "Retrieval-augmented Generation"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "NOUF43YrIL",
      "title": "Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model",
      "authors": [
        "Dongki Kim",
        "Wonbin Lee",
        "Sung Ju Hwang"
      ],
      "abstract": "Understanding molecules is key to understanding organisms and driving advances in drug discovery, requiring interdisciplinary knowledge across chemistry and biology. Although large molecular language models have achieved notable success in task transfer, they often struggle to accurately analyze molecular features due to limited knowledge and reasoning capabilities. To address this issue, we present Mol-LLaMA, a large molecular language model that grasps the general knowledge centered on molecules and exhibits explainability and reasoning ability. To this end, we design key data types that encompass the fundamental molecular features, taking into account the essential abilities for molecular reasoning. Further, to improve molecular understanding, we propose a module that integrates complementary information from different molecular encoders, leveraging the distinct advantages of molecular representations. Our experimental results demonstrate that Mol-LLaMA is capable of comprehending the general features of molecules and providing informative responses, implying its potential as a general-purpose assistant for molecular analysis. Our project page is at https://mol-llama.github.io/.",
      "arxiv_url": "https://openreview.net/forum?id=NOUF43YrIL",
      "pdf_url": "https://openreview.net/pdf/b1529dd3a9bbd1ca574802667ac4424a1f058f53.pdf",
      "primary_category": "Large Molecular Language Model, General-purpose Assistant for Molecular Analysis, Molecular Comprehension",
      "categories": [
        "Large Molecular Language Model",
        "General-purpose Assistant for Molecular Analysis",
        "Molecular Comprehension"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VrXjAfdwrN",
      "title": "Elastic Robust Unlearning of Specific Knowledge in Large Language Models",
      "authors": [
        "Yize Sui",
        "Jing Ren",
        "Wenjing Yang",
        "Ruochun Jin",
        "Liyang Xu",
        "Xiyao Liu",
        "Ji Wang"
      ],
      "abstract": "LLM unlearning aims to remove sensitive or harmful information within the model, thus reducing the potential risk of generating unexpected information. However, existing Preference Optimization (PO)-based unlearning methods suffer two limitations. First, their rigid reward setting limits the effect of unlearning. Second, the lack of robustness causes unlearned information to reappear. To remedy these two weaknesses, we present a novel LLM unlearning optimization framework, namely Elastic Robust Unlearning (ERU), to efficiently and robustly remove specific knowledge from LLMs. We design the elastic reward setting instead of the rigid reward setting to enhance the unlearning performance. Meanwhile, we incorporate the refusal feature ablation into the unlearning process to trigger specific failure patterns for efficiently enhancing the robustness of the PO-based unlearning methods in multiple scenarios. Experimental results show that ERU can improve the unlearning effectiveness significantly while maintaining a high utility performance. Especially, on the WMDP-Bio benchmark, ERU shows a 9\\% improvement over the second-best method, and maintains 83\\% performance even under 1,000 sample fine-tuned retraining attacks, significantly better than the baseline method.",
      "arxiv_url": "https://openreview.net/forum?id=VrXjAfdwrN",
      "pdf_url": "https://openreview.net/pdf/cb43f1f9f7df2aac41b017d7bd81eb5226fe7d60.pdf",
      "primary_category": "LLM Unlearning; Preference Optimization; Unlearning Robustness",
      "categories": [
        "LLM Unlearning; Preference Optimization; Unlearning Robustness"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "x1wZoyS0rC",
      "title": "SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent",
      "authors": [
        "Yandan Yang",
        "Baoxiong Jia",
        "Shujie Zhang",
        "Siyuan Huang"
      ],
      "abstract": "Indoor scene synthesis has become increasingly important with the rise of Embodied AI, which requires 3D environments that are not only visually realistic but also physically plausible and functionally diverse. While recent approaches have advanced visual fidelity, they often remain constrained to fixed scene categories, lack sufficient object-level detail and physical consistency, and struggle to align with complex user instructions. In this work, we present SceneWeaver, a reflective agentic framework that unifies diverse scene synthesis paradigms through tool-based iterative refinement. At its core, SceneWeaver employs a language model-based planner to select from a suite of extensible scene generation tools, ranging from data-driven generative models to visual- and LLM-based methods, guided by self-evaluation of physical plausibility, visual realism, and semantic alignment with user input. This closed-loop reason-act-reflect design enables the agent to identify semantic inconsistencies, invoke targeted tools, and update the environment over successive iterations. Extensive experiments on both common and open-vocabulary room types demonstrate that \\model not only outperforms prior methods on physical, visual, and semantic metrics, but also generalizes effectively to complex scenes with diverse instructions, marking a step toward general-purpose 3D environment generation.",
      "arxiv_url": "https://openreview.net/forum?id=x1wZoyS0rC",
      "pdf_url": "https://openreview.net/pdf/cc7e3c42e7880c5e04ae7e7bc988371ea51d7ee1.pdf",
      "primary_category": "3D Scene Synthesis, Reflective Agents",
      "categories": [
        "3D Scene Synthesis",
        "Reflective Agents"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zEj1FSYCRn",
      "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization",
      "authors": [
        "Dmitriy Shopkhoev",
        "Ammar Ali",
        "Magauiya Zhussip",
        "Valentin Malykh",
        "Stamatios Lefkimmiatis",
        "Nikos Komodakis",
        "Sergey Zagoruyko"
      ],
      "abstract": "We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation, which approximates the pruned blocks. The estimated linear mapping can be seam- lessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model’s performance on open benchmarks—without any training or healing steps, resulting in minimal computational overhead. We provide an open- source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe.",
      "arxiv_url": "https://openreview.net/forum?id=zEj1FSYCRn",
      "pdf_url": "https://openreview.net/pdf/334128876882158c871dd4bf17ccf8f6555a3cfd.pdf",
      "primary_category": "Pruning, Depth Pruning, Transformers",
      "categories": [
        "Pruning",
        "Depth Pruning",
        "Transformers",
        "LLMs",
        "Training-free"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "fxDCgOruk0",
      "title": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning",
      "authors": [
        "Azim Ospanov",
        "Farzan Farnia",
        "Roozbeh Yousefzadeh"
      ],
      "abstract": "Formal reasoning and automated theorem proving constitute a challenging subfield of machine learning, in which machines are tasked with proving mathematical theorems using formal languages like Lean. A formal verification system can check whether a formal proof is correct or not almost instantaneously, but generating a completely correct formal proof with large language models (LLMs) remains a formidable task. The usual approach in the literature is to prompt the LLM many times (up to several thousands) until one of the generated proofs passes the verification system. In this work, we present APOLLO (**A**utomated **P**r**O**of repair via**L**LM and **L**ean c**O**llaboration), a modular, model‑agnostic agentic framework that combines the strengths of the Lean compiler with an LLM’s reasoning abilities to achieve better proof‐generation results at a low token and sampling budgets. _Apollo_ directs a fully automated process in which the LLM generates proofs for theorems, a set of agents analyze the proofs, fix the syntax errors, identify the mistakes in the proofs using Lean, isolate failing sub‑lemmas, utilize automated solvers, and invoke an LLM on each remaining goal with a low top‑K budget. The repaired sub‑proofs are recombined and reverified, iterating up to a user‑controlled maximum number of attempts. On the miniF2F benchmark, we establish a new state‑of‑the‑art accuracy of 84.9% among sub 8B‑parameter models (as of August 2025) while keeping the sampling budget below one hundred. Moreover, _Apollo_ raises the state‑of‑the‑art accuracy for Goedel‑Prover‑SFT to 65.6% while cutting sample complexity from 25,600 to a few hundred. General‑purpose models (o3‑mini, o4‑mini) jump from 3–7% to over 40% accuracy. Our results demonstrate that targeted, compiler‑guided repair of LLM outputs yields dramatic gains in both efficiency and correctness, suggesting a general paradigm for scalable automated theorem proving. The codebase is available at https://github.com/aziksh-ospanov/APOLLO",
      "arxiv_url": "https://openreview.net/forum?id=fxDCgOruk0",
      "pdf_url": "https://openreview.net/pdf/2c1fb9b33da2255e1e4b13f81f4c8ef3eb580235.pdf",
      "primary_category": "formal reasoning, automated theorem proving, lean prover",
      "categories": [
        "formal reasoning",
        "automated theorem proving",
        "lean prover",
        "ai for math"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cECo8tetzF",
      "title": "Restoring Pruned Large Language Models via Lost Component Compensation",
      "authors": [
        "Zijian Feng",
        "Hanzhang Zhou",
        "Zixiao Zhu",
        "Tianjiao Li",
        "Chua Jia Jim Deryl",
        "Mak Lee Onn",
        "Gee Wah Ng",
        "Kezhi Mao"
      ],
      "abstract": "Pruning is a widely used technique to reduce the size and inference cost of large language models (LLMs), but it often causes performance degradation. To mitigate this, existing restoration methods typically employ parameter-efficient fine-tuning (PEFT), such as LoRA, to recover the pruned model's performance. However, most PEFT methods are designed for dense models and overlook the distinct properties of pruned models, often resulting in suboptimal recovery. In this work, we propose a targeted restoration strategy for pruned models that restores performance while preserving their low cost and high efficiency. We observe that pruning-induced information loss is reflected in attention activations, and selectively reintroducing components of this information can significantly recover model performance. Based on this insight, we introduce RestoreLCC (Restoring Pruned LLMs via Lost Component Compensation), a plug-and-play method that contrastively probes critical attention heads via activation editing, extracts lost components from activation differences, and finally injects them back into the corresponding pruned heads for compensation and recovery. RestoreLCC is compatible with structured, semi-structured, and unstructured pruning schemes. Extensive experiments demonstrate that RestoreLCC consistently outperforms state-of-the-art baselines in both general and task-specific performance recovery, without compromising the sparsity or inference efficiency of pruned models.",
      "arxiv_url": "https://openreview.net/forum?id=cECo8tetzF",
      "pdf_url": "https://openreview.net/pdf/1d315915657111b82a395d0523efc4d6ef6f4561.pdf",
      "primary_category": "large language models, pruning, performance restoration",
      "categories": [
        "large language models",
        "pruning",
        "performance restoration"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "OEawM2coNT",
      "title": "Partition to Evolve: Niching-enhanced Evolution with LLMs for Automated Algorithm Discovery",
      "authors": [
        "Qinglong Hu",
        "Qingfu Zhang"
      ],
      "abstract": "Large language model-assisted Evolutionary Search (LES) has emerged as a promising approach for Automated Algorithm Discovery (AAD). While many evolutionary search strategies have been developed for classic optimization problems, LES operates in abstract language spaces, presenting unique challenges for applying these strategies effectively. To address this, we propose a general LES framework that incorporates feature-assisted niche construction within abstract search spaces, enabling the seamless integration of niche-based search strategies from evolutionary computation. Building on this framework, we introduce PartEvo, an LES method that combines niche collaborative search and advanced prompting strategies to improve algorithm discovery efficiency. Experiments on both synthetic and real-world optimization problems show that PartEvo outperforms human-designed baselines and surpasses prior LES methods, such as Eoh and Funsearch. In particular, on resource scheduling tasks, PartEvo generates meta-heuristics with low design costs, achieving up to 90.1\\% performance improvement over widely-used baseline algorithms, highlighting its potential for real-world applications.",
      "arxiv_url": "https://openreview.net/forum?id=OEawM2coNT",
      "pdf_url": "https://openreview.net/pdf/802315d831dcf073b4c1b582c7b20d4cffee25c0.pdf",
      "primary_category": "Automated algorithm discovery, Evolutionary computation, Large language model",
      "categories": [
        "Automated algorithm discovery",
        "Evolutionary computation",
        "Large language model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6zfILEmD3u",
      "title": "CoP: Agentic Red-teaming for Large Language Models using Composition of Principles",
      "authors": [
        "Chen Xiong",
        "Pin-Yu Chen",
        "Tsung-Yi Ho"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have spurred transformative applications in various domains, ranging from open-source to proprietary LLMs. However, jailbreak attacks, which aim to break safety alignment and user compliance by tricking the target LLMs into answering harmful and risky responses, are becoming an urgent concern. The practice of red-teaming for LLMs is to proactively explore potential risks and error-prone instances before the release of frontier AI technology. This paper proposes an agentic workflow to automate and scale the red-teaming process of LLMs through the Composition-of-Principles (CoP) framework, where human users provide a set of red-teaming principles as instructions to an AI agent to automatically orchestrate effective red-teaming strategies and generate jailbreak prompts. Distinct from existing red-teaming methods, our CoP framework provides a unified and extensible framework to encompass and orchestrate human-provided red-teaming principles to enable the automated discovery of new red-teaming strategies. When tested against leading LLMs, CoP reveals unprecedented safety risks by finding novel jailbreak prompts and improving the best-known single-turn attack success rate by up to 19.0 times.",
      "arxiv_url": "https://openreview.net/forum?id=6zfILEmD3u",
      "pdf_url": "https://openreview.net/pdf/86df57f95e98062adf3f729bd820d2fbbe56a4ef.pdf",
      "primary_category": "Large Language Model, Jailbreak Attack, LLM Agent",
      "categories": [
        "Large Language Model",
        "Jailbreak Attack",
        "LLM Agent"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "EDRvVEqjau",
      "title": "CARE: Decoding-Time Safety Alignment via Rollback and Introspection Intervention",
      "authors": [
        "Xiaomeng Hu",
        "Fei Huang",
        "Chenhan Yuan",
        "Junyang Lin",
        "Tsung-Yi Ho"
      ],
      "abstract": "As large language models (LLMs) are increasingly deployed in real-world applications, ensuring the safety of their outputs during decoding has become a critical challenge. However, existing decoding-time interventions, such as Contrastive Decoding, often force a severe trade-off between safety and response quality. In this work, we propose **CARE**, a novel framework for decoding-time safety alignment that integrates three key components: (1) a guard model for real-time safety monitoring, enabling detection of potentially unsafe content; (2) a rollback mechanism with a token buffer to correct unsafe outputs efficiently at an earlier stage without disrupting the user experience; and (3) a novel introspection-based intervention strategy, where the model generates self-reflective critiques of its previous outputs and incorporates these reflections into the context to guide subsequent decoding steps. The framework achieves a superior safety-quality trade-off by using its guard model for precise interventions, its rollback mechanism for timely corrections, and our novel introspection method for effective self-correction. Experimental results demonstrate that our framework achieves a superior balance of safety, quality, and efficiency, attaining a **low harmful response rate** and **minimal disruption to the user experience** while **maintaining high response quality**.",
      "arxiv_url": "https://openreview.net/forum?id=EDRvVEqjau",
      "pdf_url": "https://openreview.net/pdf/1bddaa6ce2b57d5be31ece3afdcb510bd3c5c00a.pdf",
      "primary_category": "Language Model, AI Safety, Decoding-time intervention",
      "categories": [
        "Language Model",
        "AI Safety",
        "Decoding-time intervention"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Y0LHrY3x1S",
      "title": "T2V-OptJail: Discrete Prompt Optimization for Text-to-Video Jailbreak Attacks",
      "authors": [
        "Jiayang Liu",
        "Siyuan Liang",
        "Shiqian Zhao",
        "Rong-Cheng Tu",
        "Wenbo Zhou",
        "Aishan Liu",
        "Dacheng Tao",
        "Siew Kei Lam"
      ],
      "abstract": "In recent years, fueled by the rapid advancement of diffusion models, text-to-video (T2V) generation models have achieved remarkable progress, with notable examples including Pika, Luma, Kling, and Open-Sora. Although these models exhibit impressive generative capabilities, they also expose significant security risks due to their vulnerability to jailbreak attacks, where the models are manipulated to produce unsafe content such as pornography, violence, or discrimination. Existing works such as T2VSafetyBench provide preliminary benchmarks for safety evaluation, but lack systematic methods for thoroughly exploring model vulnerabilities.\nTo address this gap, we are the first to formalize the T2V jailbreak attack as a discrete optimization problem and propose a joint objective-based optimization framework, called \\emph{T2V-OptJail}. This framework consists of two key optimization goals: bypassing the built-in safety filtering mechanisms to increase the attack success rate, preserving semantic consistency between the adversarial prompt and the unsafe input prompt, as well as between the generated video and the unsafe input prompt, to enhance content controllability. In addition, we introduce an iterative optimization strategy guided by prompt variants, where multiple semantically equivalent candidates are generated in each round, and their scores are aggregated to robustly guide the search toward optimal adversarial prompts.\nWe conduct large-scale experiments on several T2V models, covering both open-source models (\\textit{e.g.}, Open-Sora) and real commercial closed-source models (\\textit{e.g.}, Pika, Luma, Kling). The experimental results show that the proposed method improves 11.4\\% and 10.0\\% over the existing state-of-the-art method (SoTA) in terms of attack success rate assessed by GPT-4, attack success rate assessed by human accessors, respectively, verifying the significant advantages of the method in terms of attack effectiveness and content control. This study reveals the potential abuse risk of the semantic alignment mechanism in the current T2V model and provides a basis for the design of subsequent jailbreak defense methods.",
      "arxiv_url": "https://openreview.net/forum?id=Y0LHrY3x1S",
      "pdf_url": "https://openreview.net/pdf/9a38a74675ae3d4457bfe9a09fafd530a30b0f14.pdf",
      "primary_category": "jailbreak; text-to-video model; safety filter",
      "categories": [
        "jailbreak; text-to-video model; safety filter"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "REIo9ZLSYo",
      "title": "Brain-Inspired fMRI-to-Text Decoding via Incremental and Wrap-Up Language Modeling",
      "authors": [
        "Wentao Lu",
        "Dong Nie",
        "Pengcheng Xue",
        "Zheng Cui",
        "Piji Li",
        "Daoqiang Zhang",
        "Xuyun Wen"
      ],
      "abstract": "Decoding natural language text from non-invasive brain signals, such as functional magnetic resonance imaging (fMRI), remains a central challenge in brain-computer interface research. While recent advances in large language models (LLMs) have enabled open-vocabulary fMRI-to-text decoding, existing frameworks typically process the entire fMRI sequence in a single step, leading to performance degradation when handling long input sequences due to memory overload and semantic drift. To address this limitation, we propose a brain-inspired sequential fMRI-to-text decoding framework that mimics the human cognitive strategy of segmented and inductive language processing. Specifically, we divide long fMRI time series into consecutive segments aligned with optimal language comprehension length. Each segment is decoded incrementally, followed by a wrap-up mechanism that summarizes the semantic content and incorporates it as prior knowledge into subsequent decoding steps. This sequence-wise approach alleviates memory burden and ensures semantic continuity across segments. In addition, we introduce a text-guided masking strategy integrated with a masked autoencoder (MAE) framework for fMRI representation learning. This method leverages attention distributions over key semantic tokens to selectively mask the corresponding fMRI time points, and employs MAE to guide the model toward focusing on neural activity at semantically salient moments, thereby enhancing the capability of fMRI embeddings to represent textual information. Experimental results on the two datasets demonstrate that our method significantly outperforms state-of-the-art approaches, with performance gains increasing as decoding length grows.",
      "arxiv_url": "https://openreview.net/forum?id=REIo9ZLSYo",
      "pdf_url": "https://openreview.net/pdf/fb0bca5b862550ac49d02f36f7bf49c6221680ae.pdf",
      "primary_category": "fMRI-to-text, brain decoding, sequential decoding",
      "categories": [
        "fMRI-to-text",
        "brain decoding",
        "sequential decoding",
        "text-guided mask"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XIqlxqNDCL",
      "title": "Who Reasons in the Large Language Models?",
      "authors": [
        "Jie Shao",
        "Jianxin Wu"
      ],
      "abstract": "Despite the impressive performance of large language models (LLMs), the process of endowing them with new capabilities---such as mathematical reasoning---remains largely empirical and opaque. A critical open question is whether reasoning abilities stem from the entire model, specific modules, or are merely artifacts of overfitting. In this work, we hypothesize that the reasoning capabilities in well-trained LLMs are primarily attributed to the output projection module (o_proj) in the Transformer’s multi-head self-attention (MHSA) module. To support this hypothesis, we introduce Stethoscope for Networks (SfN), a suite of diagnostic tools designed to probe and analyze the internal behaviors of LLMs. Using SfN, we provide both circumstantial and empirical evidence suggesting that o_proj plays a central role in enabling reasoning, whereas other modules contribute more to fluent dialogue. These findings offer a new perspective on LLM interpretability and open avenues for more targeted training strategies, potentially enabling more efficient and specialized LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=XIqlxqNDCL",
      "pdf_url": "https://openreview.net/pdf/75ddd20e90deb59d29d66780d429e4a77c1879f6.pdf",
      "primary_category": "LLMs, reasoning, interpretability",
      "categories": [
        "LLMs",
        "reasoning",
        "interpretability",
        "Transformer"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BcKYVmh3yH",
      "title": "Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding",
      "authors": [
        "Yiming Wang",
        "Pei Zhang",
        "Siyuan Huang",
        "Baosong Yang",
        "Zhuosheng Zhang",
        "Fei Huang",
        "Rui Wang"
      ],
      "abstract": "Test-time scaling enhances large language model performance by allocating additional compute resources during decoding. Best-of-$N$ (BoN) sampling serves as a common sampling-based scaling technique, broadening the search space in parallel to find better solutions from the model distribution. However, its cost–performance trade-off is still underexplored. Two main challenges limit the efficiency of BoN sampling:\n(1) Generating $N$ full samples consumes substantial GPU memory, reducing inference capacity under limited resources.\n(2) Reward models add extra memory and latency overhead, and training strong reward models introduces potential training data costs.\nAlthough some studies have explored efficiency improvements, none have addressed both challenges at once.\nTo address this gap, we propose **Self-Truncation Best-of-$N$ (ST-BoN)**, a decoding method that avoids fully generating all $N$ samples and eliminates the need for reward models. It leverages early sampling consistency in the model’s internal states to identify the most promising path and truncate suboptimal ones.\nIn terms of cost, ST-BoN reduces dynamic GPU memory usage by over 80% and inference latency by 50%.\nIn terms of cost–performance trade-off, ST-BoN achieves the same performance as Full-BoN while saving computational cost by 70%–80%, and under the same cost, it can improve accuracy by 3–4 points.",
      "arxiv_url": "https://openreview.net/forum?id=BcKYVmh3yH",
      "pdf_url": "https://openreview.net/pdf/a95df403a5a9fd31e6e3524083ea4c3f8facd1c0.pdf",
      "primary_category": "Large Language Models, Test-Time Scaling, Best-of-N Sampling",
      "categories": [
        "Large Language Models",
        "Test-Time Scaling",
        "Best-of-N Sampling",
        "Efficient Decoding",
        "Self-Estimation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "V1FlwrsseI",
      "title": "ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking",
      "authors": [
        "Lequan Lin",
        "Dai Shi",
        "Andi Han",
        "Feng Chen",
        "Qiuzheng Chen",
        "Jiawen Li",
        "Zhaoyang Li",
        "Jiyuan Zhang",
        "Zhenbang Sun",
        "Junbin Gao"
      ],
      "abstract": "Supervised learning relies on high-quality labeled data, but obtaining such data through human annotation is both expensive and time-consuming. Recent work explores using large language models (LLMs) for annotation, but LLM-generated labels still fall short of human-level quality. To address this problem, we propose the Annotation with Critical Thinking (ACT) data pipeline, where LLMs serve not only as annotators but also as judges to critically identify potential errors.  Human effort is then directed towards reviewing only the most \"suspicious\" cases, significantly improving the human annotation efficiency. Our major contributions are as follows: (1) ACT is applicable to a wide range of domains, including natural language processing (NLP), computer vision (CV), and multimodal understanding, by leveraging multimodal-LLMs (MLLMs). (2) Through empirical studies, we derive 7 insights on how to enhance annotation quality while efficiently reducing the human cost, and then translate these findings into user-friendly guidelines. (3) We theoretically analyze how to modify the loss function so that models trained on ACT data achieve similar performance to those trained on fully human-annotated data. Our experiments show that the performance gap can be reduced to less than 2% on most benchmark datasets while saving up to 90% of human costs.",
      "arxiv_url": "https://openreview.net/forum?id=V1FlwrsseI",
      "pdf_url": "https://openreview.net/pdf/2d4818decec8bd80bc1421d0101a796c541e0194.pdf",
      "primary_category": "data annotation, data pipeline, multimodal large language models",
      "categories": [
        "data annotation",
        "data pipeline",
        "multimodal large language models",
        "llm-as-a-judge",
        "supervised learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IIgVYnadfR",
      "title": "BNMusic: Blending Environmental Noises into Personalized Music",
      "authors": [
        "Chi Zuo",
        "Martin B. Møller",
        "Pablo Martínez-Nuevo",
        "Huayang Huang",
        "Yu Wu",
        "Ye Zhu"
      ],
      "abstract": "While being disturbed by environmental noises, the acoustic masking technique is a conventional way to reduce the annoyance in audio engineering that seeks to cover up the noises with other dominant yet less intrusive sounds. However, misalignment between the dominant sound and the noise—such as mismatched downbeats—often requires an excessive volume increase to achieve effective masking. Motivated by recent advances in cross-modal generation, in this work, we introduce an alternative method to acoustic masking, aiming to reduce the noticeability of environmental noises by blending them into personalized music generated based on user-provided text prompts. Following the paradigm of music generation using mel-spectrogram representations, we propose a Blending Noises into Personalized Music (BNMusic) framework with two key stages. The first stage synthesizes a complete piece of music in a mel-spectrogram representation that encapsulates the musical essence of the noise. In the second stage, we adaptively amplifying the generated music segment to further reduce noise perception and enhance the blending effectiveness, while preserving auditory quality. Our experiments with comprehensive evaluations on MusicBench, EPIC-SOUNDS, and ESC-50 demonstrate the effectiveness of our framework, highlighting the ability to blend environmental noise with rhythmically aligned, adaptively amplified, and enjoyable music segments, minimizing the noticeability of the noise, thereby improving overall acoustic experiences.  Project page: https://d-fas.github.io/BNMusic_page/.",
      "arxiv_url": "https://openreview.net/forum?id=IIgVYnadfR",
      "pdf_url": "https://openreview.net/pdf/8c8749c5c01fd9ba9dbf740cb26390e604b5d44f.pdf",
      "primary_category": "Music generation, Image inpainting",
      "categories": [
        "Music generation",
        "Image inpainting"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "obwRcksFZw",
      "title": "PoE-World: Compositional World Modeling with Products of Programmatic Experts",
      "authors": [
        "Wasu Top Piriyakulkij",
        "Yichao Liang",
        "Hao Tang",
        "Adrian Weller",
        "Marta Kryven",
        "Kevin Ellis"
      ],
      "abstract": "Learning how the world works is central to building AI agents that can adapt to complex environments. \nTraditional world models based on deep-learning demand vast amounts of training data, and do not flexibly update their knowledge from sparse observations. \nRecent advances in program synthesis using Large Language Models (LLMs) give an alternate approach which learns world models represented as source code, supporting strong generalization from little data. \nTo date, application of program-structured world models remains limited to natural language and grid-world domains. We introduce a novel program synthesis method for effectively modeling complex, non-gridworld domains by representing a world model as an exponentially-weighted product of programmatic experts (PoE-World) synthesized by LLMs.\nWe show that this approach can learn complex, stochastic world models from just a few observations. \nWe evaluate the learned world models by embedding them in a model-based planning agent, demonstrating efficient performance and generalization to unseen levels on Atari's Pong and Montezuma's Revenge.",
      "arxiv_url": "https://openreview.net/forum?id=obwRcksFZw",
      "pdf_url": "https://openreview.net/pdf/1983f2216c20adc421975e0092eb41f2ac1d93fa.pdf",
      "primary_category": "program synthesis, compositionality, world modeling",
      "categories": [
        "program synthesis",
        "compositionality",
        "world modeling"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QoiFdfZUJv",
      "title": "CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric Reward",
      "authors": [
        "Yandong Guan",
        "Xilin Wang",
        "XiMing Xing",
        "Jing Zhang",
        "Dong Xu",
        "Qian Yu"
      ],
      "abstract": "In this work, we introduce CAD-Coder, a novel framework that reformulates text-to-CAD as the generation of CadQuery scripts—a Python-based, parametric CAD language.\nThis representation enables direct geometric validation, a richer modeling vocabulary, and seamless integration with existing LLMs. \nTo further enhance code validity and geometric fidelity, we propose a two-stage learning pipeline: (1) supervised fine-tuning on paired text–CadQuery data, and (2) reinforcement learning with Group Reward Policy Optimization (GRPO), guided by a CAD-specific reward comprising both a geometric reward (Chamfer Distance) and a format reward.\nWe also introduce a chain-of-thought (CoT) planning process to improve model reasoning, and construct a large-scale, high-quality dataset of 110K text–CadQuery–3D model triplets and 1.5K CoT samples via an automated pipeline. Extensive experiments demonstrate that CAD-Coder enables LLMs to generate diverse, valid, and complex CAD models directly from natural language, advancing the state of the art of text-to-CAD generation and geometric reasoning.",
      "arxiv_url": "https://openreview.net/forum?id=QoiFdfZUJv",
      "pdf_url": "https://openreview.net/pdf/865ac90ccb14f604bbbbd02cde07c7c2048f7826.pdf",
      "primary_category": "CAD generation, text-to-CAD, LLM",
      "categories": [
        "CAD generation",
        "text-to-CAD",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "NQSWkmjODD",
      "title": "Learning to Instruct for Visual Instruction Tuning",
      "authors": [
        "Zhihan Zhou",
        "Feng Hong",
        "Jiaan Luo",
        "Yushi Ye",
        "Jiangchao Yao",
        "Dongsheng Li",
        "Bo Han",
        "Ya Zhang",
        "Yanfeng Wang"
      ],
      "abstract": "We propose L2T, an advancement of visual instruction tuning (VIT). While VIT equips Multimodal LLMs (MLLMs) with promising multimodal capabilities, the current design choices for VIT often result in overfitting and shortcut learning, potentially degrading performance. This gap arises from an overemphasis on instruction-following abilities, while neglecting the proactive understanding of visual information. Inspired by this, L2T adopts a simple yet effective approach by incorporating the loss function into both the instruction and response sequences. It seamlessly expands the training data, and regularizes the MLLMs from overly relying on language priors. Based on this merit, L2T achieves a significant relative improvement of up to 9% on comprehensive multimodal benchmarks, requiring no additional training data and incurring negligible computational overhead. Surprisingly, L2T attains exceptional fundamental visual capabilities, yielding up to an 18% improvement in captioning performance, while simultaneously alleviating hallucination in MLLMs. Github code: https://github.com/Feng-Hong/L2T.",
      "arxiv_url": "https://openreview.net/forum?id=NQSWkmjODD",
      "pdf_url": "https://openreview.net/pdf/daad40c54e52bacd2d91f44d07e014ed4726ff5b.pdf",
      "primary_category": "Visual Instruction Tuning, Multimodal Large Language Models",
      "categories": [
        "Visual Instruction Tuning",
        "Multimodal Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "h0LzGQq6uO",
      "title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering",
      "authors": [
        "Yuyang Hong",
        "Jiaqi Gu",
        "Qi Yang",
        "Lubin Fan",
        "Yue Wu",
        "Ying Wang",
        "Kun Ding",
        "Shiming Xiang",
        "Jieping Ye"
      ],
      "abstract": "The task of Knowlegde-Based Visual Question Answering (KB-VQA) requires the model to understand visual features and retrieve external knowledge. Retrieval-Augmented Generation (RAG) have been employed to address this problem through knowledge base querying. However, existing work demonstrate two limitations: insufficient interactivity during knowledge retrieval and ineffective organization of retrieved information for Visual-Language Model (VLM). To address these challenges, we propose a three-stage visual language model with Process, Retrieve and Filter (VLM-PRF) framework. For interactive retrieval, VLM-PRF uses reinforcement learning (RL) to guide the model to strategically process information via tool-driven operations. For knowledge filtering, our method trains the VLM to transform the raw retrieved information into into task-specific knowledge. With a dual reward as supervisory signals, VLM-PRF successfully enable model to optimize retrieval strategies and answer generation capabilities simultaneously. Experiments on two datasets demonstrate the effectiveness of our framework.",
      "arxiv_url": "https://openreview.net/forum?id=h0LzGQq6uO",
      "pdf_url": "https://openreview.net/pdf/310d8cb95b82ccbaca4f784d83ea9ef583fcc44f.pdf",
      "primary_category": "Knowledge-based Visual Question Answering; Retrieval-Augmented Generation",
      "categories": [
        "Knowledge-based Visual Question Answering; Retrieval-Augmented Generation"
      ],
      "tags": [
        "RAG",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "TdmzrkdLG0",
      "title": "LLM Interpretability with Identifiable Temporal-Instantaneous Representation",
      "authors": [
        "Xiangchen Song",
        "Jiaqi Sun",
        "Zijian Li",
        "Yujia Zheng",
        "Kun Zhang"
      ],
      "abstract": "Despite Large Language Models' remarkable capabilities, understanding their internal representations remains challenging. Mechanistic interpretability tools such as sparse autoencoders (SAEs) were developed to extract interpretable features from LLMs but lack temporal dependency modeling, instantaneous relation representation, and more importantly theoretical guarantees—undermining both the theoretical foundations and the practical confidence necessary for subsequent analyses. While causal representation learning (CRL) offers theoretically-grounded approaches for uncovering latent concepts, existing methods cannot scale to LLMs' rich conceptual space due to inefficient computation. To bridge the gap, we introduce an identifiable temporal causal representation learning framework specifically designed for LLMs' high-dimensional concept space, capturing both time-delayed and instantaneous causal relations. Our approach provides theoretical guarantees and demonstrates efficacy on synthetic datasets scaled to match real-world complexity. By extending SAE techniques with our temporal causal framework, we successfully discover meaningful concept relationships in LLM activations. Our findings show that modeling both temporal and instantaneous conceptual relationships advances the interpretability of LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=TdmzrkdLG0",
      "pdf_url": "https://openreview.net/pdf/19b3c0b3efa39ffcc7f96d9630b2030a81fbb8fc.pdf",
      "primary_category": "identifiability, causal representation learning, large language models",
      "categories": [
        "identifiability",
        "causal representation learning",
        "large language models",
        "interpretability"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tM5mjMfFmS",
      "title": "DuSA: Fast and Accurate Dual-Stage Sparse Attention Mechanism Accelerating Both Training and Inference",
      "authors": [
        "Chong Wu",
        "Jiawang Cao",
        "Renjie Xu",
        "Zhuoheng Ran",
        "Maolin Che",
        "Wenbo Zhu",
        "Hong Yan"
      ],
      "abstract": "This paper proposes the Dual-Stage Sparse Attention (DuSA) mechanism for attention acceleration of transformers. In the first stage, DuSA performs intrablock sparse attention to aggregate local inductive biases. In the second stage, DuSA performs interblock sparse attention to obtain long-range dependencies. Both stages have low computational complexity and can be further accelerated by memory acceleration attention mechanisms directly, which makes DuSA faster than some extremely fast attention mechanisms. The dual-stage sparse attention design provides a lower error in approximating vanilla scaled-dot product attention than the basic single-stage sparse attention mechanisms and further advances the basic sparse attention mechanisms to match or even outperform vanilla scaled-dot product attention. Even in some plug and play situations, DuSA can still maintain low performance loss. DuSA can be used in both training and inference acceleration. DuSA achieves leading performance in different benchmarks: long range arena, image classification, semantic segmentation, object detection, text to video generation, and long context understanding, and accelerates models of different sizes.",
      "arxiv_url": "https://openreview.net/forum?id=tM5mjMfFmS",
      "pdf_url": "https://openreview.net/pdf/33622b017fb3d44eb06332dc5c4c665112056e82.pdf",
      "primary_category": "Efficient Attention Mechanism, Sparse Attention Mechanism, Transformer",
      "categories": [
        "Efficient Attention Mechanism",
        "Sparse Attention Mechanism",
        "Transformer"
      ],
      "tags": [
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0yowBBK6tT",
      "title": "GeoCAD: Local Geometry-Controllable CAD Generation with Large Language Models",
      "authors": [
        "Zhanwei Zhang",
        "kaiyuan liu",
        "Junjie Liu",
        "Wenxiao Wang",
        "Binbin Lin",
        "Liang Xie",
        "Chen Shen",
        "Deng Cai"
      ],
      "abstract": "Local geometry-controllable computer-aided design (CAD) generation aims to modify local parts of CAD models automatically, enhancing design efficiency. \nIt also ensures that the shapes of newly generated local parts follow user-specific geometric instructions (e.g., an isosceles right triangle or a rectangle with one corner cut off).\nHowever, existing methods encounter challenges in achieving this goal.\nSpecifically, they either lack the ability to follow textual instructions or are unable to focus on the local parts.\nTo address this limitation, we introduce GeoCAD, a user-friendly and local geometry-controllable CAD generation method. \nSpecifically, we first propose a complementary captioning strategy to generate geometric instructions for local parts.\nThis strategy involves vertex-based and VLLM-based captioning for systematically annotating simple and complex parts, respectively.\nIn this way, we caption $\\sim$221k different local parts in total.\nIn the training stage, given a CAD model, we randomly mask a local part.\nThen, using its geometric instruction and the remaining parts as input, we prompt large language models (LLMs) to predict the masked part.\nDuring inference, users can specify any local part for modification while adhering to a variety of predefined geometric instructions.\nExtensive experiments demonstrate the effectiveness of GeoCAD in generation quality, validity and text-to-CAD consistency.",
      "arxiv_url": "https://openreview.net/forum?id=0yowBBK6tT",
      "pdf_url": "https://openreview.net/pdf/f3bd138535eef80977c5479e2c9225fe6f98f985.pdf",
      "primary_category": "CAD generation, large language model",
      "categories": [
        "CAD generation",
        "large language model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "hzBqQZK2iV",
      "title": "Uni-LoRA: One Vector is All You Need",
      "authors": [
        "Kaiyang Li",
        "Shaobo Han",
        "Qing Su",
        "Wei Li",
        "Zhipeng Cai",
        "Shihao Ji"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient fine-tuning (PEFT) method for large language models (LLMs) by constraining weight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and VB-LoRA push efficiency further by introducing additional constraints to reduce the trainable parameter space. In this paper, we show that the parameter space reduction strategies employed by these LoRA variants can be formulated within a unified framework, Uni-LoRA, where the LoRA parameter space, flattened as a high-dimensional vector space R^D, can be reconstructed through a projection from a subspace R^d, with d << D. We demonstrate that the fundamental difference among various LoRA methods lies in the choice of the projection matrix, P ∈ R^{D×d}. \nMost existing LoRA variants rely on layer-wise or structure-specific projections that limit cross-layer parameter sharing, thereby compromising parameter efficiency. In light of this, we introduce an efficient and theoretically grounded projection matrix that is isometric, enabling global parameter sharing and reducing computation overhead. Furthermore, under the unified view of Uni-LoRA, this design requires only a single trainable vector to reconstruct LoRA parameters for the entire LLM -- making Uni-LoRA both a unified framework and a “one-vector-only” solution. Extensive experiments on GLUE, mathematical reasoning, and instruction tuning benchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter efficiency while outperforming or matching prior approaches in predictive performance.",
      "arxiv_url": "https://openreview.net/forum?id=hzBqQZK2iV",
      "pdf_url": "https://openreview.net/pdf/d6394e5a1af214d98d759f8e5a95139a93fbbde9.pdf",
      "primary_category": "Extreme parameter-efficient fine-tuning, unified framework, intrinsic dimension",
      "categories": [
        "Extreme parameter-efficient fine-tuning",
        "unified framework",
        "intrinsic dimension"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "dSznm3mQTD",
      "title": "LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs",
      "authors": [
        "Ran Li",
        "Hao Wang",
        "Chengzhi Mao"
      ],
      "abstract": "Efficient red-teaming method to uncover vulnerabilities in Large Language Models (LLMs) is crucial. While recent attacks often use LLMs as optimizers, the discrete language space make gradient-based methods struggle. We introduce LARGO (Latent Adversarial Reflection through Gradient Optimization), a novel latent self-reflection attack that reasserts the power of gradient-based optimization for generating fluent jailbreaking prompts. By operating within the LLM's continuous latent space, LARGO first optimizes an adversarial latent vector and then recursively call the same LLM to decode the latent into natural language. This methodology yields a fast, effective, and transferable attack that produces fluent and stealthy prompts. On standard benchmarks like AdvBench and JailbreakBench,  LARGO surpasses leading jailbreaking techniques, including AutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent alternative to agentic LLM prompting, highlighting the efficacy of interpreting and attacking LLM internals through gradient optimization.",
      "arxiv_url": "https://openreview.net/forum?id=dSznm3mQTD",
      "pdf_url": "https://openreview.net/pdf/27b21f8a9c34a145bb7b9928bb0dcb8abf63cf2e.pdf",
      "primary_category": "Jailbreak, LLM",
      "categories": [
        "Jailbreak",
        "LLM"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9LoVCfMLDl",
      "title": "Agnostic Continuous-Time Online Learning",
      "authors": [
        "Pramith Devulapalli",
        "Changlong Wu",
        "Ananth Grama",
        "Wojciech Szpankowski"
      ],
      "abstract": "We study agnostic online learning from continuous-time data streams, a setting that naturally arises in applications such as environmental monitoring, personalized recommendation, and high-frequency trading. Unlike classical discrete-time models, learners in this setting must interact with a continually evolving data stream while making queries and updating models only at sparse, strategically selected times. We develop a general theoretical framework for learning from both *oblivious* and *adaptive* data streams, which may be noisy and non-stationary. For oblivious streams, we present a black-box reduction to classical online learning that yields a regret bound of $T \\cdot R(S)/S$ for any class with discrete-time regret $R(S)$, where $T$ is the time horizon and $S$ is the *query budget*. For adaptive streams, which can evolve in response to learner actions, we design a dynamic query strategy in conjunction with a novel importance weighting scheme that enables unbiased loss estimation. In particular, for hypothesis class $\\mathcal{H}$ with a finite Littlestone dimension, we establish a tight regret bound of $\\tilde{\\Theta}(T \\cdot \\sqrt{\\mathsf{Ldim}(\\mathcal{H})/S})$ that holds in both settings. Our results provide the first *quantitative* characterization of agnostic learning in continuous-time online environments with limited interaction.",
      "arxiv_url": "https://openreview.net/forum?id=9LoVCfMLDl",
      "pdf_url": "https://openreview.net/pdf/ec6bd468b6196c3c52189e93e29b9ab24d9ab73e.pdf",
      "primary_category": "continuous-time online learning, query efficiency, adaptive adversaries",
      "categories": [
        "continuous-time online learning",
        "query efficiency",
        "adaptive adversaries",
        "importance weighting",
        "Littlestone dimension"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "jW8nBi6y9F",
      "title": "Large Language Models Think Too Fast To Explore Effectively",
      "authors": [
        "Lan Pan",
        "Hanbo Xie",
        "Robert Wilson"
      ],
      "abstract": "Large Language Models (LLMs) have emerged with many intellectual capacities. While numerous benchmarks assess their intelligence, limited attention has been given to their ability to explore—an essential capacity for discovering new information and adapting to novel environments in both natural and artificial systems. The extent to which LLMs can effectively explore, particularly in open-ended tasks, remains unclear. This study investigates whether LLMs can surpass humans in exploration during an open-ended task, using Little Alchemy 2 as a paradigm, where agents combine elements to discover new ones. Results show most LLMs underperform compared to humans, except for the o1 model, with those traditional LLMs relying primarily on uncertainty-driven strategies, unlike humans who balance uncertainty and empowerment. Results indicate that traditional reasoning-focused LLMs, such as GPT-4o, exhibit a significantly faster and less detailed reasoning process, limiting their exploratory performance. In contrast, the DeepSeek reasoning model demonstrates prolonged, iterative thought processes marked by repetitive analysis of combinations and past trials, reflecting a more thorough and human-like exploration strategy. Representational analysis of the models with Sparse Autoencoders (SAE) revealed that uncertainty and choices are represented at earlier transformer blocks, while empowerment values are processed later, causing LLMs to think too fast and make premature decisions, hindering effective exploration. These findings shed light on the limitations of LLM exploration and suggest directions for improving their adaptability.",
      "arxiv_url": "https://openreview.net/forum?id=jW8nBi6y9F",
      "pdf_url": "https://openreview.net/pdf/84905da6e0006182e87623ee1b532443c3ca840a.pdf",
      "primary_category": "Large Language Models, Exploration, Empowerment",
      "categories": [
        "Large Language Models",
        "Exploration",
        "Empowerment",
        "Uncertainty",
        "Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VhGUS8kyaC",
      "title": "$\\texttt{STRCMP}$: Integrating Graph Structural Priors with Language Models for Combinatorial Optimization",
      "authors": [
        "Xijun Li",
        "Jiexiang Yang",
        "Jinghao Wang",
        "Bo Peng",
        "Jianguo Yao",
        "Haibing Guan"
      ],
      "abstract": "Combinatorial optimization (CO) problems, central to operation research and theoretical computer science, present significant computational challenges due to their $\\mathcal{NP}$-hard nature. While large language models (LLMs) have emerged as promising tools for CO—either by directly generating solutions or synthesizing solver-specific codes—existing approaches often $\\textit{neglect critical structural priors inherent to CO problems}$, leading to suboptimality and iterative inefficiency. Inspired by human experts’ success in leveraging CO structures for algorithm design, we propose $\\texttt{STRCMP}$, a novel structure-aware LLM-based algorithm discovery framework that systematically integrates structure priors to enhance solution quality and solving efficiency. Our framework combines a graph neural network (GNN) for extracting structural embeddings from CO instances with an LLM conditioned on these embeddings to identify high-performed algorithms in the form of solver-specific codes. This composite architecture ensures syntactic correctness, preserves problem topology, and aligns with natural language objectives, while an evolutionary refinement process iteratively optimizes generated algorithm. Extensive evaluations across Mixed Integer Linear Programming and Boolean Satisfiability problems, using nine benchmark datasets, demonstrate that our proposed $\\texttt{STRCMP}$ outperforms five strong neural and LLM-based methods by a large margin, in terms of both solution optimality and computational efficiency. The code is publicly available in the repository: https://github.com/Y-Palver/L2O-STRCMP.",
      "arxiv_url": "https://openreview.net/forum?id=VhGUS8kyaC",
      "pdf_url": "https://openreview.net/pdf/7c2f5ebcba66526972bcb573d4ec65fa22570050.pdf",
      "primary_category": "Learning to Optimize, LLM Code Generation, Combinatorial Optimization",
      "categories": [
        "Learning to Optimize",
        "LLM Code Generation",
        "Combinatorial Optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0jvnfH0WYV",
      "title": "Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding",
      "authors": [
        "Hanyin Wang",
        "Zhenbang Wu",
        "Gururaj J. Kolar",
        "Hariprasad Reddy Korsapati",
        "Brian Bartlett",
        "Bryan Hull",
        "Jimeng Sun"
      ],
      "abstract": "Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement and operations but require labor-intensive assignment. Large Language Models (LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of the task: pretraining corpora rarely contain private clinical or billing data. We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL) for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO) using rule-based rewards, DRG-Sapphire introduces a series of RL enhancements to address domain-specific challenges not seen in previous mathematical tasks. Our model achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments, significantly enhancing explainability. Our study further sheds light on broader challenges of applying RL to knowledge-intensive, OOD tasks. We observe that RL performance scales approximately linearly with the logarithm of the number of supervised fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally constrained by the domain knowledge encoded in the base model. For OOD tasks like DRG coding, strong RL performance requires sufficient knowledge infusion prior to RL. Consequently, scaling SFT may be more effective and computationally efficient than scaling RL alone for such tasks.",
      "arxiv_url": "https://openreview.net/forum?id=0jvnfH0WYV",
      "pdf_url": "https://openreview.net/pdf/93b57d3bdb75c1fb362bd32a8c8e8777ed7af3e1.pdf",
      "primary_category": "RL, DRG, Medical Coding",
      "categories": [
        "RL",
        "DRG",
        "Medical Coding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xAHozxfuUW",
      "title": "Conformal Information Pursuit for Interactively Guiding Large Language Models",
      "authors": [
        "Kwan Ho Ryan Chan",
        "Yuyan Ge",
        "Edgar Dobriban",
        "Hamed Hassani",
        "Rene Vidal"
      ],
      "abstract": "A significant use case of instruction-finetuned Large Language Models (LLMs) is to solve question-answering tasks interactively. In this setting, an LLM agent is tasked with making a prediction by sequentially querying relevant information from the user, as opposed to a single-turn conversation. This paper explores sequential querying strategies that aim to minimize the expected number of queries. One such strategy is Information Pursuit (IP), a greedy algorithm that at each iteration selects the query that maximizes information gain or equivalently minimizes uncertainty. However, obtaining accurate estimates of mutual information or conditional entropy for LLMs is very difficult in practice due to over- or under-confident LLM probabilities, which leads to suboptimal query selection and predictive performance. To better estimate the uncertainty at each iteration, we propose *Conformal Information Pursuit (C-IP)*, an alternative approach to sequential information gain based on conformal prediction sets. More specifically, C-IP leverages a relationship between prediction sets and conditional entropy at each iteration to estimate uncertainty based on the average size of conformal prediction sets. In contrast to conditional entropy, we find that conformal prediction sets are a distribution-free and robust method of measuring uncertainty. Experiments with 20 Questions show that C-IP obtains better predictive performance and shorter query-answer chains compared to previous approaches to IP and uncertainty-based chain-of-thought methods. Furthermore, extending to an interactive medical setting between a doctor and a patient on the MediQ dataset, C-IP achieves competitive performance with direct single-turn prediction while offering greater interpretability.",
      "arxiv_url": "https://openreview.net/forum?id=xAHozxfuUW",
      "pdf_url": "https://openreview.net/pdf/9f42c92d3ee2fc75dc8db53a7ccf981c7b807668.pdf",
      "primary_category": "Large Language Models, Interactive Question Answering, Uncertainty Quantification",
      "categories": [
        "Large Language Models",
        "Interactive Question Answering",
        "Uncertainty Quantification",
        "Twenty Questions"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kZahfVKYbl",
      "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling",
      "authors": [
        "Shuhong Zheng",
        "Ashkan Mirzaei",
        "Igor Gilitschenski"
      ],
      "abstract": "Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/.",
      "arxiv_url": "https://openreview.net/forum?id=kZahfVKYbl",
      "pdf_url": "https://openreview.net/pdf/aba9b239dea0a8bc088902c7651344d1eeb4c7f2.pdf",
      "primary_category": "Subject-driven Generation, Personalization, 3D and 4D Generation",
      "categories": [
        "Subject-driven Generation",
        "Personalization",
        "3D and 4D Generation"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "G6K2NepP7S",
      "title": "Can Multi-Modal LLMs Provide Live Step-by-Step Task Guidance?",
      "authors": [
        "Apratim Bhattacharyya",
        "Bicheng Xu",
        "Sanjay Haresh",
        "Reza Pourreza",
        "Litian Liu",
        "Sunny Panchal",
        "Leonid Sigal",
        "Roland Memisevic"
      ],
      "abstract": "Multi-modal Large Language Models (LLM) have advanced conversational abilities but struggle with providing live, interactive step-by-step guidance, a key capability for future AI assistants. Effective guidance requires not only delivering instructions but also detecting their successful execution, as well as identifying and alerting users to mistakes, all of which has to happen in real-time. This requires models that are not turn-based, but that can react asynchronously to a video stream, as well as video data showing users performing tasks including mistakes and their corrections. To this end, we introduce Qualcomm Interactive Cooking, a new benchmark and dataset built upon CaptainCook4D, which contains user mistakes during task execution. Our dataset and benchmark features densely annotated, timed instructions and feedback messages, specifically including mistake alerts precisely timestamped to their visual occurrence in the video. We evaluate state-of-the-art multi-modal LLMs on the Qualcomm Interactive Cooking benchmark and introduce LiveMamba, a streaming multi-modal LLM designed for interactive instructional guidance. This work provides the first dedicated benchmark and a strong baseline for developing and evaluating on live, situated coaching.",
      "arxiv_url": "https://openreview.net/forum?id=G6K2NepP7S",
      "pdf_url": "https://openreview.net/pdf/114ddc6bfe32bf40da90e93c7396935da0773671.pdf",
      "primary_category": "Multi-modal, Large language Models, Vision and Language",
      "categories": [
        "Multi-modal",
        "Large language Models",
        "Vision and Language"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "eOLdGbXT6t",
      "title": "ToolRL: Reward is All Tool Learning Needs",
      "authors": [
        "Cheng Qian",
        "Emre Can Acikgoz",
        "Qi He",
        "Hongru WANG",
        "Xiusi Chen",
        "Dilek Hakkani-Tür",
        "Gokhan Tur",
        "Heng Ji"
      ],
      "abstract": "Current Large Language Models (LLMs) often undergo supervised fine-tuning (SFT) to acquire tool use capabilities. However, SFT struggles to generalize to unfamiliar or complex tool use scenarios. Recent advancements in reinforcement learning (RL), particularly with R1-like models, have demonstrated promising reasoning and generalization abilities. Yet, reward design for tool use presents unique challenges: multiple tools may be invoked with diverse parameters, and coarse-grained reward signals, such as answer matching, fail to offer the finegrained feedback required for effective learning.\nIn this work, we present the first comprehensive study on reward design for tool selection and application tasks within the RL paradigm. We systematically explore a wide range of reward strategies, analyzing their types, scales, granularity, and temporal dynamics. Building on these insights, we propose a principled reward design tailored for tool use tasks and apply it to train LLMs using RL methods.\nEmpirical evaluations across diverse benchmarks demonstrate that our approach yields robust, scalable, and stable training, achieving a 17\\% improvement over base models and a 15\\% gain over SFT models. These results highlight the critical role of thoughtful reward design in enhancing the tool use capabilities and generalization performance of LLMs. All the codes are released to facilitate future research.",
      "arxiv_url": "https://openreview.net/forum?id=eOLdGbXT6t",
      "pdf_url": "https://openreview.net/pdf/097ae4a34c2eb2b82b2bb8fccc279fb0e3585304.pdf",
      "primary_category": "Language Model Agent, Tool Use, Reward Design",
      "categories": [
        "Language Model Agent",
        "Tool Use",
        "Reward Design"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "W8xcKoJcrl",
      "title": "Strategic Costs of Perceived Bias in Fair Selection",
      "authors": [
        "L. Elisa Celis",
        "Lingxiao Huang",
        "Milind Sohoni",
        "Nisheeth K. Vishnoi"
      ],
      "abstract": "Meritocratic systems, from admissions to hiring, aim to impartially reward skill and effort. Yet persistent disparities across race, gender, and class challenge this ideal. Some attribute these gaps to structural inequality; others to individual choice. We develop a game-theoretic model in which candidates from different socioeconomic groups differ in their perceived post-selection value—shaped by social context and, increasingly, by AI-powered tools offering personalized career or salary guidance. Each candidate strategically chooses effort, balancing its cost against expected reward; effort translates into observable merit, and selection is based solely on merit. We characterize the unique Nash equilibrium in the large-agent limit and derive explicit formulas showing how valuation disparities and institutional selectivity jointly determine effort, representation, social welfare, and utility. We further propose a cost-sensitive optimization framework that quantifies how modifying selectivity or perceived value can reduce disparities without compromising institutional goals. Our analysis reveals a perception-driven bias: when perceptions of post-selection value differ across groups, these differences translate into rational differences in effort, propagating disparities backward through otherwise \"fair\" selection processes. While the model is static, it captures one stage of a broader feedback cycle linking perceptions, incentives, and outcomes—bridging rational-choice and structural explanations of inequality by showing how techno-social environments shape individual incentives in meritocratic systems.",
      "arxiv_url": "https://openreview.net/forum?id=W8xcKoJcrl",
      "pdf_url": "https://openreview.net/pdf/159935c2c92c4c759be0ad1e4cbc29013bdb757d.pdf",
      "primary_category": "Meritocratic selection, Strategic effort, Game-theoretic modeling",
      "categories": [
        "Meritocratic selection",
        "Strategic effort",
        "Game-theoretic modeling",
        "Perceived valuation bias",
        "Algorithmic fairness",
        "Social feedback loops",
        "Representation and efficiency",
        "Nash equilibrium",
        "Human-AI interaction"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "V0lnRCH73U",
      "title": "Breaking the Gradient Barrier: Unveiling Large Language Models for Strategic Classification",
      "authors": [
        "Xinpeng Lv",
        "Yunxin Mao",
        "Haoxuan Li",
        "KE LIANG",
        "Jinxuan Yang",
        "Wanrong Huang",
        "Haoang Chi",
        "Huan Chen",
        "Long Lan",
        "Cyuanlong",
        "Wenjing Yang",
        "Haotian Wang"
      ],
      "abstract": "Strategic classification (SC) explores how individuals or entities modify their features strategically to achieve favorable classification outcomes. However, existing SC methods, which are largely based on linear models or shallow neural networks, face significant limitations in terms of scalability and capacity when applied to real-world datasets with significantly increasing scale, especially in financial services and the internet sector. \nIn this paper, we investigate how to leverage large language models to design a more scalable and efficient SC framework, especially in the case of growing individuals engaged with decision-making processes. Specifically, we introduce GLIM, a gradient-free SC method grounded in in-context learning. \nDuring the feed-forward process of self-attention, GLIM implicitly simulates the typical bi-level optimization process of SC, including both the feature manipulation and decision rule optimization. \nWithout fine-tuning the LLMs, our proposed GLIM enjoys the advantage of cost-effective adaptation in dynamic strategic environments. Theoretically, we prove GLIM can support pre-trained LLMs to adapt to a broad range of strategic manipulations. We validate our approach through experiments with a collection of pre-trained LLMs on real-world and synthetic datasets in financial and internet domains, demonstrating that our GLIM exhibits both robustness and efficiency, and offering an effective solution for large-scale SC tasks.",
      "arxiv_url": "https://openreview.net/forum?id=V0lnRCH73U",
      "pdf_url": "https://openreview.net/pdf/92e89c149499e54f937df5a1da5b3c934948757d.pdf",
      "primary_category": "strategic classification, machine learning, LLMs",
      "categories": [
        "strategic classification",
        "machine learning",
        "LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "TCxzqnbyAF",
      "title": "Context-Aware Hierarchical Learning: A Two-Step Paradigm towards Safer LLMs",
      "authors": [
        "Tengyun Ma",
        "Jiaqi Yao",
        "Daojing He",
        "Shihao Peng",
        "YU LI",
        "Shaohui Liu",
        "Zhuotao Tian"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as powerful tools for diverse applications. However, their uniform token processing paradigm introduces critical vulnerabilities in instruction handling, particularly when exposed to adversarial scenarios. In this work, we identify and propose a novel class of vulnerabilities, termed Tool-Completion Attack (TCA), which exploits function-calling mechanisms to subvert model behavior. To evaluate LLM robustness against such threats, we introduce the Tool-Completion benchmark, a comprehensive security assessment framework, which reveals that even state-of-the-art models remain susceptible to TCA, with surprisingly high attack success rates. To address these vulnerabilities, we introduce Context-Aware Hierarchical Learning (CAHL), a sophisticated mechanism that dynamically equilibrates semantic comprehension with role-specific instruction constraints. CAHL leverages the contextual correlations between different instruction segments to establish a robust, context-aware instruction hierarchy. Extensive experiments demonstrate that CAHL significantly enhances LLM robustness against both conventional attacks and the proposed TCA, exhibiting strong generalization capabilities in zero-shot evaluations while still preserving model performance on generic tasks. Our code is available at https://github.com/S2AILab/CAHL.",
      "arxiv_url": "https://openreview.net/forum?id=TCxzqnbyAF",
      "pdf_url": "https://openreview.net/pdf/735d4274ecf365c0dab0293ba80edf23c273bb71.pdf",
      "primary_category": "LLM, Large Language Model, Security",
      "categories": [
        "LLM",
        "Large Language Model",
        "Security",
        "Prompt Injection Attack",
        "Instruction Hierarchy"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zNLlglSOwD",
      "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders",
      "authors": [
        "Yuezhou Hu",
        "Jiaxin Guo",
        "Xinyu Feng",
        "Tuo Zhao"
      ],
      "abstract": "Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\\%). The code is publicly available at \\url{https://github.com/yuezhouhu/adaspec}.",
      "arxiv_url": "https://openreview.net/forum?id=zNLlglSOwD",
      "pdf_url": "https://openreview.net/pdf/4169ec79fb39e606daf2863e8f46df0a76009e66.pdf",
      "primary_category": "Speculative Decoding, Large Language Models, Knowledge Distillation",
      "categories": [
        "Speculative Decoding",
        "Large Language Models",
        "Knowledge Distillation",
        "Token Selection"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QSLlPljXxz",
      "title": "Learning to Steer: Input-dependent Steering for Multimodal LLMs",
      "authors": [
        "Jayneel Parekh",
        "Pegah KHAYATAN",
        "Mustafa Shukor",
        "Arnaud Dapogny",
        "Alasdair Newson",
        "Matthieu Cord"
      ],
      "abstract": "Steering has emerged as a practical approach to enable post-hoc guidance of LLMs towards enforcing a specific behavior. \nHowever, it remains largely underexplored for multimodal LLMs (MLLMs); furthermore, existing steering techniques, such as \\textit{mean} steering, rely on a single steering vector, applied independently of the input query. This paradigm faces limitations when the desired behavior is dependent on the example at hand. For example, a safe answer may consist in abstaining from answering when asked for an illegal activity, or may point to external resources or consultation with an expert when asked about medical advice. In this paper, we investigate a fine-grained steering that uses an input-specific linear shift. This shift is computed using contrastive input-specific prompting. However, the input-specific prompts required for this approach are not known at test time. Therefore, we propose to train a small auxiliary module to predict the input-specific steering vector. Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces hallucinations and enforces safety in MLLMs, outperforming other static baselines. We will open-source our code.",
      "arxiv_url": "https://openreview.net/forum?id=QSLlPljXxz",
      "pdf_url": "https://openreview.net/pdf/3cfaa1f72cb9c463a3cfaf00213136f8200801b9.pdf",
      "primary_category": "multimodal large language models, large multimodal models, steering",
      "categories": [
        "multimodal large language models",
        "large multimodal models",
        "steering",
        "representation steering",
        "safety alignment",
        "hallucinations",
        "vision-language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cuhQZdh2Oq",
      "title": "Crucible: Quantifying the Potential of Control Algorithms through LLM Agents",
      "authors": [
        "Lianchen Jia",
        "Chaoyang Li",
        "Qian Houde",
        "Tianchi Huang",
        "Jiangchuan Liu",
        "Lifeng Sun"
      ],
      "abstract": "Control algorithms in production environments typically require domain experts to tune their parameters and logic for specific scenarios. However, existing research predominantly focuses on algorithmic performance under ideal or default configurations, overlooking the critical aspect of Tuning Potential. To bridge this gap, we introduce \\texttt{Crucible}, an agent that employs an LLM-driven, multi-level expert simulation to turn algorithms and defines a formalized metric to quantitatively evaluate their Tuning Potential. We demonstrate \\texttt{Crucible}'s effectiveness across a wide spectrum of case studies, from classic control tasks to complex computer systems, and validate its findings in a real-world deployment. Our experimental results reveal that \\texttt{Crucible} systematically quantifies the tunable space across different algorithms. Furthermore, \\texttt{Crucible} provides a new dimension for algorithm analysis and design, which ultimately leads to performance improvements. Our code is available at https://github.com/thu-media/Crucible.",
      "arxiv_url": "https://openreview.net/forum?id=cuhQZdh2Oq",
      "pdf_url": "https://openreview.net/pdf/57ded8a1d0ced3d709538c631817f7d1a501017b.pdf",
      "primary_category": "Control Algorithms, Large Language Models, Agent",
      "categories": [
        "Control Algorithms",
        "Large Language Models",
        "Agent",
        "Potential"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wgeN1eD54P",
      "title": "Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored Mixture-of-Experts Transformers",
      "authors": [
        "Xin Zhao",
        "Xiaojun Chen",
        "Bingshan Liu",
        "Haoyu Gao",
        "Zhendong Zhao",
        "Yilong Chen"
      ],
      "abstract": "Large language models (LLMs) with Mixture-of-Experts (MoE) architectures achieve impressive performance and efficiency by dynamically routing inputs to specialized subnetworks, known as experts.  However, this sparse routing mechanism inherently exhibits task preferences due to expert specialization, introducing a new and underexplored vulnerability to backdoor attacks. In this work, we investigate the feasibility and effectiveness of injecting backdoors into MoE-based LLMs by exploiting their inherent expert routing preferences.\nWe thus propose \\textbf{BadSwitch}, a novel backdoor framework that integrates task-coupled dynamic trigger optimization with a sensitivity-guided Top-S expert tracing mechanism. Our approach jointly optimizes trigger embeddings during pretraining while identifying S most sensitive experts, subsequently constraining the Top-K gating mechanism to these targeted experts. Unlike traditional backdoor attacks that rely on superficial data poisoning or model editing, BadSwitch primarily embeds malicious triggers into expert routing paths with strong task affinity, enabling precise and stealthy model manipulation. \nThrough comprehensive evaluations across three prominent MoE architectures (Switch Transformer, QwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack pre-trained models with up to 100\\% success rate (ASR) while maintaining the highest clean accuracy (ACC) among all baselines.\nFurthermore, BadSwitch exhibits strong resilience against both text-level and model-level defense mechanisms, achieving 94.07\\% ASR and 87.18\\% ACC on the AGNews dataset. \nOur analysis of expert activation patterns reveals fundamental insights into MoE vulnerabilities. We anticipate this work will expose security risks in MoE systems and contribute to advancing AI safety.",
      "arxiv_url": "https://openreview.net/forum?id=wgeN1eD54P",
      "pdf_url": "https://openreview.net/pdf/a211f7d771a83308275c9b8e700f651fc191a63e.pdf",
      "primary_category": "Backdoor Attacks, Mixture-of-Experts, Large Language Models",
      "categories": [
        "Backdoor Attacks",
        "Mixture-of-Experts",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VBx4yMNtjt",
      "title": "Scaling Law with Learning Rate Annealing",
      "authors": [
        "Howe Tissue",
        "Venus Wang",
        "Lu Wang"
      ],
      "abstract": "We find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where $L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR curve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are constant parameters. This formulation accounts for two main effects: (1) power-law scaling over data size, and (2) the additional loss reduction during LR annealing. Unlike previous studies that only fit losses at final steps, our formulation captures the entire training curve, allowing for parameter fitting using losses from any training step. Applying the scaling law with LR annealing and fitting only one or two training curves, we can accurately predict the loss at any given step under any learning rate scheduler (LRS). This approach significantly reduces computational cost in formulating scaling laws while providing more accuracy and expressiveness. Extensive experiments demonstrate that our findings hold across a range of hyper-parameters and model architectures and can extend to scaling effect of model sizes. Moreover, our formulation provides accurate theoretical insights into empirical results observed in numerous previous studies, particularly those focusing on LR schedule and annealing. We believe that this work is promising to enhance the understanding of LLM training dynamics while democratizing scaling laws, and it is helpful to guide both research and industrial participants in refining training strategies for further LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=VBx4yMNtjt",
      "pdf_url": "https://openreview.net/pdf/6bd9ed2c31ee01c7daf58ebc1bc8e2ec6d4e6d2c.pdf",
      "primary_category": "Scaling Laws, Full Loss Curve Prediction, Learning Rate Schedule",
      "categories": [
        "Scaling Laws",
        "Full Loss Curve Prediction",
        "Learning Rate Schedule",
        "LLM Pretraining"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "E0PaeSszLz",
      "title": "A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning",
      "authors": [
        "Zhi Zhou",
        "Yuhao Tan",
        "Zenan Li",
        "Yuan Yao",
        "Lan-Zhe Guo",
        "Yu-Feng Li",
        "Xiaoxing Ma"
      ],
      "abstract": "Test-time scaling seeks to improve the reasoning performance of large language models (LLMs) by adding computational resources. A prevalent approach within the field is *sampling-based test-time scaling methods*, which enhance reasoning by generating multiple reasoning paths for a given input during inference. However, despite its practical success, the theoretical foundations remain underexplored. In this paper, we provide the first theoretical framework for analyzing sampling-based test-time scaling methods, grounded in the perspective of confidence estimation. Based on the framework, we analyze two dominant paradigms: self-consistency and perplexity, and reveal key limitations: self-consistency suffers from high estimation error while perplexity exhibits substantial modeling error and possible degradation of the estimation error convergence. To address these limitations, we introduce RPC, a hybrid method that leverages our theoretical insights through two key components: *Perplexity Consistency* and *Reasoning Pruning*. *Perplexity Consistency* combines the strengths of self-consistency and perplexity, boosting the convergence rate of estimation error from linear to exponential while preserving model error. *Reasoning Pruning* prevents degradation by eliminating low-probability reasoning paths.\nBoth theoretical analysis and empirical results across seven benchmark datasets demonstrate that RPC has a strong potential for reducing reasoning error. Notably, RPC achieves reasoning performance comparable to self-consistency while not only enhancing confidence reliability but also reducing sampling costs by 50%. The code and resources are available at https://wnjxyk.github.io/RPC.",
      "arxiv_url": "https://openreview.net/forum?id=E0PaeSszLz",
      "pdf_url": "https://openreview.net/pdf/73a0607365582aedefc2167e27fb239c96092223.pdf",
      "primary_category": "Large Language Model, Self Consistency, LLM Reasoning",
      "categories": [
        "Large Language Model",
        "Self Consistency",
        "LLM Reasoning",
        "Test-Time Scaling",
        "Best-of-N"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6TmLco2L2D",
      "title": "HoPE: Hybrid of Position Embedding for Long Context Vision-Language Models",
      "authors": [
        "Haoran Li",
        "Yingjie Qin",
        "Baoyuan Ou",
        "Lai Xu",
        "Ruiwen Xu"
      ],
      "abstract": "Vision-Language Models (VLMs) have made significant progress in multimodal tasks. However, their performance often deteriorates in long-context scenarios, particularly long videos. While Rotary Position Embedding (RoPE) has been widely adopted for length generalization in Large Language Models (LLMs), extending vanilla RoPE to capture the intricate spatial-temporal dependencies in videos remains an unsolved challenge. Existing methods typically allocate different frequencies within RoPE to encode 3D positional information. However, these allocation strategies mainly rely on heuristics, lacking in-depth theoretical analysis. In this paper, we first study how different allocation strategies impact the long-context capabilities of VLMs. Our analysis reveals that current multimodal RoPEs fail to reliably capture semantic similarities over extended contexts. To address this issue, we propose HoPE, a Hybrid of Position Embedding designed to improve the long-context capabilities of VLMs. HoPE introduces a hybrid frequency allocation strategy for reliable semantic modeling over arbitrarily long contexts, and a dynamic temporal scaling mechanism to facilitate robust learning and flexible inference across diverse context lengths. Extensive experiments across four video benchmarks on long video understanding and retrieval tasks demonstrate that HoPE consistently outperforms existing methods, confirming its effectiveness.",
      "arxiv_url": "https://openreview.net/forum?id=6TmLco2L2D",
      "pdf_url": "https://openreview.net/pdf/58fe893b3b4a893d0a420f02eff0b6c872308561.pdf",
      "primary_category": "Long Context, Position Embedding, Vision-Language Models",
      "categories": [
        "Long Context",
        "Position Embedding",
        "Vision-Language Models",
        "Transformers"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BDKkFwskot",
      "title": "FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model",
      "authors": [
        "Jinwei Hu",
        "Zhenglin Huang",
        "Xiangyu Yin",
        "Wenjie Ruan",
        "Guangliang Cheng",
        "Yi Dong",
        "Xiaowei Huang"
      ],
      "abstract": "Large language models have been widely applied, but can inadvertently encode sensitive or harmful information, raising significant safety concerns. Machine unlearning has emerged to alleviate this concern; however, existing training-time unlearning approaches, relying on coarse-grained loss combinations, have limitations in precisely separating knowledge and balancing removal effectiveness with model utility.  In contrast, we propose $\\textbf{F}$ine-grained $\\textbf{A}$ctivation manipu$\\textbf{L}$ation by $\\textbf{C}$ontrastive $\\textbf{O}$rthogonal u$\\textbf{N}$alignment (FALCON), a novel representation-guided unlearning approach that leverages information-theoretic guidance for efficient parameter selection, employs contrastive mechanisms to enhance representation separation, and projects conflict gradients onto orthogonal subspaces to resolve conflicts between forgetting and retention objectives. Extensive experiments demonstrate that FALCON achieves superior unlearning effectiveness while maintaining model utility, exhibiting robust resistance against knowledge recovery attempts.",
      "arxiv_url": "https://openreview.net/forum?id=BDKkFwskot",
      "pdf_url": "https://openreview.net/pdf/4ab520c8643597a55d82633afb824ec65fbc5067.pdf",
      "primary_category": "LLM Unlearning;Contrastive Learning;Responsible AI",
      "categories": [
        "LLM Unlearning;Contrastive Learning;Responsible AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cLbGkINOLP",
      "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
      "authors": [
        "Ibrahim Alabdulmohsin",
        "Xiaohua Zhai"
      ],
      "abstract": "Inspired by recent findings on the fractal geometry of language, we introduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe for scaling inference time in language and multimodal systems. RINS is a particular form of recursive depth that significantly outperforms +55 other variants, including the recent \"repeat-all-over\" (RAO) strategy in Mobile LLM (Liu et al., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior works, we carry out our comparisons on a compute-matched regime, and demonstrate that for a  fixed model size and training compute budget, RINS substantially improves language modeling performance. It also generalizes beyond pure language tasks, delivering gains in multimodal systems, including a +2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS improves both the asymptotic performance limits and the scaling exponents. More importantly, with light-weight (linear) adapters (comprising <1% of model parameters) and stochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled pretraining improves performance in language modeling even when recursive depth is not applied at inference time. This corresponds to improving performance on a training compute-, parameter-, and inference-matched regime, suggesting its potential as a viable component of LLM pretraining!",
      "arxiv_url": "https://openreview.net/forum?id=cLbGkINOLP",
      "pdf_url": "https://openreview.net/pdf/18b88ab02c66f556d2915689200464d3fea85d3f.pdf",
      "primary_category": "Recursive inference, scaling inference time, self-similarity",
      "categories": [
        "Recursive inference",
        "scaling inference time",
        "self-similarity",
        "language models",
        "SigLIP",
        "multimodal systems."
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cxb5EsQHW3",
      "title": "Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling",
      "authors": [
        "Hao Mark Chen",
        "Guanxi Lu",
        "Yasuyuki Okoshi",
        "Zhiwen Mo",
        "Masato Motomura",
        "Hongxiang Fan"
      ],
      "abstract": "Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity—that is, how frequently the verifier is invoked during generation, \nbeyond verifying only the final output or individual generation steps.\nTo this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter $g$. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting $g$ can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over Best-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to support future research.",
      "arxiv_url": "https://openreview.net/forum?id=cxb5EsQHW3",
      "pdf_url": "https://openreview.net/pdf/007919ac50b98a7b9200159433b2b419d1b7327b.pdf",
      "primary_category": "Large Language Models, Test-Time Scaling, Efficient Machine Learning",
      "categories": [
        "Large Language Models",
        "Test-Time Scaling",
        "Efficient Machine Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "REHjkmWdQL",
      "title": "Measuring and Guiding Monosemanticity",
      "authors": [
        "Ruben Härle",
        "Felix Friedrich",
        "Manuel Brack",
        "Björn Deiseroth",
        "Stephan Waeldchen",
        "Patrick Schramowski",
        "Kristian Kersting"
      ],
      "abstract": "There is growing interest in leveraging mechanistic interpretability and controllability to better understand and influence the internal dynamics of large language models (LLMs). However, current methods face fundamental challenges in reliably localizing and manipulating feature representations. Sparse Autoencoders (SAEs) have recently emerged as a promising direction for feature extraction at scale, yet they, too, are limited by incomplete feature isolation and unreliable monosemanticity. To systematically quantify these limitations, we introduce Feature Monosemanticity Score (FMS), a novel metric to quantify feature monosemanticity in latent representation. Building on these insights, we propose Guided Sparse Autoencoders (G-SAE), a method that conditions latent representations on labeled concepts during training. We demonstrate that reliable localization and disentanglement of target concepts within the latent space improve interpretability, detection of behavior, and control. Specifically, our evaluations on toxicity detection, writing style identification, and privacy attribute recognition show that G-SAE not only enhances monosemanticity but also enables more effective and fine-grained steering with less quality degradation. Our findings provide actionable guidelines for measuring and advancing mechanistic interpretability and control of LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=REHjkmWdQL",
      "pdf_url": "https://openreview.net/pdf/4ea937bae4804e7eaba091acf4df4371fd6023d7.pdf",
      "primary_category": "Interpretability, Monosemanticity, Sparse Autoencoder",
      "categories": [
        "Interpretability",
        "Monosemanticity",
        "Sparse Autoencoder",
        "LLM",
        "Alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "lwIQC4MVJZ",
      "title": "Efficient Large Language Model Inference with Neural Block Linearization",
      "authors": [
        "Mete Erdogan",
        "Francesco Tonin",
        "Volkan Cevher"
      ],
      "abstract": "The high inference demands of transformer-based Large Language Models (LLMs) pose substantial challenges in their deployment. To this end, we introduce *Neural Block Linearization* (NBL), a novel framework for accelerating transformer model inference by replacing self-attention layers with linear approximations derived from Linear Minimum Mean Squared Error estimators. NBL leverages Canonical Correlation Analysis to compute a theoretical upper bound on the approximation error. Then, we use this bound as a criterion for substitution, selecting the LLM layers with the lowest linearization error. NBL can be efficiently applied to pre-trained LLMs without the need for fine-tuning. In experiments, NBL achieves notable computational speed-ups while preserving competitive accuracy on multiple reasoning benchmarks. For instance, applying NBL to 12 self-attention layers in *DeepSeek-R1-Distill-Llama-8B* increases the inference speed by 32% with less than 1% accuracy trade-off, making it a flexible and promising solution to improve the inference efficiency of LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=lwIQC4MVJZ",
      "pdf_url": "https://openreview.net/pdf/f20ae3e2f2904be154a733cdd3e06fbcfc68213f.pdf",
      "primary_category": "Inference, Transformer, LLM",
      "categories": [
        "Inference",
        "Transformer",
        "LLM",
        "CCA",
        "Lightweight Architectures"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "UQT2inkLmb",
      "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback",
      "authors": [
        "Wei Shen",
        "Guanlin Liu",
        "YuYue",
        "Ruofei Zhu",
        "Qingping Yang",
        "Chao Xin",
        "Lin Yan"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences and values. While recent research has primarily focused on algorithmic advancements—such as reducing computational overhead or strengthening reward models to mitigate reward hacking—the critical role of prompt-data construction and its scalability has received comparatively less attention. In this paper, we address this gap by systematically exploring data-driven bottlenecks that currently hinder RLHF performance scaling, focusing specifically on the challenges posed by reward hacking and decreasing response diversity.\nTo mitigate reward hacking, we introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM). This approach not only exhibits enhanced resistance to reward hacking, but also enables accurate assessment of responses against clearly defined ground-truth solutions. Additionally, in order to ensure response diversity and enhance learning effectiveness, we propose a novel prompt-selection method named \\textbf{Pre-PPO}, explicitly identifying training prompts that are inherently challenging and thus less prone to reward hacking. Furthermore, we find that \\textbf{prioritizing mathematical and coding tasks during the early phases of RLHF training} significantly boosts performance, given that these tasks naturally encode fine-grained response distinctions and possess clearly defined ground truths.\nThrough comprehensive experiments conducted across two model sizes, we validate the effectiveness and scalability of our proposed methods. Results show that RTV exhibits the strongest resistance to reward hacking, followed by GenRM with ground truth, and finally GenRM relying on SFT Best-of-N responses. Moreover, our proposed strategies enable the model to rapidly capture subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work underscores the importance of careful data construction and provides practical methodologies to overcome critical performance barriers in RLHF.",
      "arxiv_url": "https://openreview.net/forum?id=UQT2inkLmb",
      "pdf_url": "https://openreview.net/pdf/367c201d53d442171a806281acff55402cafd446.pdf",
      "primary_category": "LLM, RLHF, Data Scaling",
      "categories": [
        "LLM",
        "RLHF",
        "Data Scaling"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Hk4cCTukeI",
      "title": "Triplets Better Than Pairs: Towards Stable and Effective Self-Play Fine-Tuning for LLMs",
      "authors": [
        "Yibo Wang",
        "Hai-Long Sun",
        "Guangda Huzhang",
        "Qing-Guo Chen",
        "Zhao Xu",
        "Weihua Luo",
        "Kaifu Zhang",
        "Lijun Zhang"
      ],
      "abstract": "Recently, self-play fine-tuning (SPIN) has been proposed to adapt large language models to downstream applications with scarce expert-annotated data, by iteratively generating synthetic responses from the model itself. However, SPIN is designed to optimize the current reward advantages of annotated responses over synthetic responses at hand, which may gradually vanish during iterations, leading to \\textit{unstable optimization}. Moreover, the utilization of reference policy induces a \\textit{misalignment} issue between the reward formulation for training and the metric for generation. To address these limitations, we propose a novel \\textbf{T}riplet-based \\textbf{S}elf-\\textbf{P}lay f\\textbf{I}ne-tu\\textbf{N}ing (TSPIN) method that integrates two key designs. First, beyond current advantages, TSPIN additionally incorporates historical advantages between iteratively generated responses and proto-synthetic responses produced by the initial policy. Even if the current advantages diminish, historical advantages remain effective, stabilizing the overall optimization. Second, TSPIN introduces the entropy constraint into the self-play framework, which is theoretically justified to support reference-free fine-tuning, eliminating the training-generation discrepancy. Empirical results on various tasks demonstrate not only the superior performance of TSPIN over SPIN, but also its stable evolution during iterations. Remarkably, compared to supervised fine-tuning, TSPIN achieves comparable or even better performance with only $25\\\\%$ samples, highlighting its effectiveness when faced with scarce annotated data.",
      "arxiv_url": "https://openreview.net/forum?id=Hk4cCTukeI",
      "pdf_url": "https://openreview.net/pdf/1f80f91d1de315d89839a8e993a020a6772d57a4.pdf",
      "primary_category": "Large Language Model, Self-play Optimization, Natural Language Processing",
      "categories": [
        "Large Language Model",
        "Self-play Optimization",
        "Natural Language Processing"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yfcpdY4gMP",
      "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
      "authors": [
        "Shenzhi Wang",
        "Le Yu",
        "Chang Gao",
        "Chujie Zheng",
        "Shixuan Liu",
        "Rui Lu",
        "Kai Dang",
        "Xiong-Hui Chen",
        "Jianxin Yang",
        "Zhenru Zhang",
        "Yuqiong Liu",
        "An Yang",
        "Andrew Zhao",
        "Yang Yue",
        "Shiji Song",
        "Bowen Yu",
        "Gao Huang",
        "Junyang Lin"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), yet its underlying mechanisms remain insufficiently understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction (approximately 20\\%) of tokens exhibit high entropy, and these tokens semantically act as critical forks that steer the model toward diverse reasoning pathways. We further demonstrate that moderately increasing the entropy of these high-entropy tokens via decoding temperature adjustments leads to improved performance, quantitatively confirming their role as decision points in reasoning. We ultimately refine RLVR by restricting policy gradient updates to these forking tokens. Despite utilizing only 20\\% of tokens, our approach achieves comparable performance to full-gradient updates on the Qwen3-8B base model. Moreover, it demonstrates remarkable improvements on the larger Qwen3-32B base model, boosting AIME'25 scores by 11.04 and AIME'24 scores by 7.71. In contrast, training exclusively on the 80\\% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that dictate key reasoning directions. Collectively, our results suggest promising avenues for optimizing RLVR algorithms by strategically leveraging the potential of these high-entropy minority tokens to further enhance the reasoning abilities of LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=yfcpdY4gMP",
      "pdf_url": "https://openreview.net/pdf/9d48d3daa5a11e38bedfc44ffde669541136a28b.pdf",
      "primary_category": "Large Language Model, Reasoning, Reinforcement Learning",
      "categories": [
        "Large Language Model",
        "Reasoning",
        "Reinforcement Learning",
        "Entropy"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2nIAtsUC27",
      "title": "Improve Temporal Reasoning in Multimodal Large Language Models via Video Contrastive Decoding",
      "authors": [
        "Daiqing Qi",
        "Dongliang Guo",
        "Hanzhang Yuan",
        "Handong Zhao",
        "Mengxuan Hu",
        "Lehan Yang",
        "Sheng Li"
      ],
      "abstract": "A major distinction between video and image understanding is that the former requires reasoning over time.\nExisting Video Large Language Models (VLLMs) demonstrate promising performance in general video understanding, such as brief captioning or object recognition within individual frames. However, they often struggle with temporal reasoning such as understanding continuous actions or tracking object transformations over time—which typically demands the integration of multiple frames in a temporally coherent manner.\nWe first explore and explain such failures in Video LLMs from the perspective of \\textit{language and ``image'' priors.}\nWhile existing research has attempted to enhance the temporal understanding of VLLMs through various training strategies, the demand for expensive computational resources and training data often presents significant barriers.\nTo this end, we further propose a simple yet novel idea for improving temporal reasoning in videos at no additional training cost.\nSpecifically, to better capture the temporal structure across multiple frames—the key to effective temporal reasoning—we distort the temporal consistency in key frames \\textit{during the decoding phase}. Such corruption induces time-insensitive wrong responses from the model, which are then contrastively avoided when generating the final correct output. In this way, the model is encouraged to perform more temporally coherent reasoning.\nOur method yields consistent improvements across both temporal-specific and general video understanding benchmarks, demonstrating its effectiveness and generalizability.",
      "arxiv_url": "https://openreview.net/forum?id=2nIAtsUC27",
      "pdf_url": "https://openreview.net/pdf/1494d847179a71c38929ca5d87774036b701f164.pdf",
      "primary_category": "Multimodal Large Language Model, Video Large Language Model",
      "categories": [
        "Multimodal Large Language Model",
        "Video Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZF93vyH9He",
      "title": "SeRL: Self-play Reinforcement Learning for Large Language Models with Limited Data",
      "authors": [
        "Wenkai Fang",
        "Shunyu Liu",
        "Yang Zhou",
        "Kongcheng Zhang",
        "Tongya Zheng",
        "Kaixuan Chen",
        "Mingli Song",
        "Dacheng Tao"
      ],
      "abstract": "Recent advances have demonstrated the effectiveness of Reinforcement Learning (RL) in improving the reasoning capabilities of Large Language Models (LLMs). However, existing works inevitably rely on high-quality instructions and verifiable rewards for effective training, both of which are often difficult to obtain in specialized domains. In this paper, we propose Self-play Reinforcement Learning (SeRL) to bootstrap LLM training with limited initial data. Specifically, SeRL comprises two complementary modules: self-instruction and self-rewarding. The former module generates additional instructions based on the available data at each training step, employing comprehensive online filtering strategies to ensure instruction quality, diversity, and difficulty. The latter module introduces a simple yet effective majority-voting mechanism to estimate response rewards for additional instructions, eliminating the need for external annotations. Finally, SeRL performs conventional RL based on the generated data, facilitating iterative self-play learning.\nExtensive experiments on various reasoning benchmarks and across different LLM backbones demonstrate that the proposed SeRL yields results superior to its counterparts and achieves performance on par with those obtained by high-quality data with verifiable rewards. Our code is available at https://github.com/wantbook-book/SeRL.",
      "arxiv_url": "https://openreview.net/forum?id=ZF93vyH9He",
      "pdf_url": "https://openreview.net/pdf/3e59acc887cd60e5ebe0a7e1815f8a997fb77cdf.pdf",
      "primary_category": "Self-Instruction;Self-Rewarding, Self-Play, Reinforcement Learning",
      "categories": [
        "Self-Instruction;Self-Rewarding",
        "Self-Play",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Kq08RIeXxI",
      "title": "Panoptic Captioning: An Equivalence Bridge for Image and Text",
      "authors": [
        "Kun-Yu Lin",
        "Hongjun Wang",
        "Weining Ren",
        "Kai Han"
      ],
      "abstract": "This work introduces panoptic captioning, a novel task striving to seek the minimum text equivalent of images, which has broad potential applications. We take the first step towards panoptic captioning by formulating it as a task of generating a comprehensive textual description for an image, which encapsulates all entities, their respective locations and attributes, relationships among entities, as well as global image state. Through an extensive evaluation, our work reveals that state-of-the-art Multi-modal Large Language Models (MLLMs) have limited performance in solving panoptic captioning. To address this, we propose an effective data engine named PancapEngine to produce high-quality data and a novel method named PancapChain to improve panoptic captioning. Specifically, our PancapEngine first detects diverse categories of entities in images by an elaborate detection suite, and then generates required panoptic captions using entity-aware prompts.\nAdditionally, our PancapChain explicitly decouples the challenging panoptic captioning task into multiple stages and generates panoptic captions step by step. More importantly, we contribute a comprehensive metric named PancapScore and a human-curated test set for reliable model evaluation. Experiments show that our PancapChain-13B model can beat state-of-the-art open-source MLLMs like InternVL-2.5-78B and even surpass proprietary models like GPT-4o and Gemini-2.0-Pro, demonstrating the effectiveness of our data engine and method.\nProject page: https://visual-ai.github.io/pancap/",
      "arxiv_url": "https://openreview.net/forum?id=Kq08RIeXxI",
      "pdf_url": "https://openreview.net/pdf/b30da9592486881692200dd1c70f5e903f6ebf66.pdf",
      "primary_category": "Image Captioning, Comprehensive Image Captioning",
      "categories": [
        "Image Captioning",
        "Comprehensive Image Captioning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RmqWt1btxQ",
      "title": "Transcending Cost-Quality Tradeoff in Agent Serving via Session-Awareness",
      "authors": [
        "Yanyu Ren",
        "Li Chen",
        "Dan Li",
        "Xizheng Wang",
        "Zhiyuan Wu",
        "Yukai Miao",
        "Yu Bai"
      ],
      "abstract": "Large Language Model (LLM) agents are capable of task execution across various domains by autonomously interacting with environments and refining LLM responses based on feedback.\nHowever, existing model serving systems are not optimized for the unique demands of serving agents. Compared to classic model serving, agent serving has different characteristics:\npredictable request pattern, increasing quality requirement, and unique prompt formatting. We identify a key problem for agent serving: LLM serving systems lack session-awareness. They neither perform effective KV cache management nor precisely select the cheapest yet competent model in each round.\nThis leads to a cost-quality tradeoff, and we identify an opportunity to surpass it in an agent serving system.\n\nTo this end, we introduce AgServe for AGile AGent SERVing.\nAgServe features a session-aware server that boosts KV cache reuse via Estimated-Time-of-Arrival-based eviction and in-place positional embedding calibration, a quality-aware client that performs session-aware model cascading through real-time quality assessment, and a dynamic resource scheduler that maximizes GPU utilization. \nWith AgServe, we allow agents to select and upgrade models during the session lifetime, and to achieve similar quality at much lower costs, effectively transcending the tradeoff. Extensive experiments on real testbeds demonstrate that AgServe (1) achieves comparable response quality to GPT-4o at a 16.5\\% cost. (2) delivers 1.8$\\times$ improvement in quality relative to the tradeoff curve.",
      "arxiv_url": "https://openreview.net/forum?id=RmqWt1btxQ",
      "pdf_url": "https://openreview.net/pdf/27eca12a3d907e412808e51efe4a0b94f9e7353a.pdf",
      "primary_category": "Agent Serving, LLM Serving, LLM infrastructure",
      "categories": [
        "Agent Serving",
        "LLM Serving",
        "LLM infrastructure",
        "Large language models",
        "inference optimizations"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Xh3AIqtOHM",
      "title": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling",
      "authors": [
        "Tongyao Zhu",
        "Qian Liu",
        "Haonan Wang",
        "Shiqi Chen",
        "Xiangming Gu",
        "Tianyu Pang",
        "Min-Yen Kan"
      ],
      "abstract": "Recent advancements in LLM pretraining have featured ever-expanding context windows to process longer sequences. However, our controlled study reveals that models pretrained with shorter context windows consistently outperform their long-context counterparts under a fixed token budget. This finding motivates us to explore an optimal context window scheduling strategy to better balance long-context capability with pretraining efficiency. To this end, we propose SkyLadder, a simple yet effective approach that implements a short-to-long context window transition. SkyLadder preserves strong standard benchmark performance, while matching or exceeding baseline results on long-context tasks. Through extensive experiments, we pretrain 1B-parameter models (up to 32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating that SkyLadder yields consistent gains of up to 3.7% on common benchmarks, while achieving up to 22% faster training speeds compared to baselines.",
      "arxiv_url": "https://openreview.net/forum?id=Xh3AIqtOHM",
      "pdf_url": "https://openreview.net/pdf/66cbd2bd5a19c2cc0d7e00af667a4bff9512a722.pdf",
      "primary_category": "pretraining, language models, context window",
      "categories": [
        "pretraining",
        "language models",
        "context window"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DorAT49sxj",
      "title": "WALL-E: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents",
      "authors": [
        "Siyu Zhou",
        "Tianyi Zhou",
        "Yijun Yang",
        "Guodong Long",
        "Deheng Ye",
        "Jing Jiang",
        "Chengqi Zhang"
      ],
      "abstract": "Can we build accurate world models out of large language models (LLMs)? How can world models benefit LLM agents? The gap between the prior knowledge of LLMs and the specified environment's dynamics usually bottlenecks LLMs' performance as world models. To bridge the gap, we propose a training-free \"world alignment\" that learns an environment's symbolic knowledge complementary to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and scene graphs, which are extracted by LLMs from exploration trajectories and encoded into executable codes to regulate LLM agents' policies. We further propose an RL-free, model-based agent \"WALL-E\" through the model-predictive control (MPC) framework. Unlike classical MPC requiring costly optimization on the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future steps' actions by interacting with the neurosymbolic world model. While the LLM agent's strong heuristics make it an efficient planner in MPC, the quality of its planned actions is also secured by the accurate predictions of the aligned world model. They together considerably improve learning efficiency in a new environment. On open-world challenges in Mars (Minecraft like) and ALFWorld (embodied indoor environments), WALL-E significantly outperforms existing methods, e.g., surpassing baselines in Mars by 16.1%–51.6% of success rate and by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success rate after only 4 iterations.",
      "arxiv_url": "https://openreview.net/forum?id=DorAT49sxj",
      "pdf_url": "https://openreview.net/pdf/4a70c0605cac16a41fee061564452113f886243a.pdf",
      "primary_category": "world model, embodied agent, large language model",
      "categories": [
        "world model",
        "embodied agent",
        "large language model",
        "neurosymbolic learning"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "x2Rk0lSQra",
      "title": "AutoData: A Multi-Agent System for Open Web Data Collection",
      "authors": [
        "Tianyi Ma",
        "Yiyue Qian",
        "Zheyuan Zhang",
        "Zehong Wang",
        "Xiaoye Qian",
        "Feifan Bai",
        "Yifan Ding",
        "Xuwei Luo",
        "Shinan Zhang",
        "Keerthiram Murugesan",
        "Chuxu Zhang",
        "Yanfang Ye"
      ],
      "abstract": "The exponential growth of data-driven systems and AI technologies has intensified the demand for high-quality web-sourced datasets. \nWhile existing datasets have proven valuable, conventional web data collection approaches face significant limitations in terms of human effort and scalability. \nCurrent data collecting solutions fall into two categories: wrapper-based methods that struggle with adaptability and reproducibility, and large language model (LLM)-based approaches that incur substantial computational and financial costs. \nTo address these challenges, we propose AutoData, a novel multi-agent system for Automated web Data collection, that requires minimal human intervention, i.e., only necessitating a natural language instruction specifying the desired dataset. \nIn addition, AutoData is designed for a robust multi-agent architecture, featuring a novel oriented message hypergraph coordinated by a central task manager, to efficiently organize agents across research and development squads. \nBesides, we introduce a novel hypergraph cache system to advance the multi-agent collaboration process that enables efficient automated data collection and mitigates the token cost issues prevalent in existing LLM-based systems. \nMoreover, we introduce Instruct2DS, a new benchmark dataset supporting live data collection from web sources across three domains: academic, finance, and sports. \nComprehensive evaluations over Instruct2DS and three existing benchmark datasets demonstrate AutoData's superior performance compared to baseline methods.\nCase studies on challenging tasks such as picture book collection and paper extraction from surveys further validate its applicability.",
      "arxiv_url": "https://openreview.net/forum?id=x2Rk0lSQra",
      "pdf_url": "https://openreview.net/pdf/066a04337fe56fee38b21d8e6b2366fac00856f4.pdf",
      "primary_category": "Multi-Agents, LLMs, Web Data Collection",
      "categories": [
        "Multi-Agents",
        "LLMs",
        "Web Data Collection"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MdqirFiD38",
      "title": "Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling",
      "authors": [
        "Yichuan Cao",
        "Yibo Miao",
        "Xiao-Shan Gao",
        "Yinpeng Dong"
      ],
      "abstract": "Text-to-image (T2I) models raise ethical and safety concerns due to their potential to generate inappropriate or harmful images. Evaluating these models' security through red-teaming is vital, yet white-box approaches are limited by their need for internal access, complicating their use with closed-source models. Moreover, existing black-box methods often assume knowledge about the model's specific defense mechanisms, limiting their utility in real-world commercial API scenarios. A significant challenge is how to evade unknown and diverse defense mechanisms. To overcome this difficulty, we propose a novel Rule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively employs LLM to modify prompts to query and leverages feedback from T2I systems for fine-tuning the LLM. \nRPG-RT treats the feedback from each iteration as a prior, enabling the LLM to dynamically adapt to unknown defense mechanisms. Given that the feedback is often labeled and coarse-grained, making it difficult to utilize directly,  we further propose rule-based preference modeling, which employs a set of rules to evaluate desired or undesired feedback, facilitating finer-grained control over the LLM’s dynamic adaptation process. Extensive experiments on nineteen T2I systems with varied safety mechanisms, three online commercial API services, and T2V models verify the superiority and practicality of our approach. Our codes are available at:  https://github.com/caosip/RPG-RT.",
      "arxiv_url": "https://openreview.net/forum?id=MdqirFiD38",
      "pdf_url": "https://openreview.net/pdf/8a90419f8359b430b5fe048ea28b86732da23e42.pdf",
      "primary_category": "Text-to-Image Systems, Rule-based Preference Modeling, Red-Teaming",
      "categories": [
        "Text-to-Image Systems",
        "Rule-based Preference Modeling",
        "Red-Teaming"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "LbNL8xGai2",
      "title": "S'MoRE: Structural Mixture of Residual Experts for Parameter-Efficient LLM Fine-tuning",
      "authors": [
        "Hanqing Zeng",
        "Yinglong Xia",
        "Zhuokai Zhao",
        "Chuan Jiang",
        "Qiang Zhang",
        "Jiayi Liu",
        "Qunshu Zhang",
        "Lizhu Zhang",
        "Xiangjun Fan",
        "Benyu Zhang"
      ],
      "abstract": "Fine-tuning pre-trained large language models (LLMs) presents a dual challenge of balancing parameter efficiency and model capacity. Existing methods like low-rank adaptations (LoRA) are efficient but lack flexibility, while Mixture-of-Experts (MoE) enhance model capacity at the cost of more & under-utilized parameters. To address these limitations, we propose Structural Mixture of Residual Experts (S’MoRE), a novel framework that seamlessly integrates the efficiency of LoRA with the flexibility of MoE. Conceptually, S’MoRE employs hierarchical low-rank decomposition of expert weights, yielding residuals of varying orders interconnected in a multi-layer structure. By routing input tokens through sub-trees of residuals, S’MoRE emulates the capacity of numerous experts by instantiating and assembling just a few low-rank matrices. We craft the inter-layer propagation of S’MoRE’s residuals as a special type of Graph Neural Network (GNN), and prove that under similar parameter budget, S’MoRE improves structural flexibility of traditional MoE (or Mixture-of-LoRA) by exponential order. Comprehensive theoretical analysis and empirical results demonstrate that S’MoRE achieves superior fine-tuning performance, offering a transformative approach for efficient LLM adaptation. Our implementation is available at: https://github.com/ZimpleX/SMoRE-LLM.",
      "arxiv_url": "https://openreview.net/forum?id=LbNL8xGai2",
      "pdf_url": "https://openreview.net/pdf/6a52d860cb672fec3ef5dfb8132b7346892361d0.pdf",
      "primary_category": "Mixture-of-experts, low-rank adaptation, language models",
      "categories": [
        "Mixture-of-experts",
        "low-rank adaptation",
        "language models",
        "graph neural networks"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "d1dL1ymD6N",
      "title": "GoRA: Gradient-driven Adaptive Low Rank Adaptation",
      "authors": [
        "haonan he",
        "Peng Ye",
        "Yuchen Ren",
        "yuan yuan",
        "LuyangZhou",
        "ShucunJu",
        "lei chen"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning large language models (LLMs), with its effectiveness influenced by two key factors: rank selection and weight initialization. While numerous LoRA variants have been proposed to improve performance by addressing one of these aspects, they often compromise usability or computational efficiency. In this paper, we analyze and identify the core limitations of existing approaches and propose a novel framework—**GoRA** (**G**radient-driven Adaptive L**o**w **R**ank **A**daptation)—that simultaneously adapts both the rank and initialization strategy within a unified framework. GoRA leverages gradient information during training to dynamically assign optimal ranks and initialize low-rank adapter weights in an adaptive manner. To our knowledge, GoRA is the first method that not only addresses the limitations of prior approaches—which often focus on either rank selection or initialization in isolation—but also unifies both aspects within a single framework, enabling more effective and efficient adaptation. Extensive experiments across various architectures and modalities show that GoRA consistently outperforms existing LoRA-based methods while preserving the efficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for mathematical reasoning, GoRA achieves a 5.13-point improvement over standard LoRA and even outperforms full fine-tuning by 2.05 points under high-rank settings. Code is available at: https://github.com/hhnqqq/MyTransformers.",
      "arxiv_url": "https://openreview.net/forum?id=d1dL1ymD6N",
      "pdf_url": "https://openreview.net/pdf/8fcd7715235c728bec5bdc382aecf6a74347bcfa.pdf",
      "primary_category": "LLMs, LoRA, PEFT",
      "categories": [
        "LLMs",
        "LoRA",
        "PEFT"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "D88yhLFzDR",
      "title": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement",
      "authors": [
        "Ruihan Yang",
        "Fanghua Ye",
        "Jian Li",
        "Siyu Yuan",
        "Yikai Zhang",
        "Zhaopeng Tu",
        "Xiaolong Li",
        "Deqing Yang"
      ],
      "abstract": "Large language models (LLMs) have recently transformed from text-based assistants to autonomous agents capable of planning, reasoning, and iteratively improving their actions. While numerical reward signals and verifiers can effectively rank candidate actions, they often provide limited contextual guidance. In contrast, natural language feedback better aligns with the generative capabilities of LLMs, providing richer and more actionable suggestions. However, parsing and implementing this feedback effectively can be challenging for LLM-based agents. In this work, we introduce Critique-Guided Improvement (CGI), a novel two-player framework, comprising an actor model that explores an environment and a critic model that generates detailed nature language feedback. By training the critic to produce fine-grained assessments and actionable revisions, and the actor to utilize these critiques, our approach promotes more robust exploration of alternative strategies while avoiding local optima. Experiments in three interactive environments show that CGI outperforms existing baselines by a substantial margin. Notably, even a small critic model surpasses GPT-4 in feedback quality. The resulting actor achieves state-of-the-art performance, demonstrating the power of explicit iterative guidance to enhance decision-making in LLM-based agents.",
      "arxiv_url": "https://openreview.net/forum?id=D88yhLFzDR",
      "pdf_url": "https://openreview.net/pdf/ae3bf83043c88b4e28668b095624b19cc07ed197.pdf",
      "primary_category": "Large language model, Language agent, Agent learning",
      "categories": [
        "Large language model",
        "Language agent",
        "Agent learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZdQhQOXVku",
      "title": "Make Information Diffusion Explainable: LLM-based Causal Framework for Diffusion Prediction",
      "authors": [
        "Wenbo Shang",
        "Zihan Feng",
        "Yajun Yang",
        "Xin Huang"
      ],
      "abstract": "Information diffusion prediction, which aims to forecast future infected users during the information spreading process on social platforms, is a challenging and critical task for public opinion analysis. With the development of social platforms, mass communication has become increasingly widespread. However, most existing methods based on GNNs and sequence models mainly focus on structural and temporal patterns in social networks, suffering from spurious diffusion connections and insufficient information for the diffusion analysis. We leverage strong reasoning capability of LLMs and develop a LL**M**-based causal framework for d**i**ffusion inf**l**uence **d**erivation (MILD). Comprehensively integrating four key factors of social diffusion, i.e., connections, active timelines, user profiles, and comments, MILD causally infers authentic diffusion links to construct a diffusion influence graph $G_I$. To validate the quality and reliability of our constructed graph $G_I$, we proposed a newly designed set of evaluation metrics w.r.t. diffusion prediction. We show MILD provides a reliable information diffusion structure that 12% absolutely better than the social network structure and achieves the state-of-the-art performance on diffusion prediction. MILD is expected to contribute to high-quality, more explainable, and more trustworthy public opinion analysis.",
      "arxiv_url": "https://openreview.net/forum?id=ZdQhQOXVku",
      "pdf_url": "https://openreview.net/pdf/c9793442f1825fb73bd7e46bda7880de99405d53.pdf",
      "primary_category": "data mining, social networks, information diffusion",
      "categories": [
        "data mining",
        "social networks",
        "information diffusion",
        "large language model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DpeJYRFRQY",
      "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing",
      "authors": [
        "Tianyu Fu",
        "Yi Ge",
        "Yichen You",
        "Enshu Liu",
        "Zhihang Yuan",
        "Guohao Dai",
        "Shengen Yan",
        "Huazhong Yang",
        "Yu Wang"
      ],
      "abstract": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token router that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6×, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8× wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency.",
      "arxiv_url": "https://openreview.net/forum?id=DpeJYRFRQY",
      "pdf_url": "https://openreview.net/pdf/14de671242c3654992d79919aba5d312e05f7347.pdf",
      "primary_category": "LLM, efficient reasoning, long context",
      "categories": [
        "LLM",
        "efficient reasoning",
        "long context"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "W0sqoTL7rL",
      "title": "Distilling LLM Prior to Flow Model for Generalizable Agent’s Imagination in Object Goal Navigation",
      "authors": [
        "Badi Li",
        "Ren-Jie Lu",
        "Yu Zhou",
        "Jingke Meng",
        "Wei-Shi Zheng"
      ],
      "abstract": "The Object Goal Navigation (ObjectNav) task challenges agents to locate a specified object in an unseen environment by imagining unobserved regions of the scene. Prior approaches rely on deterministic and discriminative models to complete semantic maps, overlooking the inherent uncertainty in indoor layouts and limiting their ability to generalize to unseen environments. In this work, we propose GOAL, a generative flow-based framework that models the semantic distribution of indoor environments by bridging observed regions with LLM-enriched full-scene semantic maps. During training, spatial priors inferred from large language models (LLMs) are encoded as two-dimensional Gaussian fields and injected into target maps, distilling rich contextual knowledge into the flow model and enabling more generalizable completions. Extensive experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D and Gibson, and shows strong generalization in transfer settings to HM3D.",
      "arxiv_url": "https://openreview.net/forum?id=W0sqoTL7rL",
      "pdf_url": "https://openreview.net/pdf/fd4b1ee5099804b40602b7867c724285c5791fe0.pdf",
      "primary_category": "Visual Navigation; Object Goal Navigation",
      "categories": [
        "Visual Navigation; Object Goal Navigation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cWlRC064GK",
      "title": "Understanding Differential Transformer Unchains Pretrained Self-Attentions",
      "authors": [
        "Chaerin Kong",
        "Jiho Jang",
        "Nojun Kwak"
      ],
      "abstract": "Differential Transformer has recently gained significant attention for its impressive empirical performance, often attributed to its ability to perform noise canceled attention. However, precisely how differential attention achieves its empirical benefits remains poorly understood. Moreover, Differential Transformer architecture demands large-scale training from scratch, hindering utilization of open pretrained weights. In this work, we conduct an in-depth investigation of Differential Transformer, uncovering three key factors behind its success: (1) enhanced expressivity via negative attention, (2) reduced redundancy among attention heads, and (3) improved learning dynamics. Based on these findings, we propose DEX, a novel method to efficiently integrate the advantages of differential attention into pretrained language models. By reusing the softmax attention scores and adding a lightweight differential operation on the output value matrix, DEX effectively incorporates the key advantages of differential attention while remaining lightweight in both training and inference. Evaluations confirm that DEX substantially improves the pretrained LLMs across diverse benchmarks, achieving significant performance gains with minimal adaptation data (< 0.01%).",
      "arxiv_url": "https://openreview.net/forum?id=cWlRC064GK",
      "pdf_url": "https://openreview.net/pdf/ba097db202ac1032f479a9f4c8c7e8a9ea4cbab6.pdf",
      "primary_category": "Attention, Transformer, Language Models",
      "categories": [
        "Attention",
        "Transformer",
        "Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6BHDre6WQW",
      "title": "Hierachical Balance Packing: Towards Efficient Supervised Fine-tuning for Long-Context LLM",
      "authors": [
        "Yongqiang Yao",
        "Jingru Tan",
        "Kaihuan Liang",
        "Feizhao Zhang",
        "Jiahao Hu",
        "Shuo Wu",
        "Yazhe Niu",
        "Ruihao Gong",
        "Dahua Lin",
        "Ningyi Xu"
      ],
      "abstract": "Training Long-Context Large Language Models (LLMs) is challenging, as hybrid training with long-context and short-context data often leads to workload imbalances. Existing works mainly use data packing to alleviate this issue, but fail to consider imbalanced attention computation and wasted communication overhead. This paper proposes Hierarchical Balance Packing (HBP), which designs a novel batch-construction method and training recipe to address those inefficiencies. In particular, the HBP constructs multi-level data packing groups, each optimized with a distinct packing length. It assigns training samples to their optimal groups and configures each group with the most effective settings, including sequential parallelism degree and gradient checkpointing configuration. To effectively utilize multi-level groups of data, we design a dynamic training pipeline specifically tailored to HBP, including curriculum learning, adaptive sequential parallelism, and stable loss. Our extensive experiments demonstrate that our method significantly reduces training time over multiple datasets and open-source models while maintaining strong performance. For the largest DeepSeek-V2 (236B) MoE model, our method speeds up the training by 2.4$\\times$ with competitive performance. Codes will be released at https://github.com/ModelTC/HBP.",
      "arxiv_url": "https://openreview.net/forum?id=6BHDre6WQW",
      "pdf_url": "https://openreview.net/pdf/3d55471711afe8f6378c0c3608b71d8ba929d827.pdf",
      "primary_category": "Deep Learning, LLM, SFT",
      "categories": [
        "Deep Learning",
        "LLM",
        "SFT",
        "LongContext"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "paRLw86ONU",
      "title": "Disentangled Concepts Speak Louder Than Words: Explainable Video Action Recognition",
      "authors": [
        "Jongseo Lee",
        "Wooil Lee",
        "Gyeong-Moon Park",
        "Seong Tae Kim",
        "Jinwoo Choi"
      ],
      "abstract": "Effective explanations of video action recognition models should disentangle how movements unfold over time from the surrounding spatial context. However, existing methods—based on saliency—produce entangled explanations, making it unclear whether predictions rely on motion or spatial context. Language-based approaches offer structure but often fail to explain motions due to their tacit nature—intuitively understood but difficult to verbalize. To address these challenges, we propose Disentangled Action aNd Context concept-based Explainable (DANCE) video action recognition, a framework that predicts actions through disentangled concept types: motion dynamics, objects, and scenes. We define motion dynamics concepts as human pose sequences. We employ a large language model to automatically extract object and scene concepts. Built on an ante-hoc concept bottleneck design, DANCE enforces prediction through these concepts. Experiments on four datasets—KTH, Penn Action, HAA500, and UCF101—demonstrate that DANCE significantly improves explanation clarity with competitive performance. Through a user study, we validate the superior interpretability of DANCE. Experimental results also show that DANCE is beneficial for model debugging, editing, and failure analysis.",
      "arxiv_url": "https://openreview.net/forum?id=paRLw86ONU",
      "pdf_url": "https://openreview.net/pdf/05eaafcdeb34f3b0e0e2dddcdf13404fa228f9a4.pdf",
      "primary_category": "Video Action Recognition, Explainable AI, XAI",
      "categories": [
        "Video Action Recognition",
        "Explainable AI",
        "XAI",
        "Concept",
        "Disentangling",
        "Motion Dynamics",
        "Context"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "W0eTcFyx44",
      "title": "ChemOrch: Empowering LLMs with Chemical Intelligence via Groundbreaking Synthetic Instructions",
      "authors": [
        "Yue Huang",
        "Zhengzhe Jiang",
        "Xiaonan Luo",
        "Kehan Guo",
        "Haomin Zhuang",
        "Yujun Zhou",
        "Zhengqing Yuan",
        "Xiaoqi Sun",
        "Jules Schleinitz",
        "Yanbo Wang",
        "Shuhao Zhang",
        "Mihir Surve",
        "Nitesh V Chawla",
        "Olaf Wiest",
        "Xiangliang Zhang"
      ],
      "abstract": "Empowering large language models (LLMs) with chemical intelligence remains a challenge due to the scarcity of high-quality, domain-specific instruction-response datasets and the misalignment of existing synthetic data generation pipelines with the inherently hierarchical and rule-governed structure of chemical information. To address this, we propose ChemOrch, a framework that synthesizes chemically grounded instruction–response pairs through a two-stage process: task-controlled instruction generation and tool-aware response construction. ChemOrch enables controllable diversity and levels of difficulty for the generated tasks and ensures response precision through tool planning \\& distillation, and tool-based self-repair mechanisms. The effectiveness of ChemOrch is evaluated based on: 1)  the \\textbf{high quality} of generated instruction data, demonstrating superior diversity and strong alignment with chemical constraints; 2)  the \\textbf{dynamic generation of evaluation tasks} that more effectively reveal LLM weaknesses in chemistry; and 3)  the significant \\textbf{improvement of LLM chemistry capabilities} when the generated instruction data are used for fine-tuning. Our work thus represents a critical step toward scalable and verifiable chemical intelligence in LLMs. The code is available at \\url{https://anonymous.4open.science/r/ChemOrch-854A}.",
      "arxiv_url": "https://openreview.net/forum?id=W0eTcFyx44",
      "pdf_url": "https://openreview.net/pdf/7ae0dc5774ca621bbaf89437f6a1da5a3540d602.pdf",
      "primary_category": "Chemistry, Large Language Model, Synthetic Data",
      "categories": [
        "Chemistry",
        "Large Language Model",
        "Synthetic Data"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "EBwfFrw5VA",
      "title": "Cognitive Mirrors: Exploring the Diverse Functional Roles of Attention Heads in LLM Reasoning",
      "authors": [
        "Xueqi Ma",
        "Jun Wang",
        "Yanbei Jiang",
        "Sarah Monazam Erfani",
        "Tongliang Liu",
        "James Bailey"
      ],
      "abstract": "Large language models (LLMs) have achieved state-of-the-art performance in a variety of tasks, but remain largely opaque in terms of their internal mechanisms. Understanding these mechanisms is crucial to improve their reasoning abilities. Drawing inspiration from the interplay between neural processes and human cognition, we propose a novel interpretability framework to systematically analyze the roles and behaviors of attention heads, which are key components of LLMs. We introduce CogQA, a dataset that decomposes complex questions into step-by-step subquestions with a chain-of-thought design, each associated with specific cognitive functions such as retrieval or logical reasoning. By applying a multi-label probing method, we identify the attention heads responsible for these functions. Our analysis across multiple LLM families reveals that attention heads exhibit functional specialization, characterized as cognitive heads. These cognitive heads exhibit several key properties: they are universally sparse, and vary in number and distribution across different cognitive functions, and they display interactive and hierarchical structures.  We further show that cognitive heads play a vital role in reasoning tasks—removing them leads to performance degradation, while augmenting them enhances reasoning accuracy. These insights offer a deeper understanding of LLM reasoning and suggest important implications for model design, training and fine-tuning strategies.",
      "arxiv_url": "https://openreview.net/forum?id=EBwfFrw5VA",
      "pdf_url": "https://openreview.net/pdf/3a12cd4a82443a5673cf50622b0c8aabba9d6983.pdf",
      "primary_category": "Large language models, interpretability, attention heads",
      "categories": [
        "Large language models",
        "interpretability",
        "attention heads"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tsEHIfv2lM",
      "title": "UniTransfer: Video Concept Transfer via Progressive Spatio-Temporal Decomposition",
      "authors": [
        "guojunlei",
        "Rong Zhang",
        "Tianhang Liu",
        "Hong Li",
        "Zhiyuan Ma",
        "Chi Wang",
        "Weiwei Xu"
      ],
      "abstract": "Recent advancements in video generation models have enabled the creation of diverse and realistic videos, with promising applications in advertising and film production. However, as one of the essential tasks of video generation models, video concept transfer remains significantly challenging.\nExisting methods generally model video as an entirety, leading to limited flexibility and precision when solely editing specific regions or concepts. To mitigate this dilemma, we propose a novel architecture UniTransfer, which introduces both spatial and diffusion timestep decomposition in a progressive paradigm, achieving precise and controllable video concept transfer. Specifically, in terms of spatial decomposition, we decouple videos into three key components: the foreground subject, the background, and the motion flow. Building upon this decomposed formulation, we further introduce a dual-to-single-stream DiT-based architecture for supporting fine-grained control over different components in the videos. We also introduce a self-supervised pretraining strategy based on random masking to enhance the decomposed representation learning from large-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning paradigm, we further revisit the denoising diffusion process and propose a Chain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We decompose the denoising process into three stages of different granularity and leverage large language models (LLMs) for stage-specific instructions to guide the generation progressively. We also curate an animal-centric video dataset called OpenAnimal to facilitate the advancement and benchmarking of research in video concept transfer. \nExtensive experiments demonstrate that our method achieves high-quality and controllable video concept transfer across diverse reference images and scenes, surpassing existing baselines in both visual fidelity and editability.",
      "arxiv_url": "https://openreview.net/forum?id=tsEHIfv2lM",
      "pdf_url": "https://openreview.net/pdf/d81a41b8c19d70ffeb3c52772502edba1f7d287b.pdf",
      "primary_category": "video concept transfer, video editing, controllable video generation",
      "categories": [
        "video concept transfer",
        "video editing",
        "controllable video generation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "65llKR17s4",
      "title": "Scaling Up Parameter Generation: A Recurrent Diffusion Approach",
      "authors": [
        "Kai Wang",
        "Dongwen Tang",
        "Wangbo Zhao",
        "Konstantin Schürholt",
        "Zhangyang Wang",
        "Yang You"
      ],
      "abstract": "Parameter generation has long struggled to match the scale of today's large vision and language models, curbing its broader utility. In this paper, we introduce Recurrent Diffusion for Large-Scale Parameter Generation (RPG), a novel framework that generates full neural network parameters—up to hundreds of millions—on a single GPU. Our approach first partitions a network's parameters into non-overlapping 'tokens', each corresponding to a distinct portion of the model. A recurrent mechanism then learns the inter-token relationships, producing 'prototypes' which serve as conditions for a diffusion process that ultimately synthesizes the full parameters. Across a spectrum of architectures and tasks—including ResNets, ConvNeXts and ViTs on ImageNet-1K and COCO, and even LoRA-based LLMs—RPG achieves performance on par with fully trained networks while avoiding excessive memory overhead. Notably, it generalizes beyond its training set to generate valid parameters for previously unseen tasks, highlighting its flexibility in dynamic and open-ended scenarios. By overcoming the longstanding memory and scalability barriers, RPG serves as a critical advance in 'AI generating AI', potentially enabling efficient weight generation at scales previously deemed infeasible.",
      "arxiv_url": "https://openreview.net/forum?id=65llKR17s4",
      "pdf_url": "https://openreview.net/pdf/d7296047e517ff09e4497884e89f850e18696255.pdf",
      "primary_category": "Parameter Generation",
      "categories": [
        "Parameter Generation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "PMSNd8xTHp",
      "title": "ParetoQ: Improving Scaling Laws in Extremely Low-bit LLM Quantization",
      "authors": [
        "Zechun Liu",
        "Changsheng Zhao",
        "Hanxian Huang",
        "Sijia Chen",
        "Jing Zhang",
        "Jiawei Zhao",
        "Scott Roy",
        "Lisa Jin",
        "Yunyang Xiong",
        "Yangyang Shi",
        "Lin Xiao",
        "Yuandong Tian",
        "Bilge Soran",
        "Raghuraman Krishnamoorthi",
        "Tijmen Blankevoort",
        "Vikas Chandra"
      ],
      "abstract": "The optimal bit-width for achieving the best trade-off between quantized model size and accuracy has been a subject of ongoing debate. While some advocate for 4-bit quantization, others propose that 1.58-bit offers superior results. However, the lack of a cohesive framework for different bits has left such conclusions relatively tenuous. We present ParetoQ, the first unified framework that facilitates rigorous comparisons across 1-bit, 1.58-bit, 2-bit, 3-bit, and 4-bit quantization settings. Our findings reveal a notable learning transition between 2 and 3 bits: For 3-bits and above, the fine-tuned models stay close to their original pre-trained distributions, whereas for learning 2-bit networks or below, the representations change drastically. By optimizing training schemes and refining quantization functions, ParetoQ surpasses all previous methods tailored to specific bit widths. Remarkably, our ParetoQ ternary 600M-parameter model even outperforms the previous SoTA ternary 3B-parameter model in accuracy, using only one-fifth of the parameters. Extensive experimentation shows that ternary, 2-bit, and 3-bit quantization maintains comparable performance in the size-accuracy trade-off and generally exceeds 4-bit and binary quantization. Considering hardware constraints, 2-bit quantization offers promising potential for memory reduction and speedup.",
      "arxiv_url": "https://openreview.net/forum?id=PMSNd8xTHp",
      "pdf_url": "https://openreview.net/pdf/d60552949e4c9d184b694025d9111adfcb953fac.pdf",
      "primary_category": "compression",
      "categories": [
        "compression"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nxnBaaRLnz",
      "title": "Don’t Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models",
      "authors": [
        "Sohyun An",
        "Ruochen Wang",
        "Tianyi Zhou",
        "Cho-Jui Hsieh"
      ],
      "abstract": "While recent success of large reasoning models (LRMs) significantly advanced LLMs' reasoning capability by optimizing the final answer accuracy using reinforcement learning, they may also drastically increase the output length due to *overthinking*—characterized by unnecessarily complex reasoning paths that waste computation and potentially degrade the performance.\nWe hypothesize that such inefficiencies stem from LRMs' limited capability to dynamically select the proper modular reasoning strategies, termed *thinking patterns* at the right position. To investigate this hypothesis, we propose a dynamic optimization framework that segments model-generated reasoning paths into distinct thinking patterns, systematically identifying and promoting beneficial patterns that improve the answer while removing detrimental ones. Empirical analysis confirms that our optimized thinking paths yield more concise yet sufficiently informative trajectories, enhancing reasoning efficiency by reducing attention FLOPs by up to 47% while maintaining accuracy for originally correct responses. Moreover, a non-trivial portion of originally incorrect responses are transformed into correct ones, achieving a 15.6% accuracy improvement with reduced length.\nMotivated by the improvement brought by the optimized thinking paths, we apply a preference optimization technique supported by a pairwise dataset contrasting suboptimal and optimal reasoning paths. Experimental evaluations across multiple mathematical reasoning benchmarks reveal that our method notably reduces computational overhead while simultaneously improving reasoning accuracy, achieving up to a 12% accuracy improvement and reducing token usage from approximately 5,000 to 3,000 tokens.",
      "arxiv_url": "https://openreview.net/forum?id=nxnBaaRLnz",
      "pdf_url": "https://openreview.net/pdf/1b9aa688f3d241f228fbd9b8694bdffb938d1d5f.pdf",
      "primary_category": "Reasoning, Large Language Models, Efficiency",
      "categories": [
        "Reasoning",
        "Large Language Models",
        "Efficiency"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "feLdTALuq3",
      "title": "Evaluating and Learning Optimal Dynamic Treatment Regimes under Truncation by Death",
      "authors": [
        "Sihyung Park",
        "Wenbin Lu",
        "Shu Yang"
      ],
      "abstract": "Truncation by death, a prevalent challenge in critical care, renders traditional dynamic treatment regime (DTR) evaluation inapplicable due to ill-defined potential outcomes. We introduce a principal stratification-based method, focusing on the always-survivor value function. We derive a semiparametrically efficient, multiply robust estimator for multi-stage DTRs, demonstrating its robustness and efficiency. Empirical validation and an application to electronic health records showcase its utility for personalized treatment optimization.",
      "arxiv_url": "https://openreview.net/forum?id=feLdTALuq3",
      "pdf_url": "https://openreview.net/pdf/5f84d33a0dbf67e91d6b840ca59f473d1cd612e0.pdf",
      "primary_category": "Dynamic Treatment Regimes, Panel Data, Principal Stratification",
      "categories": [
        "Dynamic Treatment Regimes",
        "Panel Data",
        "Principal Stratification",
        "Truncation by Death",
        "Multiple Robustness",
        "Nonparametric Efficiency",
        "Personalized Medicine"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4OsgYD7em5",
      "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?",
      "authors": [
        "Yang Yue",
        "Zhiqi Chen",
        "Rui Lu",
        "Andrew Zhao",
        "Zhaokai Wang",
        "Yang Yue",
        "Shiji Song",
        "Gao Huang"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly in mathematics and programming tasks. \nIt is widely believed that, similar to how traditional RL helps agents to explore and learn new strategies, RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed the capacity of the corresponding base models. \nIn this study, we take a critical look at \\textit{the current state of RLVR} by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across diverse model families, RL algorithms, and math/coding/visual reasoning benchmarks, using pass@\\textit{k} at large \\textit{k} values as the evaluation metric.\nWhile RLVR improves sampling efficiency towards the correct path, we surprisingly find that current training does \\emph{not} elicit fundamentally new reasoning patterns.\nWe observe that while RLVR-trained models outperform their base models at smaller values of $k$ (\\eg, $k$=1), base models achieve higher pass@$k$ score when $k$ is large.\nMoreover, we observe that the reasoning capability boundary of LLMs often narrows as RLVR training progresses.\nFurther coverage and perplexity analysis shows that the reasoning paths generated by RLVR models are already included in the base models' sampling distribution, suggesting that their reasoning abilities originate from and are \\textit{bounded} by the base model. \nFrom this perspective, treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in fully leveraging the potential of the base model.\nIn contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model’s reasoning capabilities.\nTaken together, our findings suggest that current RLVR methods have not fully realized the potential of RL to elicit genuinely novel reasoning abilities in LLMs. This underscores the need for improved RL paradigms—such as continual scaling and multi-turn agent-environment interaction—to unlock this potential.",
      "arxiv_url": "https://openreview.net/forum?id=4OsgYD7em5",
      "pdf_url": "https://openreview.net/pdf/c3957c2dc397dd6f7bf1e3da21cebaeca53844af.pdf",
      "primary_category": "reinforcement learning with verifiable reward, LLM reasoning",
      "categories": [
        "reinforcement learning with verifiable reward",
        "LLM reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "o87dDXYLXC",
      "title": "Concept-Guided Interpretability via Neural Chunking",
      "authors": [
        "Shuchen Wu",
        "Stephan Alaniz",
        "Shyamgopal Karthik",
        "Peter Dayan",
        "Eric Schulz",
        "Zeynep Akata"
      ],
      "abstract": "Neural networks are often described as \nblack boxes, reflecting the significant challenge of understanding their internal workings and interactions. We propose a different perspective that challenges the prevailing view: rather than being inscrutable, neural networks exhibit patterns in their raw population activity that mirror regularities in the training data. We refer to this as the \\textit{Reflection Hypothesis} and provide evidence for this phenomenon in both simple recurrent neural networks (RNNs) and complex large language models (LLMs).\nBuilding on this insight, we propose to leverage cognitively-inspired methods of \\textit{chunking} to segment high-dimensional neural population dynamics into interpretable units that reflect underlying concepts.\nWe propose three methods to extract these emerging entities, complementing each other based on label availability and neural data dimensionality. Discrete sequence chunking (DSC) creates a dictionary of entities in a lower-dimensional neural space; population averaging (PA) extracts recurring entities that correspond to known labels; and unsupervised chunk discovery (UCD) can be used when labels are absent. \nWe demonstrate the effectiveness of these methods in extracting entities across varying model sizes, ranging from inducing compositionality in RNNs to uncovering recurring neural population states in large language models with diverse architectures, and illustrate their advantage to other interpretability methods. \nThroughout, we observe a robust correspondence between the extracted entities and concrete or abstract concepts in the sequence. Artificially inducing the extracted entities in neural populations effectively alters the network's generation of associated concepts.\nOur work points to a new direction for interpretability, one that harnesses both cognitive principles and the structure of naturalistic data to reveal the hidden computations of complex learning systems, gradually transforming them from black boxes into systems we can begin to understand.\n\nImplementation and code are publicly available at _https://github.com/swu32/Chunk-Interpretability_",
      "arxiv_url": "https://openreview.net/forum?id=o87dDXYLXC",
      "pdf_url": "https://openreview.net/pdf/3ac76d7b64602e36edc9ac3790e53dd89a4b02f6.pdf",
      "primary_category": "interpretability, LLM, cognitive science",
      "categories": [
        "interpretability",
        "LLM",
        "cognitive science"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6MUgQXkxIC",
      "title": "Personalized Visual Content Generation in Conversational Systems",
      "authors": [
        "Xianquan Wang",
        "Zhaocheng Du",
        "Huibo Xu",
        "Shukang Yin",
        "Yupeng Han",
        "Jieming Zhu",
        "Kai Zhang",
        "Qi Liu"
      ],
      "abstract": "With the rapid progress of large language models (LLMs) and diffusion models, there has been growing interest in personalized content generation. However, current conversational systems often present the same recommended content to all users, falling into the dilemma of \"one-size-fits-all.\" To break this limitation and boost user engagement, in this paper, we introduce PCG (**P**ersonalized Visual **C**ontent **G**eneration), a unified framework for personalizing item images within conversational systems. We tackle two key bottlenecks: the depth of personalization and the fidelity of generated images. Specifically, an LLM-powered Inclinations Analyzer is adopted to capture user likes and dislikes from context to construct personalized prompts. Moreover, we design a dual-stage LoRA mechanism—Global LoRA for understanding task-specific visual style, and Local LoRA for capturing preferred visual elements from conversation history. During training, we introduce the visual content condition method to ensure LoRA learns both historical visual context and maintains fidelity to the original item images. Extensive experiments on benchmark conversational datasets—including objective metrics and GPT-based evaluations—demonstrate that our framework outperforms strong baselines, which highlight its potential to redefine personalization in visual content generation for conversational scenarios like e-commerce and real-world recommendation.",
      "arxiv_url": "https://openreview.net/forum?id=6MUgQXkxIC",
      "pdf_url": "https://openreview.net/pdf/e57f87a48d368b815a5ad0733f0ad5e63fcf13de.pdf",
      "primary_category": "Conversational Systems, Visual Content Generation",
      "categories": [
        "Conversational Systems",
        "Visual Content Generation"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BiEwLgytGu",
      "title": "Multi-Objective Hyperparameter Selection via Hypothesis Testing on Reliability Graphs",
      "authors": [
        "Amirmohammad Farzaneh",
        "Osvaldo Simeone"
      ],
      "abstract": "The selection of hyperparameters, such as prompt templates in large language models (LLMs), must often strike a balance between reliability and cost. In many cases, structural relationships between the expected reliability levels of the hyperparameters can be inferred from prior information and held-out data -- e.g., longer prompt templates may be more detailed and thus more reliable. However, existing hyperparameter selection methods either do not provide formal reliability guarantees or are unable to incorporate structured knowledge in the hyperparameter space. This paper introduces reliability graph-based Pareto testing (RG-PT), a novel multi-objective hyperparameter selection framework that maintains formal reliability guarantees in terms of false discovery rate (FDR), while accounting for known relationships among hyperparameters via a directed acyclic graph. Edges in the graph reflect expected reliability and cost trade-offs among hyperparameters, which are inferred via the Bradley-Terry (BT) ranking model from prior information and held-out data. Experimental evaluations demonstrate that RG-PT significantly outperforms existing methods such as learn-then-test (LTT) and Pareto testing (PT) through a more efficient exploration of the hyperparameter space.",
      "arxiv_url": "https://openreview.net/forum?id=BiEwLgytGu",
      "pdf_url": "https://openreview.net/pdf/6f2a0ac3c9d0111e9cfa9261494d5f34d0c96602.pdf",
      "primary_category": "Hyperparameter Selection, Conformal Prediction, Hypothesis Testing",
      "categories": [
        "Hyperparameter Selection",
        "Conformal Prediction",
        "Hypothesis Testing"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tA0l08qS5N",
      "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL",
      "authors": [
        "Jiarui Yao",
        "Yifan HAO",
        "Hanning Zhang",
        "Hanze Dong",
        "Wei Xiong",
        "Nan Jiang",
        "Tong Zhang"
      ],
      "abstract": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy.",
      "arxiv_url": "https://openreview.net/forum?id=tA0l08qS5N",
      "pdf_url": "https://openreview.net/pdf/c7e078578b86348d9d5cc90c5db280404242f16b.pdf",
      "primary_category": "Large Language Models; Reinforcement Learning; Math Reasoning; Gradient Variance Minimization",
      "categories": [
        "Large Language Models; Reinforcement Learning; Math Reasoning; Gradient Variance Minimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Aiqtzn3Qo2",
      "title": "Discovering Compositional Hallucinations in LVLMs",
      "authors": [
        "Sibei Yang",
        "Ge Zheng",
        "Jiajin Tang",
        "Jiaye Qian",
        "Hanzhuo Huang",
        "Cheng Shi"
      ],
      "abstract": "Large language models (LLMs) and vision-language models (LVLMs) have driven the paradigm shift towards general-purpose foundation models. However, both of them are prone to hallucinations, which compromise their factual accuracy and reliability. While existing research primarily focuses on isolated textual- or visual-centric errors, a critical yet underexplored phenomenon persists in LVLMs: Even neither of textual- or visual centric errors occur, LVLMs often struggle with a new and subtle hallucination mode that arising from composition of them. In this paper, we define this issue as Simple Compositional Hallucination (SCHall). Through an preliminary analysis, we present two key findings: (1) visual abstraction fails under compositional questioning, and (2) visual inputs induce degradation in language processing, leading to hallucinations. To facilitate future research on this phenomenon, we introduce a custom benchmark, SCBench, and propose a novel VLR-distillation method, which serves as the first baseline to effectively mitigate SCHall. Furthermore, experiment results on publicly available benchmarks, including both hallucination-specific and general-purpose ones, demonstrate the effectiveness of our VLR-distillation method.",
      "arxiv_url": "https://openreview.net/forum?id=Aiqtzn3Qo2",
      "pdf_url": "https://openreview.net/pdf/96e01a47f57625c1c899b56cf6c1cbe0fd642344.pdf",
      "primary_category": "Hallucination, LVLMs",
      "categories": [
        "Hallucination",
        "LVLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Zfgvo65gxm",
      "title": "Online Mixture of Experts: No-Regret Learning for Optimal Collective Decision-Making",
      "authors": [
        "Larkin Liu",
        "Jalal Etesami"
      ],
      "abstract": "We explore the use of expert-guided bandit learning, which we refer to as online mixture-of-experts (OMoE). In this setting, given a context, a candidate committee of experts must determine how to aggregate their outputs to achieve optimal results in terms of aggregate accuracy. We propose two algorithms to address this problem. The first algorithm combines aggregate voting with UCB-driven successive elimination, efficiently pruning suboptimal exploration actions. The second algorithm employs an online weighted-majority-voting mechanism, leveraging the respective voting power of each expert proportional to their predictive power. We derive theoretical guarantees for the regret properties in the bandit setting under ideal circumstances, and empirical results are provided accordingly. As a modern study on applications, these methods are applied to the online fine-tuning of a set of expert large language models (LLMs), where after each response, the generative LLM dynamically reweighs its set of experts and/or selects the optimal committee of experts to generate the most accurate response. Our results introduce new methodologies and no-regret guarantees for combining multiple experts to improve on the performance of the an aggregate model overall.",
      "arxiv_url": "https://openreview.net/forum?id=Zfgvo65gxm",
      "pdf_url": "https://openreview.net/pdf/45fa03110976e0eb7eb39277399403215c451cc2.pdf",
      "primary_category": "Online Learning, Mixture of Experts",
      "categories": [
        "Online Learning",
        "Mixture of Experts"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "16BGOheRzm",
      "title": "Decentralized Dynamic Cooperation of Personalized Models for Federated Continual Learning",
      "authors": [
        "Danni Yang",
        "Zhikang Chen",
        "Sen Cui",
        "Mengyue Yang",
        "Ding Li",
        "Abudukelimu Wuerkaixi",
        "Haoxuan Li",
        "Jinke Ren",
        "Mingming Gong"
      ],
      "abstract": "Federated continual learning (FCL) has garnered increasing attention for its ability to support distributed computation in environments with evolving data distributions. However, the emergence of new tasks introduces both temporal and cross-client shifts, making catastrophic forgetting a critical challenge. Most existing works aggregate knowledge from clients into a global model, which may not enhance client performance since irrelevant knowledge could introduce interference, especially in heterogeneous scenarios. Additionally, directly applying decentralized approaches to FCL suffers from ineffective group formation caused by task changes. To address these challenges, we propose a decentralized dynamic cooperation framework for FCL, where clients establish dynamic cooperative learning coalitions to balance the acquisition of new knowledge and the retention of prior learning, thereby obtaining personalized models. To maximize model performance, each client engages in selective cooperation, dynamically allying with others who offer meaningful performance gains. This results in non-overlapping, variable coalitions at each stage of the task. Moreover, we use coalitional affinity game to simulate coalition relationships between clients. By assessing both client gradient coherence and model similarity, we quantify the client benefits derived from cooperation. We also propose a merge-blocking algorithm and a dynamic cooperative evolution algorithm to achieve cooperative and dynamic equilibrium. Comprehensive experiments demonstrate the superiority of our method compared to various baselines. Code is available at: https://github.com/ydn3229/DCFCL.",
      "arxiv_url": "https://openreview.net/forum?id=16BGOheRzm",
      "pdf_url": "https://openreview.net/pdf/71745d0875ac0e82c7a30f9761380e3961762ed9.pdf",
      "primary_category": "federated continual learning, decentralized dynamic cooperation, coalitional affinity game",
      "categories": [
        "federated continual learning",
        "decentralized dynamic cooperation",
        "coalitional affinity game"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "P9gY05BDkW",
      "title": "Rethinking Residual Distribution in Locate-then-Edit Model Editing",
      "authors": [
        "Xiaopeng Li",
        "Shangwen Wang",
        "Shasha Li",
        "Shezheng Song",
        "Bin Ji",
        "Ma Jun",
        "Jie Yu"
      ],
      "abstract": "Model editing enables targeted updates to the knowledge of large language models (LLMs) with minimal retraining. Among existing approaches, locate-then-edit methods constitute a prominent paradigm: they first identify critical layers, then compute residuals at the final critical layer based on the target edit, and finally apply least-squares-based multi-layer updates via $\\textbf{residual distribution}$. While empirically effective, we identify a counterintuitive failure mode: residual distribution, a core mechanism in these methods, introduces weight shift errors that undermine editing precision. Through theoretical and empirical analysis, we show that such errors increase with the distribution distance, batch size, and edit sequence length, ultimately leading to inaccurate or suboptimal edits. To address this, we propose the $\\textbf{B}$oundary $\\textbf{L}$ayer $\\textbf{U}$pdat$\\textbf{E (BLUE)}$ strategy to enhance locate-then-edit methods. Sequential batch editing experiments on three LLMs and two datasets demonstrate that BLUE not only delivers an average performance improvement of 35.59\\%, significantly advancing the state of the art in model editing, but also enhances the preservation of LLMs' general capabilities. Our code is available at https://github.com/xpq-tech/BLUE.",
      "arxiv_url": "https://openreview.net/forum?id=P9gY05BDkW",
      "pdf_url": "https://openreview.net/pdf/d536a0344e3dda76aee2955d0e489d36d729ac40.pdf",
      "primary_category": "Model Editing; Large Language Models; Knowledge Editing; Locate-then-edit",
      "categories": [
        "Model Editing; Large Language Models; Knowledge Editing; Locate-then-edit"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "10s01YrlKp",
      "title": "metaTextGrad: Automatically optimizing language model optimizers",
      "authors": [
        "Guowei Xu",
        "Mert Yuksekgonul",
        "Carlos Guestrin",
        "James Zou"
      ],
      "abstract": "Large language models (LLMs) are increasingly used in learning algorithms, evaluations, and optimization tasks. Recent studies have shown that using LLM-based optimizers to automatically optimize model prompts, demonstrations, predictions themselves, or other components can significantly enhance the performance of AI systems, as demonstrated by frameworks such as DSPy and TextGrad. However, optimizers built on language models themselves are usually designed by humans with manual design choices; optimizers themselves are not optimized. Moreover, these optimizers are general purpose by design, to be useful to a broad audience, and are not tailored for specific tasks. To address these challenges, we propose metaTextGrad, which focuses on designing a meta-optimizer to further enhance existing optimizers and align them to be good optimizers for a given task. Our approach consists of two key components: a meta prompt optimizer and a meta structure optimizer. The combination of these two significantly improves performance across multiple benchmarks, achieving an average absolute performance improvement of up to 6% compared to the best baseline.",
      "arxiv_url": "https://openreview.net/forum?id=10s01YrlKp",
      "pdf_url": "https://openreview.net/pdf/48c50b5fa84702c691c8a18feeb4d6d9a1f5106c.pdf",
      "primary_category": "programming models, prompting techniques, meta-learning",
      "categories": [
        "programming models",
        "prompting techniques",
        "meta-learning",
        "LLM optimizer"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2T6GWekWSm",
      "title": "Beyond Single-Task: Robust Multi-Task Length Generalization for LLMs",
      "authors": [
        "Yi Hu",
        "Shijia Kang",
        "Haotong Yang",
        "Haotian Xu",
        "Muhan Zhang"
      ],
      "abstract": "Length generalization—the ability to solve problems longer than those seen during training—remains a critical challenge for large language models (LLMs). Previous work modifies positional encodings (PEs) and data formats to improve length generalization on specific symbolic tasks such as addition and sorting. However, these approaches are fundamentally limited to special tasks, often degrading general language performance. Furthermore, they are typically evaluated on small transformers trained from scratch on single tasks and can cause performance drop when applied during post-training stage of practical LLMs with general capabilities. Hu et al., (2024) proposed Rule-Following Fine-Tuning (RFFT) to improve length generalization in the post-training stage of LLMs. Despite its compatibility with practical models and strong performance, RFFT is proposed for single tasks too, requiring re-training for each individual task with extensive examples. In this paper, we study length generalization in multi-task settings and propose *Meta Rule-Following Fine-Tuning (Meta-RFFT)*, the first framework enabling robust *cross-task* length generalization. \nAs our first contribution, we construct a large length generalization dataset containing **86 tasks** spanning code execution, number processing, symbolic and logical reasoning tasks, beyond the common addition or multiplication tasks. Secondly, we show that cross-task length generalization is possible with Meta-RFFT—after training on a large number of tasks and instances, the models achieve remarkable length generalization ability on *unseen* tasks with *minimal fine-tuning or one-shot prompting*. For example, after fine-tuning on 1 to 5 digit addition, our 32B model **achieves 95% accuracy on 30 digit addition**, significantly outperforming the state-of-the-art reasoning models (DeepSeek-R1-671B: 72%; QwQ-32B: 32%), despite never seeing this task during RF-pretraining.",
      "arxiv_url": "https://openreview.net/forum?id=2T6GWekWSm",
      "pdf_url": "https://openreview.net/pdf/1d34eb92f4ba33f066e51f82df96949900c2a1da.pdf",
      "primary_category": "length generalization, systematic generalization, algorithmic reasoning",
      "categories": [
        "length generalization",
        "systematic generalization",
        "algorithmic reasoning",
        "scratchpad",
        "LLM reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ml2TynfZI0",
      "title": "Integrating Drug Substructures and Longitudinal Electronic Health Records for Personalized Drug Recommendation",
      "authors": [
        "Wenjie Du",
        "Xuqiang Li",
        "Jinke Feng",
        "Shuai Zhang",
        "Wen Zhang",
        "Yang Wang"
      ],
      "abstract": "Drug recommendation systems aim to identify optimal drug combinations for patient care, balancing therapeutic efficacy and safety. Advances in large-scale longitudinal EHRs have enabled learning-based approaches that leverage patient histories such as diagnoses, procedures, and previously prescribed drugs, to model complex patient-drug relationships. Yet, many existing solutions overlook standard clinical practices that favor certain drugs for specific conditions and fail to fully integrate the influence of molecular substructures on drug efficacy and safety. In response, we propose \\textbf{SubRec}, a unified framework that integrates representation learning across both patient and drug spaces. Specifically, SubRec introduces a conditional information bottleneck to extract core drug substructures most relevant to patient conditions, thereby enhancing interpretability and clinical alignment. Meanwhile, an adaptive vector quantization mechanism is designed to generate patient–drug interaction patterns into a condition-aware codebook which reuses clinically meaningful patterns, reduces training overhead, and provides a controllable latent space for recommendation. Crucially, the synergy between condition-specific substructure learning and discrete patient prototypes allows SubRec to make accurate and personalized drug recommendations. Experimental results on the real-world MIMIC III and IV demonstrate our model's advantages.  \nThe source code is available at \\href{https://anonymous.4open.science/r/DrugRecommendation-5173}{https://anonymous.4open.science/}.",
      "arxiv_url": "https://openreview.net/forum?id=ml2TynfZI0",
      "pdf_url": "https://openreview.net/pdf/d0fa537e13376cf9eb9bd107b7c283ab9dcd7000.pdf",
      "primary_category": "Drug-Drug interaction; Drug Recommendation; Drug Substructures",
      "categories": [
        "Drug-Drug interaction; Drug Recommendation; Drug Substructures"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IUhOGH0WiL",
      "title": "Detoxifying Large Language Models via Autoregressive Reward Guided Representation Editing",
      "authors": [
        "Yisong Xiao",
        "Aishan Liu",
        "Siyuan Liang",
        "Zonghao Ying",
        "Xianglong Liu",
        "Dacheng Tao"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across various tasks, yet they remain vulnerable to generating toxic content, necessitating detoxification strategies to ensure safe and responsible deployment. Test-time detoxification methods, which typically introduce static or dynamic interventions into LLM representations, offer a promising solution due to their flexibility and minimal invasiveness. However, current approaches often suffer from imprecise interventions, primarily due to their insufficient exploration of the transition space between toxic and non-toxic outputs. To address this challenge, we propose \\textsc{A}utoregressive \\textsc{R}eward \\textsc{G}uided \\textsc{R}epresentation \\textsc{E}diting (ARGRE), a novel test-time detoxification framework that explicitly models toxicity transitions within the latent representation space, enabling stable and precise reward-guided editing. ARGRE identifies non-toxic semantic directions and interpolates between toxic and non-toxic representations to reveal fine-grained transition trajectories. These trajectories transform sparse toxicity annotations into dense training signals, enabling the construction of an autoregressive reward model that delivers stable and precise editing guidance. At inference, the reward model guides an adaptive two-step editing process to obtain detoxified representations: it first performs directional steering based on expected reward gaps to shift representations toward non-toxic regions, followed by lightweight gradient-based refinements. Extensive experiments across 8 widely used LLMs show that ARGRE significantly outperforms leading baselines in effectiveness (-62.21\\% toxicity) and efficiency (-47.58\\% inference time), while preserving the core capabilities of the original model with minimal degradation. Our code is available at the \\href{https://anonymous.4open.science/r/ARGRE-6291}{anonymous website}.",
      "arxiv_url": "https://openreview.net/forum?id=IUhOGH0WiL",
      "pdf_url": "https://openreview.net/pdf/583d1a75beabde345a93d945d5550c182450023c.pdf",
      "primary_category": "toxicity mitigation, large language model",
      "categories": [
        "toxicity mitigation",
        "large language model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "f3sZjkQbv2",
      "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs",
      "authors": [
        "Jiaru Zou",
        "Ling Yang",
        "Jingwen Gu",
        "Jiahao Qiu",
        "Ke Shen",
        "Jingrui He",
        "Mengdi Wang"
      ],
      "abstract": "Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory–response outputs generated by frontier reasoning models like Deepseek-R1.  In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling.  Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1\\% in supervised fine-tuning, 4.5\\% in reinforcement learning, and 6.3\\% in test-time scaling. We also release an efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Our code and models are released at https://github.com/Gen-Verse/ReasonFlux.",
      "arxiv_url": "https://openreview.net/forum?id=f3sZjkQbv2",
      "pdf_url": "https://openreview.net/pdf/88d9323f85c36e29c52ce3a3cae948c2b2598eb2.pdf",
      "primary_category": "Process Reward Models, Inference Scaling, LLMs",
      "categories": [
        "Process Reward Models",
        "Inference Scaling",
        "LLMs",
        "Reasoning Trajectories Selection"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "neZSGqhxDa",
      "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
      "authors": [
        "Andrew Zhao",
        "Yiran Wu",
        "Yang Yue",
        "Tong Wu",
        "Quentin Xu",
        "Yang Yue",
        "Matthieu Lin",
        "Shenzhi Wang",
        "Qingyun Wu",
        "Zilong Zheng",
        "Gao Huang"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from rule-based outcome rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external human or distillation data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability. AZR uses a code executor to both validate self-proposed code reasoning tasks and verify answers, serving as an unified source of verifiable feedback to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.",
      "arxiv_url": "https://openreview.net/forum?id=neZSGqhxDa",
      "pdf_url": "https://openreview.net/pdf/fed932448b66316fd6823fb9a3103e478989bfd5.pdf",
      "primary_category": "reasoning, language model, reinforcement learning",
      "categories": [
        "reasoning",
        "language model",
        "reinforcement learning",
        "self-play",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RGUcF6pIZN",
      "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization",
      "authors": [
        "Yize Wu",
        "KE GAO",
        "Ling Li",
        "Yanjun Wu"
      ],
      "abstract": "Speculative decoding is an effective and lossless method for Large Language Model (LLM) inference acceleration. It employs a smaller model to generate a draft token sequence, which is then verified by the original base model. In multi-GPU systems, inference latency can be further reduced through tensor parallelism (TP), while the optimal TP size of the draft model is typically smaller than that of the base model, leading to GPU idling during the drafting stage. We observe that such inefficiency stems from the sequential execution of layers, which is seemingly natural but actually unnecessary. Therefore, we propose EasySpec, a layer-parallel speculation strategy that optimizes the efficiency of multi-GPU utilization. EasySpec breaks the inter-layer data dependencies in the draft model, enabling multiple layers to run simultaneously across multiple devices as ``fuzzy'' speculation. After each drafting-and-verification iteration, the draft model’s key-value cache is calibrated in a single forward pass, preventing long-term fuzzy-error accumulation at minimal additional latency. EasySpec is a training-free and plug-in method. We evaluated EasySpec on several mainstream open-source LLMs, using smaller versions of models from the same series as drafters. The results demonstrate that EasySpec can achieve a peak speedup of 4.17x compared to vanilla decoding, while preserving the original distributions of the base LLMs. Specifically, the drafting stage can be accelerated by up to 1.62x with a maximum speculation accuracy drop of only 7\\%. The code is available at https://github.com/Yize-Wu/EasySpec.",
      "arxiv_url": "https://openreview.net/forum?id=RGUcF6pIZN",
      "pdf_url": "https://openreview.net/pdf/400622f254de773d7e2d32482c818cbf668c53a3.pdf",
      "primary_category": "Large Language Models, inference acceleration, speculative decoding",
      "categories": [
        "Large Language Models",
        "inference acceleration",
        "speculative decoding"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wjXKFrUFzA",
      "title": "Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation",
      "authors": [
        "Jingmin Zhu",
        "Anqi Zhu",
        "Hossein Rahmani",
        "Jun Liu",
        "Mohammed Bennamoun",
        "Qiuhong Ke"
      ],
      "abstract": "We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.",
      "arxiv_url": "https://openreview.net/forum?id=wjXKFrUFzA",
      "pdf_url": "https://openreview.net/pdf/3794dfb917bda6bf195868796c0f389ba7fc9ced.pdf",
      "primary_category": "zero-shot learning, generalized zero-shot learning, skeleton-based action recognition",
      "categories": [
        "zero-shot learning",
        "generalized zero-shot learning",
        "skeleton-based action recognition",
        "test-time adaptation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ceTeM2Xl1n",
      "title": "Improving the Straight-Through Estimator with Zeroth-Order Information",
      "authors": [
        "Ningfeng Yang",
        "Tor M. Aamodt"
      ],
      "abstract": "We study the problem of training neural networks with quantized parameters.\n   Learning low-precision quantized parameters by enabling computation of gradients via the Straight-Through Estimator (STE) can be challenging. \n While the STE enables back-propagation, which is a first-order method, recent works have explored the use of zeroth-order (ZO) gradient descent for fine-tuning.\n  We note that the STE provides high-quality biased gradients, and ZO gradients are unbiased but can be expensive.\n  We thus propose First-Order-Guided Zeroth-Order Gradient Descent (FOGZO) that reduces STE bias while reducing computations relative to ZO methods. \n  Empirically, we show FOGZO improves the tradeoff between quality and training time in Quantization-Aware Pre-Training. \n  Specifically, versus STE at the same number of iterations, we show a 1-8% accuracy improvement for DeiT Tiny/Small, 1-2% accuracy improvement on ResNet 18/50, and 1-22 perplexity point improvement for LLaMA models with up to 0.3 billion parameters.  For the same loss, FOGZO yields a 796$\\times$ reduction in computation versus n-SPSA for a 2-layer MLP on MNIST. Code is available at [https://github.com/1733116199/fogzo](https://github.com/1733116199/fogzo).",
      "arxiv_url": "https://openreview.net/forum?id=ceTeM2Xl1n",
      "pdf_url": "https://openreview.net/pdf/0c1c1e67adcfb2c60df3db3bc69415babdb31507.pdf",
      "primary_category": "quantization, straight-through estimator, quantization-aware training",
      "categories": [
        "quantization",
        "straight-through estimator",
        "quantization-aware training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "A1u6BFAEGx",
      "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints",
      "authors": [
        "Changyao Tian",
        "Hao Li",
        "Gen Luo",
        "Xizhou Zhu",
        "Weijie Su",
        "Hanming Deng",
        "Jinguo Zhu",
        "Jie Shao",
        "Ziran Zhu",
        "Yunpeng Liu",
        "Lewei Lu",
        "Wenhai Wang",
        "Hongsheng Li",
        "Jifeng Dai"
      ],
      "abstract": "Compositional training has been the de-facto paradigm in existing Multimodal Large Language Models (MLLMs), where pre-trained vision encoders are connected with pre-trained  LLMs through continuous multimodal pre-training. However,  the multimodal scaling property of this paradigm remains difficult to explore due to the separated training. In this paper, we focus on the native training of MLLMs in an end-to-end manner  and systematically study its design space and scaling property under a practical setting, i.e., data constraint. Through careful study of various choices in MLLM, we obtain the optimal meta-architecture that best balances performance and training cost. After that, we further explore the scaling properties of the native MLLM and indicate the positively correlated scaling relationship between visual encoders and LLMs.  Based on these findings, we propose a native MLLM called NaViL, combined with a simple and cost-effective recipe.  Experimental results on 14 multimodal benchmarks confirm the competitive performance of NaViL against existing  MLLMs. Besides that, our findings and results provide in-depth insights for the future study of native MLLMs.",
      "arxiv_url": "https://openreview.net/forum?id=A1u6BFAEGx",
      "pdf_url": "https://openreview.net/pdf/f5e6a5257395accc255b80a6854acacd459949a3.pdf",
      "primary_category": "Multimodal Large Language Models, Scaling Properties",
      "categories": [
        "Multimodal Large Language Models",
        "Scaling Properties"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9t7gfYUp0U",
      "title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation",
      "authors": [
        "Haoyang Fang",
        "Boran Han",
        "Nick Erickson",
        "Xiyuan Zhang",
        "Su Zhou",
        "Anirudh Dagar",
        "Jiani Zhang",
        "Ali Caner Turkmen",
        "Cuixiong Hu",
        "Huzefa Rangwala",
        "Ying Nian Wu",
        "Bernie Wang",
        "George Karypis"
      ],
      "abstract": "Existing AutoML systems have advanced the automation of machine learning (ML); however, they still require substantial manual configuration and expert input, particularly when handling multimodal data. We introduce MLZero, a novel multi-agent framework powered by Large Language Models (LLMs) that enables end-to-end ML automation across diverse data modalities with minimal human intervention. A cognitive perception module is first employed, transforming raw multimodal inputs into perceptual context that effectively guides the subsequent workflow. To address key limitations of LLMs, such as hallucinated code generation and outdated API knowledge, we enhance the iterative code generation process with semantic and episodic memory. MLZero demonstrates superior performance on MLE-Bench Lite, outperforming all competitors in both success rate and solution quality, securing six gold medals. Furthermore, when evaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more challenging tasks spanning diverse data modalities, MLZero outperforms the competing methods by a large margin with a success rate of 0.92 (+263.6\\%) and an average rank of 2.28. Our approach maintains its robust effectiveness even with a compact 8B LLM, outperforming full-size systems from existing solutions.",
      "arxiv_url": "https://openreview.net/forum?id=9t7gfYUp0U",
      "pdf_url": "https://openreview.net/pdf/55f28109c8ee532fe1c950142c23f6efd636a79e.pdf",
      "primary_category": "AutoML, Large Language Models, Multi-Agent Systems",
      "categories": [
        "AutoML",
        "Large Language Models",
        "Multi-Agent Systems",
        "End-to-End Machine Learning",
        "Multimodal Learning",
        "Code Generation"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nMLvTru6Zo",
      "title": "Dynamics of Spontaneous Topic Changes in Next Token Prediction with Self-Attention",
      "authors": [
        "Mumin Jia",
        "Jairo Diaz-Rodriguez"
      ],
      "abstract": "Human cognition is punctuated by abrupt, spontaneous shifts between topics—driven by emotional, contextual, or associative cues—a phenomenon known as spontaneous thought in neuroscience. In contrast, self-attention-based models rely on structured patterns over their inputs to predict each next token, lacking spontaneity. Motivated by this distinction, we characterize *spontaneous topic changes* in self-attention architectures and reveal divergences from *spontaneous human thought*. First, we establish theoretical results under a simplified, single-layer self-attention model with suitable conditions by defining a topic as a set of Token Priority Graphs (TPGs). Specifically, we demonstrate that (1) the model maintains the priority order of tokens related to the input topic, (2) a spontaneous topic change can occur only if lower-priority tokens outnumber all higher-priority tokens of the input topic, and (3) unlike human cognition, the longer context length or the more ambiguous input topic does not increase the likelihood of spontaneous change. Second, we empirically validate that the effect of input length or topic ambiguity persists in modern, state-of-the-art LLMs, underscoring a fundamental disparity between human cognition and AI behavior in the context of spontaneous topic changes. To the best of our knowledge, no prior work has explored these questions with a focus so closely aligned to human thought.",
      "arxiv_url": "https://openreview.net/forum?id=nMLvTru6Zo",
      "pdf_url": "https://openreview.net/pdf/14cfe8c6c3c295e538251e37fa172bcea876d6e0.pdf",
      "primary_category": "Self-attention, spontaneous topic changes, spontaneous human thought",
      "categories": [
        "Self-attention",
        "spontaneous topic changes",
        "spontaneous human thought",
        "next token prediction"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4FUdUFvvmp",
      "title": "PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling",
      "authors": [
        "Guilin Li",
        "Yun Zhang",
        "Xiuyuan Chen",
        "Chengqi Li",
        "Bo Wang",
        "Linghe Kong",
        "Wenjia Wang",
        "Weiran Huang",
        "Matthias Hwai Yong Tan"
      ],
      "abstract": "Large language models (LLMs) have shown that generative pretraining can distill vast world knowledge into compact token representations. While LLMs encapsulate extensive world knowledge, they remain limited in modeling the behavioral knowledge contained within user interaction histories. User behavior forms a distinct modality, where each action—defined by multi-dimensional attributes such as time, context, and transaction type—constitutes a behavioral token. Modeling these high-cardinality, sparse, and irregular sequences is challenging, and discriminative models often falter under limited supervision. To bridge this gap, we extend generative pretraining to user behavior, learning transferable representations from unlabeled behavioral data analogous to how LLMs learn from text. We present PANTHER, a hybrid generative–discriminative framework that unifies user behavior pretraining and downstream adaptation, enabling large-scale sequential user representation learning and real-time inference. PANTHER introduces: (1) Structured Tokenization to compress multi-dimensional transaction attributes into an interpretable vocabulary; (2) Sequence Pattern Recognition Module (SPRM) for modeling periodic transaction motifs; (3) a Unified User-Profile Embedding that fuses static demographics with dynamic transaction histories, enabling both personalized predictions and population-level knowledge transfer; and (4) Real-time scalability enabled by offline caching of pre-trained embeddings for millisecond-level inference.Fully deployed and operational online at WeChat Pay, PANTHER delivers a 25.6\\% boost in next-transaction prediction HitRate@1 and a 38.6\\% relative improvement in fraud detection recall over baselines. Cross-domain evaluations on public benchmarks (CCT, MBD, MovieLens-1M, Yelp) show strong generalization, achieving up to 21\\% HitRate@1 gains over transformer baselines, establishing PANTHER as a scalable, high-performance framework for industrial user sequential behavior modeling.",
      "arxiv_url": "https://openreview.net/forum?id=4FUdUFvvmp",
      "pdf_url": "https://openreview.net/pdf/7319e3287fc8a907e4162e090f162e1bba0e4675.pdf",
      "primary_category": "Sequential User Modeling, Fraud Detection, Generative Pretraining",
      "categories": [
        "Sequential User Modeling",
        "Fraud Detection",
        "Generative Pretraining",
        "Transformer Models",
        "Representation Learning"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "bV5is3iodg",
      "title": "AC-LoRA: (Almost) Training-Free Access Control Aware Multi-Modal LLMs",
      "authors": [
        "Lara Magdalena Lazier",
        "Aritra Dhar",
        "Vasilije Stambolic",
        "Lukas Cavigelli"
      ],
      "abstract": "Corporate LLMs are gaining traction for efficient knowledge dissemination and management within organizations. \nHowever, as current LLMs are vulnerable to leaking sensitive information, it has proven difficult to apply them in settings where strict access control is necessary.\nTo this end, we design AC-LoRA, an end-to-end system for access control-aware corporate LLM chatbots that maintains a strong information isolation guarantee.\nAC-LoRA maintains separate LoRA adapters for permissioned datasets, along with the document embedding they are finetuned on.\nAC-LoRA retrieves a precise set of LoRA adapters based on the similarity score with the user query and their permission.\nThis similarity score is later used to merge the responses if more than one LoRA is retrieved, without requiring any additional training for LoRA routing.\nWe provide an end-to-end prototype of AC-LoRA, evaluate it on two datasets, and show that AC-LoRA matches\nor even exceeds the performance of state-of-the-art LoRA mixing techniques while providing strong isolation guarantees.\nFurthermore, we show that AC-LoRA design can be directly applied to different modalities.",
      "arxiv_url": "https://openreview.net/forum?id=bV5is3iodg",
      "pdf_url": "https://openreview.net/pdf/a9b03493edbd28fb516448a7114f6909ad2028dc.pdf",
      "primary_category": "Information access control, Secure LLM, corporate LLM",
      "categories": [
        "Information access control",
        "Secure LLM",
        "corporate LLM",
        "LoRA"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "it0kkaFFpK",
      "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models",
      "authors": [
        "Hoigi Seo",
        "Dong Un Kang",
        "Hyunjin Cho",
        "Joohoon Lee",
        "Se Young Chun"
      ],
      "abstract": "Large vision-language models (LVLMs), which integrate a vision encoder (VE) with a large language model, have achieved remarkable success across various tasks. However, there are still crucial challenges in LVLMs such as object hallucination, generating descriptions of objects that are not in the input image. Here, we argue that uncertain visual tokens within the VE is a key factor that contributes to object hallucination. Our statistical analysis found that there are positive correlations between visual tokens with high epistemic uncertainty and the occurrence of hallucinations. Furthermore, we show theoretically and empirically that visual tokens in early VE layers that exhibit large representation deviations under small adversarial perturbations indicate high epistemic uncertainty. \nBased on these findings, we propose a simple yet effective strategy to mitigate object hallucination by modifying the VE only. Our method comprises a proxy method with adversarial perturbations for identifying uncertain visual tokens efficiently and a method to mask these uncertain visual tokens during the self-attention process in the middle layers of the VE, suppressing their influence on visual encoding and thus alleviating hallucinations. Extensive experiments show that our method significantly reduces object hallucinations in LVLMs and can synergistically work with other prior arts.",
      "arxiv_url": "https://openreview.net/forum?id=it0kkaFFpK",
      "pdf_url": "https://openreview.net/pdf/a1366e8c48915da8e2d2968d792a0de70262f956.pdf",
      "primary_category": "Large Vision-Language Models, Object Hallucinations, Uncertain Visual Tokens",
      "categories": [
        "Large Vision-Language Models",
        "Object Hallucinations",
        "Uncertain Visual Tokens"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "mhARf5VzCn",
      "title": "Low-Rank Head Avatar Personalization with Registers",
      "authors": [
        "Sai Tanmay Reddy Chakkera",
        "Aggelina Chatziagapi",
        "Md Moniruzzaman",
        "Chen-ping Yu",
        "Yi-Hsuan Tsai",
        "Dimitris Samaras"
      ],
      "abstract": "We introduce a novel method for low-rank personalization of a generic model for head avatar generation. Prior work proposes generic models that achieve high-quality face animation by leveraging large-scale datasets of multiple identities. However, such generic models usually fail to synthesize unique identity-specific details, since they learn a general domain prior. To adapt to specific subjects, we find that it is still challenging to capture high-frequency facial details via popular solutions like low-rank adaptation (LoRA). This motivates us to propose a specific architecture, a Register Module, that enhances the performance of LoRA, while requiring only a small number of parameters to adapt to an unseen identity. Our module is applied to intermediate features of a pre-trained model, storing and re-purposing information in a learnable 3D feature space. To demonstrate the efficacy of our personalization method, we collect a dataset of talking videos of individuals with distinctive facial details, such as wrinkles and tattoos. Our approach faithfully captures unseen faces, outperforming existing methods quantitatively and qualitatively.",
      "arxiv_url": "https://openreview.net/forum?id=mhARf5VzCn",
      "pdf_url": "https://openreview.net/pdf/1866c3a2bceb957a8b63409787d8d97cdc134c62.pdf",
      "primary_category": "Personalization, Avatar Generation",
      "categories": [
        "Personalization",
        "Avatar Generation"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QWQB1ReBkJ",
      "title": "Native-Resolution Image Synthesis",
      "authors": [
        "ZiDong Wang",
        "LEI BAI",
        "Xiangyu Yue",
        "Wanli Ouyang",
        "Yiyuan Zhang"
      ],
      "abstract": "We introduce native-resolution image synthesis, a novel paradigm in generative modeling capable of synthesizing images at arbitrary resolutions and aspect ratios. This approach overcomes the limitations of standard fixed-resolution, square-image methods by inherently handling variable-length visual tokens—a core challenge for conventional techniques. To this end, we propose the Native-resolution diffusion Transformer (NiT), an architecture that explicitly models varying resolutions and aspect ratios within its denoising process. Unconstrained by fixed formats, NiT learns intrinsic visual distributions from images encompassing a wide range of resolutions and aspect ratios. Notably, a single NiT model simultaneously achieves the state-of-the-art performance on both ImageNet-256x256 and 512x512 benchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in advanced Large Language Models, NiT, pretrained solely on ImageNet, demonstrates excellent zero-shot generalization performance. It successfully generates high-fidelity images at previously unseen high resolutions (e.g., 1024x1024, 1536x1536) and diverse aspect ratios (e.g., 16:9,3:1, 4:3), as shown in Figure 1. These findings indicate the significant potential of native-resolution modeling as a bridge between visual generative modeling and advanced LLM methodologies.",
      "arxiv_url": "https://openreview.net/forum?id=QWQB1ReBkJ",
      "pdf_url": "https://openreview.net/pdf/68db04c1f92576abbb3a7e06e75d58b298256403.pdf",
      "primary_category": "Native-resolution, Image Synthesis, Diffusion Models",
      "categories": [
        "Native-resolution",
        "Image Synthesis",
        "Diffusion Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "CRyOyiVvvJ",
      "title": "Scalable Fingerprinting of Large Language Models",
      "authors": [
        "Anshul Nasery",
        "Jonathan Hayase",
        "Creston Brooks",
        "Peiyao Sheng",
        "Himanshu Tyagi",
        "Pramod Viswanath",
        "Sewoong Oh"
      ],
      "abstract": "Model fingerprinting has emerged as a powerful tool for model owners to identify their shared model given API access. In order to lower false discovery rate, fight fingerprint leakage, and defend against coalitions of model users attempting to bypass detection, we argue that scaling up the number of fingerprints one can embed into a model, i.e. *Scalability* of fingerprints, is critical. Hence, we pose scalability as a crucial requirement for fingerprinting schemes. \nWe experiment with fingerprint design at a scale significantly larger than previously considered,\nand introduce a new method, dubbed Perinucleus sampling, to generate scalable, persistent, and harmless fingerprints. We demonstrate that this scheme can add 24,576 fingerprints to a Llama-3.1-8B model---two orders of magnitude more than existing schemes---without degrading the model's utility. Our inserted fingerprints persist even after supervised fine-tuning on standard post-training data. We further address security risks for fingerprinting, and theoretically and empirically show how a scalable fingerprinting scheme like ours can mitigate these risks.",
      "arxiv_url": "https://openreview.net/forum?id=CRyOyiVvvJ",
      "pdf_url": "https://openreview.net/pdf/44b31368bb4fa3c7f9b4f68bf8ae423059761c3b.pdf",
      "primary_category": "fingerprinting, model sharing, security",
      "categories": [
        "fingerprinting",
        "model sharing",
        "security",
        "memorization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "po0eyoYFUa",
      "title": "Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?",
      "authors": [
        "Junchi Yu",
        "Yujie Liu",
        "Jindong Gu",
        "Philip Torr",
        "Dongzhan Zhou"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances large language models (LLMs) by providing structured and interpretable external knowledge.\nHowever, existing KG-based RAG methods struggle to retrieve accurate and diverse information from text-rich KGs for complex real-world queries.\nProcess Reward Models (PRMs) offer a way to align the retrieval process of KG-based RAG with query-specific knowledge requirements, \nbut they heavily rely on process-level supervision signals that are expensive and hard to obtain on KGs.\nTo address this challenge, we propose GraphFlow, a framework that efficiently retrieves accurate and diverse knowledge required for real-world queries from text-rich KGs.\nGraphFlow employs a transition-based flow matching objective to jointly optimize a retrieval policy and a flow estimator.\nThe flow estimator factorizes the reward of the retrieval outcome into the intermediate retrieval states.\nSuch reward factorization guides the retrieval policy to retrieve candidates from KGs in proportion to their reward.\nThis allows GraphFlow to explore high-quality regions of KGs that yield diverse and relevant results. \nWe evaluate GraphFlow on the STaRK benchmark, which includes real-world queries from multiple domains over text-rich KGs. \nGraphFlow outperforms strong KG-RAG baselines, including GPT-4o, by 10\\% on average in hit rate and recall. \nIt also shows strong generalization to unseen KGs, demonstrating its effectiveness and robustness.",
      "arxiv_url": "https://openreview.net/forum?id=po0eyoYFUa",
      "pdf_url": "https://openreview.net/pdf/f63ec53beaace4811759af538c8441f89918521a.pdf",
      "primary_category": "graph machine learning, knowledge graph, LLM",
      "categories": [
        "graph machine learning",
        "knowledge graph",
        "LLM"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2exr4mlbx1",
      "title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model",
      "authors": [
        "Pengteng Li",
        "Pinhao Song",
        "Wuyang Li",
        "Huizai Yao",
        "Weiyu Guo",
        "Yijie Xu",
        "Dugang Liu",
        "Hui Xiong"
      ],
      "abstract": "We introduce See&Trek, the first training-free prompting framework tailored to enhance the spatial understanding of Multimodal Large Language Models (MLLMs) under vision-only constraints. While prior efforts have incorporated modalities like depth or point clouds to improve spatial reasoning, purely visual-spatial understanding remains underexplored. See&Trek addresses this gap by focusing on two core principles: increasing visual diversity and motion reconstruction. For visual diversity, we conduct Maximum Semantic Richness Sampling, which employs an off-the-shell perception model to extract semantically rich keyframes that capture scene structure. For motion reconstruction, we simulate visual trajectories and encode relative spatial positions into keyframes to preserve both spatial relations and temporal coherence. Our method is training&GPU-free, requiring only a single forward pass, and can be seamlessly integrated into existing MLLMs. Extensive experiments on the VSI-Bench and STI-Bench show that See&Trek consistently boosts various MLLMs performance across diverse spatial reasoning tasks with the most +3.5% improvement, offering a promising path toward stronger spatial intelligence.",
      "arxiv_url": "https://openreview.net/forum?id=2exr4mlbx1",
      "pdf_url": "https://openreview.net/pdf/e4551afa8ea71660ea89d6b52a2e28fc90d52736.pdf",
      "primary_category": "MLLM, Spatial Understanding, VLM",
      "categories": [
        "MLLM",
        "Spatial Understanding",
        "VLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4uy6GI3vzo",
      "title": "On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding",
      "authors": [
        "Haoyuan Wu",
        "Rui Ming",
        "Jilong Gao",
        "Hangyu Zhao",
        "Xueyi Chen",
        "Yikai Yang",
        "Haisheng Zheng",
        "Zhuolun He",
        "Bei Yu"
      ],
      "abstract": "Large language models (LLMs) achieve remarkable performance in code generation tasks.\nHowever, a significant performance disparity persists between popular programming languages (e.g., Python, C++) and others. \nTo address this capability gap, we leverage the code translation task to train LLMs, thereby facilitating the transfer of coding proficiency across diverse programming languages.\nMoreover, we introduce OORL for training, a novel reinforcement learning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a rule-based reward signal derived from unit tests.\nComplementing this coarse-grained rule-based reward, we propose Group Equivalent Preference Optimization (GEPO), a novel preference optimization method.\nSpecifically, GEPO trains the LLM using intermediate representations (IRs) groups.\nLLMs can be guided to discern IRs equivalent to the source code from inequivalent ones, while also utilizing signals about the mutual equivalence between IRs within the group.\nThis process allows LLMs to capture nuanced aspects of code functionality.\nBy employing OORL for training with code translation tasks, LLMs improve their recognition of code functionality and their understanding of the relationships between code implemented in different languages. \nExtensive experiments demonstrate that our OORL for LLMs training with code translation tasks achieves significant performance improvements on code benchmarks across multiple programming languages.",
      "arxiv_url": "https://openreview.net/forum?id=4uy6GI3vzo",
      "pdf_url": "https://openreview.net/pdf/db694fdf0494ffcdff178122e79095c8d7dfff4e.pdf",
      "primary_category": "Large Language Models, Reinforcement Learning, Multi-Programming Language Understanding",
      "categories": [
        "Large Language Models",
        "Reinforcement Learning",
        "Multi-Programming Language Understanding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "5ZDT1dxojA",
      "title": "DLoFT: Gradient-Decoupled Fine-Tuning for Generalizable Long Chain-of-Thought Reasoning",
      "authors": [
        "Sitong Wu",
        "Haoru Tan",
        "Jingyao Li",
        "Shaofeng Zhang",
        "XIAOJUAN QI",
        "Bei Yu",
        "Jiaya Jia"
      ],
      "abstract": "Long chain-of-thought (LongCoT) has emerged as a powerful reasoning paradigm for enabling large language models (LLMs) to solve complex tasks through a systematic and thorough thinking phase.\nAlthough supervised fine-tuning (SFT) on high-quality LongCoT traces has proven effective to activate LongCoT abilities, we find that models trained in this way tend to overfit problem-specific knowledge and heuristics, leading to degraded out-of-distribution performance.\nTo address this issue, we propose a Decoupled LongCoT Fine-Tuning (DLoFT) algorithm, which enables the model to learn generalizable LongCoT reasoning abilities while preventing overfitting to the reasoning content with problem-specific information.\nThe key idea is to decouple the gradient into two orthogonal components: 1) a paradigm-relevant gradient corresponding to the general LongCoT paradigm and 2) a content-relevant gradient reflecting the problem-specific information, where only the former gradient is used to update model parameters.\nSpecifically, by leveraging the unique two-phase composition (thinking and solution) of the LongCoT response, our gradient decoupling mechanism isolates the content-relevant gradient via a projection operation and separates the paradigm-relevant gradient through orthogonalization.\nOur DLoFT ensures the model concentrate on internalizing the LongCoT paradigm rather than memorizing problem-specific knowledge and heuristics.\nExtensive experiments demonstrate that our DLoFT significantly improves the generalization behavior of LongCoT abilities compared to SFT while maintaining strong in-distribution performance.",
      "arxiv_url": "https://openreview.net/forum?id=5ZDT1dxojA",
      "pdf_url": "https://openreview.net/pdf/07dcbfe328245db6896ad4f259b722a8d266ced8.pdf",
      "primary_category": "Reasoning, LLM",
      "categories": [
        "Reasoning",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QsQGMijLhL",
      "title": "Generalizing Experience for Language Agents with Hierarchical MetaFlows",
      "authors": [
        "Shengda Fan",
        "Xin Cong",
        "Zhong Zhang",
        "Yuepeng Fu",
        "Yesai Wu",
        "Hao Wang",
        "Xinyu Zhang",
        "Enrui Hu",
        "Yankai Lin"
      ],
      "abstract": "Recent efforts to employ large language models (LLMs) as agents have demonstrated promising results in a wide range of multi-step agent tasks. However, existing agents lack an effective experience reuse approach to leverage historical completed tasks. In this paper, we propose a novel experience reuse framework MetaFlowLLM, which constructs a hierarchical experience tree from historically completed tasks. Each node in this experience tree is presented as a MetaFlow which contains static execution workflow and subtask required by agents to complete dynamically. Then, we propose a Hierarchical MetaFlow Merging algorithm to construct the hierarchical experience tree. When accomplishing a new task, MetaFlowLLM can first retrieve the most relevant MetaFlow node from the experience tree and then execute it accordingly. To effectively generate valid MetaFlows from historical data, we further propose a reinforcement learning pipeline to train the MetaFlowGen. Extensive experimental results on AppWorld and WorkBench demonstrate that integrating with MetaFlowLLM, existing agents (e.g., ReAct, Reflexion) can gain substantial performance improvement with reducing execution costs. Notably, MetaFlowLLM achieves an average success rate improvement of 32.3% on AppWorld and 6.2% on WorkBench, respectively.",
      "arxiv_url": "https://openreview.net/forum?id=QsQGMijLhL",
      "pdf_url": "https://openreview.net/pdf/e5e7cd3c074c77adae516f8d392136a8fe8657f5.pdf",
      "primary_category": "agent, workflow, large language models",
      "categories": [
        "agent",
        "workflow",
        "large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "JFygzwx8SJ",
      "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
      "authors": [
        "Jang-Hyun Kim",
        "Jinuk Kim",
        "Sangwoo Kwon",
        "Jae W. Lee",
        "Sangdoo Yun",
        "Hyun Oh Song"
      ],
      "abstract": "Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces \\textit{KVzip}, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by $3$-$4\\times$ and FlashAttention decoding latency by approximately $2\\times$, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90\\% cache budget ratio under multi-query scenarios.",
      "arxiv_url": "https://openreview.net/forum?id=JFygzwx8SJ",
      "pdf_url": "https://openreview.net/pdf/31fbad5b02e0ec167a202d1b5481e2b478a8dcfc.pdf",
      "primary_category": "Large Language Models, Efficient Inference, Long-Context Processing",
      "categories": [
        "Large Language Models",
        "Efficient Inference",
        "Long-Context Processing",
        "KV Cache Compression"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qILtlupwrR",
      "title": "Computation and Memory-Efficient Model Compression with  Gradient Reweighting",
      "authors": [
        "Zhiwei Li",
        "Yuesen Liao",
        "Binrui Wu",
        "Yuquan Zhou",
        "Xupeng Shi",
        "Dongsheng Jiang",
        "Yin Li",
        "WEIZHONG ZHANG"
      ],
      "abstract": "Pruning is a commonly employed technique for deep neural networks (DNNs) aiming at compressing the model size to reduce computational and memory costs during inference. In contrast to conventional neural networks, large language models (LLMs) pose a unique challenge regarding pruning efficiency due to their substantial computational and memory demands. Existing methods, particularly optimization-based ones, often require considerable computational resources in gradient estimation because they cannot effectively leverage weight sparsity of the intermediate pruned network to lower compuation and memory costs in each iteration. The fundamental challenge  lies in the need to frequently instantiate intermediate pruned sub-models to achieve these savings, a task that becomes infeasible even for moderately sized neural networks.  To this end, this paper proposes a novel pruning method for DNNs that is both computationally and memory-efficient. Our key idea is to develop an effective reweighting mechanism that enables us to estimate the gradient of the pruned network in current iteration via reweigting the gradient estimated on an outdated intermediate sub-model instantiated at an earlier stage, thereby significantly reducing model instantiation frequency. We further develop a series of techniques, e.g., clipping and preconditioning matrix, to reduce the variance of gradient estimation and stabilize the optimization process. We conducted extensive experimental validation across various domains. Our approach achieves 50\\% sparsity and a 1.58$\\times$ speedup in forward pass on Llama2-7B model with only 6 GB of memory usage, outperforming state-of-the-art methods with respect to both perplexity and zero-shot performance. As a by-product, our method is highly suited for  distributed sparse training  and can achieve a 2 $\\times$ speedup over the dense distributed baselines.",
      "arxiv_url": "https://openreview.net/forum?id=qILtlupwrR",
      "pdf_url": "https://openreview.net/pdf/92754dce25ddf4827f44c09c5afb26a84249c340.pdf",
      "primary_category": "model compression, deep neural networks, sparse training",
      "categories": [
        "model compression",
        "deep neural networks",
        "sparse training",
        "deep learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9env0BdcDV",
      "title": "Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models",
      "authors": [
        "Siwei Zhang",
        "Yun Xiong",
        "Yateng Tang",
        "Jiarong Xu",
        "Xi Chen",
        "Zehao Gu",
        "Xuehao Zheng",
        "Zi'an Jia",
        "Jiawei Zhang"
      ],
      "abstract": "Temporal graph neural networks (TGNNs) have shown remarkable performance in temporal graph modeling. However, real-world temporal graphs often possess rich textual information, giving rise to temporal text-attributed graphs (TTAGs). Such combination of dynamic text semantics and evolving graph structures introduces heightened complexity. Existing TGNNs embed texts statically and rely heavily on encoding mechanisms that biasedly prioritize structural information, overlooking the temporal evolution of text semantics and the essential interplay between semantics and structures for synergistic reinforcement.\nTo tackle these issues, we present $\\textbf{CROSS}$, a flexible framework that seamlessly extends existing TGNNs for TTAG modeling. CROSS is designed by decomposing the TTAG modeling process into two phases: (i) temporal semantics extraction; and (ii) semantic-structural information unification. The key idea is to advance the large language models (LLMs) to $\\textit{dynamically}$ extract the temporal semantics in text space and then generate $\\textit{cohesive}$ representations unifying both semantics and structures.\nSpecifically, we propose a Temporal Semantics Extractor in the CROSS framework, which empowers LLMs to offer the temporal semantic understanding of node's evolving contexts of textual neighborhoods, facilitating semantic dynamics.\nSubsequently, we introduce the Semantic-structural Co-encoder, which collaborates with the above Extractor for synthesizing illuminating representations by jointly considering both semantic and structural information while encouraging their mutual reinforcement. Extensive experiments show that CROSS achieves state-of-the-art results on four public datasets and one industrial dataset, with 24.7\\% absolute MRR gain on average in temporal link prediction and 3.7\\% AUC gain in node classification of industrial application.",
      "arxiv_url": "https://openreview.net/forum?id=9env0BdcDV",
      "pdf_url": "https://openreview.net/pdf/a307c96ad7de514026f806abcd65e23d148d8df1.pdf",
      "primary_category": "Temporal Text-attributed Graph, Large Language Models, Data Mining",
      "categories": [
        "Temporal Text-attributed Graph",
        "Large Language Models",
        "Data Mining"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zWHKKspghT",
      "title": "Mozart: Modularized and Efficient MoE Training on 3.5D Wafer-Scale Chiplet Architectures",
      "authors": [
        "Shuqing Luo",
        "Ye Han",
        "Pingzhi Li",
        "Jiayin Qin",
        "Jie Peng",
        "Yang Katie Zhao",
        "Yu Cao",
        "Tianlong Chen"
      ],
      "abstract": "Mixture-of-Experts (MoE) architecture offers enhanced efficiency for Large Language Models (LLMs) with modularized computation, yet its inherent sparsity poses significant hardware deployment challenges, including memory locality issues, communication overhead, and inefficient computing resource utilization. Inspired by the modular organization of the human brain, we propose $\\texttt{Mozart}$, a novel algorithm-hardware co-design framework tailored for efficient training of MoE-based LLMs on 3.5D wafer-scale chiplet architectures. On the algorithm side, $\\texttt{Mozart}$ exploits the inherent modularity of chiplets and introduces: \n($1$) an expert allocation strategy that enables efficient on-package all-to-all communication, and ($2$) a fine-grained scheduling mechanism that improves communication-computation overlap through streaming tokens and experts. On the architecture side, $\\texttt{Mozart}$ adaptively co-locates heterogeneous modules on specialized chiplets with a 2.5D NoP-Tree topology and hierarchical memory structure.\nEvaluation across three popular MoE models demonstrates significant efficiency gains, enabling more effective parallelization and resource utilization for large-scale modularized MoE-LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=zWHKKspghT",
      "pdf_url": "https://openreview.net/pdf/57f1a00a4e2c31ced71cb0c9afc8b0c489a93eeb.pdf",
      "primary_category": "MoE, Chiplet Architecture, Post-Training Efficiency",
      "categories": [
        "MoE",
        "Chiplet Architecture",
        "Post-Training Efficiency"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tQZK5frjVU",
      "title": "Data Mixing Can Induce Phase Transitions in Knowledge Acquisition",
      "authors": [
        "Xinran Gu",
        "Kaifeng Lyu",
        "Jiazheng Li",
        "Jingzhao Zhang"
      ],
      "abstract": "Large Language Models (LLMs) are typically trained on data mixtures: most data come from web scrapes, while a small portion is curated from high-quality sources with dense domain-specific knowledge.\nIn this paper, we show that when training LLMs on such data mixtures, knowledge acquisition from knowledge-dense datasets—unlike training exclusively on knowledge-dense data—does not always follow a smooth scaling law but can exhibit phase transitions with respect to the mixing ratio and model size. Through controlled experiments on a synthetic biography dataset mixed with web-scraped data, we demonstrate that: (1) as we increase the model size to a critical value, the model suddenly transitions from memorizing very few to most of the biographies; (2) below a critical mixing ratio, the model memorizes almost nothing even with extensive training, but beyond this threshold, it rapidly memorizes more biographies. We attribute these phase transitions to a capacity allocation phenomenon: a model with bounded capacity must act like a knapsack problem solver to minimize the overall test loss, and the optimal allocation across datasets can change discontinuously as the model size or mixing ratio varies. We formalize this intuition in an information-theoretic framework and reveal that these phase transitions are predictable, with the critical mixing ratio following a power-law relationship with the model size. Our findings highlight a concrete case where a good mixing recipe for large models may not be optimal for small models, and vice versa.",
      "arxiv_url": "https://openreview.net/forum?id=tQZK5frjVU",
      "pdf_url": "https://openreview.net/pdf/95bd21460976cad8c6926b5c9ab1243abb9dfdbf.pdf",
      "primary_category": "knowledge acquisition, memorization, scaling laws",
      "categories": [
        "knowledge acquisition",
        "memorization",
        "scaling laws",
        "LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6QJZDAIhfk",
      "title": "F-Adapter: Frequency-Adaptive Parameter-Efficient Fine-Tuning in Scientific Machine Learning",
      "authors": [
        "Hangwei Zhang",
        "KangChun",
        "Yan Wang",
        "Difan Zou"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) powerful pre-trained models for complex downstream tasks has proven effective in vision and language processing, yet this paradigm remains unexplored in scientific machine learning, where the objective is to model complex physical systems. We conduct the first systematic study of PEFT for pre-trained Large Operator Models (LOMs) obtained by scaling variants of Fourier Neural Operator. We observe that the widely used Low-Rank Adaptation (LoRA) yields markedly poorer performance on LOMs than Adapter tuning.  We further theoretically establish that stacked LoRA incurs a depth-amplified lower bound on approximation error within Fourier layers, whereas adapters retain universal approximation capacity and, by concentrating parameters on energy-dominant low-frequency modes, attain exponentially decaying error with bottleneck width in the Fourier domain. Motivated by the robust empirical gains of adapters and by our theoretical characterization of PDE solutions as spectrally sparse, we introduce Frequency-Adaptive Adapter (F-Adapter). F-Adapter allocates adapter capacity based on spectral complexity, assigning higher-dimension modules to low-frequency components and lower-dimension modules to high-frequency components. Our F-Adapters establish state-of-the-art results on multiple challenging 3D Navier–Stokes benchmarks, markedly enhancing both generalization and spectral fidelity over LoRA and other PEFT techniques commonly used in LLMs. To the best of our knowledge, this work is the first to explore PEFT for scientific machine-learning and establishes F-Adapter as an effective paradigm for this domain. The code will be made publicly available upon acceptance.",
      "arxiv_url": "https://openreview.net/forum?id=6QJZDAIhfk",
      "pdf_url": "https://openreview.net/pdf/6fb78abde6bfc437b0f5a8d39a0bd53231be73cc.pdf",
      "primary_category": "Scientifc Machine Learning",
      "categories": [
        "Scientifc Machine Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DBybUx7ARy",
      "title": "Accelerating Block Coordinate Descent for LLM Finetuning via Landscape Expansion",
      "authors": [
        "Qijun Luo",
        "Yifei Shen",
        "Liangzu Peng",
        "Dongsheng Li",
        "Xiao Li"
      ],
      "abstract": "Finetuning large language models (LLMs) is a resource-intensive task for researchers in academia, with memory constraints posing a key bottleneck. A classic optimization method, block coordinate descent (BCD), significantly reduces memory cost by segmenting the trainable parameters into multiple blocks and optimizing one active block at a time while freezing the others. However, we identify that blindly applying BCD to train LLMs can be inefficient for two reasons. First, optimizing only the active block requires backpropagating through multiple deeper yet inactive blocks, resulting in wasteful computations. Second, the frozen blocks, when they are not quite close to optimality, can narrow the optimization landscape, potentially misguiding the training of the active block. To address these issues simultaneously, we propose integrating BCD with *landscape expansion*, which unfreezes the inactive blocks and updates them in a cost-efficient manner during the same backpropagation as the update to the active block. Experiments on 8B and 70B models demonstrate that our proposed method surpasses memory-efficient baselines and matches Adam's downstream performance while requiring only 24 GB of memory for the 8B model and 300 GB for the 70B model.",
      "arxiv_url": "https://openreview.net/forum?id=DBybUx7ARy",
      "pdf_url": "https://openreview.net/pdf/980d28f2eee0e69c82f8b49bd7958df3b79f1509.pdf",
      "primary_category": "Memory efficiency; optimization",
      "categories": [
        "Memory efficiency; optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "itXwzGwGFC",
      "title": "Training-Free Bayesianization for Low-Rank Adapters of Large Language Models",
      "authors": [
        "Haizhou Shi",
        "Yibin Wang",
        "Ligong Han",
        "Huan Zhang",
        "Hao Wang"
      ],
      "abstract": "Estimating the uncertainty of responses from Large Language Models (LLMs) remains a critical challenge. While recent Bayesian methods have demonstrated effectiveness in quantifying uncertainty through low-rank weight updates, they typically require complex fine-tuning or post-training procedures. In this paper, we propose **T**raining-**F**ree **B**ayesianization (**TFB**), a simple yet theoretically grounded framework that efficiently transforms trained low-rank adapters into Bayesian ones without additional training. TFB systematically searches for the maximally acceptable level of variance in the weight posterior, constrained within a family of low-rank isotropic Gaussian distributions. Our theoretical analysis shows that under mild conditions, this search process is equivalent to KL-regularized variational optimization, a generalized form of variational inference. Through comprehensive experiments, we show that TFB achieves superior uncertainty estimation and generalization compared to existing methods while eliminating the need for complex Bayesianization training procedures.",
      "arxiv_url": "https://openreview.net/forum?id=itXwzGwGFC",
      "pdf_url": "https://openreview.net/pdf/add7a5944c501cb6b99f02e25c23eebc443bcbfe.pdf",
      "primary_category": "Large Language Models, Uncertainty Estimation, Bayesian Deep Learning",
      "categories": [
        "Large Language Models",
        "Uncertainty Estimation",
        "Bayesian Deep Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zwCb9cKHpd",
      "title": "SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing",
      "authors": [
        "Mingfei Chen",
        "Zijun Cui",
        "Xiulong Liu",
        "Jinlin Xiang",
        "Caleb Zheng",
        "Jingyuan Li",
        "Eli Shlizerman"
      ],
      "abstract": "3D spatial reasoning in dynamic, audio-visual environments is a cornerstone of human cognition yet remains largely unexplored by existing Audio-Visual Large Language Models (AV-LLMs) and benchmarks, which predominantly focus on static or 2D scenes. We introduce SAVVY-Bench, the first benchmark for 3D spatial reasoning in dynamic scenes with synchronized spatial audio. SAVVY-Bench is comprised of thousands of carefully curated question–answer pairs probing both directional and distance relationships involving static and moving objects, and requires fine-grained temporal grounding, consistent 3D localization, and multi-modal annotation. To tackle this challenge, we propose SAVVY, a novel training-free reasoning pipeline that consists of two stages: (i) Egocentric Spatial Tracks Estimation, which leverages AV-LLMs as well as other audio-visual methods to track the trajectories of key objects related to the query using both visual and spatial audio cues, and (ii) Dynamic Global Map Construction, which aggregates multi-modal queried object trajectories and converts them into a unified global dynamic map. Using the constructed map, a final QA answer is obtained through a coordinate transformation that aligns the global map with the queried viewpoint. Empirical evaluation demonstrates that SAVVY substantially enhances performance of state-of-the-art AV-LLMs, setting a new standard and stage for approaching dynamic 3D spatial reasoning in AV-LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=zwCb9cKHpd",
      "pdf_url": "https://openreview.net/pdf/1e74d98676203a254bd6c3076b8adda5fb7f2c26.pdf",
      "primary_category": "Audio-Visual, 3D Spatial Reasoning, Multi-modal LLMs",
      "categories": [
        "Audio-Visual",
        "3D Spatial Reasoning",
        "Multi-modal LLMs",
        "Audio-Visual QA"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VYdzGigCBC",
      "title": "R$^2$ec: Towards Large Recommender Models with Reasoning",
      "authors": [
        "Runyang You",
        "Yongqi Li",
        "Xinyu Lin",
        "Xin Zhang",
        "Wenjie Wang",
        "Wenjie Li",
        "Liqiang Nie"
      ],
      "abstract": "Large recommender models have extended LLMs as powerful recommenders via encoding or item generation, and recent breakthroughs in LLM reasoning synchronously motivate the exploration of reasoning in recommendation. \nIn this work, we propose R$^2$ec, a unified large recommender model with intrinsic reasoning capability. \nR$^2$ec introduces a dual-head architecture that supports both reasoning chain generation and efficient item prediction in a single model, significantly reducing inference latency. To overcome the lack of annotated reasoning data, we design RecPO, a reinforcement learning framework that optimizes reasoning and recommendation jointly with a novel fused reward mechanism. \nExtensive experiments on three datasets demonstrate that R$^2$ec outperforms traditional, LLM-based, and reasoning-augmented recommender baselines, while further analyses validate its competitive efficiency among conventional LLM-based recommender baselines\nand strong adaptability to diverse recommendation scenarios. Code and checkpoints available at https://github.com/YRYangang/RRec.",
      "arxiv_url": "https://openreview.net/forum?id=VYdzGigCBC",
      "pdf_url": "https://openreview.net/pdf/f29b278eccadd4b83c9cca979b6b35476c14e3a8.pdf",
      "primary_category": "Generative Recommendation, LLM-based Recommendation",
      "categories": [
        "Generative Recommendation",
        "LLM-based Recommendation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "e8DrPuJekZ",
      "title": "Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA",
      "authors": [
        "Shuangyi Chen",
        "Yuanxin Guo",
        "Yue Ju",
        "Hardik Dalal",
        "Zhongwen Zhu",
        "Ashish J Khisti"
      ],
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) optimize federated training by reducing computational and communication costs.  We propose RoLoRA, a federated framework using alternating optimization to fine-tune LoRA adapters. Our approach emphasizes the importance of learning up and down projection matrices to enhance expressiveness and robustness. We use both theoretical analysis and extensive experiments to demonstrate the advantages of RoLoRA over prior approaches that either generate imperfect model updates or limit expressiveness of the model. We provide a theoretical analysis on a linear model to highlight the importance of learning both the down-projection and up-projection matrices in LoRA. We validate the insights on a non-linear model and separately provide a convergence proof under general conditions. To bridge theory and practice, we conducted extensive experimental evaluations on language models including RoBERTa-Large, Llama-2-7B on diverse tasks and FL settings to demonstrate the advantages of RoLoRA over other methods.",
      "arxiv_url": "https://openreview.net/forum?id=e8DrPuJekZ",
      "pdf_url": "https://openreview.net/pdf/d8be7e3f6ac932d2ab0d7c735bb48b5aa9b29084.pdf",
      "primary_category": "LoRA, Federated Learning",
      "categories": [
        "LoRA",
        "Federated Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "PAIVwOaAnq",
      "title": "Shape it Up! Restoring LLM Safety during Finetuning",
      "authors": [
        "ShengYun Peng",
        "Pin-Yu Chen",
        "Jianfeng Chi",
        "Seongmin Lee",
        "Duen Horng Chau"
      ],
      "abstract": "Finetuning large language models (LLMs) enables user-specific customization but introduces important safety risks: even a few harmful examples can compromise safety alignment. A common mitigation strategy is to update the model more strongly on examples deemed safe, while downweighting or excluding those flagged as unsafe. However, because safety context can shift within a single example, updating the model equally on both harmful and harmless parts of a response is suboptimal — an atomic treatment we term static safety shaping. In contrast, we propose dynamic safety shaping (DSS), a dynamic shaping framework that uses fine-grained safety signals to reinforce learning from safe segments of a response while suppressing unsafe content. To enable such fine-grained control during finetuning, we introduce a key insight: guardrail models, traditionally used for filtering, can be repurposed to evaluate partial responses, tracking how safety risk evolves throughout the response, segment by segment. This leads to the Safety Trajectory Assessment of Response (STAR), a token-level signal that enables shaping to operate dynamically over the training sequence. Building on this, we present ★DSS, a DSS method guided by STAR scores that robustly mitigates finetuning risks and delivers substantial safety improvements across diverse threats, datasets, and model families, all without compromising capability on intended tasks. We encourage future safety research to build on dynamic shaping principles for stronger mitigation against evolving finetuning risks. Our code is publicly available at https://github.com/poloclub/star-dss",
      "arxiv_url": "https://openreview.net/forum?id=PAIVwOaAnq",
      "pdf_url": "https://openreview.net/pdf/2435c6e51251b7ec7bcb9ced9b39871232786020.pdf",
      "primary_category": "LLM, LLM finetuning, LLM finetuning risks",
      "categories": [
        "LLM",
        "LLM finetuning",
        "LLM finetuning risks",
        "harmful finetuning",
        "guardrail",
        "safe guard",
        "safety shaping"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "fZsd3KLMje",
      "title": "Repo2Run: Automated Building Executable Environment for Code Repository at Scale",
      "authors": [
        "Ruida Hu",
        "Chao Peng",
        "XinchenWang",
        "Junjielong Xu",
        "Cuiyun Gao"
      ],
      "abstract": "Scaling up executable code data is significant for improving language models’ software engineering capability. The intricate nature of the process makes it labor-intensive, time-consuming and expert-knowledge-dependent to build a large number of executable code repositories, limiting the scalability of existing work based on running tests. The primary bottleneck lies in the automated building of test environments for different repositories, which is an essential yet underexplored task. To mitigate the gap, we introduce Repo2Run, the first LLM-based agent aiming at automating the building of executable test environments for any repositories at scale. Specifically, given a code repository, Repo2Run iteratively builds the Docker image, runs unit tests based on the feedback of the building, and synthesizes the Dockerfile until the entire pipeline is executed successfully. The resulting Dockerfile can then be used to create Docker container environments for running code and tests. We created a benchmark containing 420 Python repositories with unit tests for evaluation. The results illustrate that Repo2Run achieves an 86.0% success rate, outperforming SWE-agent by 77.0%. The resources of Repo2Run are available at https://github.com/bytedance/Repo2Run.",
      "arxiv_url": "https://openreview.net/forum?id=fZsd3KLMje",
      "pdf_url": "https://openreview.net/pdf/392d320f831f99ec8a5dfbaaef5bb70c4339a1fd.pdf",
      "primary_category": "Code data scaling, Large language model, Executable environment building",
      "categories": [
        "Code data scaling",
        "Large language model",
        "Executable environment building"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "uGDNHlslgO",
      "title": "MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation",
      "authors": [
        "Zhenyu Pan",
        "Yucheng Lu",
        "Han Liu"
      ],
      "abstract": "We present MetaFind, a scene-aware multi-modal retrieval framework designed to enhance scene generation in the metaverse by retrieving 3D assets from large-scale repositories. MetaFind addresses two core challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic, and stylistic constraints, and (ii) the absence of a standardized retrieval paradigm specifically tailored for 3D asset retrieval, as existing approaches predominantly rely on general-purpose 3D shape representation models. Our key innovation is a retrieval mechanism that enhances both spatial reasoning and style consistency by jointly modeling object-level features (including appearance) and scene-level layout structures. Methodologically, MetaFind introduces a plug-and-play layout encoder that captures both spatial relationships and object appearance features, ensuring retrieved 3D assets are contextually and stylistically coherent with the existing scene. The framework supports iterative scene construction by continuously adapting retrieval results to current scene updates. Empirical evaluations demonstrate the improved spatial and stylistic consistency of MetaFind in various retrieval tasks compared to baseline methods.",
      "arxiv_url": "https://openreview.net/forum?id=uGDNHlslgO",
      "pdf_url": "https://openreview.net/pdf/24ff05b76e73a0c9959f4af477bf10594eef7ead.pdf",
      "primary_category": "information retrieval, scene generation, 3D world generation",
      "categories": [
        "information retrieval",
        "scene generation",
        "3D world generation",
        "scene generation"
      ],
      "tags": [
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "OhUu5PlRkF",
      "title": "CAML: Collaborative Auxiliary Modality Learning for Multi-Agent Systems",
      "authors": [
        "Rui Liu",
        "Yu Shen",
        "Peng Gao",
        "Pratap Tokekar",
        "Ming Lin"
      ],
      "abstract": "Multi-modal learning has emerged as a key technique for improving performance across domains such as autonomous driving, robotics, and reasoning. However, in certain scenarios, particularly in resource-constrained environments, some modalities available during training may be absent during inference. While existing frameworks effectively utilize multiple data sources during training and enable inference with reduced modalities, they are primarily designed for single-agent settings. This poses a critical limitation in dynamic environments such as connected autonomous vehicles (CAV), where incomplete data coverage can lead to decision-making blind spots. Conversely, some works explore multi-agent collaboration but without addressing missing modality at test time. To overcome these limitations, we propose Collaborative Auxiliary Modality Learning (CAML), a novel multi-modal multi-agent framework that enables agents to collaborate and share multi-modal data during training, while allowing inference with reduced modalities during testing. Experimental results in collaborative decision-making for CAV in accident-prone scenarios demonstrate that CAML achieves up to a 58.1% improvement in accident detection. Additionally, we validate CAML on real-world aerial-ground robot data for collaborative semantic segmentation, achieving up to a 10.6% improvement in mIoU.",
      "arxiv_url": "https://openreview.net/forum?id=OhUu5PlRkF",
      "pdf_url": "https://openreview.net/pdf/3d1baca92b6ced6fdf58596f52b56cae56db7214.pdf",
      "primary_category": "Multi-Modal Learning, Multi-Agent Systems",
      "categories": [
        "Multi-Modal Learning",
        "Multi-Agent Systems"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "heIh4lkBEd",
      "title": "RULE: Reinforcement UnLEarning Achieves Forget-retain Pareto Optimality",
      "authors": [
        "Chenlong Zhang",
        "Zhuoran Jin",
        "Hongbang Yuan",
        "Jiaheng Wei",
        "Tong Zhou",
        "Kang Liu",
        "Jun Zhao",
        "Yubo Chen"
      ],
      "abstract": "The widespread deployment of Large Language Models (LLMs) trained on massive, uncurated corpora has raised growing concerns about the inclusion of sensitive, copyrighted, or illegal content. This has led to increasing interest in LLM unlearning: the task of selectively removing specific information from a model without retraining from scratch or degrading overall utility.\nHowever, existing methods often rely on large-scale forget and retain datasets, and suffer from unnatural responses, poor generalization, or catastrophic utility loss.\nIn this work, we propose $\\textbf{R}$einforcement $\\textbf{U}$n$\\textbf{LE}$arning ($\\textbf{RULE}$), an efficient framework that formulates unlearning as a refusal boundary optimization problem. RULE is trained with a small portion of forget set and synthesized boundary queries, using a verifiable reward function that encourages safe refusal on forget-related queries while preserving helpful responses on permissible inputs.\nWe provide both theoretical and empirical evidence demonstrating the effectiveness of RULE in achieving targeted unlearning without compromising model utility. Experimental results show that, with only 12\\% forget set and 8\\% synthesized boundary data, RULE outperforms existing baselines by up to $17.4\\%$ forget quality and $16.3\\%$ naturalness response while maintaining general utility, achieving $\\textit{forget-retain Pareto Optimality}$. Remarkably, we further observe that RULE improves the $\\textit{naturalness}$ of model outputs, enhances training $\\textit{efficiency}$, and exhibits strong $\\textit{generalization ability}$, generalizing refusal behavior to semantically related but unseen queries.",
      "arxiv_url": "https://openreview.net/forum?id=heIh4lkBEd",
      "pdf_url": "https://openreview.net/pdf/745a3cff81ec82eb4ec09da3b393ea1c6e3a8a2b.pdf",
      "primary_category": "Machine Unlearning",
      "categories": [
        "Machine Unlearning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "gb2wOj7V74",
      "title": "On the Robustness of Transformers against Context Hijacking for Linear Classification",
      "authors": [
        "Tianle Li",
        "Chenyang Zhang",
        "Xingwu Chen",
        "Yuan Cao",
        "Difan Zou"
      ],
      "abstract": "Transformer-based Large Language Models (LLMs) have demonstrated powerful in-context learning capabilities. However, their predictions can be disrupted by factually correct context, a phenomenon known as context hijacking, revealing a significant robustness issue. To understand this phenomenon theoretically, we explore an in-context linear classification problem based on recent advances in linear transformers. In our setup, context tokens are designed as factually correct query-answer pairs, where the queries are similar to the final query but have opposite labels. Then, we develop a general theoretical analysis on the robustness of the linear transformers, which is formulated as a function of the model depth, training context lengths, and number of hijacking context tokens. A key finding is that a well-trained deeper transformer can achieve higher robustness, which aligns with empirical observations. We show that this improvement arises because deeper layers enable more fine-grained optimization steps, effectively mitigating interference from context hijacking. This is also well supported by our numerical and real-world experiments. Our findings provide theoretical insights into the benefits of deeper architectures and contribute to enhancing the understanding of transformer architectures.",
      "arxiv_url": "https://openreview.net/forum?id=gb2wOj7V74",
      "pdf_url": "https://openreview.net/pdf/3175ece6e92218513725bc997e4e62662a59e3af.pdf",
      "primary_category": "in-context learning, transformers, robustness",
      "categories": [
        "in-context learning",
        "transformers",
        "robustness",
        "deep learning theory",
        "learning theory"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "gNwJTjxmBe",
      "title": "UniCTokens: Boosting Personalized Understanding and Generation via Unified Concept Tokens",
      "authors": [
        "Ruichuan An",
        "Sihan Yang",
        "Renrui Zhang",
        "Zijun Shen",
        "Ming Lu",
        "Gaole Dai",
        "Hao Liang",
        "Ziyu Guo",
        "Shilin Yan",
        "Yulin Luo",
        "Bocheng Zou",
        "Chaoqun Yang",
        "Wentao Zhang"
      ],
      "abstract": "Personalized models have demonstrated remarkable success in understanding and generating concepts provided by users. However, existing methods use separate concept tokens for understanding and generation, treating these tasks in isolation. This may result in limitations for generating images with complex prompts. For example, given the concept $\\langle bo\\rangle$, generating \"$\\langle bo\\rangle$ wearing its hat\" without additional textual descriptions of its hat. We call this kind of generation \\textit{\\textbf{personalized attribute-reasoning generation}}. To address the limitation, we present UniCTokens, a novel framework that effectively integrates personalized information into a unified vision language model (VLM) for understanding and generation. UniCTokens trains a set of unified concept tokens to leverage complementary semantics, boosting two personalized tasks. Moreover, we propose a progressive training strategy with three stages: understanding warm-up, bootstrapping generation from understanding, and deepening understanding from generation to enhance mutual benefits between both tasks. To quantitatively evaluate the unified VLM personalization, we present UnifyBench, the first benchmark for assessing concept understanding, concept generation, and attribute-reasoning generation. Experimental results on UnifyBench indicate that UniCTokens shows competitive performance compared to leading methods in concept understanding, concept generation, and achieving state-of-the-art results in personalized attribute-reasoning generation. Our research demonstrates that enhanced understanding improves generation, and the generation process can yield valuable insights into understanding. Our code and dataset will be released at: \\href{https://github.com/arctanxarc/UniCTokens}{https://github.com/arctanxarc/UniCTokens}.",
      "arxiv_url": "https://openreview.net/forum?id=gNwJTjxmBe",
      "pdf_url": "https://openreview.net/pdf/1ae2bd941680e6165e1ce651c7d75b69e44c9d7f.pdf",
      "primary_category": "Unified Vision-Language Model, Personalization, Understanding and Generation",
      "categories": [
        "Unified Vision-Language Model",
        "Personalization",
        "Understanding and Generation"
      ],
      "tags": [
        "Personalization",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9ccmoYhZue",
      "title": "MODEL SHAPLEY: Find Your Ideal Parameter Player via One Gradient Backpropagation",
      "authors": [
        "Xu Chu",
        "Xinke Jiang",
        "Rihong Qiu",
        "Jiaran Gao",
        "Junfeng Zhao"
      ],
      "abstract": "Measuring parameter importance is crucial for understanding and optimizing large language models (LLMs). Existing work predominantly focuses on pruning or probing at neuron/feature levels without fully considering the cooperative behaviors of model parameters. In this paper, we introduce a novel approach--Model Shapley to quantify parameter importance based on the Shapley value, a principled method from cooperative game theory that captures both individual and synergistic contributions among parameters, via only one gradient backpropagation. We derive a scalable second-order approximation to compute Shapley values at the parameter level, leveraging blockwise Fisher information for tractability in large-scale settings. Our method enables fine-grained differentiation of parameter importance, facilitating targeted knowledge injection and model compression. Through mini-batch Monte Carlo updates and efficient approximation of the Hessian structure, we achieve robust Shapley-based attribution with only modest computational overhead. Experimental results indicate that this cooperative game perspective enhances interpretability, guides more effective parameter-specific fine-tuning and model compressing, and paves the way for continuous model improvement in various downstream tasks.",
      "arxiv_url": "https://openreview.net/forum?id=9ccmoYhZue",
      "pdf_url": "https://openreview.net/pdf/93f9ef3cfb7e6e43c451f0dafe7b02bd66910332.pdf",
      "primary_category": "Model Shapley; LLM",
      "categories": [
        "Model Shapley; LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "es4TTVGJ9x",
      "title": "Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning",
      "authors": [
        "Lei Wang",
        "Jieming Bian",
        "Letian Zhang",
        "Jie Xu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across various tasks, but fine-tuning them for domain-specific applications often requires substantial domain-specific data that may be distributed across multiple organizations. Federated Learning (FL) offers a privacy-preserving solution, but faces challenges with computational constraints when applied to LLMs. Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning approach, though a single LoRA module often struggles with heterogeneous data across diverse domains. This paper addresses two critical challenges in federated LoRA fine-tuning: 1. determining the optimal number and allocation of LoRA experts across heterogeneous clients, and 2. enabling clients to selectively utilize these experts based on their specific data characteristics. We propose FedLEASE (Federated adaptive LoRA Expert Allocation and SElection), a novel framework that adaptively clusters clients based on representation similarity to allocate and train domain-specific LoRA experts. It also introduces an adaptive top-$M$ Mixture-of-Experts mechanism that allows each client to select the optimal number of utilized experts. Our extensive experiments on diverse benchmark datasets demonstrate that FedLEASE significantly outperforms existing federated fine-tuning approaches in heterogeneous client settings while maintaining communication efficiency.",
      "arxiv_url": "https://openreview.net/forum?id=es4TTVGJ9x",
      "pdf_url": "https://openreview.net/pdf/7216e710ad598567815f5a012d06c4d14410eb58.pdf",
      "primary_category": "Federated Learning, LLM fine-tuning",
      "categories": [
        "Federated Learning",
        "LLM fine-tuning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "rbodZQBfd9",
      "title": "Meta-Learning Objectives for Preference Optimization",
      "authors": [
        "Carlo Alfano",
        "Silvia Sapora",
        "Jakob Nicolaus Foerster",
        "Patrick Rebeschini",
        "Yee Whye Teh"
      ],
      "abstract": "Evaluating preference optimization (PO) algorithms on LLM alignment is a challenging task that presents prohibitive costs, noise, and several variables like model size and hyper-parameters. In this work, we show that it is possible to gain insights on the efficacy of PO algorithm on simpler benchmarks. We design a diagnostic suite of MuJoCo tasks and datasets, which we use to systematically evaluate PO algorithms, establishing a more controlled and cheaper benchmark. We then propose a novel family of PO algorithms based on mirror descent, which we call Mirror Preference Optimization (MPO). Through evolutionary strategies, we search this class to discover algorithms specialized to specific properties of preference datasets, such as mixed-quality or noisy data. We demonstrate that our discovered PO algorithms outperform all known algorithms in the targeted MuJoCo settings. Finally, based on the insights gained from our MuJoCo experiments, we design a PO algorithm that significantly outperform existing baselines in an LLM alignment task.",
      "arxiv_url": "https://openreview.net/forum?id=rbodZQBfd9",
      "pdf_url": "https://openreview.net/pdf/da6799f2883750b8da28b0df5c538ddb5ae73698.pdf",
      "primary_category": "Preference Optimisation, Evolution Strategies, Meta-Learning",
      "categories": [
        "Preference Optimisation",
        "Evolution Strategies",
        "Meta-Learning",
        "Mirror Maps"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1YprrVfIp8",
      "title": "Stackelberg Self-Annotation: A Robust Approach to Data-Efficient LLM Alignment",
      "authors": [
        "Xu Chu",
        "Zhixin Zhang",
        "Tianyu Jia",
        "Yujie Jin"
      ],
      "abstract": "Aligning large language models (LLMs) with human preferences typically demands vast amounts of meticulously curated data, which is both expensive and prone to labeling noise. We propose Stackelberg Game Preference Optimization (SGPO), a robust alignment framework that models alignment as a two-player Stackelberg game between a policy (leader) and a worst-case preference distribution (follower). The proposed SGPO guarantees $\\mathcal{O}(\\epsilon)$-bounded regret within an $\\epsilon$-Wasserstein ball, offering formal robustness to (self-)annotation noise. We instantiate SGPO with Stackelberg Self-Annotated Preference Optimization (SSAPO), which uses minimal human-labeled “seed” preferences and iteratively self-annotates new prompts. In each iteration, SSAPO applies a distributionally robust reweighting of synthetic annotations, ensuring that noisy or biased self-labels do not derail training. Remarkably, using only 2K seed preferences—about 1/30 of standard human labels—SSAPO achieves strong win rates against GPT-4 across multiple benchmarks within three iterations. These results highlight that a principled Stackelberg formulation yields data-efficient alignment for LLMs, significantly reducing reliance on costly human annotations.",
      "arxiv_url": "https://openreview.net/forum?id=1YprrVfIp8",
      "pdf_url": "https://openreview.net/pdf/89cca5fb48c05a0a570188e4d2d17025a14a0323.pdf",
      "primary_category": "Data-Efficient, Alignment, Large Language Models",
      "categories": [
        "Data-Efficient",
        "Alignment",
        "Large Language Models",
        "Stackelberg Games"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "96I8PGPALv",
      "title": "Unlocking Multimodal Mathematical Reasoning via Process Reward Model",
      "authors": [
        "Ruilin Luo",
        "Zhuofan Zheng",
        "Lei Wang",
        "Yifan Wang",
        "Xinzhe Ni",
        "Zicheng Lin",
        "Songtao Jiang",
        "Yiyao Yu",
        "Chufan Shi",
        "Ruihang Chu",
        "Jin zeng",
        "Yujiu Yang"
      ],
      "abstract": "Process Reward Models (PRMs) have shown promise in enhancing the mathematical reasoning capabilities of Large Language Models (LLMs) through Test-Time Scaling (TTS). However, their integration into multimodal reasoning remains largely unexplored. In this work, we take the first step toward unlocking the potential of PRMs in multimodal mathematical reasoning. We identify three key challenges: (i) the scarcity of high-quality reasoning data constrains the capabilities of foundation Multimodal Large Language Models (MLLMs), which imposes further limitations on the upper bounds of TTS and reinforcement learning (RL); (ii) a lack of automated methods for process labeling within multimodal contexts persists; (iii) the employment of process rewards in unimodal RL faces issues like reward hacking, which may extend to multimodal scenarios. To address these issues, we introduce URSA, a three-stage Unfolding multimodal pRocess-Supervision Aided training framework. We first construct MMathCoT-1M, a high-quality large-scale multimodal Chain-of-Thought (CoT) reasoning dataset, to build a stronger math reasoning foundation MLLM, URSA-8B. Subsequently, we go through an automatic process to synthesize process supervision data, which emphasizes both logical correctness and perceptual consistency. We introduce DualMath-1.1M to facilitate the training of URSA-8B-RM. Finally, we propose Process-Supervised Group-Relative-Policy-Optimization (PS-GRPO), pioneering a multimodal PRM-aided online RL method that outperforms vanilla GRPO. With PS-GRPO application, URSA-8B-PS-GRPO outperforms Gemma3-12B and GPT-4o by 8.4% and 2.7% on average across 6 benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=96I8PGPALv",
      "pdf_url": "https://openreview.net/pdf/9bc76b4ee1cdbe244a893c162b08d3ed55059de4.pdf",
      "primary_category": "Multimodal Reasoning, Data Synthesis, Process Reward Model",
      "categories": [
        "Multimodal Reasoning",
        "Data Synthesis",
        "Process Reward Model",
        "Reinforcement Learning."
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tMf3keuPOl",
      "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO",
      "authors": [
        "Chengzhuo Tong",
        "Ziyu Guo",
        "Renrui Zhang",
        "Wenyu Shan",
        "Xinyu Wei",
        "Zhenghao Xing",
        "Hongsheng Li",
        "Pheng-Ann Heng"
      ],
      "abstract": "Recent advancements underscore the significant role of Reinforcement Learning (RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large language models (LLMs). Two prominent RL algorithms, Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central to these developments, showcasing different pros and cons. Autoregressive image generation, also interpretable as a sequential CoT reasoning process, presents unique challenges distinct from LLM-based CoT reasoning. These encompass ensuring text-image consistency, improving image aesthetic quality, and designing sophisticated reward models, rather than relying on simpler rule-based rewards. While recent efforts have extended RL to this domain, these explorations typically lack an in-depth analysis of the domain-specific challenges and the characteristics of different RL strategies. To bridge this gap, we provide the first comprehensive investigation of the GRPO and DPO algorithms in autoregressive image generation, evaluating their ***in-domain*** performance and ***out-of-domain*** generalization, while scrutinizing the impact of ***different reward models*** on their respective capabilities. Our findings reveal that GRPO and DPO exhibit distinct advantages, and crucially, that reward models possessing stronger intrinsic generalization capabilities potentially enhance the generalization potential of the applied RL algorithms. Furthermore, we systematically explore ***three prevalent scaling strategies*** to enhance both their in-domain and out-of-domain proficiency, deriving unique insights into efficiently scaling performance for each paradigm. We hope our study paves a new path for inspiring future work on developing more effective RL algorithms to achieve robust CoT reasoning in the realm of autoregressive image generation.",
      "arxiv_url": "https://openreview.net/forum?id=tMf3keuPOl",
      "pdf_url": "https://openreview.net/pdf/5e8a0093ba272e1e3b615bc112a5e48cabb28f27.pdf",
      "primary_category": "Large Multi-modal Models, Image Generation, Reinforcement Learning",
      "categories": [
        "Large Multi-modal Models",
        "Image Generation",
        "Reinforcement Learning",
        "Chain-of-Thought (CoT) Reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tu3P6KSHGN",
      "title": "Safety Depth in Large Language Models: A Markov Chain Perspective",
      "authors": [
        "Ching-Chia Kao",
        "Chia-Mu Yu",
        "Chun-Shien Lu",
        "Chu-Song Chen"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly adopted in high-stakes scenarios, yet their safety mechanisms often remain fragile. Simple jailbreak prompts or even benign fine-tuning can bypass internal safeguards, underscoring the need to understand the failure modes of current safety strategies.  Recent findings suggest that vulnerabilities emerge when alignment is confined to only the initial output tokens. To address this, we introduce the notion of safety depth, a designated output position where the model refuses to generate harmful content. While deeper alignment appears promising, identifying the optimal safety depth remains an open and underexplored challenge.\n\nWe leverage the equivalence between autoregressive language models and Markov chains to derive the first theoretical result on identifying the optimal safety depth. To reach this safety depth effectively, we propose a cyclic group augmentation strategy that improves safety scores across six LLMs. In addition, we uncover a critical interaction between safety depth and ensemble width, demonstrating that larger ensembles can offset shallower alignments. These results suggest that test-time computation, often overlooked in safety alignment, can play a key role. Our approach provides actionable insights for building safer LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=tu3P6KSHGN",
      "pdf_url": "https://openreview.net/pdf/3f5b2b09f477f083a3a8d8af2018b13a3bd20cd9.pdf",
      "primary_category": "Safety Alignment, Large Language Models, Markov Chain",
      "categories": [
        "Safety Alignment",
        "Large Language Models",
        "Markov Chain",
        "Group Theory",
        "Ensemble"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "v6vBK4t8vB",
      "title": "Bilevel ZOFO: Efficient LLM Fine-Tuning and Meta-Training",
      "authors": [
        "Reza Shirkavand",
        "Peiran Yu",
        "Qi He",
        "Heng Huang"
      ],
      "abstract": "Fine-tuning pre-trained Large Language Models (LLMs) for downstream tasks using First-Order (FO) optimizers presents significant computational challenges. Parameter-Efficient Fine-Tuning~(PEFT) methods have been proposed to address these challenges by freezing most model parameters and training only a small subset. While PEFT is efficient, it may not outperform full fine-tuning when high task-specific performance is required.\nZeroth-Order (ZO) methods offer an alternative for fine-tuning the entire pre-trained model by approximating gradients using only the forward pass, thus eliminating the computational burden of back-propagation,\n% in first-order methods, \nbut they converge painfully slowly and are very sensitive to the choice of task prompts.\nWe bridge these worlds with Bilevel‑ZOFO, a penalty‑based bilevel formulation that treats adapter parameters as a lower‑level learner coupled to an upper‑level ZO optimizer of the full backbone. This double-loop optimization strategy only requires the gradient of the PEFT model and the forward pass of the base model. We provide theoretical convergence guarantees for Bilevel ZOFO. Empirically, we demonstrate that Bilevel-ZOFO significantly outperforms existing ZO methods, achieves 2–4$\\times$ faster training, and reduces sensitivity to prompts. Bilevel-ZOFO also outperforms FO PEFT methods while maintaining similar memory efficiency. Additionally, we show its strong potential for meta learning.",
      "arxiv_url": "https://openreview.net/forum?id=v6vBK4t8vB",
      "pdf_url": "https://openreview.net/pdf/6ad690d393c387448b51f66e6a9e12961eb772dd.pdf",
      "primary_category": "arge language model, Multi-Task Learning, Zeroth-order Fine-tuning",
      "categories": [
        "arge language model",
        "Multi-Task Learning",
        "Zeroth-order Fine-tuning",
        "Parameter Efficient Fine-tuning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8GjSf9Rh7Z",
      "title": "Titans: Learning to Memorize at Test Time",
      "authors": [
        "Ali Behrouz",
        "Peilin Zhong",
        "Vahab Mirrokni"
      ],
      "abstract": "Over more than a decade there has been an extensive research effort on how to effectively utilize recurrent models and attention. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows attending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling of dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a neural long-term memory module that learns to memorize historical context and helps attention to attend to the current context while utilizing long-past information. We show that this neural memory has the advantage of fast parallelizable training. From a memory perspective, we argue that attention due to its limited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its ability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce a new family of architectures, called Titans, and present three variants to address how one can effectively incorporate memory into this architecture. Our experimental results on language modeling, common-sense reasoning, and time series tasks show that Titans are effective compared to baselines, while they can effectively scale to larger context window in needle-in-haystack tasks.",
      "arxiv_url": "https://openreview.net/forum?id=8GjSf9Rh7Z",
      "pdf_url": "https://openreview.net/pdf/8cd62cb08869de77128e63a849345dea7da48b60.pdf",
      "primary_category": "Attention, Associative Memory, Long-term Memory",
      "categories": [
        "Attention",
        "Associative Memory",
        "Long-term Memory",
        "Hybrid Models"
      ],
      "tags": [
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "YdggdEL41C",
      "title": "Vision-centric Token Compression in Large Language Model",
      "authors": [
        "Ling Xing",
        "Alex Jinpeng Wang",
        "Rui Yan",
        "Xiangbo Shu",
        "Jinhui Tang"
      ],
      "abstract": "Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters.\nThis dual expansion send compute and memory costs skyrocketing, making $\\textit{token compression}$ indispensable.\nWe introduce Vision Centric Token Compression ($\\textbf{Vist}$), a $\\textit{slow–fast}$ compression framework that mirrors human reading: \nthe $\\textit{fast}$ path renders distant tokens into images, letting a $\\textbf{frozen, lightweight vision encoder}$ skim the low-salience context; \nthe $\\textit{slow}$ path feeds the proximal window into the LLM for fine-grained reasoning.\nA Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions—just as skilled reader gloss over function words.\nOn eleven in-context learning benchmarks, $\\textbf{Vist}$ achieves the same accuracy with 2.3$\\times$ fewer tokens, cutting FLOPs by 16\\% and memory by 50\\%.\nThis method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by $\\textbf{7.6}$\\% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The project is at https://github.com/CSU-JPG/VIST.",
      "arxiv_url": "https://openreview.net/forum?id=YdggdEL41C",
      "pdf_url": "https://openreview.net/pdf/4b6503e82dcc8dce34f5860c08697a359fe881cb.pdf",
      "primary_category": "Token Compression, Long Context LLMs, Large Language Model",
      "categories": [
        "Token Compression",
        "Long Context LLMs",
        "Large Language Model",
        "Visual-Text",
        "Vision-centric"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8PUzLga3lU",
      "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
      "authors": [
        "Chaoyou Fu",
        "Haojia Lin",
        "Xiong Wang",
        "YiFan Zhang",
        "Yunhang Shen",
        "Xiaoyu Liu",
        "Haoyu Cao",
        "Zuwei Long",
        "Heting Gao",
        "Ke Li",
        "Long MA",
        "Xiawu Zheng",
        "Rongrong Ji",
        "Xing Sun",
        "Caifeng Shan",
        "Ran He"
      ],
      "abstract": "Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing against state-of-the-art counterparts across benchmarks for image, video, and speech, we demonstrate that our omni model is equipped with both strong visual and speech capabilities, making omni understanding and interaction.",
      "arxiv_url": "https://openreview.net/forum?id=8PUzLga3lU",
      "pdf_url": "https://openreview.net/pdf/9b1f60741b0e613494a9f3c8aaabf54c5c988f7c.pdf",
      "primary_category": "Multimodal Large Language Models, Large Vision Language Models",
      "categories": [
        "Multimodal Large Language Models",
        "Large Vision Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7aSBAw7tJf",
      "title": "Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?",
      "authors": [
        "Xi Chen",
        "Kaituo Feng",
        "Changsheng Li",
        "Xunhao Lai",
        "Xiangyu Yue",
        "Ye Yuan",
        "Guoren Wang"
      ],
      "abstract": "Low-rank training has emerged as a promising approach for reducing memory usage in training Large Language Models (LLMs). Previous methods either rely on decomposing weight matrices (e.g., LoRA), or seek to decompose gradient matrices (e.g., GaLore) to ensure reduced memory consumption. However, both of them constrain the training in a low-rank subspace, thus inevitably leading to sub-optimal performance. To resolve this, we propose a new plug-and-play training framework for LLMs called Fira, as the first attempt to consistently preserve the low-rank constraint for memory efficiency, while achieving full-rank training (i.e., training with full-rank gradients of full-rank weights) to avoid inferior outcomes. \nFirst, we observe an interesting phenomenon during LLM training: the scaling impact of adaptive optimizers (e.g., Adam) on the gradient norm remains similar from low-rank to full-rank training. In light of this, we propose a \\textit{norm-based scaling} method, which utilizes the scaling impact of low-rank optimizers as substitutes for that of original full-rank optimizers to achieve this goal.\nMoreover, we find that there are potential loss spikes during training. To address this, we further put forward a norm-growth limiter to smooth the gradient.\nExtensive experiments on the pre-training and fine-tuning of LLMs show that Fira outperforms both LoRA and GaLore. Notably, for pre-training LLaMA 7B, our Fira uses $8\\times$ smaller memory of optimizer states than Galore, yet outperforms it by a large margin.",
      "arxiv_url": "https://openreview.net/forum?id=7aSBAw7tJf",
      "pdf_url": "https://openreview.net/pdf/623f06afa08d7e3bfb893ecd92b1f91f5238d10f.pdf",
      "primary_category": "Large Language Model, Memory Efficient Training, Full-rank Training",
      "categories": [
        "Large Language Model",
        "Memory Efficient Training",
        "Full-rank Training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2KKqp7MWJM",
      "title": "AgentAuditor: Human-level Safety and Security Evaluation for LLM Agents",
      "authors": [
        "Hanjun Luo",
        "Shenyu Dai",
        "Chiming Ni",
        "Xinfeng Li",
        "Guibin Zhang",
        "Kun Wang",
        "Tongliang Liu",
        "Hanan Salam"
      ],
      "abstract": "Despite the rapid advancement of LLM-based agents, the reliable evaluation of their safety and security remains a significant challenge. Existing rule-based or LLM-based evaluators often miss dangers in agents' step-by-step actions, overlook subtle meanings, fail to see how small issues compound, and get confused by unclear safety or security rules. To overcome this evaluation crisis, we introduce AgentAuditor, a universal, training-free, memory-augmented reasoning framework that empowers LLM evaluators to emulate human expert evaluators. AgentAuditor constructs an experiential memory by having an LLM adaptively extract structured semantic features (e.g., scenario, risk, behavior) and generate associated chain-of-thought reasoning traces for past interactions. A multi-stage, context-aware retrieval-augmented generation process then dynamically retrieves the most relevant reasoning experiences to guide the LLM evaluator's assessment of new cases. Moreover, we developed ASSEBench, the first benchmark designed to check how well LLM-based evaluators can spot both safety risks and security threats. ASSEBench comprises 2293 meticulously annotated interaction records, covering 15 risk types across 29 application scenarios. A key feature of ASSEBench is its nuanced approach to ambiguous risk situations, employing \"Strict\" and \"Lenient\" judgment standards. Experiments demonstrate that AgentAuditor not only consistently improves the evaluation performance of LLMs across all benchmarks but also sets a new state-of-the-art in LLM-as-a-judge for agent safety and security, achieving human-level accuracy. Our work is openly accessible at https://github.com/Astarojth/AgentAuditor-ASSEBench.",
      "arxiv_url": "https://openreview.net/forum?id=2KKqp7MWJM",
      "pdf_url": "https://openreview.net/pdf/d75abc5592e5b59fe8c700be9aa1d7b0f44a64b2.pdf",
      "primary_category": "Agent Safety, Agent Security, LLM-as-a-Judge",
      "categories": [
        "Agent Safety",
        "Agent Security",
        "LLM-as-a-Judge"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "m5wrqqcWbN",
      "title": "Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs",
      "authors": [
        "Insu Lee",
        "Wooje Park",
        "Jaeyun Jang",
        "Minyoung Noh",
        "Kyuhong Shim",
        "Byonghyo Shim"
      ],
      "abstract": "Large vision-language models (LVLMs) are increasingly deployed in interactive applications such as virtual and augmented reality, where a first-person (egocentric) view captured by head-mounted cameras serves as key input.\nWhile this view offers fine-grained cues about user attention and hand-object interactions, its narrow field of view and lack of global context often lead to failures on spatially or contextually demanding queries.\nTo address this, we introduce a framework that augments egocentric inputs with third-person (exocentric) views, providing complementary information such as global scene layout and object visibility to LVLMs.\nWe present E3VQA, the first benchmark for multi-view question answering with 4K high-quality question-answer pairs grounded in synchronized ego-exo image pairs.\nAdditionally, we propose M3CoT, a training-free prompting technique that constructs a unified scene representation by integrating scene graphs from three complementary perspectives.\nM3CoT enables LVLMs to reason more effectively across views, yielding consistent performance gains (4.84\\% for GPT-4o and 5.94\\% for Gemini 2.0 Flash) over a recent CoT baseline.\nOur extensive evaluation reveals key strengths and limitations of LVLMs in multi-view reasoning and highlights the value of leveraging both egocentric and exocentric inputs.\nThe dataset and source code are available at [https://github.com/Leeinsu1/Towards-Comprehensive-Scene-Understanding](https://github.com/Leeinsu1/Towards-Comprehensive-Scene-Understanding).",
      "arxiv_url": "https://openreview.net/forum?id=m5wrqqcWbN",
      "pdf_url": "https://openreview.net/pdf/18fca9c164e4dd85cd23d2aa4f2e047bed8a2348.pdf",
      "primary_category": "Large Vision-Language Models, Egocentric, Exocentric",
      "categories": [
        "Large Vision-Language Models",
        "Egocentric",
        "Exocentric",
        "Visual Question Answering"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "s3IT4Qo7bm",
      "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval",
      "authors": [
        "Wenhao Li",
        "Yuxin Zhang",
        "Gen Luo",
        "Haiyuan Wan",
        "ZiYang Gong",
        "Fei Chao",
        "Rongrong Ji"
      ],
      "abstract": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in LLMs. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5$\\times$ compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a single A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla decoding.",
      "arxiv_url": "https://openreview.net/forum?id=s3IT4Qo7bm",
      "pdf_url": "https://openreview.net/pdf/ed67f98841d797ac112893481f9f76623cdb124a.pdf",
      "primary_category": "KV Cache Compression, Efficient Attention",
      "categories": [
        "KV Cache Compression",
        "Efficient Attention"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "fy5InEg0OL",
      "title": "Do different prompting methods yield a common task representation in language models?",
      "authors": [
        "Guy Davidson",
        "Todd M. Gureckis",
        "Brenden Lake",
        "Adina Williams"
      ],
      "abstract": "Demonstrations and instructions are two primary approaches for prompting language models to perform in-context learning (ICL) tasks.\nDo identical tasks elicited in different ways result in similar representations of the task? An improved understanding of task representation mechanisms would offer interpretability insights and may aid in steering models. We study this through function vectors (FVs), recently proposed as a mechanism to extract few-shot ICL task representations. We generalize FVs to alternative task presentations, focusing on short textual instruction prompts, and successfully extract instruction function vectors that promote zero-shot task accuracy. We find evidence that demonstration- and instruction-based function vectors leverage different model components, and offer several controls to dissociate their contributions to task performance. Our results suggest that different task prompting forms do not induce a common task representation through FVs but elicit different, partly overlapping mechanisms. Our findings offer principled support to the practice of combining instructions and task demonstrations, imply challenges in universally monitoring task inference across presentation forms, and encourage further examinations of LLM task inference mechanisms.",
      "arxiv_url": "https://openreview.net/forum?id=fy5InEg0OL",
      "pdf_url": "https://openreview.net/pdf/cccf1e3147fa4fa8a3429b57915872acc655b415.pdf",
      "primary_category": "task representations; function vectors; in-context learning; instructions; mechanistic interpretability",
      "categories": [
        "task representations; function vectors; in-context learning; instructions; mechanistic interpretability"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "p8HwGym5EH",
      "title": "Metritocracy: Representative Metrics for Lite Benchmarks",
      "authors": [
        "Ariel D. Procaccia",
        "Benjamin Schiffer",
        "Serena Lutong Wang",
        "Shirley Zhang"
      ],
      "abstract": "A common problem in LLM evaluation is how to choose a subset of metrics from a full suite of possible metrics. Subset selection is usually done for efficiency or interpretability reasons, and the goal is often to select a \"representative\" subset of metrics. However, \"representative\" is rarely clearly defined. In this work, we use ideas from social choice theory to formalize two notions of representation for the selection of a subset of evaluation metrics. We first introduce *positional representation*, which guarantees every alternative is sufficiently represented at every position cutoff. We then introduce *positional proportionality*, which guarantees no alternative is proportionally over- or under-represented by more than a small error at any position. We prove upper and lower bounds on the smallest number of metrics needed to guarantee either of these properties in the worst case. We also study a generalized form of each property that allows for additional input on groups of metrics that must be represented. Finally, we tie theory to practice through real-world case studies on both LLM evaluation and hospital quality evaluation.",
      "arxiv_url": "https://openreview.net/forum?id=p8HwGym5EH",
      "pdf_url": "https://openreview.net/pdf/cd6ad9a902aec3c10e7ef7813f7181cd47692b0c.pdf",
      "primary_category": "Evaluation, benchmarks, social choice",
      "categories": [
        "Evaluation",
        "benchmarks",
        "social choice"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8LMwwt8E2s",
      "title": "Probabilistic Token Alignment for Large Language Model Fusion",
      "authors": [
        "Runjia Zeng",
        "James Chenhao Liang",
        "Cheng Han",
        "Zhiwen Cao",
        "Jiahao Liu",
        "Xiaojun Quan",
        "Yingjie Victor Chen",
        "Lifu Huang",
        "Tong Geng",
        "Qifan Wang",
        "Dongfang Liu"
      ],
      "abstract": "Training large language models (LLMs) from scratch can yield models with unique functionalities and strengths, but it is costly and often leads to redundant capabilities. A more cost-effective alternative is to fuse existing pre-trained LLMs with different architectures into a more powerful model. However, a key challenge in existing model fusion is their dependence on manually predefined vocabulary alignment, which may not generalize well across diverse contexts, leading to performance degradation in several evaluation. To solve this, we draw inspiration from distribution learning and propose the probabilistic token alignment method as a general and soft mapping for alignment, named as PTA-LLM. Our approach innovatively reformulates token alignment into a classic mathematical problem: optimal transport, seamlessly leveraging distribution-aware learning to facilitate more coherent model fusion. Apart from its inherent generality, PTA-LLM exhibits interpretability from a distributional perspective, offering insights into the essence of the token alignment. Empirical results demonstrate that probabilistic token alignment enhances the target model's performance across multiple capabilities.",
      "arxiv_url": "https://openreview.net/forum?id=8LMwwt8E2s",
      "pdf_url": "https://openreview.net/pdf/2f1bdbc7173ee16573f3ef94b303e12a2ce05860.pdf",
      "primary_category": "knowledge fusion, model fusion, LLMs",
      "categories": [
        "knowledge fusion",
        "model fusion",
        "LLMs",
        "token alignment",
        "optimal transport"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "bRWkBD2BfK",
      "title": "VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs",
      "authors": [
        "Shmuel Berman",
        "Jia Deng"
      ],
      "abstract": "Vision Language Models (VLMs) excel at complex visual tasks such as VQA and chart understanding, yet recent work suggests they struggle with simple perceptual tests. We present an evaluation that tests vision-language models’ capacity for non-local visual reasoning— reasoning that requires chaining evidence collected from multiple, possibly distant, regions of an image.  We isolate three distinct forms of non‑local vision: comparative perception, which demands holding two images in working memory and comparing them; saccadic search, which requires making discrete, evidence‑driven jumps to locate successive targets; and smooth visual search, which involves searching smoothly along a continuous contour. Flagship models (e.g., GPT-5, Gemini 2.5 Pro, Claude Sonnet 4), even those that perform well on prior primitive‑vision benchmarks, fail these tests and barely exceed random accuracy on two variants of our tasks that are trivial for humans. Our structured evaluation suite allows us to test if VLMs can perform similar visual algorithms to humans.  Our findings show that despite gains in raw visual acuity, current models lack core visual reasoning capabilities.",
      "arxiv_url": "https://openreview.net/forum?id=bRWkBD2BfK",
      "pdf_url": "https://openreview.net/pdf/ca79a743ac55c7fce5e100be3b2d695bf84a71ba.pdf",
      "primary_category": "vision, vlms, visual understanding",
      "categories": [
        "vision",
        "vlms",
        "visual understanding",
        "visual reasoning",
        "reasoning"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Xr73jEYG29",
      "title": "LLM-PySC2: Starcraft II learning environment for Large Language Models",
      "authors": [
        "Zongyuan Li",
        "Yanan Ni",
        "Runnan Qi",
        "Chang Lu",
        "Lumin Jiang",
        "Xu Xiaojie",
        "Xiangbei Liu",
        "Pengfei Li",
        "Yunzheng Guo",
        "Zhe Ma",
        "Huanyu Li",
        "wu hui",
        "Guo Xian",
        "Kuihua Huang",
        "Xuebo Zhang"
      ],
      "abstract": "The tremendous potential has been demonstrated by large language models (LLMs) in intelligent decision-making problems, with unprecedented capabilities shown across diverse applications ranging from gaming AI systems to complex strategic planning frameworks. However, the StarCraft II platform, which has been widely adopted for validating decision-making algorithms in the past decade, has not yet provided substantial support for this emerging domain. To address issues that LLMs cannot interface with the hundreds of actions of the pysc2 backend and the lack of native support for multi-agent (MA) collaboration, we propose the LLM-PySC2 environment. This is the first environment that offers LLMs the complete pysc2 action space with sufficient multi-modal information and game Wiki knowledge. With an asynchronous query architecture, the environment efficiently interacts with LLMs that maintain a constant latency regardless of the scale of the agents' population. In the experiments, we evaluated LLMs' decision-making performance in both the macro-decision and micro-operation scenarios, with traditional StarCraft II Multi-Agent Challenge (SMAC) tasks and a series of new proposed. Results indicate that LLMs possess the potential to achieve victories in complex scenarios but cannot constantly generate correct decisions, especially in the recovered pysc2 action space and MA settings. Without task-relevant instructions, the pre-trained models suffer from issues such as hallucinations and inefficient collaboration. \nOur findings suggest that StarCraft II still challenges in the era of large models, revealing that there is a lot to do to develop an advanced LLM decision-making system, and the proposed LLM-PySC2 environment will support future development of LLM-based decision-making solutions.",
      "arxiv_url": "https://openreview.net/forum?id=Xr73jEYG29",
      "pdf_url": "https://openreview.net/pdf/ae23b5a1cbb9529359c184cdf3d394c6383add5a.pdf",
      "primary_category": "Large Language Model, Decision-Making, Multi-Agent",
      "categories": [
        "Large Language Model",
        "Decision-Making",
        "Multi-Agent",
        "StarCraft II Environment"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nsv3ogqRIU",
      "title": "Adaptive Latent-Space Constraints in Personalized Federated Learning",
      "authors": [
        "Sana Ayromlou",
        "D. B. Emerson"
      ],
      "abstract": "Federated learning (FL) is an effective and widely used approach to training deep learning models on decentralized datasets held by distinct clients. FL also strengthens both security and privacy protections for training data. Common challenges associated with statistical heterogeneity between distributed datasets have spurred significant interest in personalized FL (pFL) methods, where models combine aspects of global learning with local modeling specific to each client’s unique characteristics. This work investigates the efficacy of theoretically supported, adaptive MMD measures in pFL, primarily focusing on the Ditto framework, a state-of-the-art technique for distributed data heterogeneity. The use of such measures significantly improves model performance across a variety of tasks, especially those with pronounced feature heterogeneity. Additional experiments demonstrate that such measures are directly applicable to other pFL techniques and yield similar improvements across a number of datasets. Finally, the results motivate the use of constraints tailored to the various kinds of heterogeneity expected in FL systems.",
      "arxiv_url": "https://openreview.net/forum?id=nsv3ogqRIU",
      "pdf_url": "https://openreview.net/pdf/2160277ae945271744e5f47db3803f187e5f5f71.pdf",
      "primary_category": "Federated Learning, Maximum Mean Discrepancy, Data Heterogeneity",
      "categories": [
        "Federated Learning",
        "Maximum Mean Discrepancy",
        "Data Heterogeneity",
        "Deep Learning",
        "Medical Imaging"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yPXOfBoQL7",
      "title": "FALQON: Accelerating LoRA Fine-tuning with Low-Bit Floating-Point Arithmetic",
      "authors": [
        "Kanghyun Choi",
        "Hyeyoon Lee",
        "SunJong Park",
        "Dain Kwon",
        "Jinho Lee"
      ],
      "abstract": "Low-bit floating-point (FP) formats, such as FP8, provide significant acceleration and memory savings in model training thanks to native hardware support on modern GPUs and NPUs. However, we analyze that FP8 quantization offers speedup primarily for large-dimensional matrix multiplications, while inherent quantization overheads diminish speedup when applied to low-rank adaptation (LoRA), which uses small-dimensional matrices for efficient fine-tuning of large language models (LLMs). To address this limitation, we propose FALQON, a novel framework that eliminates the quantization overhead from separate LoRA computational paths by directly merging LoRA adapters into an FP8-quantized backbone during fine-tuning. Furthermore, we reformulate the forward and backward computations for merged adapters to significantly reduce quantization overhead, and introduce a row-wise proxy update mechanism that efficiently integrates substantial updates into the quantized backbone. Experimental evaluations demonstrate that FALQON achieves approximately a 3$\\times$ training speedup over existing quantized LoRA methods with a similar level of accuracy, providing a practical solution for efficient large-scale model fine-tuning. Moreover, FALQON’s end-to-end FP8 workflow removes the need for post-training quantization, facilitating efficient deployment. Code is available at https://github.com/iamkanghyunchoi/falqon.",
      "arxiv_url": "https://openreview.net/forum?id=yPXOfBoQL7",
      "pdf_url": "https://openreview.net/pdf/5a683d332fb2a627ab2b9914a6fb47c792ba9c35.pdf",
      "primary_category": "FP8, Quantization, LLM",
      "categories": [
        "FP8",
        "Quantization",
        "LLM",
        "LoRA",
        "Quantized training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DG0F1cdjN7",
      "title": "RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models",
      "authors": [
        "Yeongtak Oh",
        "Dohyun Chung",
        "Juhyeon Shin",
        "Sangha Park",
        "Johan Barthelemy",
        "Jisoo Mok",
        "Sungroh Yoon"
      ],
      "abstract": "Recent multi-modal large language models (MLLMs) often struggle to generate personalized image captions, even when trained on high-quality captions. In this work, we observe that such limitations persist in existing post-training-based MLLM personalization methods. Specifically, despite being post-tuned with large-scale caption data through supervised fine-tuning (SFT), these models frequently fail to produce faithful descriptions in real-world scenarios, such as multi-concept image captioning. However, acquiring large-scale, high-quality captions for such complex settings is both costly and difficult. To address the data-centric nature of SFT, we propose a reinforcement learning (RL)-based post-training framework. To the best of our knowledge, this is the first RL-based approach to post-train MLLMs for personalized image captioning. Our method significantly enhances both visual recognition and personalized generation capabilities of MLLMs, and consistently outperforms existing SFT-based baselines, especially in the challenging multi-concept image captioning task. Project page: https://github.com/oyt9306/RePIC",
      "arxiv_url": "https://openreview.net/forum?id=DG0F1cdjN7",
      "pdf_url": "https://openreview.net/pdf/cf2d4f687d551cd5029190a05298e78fb019b4b6.pdf",
      "primary_category": "Multi-Modal Large Language Models, Personalization, Post-Training",
      "categories": [
        "Multi-Modal Large Language Models",
        "Personalization",
        "Post-Training"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vSLzoUoJt6",
      "title": "OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis",
      "authors": [
        "Junting Chen",
        "Haotian Liang",
        "Lingxiao Du",
        "Weiyun Wang",
        "Mengkang Hu",
        "Yao Mu",
        "Wenhai Wang",
        "Jifeng Dai",
        "Ping Luo",
        "Wenqi Shao",
        "Lin Shao"
      ],
      "abstract": "The rapid progress of navigation, manipulation, and vision models has made mobile manipulators capable in many specialized tasks. \nHowever, the open-world mobile manipulation (OWMM) task remains a challenge due to the need for generalization to open-ended instructions and environments, as well as the systematic complexity to integrate high-level decision making with low-level robot control based on both global scene understanding and current agent state. To address this complexity, we propose a novel multi-modal agent architecture that maintains multi-view scene frames and agent states for decision-making and controls the robot by function calling.\nA second challenge is the hallucination from domain shift. To enhance the agent performance, we further introduce an agentic data synthesis pipeline for the OWMM task to adapt the VLM model to our task domain with instruction fine-tuning. We highlight our fine-tuned OWMM-VLM as the first dedicated foundation model for mobile manipulators with global scene understanding, robot state tracking, and multi-modal action generation in a unified model. Through experiments, we demonstrate that our model achieves SOTA performance compared to other foundation models including GPT-4o and strong zero-shot generalization in real world.\nThe project page is at https://hhyhrhy.github.io/owmm-agent-project.",
      "arxiv_url": "https://openreview.net/forum?id=vSLzoUoJt6",
      "pdf_url": "https://openreview.net/pdf/b83bcc6b13bf3bed81ebb73be9bae7cc2be710e7.pdf",
      "primary_category": "Embodied AI, Mobile Manipulation, Agentic Data Synthesis",
      "categories": [
        "Embodied AI",
        "Mobile Manipulation",
        "Agentic Data Synthesis",
        "Vision-Language Model",
        "LLM Agent"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "JddJvNSiHk",
      "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning",
      "authors": [
        "Jiaqi Chen",
        "Bang Zhang",
        "Ruotian Ma",
        "Peisong Wang",
        "Xiaodan Liang",
        "Zhaopeng Tu",
        "Xiaolong Li",
        "Kwan-Yee K. Wong"
      ],
      "abstract": "Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a \"sneaky generator\" that deliberately produces erroneous steps designed to be difficult to detect, and a \"critic\" that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, SPC can guide the test-time search of diverse LLMs and significantly improve their mathematical reasoning performance on MATH500 and AIME2024, surpassing those guided by state-of-the-art process reward models.",
      "arxiv_url": "https://openreview.net/forum?id=JddJvNSiHk",
      "pdf_url": "https://openreview.net/pdf/37589115324975c5657483b37546cdd4916aeeed.pdf",
      "primary_category": "Self-play, critic model, Reasoning",
      "categories": [
        "Self-play",
        "critic model",
        "Reasoning",
        "RL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "KGt0F2yjBz",
      "title": "Angles Don’t Lie: Unlocking Training‑Efficient RL Through the Model’s Own Signals",
      "authors": [
        "Qinsi Wang",
        "Jinghan Ke",
        "Hancheng Ye",
        "Yueqian Lin",
        "Yuzhe Fu",
        "Jianyi Zhang",
        "Kurt Keutzer",
        "Chenfeng Xu",
        "Yiran Chen"
      ],
      "abstract": "Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency due to the redundant exposure of identical queries under uniform data sampling. While previous work has explored curriculum learning via heuristic difficulty metrics, these strategies exhibit limitations by neglecting the intrinsic learning signals generated by the model itself, thus leading to suboptimal training regimes. In this paper, we identify a model-inherent signal termed *angle concentration* that effectively reflects an LLM's capacity to learn from specific data. We theoretically and empirically demonstrate a correlation between the angular distribution of token hidden state vectors and the resulting gradient, revealing a learning preference for data exhibiting higher angle concentration. Inspired by this finding, we propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. \nBy leveraging the model's intrinsic angle concentration signal, GAIN-RL dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5$\\times$ acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient sampling yields data-efficient training, achieving better performance with half the original data compared to vanilla GRPO with full training data.",
      "arxiv_url": "https://openreview.net/forum?id=KGt0F2yjBz",
      "pdf_url": "https://openreview.net/pdf/3127599568ea4ccd2c4059081cd513cad55b9b26.pdf",
      "primary_category": "LLM Efficiency, Data Optimization, Reinforcement Fine-Tuning",
      "categories": [
        "LLM Efficiency",
        "Data Optimization",
        "Reinforcement Fine-Tuning",
        "Training‑Efficient RFT"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "R6m6bNnmWm",
      "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning",
      "authors": [
        "Senqiao Yang",
        "Junyi Li",
        "Xin Lai",
        "Jinming Wu",
        "Wei Li",
        "Zejun MA",
        "Bei Yu",
        "Hengshuang Zhao",
        "Jiaya Jia"
      ],
      "abstract": "Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens.\nHowever, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution.\nTherefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink.\nIt starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks.\nWe adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreoever, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio.\nExtensive experiments demonstrate the superiority, efficiency, and effectiveness of our method.\nAll our code and data are open-sourced.",
      "arxiv_url": "https://openreview.net/forum?id=R6m6bNnmWm",
      "pdf_url": "https://openreview.net/pdf/a1ce03f1786df2e67c61dd99ba4f40d2d92f913b.pdf",
      "primary_category": "Vision Language Models, Reinforcement Learning",
      "categories": [
        "Vision Language Models",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "p2EycV4XOa",
      "title": "Domain-RAG: Retrieval-Guided Compositional Image Generation for Cross-Domain Few-Shot Object Detection",
      "authors": [
        "Yu Li",
        "Xingyu Qiu",
        "Yuqian Fu",
        "Jie Chen",
        "Tianwen Qian",
        "Xu Zheng",
        "Danda Pani Paudel",
        "Yanwei Fu",
        "Xuanjing Huang",
        "Luc Van Gool",
        "Yu-Gang Jiang"
      ],
      "abstract": "Cross-Domain Few-Shot Object Detection (CD-FSOD) aims to detect novel objects with only a handful of labeled samples from previously unseen domains. While data augmentation and generative methods have shown promise in few-shot learning, their effectiveness for CD-FSOD remains unclear due to the need for both visual realism and domain alignment. Existing strategies, such as copy-paste augmentation and text-to-image generation, often fail to preserve the correct object category or produce backgrounds coherent with the target domain, making them non-trivial to apply directly to CD-FSOD. To address these challenges, we propose Domain-RAG, a training-free, retrieval-guided compositional image generation framework tailored for CD-FSOD. Domain-RAG consists of three stages: domain-aware background retrieval, domain-guided background generation, and foreground-background composition. Specifically, the input image is first decomposed into foreground and background regions. We then retrieve semantically and stylistically similar images to guide a generative model in synthesizing a new background, conditioned on both the original and retrieved contexts. Finally, the preserved foreground is composed with the newly generated domain-aligned background to form the generated image. Without requiring any additional supervision or training, Domain-RAG produces high-quality, domain-consistent samples across diverse tasks, including CD-FSOD, remote sensing FSOD, and camouflaged FSOD. Extensive experiments show consistent improvements over strong baselines and establish new state-of-the-art results. Codes will be released upon acceptance.The source code and instructions are available at https://github.com/LiYu0524/Domain-RAG.",
      "arxiv_url": "https://openreview.net/forum?id=p2EycV4XOa",
      "pdf_url": "https://openreview.net/pdf/cafce1bda787cc212170de9d0b105679d7338f03.pdf",
      "primary_category": "Cross-Domain Few-Shot Object Detection;  RAG; Few-Shot Learning;",
      "categories": [
        "Cross-Domain Few-Shot Object Detection;  RAG; Few-Shot Learning;"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "LKAp7Dknxf",
      "title": "LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization",
      "authors": [
        "Zhenpeng Huang",
        "Jiaqi Li",
        "Zihan Jia",
        "Xinhao Li",
        "Desen Meng",
        "Lingxue Song",
        "Xi Chen",
        "Liang Li",
        "Limin Wang"
      ],
      "abstract": "We present LongVPO, a novel two‑stage Direct Preference Optimization framework that enables short‑context vision‑language models to robustly understand ultra‑long videos without any long‑video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual‑similarity and question‑specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model’s scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, and then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, \\model{} outperforms the state‑of‑the‑art open‑source models on multiple long‑video benchmarks, while maintaining strong short‑video performance (e.g., on MVBench), offering a scalable paradigm for efficient long‑form video understanding.",
      "arxiv_url": "https://openreview.net/forum?id=LKAp7Dknxf",
      "pdf_url": "https://openreview.net/pdf/656de1a642f0c514a335cfb120f84002379596f0.pdf",
      "primary_category": "Video Large Language Models; Long Video Understanding; Preference Optimization",
      "categories": [
        "Video Large Language Models; Long Video Understanding; Preference Optimization"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "894Yo61h1P",
      "title": "Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning",
      "authors": [
        "Jiwon Song",
        "Dongwon Jo",
        "Yulhwa Kim",
        "Jae-Joon Kim"
      ],
      "abstract": "Recent reasoning-focused language models achieve high accuracy by generating lengthy intermediate reasoning paths before producing final answers.\nWhile this approach is effective in solving problems that require logical thinking, long reasoning paths significantly increase memory usage and reduce throughput of token generation, limiting the practical deployment of such models.\nWe propose Reasoning Path Compression (RPC), a training-free method that accelerates inference by leveraging the semantic sparsity of reasoning paths.\nRPC periodically compresses the KV cache by retaining cache entries that receive high importance score, which are computed using a selector window composed of recently generated queries.\nExperiments show that RPC improves generation throughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full KV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark.\nOur findings demonstrate that semantic sparsity in reasoning traces can be effectively exploited for compression, offering a practical path toward efficient deployment of reasoning LLMs. Our code is available at https://github.com/jiwonsong-dev/ReasoningPathCompression.",
      "arxiv_url": "https://openreview.net/forum?id=894Yo61h1P",
      "pdf_url": "https://openreview.net/pdf/1da03840d2fbec379445fdc3e64cb5047dfc2493.pdf",
      "primary_category": "large language model, reasoning, test-time compute scaling",
      "categories": [
        "large language model",
        "reasoning",
        "test-time compute scaling",
        "KV cache"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tTwZhy8JqY",
      "title": "SeCon-RAG: A Two-Stage Semantic  Filtering and Conflict-Free Framework for Trustworthy RAG",
      "authors": [
        "Xiaonan si",
        "Meilin Zhu",
        "Simeng Qin",
        "Lijia Yu",
        "Lijun Zhang",
        "Shuaitong Liu",
        "Xinfeng Li",
        "Ranjie Duan",
        "Yang Liu",
        "Xiaojun Jia"
      ],
      "abstract": "Retrieval-augmented generation (RAG) systems enhance large language models (LLMs) with external knowledge but are vulnerable to corpus poisoning and contamination attacks, which can compromise output integrity. Existing defenses often apply aggressive filtering, leading to unnecessary loss of valuable information and reduced reliability in generation.\nTo address this problem, we propose a two-stage semantic filtering and conflict-free framework for trustworthy RAG. \nIn the first stage, we perform a joint filter with semantic and cluster-based filtering  which is guided by the Entity-intent-relation extractor (EIRE). EIRE extracts entities, latent objectives, and entity relations from both the user query and filtered documents, scores their semantic relevance, and selectively adds valuable documents into the clean retrieval database. \nIn the second stage, we proposed an EIRE-guided conflict-aware filtering module, which analyzes semantic consistency between the query, candidate answers, and retrieved knowledge before final answer generation, filtering out internal and external contradictions that could mislead the model.\nThrough this two-stage process, SeCon-RAG effectively preserves useful knowledge while mitigating conflict contamination, achieving significant improvements in both generation robustness and output trustworthiness.\nExtensive experiments across various LLMs and datasets demonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art defense methods.",
      "arxiv_url": "https://openreview.net/forum?id=tTwZhy8JqY",
      "pdf_url": "https://openreview.net/pdf/12f20519cee84a096511707c242d735d6607f293.pdf",
      "primary_category": "RAG, Prompt of LLMs, Defense against RAG attack",
      "categories": [
        "RAG",
        "Prompt of LLMs",
        "Defense against RAG attack"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cnUq7GkS6d",
      "title": "Retrieval is Not Enough: Enhancing RAG through Test-Time Critique and Optimization",
      "authors": [
        "Jiaqi Wei",
        "Hao Zhou",
        "Xiang Zhang",
        "Di Zhang",
        "Zijie Qiu",
        "Noah Wei",
        "Jinzhe Li",
        "Wanli Ouyang",
        "Siqi Sun"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has become a widely adopted paradigm for enabling knowledge-grounded large language models (LLMs). However, standard RAG pipelines often fail to ensure that model reasoning remains consistent with the evidence retrieved, leading to factual inconsistencies or unsupported conclusions. In this work, we reinterpret RAG as \\textit{Retrieval-Augmented Reasoning} and identify a central but underexplored problem: \\textit{Reasoning Misalignment}—the divergence between an LLM's internal reasoning trajectory and the evidential constraints provided by retrieval. To address this issue, we propose \\textsc{AlignRAG}, a novel iterative framework grounded in \\textit{Critique-Driven Alignment (CDA)}. We further introduce \\textsc{AlignRAG-auto}, an autonomous variant that dynamically terminates refinement, removing the need to pre-specify the number of critique iterations. At the heart of \\textsc{AlignRAG} lies a \\textit{contrastive critique synthesis} mechanism that generates retrieval-sensitive critiques while mitigating self-bias. This mechanism trains a dedicated retrieval-augmented \\textit{Critic Language Model (CLM)} using labeled critiques that distinguish between evidence-aligned and misaligned reasoning. Empirical evaluations show that our approach significantly improves reasoning fidelity. Our 8B-parameter CLM improves performance over the Self-Refine baseline by \\textbf{12.1\\%} on out-of-domain tasks and outperforms a standard 72B-parameter CLM by \\textbf{2.2\\%}. Furthermore, \\textsc{AlignRAG-auto} achieves this state-of-the-art performance while dynamically determining the optimal number of refinement steps, enhancing efficiency and usability. \\textsc{AlignRAG} remains compatible with existing RAG architectures as a \\textit{plug-and-play} module and demonstrates strong robustness under both informative and noisy retrieval scenarios. Overall, \\textsc{AlignRAG} offers a principled solution for aligning model reasoning with retrieved evidence, substantially improving the factual reliability and robustness of RAG systems. Our source code is provided at \\href{https://github.com/upup-wei/RAG-ReasonAlignment}{link}.",
      "arxiv_url": "https://openreview.net/forum?id=cnUq7GkS6d",
      "pdf_url": "https://openreview.net/pdf/84142e9390a5dfd326ba8a046940f2ff342527cc.pdf",
      "primary_category": "Retrieval-Augmented Generation (RAG), Test-time Scaling",
      "categories": [
        "Retrieval-Augmented Generation (RAG)",
        "Test-time Scaling"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "V75MK7uh67",
      "title": "Personalized Bayesian Federated Learning with Wasserstein Barycenter Aggregation",
      "authors": [
        "Ting Wei",
        "Biao Mei",
        "Junliang Lyu",
        "Renquan Zhang",
        "Feng Zhou",
        "Yifan Sun"
      ],
      "abstract": "Personalized Bayesian federated learning (PBFL) handles non-i.i.d. client data and quantifies uncertainty by combining personalization with Bayesian inference. However, current PBFL methods face two main limitations: posterior inference on clients often assumes restrictive parametric forms, and server-side posterior aggregation typically relies on naive parameter averaging. To overcome these issues, we propose FedWBA, a novel PBFL method that enhances both local inference and global aggregation. At the client level, we use particle-based variational inference for nonparametric posterior representation. At the server level, we introduce particle-based Wasserstein barycenter aggregation, offering a more geometrically meaningful approach. Theoretically, we provide local and global convergence guarantees for FedWBA. Locally, we prove a KL divergence decrease lower bound per iteration for variational inference convergence. Globally, we show that the Wasserstein barycenter converges to the true parameter as the client data size increases. Empirically, experiments show that FedWBA outperforms baselines in prediction accuracy, uncertainty calibration, and convergence rate, with ablation studies confirming its robustness.",
      "arxiv_url": "https://openreview.net/forum?id=V75MK7uh67",
      "pdf_url": "https://openreview.net/pdf/16d3a8da976fd902f3fb517f9d9c759a7a18d56e.pdf",
      "primary_category": "Personalized Bayesian Federated Learning, Stein Variational Gradient Descent, Wasserstein Barycenter Aggregation",
      "categories": [
        "Personalized Bayesian Federated Learning",
        "Stein Variational Gradient Descent",
        "Wasserstein Barycenter Aggregation"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oUghNi5XWc",
      "title": "SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs",
      "authors": [
        "Jinhong Deng",
        "Wen Li",
        "Joey Tianyi Zhou",
        "Yang He"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) typically process a large number of visual tokens, leading to considerable computational overhead, even though many of these tokens are redundant. Existing visual token pruning methods primarily focus on selecting the most salient tokens based on attention scores, resulting in the semantic incompleteness of the selected tokens. In this paper, we propose a novel visual token pruning strategy, called **S**aliency-**C**overage **O**riented token **P**runing for **E**fficient MLLMs (SCOPE), to jointly model both the saliency and coverage of the selected visual tokens to better preserve semantic completeness. Specifically, we introduce a set-coverage for a given set of selected tokens, computed based on the token relationships. We then define a token-coverage gain for each unselected token, quantifying how much additional coverage would be obtained by including it. By integrating the saliency score into the token-coverage gain, we propose our SCOPE score and iteratively select the token with the highest SCOPE score. We conduct extensive experiments on multiple vision-language understanding benchmarks using the LLaVA-1.5 and LLaVA-Next models. Experimental results demonstrate that our method consistently outperforms prior approaches.",
      "arxiv_url": "https://openreview.net/forum?id=oUghNi5XWc",
      "pdf_url": "https://openreview.net/pdf/c831447b06dbadc03fcdd6afd8a908e5b220eb4c.pdf",
      "primary_category": "Visual Token Pruning, Efficient MLLM",
      "categories": [
        "Visual Token Pruning",
        "Efficient MLLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "YktFxpaEmR",
      "title": "Selftok-Zero: Reinforcement Learning for Visual Generation via Discrete and Autoregressive Visual Tokens",
      "authors": [
        "Bohan Wang",
        "Mingze Zhou",
        "Zhongqi Yue",
        "Wang Lin",
        "Kaihang Pan",
        "Liyu Jia",
        "Wentao Hu",
        "Wei Zhao",
        "Hanwang Zhang"
      ],
      "abstract": "Reinforcement learning (RL) has become an indispensable post-training step for unlocking the full potential of Large Language Models (LLMs). Its core motivation is to incentivize the model’s inference trajectory via a reward model, effectively balancing the exploration–exploitation trade-off in scenarios where collecting exhaustive input–output ground-truth pairs is infeasible. This motivation naturally extends to visual generation, where perfect alignment between an image and a textual prompt is inherently ambiguous and often unattainable. However, existing visual generative models are not yet ready for RL due to the following two fundamental drawbacks that undermine the foundations of RL: 1) For diffusion-based models, the actual generation trajectories of sampled images cannot be reliably rewarded, as diffusion inversion is notoriously difficult. 2) For autoregressive (AR) models, we show that the widely used spatial visual tokens do not satisfy the Bellman equation and thus violate the policy improvement theorem of RL. To this end, we propose to use Selftok (Self-consistency Tokenizer), which represents each image as a sequential 1D stream of discrete, autoregressive tokens. Together with language, we train a pure AR vision-language model (VLM) for visual generation. Impressively, without using any text-image training pairs, a simple policy gradient algorithm applied to Selftok tokens significantly boosts visual generation performance, surpassing existing models by a large margin. Implementation details are provided in the Appendix.",
      "arxiv_url": "https://openreview.net/forum?id=YktFxpaEmR",
      "pdf_url": "https://openreview.net/pdf/75e9ddf2fa015039429cc0508904811592630c03.pdf",
      "primary_category": "Tokenizer, MLLM, Reinforcement Learning",
      "categories": [
        "Tokenizer",
        "MLLM",
        "Reinforcement Learning"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vAT2xlaWJY",
      "title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants",
      "authors": [
        "Zeyu Zhang",
        "Quanyu Dai",
        "Luyu Chen",
        "Zeren Jiang",
        "Rui Li",
        "Jieming Zhu",
        "Xu Chen",
        "Yi Xie",
        "Zhenhua Dong",
        "Ji-Rong Wen"
      ],
      "abstract": "LLM-based agents have been widely applied as personal assistants, capable of memorizing information from user messages and responding to personal queries. However, there still lacks an objective and automatic evaluation on their memory capability, largely due to the challenges in constructing reliable questions and answers (QAs) according to user messages. In this paper, we propose MemSim, a Bayesian simulator designed to automatically construct reliable QAs from generated user messages, simultaneously keeping their diversity and scalability. Specifically, we introduce the Bayesian Relation Network (BRNet) and a causal generation mechanism to mitigate the impact of LLM hallucinations on factual information, facilitating the automatic creation of an evaluation dataset. Based on MemSim, we generate a dataset in the daily-life scenario, named MemDaily, and conduct extensive experiments to assess the effectiveness of our approach. We also provide a benchmark for evaluating different memory mechanisms in LLM-based agents with the MemDaily dataset.",
      "arxiv_url": "https://openreview.net/forum?id=vAT2xlaWJY",
      "pdf_url": "https://openreview.net/pdf/96e1b7b1eeb53a530580aff14cf9527fe3e3d1ac.pdf",
      "primary_category": "LLM-based Agents, Memory, Personal Assistants",
      "categories": [
        "LLM-based Agents",
        "Memory",
        "Personal Assistants"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "l6C6Pw30Gl",
      "title": "Mixture of Inputs: Text Generation Beyond Discrete Token Sampling",
      "authors": [
        "Yufan Zhuang",
        "Liyuan Liu",
        "Chandan Singh",
        "Jingbo Shang",
        "Jianfeng Gao"
      ],
      "abstract": "In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution’s rich information, we propose Mixture of Inputs (MoI), a training-free method for autoregressive generation. After generating a token following the standard paradigm, we construct a new input that blends the generated discrete token with the previously discarded token distribution. Specifically, we employ a Bayesian estimation method that treats the token distribution as the prior, the sampled token as the observation, and replaces the conventional one-hot vector with the continuous posterior expectation as the new model input. MoI allows the model to maintain a richer internal representation throughout the generation process, resulting in improved text quality and reasoning capabilities. On mathematical reasoning, code generation, and PhD-level QA tasks, MoI consistently improves performance across multiple models including QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional training and negligible computational overhead.",
      "arxiv_url": "https://openreview.net/forum?id=l6C6Pw30Gl",
      "pdf_url": "https://openreview.net/pdf/ffbd9e60685bc4a0005eada53244f25de6aea675.pdf",
      "primary_category": "language model, continuous representation mixture",
      "categories": [
        "language model",
        "continuous representation mixture"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "JrZY7ilKLs",
      "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
      "authors": [
        "Zeju Qiu",
        "Simon Buchholz",
        "Tim Z. Xiao",
        "Maximilian Dax",
        "Bernhard Schölkopf",
        "Weiyang Liu"
      ],
      "abstract": "While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=JrZY7ilKLs",
      "pdf_url": "https://openreview.net/pdf/c0636d840d7b5e6a3b0ad84dec8a95d0ea5ceda1.pdf",
      "primary_category": "large language models, pretraining, orthogonality",
      "categories": [
        "large language models",
        "pretraining",
        "orthogonality"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2bgwni6Ber",
      "title": "Conformal Inference under High-Dimensional Covariate Shifts via Likelihood-Ratio Regularization",
      "authors": [
        "Sunay Joshi",
        "Shayan Kiyani",
        "George J. Pappas",
        "Edgar Dobriban",
        "Hamed Hassani"
      ],
      "abstract": "We consider the problem of conformal prediction under covariate shift. Given labeled data from a source domain and unlabeled data from a covariate shifted target domain, we seek to construct prediction sets with valid marginal coverage in the target domain. Most existing methods require estimating the unknown likelihood ratio function, which can be prohibitive for high-dimensional data such as images. To address this challenge, we introduce the likelihood ratio regularized quantile regression (LR-QR) algorithm, which combines the pinball loss with a novel choice of regularization in order to construct a threshold function without directly estimating the unknown likelihood ratio. We show that the LR-QR method has coverage at the desired level in the target domain, up to a small error term that we can control. Our proofs draw on a novel analysis of coverage via stability bounds from learning theory. Our experiments demonstrate that the LR-QR algorithm outperforms existing methods on high-dimensional prediction tasks, including a regression task for the Communities and Crime dataset, an image classification task from the WILDS repository, and an LLM question-answering task on the MMLU benchmark.",
      "arxiv_url": "https://openreview.net/forum?id=2bgwni6Ber",
      "pdf_url": "https://openreview.net/pdf/eeec7318dec02606fd2c620652df3fcdb098c7bc.pdf",
      "primary_category": "conformal prediction, covariate shift, uncertainty quantification",
      "categories": [
        "conformal prediction",
        "covariate shift",
        "uncertainty quantification",
        "distribution shift",
        "distribution-free prediction",
        "prediction sets",
        "high-dimensional data"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6Mah2bx7ZI",
      "title": "Logic.py: Bridging the Gap between LLMs and Constraint Solvers",
      "authors": [
        "Pascal Kesseli",
        "Peter O'Hearn",
        "Ricardo Silveira Cabral"
      ],
      "abstract": "We present a novel approach to formalise and solve search-based problems using large language models, which significantly improves upon previous state-of-the-art results. We demonstrate the efficacy of this approach on benchmarks like the logic puzzles tasks in ZebraLogicBench. Instead of letting the LLM attempt to directly solve the puzzles, our method prompts the model to formalise the problem in a logic-focused, human-readable domain-specific language (DSL) called Logic.py. This formalised representation is then solved using a constraint solver, leveraging the strengths of both the language model and the solver. Our approach achieves a remarkable 65% absolute improvement over the baseline performance of Llama 3.1 70B on ZebraLogicBench, setting a new state-of-the-art with an accuracy of over 90%. This significant advancement demonstrates the potential of combining language models with domain-specific languages and auxiliary tools on traditionally challenging tasks for LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=6Mah2bx7ZI",
      "pdf_url": "https://openreview.net/pdf/a811f4bc76e7ad2fcf67bc0ce62afd3123512b8d.pdf",
      "primary_category": "formal reasoning, machine learning, large language models",
      "categories": [
        "formal reasoning",
        "machine learning",
        "large language models",
        "constraint solvers"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "gOwqPdBlRB",
      "title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection",
      "authors": [
        "Mengjia Niu",
        "Hamed Haddadi",
        "Guansong Pang"
      ],
      "abstract": "Hallucinations in large language models (LLMs) pose significant safety concerns that impede their broader deployment. Recent research in hallucination detection has demonstrated that LLMs' internal representations contain truthfulness hints, which can be harnessed for detector training. However, the performance of these detectors is heavily dependent on the internal representations of predetermined tokens, fluctuating considerably when working on free-form generations with varying lengths and sparse distributions of hallucinated entities. To address this, we propose HaMI, a novel approach that enables robust detection of hallucinations through adaptive selection and learning of critical tokens that are most indicative of hallucinations. We achieve this robustness by an innovative formulation of the Hallucination detection task as Multiple Instance (HaMI) learning over token-level representations within a sequence, thereby facilitating a joint optimisation of token selection and hallucination detection on generation sequences of diverse forms. Comprehensive experimental results on four hallucination benchmarks show that HaMI significantly outperforms existing state-of-the-art approaches.",
      "arxiv_url": "https://openreview.net/forum?id=gOwqPdBlRB",
      "pdf_url": "https://openreview.net/pdf/6e36e6ecbe04ac41b08bdb225a947bda56a47410.pdf",
      "primary_category": "Hallucination Detection; Large Language Models",
      "categories": [
        "Hallucination Detection; Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "D0YNbanYfB",
      "title": "Video Perception Models for 3D Scene Synthesis",
      "authors": [
        "Rui Huang",
        "Guangyao Zhai",
        "Zuria Bauer",
        "Marc Pollefeys",
        "Federico Tombari",
        "Leonidas Guibas",
        "Gao Huang",
        "Francis Engelmann"
      ],
      "abstract": "Automating the expert-dependent and labor-intensive task of 3D scene synthesis would significantly benefit fields such as architectural design, robotics simulation, and virtual reality. Recent approaches to 3D scene synthesis often rely on the commonsense reasoning of large language models (LLMs) or strong visual priors from image generation models. However, current LLMs exhibit limited 3D spatial reasoning, undermining the realism and global coherence of synthesized scenes, while image-generation-based methods often constrain viewpoint control and introduce multi-view inconsistencies. In this work, we present Video Perception models for 3D Scene synthesis (VIPScene), a novel framework that exploits the encoded commonsense knowledge of the 3D physical world in video generation models to ensure coherent scene layouts and consistent object placements across views. VIPScene accepts both text and image prompts and seamlessly integrates video generation, feedforward 3D reconstruction, and open-vocabulary perception models to semantically and geometrically analyze each object in a scene. This enables flexible scene synthesis with high realism and structural consistency. For a more sufficient evaluation on coherence and plausibility, we further introduce First-Person View Score (FPVScore), utilizing a continuous first-person perspective to capitalize on the reasoning ability of multimodal large language models. Extensive experiments show that VIPScene significantly outperforms existing methods and generalizes well across diverse scenarios.",
      "arxiv_url": "https://openreview.net/forum?id=D0YNbanYfB",
      "pdf_url": "https://openreview.net/pdf/7e857c73d9055476e814af9a166b9358d74e3bc2.pdf",
      "primary_category": "3D scene synthesis, Video generation model",
      "categories": [
        "3D scene synthesis",
        "Video generation model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "98TY64tOFB",
      "title": "CoT-lized Diffusion: Let's Reinforce T2I Generation Step-by-step",
      "authors": [
        "Zheyuan Liu",
        "Munan Ning",
        "Qihui Zhang",
        "Shuo Yang",
        "Zhongrui Wang",
        "Yiwei Yang",
        "Xianzhe Xu",
        "Yibing Song",
        "Weihua Chen",
        "Fan Wang",
        "Li Yuan"
      ],
      "abstract": "Current text-to-image (T2I) generation models struggle to align spatial composition with the input text, especially in complex scenes. \nEven layout-based approaches yield suboptimal spatial control, as their generation process is decoupled from layout planning, making it difficult to refine the layout during synthesis.\nWe present CoT-Diff, a framework that brings step-by-step CoT-style reasoning into T2I generation by tightly integrating Multimodal Large Language Model (MLLM)-driven 3D layout planning with the diffusion process.\nCoT-Diff enables layout-aware reasoning inline within a single diffusion round: at each denoising step, the MLLM evaluates intermediate predictions, dynamically updates the 3D scene layout, and continuously guides the generation process. \nThe updated layout is converted into semantic conditions and depth maps, which are fused into the diffusion model via a condition-aware attention mechanism, enabling precise spatial control and semantic injection. \nExperiments on 3D Scene benchmarks show that CoT-Diff significantly improves spatial alignment and compositional fidelity, and outperforms the state-of-the-art method by 34.7% in complex scene spatial accuracy, thereby validating the effectiveness of this entangled generation paradigm.",
      "arxiv_url": "https://openreview.net/forum?id=98TY64tOFB",
      "pdf_url": "https://openreview.net/pdf/fb2f2d2c004da677806ae3c4926e918df30f0f41.pdf",
      "primary_category": "MLLM, Diffusion, CoT",
      "categories": [
        "MLLM",
        "Diffusion",
        "CoT",
        "3D Layout",
        "Image Generation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "S43003uMGq",
      "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning",
      "authors": [
        "Yibin Wang",
        "Zhimin Li",
        "Yuhang Zang",
        "Chunyu Wang",
        "Qinglin Lu",
        "Cheng Jin",
        "Jiaqi Wang"
      ],
      "abstract": "Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the model's cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments confirm that incorporating long CoT reasoning significantly enhances the accuracy of reward signals. Notably, after mastering CoT reasoning, the model exhibits implicit reasoning capabilities, allowing it to surpass existing baselines even without explicit reasoning traces.",
      "arxiv_url": "https://openreview.net/forum?id=S43003uMGq",
      "pdf_url": "https://openreview.net/pdf/e4dafb5f56d85a3ea3c98693dd6bf3a57ca6f9c7.pdf",
      "primary_category": "reward model, reinforcement leaning, chain-of-though reasoning",
      "categories": [
        "reward model",
        "reinforcement leaning",
        "chain-of-though reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "TEAASoJWsb",
      "title": "Generalized Contrastive Learning for Universal Multimodal Retrieval",
      "authors": [
        "Jungsoo Lee",
        "Janghoon Cho",
        "Hyojin Park",
        "Munawar Hayat",
        "Kyuwoong Hwang",
        "Fatih Porikli",
        "Sungha Choi"
      ],
      "abstract": "Despite their consistent performance improvements, cross-modal retrieval models (e.g., CLIP) show degraded performances with retrieving keys composed of fused image-text modality (e.g., Wikipedia pages with both images and text). To address this critical challenge, multimodal retrieval has been recently explored to develop a unified single retrieval model capable of retrieving keys across diverse modality combinations. A common approach involves constructing new composed sets of image-text triplets (e.g., retrieving a pair of image and text given a query image). However, such an approach requires careful curation to ensure the dataset quality and fails to generalize to unseen modality combinations. To overcome these limitations, this paper proposes Generalized Contrastive Learning (GCL), a novel loss formulation that improves multimodal retrieval performance without the burdensome need for new dataset curation. Specifically, GCL operates by enforcing contrastive learning across all modalities within a mini-batch, utilizing existing image-caption paired datasets to learn a unified representation space. We demonstrate the effectiveness of GCL by showing consistent performance improvements on off-the-shelf multimodal retrieval models (e.g., VISTA, CLIP, and TinyCLIP) using the M-BEIR, MMEB, and CoVR benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=TEAASoJWsb",
      "pdf_url": "https://openreview.net/pdf/2dd41a00bad5786a3ffe8c7df224f4ecc02a338b.pdf",
      "primary_category": "multimodal Retrieval, unified representation space, generalized contrastive learning",
      "categories": [
        "multimodal Retrieval",
        "unified representation space",
        "generalized contrastive learning"
      ],
      "tags": [
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Sct4sajCi6",
      "title": "SATURN: SAT-based Reinforcement Learning to Unleash LLMs Reasoning",
      "authors": [
        "Huanyu Liu",
        "Ge Li",
        "Jia Li",
        "Hao Zhu",
        "Kechi Zhang",
        "Yihong Dong"
      ],
      "abstract": "How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard.\n\nTo address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions.\n\nWe introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results:\n(1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively.\n(2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench).\n(3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8\\%.\nWe release the source code, data, and models to support future research.",
      "arxiv_url": "https://openreview.net/forum?id=Sct4sajCi6",
      "pdf_url": "https://openreview.net/pdf/23436edf28fa671c741cc536af8d024f770c6f34.pdf",
      "primary_category": "Large Language Models, LLMs Reasoning, Reinforcement Learning",
      "categories": [
        "Large Language Models",
        "LLMs Reasoning",
        "Reinforcement Learning",
        "Curriculum Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MZoOpD9NHV",
      "title": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation",
      "authors": [
        "Kai Liu",
        "Jungang Li",
        "Yuchong Sun",
        "Shengqiong Wu",
        "jianzhang gao",
        "Daoan Zhang",
        "Wei Zhang",
        "Sheng Jin",
        "Sicheng Yu",
        "Geng Zhan",
        "Jiayi Ji",
        "Fan Zhou",
        "Liang Zheng",
        "Shuicheng YAN",
        "Hao Fei",
        "Tat-Seng Chua"
      ],
      "abstract": "This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for joint audio-video (JAV) comprehension and generation. JavisGPT has a concise encoder-LLM-decoder architecture, which has a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. For instruction tuning, we construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that cover diverse and multi-level comprehension and generation scenarios. On JAV comprehension and generation benchmarks, our experiments show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.",
      "arxiv_url": "https://openreview.net/forum?id=MZoOpD9NHV",
      "pdf_url": "https://openreview.net/pdf/79ebce628299b86607dc0cefd443cdebd61a6859.pdf",
      "primary_category": "Multi-Modality, Large Language Model, Souding-Video Comprehension and Generation",
      "categories": [
        "Multi-Modality",
        "Large Language Model",
        "Souding-Video Comprehension and Generation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0bvc7Zslu3",
      "title": "Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images via Reinforced Multimodal Language Models",
      "authors": [
        "Jun Ling",
        "Yao Qi",
        "Tao Huang",
        "Shibo Zhou",
        "Yanqin Huang",
        "Yang Jiang",
        "Ziqi Song",
        "Ying Zhou",
        "Yang Yang",
        "Heng Tao Shen",
        "Peng Wang"
      ],
      "abstract": "In this work, we address the task of table image to LaTeX code generation, with the goal of automating the reconstruction of high-quality, publication-ready tables from visual inputs. A central challenge of this task lies in accurately handling complex tables—those with large sizes, deeply nested structures, and semantically rich or irregular cell content—where existing methods often fail. We begin with a comprehensive analysis, identifying key challenges and highlighting the limitations of current evaluation protocols. To overcome these issues, we propose a reinforced multimodal large language model (MLLM) framework, where a pre-trained MLLM is fine-tuned on a large-scale table-to-LaTeX dataset. To further improve generation quality, we introduce a dual-reward reinforcement learning strategy based on Group Relative Policy Optimization (GRPO). Unlike standard approaches that optimize purely over text outputs, our method incorporates both a structure-level reward on LaTeX code and a visual fidelity reward computed from rendered outputs, enabling direct optimization of the visual output quality.\nWe adopt a hybrid evaluation protocol combining TEDS-Structure and CW-SSIM, and show that our method achieves state-of-the-art performance, particularly on structurally complex tables, demonstrating the effectiveness and robustness of our approach.",
      "arxiv_url": "https://openreview.net/forum?id=0bvc7Zslu3",
      "pdf_url": "https://openreview.net/pdf/3de7c889c76ab1a0b26cc94bd259f196ea3547e7.pdf",
      "primary_category": "table recognition, latex generation",
      "categories": [
        "table recognition",
        "latex generation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vhPy3NMsO5",
      "title": "OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions",
      "authors": [
        "Cheng Luo",
        "Jianghui Wang",
        "Bing Li",
        "Siyang Song",
        "Bernard Ghanem"
      ],
      "abstract": "In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task designed to produce synchronized verbal and non-verbal listener feedback online, based on the speaker's multimodal inputs. OMCRG captures natural dyadic interactions and introduces new challenges in aligning generated audio with listeners' facial responses. To tackle these challenges, we incorporate text as an intermediate modality to connect audio and facial responses. We propose OmniResponse, a Multimodal Large Language Model (MLLM) that autoregressively generates accurate multimodal listener responses. OmniResponse leverages a pretrained LLM enhanced with two core components: Chrono-Text Markup, which precisely timestamps generated text tokens, and TempoVoice, a controllable online text-to-speech (TTS) module that outputs speech synchronized with facial responses. To advance OMCRG research, we offer ResponseNet, a dataset of 696 detailed dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and annotated facial behaviors. Comprehensive evaluations on ResponseNet demonstrate that OmniResponse outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality. Our dataset, code, and models are publicly available at https://omniresponse.github.io/.",
      "arxiv_url": "https://openreview.net/forum?id=vhPy3NMsO5",
      "pdf_url": "https://openreview.net/pdf/49dad38a8c769542a3574b315832d78eb5557230.pdf",
      "primary_category": "Human-computer Interaction, Human Behavior Understanding, Multi-model Generation",
      "categories": [
        "Human-computer Interaction",
        "Human Behavior Understanding",
        "Multi-model Generation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kXieirlPjF",
      "title": "Agentic RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving",
      "authors": [
        "Xinji Mai",
        "Haotian Xu",
        "Xing W",
        "Weinong Wang",
        "Yingying Zhang",
        "Wenqiang Zhang"
      ],
      "abstract": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards  enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\\_async\\_pipline}.",
      "arxiv_url": "https://openreview.net/forum?id=kXieirlPjF",
      "pdf_url": "https://openreview.net/pdf/6d8a6cfbc785966d9381495a786eea81de48d681.pdf",
      "primary_category": "Agentic RL, Code Execution, LLMs",
      "categories": [
        "Agentic RL",
        "Code Execution",
        "LLMs",
        "Math"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "dbaYQyruY2",
      "title": "Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis",
      "authors": [
        "Leitian Tao",
        "Xuefeng Du",
        "Sharon Li"
      ],
      "abstract": "Reward modeling, crucial for aligning large language models (LLMs) with human preferences, is often bottlenecked by the high cost of preference data. Existing textual data synthesis methods are computationally expensive. We propose a novel framework LENS for synthesizing preference data directly in the LLM's latent embedding space. Our method employs a Variational Autoencoder (VAE) to learn a structured latent representation of response embeddings. By performing controlled perturbations in this latent space and decoding back to the embedding space, we efficiently generate diverse, semantically consistent synthetic preference pairs, bypassing costly text generation and annotation. We provide theoretical guarantees that our synthesized pairs approximately preserve original preference ordering and improve reward model generalization. Empirically, our latent-space synthesis significantly outperforms text-based augmentation on standard benchmarks, achieving superior results while being 18× faster in generation and using a 16,000× smaller model. Our work offers a scalable and effective alternative for enhancing reward modeling through efficient data augmentation. Code is publicly available at https://github.com/deeplearning-wisc/lens.",
      "arxiv_url": "https://openreview.net/forum?id=dbaYQyruY2",
      "pdf_url": "https://openreview.net/pdf/12e779439c922ba09d37907c04e017559b3a804c.pdf",
      "primary_category": "Large language model;reward modeling; latent space synthesis",
      "categories": [
        "Large language model;reward modeling; latent space synthesis"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BcYfsMMpV1",
      "title": "Imagine360: Immersive 360 Video Generation from Perspective Anchor",
      "authors": [
        "Jing Tan",
        "Shuai Yang",
        "Tong Wu",
        "Jingwen He",
        "Yuwei Guo",
        "Ziwei Liu",
        "Dahua Lin"
      ],
      "abstract": "$360^\\circ$ videos offer a hyper-immersive experience that allows the viewers to explore a dynamic scene from full 360 degrees. \nTo achieve more accessible and personalized content creation in $360^\\circ$ video format, we seek to lift standard perspective videos into $360^\\circ$ equirectangular videos. To this end, we introduce **Imagine360**, the first perspective-to-$360^\\circ$ video generation framework that creates high-quality $360^\\circ$ videos with rich and diverse motion patterns from video anchors.\nImagine360 learns fine-grained spherical visual and motion patterns from limited $360^\\circ$ video data with several key designs. \n**1)** Firstly we adopt the dual-branch design, including a perspective and a panorama video denoising branch to provide local and global constraints for $360^\\circ$ video generation, with motion module and spatial LoRA layers fine-tuned on $360^\\circ$ videos.\n**2)** Additionally, an antipodal mask is devised to capture long-range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres.\n**3)** To handle diverse perspective video inputs, we propose rotation-aware designs that adapt to varying video masking due to changing camera poses across frames.\n**4)** Lastly, we introduce a new 360 video dataset featuring 10K high-quality, trimmed 360 video clips with structured motion to facilitate training.\nExtensive experiments show Imagine360 achieves superior graphics quality and motion coherence with our curated dataset among state-of-the-art $360^\\circ$ video generation methods. We believe Imagine360 holds promise for advancing personalized, immersive $360^\\circ$ video creation.",
      "arxiv_url": "https://openreview.net/forum?id=BcYfsMMpV1",
      "pdf_url": "https://openreview.net/pdf/d100408170e96731275fdddbced5270eff7bd0e9.pdf",
      "primary_category": "video generation, video outpainting, panorama video generation",
      "categories": [
        "video generation",
        "video outpainting",
        "panorama video generation",
        "360 video generation"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xlDmm4r98R",
      "title": "FACT: Mitigating Inconsistent Hallucinations in LLMs via Fact-Driven Alternating Code-Text Training",
      "authors": [
        "Xinxin You",
        "Qixin Sun",
        "Chenwei Yan",
        "Xiao Zhang",
        "Chen Ning",
        "Xiangling Fu",
        "Si Liu",
        "Guoping Hu",
        "Shijin Wang",
        "Ji Wu",
        "Xien Liu"
      ],
      "abstract": "Inconsistent hallucinations remain a major challenge for large language models (LLMs), undermining the accuracy and reliability of fact-based reasoning in real-world applications. Existing approaches often rely on task-specific training or adaptation, such as hand-crafted synthetic datasets for domain tasks or solutions mainly focused on numerical reasoning, thereby limiting generalizability to broader, unseen NLP tasks. Inspired by the structural rigor and logical consistency of programming languages, we observe that fact-based texts can be mapped to programming structures due to their inherent patterns. We further propose FACT, a novel Fact-driven Alternating Code-text Training framework that alternates between text-to-code and code-to-text prediction. FACT is the first task-agnostic paradigm that embeds code and natural language in a shared semantic space, thereby transferring the logical consistency of code to LLM outputs in NLP tasks. Experiments show that with only a small subset of Wiki-40B-en for training, FACT reduces inconsistent hallucinations by 2.7%–8.0% and improves overall performance by 2.5%–6.1% in three leading LLMs and four diverse datasets covering QA and summarization tasks. This framework offers a new perspective on addressing challenging hallucinations in LLMs, contributing to more reliable AI.",
      "arxiv_url": "https://openreview.net/forum?id=xlDmm4r98R",
      "pdf_url": "https://openreview.net/pdf/8f06a63c8a49d0e18846db034cd1bfcf042c535e.pdf",
      "primary_category": "Inconsistent Hallucination, Fact-Driven Code-Text, Alternating Training",
      "categories": [
        "Inconsistent Hallucination",
        "Fact-Driven Code-Text",
        "Alternating Training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "BBZEcVu1nA",
      "title": "Personalized Image Editing in Text-to-Image Diffusion Models via Collaborative Direct Preference Optimization",
      "authors": [
        "Connor Dunlop",
        "Matthew Zheng",
        "Kavana Venkatesh",
        "Pinar Yanardag"
      ],
      "abstract": "Text-to-image (T2I) diffusion models have made remarkable strides in generating and editing high-fidelity images from text. Yet, these models remain fundamentally generic, failing to adapt to the nuanced aesthetic preferences of individual users. In this work, we present the first framework for personalized image editing in diffusion models, introducing Collaborative Direct Preference Optimization (C-DPO), a novel  method that aligns image edits with user-specific preferences while leveraging collaborative signals from like-minded individuals. Our approach encodes each user as a node in a dynamic preference graph and learns  embeddings via a lightweight graph neural network, enabling information sharing across users with overlapping visual tastes. We enhance a diffusion model's editing capabilities by integrating these personalized embeddings into a novel DPO objective, which jointly optimizes for individual alignment and neighborhood coherence.  Comprehensive experiments, including user studies and quantitative benchmarks, demonstrate that our method consistently outperforms baselines in generating edits that are aligned with user preferences.",
      "arxiv_url": "https://openreview.net/forum?id=BBZEcVu1nA",
      "pdf_url": "https://openreview.net/pdf/489f2813843a9788972f8132fb48ad706209bef6.pdf",
      "primary_category": "image editing, t2i, personalization dpo",
      "categories": [
        "image editing",
        "t2i",
        "personalization dpo"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "uWEcZkrSkZ",
      "title": "MuSLR: Multimodal Symbolic Logical Reasoning",
      "authors": [
        "Jundong Xu",
        "Hao Fei",
        "Yuhui Zhang",
        "Liangming Pan",
        "Qijun Huang",
        "Qian Liu",
        "Preslav Nakov",
        "Min-Yen Kan",
        "William Yang Wang",
        "Mong-Li Lee",
        "Wynne Hsu"
      ],
      "abstract": "Multimodal symbolic logical reasoning, which aims to deduce new facts from multimodal input via formal logic, is critical in high-stakes applications such as autonomous driving and medical diagnosis, as its rigorous, deterministic reasoning helps prevent serious consequences. To evaluate such capabilities of current state-of-the-art vision language models (VLMs), we introduce the first benchmark MuSLR for multimodal symbolic logical reasoning grounded in formal logical rules. MuSLR comprises 1,093 instances across 7 domains, including 35 atomic symbolic logic and 976 logical combinations, with reasoning depths ranging from 2 to 9. We evaluate 7 state-of-the-art VLMs on MuSLR and find that they all struggle with multimodal symbolic reasoning, with the best model, GPT-4.1, achieving only 46.8%.\nThus, we propose LogiCAM, a modular framework that applies formal logical rules to multimodal inputs, boosting GPT-4.1’s Chain-of-Thought performance by 14.13%, and delivering even larger gains on complex logics such as first-order logic. We also conduct a comprehensive error analysis, showing that around 70% of failures stem from logical misalignment between modalities, offering key insights to guide future improvements.",
      "arxiv_url": "https://openreview.net/forum?id=uWEcZkrSkZ",
      "pdf_url": "https://openreview.net/pdf/c3fc5a8b459eec88aa0b5113f6e1f32a2d2743e4.pdf",
      "primary_category": "Symbolic Reasoning, Multimodal Learning",
      "categories": [
        "Symbolic Reasoning",
        "Multimodal Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "t49olghJ3w",
      "title": "From Self-Check to Consensus: Bayesian Strategic Decoding in Large Language Models",
      "authors": [
        "Weitong Zhang",
        "Chengqi Zang",
        "Bernhard Kainz"
      ],
      "abstract": "Large Language Models exhibit logical inconsistency across multi-turn inference processes, undermining correctness in complex inferential tasks. Challenges arise from ensuring that outputs align with both factual correctness and human intent. Approaches like single-agent reflection and multi-agent debate frequently prioritize consistency, but at the expense of accuracy. \nTo address this problem, we propose a novel game-theoretic consensus mechanism that enables LLMs to self-check their outputs during the decoding stage of output generation. Our method models the decoding process as a multistage Bayesian Decoding Game, where strategic interactions dynamically converge to a consensus on the most reliable outputs without human feedback or additional training. Remarkably, our game design allows smaller models to outperform much larger models through game mechanisms (e.g., 78.1 LLaMA13B vs. 76.6 PaLM540B). As a model-agnostic method, our approach consistently improves even the latest models, enhancing DeepSeek-7B's performance on MMLU by 12.4%. Our framework effectively balances correctness and consistency, demonstrating that properly designed game-theoretic mechanisms can significantly enhance the self-verification capabilities of language models across various tasks and model architectures.",
      "arxiv_url": "https://openreview.net/forum?id=t49olghJ3w",
      "pdf_url": "https://openreview.net/pdf/1b41e05b218d478917e719a479330a4b6c7bee60.pdf",
      "primary_category": "Multi-agent System, Game Theory, Mechanism Design",
      "categories": [
        "Multi-agent System",
        "Game Theory",
        "Mechanism Design"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "m0q8NfGWnv",
      "title": "MF-LLM: Simulating Population Decision Dynamics via a Mean-Field Large Language Model Framework",
      "authors": [
        "Qirui Mi",
        "Mengyue Yang",
        "Xiangning Yu",
        "Zhiyu Zhao",
        "Cheng Deng",
        "Bo An",
        "Haifeng Zhang",
        "Xu Chen",
        "Jun Wang"
      ],
      "abstract": "Simulating collective decision-making involves more than aggregating individual behaviors; it emerges from dynamic interactions among individuals. While large language models (LLMs) offer strong potential for social simulation, achieving quantitative alignment with real-world data remains a key challenge. To bridge this gap, we propose the \\textbf{M}ean-\\textbf{F}ield \\textbf{LLM} (\\textbf{MF-LLM}) framework, the first to incorporate mean field theory into LLM-based social simulation. MF-LLM models bidirectional interactions between individuals and the population through an iterative process, generating population signals to guide individual decisions, which in turn update the signals. This interplay produces coherent trajectories of collective behavior.\nTo improve alignment with real-world data, we introduce \\textbf{IB-Tune}, a novel fine-tuning method inspired by the \\textbf{I}nformation \\textbf{B}ottleneck principle, which retains population signals most predictive of future actions while filtering redundant history. Evaluated on a real-world social dataset, MF-LLM reduces KL divergence to human population distributions by \\textbf{47\\%} compared to non-mean-field baselines, enabling accurate trend forecasting and effective intervention planning. \nGeneralizing across 7 domains and 4 LLM backbones, MF-LLM provides a scalable, high-fidelity foundation for social simulation.",
      "arxiv_url": "https://openreview.net/forum?id=m0q8NfGWnv",
      "pdf_url": "https://openreview.net/pdf/3760dc8737ffc828c6f397542f2e78dcbcc8f772.pdf",
      "primary_category": "social simulation, large langauge model, population",
      "categories": [
        "social simulation",
        "large langauge model",
        "population",
        "decision making",
        "mean-field"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VlvtStQN34",
      "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers",
      "authors": [
        "Yusuf Dalva",
        "Hidir Yesiltepe",
        "Pinar Yanardag"
      ],
      "abstract": "We introduce LoRAShop, the first framework for multi-concept image generation and editing with LoRA models. LoRAShop builds on a key observation about the feature interaction patterns inside Flux-style diffusion transformers: concept-specific transformer features activate spatially coherent regions early in the denoising process. We harness this observation to derive a disentangled latent mask for each concept in a prior forward pass and blend the corresponding LoRA weights only within regions bounding the concepts to be personalized. The resulting edits seamlessly integrate multiple subjects or styles into the original scene while preserving global context, lighting, and fine details. Our experiments  demonstrate that LoRAShop delivers better identity preservation compared to baselines. By eliminating retraining and external constraints, LoRAShop turns personalized diffusion models into a practical `photoshop-with-LoRAs' tool and opens new avenues for compositional visual storytelling and rapid creative iteration.",
      "arxiv_url": "https://openreview.net/forum?id=VlvtStQN34",
      "pdf_url": "https://openreview.net/pdf/d6eca930d50f8583191fccabd87bbe6fb67a0c01.pdf",
      "primary_category": "lora, image editing, t2i",
      "categories": [
        "lora",
        "image editing",
        "t2i",
        "flux"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cU4ow1odRe",
      "title": "ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback",
      "authors": [
        "Litao Guo",
        "Xinli Xu",
        "Luozhou Wang",
        "Jiantao Lin",
        "Jinsong Zhou",
        "Zixin Zhang",
        "Bolan Su",
        "Ying-Cong Chen"
      ],
      "abstract": "With the rapid advancement of generative models, general-purpose generation has gained increasing attention as a promising approach to unify diverse tasks across modalities within a single system. Despite this progress, existing open-source frameworks often remain fragile and struggle to support complex real-world applications due to the lack of structured workflow planning and execution-level feedback. To address these limitations, we present ComfyMind, a collaborative AI system designed to enable robust and scalable general-purpose generation, built on the ComfyUI platform. ComfyMind introduces two core innovations: Semantic Workflow Interface (SWI) that abstracts low-level node graphs into callable functional modules described in natural language, enabling high-level composition and reducing structural errors; Search Tree Planning mechanism with localized feedback execution, which models generation as a hierarchical decision process and allows adaptive correction at each stage. Together, these components improve the stability and flexibility of complex generative workflows. We evaluate ComfyMind on three public benchmarks: ComfyBench, GenEval, and Reason-Edit, which span generation, editing, and reasoning tasks. Results show that ComfyMind consistently outperforms existing open-source baselines and achieves performance comparable to GPT-Image-1. ComfyMind paves a promising path for the development of open-source general-purpose generative AI systems.",
      "arxiv_url": "https://openreview.net/forum?id=cU4ow1odRe",
      "pdf_url": "https://openreview.net/pdf/d05ca720fd263021237eba3c1d2a29145f7e4dae.pdf",
      "primary_category": "General-Purpose Generation, Collaborative AI system, ComfyUI",
      "categories": [
        "General-Purpose Generation",
        "Collaborative AI system",
        "ComfyUI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "MRvxlTlkNQ",
      "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning",
      "authors": [
        "Jiaru Zou",
        "Yikun Ban",
        "Zihao Li",
        "Yunzhe Qi",
        "Ruizhong Qiu",
        "Ling Yang",
        "Jingrui He"
      ],
      "abstract": "Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model’s own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model’s learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot’s inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot’s logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability. Our code is released at https://github.com/jiaruzouu/TransformerCopilot.",
      "arxiv_url": "https://openreview.net/forum?id=MRvxlTlkNQ",
      "pdf_url": "https://openreview.net/pdf/6febc0a676342ca1cd887cb43d8580f5d3106244.pdf",
      "primary_category": "Supervised Fine-tuning, Auxiliary Learning Framework, LLMs",
      "categories": [
        "Supervised Fine-tuning",
        "Auxiliary Learning Framework",
        "LLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "JZuDhckogK",
      "title": "OmniConsistency: Learning Style-Agnostic Consistency from Paired Stylization Data",
      "authors": [
        "Yiren Song",
        "Cheng Liu",
        "Mike Zheng Shou"
      ],
      "abstract": "Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) maintaining consistent stylization in complex scenes, particularly identity, composition, and fine details, and (2) preventing style degradation in image-to-image pipelines with style LoRAs. GPT-4o's exceptional stylization consistency highlights the performance gap between open-source methods and proprietary models. To bridge this gap, we propose \\textbf{OmniConsistency}, a universal consistency plugin leveraging large-scale Diffusion Transformers (DiTs). OmniConsistency contributes: (1) an in-context consistency learning framework trained on aligned image pairs for robust generalization; (2) a two-stage progressive learning strategy decoupling style learning from consistency preservation to mitigate style degradation; and (3) a fully plug-and-play design compatible with arbitrary style LoRAs under the Flux framework. Extensive experiments show that OmniConsistency significantly enhances visual coherence and aesthetic quality, achieving performance comparable to commercial state-of-the-art model GPT-4o.",
      "arxiv_url": "https://openreview.net/forum?id=JZuDhckogK",
      "pdf_url": "https://openreview.net/pdf/baacd716ccf947bcd4e5561ad487b6d24e4daa70.pdf",
      "primary_category": "Diffusion model, Conditional generation, Image generation",
      "categories": [
        "Diffusion model",
        "Conditional generation",
        "Image generation",
        "Style transfer"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "DOb47fj0cl",
      "title": "RelationAdapter: Learning and Transferring Visual Relation with Diffusion Transformers",
      "authors": [
        "Yan Gong",
        "Yiren Song",
        "Yicheng Li",
        "Chenglin Li",
        "Yin Zhang"
      ],
      "abstract": "Inspired by the in-context learning mechanism of large language models (LLMs), a new paradigm of generalizable visual prompt-based image editing is emerging. Existing single-reference methods typically focus on style or appearance adjustments and struggle with non-rigid transformations. To address these limitations, we propose leveraging source-target image pairs to extract and transfer content-aware editing intent to novel query images. To this end, we introduce RelationAdapter, a lightweight module that enables Diffusion Transformer (DiT) based models to effectively capture and apply visual transformations from minimal examples. We also introduce Relation252K, a comprehensive dataset comprising 218 diverse editing tasks, to evaluate model generalization and adaptability in visual prompt-driven scenarios. Experiments on Relation252K show that RelationAdapter significantly improves the model’s ability to understand and transfer editing intent, leading to notable gains in generation quality and overall editing performance.",
      "arxiv_url": "https://openreview.net/forum?id=DOb47fj0cl",
      "pdf_url": "https://openreview.net/pdf/2c9877508b7505c4c6e730f2b3f8e054edbe33ea.pdf",
      "primary_category": "Diffusion model, In-context learning",
      "categories": [
        "Diffusion model",
        "In-context learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8pEqukyGrj",
      "title": "CryptoMoE: Privacy-Preserving and Scalable Mixture of Experts Inference via Balanced Expert Routing",
      "authors": [
        "Yifan Zhou",
        "Tianshi Xu",
        "Jue Hong",
        "Ye Wu",
        "Meng Li"
      ],
      "abstract": "Private large language model (LLM) inference based on cryptographic primitives offers a promising path towards privacy-preserving deep learning. However, existing frameworks only support dense LLMs like LLaMA-1 and struggle to scale to mixture-of-experts (MoE) architectures. The key challenge comes from securely evaluating the dynamic routing mechanism in MoE layers, which may reveal sensitive input information if not fully protected. In this paper, we propose CryptoMoE, the first framework that enables private, efficient, and accurate inference for MoE-based models. CryptoMoE balances expert loads to protect expert routing information and proposes novel protocols for secure expert dispatch and combine. CryptoMoE also develops a confidence-aware token selection strategy and a batch matrix multiplication protocol to improve accuracy and efficiency further. Extensive experiments on DeepSeekMoE-16.4B, OLMoE-6.9B, and QWenMoE-14.3B show that CryptoMoE achieves $2.8\\sim3.5\\times$ end-to-end latency reduction and $3\\sim6\\times$ communication reduction over a dense baseline with minimum accuracy loss. We also adapt CipherPrune (ICLR'25) for MoE inference and demonstrate CryptoMoE can reduce the communication by up to $4.3 \\times$.",
      "arxiv_url": "https://openreview.net/forum?id=8pEqukyGrj",
      "pdf_url": "https://openreview.net/pdf/7f12a8a706740b5d41611edbbe30056ef95d1203.pdf",
      "primary_category": "Privacy-Preserving Deep Learning, Homomorphic Encryption, Multi-party Computation",
      "categories": [
        "Privacy-Preserving Deep Learning",
        "Homomorphic Encryption",
        "Multi-party Computation",
        "Mixture of Experts",
        "Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "JnKfAqLJb4",
      "title": "ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient RAG",
      "authors": [
        "Yikuan Hu",
        "Jifeng Zhu",
        "Lanrui Tang",
        "Chen Huang"
      ],
      "abstract": "Knowledge graphs (KGs), with their structured representation capabilities, offer promising avenue for enhancing Retrieval Augmented Generation (RAG) systems, leading to the development of KG-RAG systems. Nevertheless, existing methods often struggle to achieve effective synergy between system effectiveness and cost efficiency, leading to neither unsatisfying performance nor excessive LLM prompt tokens and inference time. To this end, this paper proposes REMINDRAG, which employs an LLM-guided graph traversal featuring node exploration, node exploitation, and, most notably, memory replay, to improve both system effectiveness and cost efficiency. Specifically, REMINDRAG memorizes traversal experience within KG edge embeddings, mirroring the way LLMs \"memorize\" world knowledge within their parameters, but in a train-free manner. We theoretically and experimentally confirm the effectiveness of REMINDRAG, demonstrating its superiority over existing baselines across various benchmark datasets and LLM backbones. Our code is available at https://github.com/kilgrims/ReMindRAG.",
      "arxiv_url": "https://openreview.net/forum?id=JnKfAqLJb4",
      "pdf_url": "https://openreview.net/pdf/f95dad169fbcf455ac4908824c0b81d0e81b32ac.pdf",
      "primary_category": "Retrieval-Augmented Generation, Knowledge Graph, Large Language Models",
      "categories": [
        "Retrieval-Augmented Generation",
        "Knowledge Graph",
        "Large Language Models",
        "Low-Cost Inference"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "JwnAItQF9v",
      "title": "GRIFFIN: Effective Token Alignment for Faster Speculative Decoding",
      "authors": [
        "Shijing Hu",
        "Jingyang Li",
        "Xingyu Xie",
        "Zhihui Lu",
        "Kim-chuan Toh",
        "Pan Zhou"
      ],
      "abstract": "Speculative decoding accelerates inference in large language models (LLMs) by generating multiple draft tokens simultaneously. However, existing methods often struggle with token misalignment between the training and decoding phases, limiting their performance. To address this, we propose GRIFFIN, a novel framework that incorporates a token-alignable training strategy and a token-alignable draft model to mitigate misalignment.\nThe training strategy employs a loss masking mechanism to exclude highly misaligned tokens during training, preventing them from negatively impacting the draft model's optimization. The token-alignable draft model introduces input tokens to correct inconsistencies in generated features.\nExperiments on LLaMA, Vicuna, Qwen and Mixtral models demonstrate that GRIFFIN achieves an average acceptance length improvement of over 8\\% and a speedup ratio exceeding 7\\%, outperforming current speculative decoding state-of-the-art methods. Our code and GRIFFIN's draft models will be released publicly in https://github.com/hsj576/GRIFFIN.",
      "arxiv_url": "https://openreview.net/forum?id=JwnAItQF9v",
      "pdf_url": "https://openreview.net/pdf/2f7db559d934562ae3db43319f9b4b3c7a26c928.pdf",
      "primary_category": "Large Language Model; Speculative Decoding; LLM Inference Acceleration; Token Alignment",
      "categories": [
        "Large Language Model; Speculative Decoding; LLM Inference Acceleration; Token Alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nn51ewu5k2",
      "title": "Efficiently Scaling LLM Reasoning Programs with Certaindex",
      "authors": [
        "Yichao Fu",
        "Junda Chen",
        "Siqi Zhu",
        "Zheyu Fu",
        "Zhongdongming Dai",
        "Yonghao Zhuang",
        "Yian Ma",
        "Aurick Qiao",
        "Tajana Rosing",
        "Ion Stoica",
        "Hao Zhang"
      ],
      "abstract": "Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy. At the same time, we observe that these algorithms exhibit answer stabilization: their intermediate solutions often cease to change after a certain point, and further investment of compute does not change their final answer. To quantify this phenomenon, we introduce Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result. Certaindex is lightweight, can accelerate reasoning program inference via early exit, and further enables dynamic token allocation, gang scheduling, and many opportunities when integrated with real-world LLM serving systems. To quantify real-world benefits, we built Certaindex as a scheduler into Dynasor, our reasoning-aware LLM serving system, and demonstrate up to 50\\% compute savings and 3.3$\\times$ higher throughput in real workloads with no accuracy drop. Our code is available at https://github.com/hao-ai-lab/Dynasor.git",
      "arxiv_url": "https://openreview.net/forum?id=nn51ewu5k2",
      "pdf_url": "https://openreview.net/pdf/4ca50f6ea693aeda7b95d22f1929d6a5d49cf4ff.pdf",
      "primary_category": "Large Language Models, LLM Reasoning, LLM Efficiency",
      "categories": [
        "Large Language Models",
        "LLM Reasoning",
        "LLM Efficiency"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "iZC5xoQQkX",
      "title": "Causal LLM Routing: End-to-End Regret Minimization from Observational Data",
      "authors": [
        "Asterios Tsiourvas",
        "Wei Sun",
        "Georgia Perakis"
      ],
      "abstract": "LLM routing aims to select the most appropriate model for each query, balancing competing performance metrics such as accuracy and cost across a pool of language models. Prior approaches typically adopt a decoupled strategy, where the metrics are first predicted and the model is then selected based on these estimates. This setup is prone to compounding errors and often relies on full-feedback data, where each query is evaluated by all candidate models, which is costly to obtain and maintain in practice. In contrast, we learn from observational data, which records only the outcome of the model actually deployed. We propose a causal end-to-end framework that learns routing policies by minimizing decision-making regret from observational data. To enable efficient optimization, we introduce two theoretically grounded surrogate objectives: a classification-based upper bound, and a softmax-weighted regret approximation shown to recover the optimal policy at convergence. We further extend our framework to handle heterogeneous cost preferences via an interval-conditioned architecture. Experiments on public benchmarks show that our method outperforms existing baselines, achieving state-of-the-art performance across different embedding models.",
      "arxiv_url": "https://openreview.net/forum?id=iZC5xoQQkX",
      "pdf_url": "https://openreview.net/pdf/511a9b68d25f5e15c06a87acb5a057f90f002bf8.pdf",
      "primary_category": "LLMs, Routing, Causal ML",
      "categories": [
        "LLMs",
        "Routing",
        "Causal ML"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tRvzEL64dY",
      "title": "Interpreting Arithmetic Reasoning in Large Language Models using Game-Theoretic Interactions",
      "authors": [
        "Leilei Wen",
        "Liwei Zheng",
        "Hongda Li",
        "Lijun Sun",
        "Zhihua Wei",
        "Wen Shen"
      ],
      "abstract": "In recent years, large language models (LLMs) have made significant advancements in arithmetic reasoning. \nHowever, the internal mechanism of how LLMs solve arithmetic problems remains unclear.\nIn this paper, we propose explaining arithmetic reasoning in LLMs using game-theoretic interactions.\nSpecifically, we disentangle the output score of the LLM into numerous interactions between the input words.\nWe quantify different types of interactions encoded by LLMs during forward propagation to explore the internal mechanism of LLMs for solving arithmetic problems.\nWe find that (1) the internal mechanism of LLMs for solving simple one-operator arithmetic problems is their capability to encode operand-operator interactions and high-order interactions from input samples.\nAdditionally, we find that LLMs with weak one-operator arithmetic capabilities focus more on background interactions.\n(2) The internal mechanism of LLMs for solving relatively complex two-operator arithmetic problems is their capability to encode operator interactions and operand interactions from input samples.\n(3) We explain the task-specific nature of the LoRA method from the perspective of interactions.",
      "arxiv_url": "https://openreview.net/forum?id=tRvzEL64dY",
      "pdf_url": "https://openreview.net/pdf/8e25e13e0fa2c765ecf3b2f1c805bc1b10917eca.pdf",
      "primary_category": "Explainable Artificial Intelligence, Large Language Models, Arithmetic Reasoning",
      "categories": [
        "Explainable Artificial Intelligence",
        "Large Language Models",
        "Arithmetic Reasoning",
        "Interactions",
        "Interpretability",
        "Deep Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "mHHrnCWwrD",
      "title": "Exploring Polyglot Harmony: On Multilingual Data Allocation for  Large Language Models Pretraining",
      "authors": [
        "Ping Guo",
        "Yubing Ren",
        "BINBINLIU",
        "Fengze Liu",
        "Haobin Lin",
        "Yifan Zhang",
        "Bingni Zhang",
        "Taifeng Wang",
        "Yin Zheng"
      ],
      "abstract": "Large language models (LLMs) have become integral to a wide range of applications worldwide, driving an unprecedented global demand for effective multilingual capabilities. Central to achieving robust multilingual performance is the strategic allocation of language proportions within training corpora. However, determining optimal language ratios is highly challenging due to intricate cross-lingual interactions and sensitivity to dataset scale. This paper introduces CLIMB (Cross-Lingual Interaction-aware Multilingual Balancing), a novel framework designed to systematically optimize multilingual data allocation. At its core, CLIMB introduces a cross-lingual interaction-aware language ratio, explicitly quantifying each language’s effective allocation by capturing inter-language dependencies. Leveraging this ratio, CLIMB proposes a principled two-step optimization procedure—first equalizing marginal benefits across languages, then maximizing the magnitude of the resulting language allocation vectors—significantly simplifying the inherently complex multilingual optimization problem. Extensive experiments confirm that CLIMB can accurately measure cross-lingual interactions across various multilingual settings. LLMs trained with CLIMB-derived proportions consistently achieve state-of-the-art multilingual performance, even achieve competitive performance with open-sourced LLMs trained with more tokens.",
      "arxiv_url": "https://openreview.net/forum?id=mHHrnCWwrD",
      "pdf_url": "https://openreview.net/pdf/e9de76524170f56669841fa05c57f5a383c03730.pdf",
      "primary_category": "Large Language Model, Pre-training, Scaling Law",
      "categories": [
        "Large Language Model",
        "Pre-training",
        "Scaling Law",
        "Language Mix Ratio"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zDOo34mbpl",
      "title": "Accelerated Evolving Set Processes for Local PageRank Computation",
      "authors": [
        "BinbinHuang",
        "Luo Luo",
        "Yanghua Xiao",
        "Deqing Yang",
        "Baojian Zhou"
      ],
      "abstract": "This work proposes a novel framework based on nested evolving set processes to accelerate Personalized PageRank (PPR) computation. At each stage of the process, we employ a localized inexact proximal point iteration to solve a simplified linear system. We show that the time complexity of such localized methods is upper bounded by $\\min\\{\\tilde{\\mathcal{O}}(R^2/\\epsilon^2), \\tilde{\\mathcal{O}}(m)\\}$ to obtain an $\\epsilon$-approximation of the PPR vector, where $m$ denotes the number of edges in the graph and $R$ is a constant defined via nested evolving set processes. Furthermore, the algorithms induced by our framework require solving only $\\tilde{\\mathcal{O}}(1/\\sqrt{\\alpha})$ such linear systems, where $\\alpha$ is the damping factor. When $1/\\epsilon^2\\ll m$, this implies the existence of an algorithm that computes an $\\epsilon$-approximation of the PPR vector with an overall time complexity of $\\tilde{\\mathcal{O}}(R^2 / (\\sqrt{\\alpha}\\epsilon^2))$, independent of the underlying graph size. Our result resolves an open conjecture from existing literature. Experimental results on real-world graphs validate the efficiency of our methods, demonstrating significant convergence in the early stages.",
      "arxiv_url": "https://openreview.net/forum?id=zDOo34mbpl",
      "pdf_url": "https://openreview.net/pdf/64418067dbd82b1dbbe708737360fcc120fea5ae.pdf",
      "primary_category": "Personalized PageRank, Local methods, Sublinear methods",
      "categories": [
        "Personalized PageRank",
        "Local methods",
        "Sublinear methods"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "22CqLfjiVl",
      "title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs",
      "authors": [
        "Jingyao Wang",
        "Wenwen Qiang",
        "Zeen Song",
        "Changwen Zheng",
        "Hui Xiong"
      ],
      "abstract": "Large language models (LLMs) excel at complex tasks thanks to advances in their reasoning abilities. However, existing methods overlook the trade-off between reasoning effectiveness and efficiency, often encouraging unnecessarily long reasoning chains and wasting tokens. To address this, we propose Learning to Think (L2T), an information-theoretic reinforcement fine-tuning framework for LLMs to make the models achieve optimal reasoning with fewer tokens. Specifically, L2T treats each query-response interaction as a hierarchical session of multiple episodes and proposes a universal dense process reward, i.e., quantifies the episode-wise information gain in parameters, requiring no extra annotations or task-specific evaluators. We propose a method to quickly estimate this reward based on PAC-Bayes bounds and the Fisher information matrix. Theoretical analyses show that it significantly reduces computational complexity with high estimation accuracy. By immediately rewarding each episode's contribution and penalizing excessive updates, L2T optimizes the model via reinforcement learning to maximize the use of each episode and achieve effective updates. Empirical results on various reasoning benchmarks and base models demonstrate the advantage of L2T across different tasks, boosting both reasoning effectiveness and efficiency.",
      "arxiv_url": "https://openreview.net/forum?id=22CqLfjiVl",
      "pdf_url": "https://openreview.net/pdf/de9fc962acefe20dd0d80073eadeb19263afeb06.pdf",
      "primary_category": "LLMs, Reinforcement Fine-Tuning, Process Reward Function",
      "categories": [
        "LLMs",
        "Reinforcement Fine-Tuning",
        "Process Reward Function"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "01hPO0uJhS",
      "title": "Who You Are Matters: Bridging Interests and Social Roles via LLM-Enhanced Logic Recommendation",
      "authors": [
        "Qing Yu",
        "Xiaobei Wang",
        "Shuchang Liu",
        "cheng.feng",
        "Xiaoyu Yang",
        "Xueliang Wang",
        "Chang Meng",
        "Shanshan Wu",
        "HailanYang",
        "Bin Wen",
        "Huihui Xiao",
        "Xiang Li",
        "Fan Yang",
        "Xiaoqiang Feng",
        "Lantao Hu",
        "Han Li",
        "Kun Gai",
        "Lixin Zou"
      ],
      "abstract": "Recommender systems filter contents/items valuable to users by inferring preferences from user features and historical behaviors. \nMainstream approaches follow the learning-to-rank paradigm, which focus on discovering and modeling item topics (e.g.,\ncategories), and capturing user preferences on these topics based on historical interactions.\nHowever, this paradigm often neglects the modeling of user characteristics and their social roles, which are logical confounders influencing the correlated interest and user preference transition. \nTo bridge this gap, we introduce the user role identification task and the behavioral logic modeling task that aim to explicitly model user roles and learn the logical relations between item topics and user social roles. \nWe show that it is possible to explicitly solve these tasks through an efficient integration framework of Large Language Model (LLM) and recommendation systems, for which we propose TagCF. \nOn the one hand, TagCF exploits the (Multi-modal) LLM's world knowledge and logic inference ability to extract realistic tag-based virtual logic graphs that reveal dynamic and expressive knowledge of users, refining our understanding of user behaviors.\nOn the other hand, TagCF presents empirically effective integration modules that take advantage of the extracted tag-logic information, augmenting the recommendation performance.\nWe conduct both online experiments and offline experiments with industrial and public datasets as verification of TagCF's effectiveness, and we empirically show that the user role modeling strategy is potentially a better choice than the modeling of item topics. \nAdditionally, we provide evidence that the extracted logic graphs are empirically a general and transferable knowledge that can benefit a wide range of recommendation tasks. Our code is available in https://github.com/Code2Q/TagCF.",
      "arxiv_url": "https://openreview.net/forum?id=01hPO0uJhS",
      "pdf_url": "https://openreview.net/pdf/35643faca79c1fa903aa75c720fce31053ff7a4b.pdf",
      "primary_category": "recommendation with large language models, large language models, recommender systems",
      "categories": [
        "recommendation with large language models",
        "large language models",
        "recommender systems",
        "user roles",
        "collaborative filtering"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cpgCK7LdgU",
      "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
      "authors": [
        "BingQuan Dai",
        "Luo Li",
        "Qihong Tang",
        "Jie Wang",
        "Xinyu Lian",
        "Hao Xu",
        "Minghan Qin",
        "Xudong XU",
        "Bo Dai",
        "Haoqian Wang",
        "Zhaoyang Lyu",
        "Jiangmiao Pang"
      ],
      "abstract": "Reconstructing 3D objects into editable programs is pivotal for applications like reverse engineering and shape editing. However, existing methods often rely on limited domain-specific languages (DSLs) and small-scale datasets, restricting their ability to model complex geometries and structures. To address these challenges, we introduce MeshLLM, a novel framework that reconstructs complex 3D objects from point clouds into editable Blender Python scripts. We develop a comprehensive set of expressive Blender Python APIs capable of synthesizing intricate geometries. Leveraging these APIs, we construct a large-scale paired object-code dataset, where the code for each object is decomposed into distinct semantic parts. Subsequently, we train a multimodal large language model (LLM) that translates 3D point cloud into executable Blender Python scripts. Our approach not only achieves superior performance in shape-to-code reconstruction tasks but also facilitates intuitive geometric and topological editing through convenient code modifications. Furthermore, our code-based representation enhances the reasoning capabilities of LLMs in 3D shape understanding tasks. Together, these contributions establish MeshLLM as a powerful and flexible solution for programmatic 3D shape reconstruction and understanding.",
      "arxiv_url": "https://openreview.net/forum?id=cpgCK7LdgU",
      "pdf_url": "https://openreview.net/pdf/3234b9639aab63b30612b018cf50c1e7223223ec.pdf",
      "primary_category": "3D reconstruction, point cloud, LLM",
      "categories": [
        "3D reconstruction",
        "point cloud",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tXnyVPNOfa",
      "title": "SAEMark: Steering Personalized Multilingual LLM Watermarks with Sparse Autoencoders",
      "authors": [
        "Zhuohao Yu",
        "Xingru Jiang",
        "Weizheng Gu",
        "Yidong Wang",
        "Qingsong Wen",
        "Shikun Zhang",
        "Wei Ye"
      ],
      "abstract": "Watermarking LLM-generated text is critical for content attribution and misinformation prevention, yet existing methods compromise text quality and require white-box model access with logit manipulation or training, which exclude API-based models and multilingual scenarios. We propose SAEMark, an **inference-time framework** for *multi-bit* watermarking that embeds personalized information through *feature-based rejection sampling*, fundamentally different from logit-based or rewriting-based approaches: we **do not modify model outputs directly** and require only **black-box access**, while naturally supporting multi-bit message embedding and generalizing across diverse languages and domains. We instantiate the framework using *Sparse Autoencoders* as deterministic feature extractors and provide theoretical worst-case analysis relating watermark accuracy to computational budget. Experiments across 4 datasets demonstrate strong watermarking performance on English, Chinese, and code while preserving text quality. SAEMark establishes a new paradigm for **scalable, quality-preserving watermarks** that work seamlessly with closed-source LLMs across languages and domains.",
      "arxiv_url": "https://openreview.net/forum?id=tXnyVPNOfa",
      "pdf_url": "https://openreview.net/pdf/e4e1219174f817452167986cc40e52e34557aec5.pdf",
      "primary_category": "Watermarking, Large Language Models, Sparse Autoencoders",
      "categories": [
        "Watermarking",
        "Large Language Models",
        "Sparse Autoencoders",
        "Interpretability",
        "Accountability of LLMs",
        "AIGC Detection"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "p4jKtPCcUh",
      "title": "Perceive Anything: Recognize, Explain, Caption, and Segment Anything in Images and Videos",
      "authors": [
        "Weifeng Lin",
        "Xinyu Wei",
        "Ruichuan An",
        "Tianhe Ren",
        "Tingwei Chen",
        "Renrui Zhang",
        "Ziyu Guo",
        "Wentao Zhang",
        "Lei Zhang",
        "Hongsheng Li"
      ],
      "abstract": "We present Perceive Anything Model (PAM), a conceptually straightforward and efficient framework for comprehensive region-level visual understanding in images and videos. Our approach extends the powerful segmentation model SAM 2 by integrating Large Language Models (LLMs), enabling simultaneous object segmentation with the generation of diverse, region-specific semantic outputs, including categories, label definition, functional explanations, and detailed captions. A key component, Semantic Perceiver, is introduced to efficiently transform SAM 2's rich visual features, which inherently carry general vision, localization, and semantic priors into multi-modal tokens for LLM comprehension. To support robust multi-granularity understanding, we also develop a dedicated data refinement and augmentation pipeline, yielding a high-quality dataset of 1.5M image and 0.6M video region-semantic annotations, including novel region-level streaming video caption data. PAM is designed for lightweightness and efficiency, while also demonstrates strong performance across a diverse range of region understanding tasks. It runs 1.2$-$2.4$\\times$ faster and consumes less GPU memory than prior approaches, offering a practical solution for real-world applications. We believe that our effective approach will serve as a strong baseline for future research in region-level visual understanding.",
      "arxiv_url": "https://openreview.net/forum?id=p4jKtPCcUh",
      "pdf_url": "https://openreview.net/pdf/e2afd7ba0d0b391c22e7ec59a588eec19e695b81.pdf",
      "primary_category": "Vision Foundation Model; VLM; Image and Video Caption;",
      "categories": [
        "Vision Foundation Model; VLM; Image and Video Caption;"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2X0Fy0jqPa",
      "title": "Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video",
      "authors": [
        "Yulin Zhang",
        "Cheng Shi",
        "Yang Wang",
        "Sibei Yang"
      ],
      "abstract": "Envision an AI capable of functioning in human-like settings, moving beyond mere observation to actively understand, anticipate, and proactively respond to unfolding events. Towards this vision, we focus on the innovative task where, given ego-streaming video input, an assistant proactively answers diverse, evolving questions at the opportune moment, while maintaining synchronized perception and reasoning. This task embodies three key properties: (1) Proactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized Efficiency.\nTo evaluate and address these properties, we first introduce ESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric—a novel framework designed for their rigorous assessment. Secondly, we propose a comprehensive technical pipeline to enable models to tackle this challenging task. This pipeline comprises: (1) a data engine, (2) a multi-stage training strategy, and (3) a proactive dynamic compression technique. Our proposed model effectively addresses these critical properties while achieving state-of-the-art (SOTA) performance on the standard COIN benchmark.",
      "arxiv_url": "https://openreview.net/forum?id=2X0Fy0jqPa",
      "pdf_url": "https://openreview.net/pdf/ab47893fbf025d19b2616f355310240a7f85a384.pdf",
      "primary_category": "Egocentric Video Understanding, Multimodel Large Language Model, Streaming Understanding",
      "categories": [
        "Egocentric Video Understanding",
        "Multimodel Large Language Model",
        "Streaming Understanding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nTc0LSqtqE",
      "title": "Vision Function Layer in Multimodal LLMs",
      "authors": [
        "Cheng Shi",
        "Yizhou Yu",
        "Sibei Yang"
      ],
      "abstract": "This study identifies that visual-related functional decoding is distributed across different decoder layers in Multimodal Large Language Models (MLLMs). Typically, each function, such as counting, grounding, or OCR recognition, narrows down to two or three layers, which we define as Vision Function Layers (VFL). Additionally, the depth and its order of different VFLs exhibits a consistent pattern across different MLLMs, which is well-aligned with human behaviors (e.g., recognition occurs first, followed by counting, and then grounding). These findings are derived from Visual Token Swapping, our novel analytical framework that modifies targeted KV cache entries to precisely elucidate layer-specific functions during decoding. Furthermore, these insights offer substantial utility in tailoring MLLMs for real-world downstream applications. For instance, when LoRA training is selectively applied to VFLs whose functions align with the training data, VFL-LoRA not only outperform full-LoRA but also prevent out-of-domain function forgetting. Moreover, by analyzing the performance differential on training data when particular VFLs are ablated, VFL-select automatically classifies data by function, enabling highly efficient data selection to directly bolster corresponding capabilities. Consequently, VFL-select surpasses human experts in data selection, and achieves 98% of full-data performance with only 20% of the original dataset. This study delivers deeper comprehension of MLLM visual processing, fostering the creation of more efficient, interpretable, and robust models.",
      "arxiv_url": "https://openreview.net/forum?id=nTc0LSqtqE",
      "pdf_url": "https://openreview.net/pdf/7c1d2b79da0c0973cc9eecde4bb02ec762d41f4b.pdf",
      "primary_category": "Multimodal Large Language Model, Vision Function Layer",
      "categories": [
        "Multimodal Large Language Model",
        "Vision Function Layer"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ILr4UNiZcQ",
      "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations",
      "authors": [
        "Jiaming Han",
        "Hao Chen",
        "Yang Zhao",
        "Hanyu Wang",
        "Qi Zhao",
        "Ziyan Yang",
        "Hao He",
        "Xiangyu Yue",
        "Lu Jiang"
      ],
      "abstract": "This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, **Tar**, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a \ngenerative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that **Tar** matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. All code, models, and data will be made publicly available.",
      "arxiv_url": "https://openreview.net/forum?id=ILr4UNiZcQ",
      "pdf_url": "https://openreview.net/pdf/eb5bbda60f590ecdfd40b87a87e9ad051d936d56.pdf",
      "primary_category": "Multimodal Large Language Model, Unified MLLM, Image Generation",
      "categories": [
        "Multimodal Large Language Model",
        "Unified MLLM",
        "Image Generation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tq9lyV9Cml",
      "title": "Thought Communication in Multiagent Collaboration",
      "authors": [
        "Yujia Zheng",
        "Zhuokai Zhao",
        "Zijian Li",
        "Yaqi Xie",
        "Mingze Gao",
        "Lizhu Zhang",
        "Kun Zhang"
      ],
      "abstract": "Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect nature limits the potential of collective intelligence. While machines are not subject to these constraints, most LLM-based multi-agent systems still rely solely on natural language, exchanging tokens or their embeddings. To go beyond language, we introduce a new paradigm, *thought communication*, which enables agents to interact directly mind-to-mind, akin to telepathy. To uncover these latent thoughts in a principled way, we formalize the process as a general latent variable model, where agent states are generated by an unknown function of underlying thoughts. We prove that, in a nonparametric setting without auxiliary information, both shared and private latent thoughts between any pair of agents can be identified. Moreover, the global structure of thought sharing, including which agents share which thoughts and how these relationships are structured, can also be recovered with theoretical guarantees. Guided by the established theory, we develop a framework that extracts latent thoughts from all agents prior to communication and assigns each agent the relevant thoughts, along with their sharing patterns. This paradigm naturally extends beyond LLMs to all modalities, as most observational data arise from hidden generative processes. Experiments on both synthetic and real-world benchmarks validate the theory and demonstrate the collaborative advantages of thought communication. We hope this work illuminates the potential of leveraging the hidden world, as many challenges remain unsolvable through surface-level observation alone, regardless of compute or data scale.",
      "arxiv_url": "https://openreview.net/forum?id=tq9lyV9Cml",
      "pdf_url": "https://openreview.net/pdf/7e84ecbda8064983f20c395751d38019a0ab3981.pdf",
      "primary_category": "multiagent system, inter-agent communication, latent variable models",
      "categories": [
        "multiagent system",
        "inter-agent communication",
        "latent variable models",
        "foundation models"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nPYtkZu65y",
      "title": "Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression",
      "authors": [
        "Xiaohui Wang",
        "Peng Ye",
        "Chenyu Huang",
        "Shenghe Zheng",
        "Bo Zhang",
        "LEI BAI",
        "Wanli Ouyang",
        "Tao Chen"
      ],
      "abstract": "With the rise of the fine-tuned–pretrained paradigm, storing numerous fine-tuned models for multi-tasking creates significant storage overhead.\nDelta compression alleviates this by storing only the pretrained model and the highly compressed delta weights (the differences between fine-tuned and pretrained model weights). \nHowever, existing methods fail to maintain both high compression and performance, and often rely on data.\nTo address these challenges, we propose UltraDelta, the first data-free delta compression pipeline that achieves both ultra-high compression and strong performance. \nUltraDelta is designed to minimize redundancy, maximize information, and stabilize performance across inter-layer, intra-layer, and global dimensions, using three key components:\n(1) Variance-Based Mixed Sparsity Allocation assigns sparsity based on variance, giving lower sparsity to high-variance layers to preserve inter-layer information.\n(2) Distribution-Aware Compression applies uniform quantization and then groups parameters by value, followed by group-wise pruning, to better preserve intra-layer distribution.\n(3) Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a global rescaling factor, improving model stability under higher compression.\nExtensive experiments across \n(a) large language models (fine-tuned on LLaMA-2 7B and 13B) with up to 50$\\times$ compression, \n(b) general NLP models (RoBERTa-base, T5-base) with up to 224$\\times$ compression,\n(c) vision models (ViT-B/32, ViT-L/14) with up to 132$\\times$ compression, and\n(d) multi-modal models (BEiT-3) with 18$\\times$ compression, \ndemonstrate that UltraDelta consistently outperforms existing methods, especially under ultra-high compression.\nCode is available at https://github.com/xiaohuiwang000/UltraDelta.",
      "arxiv_url": "https://openreview.net/forum?id=nPYtkZu65y",
      "pdf_url": "https://openreview.net/pdf/6af796d0b594a6393b92c4ac8277de8aa2e70802.pdf",
      "primary_category": "Delta Compression, Pruning, Quantization",
      "categories": [
        "Delta Compression",
        "Pruning",
        "Quantization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "IVWHe60vfA",
      "title": "Distribution-Aligned Decoding for Efficient LLM Task Adaptation",
      "authors": [
        "Senkang Hu",
        "Xudong Han",
        "Jinqi Jiang",
        "Yihang Tao",
        "Zihan Fang",
        "Yong Dai",
        "Sam Kwong",
        "Yuguang Fang"
      ],
      "abstract": "Adapting billion-parameter language models to a downstream task is still costly, even with parameter-efficient fine-tuning (PEFT). We re-cast task adaptation as output-distribution alignment: the objective is to steer the output distribution toward the task distribution directly during decoding rather than indirectly through weight updates. Building on this view, we introduce Steering Vector Decoding (SVDecode), a lightweight, PEFT-compatible, and theoretically grounded method. We start with a short warm-start fine-tune and extract a task-aware steering vector from the Kullback-Leibler (KL) divergence gradient between the output distribution of the warm-started and pre-trained models. This steering vector is then used to guide the decoding process to steer the model's output distribution towards the task distribution. We theoretically prove that SVDecode is first-order equivalent to the gradient step of full fine-tuning and derive a globally optimal solution for the strength of the steering vector. Across three tasks and nine benchmarks, SVDecode paired with four standard PEFT methods improves multiple-choice accuracy by up to 5 percentage points and open-ended truthfulness by 2 percentage points, with similar gains (1-2 percentage points) on commonsense datasets without adding trainable parameters beyond the PEFT adapter. SVDecode thus offers a lightweight, theoretically grounded path to stronger task adaptation for large language models.",
      "arxiv_url": "https://openreview.net/forum?id=IVWHe60vfA",
      "pdf_url": "https://openreview.net/pdf/43b00cd1ef1085ddf78f1ce1c08f01c5cea99f54.pdf",
      "primary_category": "large language models, task adaptation, parameter-effcient fine-tuning",
      "categories": [
        "large language models",
        "task adaptation",
        "parameter-effcient fine-tuning",
        "LLM decoding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "9T6Pu6iWL6",
      "title": "Semantic Retrieval Augmented Contrastive Learning for Sequential Recommendation",
      "authors": [
        "Ziqiang Cui",
        "Yunpeng Weng",
        "Xing Tang",
        "Xiaokun Zhang",
        "Shiwei Li",
        "Peiyang Liu",
        "Bowei He",
        "Dugang Liu",
        "Weihong Luo",
        "xiuqiang He",
        "Chen Ma"
      ],
      "abstract": "Contrastive learning has shown effectiveness in improving sequential recommendation models. However, existing methods still face challenges in generating high-quality contrastive pairs: they either rely on random perturbations that corrupt user preference patterns or depend on sparse collaborative data that generates unreliable contrastive pairs. Furthermore, existing approaches typically require predefined selection rules that impose strong assumptions, limiting the model's ability to autonomously learn optimal contrastive pairs. To address these limitations, we propose a novel approach named Semantic Retrieval Augmented Contrastive Learning (SRA-CL). SRA-CL leverages the semantic understanding and reasoning capabilities of LLMs to generate expressive embeddings that capture both user preferences and item characteristics. These semantic embeddings enable the construction of candidate pools for inter-user and intra-user contrastive learning through semantic-based retrieval. To further enhance the quality of the contrastive samples, we introduce a learnable sample synthesizer that optimizes the contrastive sample generation process during model training. SRA-CL adopts a plug-and-play design, enabling seamless integration with existing sequential recommendation architectures. Extensive experiments on four public datasets demonstrate the effectiveness and model-agnostic nature of our approach. \nOur code is available at https://github.com/ziqiangcui/SRA-CL",
      "arxiv_url": "https://openreview.net/forum?id=9T6Pu6iWL6",
      "pdf_url": "https://openreview.net/pdf/de3655de0e4af20e270594e8bfbe5be6efd3f7f2.pdf",
      "primary_category": "Large Language Models, Recommender Systems, Contrastive Learning",
      "categories": [
        "Large Language Models",
        "Recommender Systems",
        "Contrastive Learning"
      ],
      "tags": [
        "LLM",
        "RAG",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "aTqfufujj7",
      "title": "From Indicators to Insights: Diversity-Optimized for Medical Series-Text Decoding via LLMs",
      "authors": [
        "Xiyuan Jin",
        "Jing Wang",
        "Ziwei Lin",
        "Qianru Jia",
        "Yuqing Huang",
        "Xiaojun Ning",
        "Zhonghua Shi",
        "Youfang Lin"
      ],
      "abstract": "Medical time-series analysis differs fundamentally from general ones by requiring specialized domain knowledge to interpret complex signals and clinical context.\nLarge language models (LLMs) hold great promise for augmenting medical time-series analysis by complementing raw series with rich contextual knowledge drawn from biomedical literature and clinical guidelines.\nHowever, realizing this potential depends on precise and meaningful prompts that guide the LLM to key information.\nYet, determining what constitutes effective prompt content remains non-trivial—especially in medical settings where signal interpretation often hinges on subtle, expert-defined decision-making indicators. \nTo this end, we propose InDiGO, a knowledge-aware evolutionary learning framework that integrates clinical signals and decision-making indicators through iterative optimization.\nAcross four medical benchmarks, InDiGO consistently outperforms prior methods. \nThe code is available at: https://github.com/jinxyBJTU/InDiGO.",
      "arxiv_url": "https://openreview.net/forum?id=aTqfufujj7",
      "pdf_url": "https://openreview.net/pdf/67bc5bef10544d2a90987a26196a07f6ddd2fe66.pdf",
      "primary_category": "biomedical signal analysis, medical time series, knowledge-guided learning",
      "categories": [
        "biomedical signal analysis",
        "medical time series",
        "knowledge-guided learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pebVFFVs2R",
      "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs",
      "authors": [
        "Yunheng Li",
        "JingCheng",
        "Shaoyong Jia",
        "Hangyi Kuang",
        "Shaohui Jiao",
        "Qibin Hou",
        "Ming-Ming Cheng"
      ],
      "abstract": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets:Charades-STA (R1\\@0.7: 52.9\\%, +**2.7**\\%), ActivityNet Captions (R1\\@0.5: 56.0\\%, +**5.3**\\%), and QVHighlights (mAP: 30.0\\%, +**3.0**\\%). Moreover,  TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code is available at https://github.com/HVision-NKU/TempSamp-R1.",
      "arxiv_url": "https://openreview.net/forum?id=pebVFFVs2R",
      "pdf_url": "https://openreview.net/pdf/2d01d96582304da4be0ebb7741e61b36c9a8d81d.pdf",
      "primary_category": "Temporal Grounding; Multimodal Large Language Model; Reinforcement Fine-Tuning",
      "categories": [
        "Temporal Grounding; Multimodal Large Language Model; Reinforcement Fine-Tuning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "mj8VN4MyrO",
      "title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal LLMs for Audio Generation and Editing",
      "authors": [
        "Huadai Liu",
        "Kaicheng Luo",
        "Jialei Wang",
        "Wen Wang",
        "Qian Chen",
        "Zhou Zhao",
        "Wei Xue"
      ],
      "abstract": "While end-to-end video-to-audio generation has greatly improved, producing high-fidelity audio that authentically captures the nuances of visual content remains challenging. Like professionals in the creative industries, this generation requires sophisticated reasoning about items such as visual dynamics, acoustic environments, and temporal relationships. We present **ThinkSound**, a novel framework that leverages Chain-of-Thought (CoT) reasoning to enable stepwise, interactive audio generation and editing for videos. Our approach decomposes the process into three complementary stages: foundational foley generation that creates semantically coherent soundscapes, interactive object-centric refinement through precise user interactions, and targeted editing guided by natural language instructions. At each stage, a multimodal large language model generates contextually aligned CoT reasoning that guides a unified audio foundation model. Furthermore, we introduce **AudioCoT**, a comprehensive dataset with structured reasoning annotations that establishes connections between visual content, textual descriptions, and sound synthesis.  Experiments demonstrate that ThinkSound achieves state-of-the-art performance in video-to-audio generation across both audio metrics and CoT metrics, and excels in the out-of-distribution Movie Gen Audio benchmark. The project page is available at https://ThinkSound-Project.github.io.",
      "arxiv_url": "https://openreview.net/forum?id=mj8VN4MyrO",
      "pdf_url": "https://openreview.net/pdf/ab607e7bafedbbb70436016fbdc46ffe23acfd66.pdf",
      "primary_category": "Any-to-Audio Generation, Chain-of-thought, Multimodal Large Language Model",
      "categories": [
        "Any-to-Audio Generation",
        "Chain-of-thought",
        "Multimodal Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "X8oEu4Gs3W",
      "title": "VQToken: Neural Discrete Token Representation Learning for Extreme Token Reduction in Video Large Language Models",
      "authors": [
        "Haichao Zhang",
        "Yun Fu"
      ],
      "abstract": "Token-based video representation has emerged as a promising approach for enabling large language models (LLMs) to interpret video content. However, existing token reduction techniques, such as pruning and merging, often disrupt essential positional embeddings and rely on continuous visual tokens sampled from nearby pixels with similar spatial–temporal locations. By removing only a small fraction of tokens, these methods still produce relatively lengthy continuous sequences, which falls short of the extreme compression required to balance computational efficiency and token count in video LLMs.\nIn this paper, we introduce the novel task of **Extreme Short Token Reduction**, which aims to represent entire videos using a minimal set of discrete tokens.  We propose **VQToken**, a neural discrete token representation framework that\n(i) applies adaptive vector quantization to continuous ViT embeddings to learn a compact codebook and \n(ii) preserves spatial–temporal positions via a token hash function by assigning each grid-level token to its nearest codebook entry.\nOn the Extreme Short Token Reduction task, our VQToken compresses sequences to just **0.07\\%** of their original length while incurring only a **0.66\\%** drop in accuracy on NextQA-MC benchmark. It also achieves comparable performance on ActNet-QA, Long Video Bench, and VideoMME. \nWe further introduce the **Token Information Density** (**TokDense**) metric and formalize fixed-length and adaptive-length subtasks, achieving state-of-the-art results in both settings. Our approach dramatically lowers theoretical complexity, increases information density, way fewer tokens counts, and enables efficient video large language models in resource-constrained environments.",
      "arxiv_url": "https://openreview.net/forum?id=X8oEu4Gs3W",
      "pdf_url": "https://openreview.net/pdf/a40769a7cbdb383c92f583c36410de2950277c2d.pdf",
      "primary_category": "Video LLMs; Token Reduction; Discrete Token Representation; Extreme Token Reduction; Token Merge; Token Clustering",
      "categories": [
        "Video LLMs; Token Reduction; Discrete Token Representation; Extreme Token Reduction; Token Merge; Token Clustering"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7x5X6gTCUH",
      "title": "Miss-ReID: Delivering Robust Multi-Modality Object Re-Identification Despite Missing Modalities",
      "authors": [
        "Ruida Xi"
      ],
      "abstract": "Multi-modality object Re-IDentification (ReID) targets to retrieve special objects by integrating complementary information from diverse visual sources.\nHowever, existing models that are trained on modality-complete datasets typically exhibit significantly degraded discrimination\nduring inference with modality-incomplete inputs.\nThis disparity highlights the necessity of developing a robust multi-modality ReID model that remains effective in real-world applications. For that, this paper delivers a flexible framework tailored for more realistic multi-modality retrieval scenario, dubbed as Miss-ReID, which is the first work to friendly support both the modality-missing training and inference conditions. The core of Miss-ReID lies in compensating for missing visual cues via vision-text knowledge transfer driven by Vision-Language foundation Models (VLMs), effectively mitigating performance degradation. In brief, we capture diverse visual features from accessible modalities first, and then build memory banks to store heterogeneous prototypes for each identity, preserving multi-modality characteristics. Afterwards, we employ structure-aware query interactions to dynamically distill modality-invariant object structures from existing localized visual patches, which are further reversed into pseudo-word tokens that encapsulate the identity-relevant structural semantics.\nIn tandem, the inverted tokens, integrated with learnable modality prompts, are embedded into crafted textual template to form the personalized linguistic descriptions tailored for diverse modalities.\nUltimately, harnessing VLMs' inherent vision-text alignment capability, the resulting textual features effectively function as compensatory semantic representations for missing visual modalities, after being optimized with some memory-based alignment constraints.\nExtensive experiments demonstrate our model's efficacy and superiority over state-of-the-art methods in various modality-missing scenarios, and our endeavors further propel multi-modality ReID into real-world applications.",
      "arxiv_url": "https://openreview.net/forum?id=7x5X6gTCUH",
      "pdf_url": "https://openreview.net/pdf/c3f9367bd1a793c02ccb2db3a34298937d11a970.pdf",
      "primary_category": "Multi-Modality Object Re-Identification, Missing Modalities, Vision-Language Foundation Models",
      "categories": [
        "Multi-Modality Object Re-Identification",
        "Missing Modalities",
        "Vision-Language Foundation Models"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "qE3knKF1rz",
      "title": "PANDA: Towards Generalist Video Anomaly Detection via Agentic AI Engineer",
      "authors": [
        "Zhiwei Yang",
        "Chen Gao",
        "Mike Zheng Shou"
      ],
      "abstract": "Video anomaly detection (VAD) is a critical yet challenging task due to the complex and diverse nature of real-world scenarios. Previous methods typically rely on domain-specific training data and manual adjustments when applying to new scenarios and unseen anomaly types, suffering from high labor costs and limited generalization. Therefore, we aim to achieve generalist VAD, \\ie, automatically handle any scene and any anomaly types without training data or human involvement. In this work, we propose PANDA, an agentic AI engineer based on MLLMs. Specifically, we achieve PANDA by comprehensively devising four key capabilities: (1) self-adaptive scene-aware strategy planning, (2) goal-driven heuristic reasoning, (3) tool-augmented self-reflection, and (4) self-improving chain-of-memory. Concretely, we develop a self-adaptive scene-aware RAG mechanism, enabling PANDA to retrieve anomaly-specific knowledge for anomaly detection strategy planning. Next, we introduce a latent anomaly-guided heuristic prompt strategy to enhance reasoning precision. Furthermore, PANDA employs a progressive reflection mechanism alongside a suite of context-aware tools to iteratively refine decision-making in complex scenarios. Finally, a chain-of-memory mechanism enables PANDA to leverage historical experiences for continual performance improvement. Extensive experiments demonstrate that PANDA achieves state-of-the-art performance in multi-scenario, open-set, and complex scenario settings without training and manual involvement, validating its generalizable and robust anomaly detection capability. Code is released at https://github.com/showlab/PANDA.",
      "arxiv_url": "https://openreview.net/forum?id=qE3knKF1rz",
      "pdf_url": "https://openreview.net/pdf/467380f59815c312492797fd471737efc6228947.pdf",
      "primary_category": "Video Anomaly  Detection, AI Agent, Agentic AI Engineer",
      "categories": [
        "Video Anomaly  Detection",
        "AI Agent",
        "Agentic AI Engineer",
        "Multimodal Large Language Model"
      ],
      "tags": [
        "RAG",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "rZ2nSt1X58",
      "title": "Optimization Inspired Few-Shot Adaptation for Large Language Models",
      "authors": [
        "Boyan Gao",
        "Xin Wang",
        "Yibo Yang",
        "David A. Clifton"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in real-world applications. However, adapting LLMs to novel tasks via fine-tuning often requires substantial training data and computational resources that are impractical in few-shot scenarios. Existing approaches, such as In-context learning and Parameter-Efficient Fine-Tuning (PEFT), face key limitations: In-context learning introduces additional inference computational overhead with limited performance gains, while PEFT models are prone to overfitting on the few demonstration examples. In this work, we reinterpret the forward pass of LLMs as an optimization process, a sequence of preconditioned gradient descent steps refining internal representations. Based on this connection, we propose Optimization-Inspired Few-Shot Adaptation (OFA), integrating a parameterization that learns preconditioners without introducing additional trainable parameters, and an objective that improves optimization efficiency by learning preconditioners based on a convergence bound, while simultaneously steering the optimization path toward the flat local minimum. Our method overcomes both issues of ICL-based and PEFT-based methods, and demonstrates superior performance over the existing methods on a variety of few-shot adaptation tasks in experiments.",
      "arxiv_url": "https://openreview.net/forum?id=rZ2nSt1X58",
      "pdf_url": "https://openreview.net/pdf/e7d0199b09bc7bbe4186eaa16911e8e1f1d10050.pdf",
      "primary_category": "Large Language Models, Opitmizer learning, Few-shot adaptation",
      "categories": [
        "Large Language Models",
        "Opitmizer learning",
        "Few-shot adaptation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "26kUrQm4zw",
      "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging",
      "authors": [
        "Hongjin Qian",
        "Zheng Liu"
      ],
      "abstract": "Augmenting large language models (LLMs) with external retrieval has become a standard method to address their inherent knowledge cutoff limitations. However, traditional retrieval-augmented generation methods employ static, pre-inference retrieval strategies, making them inadequate for complex tasks involving ambiguous, multi-step, or evolving information needs. Recent advances in test-time scaling techniques have demonstrated significant potential in enabling LLMs to dynamically interact with external tools, motivating the shift toward adaptive inference-time retrieval.\nInspired by Information Foraging Theory (IFT), we propose InForage, a reinforcement learning framework that formalizes retrieval-augmented reasoning as a dynamic information-seeking process. Unlike existing approaches, InForage explicitly rewards intermediate retrieval quality, encouraging LLMs to iteratively gather and integrate information through adaptive search behaviors. To facilitate training, we construct a human-guided dataset capturing iterative search and reasoning trajectories for complex, real-world web tasks. Extensive evaluations across general question answering, multi-hop reasoning tasks, and a newly developed real-time web QA dataset demonstrate InForage's superior performance over baseline methods. \nThese results highlight InForage's effectiveness in building robust, adaptive, and efficient reasoning agents. \nWe provide all codes and datasets in the supplementary materials.",
      "arxiv_url": "https://openreview.net/forum?id=26kUrQm4zw",
      "pdf_url": "https://openreview.net/pdf/a39cc30f01a1626169906a2a00ee2c02c20ae1c9.pdf",
      "primary_category": "Complex knowledge discovery, Search-Enhanced Reasoning, Retrieval-Augmented Generation",
      "categories": [
        "Complex knowledge discovery",
        "Search-Enhanced Reasoning",
        "Retrieval-Augmented Generation"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "F9SSJLg55j",
      "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding",
      "authors": [
        "Zaiquan Yang",
        "Yuhao LIU",
        "Gerhard Petrus Hancke",
        "Rynson W. H. Lau"
      ],
      "abstract": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query.\nIn this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG.\nWe reveal two key insights about MLLMs: \n(1) MLLMs tend to dynamically assign special tokens, referred to as \\textit{grounding tokens}, for grounding the text query; and\n(2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (\\textit{e.g.}, attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs.\nThe DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally.\nIt then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query.\nThese prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions.\nIn addition, as the spatial grounding by the attribute sub-query should be temporally consistent,\nwe introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency.\nWe evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.",
      "arxiv_url": "https://openreview.net/forum?id=F9SSJLg55j",
      "pdf_url": "https://openreview.net/pdf/b52fe8badf9d51d238b11c78837e98e5452ef5c5.pdf",
      "primary_category": "Multimodal Large Language Models, Spatio-Temporal Video Grounding, Zero-shot Learning",
      "categories": [
        "Multimodal Large Language Models",
        "Spatio-Temporal Video Grounding",
        "Zero-shot Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8y18QBU2s6",
      "title": "Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient Multimodal Retrieval",
      "authors": [
        "Lanyun Zhu",
        "Deyi Ji",
        "Tianrun Chen",
        "Haiyang Wu",
        "Shiqi Wang"
      ],
      "abstract": "The success of DeepSeek-R1 demonstrates the immense potential of using reinforcement learning (RL) to enhance LLMs' reasoning capabilities. This paper introduces Retrv-R1, the first R1-style MLLM specifically designed for multimodal universal retrieval, achieving higher performance by employing step-by-step reasoning to produce more accurate retrieval results. We find that directly applying the methods of DeepSeek-R1 to retrieval tasks is not feasible, mainly due to (1) the high computational cost caused by the large token consumption required for multiple candidates with reasoning processes, and (2) the instability and suboptimal results when directly applying RL to train for retrieval tasks. To address these issues, Retrv-R1 introduces an information compression module with a details inspection mechanism, which enhances computational efficiency by reducing the number of tokens while ensuring that critical information for challenging candidates is preserved. Additionally, a new training paradigm is proposed, including an activation stage using a retrieval-tailored synthetic CoT dataset for more effective optimization, followed by RL with a novel curriculum reward to improve both performance and efficiency. Incorporating these novel designs, Retrv-R1 achieves SOTA performance, high efficiency, and strong generalization ability, as demonstrated by extensive experiments across multiple benchmarks and tasks.",
      "arxiv_url": "https://openreview.net/forum?id=8y18QBU2s6",
      "pdf_url": "https://openreview.net/pdf/7cb940939c9f96e79b0c9c85d4e93db10956f054.pdf",
      "primary_category": "Multimodal retrieval",
      "categories": [
        "Multimodal retrieval"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "kWGJa9ZO3M",
      "title": "Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM",
      "authors": [
        "Zinuo Li",
        "Xian Zhang",
        "Yongxin Guo",
        "Mohammed Bennamoun",
        "Farid Boussaid",
        "Girish Dwivedi",
        "Luqi Gong",
        "Qiuhong Ke"
      ],
      "abstract": "Humans naturally understand moments in a video by integrating visual and auditory cues. For example, localizing a scene in the video like “A scientist passionately speaks on wildlife conservation as dramatic orchestral music plays, with the audience nodding and applauding” requires simultaneous processing of visual, audio, and speech signals. However, existing models often struggle to effectively fuse and interpret audio information, limiting their capacity for comprehensive video temporal understanding. To address this, we present TriSense, a triple-modality large language model designed for holistic video temporal understanding through the integration of visual, audio, and speech modalities. Central to TriSense is a Query-Based Connector that adaptively reweights modality contributions based on the input query, enabling robust performance under modality dropout and allowing flexible combinations of available inputs. To support TriSense's multimodal capabilities, we introduce TriSense-2M, a high-quality dataset of over 2 million curated samples generated via an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes long-form videos and diverse modality combinations, facilitating broad generalization. Extensive experiments across multiple benchmarks demonstrate the effectiveness of TriSense and its potential to advance multimodal video analysis.",
      "arxiv_url": "https://openreview.net/forum?id=kWGJa9ZO3M",
      "pdf_url": "https://openreview.net/pdf/9c1a45b8c6ca39e0873dd3551bffff035ddbe445.pdf",
      "primary_category": "Video Understanding, Video Temporal Understanding",
      "categories": [
        "Video Understanding",
        "Video Temporal Understanding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "xgiMK8FtSI",
      "title": "LLM-DAMVC: A Large Language Model Assisted Dynamic Agent for Multi-View Clustering",
      "authors": [
        "HaiMing Xu",
        "Qianqian Wang"
      ],
      "abstract": "Multi-view clustering integrates the consistency and complementarity of different views to achieve unsupervised data grouping. Existing multi-view clustering methods primarily confront two challenges: i) they generally perform feature extraction in the feature domain, which is sensitive to noise and may neglect cluster-specific information that is indistinguishable in the original space; ii) current dynamic fusion methods adopt static strategies to learn weights, lacking capability to adjust strategies adaptively under complex scenarios according to variations in data distribution and view quality. To address these issues, we propose a large language model assisted dynamic agent for multi-view clustering (LLM-DAMVC), a novel framework that recasts multi-view clustering as a dynamic decision-making problem orchestrated by a large language model. Specifically, each view is equipped with complementary agents dedicated to feature extraction. A dual-domain contrastive module is introduced to optimize feature consistency and enhance cluster separability in both the feature domain and frequency domain. Additionally, an LLM-assisted view fusion mechanism provides a flexible fusion weight learning strategy that can be adaptively applied to complex scenarios and significantly different views. Extensive experimental results validate the effectiveness and superiority of the proposed method.",
      "arxiv_url": "https://openreview.net/forum?id=xgiMK8FtSI",
      "pdf_url": "https://openreview.net/pdf/d914ffb9e0367b3b16b179576b3f9c84161ec202.pdf",
      "primary_category": "Multi-View Clustering",
      "categories": [
        "Multi-View Clustering"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "p6Huickfj7",
      "title": "Bridging Sign and Spoken Languages: Pseudo Gloss Generation for Sign Language Translation",
      "authors": [
        "Jianyuan Guo",
        "Peike Li",
        "Trevor Cohn"
      ],
      "abstract": "Sign Language Translation (SLT) aims to map sign language videos to spoken language text. A common approach relies on gloss annotations as an intermediate representation, decomposing SLT into two sub-tasks: video-to-gloss recognition and gloss-to-text translation. While effective, this paradigm depends on expert-annotated gloss labels, which are costly and rarely available in existing datasets, limiting its scalability. To address this challenge, we propose a gloss-free pseudo gloss generation framework that eliminates the need for human-annotated glosses while preserving the structured intermediate representation.\nSpecifically, we prompt a Large Language Model (LLM) with a few example text-gloss pairs using in-context learning to produce draft sign glosses from spoken language text. \nTo enhance the correspondence between LLM-generated pseudo glosses and the sign sequences in video, we correct the ordering in the pseudo glosses for better alignment via a weakly supervised learning process.\nThis reordering facilitates the incorporation of auxiliary alignment objectives, and allows for the use of efficient supervision via a Connectionist Temporal Classification (CTC) loss.\nWe train our SLT model—consisting of a vision encoder and a translator—through a three-stage pipeline, which progressively narrows the modality gap between sign language and spoken language.\nDespite its simplicity, our approach outperforms previous state-of-the-art gloss-free frameworks on two SLT benchmarks and achieves competitive results compared to gloss-based methods.",
      "arxiv_url": "https://openreview.net/forum?id=p6Huickfj7",
      "pdf_url": "https://openreview.net/pdf/a4c88dd7070ac5d4f242e46a851384938fd7746d.pdf",
      "primary_category": "Sign Language Translation, Pseudo Gloss Generation, Large Language Model",
      "categories": [
        "Sign Language Translation",
        "Pseudo Gloss Generation",
        "Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "TcVCu2PKb9",
      "title": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility and Speedup",
      "authors": [
        "Fanxu Meng",
        "Pingzhi Tang",
        "Zengwei Yao",
        "Xing Sun",
        "Muhan Zhang"
      ],
      "abstract": "Modern large-language models often face communication bottlenecks on current hardware rather than computational limitations. \n*Multi-head latent attention (MLA)* addresses this by compressing the key-value cache using low-rank matrices, while the Absorb operation prevents the KV cache from reverting to its original size, significantly boosting both training and inference speed.\nDespite the success of DeepSeek V2/V3/R1, most model providers have heavily invested in optimizing GQA-based models and, therefore, lack strong incentives to retrain MLA-based models from scratch.\nThis paper demonstrates that MLA provides superior expressive power compared to GQA with the same KV cache overhead, thereby offering a rationale for transitioning from GQA to MLA.\nIn addition, we introduce TransMLA, a framework that seamlessly converts any GQA-based pre-trained model (e.g., LLaMA, Qwen, Gemma, Mistral/Mixtral) into an MLA-based model. \nFor the first time, our method enables *direct conversion of these models into a format compatible with DeepSeek's codebase*, allowing them to fully leverage the existing, highly-optimized support for the DeepSeek architecture within inference engines like vLLM and SGlang.\nBy compressing 93\\% of the KV cache in LLaMA-2-7B, we achieve a **10x speedup** with an 8K context length while maintaining meaningful output. \nMoreover, the model requires only **6B tokens** for fine-tuning to recover comparable performance across multiple benchmarks.\nTransMLA provides a practical path for migrating GQA-based models to the MLA structure, and when combined with DeepSeek’s advanced optimizations—such as FP8 quantization and Multi-Token Prediction—further inference acceleration can be achieved.",
      "arxiv_url": "https://openreview.net/forum?id=TcVCu2PKb9",
      "pdf_url": "https://openreview.net/pdf/15d1bbe6b7effe0eddb887b962c88b630785366a.pdf",
      "primary_category": "Deepseek, MLA, RoPE",
      "categories": [
        "Deepseek",
        "MLA",
        "RoPE",
        "PCA",
        "LLaMA",
        "Qwen",
        "GQA",
        "MHA"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QvV8oF08HA",
      "title": "ShiQ: Bringing back Bellman to LLMs",
      "authors": [
        "Pierre Clavier",
        "Nathan Grinsztajn",
        "Raphaël Avalos",
        "Yannis Flet-Berliac",
        "Irem Ergun",
        "Omar Darwiche Domingues",
        "Olivier Pietquin",
        "Pierre Harvey Richemond",
        "Florian Strub",
        "Matthieu Geist"
      ],
      "abstract": "The fine-tuning of pre-trained large language models (LLMs) using reinforcement learning (RL) is generally formulated as direct policy optimization. This approach was naturally favored as it efficiently improves a pretrained LLM with simple gradient updates. Another RL paradigm, Q-learning methods, has received far less attention in the LLM community while demonstrating major success in various non-LLM RL tasks. In particular, Q-learning effectiveness stems from its sample efficiency and ability to learn offline, which is particularly valuable given the high computational cost of sampling with LLM. However, naively applying a Q-learning–style update to the model’s logits is ineffective due to the specificity of LLMs. Our contribution is to derive theoretically grounded loss functions from Bellman equations to adapt Q-learning methods to LLMs. To do so, we interpret LLM logits as Q-values and carefully adapt insights from the RL literature to account for LLM-specific characteristics. It thereby ensures that the logits become reliable Q-value estimates. We then use this loss to build a practical algorithm, ShiQ for Shifted-Q, that supports off-policy, token-wise learning while remaining simple to implement. Finally,  ShiQ is evaluated on both synthetic data and real-world benchmarks, e.g., UltraFeedback, BFCL-V3, demonstrating its effectiveness in both single-turn and multi-turn LLM settings.",
      "arxiv_url": "https://openreview.net/forum?id=QvV8oF08HA",
      "pdf_url": "https://openreview.net/pdf/4e9542daf18fbf921c3dca3c23f2b110547a310d.pdf",
      "primary_category": "Reinforcement Learning with Human Feedbacks, Reinforcement Learning, Q-Learning",
      "categories": [
        "Reinforcement Learning with Human Feedbacks",
        "Reinforcement Learning",
        "Q-Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "CA1xVSvn72",
      "title": "Lua-LLM: Learning Unstructured-Sparsity Allocation for Large Language Models",
      "authors": [
        "Mingge Lu",
        "Jingwei Sun",
        "Junqing Lin",
        "Zechun Zhou",
        "Guangzhong Sun"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their extensive parameter scales pose significant challenges for practical deployment. Unstructured pruning has emerged as an effective model compression strategy with minimal performance loss, which introduces fine-grained sparsity for weight parameters. While existing methods employ a layer-wise pruning strategy to avoid the complexity of global pruning for billion-scale LLMs, they require appropriate sparsity allocation for the layer-wise pruning objectives and often lead to suboptimal solutions for the overall model. In this paper, we propose Lua-LLM ($\\textbf{L}$earning $\\textbf{u}$nstructured-sparsity $\\textbf{a}$llocation in LLMs), a learning-based global pruning framework that explores the optimal unstructured sparsity allocation. Unlike existing pruning methods, which primarily focus on allocating per-layer sparsity, Lua-LLM achieves flexible allocation for both layer-wise and intra-layer sparsity. Furthermore, Lua-LLM leverages a soft Top-K operator to approximate the importance-based mask selection mechanism, enabling efficient binary mask learning. Experimental results on LLaMA and OPT families demonstrate significant performance improvements over existing methods.",
      "arxiv_url": "https://openreview.net/forum?id=CA1xVSvn72",
      "pdf_url": "https://openreview.net/pdf/21279c0520792d04bed63335a2db748bcc57ae5d.pdf",
      "primary_category": "Sparsity, Pruning, Efficient Inference",
      "categories": [
        "Sparsity",
        "Pruning",
        "Efficient Inference",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2xS4VtpApy",
      "title": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models",
      "authors": [
        "Leqi Shen",
        "Guoqiang Gong",
        "Tao He",
        "Yifeng Zhang",
        "pengzhang liu",
        "Sicheng Zhao",
        "Guiguang Ding"
      ],
      "abstract": "Video Large Language Models have demonstrated strong video understanding capabilities, yet their practical deployment is hindered by substantial inference costs caused by redundant video tokens. \nExisting pruning techniques fail to effectively exploit the spatiotemporal redundancy present in video data. \nTo bridge this gap, we perform a systematic analysis of video redundancy from two perspectives: temporal context and visual context. \nLeveraging these insights, we propose Dynamic Density Pruning for Fast Video LLMs termed FastVID.\nSpecifically, FastVID dynamically partitions videos into temporally ordered segments to preserve temporal structure and applies a density-based token pruning strategy to maintain essential spatial and temporal information.\nOur method significantly reduces computational overhead while maintaining temporal and visual integrity. \nExtensive evaluations show that FastVID achieves state-of-the-art performance across various short- and long-video benchmarks on leading Video LLMs, including LLaVA-OneVision, LLaVA-Video, Qwen2-VL, and Qwen2.5-VL.\nNotably, on LLaVA-OneVision-7B, FastVID effectively prunes $\\textbf{90.3\\%}$ of video tokens, reduces FLOPs to $\\textbf{8.3\\%}$, and accelerates the LLM prefill stage by $\\textbf{7.1}\\times$, while maintaining $\\textbf{98.0\\%}$ of the original accuracy. \nThe code is available at https://github.com/LunarShen/FastVID.",
      "arxiv_url": "https://openreview.net/forum?id=2xS4VtpApy",
      "pdf_url": "https://openreview.net/pdf/b1d8ef881027fb67c7211f668832505fc7ace03f.pdf",
      "primary_category": "Video Large Language Models, Inference Acceleration",
      "categories": [
        "Video Large Language Models",
        "Inference Acceleration"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Fg9HufTI0K",
      "title": "CogVLA: Cognition-Aligned Vision-Language-Action Models via Instruction-Driven Routing & Sparsification",
      "authors": [
        "Wei Li",
        "Renshan Zhang",
        "Rui Shao",
        "Jie He",
        "Liqiang Nie"
      ],
      "abstract": "Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment. Existing sparsification strategies—such as Mixture-of-Depths, layer skipping, and early exit—fall short by neglecting the semantic coupling across vision-language-action modalities, and focusing narrowly on intra-LLM computation while overlooking end-to-end coherence from perception to control. To address these challenges, we propose **CogVLA**, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) **Encoder-FiLM based Aggregation Routing (EFA-Routing)** injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, **LLM-FiLM based Pruning Routing (LFP-Routing)** introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce **V‑L‑A Coupled Attention (CAtten)**, which combines causal vision-language attention with bidirectional action parallel decoding.\nExtensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4\\% and 70.0\\%, respectively, while reducing training costs by 2.5$\\times$ and decreasing inference latency by 2.8$\\times$ compared to OpenVLA.",
      "arxiv_url": "https://openreview.net/forum?id=Fg9HufTI0K",
      "pdf_url": "https://openreview.net/pdf/e67f52c1abdf244a6b56b29468a234e1c2b8d202.pdf",
      "primary_category": "Vision Language Action models; Instruction-Driven; Vision-Language Models; Vision Sparsification",
      "categories": [
        "Vision Language Action models; Instruction-Driven; Vision-Language Models; Vision Sparsification"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VZQSrNfNHd",
      "title": "RobotSmith: Generative Robotic Tool Design for Acquisition of Complex Manipulation Skills",
      "authors": [
        "Chunru Lin",
        "Haotian Yuan",
        "Yian Wang",
        "Xiaowen Qiu",
        "Tsun-Hsuan Wang",
        "Minghao Guo",
        "Bohan Wang",
        "Yashraj Narang",
        "Dieter Fox",
        "Chuang Gan"
      ],
      "abstract": "Endowing robots with tool design abilities is critical for enabling them to solve complex manipulation tasks that would otherwise be intractable. While recent generative frameworks can automatically synthesize task settings—such as 3D scenes and reward functions—they have not yet addressed the challenge of tool-use scenarios. Simply retrieving human-designed tools might not be ideal since many tools (e.g., a rolling pin) are difficult for robotic manipulators to handle. Furthermore, existing tool design approaches either rely on predefined templates with limited parameter tuning or apply generic 3D generation methods that are not optimized for tool creation.\nTo address these limitations, we propose **RobotSmith**, an automated pipeline that leverages the implicit physical knowledge embedded in vision-language models (VLMs) alongside the more accurate physics provided by physics simulations to design and use tools for robotic manipulation. Our system (1) iteratively proposes tool designs using collaborative VLM agents, (2) generates low-level robot trajectories for tool use, and (3) jointly optimizes tool geometry and usage for task performance.\nWe evaluate our approach across a wide range of manipulation tasks involving rigid, deformable, and fluid objects. Experiments show that our method consistently outperforms strong baselines in both task success rate and overall performance. Notably, our approach achieves a 50.0\\% average success rate, significantly surpassing other baselines such as 3D generation (21.4\\%) and tool retrieval (11.1\\%). Finally, we deploy our system in real-world settings, demonstrating that the generated tools and their usage plans transfer effectively to physical execution, validating the practicality and generalization capabilities of our approach.",
      "arxiv_url": "https://openreview.net/forum?id=VZQSrNfNHd",
      "pdf_url": "https://openreview.net/pdf/7b0e2eda782d65daf3f605c4086111c4502ce051.pdf",
      "primary_category": "Robotic Tool Design, Robotic Tool Use, Soft Material Manipulation",
      "categories": [
        "Robotic Tool Design",
        "Robotic Tool Use",
        "Soft Material Manipulation",
        "Generative Models"
      ],
      "tags": [
        "Multi-Modal RAG",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "feAzLLT9to",
      "title": "Edit Less, Achieve More: Dynamic Sparse Neuron Masking for Lifelong Knowledge Editing in LLMs",
      "authors": [
        "Jinzhe Liu",
        "Junshu Sun",
        "Shufan Shen",
        "Chenxue Yang",
        "Shuhui Wang"
      ],
      "abstract": "Lifelong knowledge editing enables continuous, precise updates to outdated knowledge in large language models (LLMs) without computationally expensive full retraining. However, existing methods often accumulate errors throughout the editing process, causing a gradual decline in both editing accuracy and generalization.  To tackle this problem, we propose Neuron-Specific Masked Knowledge Editing (NMKE), a novel fine-grained editing framework that combines neuron-level attribution with dynamic sparse masking. \nLeveraging neuron functional attribution, we identify two key types of knowledge neurons, with knowledge-general neurons activating consistently across prompts and knowledge-specific neurons activating to specific prompts.\nNMKE further introduces an entropy-guided dynamic sparse mask, locating relevant neurons to the target knowledge. This strategy enables precise neuron-level knowledge editing with fewer parameter modifications.\nExperimental results from thousands of sequential edits demonstrate that NMKE outperforms existing methods in maintaining high editing success rates and preserving model general capabilities in lifelong editing.",
      "arxiv_url": "https://openreview.net/forum?id=feAzLLT9to",
      "pdf_url": "https://openreview.net/pdf/958568a24ea48cacbdd42070c8d13ea592f7acbc.pdf",
      "primary_category": "Lifelong model editing, fine-grained editing, knowledge editing",
      "categories": [
        "Lifelong model editing",
        "fine-grained editing",
        "knowledge editing",
        "large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "um4aiicz3L",
      "title": "Mellow: a small audio language model for reasoning",
      "authors": [
        "Soham Deshmukh",
        "Satvik Dixit",
        "Rita Singh",
        "Bhiksha Raj"
      ],
      "abstract": "Multimodal Audio-Language Models (ALMs) can understand and reason over both audio and text. Typically, reasoning performance correlates with model size, with the best results achieved by models exceeding 8 billion parameters. However, no prior work has explored enabling small audio-language models to perform reasoning tasks, despite the potential applications for edge devices. To address this gap, we introduce Mellow, a small Audio-Language Model specifically designed for reasoning. Mellow achieves state-of-the-art performance among existing small audio-language models and surpasses several larger models in reasoning capabilities. For instance, Mellow scores 52.11 on MMAU, comparable to SoTA Qwen2 Audio (which scores 52.5) while using 50 times fewer parameters and being trained on 60 times less data (audio hrs). To train Mellow, we introduce ReasonAQA, a dataset designed to enhance audio-grounded reasoning in models. It consists of a mixture of existing datasets (30\\% of the data) and synthetically generated data (70\\%). The synthetic dataset is derived from audio captioning datasets, where Large Language Models (LLMs) generate detailed and multiple-choice questions focusing on audio events, objects, acoustic scenes, signal properties, semantics, and listener emotions. To evaluate Mellow’s reasoning ability, we benchmark it on a diverse set of tasks, assessing on both in-distribution and out-of-distribution data, including audio understanding, deductive reasoning, and comparative reasoning. Finally, we conduct extensive ablation studies to explore the impact of projection layer choices, synthetic data generation methods, and language model pretraining on reasoning performance. Our training dataset, findings, and baseline pave the way for developing small ALMs capable of reasoning.",
      "arxiv_url": "https://openreview.net/forum?id=um4aiicz3L",
      "pdf_url": "https://openreview.net/pdf/988b1f67467c24e2ca4afd17d8656a84d73fad4d.pdf",
      "primary_category": "audio-language, multimodal learning, audio reasoning",
      "categories": [
        "audio-language",
        "multimodal learning",
        "audio reasoning",
        "audio processing"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "bulTwq5kNK",
      "title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs",
      "authors": [
        "Jie Ma",
        "Ning Qu",
        "Zhitao Gao",
        "Xing Rui",
        "Jun Liu",
        "Hongbin Pei",
        "Jiang Xie",
        "Lingyun Song",
        "Pinghui Wang",
        "Jing Tao",
        "su zhou"
      ],
      "abstract": "Knowledge graph-based retrieval-augmented generation seeks to mitigate hallucinations in Large Language Models (LLMs) caused by insufficient or outdated knowledge. However, existing methods often fail to fully exploit the prior knowledge embedded in knowledge graphs (KGs), particularly their structural information and explicit or implicit constraints. The former can enhance the faithfulness of LLMs' reasoning, while the latter can improve the reliability of response generations. Motivated by these, we propose a trustworthy reasoning framework, termed Deliberation over Priors (\\texttt{DP}), which sufficiently utilizes the priors contained in KGs. Specifically, \\texttt{DP} adopts a progressive knowledge distillation strategy that integrates structural priors into LLMs through a combination of supervised fine-tuning and Kahneman-Tversky Optimization, thereby improving the faithfulness of relation path generation. Furthermore, our framework employs a reasoning-introspection strategy, which guides LLMs to perform refined reasoning verification based on extracted constraint priors, ensuring the reliability of response generation. Extensive experiments on three benchmark datasets demonstrate that \\texttt{DP} achieves new state-of-the-art performance, especially a H@1 improvement of 13% on the ComplexWebQuestions dataset, and generates highly trustworthy responses. We also conduct various analyses to verify its flexibility and practicality. Code is available at [https://github.com/mira-ai-lab/Deliberation-on-Priors](https://github.com/mira-ai-lab/Deliberation-on-Priors).",
      "arxiv_url": "https://openreview.net/forum?id=bulTwq5kNK",
      "pdf_url": "https://openreview.net/pdf/480064ff42f3bc89e87acc783f4c349101c98dbf.pdf",
      "primary_category": "Knowledge Graph-based Retrieval Augmentation, Knowledge Graph Question Answering, Large Language Models",
      "categories": [
        "Knowledge Graph-based Retrieval Augmentation",
        "Knowledge Graph Question Answering",
        "Large Language Models"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "btZm6DUaDO",
      "title": "Quadratic Coreset Selection: Certifying and Reconciling Sequence and Token Mining for Efficient Instruction Tuning",
      "authors": [
        "Ziliang Chen",
        "Yongsen Zheng",
        "Zhao-Rong Lai",
        "Zhanfu Yang",
        "Cuixi Li",
        "Yang Liu",
        "Liang Lin"
      ],
      "abstract": "Instruction-Tuning (IT) was recently found the impressive data efficiency in post-training large language models (LLMs). While the pursuit of efficiency predominantly focuses on sequence-level curation, often overlooking the nuanced impact of critical tokens and the inherent risks of token noise and biases. Drawing inspiration from bi-level coreset selection, our work provides the principled view of the motivation behind selecting instructions' responses. It leads to our approach Quadratic Coreset Selection (QCS) that reconciles sequence-level and token-level influence contributions, deriving more expressive LLMs with established theoretical result. Despite the original QCS framework challenged by prohibitive computation from inverted LLM-scale Hessian matrices, we overcome this barrier by proposing a novel QCS probabilistic variant, which relaxes the original formulation through re-parameterized densities. This innovative solver is efficiently learned using hierarchical policy gradients without requiring back-propagation, achieving provable convergence and certified asymptotic equivalence to the original objective. Our experiments demonstrate QCS's superior sequence-level data efficiency and reveal how strategically leveraging token-level influence elevates the performance ceiling of data-efficient IT. Furthermore, QCS's adaptability is showcased through its successes in regular IT and challenging targeted IT scenarios, particularly in the cases of free-form complex instruction-following and CoT reasoning. They underscore QCS's potential for a wide array of versatile post-training applications.",
      "arxiv_url": "https://openreview.net/forum?id=btZm6DUaDO",
      "pdf_url": "https://openreview.net/pdf/e0981475b750f425d4aa6fe7e964c41efb95e484.pdf",
      "primary_category": "instruction tuning;principled data selection for LLM",
      "categories": [
        "instruction tuning;principled data selection for LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "gA3fFAEXNT",
      "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards",
      "authors": [
        "Xiaoyuan Liu",
        "Tian Liang",
        "Zhiwei He",
        "Jiahao Xu",
        "Wenxuan Wang",
        "Pinjia He",
        "Zhaopeng Tu",
        "Haitao Mi",
        "Dong Yu"
      ],
      "abstract": "Large Language Models (LLMs) show great promise in complex reasoning, with Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement strategy. However, a prevalent issue is ``superficial self-reflection'', where models fail to robustly verify their own outputs. We introduce RISE (Reinforcing Reasoning with Self-Verification), a novel online RL framework designed to tackle this. RISE explicitly and simultaneously trains an LLM to improve both its problem-solving and self-verification abilities within a single, integrated RL process. The core mechanism involves leveraging verifiable rewards from an outcome verifier to provide on-the-fly feedback for both solution generation and self-verification tasks. In each iteration, the model generates solutions, then critiques its own on-policy generated solutions, with both trajectories contributing to the policy update. \nExtensive experiments on diverse mathematical reasoning benchmarks show that RISE consistently improves model's problem-solving accuracy while concurrently fostering strong self-verification skills. Our analyses highlight the advantages of online verification and the benefits of increased verification compute. Additionally, RISE models exhibit more frequent and accurate self-verification behaviors during reasoning. These advantages reinforce RISE as a flexible and effective path towards developing more robust and self-aware reasoners.",
      "arxiv_url": "https://openreview.net/forum?id=gA3fFAEXNT",
      "pdf_url": "https://openreview.net/pdf/ab9fa2d91b7a48918725f69e5c5a51085f658bf2.pdf",
      "primary_category": "LLM Reasoning, RLVR, Self-verification",
      "categories": [
        "LLM Reasoning",
        "RLVR",
        "Self-verification"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6tOKqqiyWE",
      "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning",
      "authors": [
        "Xiangning Yu",
        "Zhuohan Wang",
        "Linyi Yang",
        "Haoxuan Li",
        "Anjie Liu",
        "Xiao Xue",
        "Jun Wang",
        "Mengyue Yang"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing large language models (LLMs) with complex reasoning capabilities. However, CoT currently faces two fundamental challenges: (1) Sufficiency, which ensures that the generated intermediate inference steps comprehensively cover and substantiate the final conclusion; and (2) Necessity, which identifies the inference steps that are truly indispensable for the soundness of the resulting answer. We propose a causal framework that characterizes CoT reasoning through the dual lenses of sufficiency and necessity. Incorporating causal Probability of Sufficiency and Necessity allows us not only to determine which steps are logically sufficient or necessary to the prediction outcome, but also to quantify their actual influence on the final reasoning outcome under different intervention scenarios, thereby enabling the automated addition of missing steps and the pruning of redundant ones. Extensive experimental results on various mathematical and commonsense reasoning benchmarks confirm substantial improvements in reasoning efficiency and reduced token usage without sacrificing accuracy. Our work provides a promising direction for improving LLM reasoning performance and cost-effectiveness. The code will be publicly available upon acceptance at: https://anonymous.4open.science/r/causalmath-1CEF.",
      "arxiv_url": "https://openreview.net/forum?id=6tOKqqiyWE",
      "pdf_url": "https://openreview.net/pdf/722d843c0b72fca7838e93dad027bbb061500956.pdf",
      "primary_category": "PNS, Chain-of-Thought(CoT), Causal Suffciency Necessity",
      "categories": [
        "PNS",
        "Chain-of-Thought(CoT)",
        "Causal Suffciency Necessity"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "XPLf9H27aO",
      "title": "JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent",
      "authors": [
        "Yunlong Lin",
        "ZiXu Lin",
        "Kunjie Lin",
        "Jinbin Bai",
        "Panwang Pan",
        "Chenxin Li",
        "Haoyu Chen",
        "Zhongdao Wang",
        "Xinghao Ding",
        "Wenbo Li",
        "Shuicheng YAN"
      ],
      "abstract": "Photo retouching has become integral to contemporary visual storytelling, enabling users to capture aesthetics and express creativity. While professional tools such as Adobe Lightroom offer powerful capabilities, they demand substantial expertise and manual effort. In contrast, existing AI-based solutions provide automation but often suffer from limited adjustability and poor generalization, failing to meet diverse and personalized editing needs. To bridge this gap, we introduce JarvisArt, a multi-modal large language model (MLLM)-driven agent that understands user intent, mimics the reasoning process of professional artists, and intelligently coordinates over 200 retouching tools within Lightroom. JarvisArt undergoes a two-stage training process: an initial Chain-of-Thought supervised fine-tuning to establish basic reasoning and tool-use skills, followed by Group Relative Policy Optimization for Retouching (GRPO-R) to further enhance its decision-making and tool proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate seamless integration with Lightroom. To evaluate performance, we develop MMArt-Bench, a novel benchmark constructed from real-world user edits. JarvisArt demonstrates user-friendly interaction, superior generalization, and fine-grained control over both global and local adjustments, paving a new avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a 60\\% improvement in average pixel-level metrics on MMArt-Bench for content fidelity, while maintaining comparable instruction-following capabilities.",
      "arxiv_url": "https://openreview.net/forum?id=XPLf9H27aO",
      "pdf_url": "https://openreview.net/pdf/24cb8b7affa9be90e1900da83592fd3256640ece.pdf",
      "primary_category": "AI agent; Photo retouching; Vision-foundation model",
      "categories": [
        "AI agent; Photo retouching; Vision-foundation model"
      ],
      "tags": [
        "LLM",
        "Personalization",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "lMU2kaMANl",
      "title": "Don't be lazy: CompleteP enables compute-efficient deep transformers",
      "authors": [
        "Nolan Simran Dey",
        "Bin Claire Zhang",
        "Lorenzo Noci",
        "Mufan Li",
        "Blake Bordelon",
        "Shane Bergsma",
        "Cengiz Pehlevan",
        "Boris Hanin",
        "Joel Hestness"
      ],
      "abstract": "We study compute efficiency of LLM training when using different parameterizations, i.e., rules for adjusting model and optimizer hyperparameters (HPs) as model size changes. Some parameterizations fail to transfer optimal base HPs (such as learning rate) across changes in model depth, requiring practitioners to either re-tune these HPs as they scale up (expensive), or accept sub-optimal training when re-tuning is prohibitive. Even when they achieve HP transfer, we develop theory to show parameterizations may still exist in the lazy learning regime where layers learn only features close to their linearization, preventing effective use of depth and nonlinearity. Finally, we identify and adopt the parameterization we call CompleteP that achieves both depth-wise HP transfer and non-lazy learning in all layers. CompleteP enables a wider range of model width/depth ratios to remain compute-efficient, unlocking shapes better suited for different hardware settings and operational contexts. Moreover, CompleteP enables 12-34% compute efficiency improvements over the prior state-of-the-art. All experiments were run on Cerebras CS-3 systems. A minimal implementation is available at https://github.com/EleutherAI/nanoGPT-mup/tree/completep.",
      "arxiv_url": "https://openreview.net/forum?id=lMU2kaMANl",
      "pdf_url": "https://openreview.net/pdf/03b15e4cad95e4cade4da13ce42d6d1ee85d2f20.pdf",
      "primary_category": "Deep learning, depth, transformer",
      "categories": [
        "Deep learning",
        "depth",
        "transformer",
        "LLM",
        "compute-efficiency",
        "parameterization",
        "stability",
        "hyperparameter transfer"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "quY3zRPalR",
      "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing",
      "authors": [
        "Jaihoon Kim",
        "TaeHoon Yoon",
        "Jisung Hwang",
        "Minhyuk Sung"
      ],
      "abstract": "We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation and variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches.",
      "arxiv_url": "https://openreview.net/forum?id=quY3zRPalR",
      "pdf_url": "https://openreview.net/pdf/b9c90ba14419aae1848687ef5d40470beb1b8055.pdf",
      "primary_category": "Inference-time scaling, flow model, reward alignment",
      "categories": [
        "Inference-time scaling",
        "flow model",
        "reward alignment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pEUBqS8nTk",
      "title": "Towards Physics-informed Spatial Intelligence with Human Priors: An Autonomous Driving Pilot Study",
      "authors": [
        "Guanlin Wu",
        "Boyan Su",
        "Yang Zhao",
        "Pu Wang",
        "Yichen Lin",
        "Hao Frank Yang"
      ],
      "abstract": "How to integrate and verify spatial intelligence in foundation models remains an open challenge. Current practice often proxies Visual-Spatial Intelligence (VSI) with purely textual prompts and VQA-style scoring, which obscures geometry, invites linguistic shortcuts, and weakens attribution to genuinely spatial skills. We introduce Spatial Intelligence Grid (SIG): a structured, grid-based schema that explicitly encodes object layouts, inter-object relations, and physically grounded priors. As a complementary channel to text, SIG provides a faithful, compositional representation of scene structure for foundation-model reasoning. Building on SIG, we derive SIG-informed evaluation metrics that quantify a model’s intrinsic VSI, which separates spatial capability from language priors. In few-shot in-context learning with state-of-the-art multimodal LLMs (e.g. GPT- and Gemini-family models), SIG yields consistently larger, more stable, and more comprehensive gains across all VSI metrics compared to VQA-only representations, indicating its promise as a data-labeling and training schema for learning VSI. We also release SIGBench, a benchmark of 1.4K driving frames annotated with ground-truth SIG labels and human gaze traces, supporting both grid-based machine VSI tasks and attention-driven, human-like VSI tasks in autonomous-driving scenarios.",
      "arxiv_url": "https://openreview.net/forum?id=pEUBqS8nTk",
      "pdf_url": "https://openreview.net/pdf/c0d96ddfdbb8839ba9e22328928c92be2372f562.pdf",
      "primary_category": "Multimodal LLM, VLM, Spatial Intelligence",
      "categories": [
        "Multimodal LLM",
        "VLM",
        "Spatial Intelligence",
        "Autonomous Driving"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4n7IifN7yr",
      "title": "LiveStar: Live Streaming Assistant for Real-World Online Video Understanding",
      "authors": [
        "Zhenyu Yang",
        "Kairui Zhang",
        "Yuhang Hu",
        "Bing Wang",
        "Shengsheng Qian",
        "Bin Wen",
        "Fan Yang",
        "Tingting Gao",
        "Weiming Dong",
        "Changsheng Xu"
      ],
      "abstract": "Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53× faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5\\% improvement in semantic correctness with 18.1\\% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0\\% across all five OmniStar tasks. Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.",
      "arxiv_url": "https://openreview.net/forum?id=4n7IifN7yr",
      "pdf_url": "https://openreview.net/pdf/d5d3569553a33d7568c9cba9ab9ba6de8fbb66ba.pdf",
      "primary_category": "Multimodal Large Language Model, Online Video Understanding, Streaming Video Understanding",
      "categories": [
        "Multimodal Large Language Model",
        "Online Video Understanding",
        "Streaming Video Understanding"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QclFsekj9B",
      "title": "X-Scene: Large-Scale Driving Scene Generation with High Fidelity and Flexible Controllability",
      "authors": [
        "Yu Yang",
        "Alan Liang",
        "Jianbiao Mei",
        "Yukai Ma",
        "Yong Liu",
        "Gim Hee Lee"
      ],
      "abstract": "Diffusion models are advancing autonomous driving by enabling realistic data synthesis, predictive end-to-end planning, and closed-loop simulation, with a primary focus on temporally consistent generation. However, large-scale 3D scene generation requiring spatial coherence remains underexplored. In this paper, we present X-Scene, a novel framework for large-scale driving scene generation that achieves geometric intricacy, appearance fidelity, and flexible controllability. Specifically, X-Scene supports multi-granular control, including low-level layout conditioning driven by user input or text for detailed scene composition, and high-level semantic guidance informed by user intent and LLM-enriched prompts for efficient customization. To enhance geometric and visual fidelity, we introduce a unified pipeline that sequentially generates 3D semantic occupancy and corresponding multi-view images and videos, ensuring alignment and temporal consistency across modalities. We further extend local regions into large-scale scenes via consistency-aware outpainting, which extrapolates occupancy and images from previously generated areas to maintain spatial and visual coherence. The resulting scenes are lifted into high-quality 3DGS representations, supporting diverse applications such as simulation and scene exploration. Extensive experiments demonstrate that X-Scene substantially advances controllability and fidelity in large-scale scene generation, empowering data generation and simulation for autonomous driving.",
      "arxiv_url": "https://openreview.net/forum?id=QclFsekj9B",
      "pdf_url": "https://openreview.net/pdf/65d76183b65a7b09ddfddb344a4153a37ff58b84.pdf",
      "primary_category": "3D Scene Generation, Autonomous Driving",
      "categories": [
        "3D Scene Generation",
        "Autonomous Driving"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "icoV59tH6D",
      "title": "Vid-SME: Membership Inference Attacks against Large Video Understanding Models",
      "authors": [
        "Qi Li",
        "Runpeng Yu",
        "Xinchao Wang"
      ],
      "abstract": "Multimodal large language models (MLLMs) demonstrates remarkable capabilities in handling complex multimodal tasks and are increasingly adopted in video understanding applications. However, their rapid advancement raises serious data privacy concerns, particularly given the potential inclusion of sensitive video content, such as personal recordings and surveillance footage, in their training datasets. Determining improperly used videos during training remains a critical and unresolved challenge. Despite considerable progress on membership inference attacks (MIAs) for text and image data in MLLMs, existing methods fail to generalize effectively to the video domain. These methods suffer from poor scalability as more frames are sampled and generally achieve negligible true positive rates at low false positive rates (TPR@Low FPR), mainly due to their failure to capture the inherent temporal variations of video frames and to account for model behavior differences as the number of frames varies. To address these challenges, we introduce Vid-SME (**Vid**eo **S**harma–**M**ittal **E**ntropy), the first membership inference method tailored for video data used in video understanding LLMs (VULLMs). Vid-SME leverages the confidence of model output and integrates adaptive parameterization to compute Sharma–Mittal entropy (SME) for video inputs. By leveraging the SME difference between natural and temporally-reversed video frames, Vid-SME derives robust membership scores to determine whether a given video is part of the model's training set. Experiments on various self-trained and open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME.",
      "arxiv_url": "https://openreview.net/forum?id=icoV59tH6D",
      "pdf_url": "https://openreview.net/pdf/351a652e0b131312b9e2114948ba732a77dae982.pdf",
      "primary_category": "Membership Inference, Video-Language Models",
      "categories": [
        "Membership Inference",
        "Video-Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "de07K7kreI",
      "title": "LoRO: Real-Time on-Device Secure Inference for LLMs via TEE-Based Low Rank Obfuscation",
      "authors": [
        "Gaojian Xiong",
        "Yu Sun",
        "Jianhua Liu",
        "Jian Cui",
        "Jianwei Liu"
      ],
      "abstract": "While Large Language Models (LLMs) have gained remarkable success, they are consistently at risk of being stolen when deployed on untrusted edge devices. As a solution, TEE-based secure inference has been proposed to protect valuable model property. However, we identify a statistical vulnerability in existing protection methods, and furtherly compromise their security guarantees by proposed Model Stealing Attack with Prior. To eliminate this vulnerability, LoRO is presented in this paper, which leverages dense mask to completely obfuscate parameters. LoRO includes two innovations: (1) Low Rank Mask, which uses low-rank factors to generate dense masks efficiently. The computing complexity in TEE is hence reduced by an exponential amount to achieve inference speed up, while providing robust model confidentiality. (2) Factors Multiplexing, which reuses several cornerstone factors to generate masks for all layers. Compared to one-mask-per-layer, the secure memory requirement is reduced from GB-level to tens of MB, hence avoiding the hundred-fold latency introduced by secure memory paging. Experimental results indicate that LoRO achieve a $0.94\\times$ Model Stealing (MS) accuracy, while SOTA methods presents $3.37\\times$ at least. The averaged inference latency of LoRO is only $1.49\\times$, compared to the $112\\times$ of TEE-shielded inference. Moreover, LoRO results no accuracy loss, and requires no re-training and structure modification. LoRO can solve the concerns regarding model thefts on edge devices in an efficient and secure manner, facilitating the wide edge application of LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=de07K7kreI",
      "pdf_url": "https://openreview.net/pdf/e5c043507afdf239b94e805b11193b7df5d98947.pdf",
      "primary_category": "secure inference, large language models, trusted execution environments",
      "categories": [
        "secure inference",
        "large language models",
        "trusted execution environments",
        "intellectual property protection"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "WBEknRZBpT",
      "title": "Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling",
      "authors": [
        "Hongtao Xu",
        "Wenting Shen",
        "Yuanxin Wei",
        "Ang Wang",
        "Guo Runfan",
        "Tianxing Wang",
        "Yong Li",
        "Mingzhen Li",
        "Weile Jia"
      ],
      "abstract": "Long-context supervised fine-tuning (Long-SFT) plays a vital role in enhancing the performance of large language models (LLMs) on long-context tasks. To smoothly adapt LLMs to long-context scenarios, this process typically entails training on mixed datasets containing both long and short sequences. However, this heterogeneous sequence length distribution poses significant challenges for existing training systems, as they fail to simultaneously achieve high training efficiency for both long and short sequences, resulting in sub-optimal end-to-end system performance in Long-SFT.\nIn this paper, we present a novel perspective on data scheduling to address the challenges posed by the heterogeneous data distributions in Long-SFT. We propose Skrull, a dynamic data scheduler specifically designed for efficient long-SFT. Through dynamic data scheduling, Skrull balances the computation requirements of long and short sequences, improving overall training efficiency. Furthermore, we formulate the scheduling process as a joint optimization problem and thoroughly analyze the trade-offs involved. Based on those analysis, Skrull employs a lightweight scheduling algorithm to achieve near-zero cost online scheduling in Long-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art distributed training system for LLMs. Experimental results demonstrate that Skrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world long-SFT scenarios.",
      "arxiv_url": "https://openreview.net/forum?id=WBEknRZBpT",
      "pdf_url": "https://openreview.net/pdf/d0d27fc5cca267d961112ed3a82c158ad8a868a4.pdf",
      "primary_category": "long context fine-tuning, context parallelism, supervised fine-tuning",
      "categories": [
        "long context fine-tuning",
        "context parallelism",
        "supervised fine-tuning",
        "LLM",
        "Large language model"
      ],
      "tags": [
        "LLM",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3Sxby0hH1q",
      "title": "Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning",
      "authors": [
        "Jie Cheng",
        "Gang Xiong",
        "Ruixi Qiao",
        "Lijun Li",
        "Chao Guo",
        "Junle Wang",
        "Yisheng Lv",
        "Fei-Yue Wang"
      ],
      "abstract": "Process reward model (PRM) has been proven effective in test-time scaling of LLM on challenging reasoning tasks. However, the reward hacking induced by PRM hinders its successful applications in reinforcement fine-tuning. We find the primary cause of reward hacking induced by PRM is that: the canonical summation-form credit assignment in reinforcement learning (RL), i.e. cumulative gamma-decayed future rewards, causes the LLM to hack steps with high rewards. Therefore, to unleashing the power of PRM in training-time, we propose PURE: Process sUpervised Reinforcement lEarning. The core of PURE is the min-form credit assignment that defines the value function as the minimum future rewards. This method unifies the optimization objective with respect to process rewards during test-time and training-time, and significantly alleviates reward hacking due to the limits on the range of values of value function and more rational assignment of advantages. Through extensively experiments on 3 base models, we achieve similar reasoning performance using PRM-based approach compared with verifiable reward-based approach if enabling min-form credit assignment. In contrast, the canonical sum-form credit assignment even collapses training at the beginning. Moreover, when we incorporate 1/10th verifiable rewards to auxiliary the PRM-based fine-tuning, it further alleviate reward hacking and results in the best fine-tuned model based on Qwen2.5-Math-7B with 82.5% accuracy on AMC23 and 53.3% average accuracy across 5 benchmarks. Furthermore, we summary the reward hacking cases we encountered during training and analysis the cause of training collapse.",
      "arxiv_url": "https://openreview.net/forum?id=3Sxby0hH1q",
      "pdf_url": "https://openreview.net/pdf/d2d109a39fc4db093c90915d681c4eeaf56313dc.pdf",
      "primary_category": "LLM, reasoning, process reward model",
      "categories": [
        "LLM",
        "reasoning",
        "process reward model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "k8Mim6RI5O",
      "title": "Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization",
      "authors": [
        "Qingyang Zhang",
        "Haitao Wu",
        "Changqing Zhang",
        "Peilin Zhao",
        "Yatao Bian"
      ],
      "abstract": "Existing methods to enhance the reasoning capability of large language models predominantly rely on supervised fine-tuning (SFT) followed by reinforcement learning (RL) on reasoning-specific data. These approaches critically depend on external supervisions--such as labeled reasoning traces, verified golden answers, or pre-trained reward models. In this work, we propose Entropy Minimized Policy Optimization (EMPO), which makes an early attempt at fully unsupervised LLM reasoning incentivization. By minimizing the semantic entropy of LLMs on unlabeled questions, EMPO achieves competitive performance compared to supervised counterparts. Specifically, without any supervised signals, EMPO boosts the accuracy of Qwen2.5-Math-7B Base from 33.7\\% to 51.6\\% on math benchmarks and improves the accuracy of Qwen2.5-7B Base from 32.1\\% to 50.1\\% on MMLU-Pro. Primary analysis are also provided to interpret the effectiveness of EMPO. Code is available at https://github.com/QingyangZhang/EMPO.",
      "arxiv_url": "https://openreview.net/forum?id=k8Mim6RI5O",
      "pdf_url": "https://openreview.net/pdf/6f471db2f6eab60edb0c603a69b5506d88b0bf37.pdf",
      "primary_category": "LLM Reasoning, Reinforcement Learning, Unsupervised Learning",
      "categories": [
        "LLM Reasoning",
        "Reinforcement Learning",
        "Unsupervised Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7JjS2cdBYN",
      "title": "LOMIA: Label-Only Membership Inference Attacks against Pre-trained Large Vision-Language Models",
      "authors": [
        "Yihao LIU",
        "Xinqi LYU",
        "Dong Wang",
        "Yanjie Li",
        "Bin Xiao"
      ],
      "abstract": "Large vision-language models (VLLMs) have driven significant progress in multi-modal systems, enabling a wide range of applications across domains such as healthcare, education, and content generation. Despite the success, the large-scale datasets used to train these models often contain sensitive or personally identifiable information, raising serious privacy concerns. To audit and better understand such risks, membership inference attacks (MIAs) have become a key tool. However, existing MIAs against VLLMs predominantly assume access to full-model logits, which are typically unavailable in many practical deployments. To facilitate MIAs in a more realistic and restrictive setting, we propose a novel framework: label-only membership inference attacks (LOMIA) targeting pre-trained VLLMs where only the model’s top-1 prediction is available. Within this framework, we propose three effective attack methods, all of which exploit the intuition that training samples are more likely to be memorized by the VLLMs, resulting in outputs that exhibit higher semantic alignment and lower perplexity. Our experiments show that our framework surpasses existing label-only attack adaptations for different VLLMs and competes with state-of-the-art logits-based attacks across all metrics on three widely used open-source VLLMs and GPT-4o.",
      "arxiv_url": "https://openreview.net/forum?id=7JjS2cdBYN",
      "pdf_url": "https://openreview.net/pdf/a1e7aad861207def2eceb129a98eb61107e822d2.pdf",
      "primary_category": "Label-Only Membership Inference Attack; Large Vision-Language Models",
      "categories": [
        "Label-Only Membership Inference Attack; Large Vision-Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "FDAI0PY9Qp",
      "title": "AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented Efficient Long Video Understanding",
      "authors": [
        "Zhucun Xue",
        "Jiangning Zhang",
        "Xurong Xie",
        "yuxuan cai",
        "Yong Liu",
        "Xiangtai Li",
        "Dacheng Tao"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated excellent performance in video understanding but suffer from degraded effectiveness when processing long videos due to fixed-length contexts and weaknesses in modeling long-term dependencies. Retrieval-Augmented Generation (RAG) technology can mitigate these limitations through dynamic knowledge expansion, but existing RAG schemes for video understanding employ fixed retrieval paradigms that use uniform structures regardless of input query difficulty. This introduces redundant computational overhead and latency (*e.g.*, complex graph traversal operations) for simple queries (*e.g.*, frame-level object recognition) while potentially causing critical information loss due to insufficient retrieval granularity for multi-hop reasoning. Such single-step retrieval mechanisms severely constrain the model's balance between resource efficiency and cognitive depth. \nTo address this, we first propose a novel AdaVideoRAG framework for long-video understanding, which uses a lightweight intent classifier to dynamically and adaptively allocate appropriate retrieval schemes, ranging from the simplest to the most sophisticated, for different video understanding tasks based on query complexity. We introduce an Omni-Knowledge Indexing module to extract valuable information from multi-modal signals for context modeling and build corresponding databases, *i.e.*, a text base from clip captions, ASR, and OCR; a visual base; and a graph for deep semantic understanding. This enables hierarchical knowledge access, integration, and generation from naive retrieval to graph retrieval, achieving an optimal balance between resource consumption and video understanding capabilities.  \nFinally, we construct the HiVU benchmark for deep understanding evaluation. Extensive experiments show that our framework enhances the overall efficiency and accuracy of Video-QA for long videos and can be seamlessly integrated with existing MLLMs via lightweight API calls, establishing a new paradigm for adaptive retrieval augmentation in video analysis.",
      "arxiv_url": "https://openreview.net/forum?id=FDAI0PY9Qp",
      "pdf_url": "https://openreview.net/pdf/9bc7379fdd250fabf83c21b5f4eccf9c62d83552.pdf",
      "primary_category": "Video Understanding, Multimodal Large Language Models, RAG",
      "categories": [
        "Video Understanding",
        "Multimodal Large Language Models",
        "RAG"
      ],
      "tags": [
        "RAG",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "lMxuq0GNeC",
      "title": "SmartCache: Context-aware Semantic Cache for Efficient Multi-turn LLM Inference",
      "authors": [
        "Chengye YU",
        "Tianyu Wang",
        "Zili Shao",
        "Song Jiang"
      ],
      "abstract": "Large Language Models (LLMs) for multi-turn conversations suffer from inefficiency: semantically similar queries across different user sessions trigger redundant computation and duplicate memory-intensive Key-Value (KV) caches. Existing optimizations such as prefix caching overlook semantic similarities, while typical semantic caches either ignore conversational context or are not integrated with low-level KV cache management.\nWe propose SmartCache, a system-algorithm co-design framework that tackles this inefficiency by exploiting semantic query similarity across sessions. SmartCache leverages a Semantic Forest structure to hierarchically index conversational turns, enabling efficient retrieval and reuse of responses only when both the semantic query and conversational context match.\nTo maintain accuracy during topic shifts, it leverages internal LLM attention scores—computed during standard prefill—to dynamically detect context changes with minimal computational overhead. Importantly, this semantic understanding is co-designed alongside the memory system: a novel two-level mapping enables transparent cross-session KV cache sharing for semantically equivalent states, complemented by a semantics-aware eviction policy that significantly improves memory utilization. This holistic approach significantly reduces redundant computations and optimizes GPU memory utilization. \nThe evaluation demonstrates SmartCache's effectiveness across multiple benchmarks. On the CoQA and SQuAD datasets, SmartCache reduces KV cache memory usage by up to $59.1\\%$ compared to prefix caching and $56.0\\%$ over semantic caching, while cutting Time-to-First-Token (TTFT) by $78.0\\%$ and $71.7\\%$, respectively. It improves answer quality metrics, achieving $39.9\\%$ higher F1 and $39.1\\%$ higher ROUGE-L for Qwen-2.5-1.5B on CoQA. The Semantic-aware Tiered Eviction Policy (STEP) outperforms LRU/LFU by $29.9\\%$ in reuse distance under skewed workloads.",
      "arxiv_url": "https://openreview.net/forum?id=lMxuq0GNeC",
      "pdf_url": "https://openreview.net/pdf/5bc13f5689dfb66b132abd36782eb71e1da88f36.pdf",
      "primary_category": "Semantic Cache, Key-Value Cache, Large Language Model",
      "categories": [
        "Semantic Cache",
        "Key-Value Cache",
        "Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "2bbDg587uh",
      "title": "Recursive Transformer: Boosting Reasoning Ability with State Stack",
      "authors": [
        "Kechi Zhang",
        "Ge Li",
        "Jia Li",
        "Huangzhao Zhang",
        "Yihong Dong",
        "Jia Li",
        "Jingjing Xu",
        "Zhi Jin"
      ],
      "abstract": "The Transformer architecture has emerged as a landmark advancement within the broad field of artificial intelligence, effectively catalyzing the advent of large language models (LLMs).\nHowever, despite its remarkable capabilities and the substantial progress it has facilitated, the Transformer architecture still has some limitations.\nOne such intrinsic limitation is its inability to effectively recognize regular expressions or deterministic context-free grammars.\nStandard Transformers lack an explicit mechanism for recursion and structured state transitions, which can hinder systematic generalization on nested and hierarchical patterns.\nDrawing inspiration from pushdown automata, which efficiently resolve deterministic context-free grammars using stacks, we equip layers with a differentiable stack and propose StackTrans with recursion to address the aforementioned issue within LLMs.\nUnlike previous approaches that modify the attention computation, StackTrans explicitly incorporates hidden state stacks between Transformer layers. This design maintains compatibility with existing frameworks like flash-attention.  \nSpecifically, our design features stack operations -- such as pushing and popping hidden states -- that are differentiable and can be learned in an end-to-end manner.\nOur comprehensive evaluation spans benchmarks for both Chomsky hierarchy and large-scale natural languages. \nAcross these diverse tasks, StackTrans consistently outperforms standard Transformer models and other baselines. \nWe have successfully scaled StackTrans up from 360M to 7B parameters. In particular, our from-scratch pretrained model StackTrans-360M outperforms several larger open-source LLMs with 2–3x more parameters, showcasing its superior efficiency and reasoning capability.",
      "arxiv_url": "https://openreview.net/forum?id=2bbDg587uh",
      "pdf_url": "https://openreview.net/pdf/46b29957f497f4210023aec25be3b8789b0d7a26.pdf",
      "primary_category": "Foundation Model",
      "categories": [
        "Foundation Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "FVtu7yC7fY",
      "title": "Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning",
      "authors": [
        "Tianle Zhang",
        "Wanlong Fang",
        "Jonathan Woo",
        "Paridhi Latawa",
        "Deepak A. Subramanian",
        "Alvin Chan"
      ],
      "abstract": "The remarkable performance of Large Language Models (LLMs) can be enhanced with test-time computation, which relies on external tools and even other deep learning models.\nHowever, existing approaches for integrating non-text modality representations into LLMs typically require additional costly supervised training, restricting on-the-fly adaptation to new domains and modalities. \nIn this work, we explore the feasibility of integrating representations from non-text foundational models (FMs) into text-based LLMs in a training-free manner. We propose In-Context Representation Learning (ICRL) as a proof-of-concept to allow LLMs to adaptively utilize non-text modality representations with few-shot learning. \nUnlike traditional in-context learning, which incorporates text-label pairs, ICRL replaces text inputs with FM representations, enabling the LLM to perform multi-modal inference without fine-tuning. \nWe evaluate ICRL on a suite of tasks in the molecular domain, investigating three core research questions: (i) how to map FM representations into LLMs in a training-free manner, (ii) what factors influence ICRL performance, and (iii) what mechanisms underlie the effectiveness of ICRL. \nTo the best of our knowledge, ICRL is the first training-free framework for integrating non-text modality representations into text-based LLMs, presenting a promising direction for adaptable, multi-modal generalization.",
      "arxiv_url": "https://openreview.net/forum?id=FVtu7yC7fY",
      "pdf_url": "https://openreview.net/pdf/7986507b932955490e4e477f3ec08631fafb370b.pdf",
      "primary_category": "Large Language Models, Training-Free Adaptation, Foundation Models",
      "categories": [
        "Large Language Models",
        "Training-Free Adaptation",
        "Foundation Models",
        "Multi-Modal Large Language Models",
        "In-Context Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ezOfR26pGQ",
      "title": "Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning",
      "authors": [
        "Xiaoxue Cheng",
        "Junyi Li",
        "Zhenduo Zhang",
        "Xinyu Tang",
        "Xin Zhao",
        "XinYu KONG",
        "Zhiqiang Zhang"
      ],
      "abstract": "Large reasoning models (LRMs) have demonstrated strong performance on complex reasoning tasks, but often suffer from overthinking, generating redundant content regardless of task difficulty. Inspired by the dual process theory in cognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a reinforcement learning framework that enables LRMs to achieve efficient reasoning through adaptive cognitive allocation and dynamic system switch.\nACPO incorporates two key components: (1) introducing system-aware reasoning tokens to explicitly represent the thinking modes thereby making the model's cognitive process transparent, and (2) integrating online difficulty estimation and token length budget to guide adaptive system switch and reasoning during reinforcement learning. \nTo this end, we propose a two-stage training strategy. The first stage begins with supervised fine-tuning to cold start the model, enabling it to generate reasoning paths with explicit thinking modes. In the second stage, we apply ACPO to further enhance adaptive system switch for difficulty-aware reasoning.\nExperimental results demonstrate that ACPO effectively reduces redundant reasoning while adaptively adjusting cognitive allocation based on task complexity, achieving efficient hybrid reasoning.",
      "arxiv_url": "https://openreview.net/forum?id=ezOfR26pGQ",
      "pdf_url": "https://openreview.net/pdf/1d54ebca47124f3efdafcb5b44f5c7243ca94f82.pdf",
      "primary_category": "Slow Thinking, System Switch, Large Reasoning Model",
      "categories": [
        "Slow Thinking",
        "System Switch",
        "Large Reasoning Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4EkEL77k6O",
      "title": "Compress Large Language Models via  Collaboration Between Learning and Matrix Approximation",
      "authors": [
        "Yuesen Liao",
        "Zhiwei Li",
        "Binrui Wu",
        "Zihao Cheng",
        "Su Zhao",
        "Shuai Chen",
        "WEIZHONG ZHANG"
      ],
      "abstract": "Sparse and low-rank matrix composite approximation has emerged as a promising paradigm for compressing large language models (LLMs), offering a more flexible pruning structure than conventional methods based solely on sparse matrices. The significant variation in weight redundancy across layers, along with the differing rank and sparsity structures of weight matrices, makes identifying the globally optimal pruning structure extremely challenging. Existing methods often depend on uniform or manually designed heuristic rules to allocate weight sparsity across layers, subsequently compressing each matrix using matrix approximation techniques. Given the above theoretical difficulty in global compression of LLMs and the limited computational and data resources available compared to the training phase, we argue that a collaboration between learning and matrix approximation is essential for effective compression. In this paper, we propose a novel LLM compression framework based on generalized bilevel optimization that naturally formulates an effective collaborative mechanism. Specifically, the outer loop frames the weight allocation task as a probabilistic optimization problem, enabling the automatic learning of both layer-wise sparsities and matrix-wise retained ranks, while the inner loop solves the corresponding sparsity and rank-constrained model compression problem via matrix approximation. Our main technical contributions include two key innovations for efficiently solving this bilevel optimization problem. First,  we  introduce a truncated Gaussian prior-based probabilistic parameterization integrated with a policy gradient estimator, which avoids expensive backpropagation and stabilizes the optimization process. Second, we design an adapted QR-based matrix approximation algorithm that significantly accelerates inner loop computations. Extensive experiments on Phi-3 and the LLama-2/3 family demonstrate the effectiveness of our method. Notably, it maintains over 95\\% zero-shot accuracy under 50\\% sparsity and achieves up to 2× inference speedup.",
      "arxiv_url": "https://openreview.net/forum?id=4EkEL77k6O",
      "pdf_url": "https://openreview.net/pdf/334cbc6da780d9a5e021d78140cd02e165f9c643.pdf",
      "primary_category": "Model Compression, Large Language Models",
      "categories": [
        "Model Compression",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "koG76YqOwo",
      "title": "HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance",
      "authors": [
        "Jiazi Bu",
        "Pengyang Ling",
        "Yujie Zhou",
        "Pan Zhang",
        "Tong Wu",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Yuhang Cao",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "abstract": "Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. Recent approaches have investigated training-free strategies to enable high-resolution image synthesis with pre-trained models. However, these techniques often struggle with generating high-quality visuals and tend to exhibit artifacts or low-fidelity details, as they typically rely solely on the endpoint of the low-resolution sampling trajectory while neglecting intermediate states that are critical for preserving structure and synthesizing finer detail. To this end,  we present HiFlow, a training-free and model-agnostic framework to unlock the resolution potential of pre-trained flow models. Specifically, HiFlow establishes a virtual reference flow within the high-resolution space that effectively captures the characteristics of low-resolution flow information, offering guidance for high-resolution generation through three key aspects: initialization alignment for low-frequency consistency, direction alignment for structure preservation, and acceleration alignment for detail fidelity. By leveraging such flow-aligned guidance, HiFlow substantially elevates the quality of high-resolution image synthesis of T2I models and demonstrates versatility across their personalized variants. Extensive experiments validate HiFlow's capability in achieving superior high-resolution image quality over state-of-the-art methods.",
      "arxiv_url": "https://openreview.net/forum?id=koG76YqOwo",
      "pdf_url": "https://openreview.net/pdf/54cd3b7681e9b6aac8f25483a2894979905b44fe.pdf",
      "primary_category": "high-resolution generation, text-to-image generation",
      "categories": [
        "high-resolution generation",
        "text-to-image generation"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "LCFYj0R2rH",
      "title": "LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades",
      "authors": [
        "Yanan Li",
        "Fanxu Meng",
        "Muhan Zhang",
        "Shiai Zhu",
        "Shangguang Wang",
        "Mengwei Xu"
      ],
      "abstract": "As Large Language Models (LLMs) are frequently updated, LoRA weights trained on earlier versions quickly become obsolete. The conventional practice of retraining LoRA weights from scratch on the latest model is costly, time-consuming, and environmentally detrimental, particularly as the diversity of LLMs and downstream tasks expands. This motivates a critical question: \"How can we efficiently leverage existing LoRA weights to adapt to newer model versions?\" To address this, we propose LoRASuite, a modular approach tailored specifically to various types of LLM updates. First, we compute a transfer matrix utilizing known parameters from both old and new LLMs. Next, we allocate corresponding layers and attention heads based on centered kernel alignment and cosine similarity metrics, respectively. A subsequent small-scale, skillful fine-tuning step ensures numerical stability. Experimental evaluations demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even exceeds the performance of full-scale LoRA retraining, with average improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally, LoRASuite significantly reduces memory consumption by 5.5 GB and computational time by 78.23%.",
      "arxiv_url": "https://openreview.net/forum?id=LCFYj0R2rH",
      "pdf_url": "https://openreview.net/pdf/f13d757e8c39cd68d8d3c0678a4e7e3dc4d9b25a.pdf",
      "primary_category": "LLM Upgrades; LoRA Adaptation",
      "categories": [
        "LLM Upgrades; LoRA Adaptation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1PvMSoKvZG",
      "title": "PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding",
      "authors": [
        "Kangcong Li",
        "Peng Ye",
        "Chongjun Tu",
        "Lin Zhang",
        "Chunfeng Song",
        "Jiamin Wu",
        "Tao Yang",
        "Qihao Zheng",
        "Tao Chen"
      ],
      "abstract": "While Large Language Models (LLMs) demonstrate strong performance across domains, their long-context capabilities are limited by transient neural activations causing information decay and unstructured feed-forward network (FFN) weights leading to semantic fragmentation. Inspired by the brain’s working memory and cortical modularity, we propose PaceLLM, featuring two innovations: (1) a Persistent Activity (PA) Mechanism that mimics prefrontal cortex (PFC) neurons’ persistent firing by introducing an activation-level memory bank to dynamically retrieve, reuse, and update critical FFN states, addressing contextual decay; and (2) Cortical Expert (CE) Clustering that emulates task-adaptive neural specialization to reorganize FFN weights into semantic modules, establishing cross-token dependencies and mitigating fragmentation. Extensive evaluations show that PaceLLM achieves 6% improvement on LongBench’s Multi-document QA and 12.5–17.5% performance gains on $\\infty$-Bench tasks, while extending measurable context length to 200K tokens in Needle-In-A-Haystack (NIAH) tests. This work pioneers brain-inspired LLM optimization and is complementary to other works. Besides, it can be generalized to any model and enhance their long-context performance and interpretability without structural overhauls.",
      "arxiv_url": "https://openreview.net/forum?id=1PvMSoKvZG",
      "pdf_url": "https://openreview.net/pdf/0716709b8f463ddd4564683201519db51cfdd9df.pdf",
      "primary_category": "Large Language Models, Long Contexts, Brain-inspired LLMs",
      "categories": [
        "Large Language Models",
        "Long Contexts",
        "Brain-inspired LLMs",
        "Feed-Foward Networks",
        "Working Memory",
        "Persistent Activity",
        "Cortical Expert"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "LfcfwlLCHM",
      "title": "DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization",
      "authors": [
        "Yuantian Shao",
        "Yuanteng Chen",
        "Peisong Wang",
        "Jianlin Yu",
        "Jing Lin",
        "Yiwu Yao",
        "Zhihui Wei",
        "Jian Cheng"
      ],
      "abstract": "Quantization plays a crucial role in accelerating the inference of large-scale models, and rotational matrices have been shown to effectively improve quantization performance by smoothing outliers. However, end-to-end fine-tuning of rotational optimization algorithms incurs high computational costs and is prone to overfitting. To address this challenge, we propose an efficient distribution-aware rotational calibration method, DartQuant, which reduces the complexity of rotational optimization by constraining the distribution of the activations after rotation. This approach also effectively reduces reliance on task-specific losses, thereby mitigating the risk of overfitting. Additionally, we introduce the QR-Orth optimization scheme, which replaces expensive alternating optimization with a more efficient solution. In a variety of model quantization experiments, DartQuant demonstrates superior performance. Compared to existing methods, it achieves 47$\\times$ acceleration and 10$\\times$ memory savings for rotational optimization on a 70B model. Furthermore, it is the first to successfully complete rotational calibration for a 70B model on a single 3090 GPU, making quantization of large language models feasible in resource-constrained environments.",
      "arxiv_url": "https://openreview.net/forum?id=LfcfwlLCHM",
      "pdf_url": "https://openreview.net/pdf/558f2c2f758af90ca5cc88dc8c7a55449762ca20.pdf",
      "primary_category": "Large Language Models, Quantization, Rotation",
      "categories": [
        "Large Language Models",
        "Quantization",
        "Rotation",
        "Efficient inference"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "fTkBZLxBzV",
      "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
      "authors": [
        "Zhiyuan Liang",
        "Dongwen Tang",
        "Yuhao Zhou",
        "Xuanlei Zhao",
        "Mingjia Shi",
        "Wangbo Zhao",
        "Zekai Li",
        "Peihao Wang",
        "Konstantin Schürholt",
        "Damian Borth",
        "Michael M. Bronstein",
        "Yang You",
        "Zhangyang Wang",
        "Kai Wang"
      ],
      "abstract": "Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce \\textbf{Drag-and-Drop LLMs (\\textit{DnD})}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into  condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of\nprompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to\n\\textbf{12,000$\\times$} lower overhead than full fine-tuning, ii) average gains up to \\textbf{30\\%} in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization improving \\textbf{40\\%} performance without access to the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs.\nWe open source \\href{https://jerryliang24.github.io/DnD}{our project} in support of future research.",
      "arxiv_url": "https://openreview.net/forum?id=fTkBZLxBzV",
      "pdf_url": "https://openreview.net/pdf/aba6e3723f2c22ae934c3f7b6a1b734942cedbed.pdf",
      "primary_category": "Parameter Generation, Parameter-Efficient-Fine-Tuning, Large Language Models",
      "categories": [
        "Parameter Generation",
        "Parameter-Efficient-Fine-Tuning",
        "Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RnOKrKVMfC",
      "title": "OmniGaze: Reward-inspired Generalizable Gaze Estimation in the Wild",
      "authors": [
        "Hongyu Qu",
        "Jianan Wei",
        "Xiangbo Shu",
        "Yazhou Yao",
        "Wenguan Wang",
        "Jinhui Tang"
      ],
      "abstract": "Current 3D gaze estimation methods struggle to generalize across diverse data domains, primarily due to $\\textbf{i)}$ $\\textit{the scarcity of annotated datasets}$, and $\\textbf{ii)}$ $\\textit{the insufficient diversity of labeled data}$. In this work, we present OmniGaze, a semi-supervised framework for 3D gaze estimation, which utilizes large-scale unlabeled data collected from diverse and unconstrained real-world environments to mitigate domain bias and generalize gaze estimation in the wild. First, we build a diverse collection of unlabeled facial images, varying in facial appearances, background environments, illumination conditions, head poses, and eye occlusions. In order to leverage unlabeled data spanning a broader distribution, OmniGaze adopts a standard pseudo-labeling strategy and devises a reward model to assess the reliability of pseudo labels. Beyond pseudo labels as 3D direction vectors, the reward model also incorporates visual embeddings extracted by an off-the-shelf visual encoder and semantic cues from gaze perspective generated by prompting a Multimodal Large Language Model to compute confidence scores. Then, these scores are utilized to select high-quality pseudo labels and weight them for loss computation. Extensive experiments demonstrate that OmniGaze achieves state-of-the-art performance on five datasets under both in-domain and cross-domain settings. Furthermore, we also evaluate the efficacy of OmniGaze as a scalable data engine for gaze estimation, which exhibits robust zero-shot generalization on four unseen datasets.",
      "arxiv_url": "https://openreview.net/forum?id=RnOKrKVMfC",
      "pdf_url": "https://openreview.net/pdf/84fbf37c5cc1da2aa0a99ff67786d9477a07cec3.pdf",
      "primary_category": "3D Gaze Estimation, In-The-Wild Generalization, Semi-supervised Learning",
      "categories": [
        "3D Gaze Estimation",
        "In-The-Wild Generalization",
        "Semi-supervised Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tJhJYcCABr",
      "title": "Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?",
      "authors": [
        "Yijie Hu",
        "Zihao Zhou",
        "Kaizhu Huang",
        "Xiaowei Huang",
        "Qiufeng Wang"
      ],
      "abstract": "Math reasoning has been one crucial ability of large language models (LLMs), where significant advancements have been achieved in recent years. However, most efforts focus on LLMs by curating high-quality annotation data and intricate training (or inference) paradigms, while the math reasoning performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM typically consists of an LLM and vision block, we wonder: \\textit{Can MLLMs directly absorb math reasoning abilities from off-the-shelf math LLMs without tuning?} Recent model-merging approaches may offer insights into this question. However, they overlook the alignment between the MLLM and LLM, where we find that there is a large gap between their parameter spaces, resulting in lower performance. \nOur empirical evidence reveals two key factors behind this issue: the identification of crucial reasoning-associated layers in the model and the mitigation of the gaps in parameter space. Based on the empirical insights, we propose \\textbf{IP-Merging} that first \\textbf{I}dentifies the reasoning-associated parameters in both MLLM and Math LLM, then \\textbf{P}rojects them into the subspace of MLLM aiming to maintain the alignment, finally merges parameters in this subspace. IP-Merging is a tuning-free approach since parameters are directly adjusted. Extensive experiments demonstrate that our IP-Merging method can enhance the math reasoning ability of MLLMs directly from Math LLMs without compromising their other capabilities.",
      "arxiv_url": "https://openreview.net/forum?id=tJhJYcCABr",
      "pdf_url": "https://openreview.net/pdf/0496fdded22f85ac0744afdbe0292403ef546c33.pdf",
      "primary_category": "Multi-modal reasoning, math reasoning, model merging",
      "categories": [
        "Multi-modal reasoning",
        "math reasoning",
        "model merging"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "rbIlWjTFKj",
      "title": "Struct2D: A Perception-Guided Framework for Spatial Reasoning in MLLMs",
      "authors": [
        "Fangrui Zhu",
        "Hanhui Wang",
        "Yiming Xie",
        "Jing Gu",
        "Tianye Ding",
        "Jianwei Yang",
        "Huaizu Jiang"
      ],
      "abstract": "Unlocking spatial reasoning in Multimodal Large Language Models (MLLMs) is crucial for enabling intelligent interaction with 3D environments. While prior efforts often rely on explicit 3D inputs or specialized model architectures, we ask: can MLLMs reason about 3D space using only structured 2D representations derived from perception?\nIn this work, we introduce Struct2D, a perception-guided prompting framework that combines bird’s-eye-view (BEV) images with object marks and object-centric metadata, optionally incorporating egocentric keyframes when needed. Using Struct2D, we conduct an in-depth zero-shot analysis of closed-source MLLMs (e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning abilities when provided with projected 2D inputs, effectively handling tasks such as relative direction estimation and route planning.\nMotivated by these findings, we construct a large-scale instructional tuning dataset, \\textbf{Struct2D-Set}, using an automated pipeline that generates fine-grained QA pairs grounded in 3D indoor scenes. We then fine-tune an open-source MLLM (Qwen2.5VL) using Struct2D-Set, relying on noisy 3D perception rather than ground-truth annotations. Despite this, the tuned model achieves strong performance across multiple spatial reasoning benchmarks, including 3D question answering, captioning, and object grounding, spanning eight diverse reasoning categories.\nOur approach demonstrates that structured 2D inputs can effectively bridge perception and language reasoning in MLLMs—without requiring explicit 3D representations as input. We will release both our code and dataset to support future research.",
      "arxiv_url": "https://openreview.net/forum?id=rbIlWjTFKj",
      "pdf_url": "https://openreview.net/pdf/45604c251617139c1947693fb938f69e20844fb1.pdf",
      "primary_category": "multimodal learning, spatial reasoning, multimodal large language models",
      "categories": [
        "multimodal learning",
        "spatial reasoning",
        "multimodal large language models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ariVQf0KZx",
      "title": "Thinkless: LLM Learns When to Think",
      "authors": [
        "Gongfan Fang",
        "Xinyin Ma",
        "Xinchao Wang"
      ],
      "abstract": "Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model's ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, \\<short\\> for concise responses and \\<think\\> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing the collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at \\url{https://github.com/VainF/Thinkless}",
      "arxiv_url": "https://openreview.net/forum?id=ariVQf0KZx",
      "pdf_url": "https://openreview.net/pdf/69d5f7c0fe7ed54d42a4a1908d50a94bc062b721.pdf",
      "primary_category": "LLM, Reasoning Models, Hybrid Reasoning",
      "categories": [
        "LLM",
        "Reasoning Models",
        "Hybrid Reasoning",
        "Reinforcement Learning",
        "Efficienct Deep Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yFasd68NyI",
      "title": "SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation",
      "authors": [
        "Zhenyuan Qin",
        "Xincheng Shuai",
        "Henghui Ding"
      ],
      "abstract": "Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose ***SceneDesigner***, a method for accurate and flexible multi-object 9-DoF pose manipulation. ***SceneDesigner*** incorporates a branched network to the pre-trained base model and leverages a new representation, ***CNOCS map***, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ***ObjectPose9D***, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose ***Disentangled Object Sampling***, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, ***SceneDesigner*** enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that ***SceneDesigner*** significantly outperforms existing approaches in both controllability and quality.",
      "arxiv_url": "https://openreview.net/forum?id=yFasd68NyI",
      "pdf_url": "https://openreview.net/pdf/421dd4be8a9777829469d9a9ccdddc7c3db121c3.pdf",
      "primary_category": "Image Generation, SceneDesigner, 9-DoF Pose Manipulation",
      "categories": [
        "Image Generation",
        "SceneDesigner",
        "9-DoF Pose Manipulation"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "VzSjSUE0BZ",
      "title": "CREA: A Collaborative Multi-Agent Framework for Creative Image Editing and Generation",
      "authors": [
        "Kavana Venkatesh",
        "Connor Dunlop",
        "Pinar Yanardag"
      ],
      "abstract": "Creativity in AI imagery remains a fundamental challenge, requiring not only the generation of visually compelling content but also the capacity to add novel, expressive, and artistically rich transformations to images. Unlike conventional editing tasks that rely on direct prompt-based modifications, creative image editing demands an autonomous, iterative approach that balances originality, coherence, and artistic intent. To address this, we introduce CREA, a novel multi-agent collaborative framework that mimics the human creative process. Our framework leverages a team of specialized AI agents who dynamically collaborate to conceptualize, generate, critique, and enhance images.  Through extensive qualitative and quantitative evaluations, we demonstrate that CREA significantly outperforms state-of-the-art methods in diversity, semantic alignment, and creative transformation. To the best of our knowledge, this is the first work to introduce the task of creative editing.",
      "arxiv_url": "https://openreview.net/forum?id=VzSjSUE0BZ",
      "pdf_url": "https://openreview.net/pdf/28e7d3d9e1c9989f49f1595e3f193be85c855561.pdf",
      "primary_category": "creativity, t2i, diffusion",
      "categories": [
        "creativity",
        "t2i",
        "diffusion",
        "multi-agentic systems",
        "llm"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "GLhLU6y1uK",
      "title": "Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency",
      "authors": [
        "Yukun Jiang",
        "Mingjie Li",
        "Michael Backes",
        "Yang Zhang"
      ],
      "abstract": "Despite their superior performance on a wide range of domains, large language models (LLMs) remain vulnerable to misuse for generating harmful content, a risk that has been further amplified by various jailbreak attacks.\nExisting jailbreak attacks mainly follow sequential logic, where LLMs understand and answer each given task one by one.\nHowever, concurrency, a natural extension of the sequential scenario, has been largely overlooked.\nIn this work, we first propose a word-level method to enable task concurrency in LLMs, where adjacent words encode divergent intents.\nAlthough LLMs maintain strong utility in answering concurrent tasks, which is demonstrated by our evaluations on mathematical and general question-answering benchmarks, we notably observe that combining a harmful task with a benign one significantly reduces the probability of it being filtered by the guardrail, showing the potential risks associated with concurrency in LLMs.\nBased on these findings, we introduce $\\texttt{JAIL-CON}$, an iterative attack framework that $\\underline{\\text{JAIL}}$breaks LLMs via task $\\underline{\\text{CON}}$currency. \nExperiments on widely-used LLMs demonstrate the strong jailbreak capabilities of $\\texttt{JAIL-CON}$ compared to existing attacks.\nFurthermore, when the guardrail is applied as a defense, compared to the sequential answers generated by previous attacks, the concurrent answers in our $\\texttt{JAIL-CON}$ exhibit greater stealthiness and are less detectable by the guardrail, highlighting the unique feature of task concurrency in jailbreaking LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=GLhLU6y1uK",
      "pdf_url": "https://openreview.net/pdf/753f227cd7b34078f796838f7aad3d8f2bb3d752.pdf",
      "primary_category": "Large Language Models, Jailbreak",
      "categories": [
        "Large Language Models",
        "Jailbreak"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ACSOnSHiWe",
      "title": "CAM: A Constructivist View of Agentic Memory for LLM-Based Reading Comprehension",
      "authors": [
        "Rui Li",
        "Zeyu Zhang",
        "Xiaohe Bo",
        "Zihang Tian",
        "Xu Chen",
        "Quanyu Dai",
        "Zhenhua Dong",
        "Ruiming Tang"
      ],
      "abstract": "Current Large Language Models (LLMs) are confronted with overwhelming information volume when comprehending long-form documents. This challenge raises the imperative of a cohesive memory module, which can elevate vanilla LLMs into autonomous reading agents. Despite the emergence of some heuristic approaches, a systematic design principle remains absent. To fill this void, we draw inspiration from Jean Piaget's Constructivist Theory, illuminating three traits of the agentic memory---structured schemata, flexible assimilation, and dynamic accommodation. This blueprint forges a clear path toward a more robust and efficient memory system for LLM-based reading comprehension. To this end, we develop CAM, a prototype implementation of Constructivist Agentic Memory that simultaneously embodies the structurality, flexibility, and dynamicity. At its core, CAM is endowed with an incremental overlapping clustering algorithm for structured memory development, supporting both coherent hierarchical summarization and online batch integration. During inference, CAM adaptively explores the memory structure to activate query-relevant information for contextual response, akin to the human associative process. Compared to existing approaches, our design demonstrates dual advantages in both performance and efficiency across diverse long-text reading comprehension tasks, including question answering, query-based summarization, and claim verification.",
      "arxiv_url": "https://openreview.net/forum?id=ACSOnSHiWe",
      "pdf_url": "https://openreview.net/pdf/33d48884d041c742c7dca881251da63dbbc58a0f.pdf",
      "primary_category": "LLM-Based Reading Agent, Long-Text Reading Comprehension, LLM Memory",
      "categories": [
        "LLM-Based Reading Agent",
        "Long-Text Reading Comprehension",
        "LLM Memory"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "K2Q4Jp4RbB",
      "title": "Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations",
      "authors": [
        "Yiyou Sun",
        "Yu Gai",
        "Lijie Chen",
        "Abhilasha Ravichander",
        "Yejin Choi",
        "Nouha Dziri",
        "Dawn Song"
      ],
      "abstract": "Large language models (LLMs) frequently generate hallucinations—content that deviates from factually inaccurate or deviates from provided context—posing challenges for diagnosis. However, diagnosing the causes of hallucination is challenging due to the complex interplay of underlying causes. This paper introduces a framework to systematically understand the sources of hallucination behavior in large language models. Our key insight is that hallucinations arise when more frequent but non-factual associations outweigh faithful ones.\nThrough theoretical and empirical analyses, we demonstrate that decoder-only transformers effectively function as subsequence embedding models, with the fully-connected layers encoding input-output associations. We propose a tracing algorithm that identifies causal subsequences by analyzing hallucination probabilities across randomized input contexts. Experiments show our method outperforms standard attribution techniques in identifying hallucination causes and is supported by evidence from the model’s training corpus. This work provides a unified perspective on hallucinations and a robust framework for their cause and analysis.",
      "arxiv_url": "https://openreview.net/forum?id=K2Q4Jp4RbB",
      "pdf_url": "https://openreview.net/pdf/f07ade0b092da27b4416f33d949af9457187749a.pdf",
      "primary_category": "Large Language Model; Hallucination;",
      "categories": [
        "Large Language Model; Hallucination;"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "w2xl15ZbD3",
      "title": "Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation",
      "authors": [
        "Shiwei Li",
        "Xiandi Luo",
        "Haozhao Wang",
        "Xing Tang",
        "Ziqiang Cui",
        "Dugang Liu",
        "Yuhua Li",
        "xiuqiang He",
        "Ruixuan Li"
      ],
      "abstract": "Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method widely used in large language models (LLMs). \nLoRA essentially describes the projection of an input space into a low-dimensional output space, with the dimensionality determined by the LoRA rank.\nIn standard LoRA, all input tokens share the same weights and undergo an identical input-output projection.\nThis limits LoRA's ability to capture token-specific information due to the inherent semantic differences among tokens.\nTo address this limitation, we propose **Token-wise Projected Low-Rank Adaptation (TopLoRA)**, which dynamically adjusts LoRA weights according to the input token, thereby learning token-wise input-output projections in an end-to-end manner.\nFormally, the weights of TopLoRA can be expressed as $B\\Sigma_X A$, where $A$ and $B$ are low-rank matrices (as in standard LoRA), and $\\Sigma_X$ is a diagonal matrix generated from each input token $X$.\nNotably, TopLoRA does not increase the rank of LoRA weights but achieves more granular adaptation by learning token-wise LoRA weights (i.e., token-wise input-output projections).\nExtensive experiments across multiple models and datasets demonstrate that TopLoRA consistently outperforms LoRA and its variants. \nThe code is available at https://github.com/Leopold1423/toplora-neurips25.",
      "arxiv_url": "https://openreview.net/forum?id=w2xl15ZbD3",
      "pdf_url": "https://openreview.net/pdf/05b3a11a599e2e994b8865d4d30748696784862d.pdf",
      "primary_category": "LoRA, Token-wise Projection",
      "categories": [
        "LoRA",
        "Token-wise Projection"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oVDAfLuRie",
      "title": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception",
      "authors": [
        "Ziang Yan",
        "Yinan He",
        "Xinhao Li",
        "Zhengrong Yue",
        "Xiangyu Zeng",
        "Yali Wang",
        "Yu Qiao",
        "Limin Wang",
        "Yi Wang"
      ],
      "abstract": "Inducing reasoning in multimodal large language models (MLLMs) is critical for achieving human-level perception and understanding. Existing methods mainly leverage LLM reasoning to analyze parsed visuals, often limited by static perception stages. This paper introduces Visual Test-Time Scaling (VTTS), a novel approach to enhance MLLMs' reasoning via iterative perception during inference. VTTS mimics humans' hierarchical attention by progressively refining focus on high-confidence spatio-temporal regions, guided by updated textual predictions. Specifically, VTTS employs an Iterative Perception (ITP) mechanism, incorporating reinforcement learning with spatio-temporal supervision to optimize reasoning. To support this paradigm, we also present VTTS-80K, a dataset tailored for iterative perception.\nThese designs allows a MLLM to enhance its performance by increasing its perceptual compute.  Extensive experiments validate VTTS's effectiveness and generalization across diverse tasks and benchmarks. Our newly introduced Videochat-R1.5 model has achieved remarkable improvements, with an average increase of over 5\\%, compared to robust baselines such as Qwen2.5VL-3B and -7B, across more than 15 benchmarks that encompass video conversation, video reasoning, and spatio-temporal perception.",
      "arxiv_url": "https://openreview.net/forum?id=oVDAfLuRie",
      "pdf_url": "https://openreview.net/pdf/7cf4479fc77f583f1cdede042be97389be035f9a.pdf",
      "primary_category": "MLLM, Test Time Scaling",
      "categories": [
        "MLLM",
        "Test Time Scaling"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "L0xZPXT3le",
      "title": "Multi-Agent Collaboration via Evolving Orchestration",
      "authors": [
        "Yufan Dang",
        "Chen Qian",
        "Xueheng Luo",
        "Jingru Fan",
        "Zihao Xie",
        "Ruijie Shi",
        "Weize Chen",
        "Cheng Yang",
        "Xiaoyin Che",
        "Ye Tian",
        "Xuantang Xiong",
        "Lei Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable results across diverse downstream tasks, but their monolithic nature restricts scalability and efficiency in complex problem-solving. While recent research explores multi-agent collaboration among LLMs, most approaches rely on static organizational structures that struggle to adapt as task complexity and agent numbers grow, resulting in coordination overhead and inefficiencies. To this end, we propose a puppeteer-style paradigm for LLM-based multi-agent collaboration, where a centralized orchestrator (\"puppeteer\") dynamically directs agents (\"puppets\") in response to evolving task states. This orchestrator is trained via reinforcement learning to adaptively sequence and prioritize agents, enabling flexible and evolvable collective reasoning. Experiments on closed- and open-domain scenarios show that this method achieves superior performance with reduced computational costs. Analyses further reveal that the key improvements consistently stem from the emergence of more compact, cyclic reasoning structures under the orchestrator’s evolution. Our code is available at https://github.com/OpenBMB/ChatDev/tree/puppeteer.",
      "arxiv_url": "https://openreview.net/forum?id=L0xZPXT3le",
      "pdf_url": "https://openreview.net/pdf/9727f658d788c52f49f12ae4b230baf4cf0d4007.pdf",
      "primary_category": "Large Language Model, Autonomous Agent, Multi-Agent Collaboration",
      "categories": [
        "Large Language Model",
        "Autonomous Agent",
        "Multi-Agent Collaboration",
        "Evolving Orchestration"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vRVfgcoeIl",
      "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models",
      "authors": [
        "Yulei Qin",
        "Gang Li",
        "Zongyi Li",
        "Zihan Xu",
        "Yuchen Shi",
        "Zhekai Lin",
        "Xiao Cui",
        "Ke Li",
        "Xing Sun"
      ],
      "abstract": "Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose RAIF, a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on OOD constraints also confirms the generalizability of our RAIF.",
      "arxiv_url": "https://openreview.net/forum?id=vRVfgcoeIl",
      "pdf_url": "https://openreview.net/pdf/0de94b848e4b6f73ac84843d481395c51d499376.pdf",
      "primary_category": "instruction following, large language model, reasoning",
      "categories": [
        "instruction following",
        "large language model",
        "reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "uL7lCOHtiZ",
      "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank",
      "authors": [
        "Tianhe Wu",
        "Jian Zou",
        "Jie Liang",
        "Lei Zhang",
        "Kede Ma"
      ],
      "abstract": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing reasoning and generalization capabilities of large language models (LLMs) through reinforcement learning. Nevertheless, the potential of reasoning-induced computation has not been thoroughly explored in the context of image quality assessment (IQA), a task depending critically on visual reasoning. In this paper, we introduce VisualQuality-R1, a reasoning-induced no-reference IQA (NR-IQA) model, and we train it with reinforcement learning to rank, a learning algorithm tailored to the intrinsically relative nature of visual quality. Specifically, for a pair of images, we employ group relative policy optimization to generate multiple quality scores for each image. These estimates are used to compute comparative probabilities of one image having higher quality than the other under the Thurstone model. Rewards for each quality estimate are defined using continuous fidelity measures rather than discretized binary labels. Extensive experiments show that the proposed VisualQuality-R1 consistently outperforms discriminative deep learning-based NR-IQA models as well as a recent reasoning-induced quality regression method. Moreover, VisualQuality-R1 is capable of generating contextually rich, human-aligned quality descriptions, and supports multi-dataset training without requiring perceptual scale realignment. These features make VisualQuality-R1 especially well-suited for reliably measuring progress in a wide range of image processing tasks like super-resolution and image generation.",
      "arxiv_url": "https://openreview.net/forum?id=uL7lCOHtiZ",
      "pdf_url": "https://openreview.net/pdf/e2ba21b9f67297b5a5b9fbb0eb32099f5d6c8fd0.pdf",
      "primary_category": "Image Quality Assessment, Reinforcement Learning, Reasoning-induced no-reference IQA model",
      "categories": [
        "Image Quality Assessment",
        "Reinforcement Learning",
        "Reasoning-induced no-reference IQA model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "q5QaTQcUbS",
      "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model",
      "authors": [
        "Wenbo Hu",
        "Yining Hong",
        "Yanjun Wang",
        "Leison Gao",
        "Zibu Wei",
        "Xingcheng Yao",
        "Nanyun Peng",
        "Yonatan Bitton",
        "Idan Szpektor",
        "Kai-Wei Chang"
      ],
      "abstract": "Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. \nWe posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. \nTo address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agent's ability to reason over long-term memory in 3D environments.\nSecond, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. \nOur model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5\\% in success rate on 3DMem-Bench's  most challenging in-the-wild embodied tasks.",
      "arxiv_url": "https://openreview.net/forum?id=q5QaTQcUbS",
      "pdf_url": "https://openreview.net/pdf/8e3474ff3b680185f73a397352845c1347b77d73.pdf",
      "primary_category": "multimodal, llm, 3D",
      "categories": [
        "multimodal",
        "llm",
        "3D",
        "embodied",
        "memory",
        "agent"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "wczmXLuLGd",
      "title": "MMaDA: Multimodal Large Diffusion Language Models",
      "authors": [
        "Ling Yang",
        "Ye Tian",
        "Bowen Li",
        "Xinchen Zhang",
        "Ke Shen",
        "Yunhai Tong",
        "Mengdi Wang"
      ],
      "abstract": "We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a unified diffusion architecture with a shared probabilistic formulation and a modality-agnostic design, eliminating the need for modality-specific components. This architecture ensures seamless integration and processing across different data types. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning strategy that curates a unified CoT format across modalities. By aligning reasoning processes between textual and visual domains, this strategy facilitates cold-start training for the final reinforcement learning (RL) stage, thereby enhancing the model's ability to handle complex tasks from the outset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm specifically tailored for diffusion foundation models. Utilizing diversified reward modeling, UniGRPO unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements. Experimental results demonstrate that MMaDA-8B exhibits strong generalization capabilities as a unified multimodal foundation model. It surpasses powerful models like LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in multimodal understanding, and excels over SDXL and Janus in text-to-image generation. These achievements highlight MMaDA's effectiveness in bridging the gap between pretraining and post-training within unified diffusion architectures, providing a comprehensive framework for future research and development. We open-source our code and trained models at: https://github.com/Gen-Verse/MMaDA",
      "arxiv_url": "https://openreview.net/forum?id=wczmXLuLGd",
      "pdf_url": "https://openreview.net/pdf/7cf07f47b6988f90481532d5181d8605e72a836f.pdf",
      "primary_category": "Unified Diffusion Foundation Model, Reinforcement Learning, LLM Reasoning",
      "categories": [
        "Unified Diffusion Foundation Model",
        "Reinforcement Learning",
        "LLM Reasoning",
        "Multimodal Understanding",
        "Image Generation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "lC7q2cwpov",
      "title": "iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning",
      "authors": [
        "Manyi Yao",
        "Bingbing Zhuang",
        "Sparsh Garg",
        "Amit Roy-Chowdhury",
        "Christian R. Shelton",
        "Manmohan Chandraker",
        "Abhishek Aich"
      ],
      "abstract": "Grounding large language models (LLMs) in domain-specific tasks like post-hoc dash-cam driving video analysis is challenging due to their general-purpose training and lack of structured inductive biases. As vision is often the sole modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing video-based vision-language models (V-VLMs) struggle with spatial reasoning, causal inference, and explainability of events in the input video. To this end, we introduce iFinder, a structured semantic grounding framework that decouples perception from reasoning by translating dash-cam videos into a hierarchical, interpretable data structure for LLMs. iFinder operates as a modular, training-free pipeline that employs pretrained vision models to extract critical cues—object pose, lane positions, and object trajectories—which are hierarchically organized into frame- and video-level structures. Combined with a three-block prompting strategy, it enables step-wise, grounded reasoning for the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.\nEvaluations on four public dash-cam video benchmarks show that iFinder's proposed grounding with domain-specific cues—especially object orientation and global context—significantly outperforms end-to-end V-VLMs on four zero-shot driving benchmarks, with up to 39% gains in accident reasoning accuracy. By grounding LLMs with driving domain-specific representations, iFinder offers a zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for post-hoc driving video understanding.",
      "arxiv_url": "https://openreview.net/forum?id=lC7q2cwpov",
      "pdf_url": "https://openreview.net/pdf/4f80ab987408054880783015662376cda79b4115.pdf",
      "primary_category": "video understanding, driving video analysis",
      "categories": [
        "video understanding",
        "driving video analysis"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "g52NwTQj0Q",
      "title": "StateSpaceDiffuser: Bringing Long Context to Diffusion World Models",
      "authors": [
        "Nedko Savov",
        "Naser Kazemi",
        "Deheng Zhang",
        "Danda Pani Paudel",
        "Xi Wang",
        "Luc Van Gool"
      ],
      "abstract": "World models have recently gained prominence for action-conditioned visual prediction in complex environments. However, relying on only a few recent observations causes them to lose long-term context. Consequently, within a few steps, the generated scenes drift from what was previously observed, undermining temporal coherence. This limitation, common in state-of-the-art world models, which are diffusion-based, stems from the lack of a lasting environment state.\n\nTo address this problem, we introduce StateSpaceDiffuser, where a diffusion model is enabled to perform long-context tasks by integrating features from a state-space model, representing the entire interaction history. This design restores long-term memory while preserving the high-fidelity synthesis of diffusion models.\n\nTo rigorously measure temporal consistency, we develop an evaluation protocol that probes a model’s ability to reinstantiate seen content in extended rollouts. Comprehensive experiments show that StateSpaceDiffuser significantly outperforms a strong diffusion-only baseline, maintaining a coherent visual context for an order of magnitude more steps. It delivers consistent views in both a 2D maze navigation and a complex 3D environment.  These results establish that bringing state-space representations into diffusion models is highly effective in demonstrating both visual details and long-term memory. Project page: https://insait-institute.github.io/StateSpaceDiffuser/",
      "arxiv_url": "https://openreview.net/forum?id=g52NwTQj0Q",
      "pdf_url": "https://openreview.net/pdf/1309bddc133aa9a2f798f3d776ffd8e4733decf5.pdf",
      "primary_category": "world model, state-space model, long context",
      "categories": [
        "world model",
        "state-space model",
        "long context",
        "generative model",
        "computer vision"
      ],
      "tags": [
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "mmIAp3cVS0",
      "title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems",
      "authors": [
        "Guibin Zhang",
        "Muxin Fu",
        "Kun Wang",
        "Guancheng Wan",
        "Miao Yu",
        "Shuicheng YAN"
      ],
      "abstract": "Large language model (LLM)-powered multi-agent systems (MAS) have demonstrated cognitive and execution capabilities that far exceed those of single LLM agents, yet their capacity for self-evolution remains hampered by underdeveloped memory architectures. Upon close inspection, we are alarmed to discover that prevailing MAS memory mechanisms (1) are overly simplistic, completely disregarding the nuanced inter-agent collaboration trajectories, and (2) lack cross-trial and agent-specific customization, in stark contrast to the expressive memory developed for single agents. To bridge this gap, we introduce G-Memory, a hierarchical, agentic memory system for MAS inspired by organizational memory theory, which manages the lengthy MAS interaction via a three-tier graph hierarchy: insight, query, and interaction graphs. Upon receiving a new user query, G-Memory performs bi-directional memory traversal to retrieve both \\textit{high-level, generalizable insights} that enable the system to leverage cross-trial knowledge, and \\textit{fine-grained, condensed interaction trajectories} that compactly encode prior collaboration experiences. Upon task execution, the entire hierarchy evolves by assimilating new collaborative trajectories, nurturing the progressive evolution of agent teams. Extensive experiments across five benchmarks, three LLM backbones, and three popular MAS frameworks demonstrate that G-Memory improves success rates in embodied action and accuracy in knowledge QA by up to $20.89\\\\%$ and $10.12\\\\%$, respectively, without any modifications to the original frameworks.",
      "arxiv_url": "https://openreview.net/forum?id=mmIAp3cVS0",
      "pdf_url": "https://openreview.net/pdf/52f961783a3212459f228b4ec297f523ba2d0c95.pdf",
      "primary_category": "Multi-agent systems, agent memory, LLM agent",
      "categories": [
        "Multi-agent systems",
        "agent memory",
        "LLM agent"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "WZS7sJiSUm",
      "title": "All You Need is One: Capsule Prompt Tuning with a Single Vector",
      "authors": [
        "Yiyang Liu",
        "James Chenhao Liang",
        "Heng Fan",
        "Wenhao Yang",
        "Yiming Cui",
        "Xiaotian Han",
        "Lifu Huang",
        "Dongfang Liu",
        "Qifan Wang",
        "Cheng Han"
      ],
      "abstract": "Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT) approach to facilitate Large Language Model (LLM) adaptation to downstream tasks by conditioning generation with task-aware guidance. Despite its successes, current prompt-based learning methods heavily rely on laborious grid searching for optimal prompt length and typically require considerable number of prompts, introducing additional computational burden. Worse yet, our pioneer findings indicate that the task-aware prompt design is inherently limited by its absence of instance-aware information, leading to a subtle attention interplay with the input sequence. In contrast, simply incorporating instance-aware information as a part of the guidance can enhance the prompt-tuned model performance without additional fine-tuning. Moreover, we find an interesting phenomenon, namely \"attention anchor\", that incorporating instance-aware tokens at the earliest position of the sequence can successfully preserve strong attention to critical structural information and exhibit more active attention interaction with all input tokens. In light of our observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and effective solution that leverages off-the-shelf, informative instance semantics into prompt-based learning. Our approach innovatively integrates both instance-aware and task-aware information in a nearly parameter-free manner (i.e., one single capsule prompt).\nEmpirical results demonstrate that our method can exhibit superior performance across various language tasks (e.g., 84.03\\% average accuracy on T5-Large), serving as an \"attention anchor,\" while enjoying high parameter efficiency (e.g., 0.003\\% of model parameters on Llama3.2-1B).",
      "arxiv_url": "https://openreview.net/forum?id=WZS7sJiSUm",
      "pdf_url": "https://openreview.net/pdf/66d1effdb19558fa52a14e3d6f9475451d810c9d.pdf",
      "primary_category": "Prompt Tuning, Large Language Models, Parameter-Efficient Fine-Tuning",
      "categories": [
        "Prompt Tuning",
        "Large Language Models",
        "Parameter-Efficient Fine-Tuning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RoVS9vmpd2",
      "title": "Codifying Character Logic in Role-Playing",
      "authors": [
        "Letian Peng",
        "Jingbo Shang"
      ],
      "abstract": "This paper introduces Codified Profiles for role-playing, a novel approach that represents character logic as structured, executable functions for behavioral decision-making. \nConverted by large language model (LLM) from textual profiles, each codified profile defines a set of functions parse_by_scene(scene) that output multiple logic-grounded assertions according to scene, using both explicit control structures (e.g., if-then-else) and flexible check_condition(scene, question) functions where each question is a semantically meaningful prompt about the scene (e.g., \"Is the character in danger?\") discriminated by the role-playing LLM as true, false, or unknown.\nThis explicit representation offers three key advantages over traditional prompt-based textual profiles, which append character descriptions directly into text prompts:\n(1) Persistence, by enforcing complete and consistent execution of character logic, rather than relying on the model's implicit reasoning;  \n(2) Updatability, through systematic inspection and revision of behavioral logic, which is difficult to track or debug in prompt-only approaches;  \n(3) Controllable Randomness, by supporting stochastic behavior directly within the logic, enabling fine-grained variability that prompting alone struggles to achieve.  \nTo validate these advantages, we introduce a new benchmark constructed from 83 characters and 5,141 scenes curated from Fandom, using natural language inference (NLI)-based scoring to compare character responses against ground-truths.  \nOur experiments demonstrate the significant benefits of codified profiles in improving persistence, updatability, and behavioral diversity. \nNotably, by offloading a significant portion of reasoning to preprocessing, codified profiles enable even 1B-parameter models to perform high-quality role-playing, providing an efficient, lightweight foundation for local deployment of role-play agents.",
      "arxiv_url": "https://openreview.net/forum?id=RoVS9vmpd2",
      "pdf_url": "https://openreview.net/pdf/cc0b8f7985b118e5590ea15bb5300e3a42e2984a.pdf",
      "primary_category": "Role-playing, Large Language Models, Knowledge Grounding",
      "categories": [
        "Role-playing",
        "Large Language Models",
        "Knowledge Grounding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tY8ctrD4W2",
      "title": "Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning",
      "authors": [
        "Haolin Pan",
        "Hongyu Lin",
        "Haoran Luo",
        "Yang Liu",
        "Kaichun Yao",
        "Libo Zhang",
        "Mingjie Xing",
        "Yanjun Wu"
      ],
      "abstract": "Compiler auto-tuning optimizes pass sequences to improve performance metrics such as Intermediate Representation (IR) instruction count. Although recent advances leveraging Large Language Models (LLMs) have shown promise in automating compiler tuning, two significant challenges still remain: the absence of high-quality reasoning datasets for agents training, and limited effective interactions with the compilation environment. In this work, we introduce Compiler-R1, the first reinforcement learning (RL)-driven framework specifically augmenting LLM capabilities for compiler auto-tuning. Compiler-R1 features a curated, high-quality reasoning dataset and a novel two-stage end-to-end RL training pipeline, enabling efficient environment exploration and learning through an outcome-based reward. Extensive experiments across seven datasets demonstrate Compiler-R1 achieving an average 8.46\\% IR instruction count reduction compared to opt -Oz, showcasing the strong potential of RL-trained LLMs for compiler optimization. Our code and datasets are publicly available at https://github.com/Panhaolin2001/Compiler-R1.",
      "arxiv_url": "https://openreview.net/forum?id=tY8ctrD4W2",
      "pdf_url": "https://openreview.net/pdf/d13dfae7f734b97ce785c0d6bd39d5acc47cade8.pdf",
      "primary_category": "LLM, Compiler Tuning, Reinforcement Learning",
      "categories": [
        "LLM",
        "Compiler Tuning",
        "Reinforcement Learning",
        "Agent",
        "Compilation Pass"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "PxximqJil4",
      "title": "StarTrail: Concentric Ring Sequence Parallelism for Efficient Near-Infinite-Context Transformer Model Training",
      "authors": [
        "Ziming Liu",
        "Shaoyu Wang",
        "Shenggan Cheng",
        "Zhongkai Zhao",
        "Kai Wang",
        "Xuanlei Zhao",
        "James Demmel",
        "Yang You"
      ],
      "abstract": "Training Transformer models on long sequences in a distributed setting poses significant challenges in terms of efficiency and scalability. Current methods are either constrained by the number of attention heads or excessive communication overheads. To address this problem, we propose StarTrail, a multi-dimensional concentric distributed training system for long sequences, fostering an efficient communication paradigm and providing additional tuning flexibility for communication arrangements. Specifically, StarTrail introduces an extra parallel dimension and divides the peer-to-peer communication into sub-rings to substantially reduce communication volume and avoid bandwidth bottlenecks. Through comprehensive experiments across diverse hardware environments and on both Natural Language Processing (NLP) and Computer Vision (CV) tasks, we demonstrate that our approach significantly surpasses state-of-the-art methods that support Long sequence lengths, achieving performance improvements of up to 77.12% on GPT-style models and up to 114.33% on DiT (Diffusion Transformer) models without affecting the computations results.",
      "arxiv_url": "https://openreview.net/forum?id=PxximqJil4",
      "pdf_url": "https://openreview.net/pdf/3a84ca5724f0d5e35a94ee04fb8937f4de89d362.pdf",
      "primary_category": "Sequence Parallelism, Machine Learning System, Distributed Training",
      "categories": [
        "Sequence Parallelism",
        "Machine Learning System",
        "Distributed Training"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "g9sWQsqemL",
      "title": "Boosting Resilience of Large Language Models through Causality-Driven Robust Optimization",
      "authors": [
        "Xiaoling Zhou",
        "Mingjie Zhang",
        "Zhemg Lee",
        "YUNCHENG HUA",
        "chengli xing",
        "Wei Ye",
        "Flora D. Salim",
        "Shikun Zhang"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable achievements across diverse applications; however, they remain plagued by spurious correlations and the generation of hallucinated content. Despite extensive efforts to enhance the resilience of LLMs, existing approaches either rely on indiscriminate fine-tuning of all parameters, resulting in parameter inefficiency and lack of specificity, or depend on post-processing techniques that offer limited adaptability and flexibility. This study introduces a novel Causality-driven Robust Optimization (CdRO) approach that selectively updates model components sensitive to causal reasoning, enhancing model causality while preserving valuable pretrained knowledge to mitigate overfitting. Our method begins by identifying the parameter components within LLMs that capture causal relationships, achieved through comparing the training dynamics of parameter matrices associated with the original samples, as well as augmented counterfactual and paraphrased variants. These comparisons are then fed into a lightweight logistic regression model, optimized in real time to dynamically identify and adapt the causal components within LLMs. The identified parameters are subsequently optimized using an enhanced policy optimization algorithm, where the reward function is designed to jointly promote both model generalization and robustness. Extensive experiments across various tasks using twelve different LLMs demonstrate the superior performance of our framework, underscoring its significant effectiveness in reducing the model’s dependence on spurious associations and mitigating hallucinations.",
      "arxiv_url": "https://openreview.net/forum?id=g9sWQsqemL",
      "pdf_url": "https://openreview.net/pdf/02e499e22c691c45b5b6f907f6bef8def2fd5cca.pdf",
      "primary_category": "Large language models, Spurious correlations, Hallucination",
      "categories": [
        "Large language models",
        "Spurious correlations",
        "Hallucination",
        "Knowledge localization",
        "Logistic regression model",
        "Policy optimization"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "b98ODdeYq5",
      "title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks",
      "authors": [
        "Hongyuan Tao",
        "Ying Zhang",
        "Zhenhao Tang",
        "Hongen Peng",
        "Xukun Zhu",
        "Bingchang Liu",
        "Yingguang Yang",
        "Ziyin Zhang",
        "Zhaogui Xu",
        "Haipeng Zhang",
        "Linchao Zhu",
        "Rui Wang",
        "Hang Yu",
        "Jianguo Li",
        "Peng Di"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.",
      "arxiv_url": "https://openreview.net/forum?id=b98ODdeYq5",
      "pdf_url": "https://openreview.net/pdf/f1cf876070f6cbbee613b2d9e9db609e1aa63f6e.pdf",
      "primary_category": "Code LLM, Repository-Level, Agentless",
      "categories": [
        "Code LLM",
        "Repository-Level",
        "Agentless",
        "Open-source",
        "Multimodal",
        "Graph"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "AqHlcF0zK6",
      "title": "Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study",
      "authors": [
        "Zhengyu Hu",
        "Jianxun Lian",
        "Zheyuan Xiao",
        "Seraphina Zhang",
        "Tianfu Wang",
        "Nicholas Jing Yuan",
        "Xing Xie",
        "Hui Xiong"
      ],
      "abstract": "Large language models (LLMs) have shown impressive capabilities across tasks such as mathematics, coding, and reasoning, yet their learning ability, which is crucial for adapting to dynamic environments and acquiring new knowledge, remains underexplored. In this work, we address this gap by introducing a  framework inspired by  cognitive psychology and education. Specifically, we decompose general learning ability into three distinct, complementary dimensions: *Learning from Instructor* (acquiring knowledge via explicit guidance), *Learning from Concept* (internalizing abstract structures and generalizing to new contexts), and *Learning from Experience* (adapting through accumulated exploration and feedback). We conduct a comprehensive empirical study across the three learning dimensions and identify several insightful findings, such as (i) interaction improves learning; (ii) conceptual understanding is scale-emergent and benefits larger models; and (iii)  LLMs are effective few-shot learners but not many-shot learners. Based on our framework and empirical findings, we introduce a benchmark that provides a unified and realistic evaluation of LLMs' general learning abilities across three learning cognition dimensions. It enables diagnostic insights and supports evaluation and development of more adaptive and human-like models.",
      "arxiv_url": "https://openreview.net/forum?id=AqHlcF0zK6",
      "pdf_url": "https://openreview.net/pdf/e59a38a0e12b89926fd278d8c8f41fbe3f9ccaec.pdf",
      "primary_category": "Large Language Models, Learning Ability, Evaluation Framework",
      "categories": [
        "Large Language Models",
        "Learning Ability",
        "Evaluation Framework"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "dw9H08UxJb",
      "title": "Omni-Mol: Multitask Molecular Model for Any-to-any Modalities",
      "authors": [
        "Chengxin Hu",
        "Hao Li",
        "Yihe Yuan",
        "Zezheng Song",
        "Chenyang Zhao",
        "Haixin Wang"
      ],
      "abstract": "In the molecular domain, numerous studies have explored the use of multimodal large language models (LLMs) to construct a general-purpose, multi-task molecular model. However, these efforts are still far from achieving a truly universal molecular model. We identify three key challenges in this endeavor: (1) Existing molecular task datasets are typically small in scale and lack comprehensive domain coverage. (2) Tasks from different molecular subfields are difficult to effectively learn jointly through LLMs due to significant distributional shifts and competition among tasks, which introduces instability in the learning process. (3) Both inter-task and intra-task molecular representations demand different intrinsic dimensions in the language space, making it challenging to balance between redundancy and insufficiency in language model representations. To address these challenges, we innovatively categorize existing small-molecule tasks into four types: Mol2Mol, Mol2Text, Mol2Num, and Text2Mol. We then collect a dataset encompassing over 16 tasks with more than 1.4 million samples, making it the largest molecular instruction-tuning dataset to date. Leveraging the extensive pretraining of LLMs on existing chemical literature, we propose a novel multimodal LLM framework, named **Omni-Mol**, which unifies all small-molecule tasks and supports both molecular generation and understanding. The core of Omni-Mol is our proposed MoGE, which dynamically adapts to the intrinsic rank of different tasks. This mixture-of-experts architecture enhances the model's ability to handle diverse tasks and modalities effectively. Our model achieves unified instruction tuning across 16 tasks and attains state-of-the-art performance on 13 of them. Extensive experiments further demonstrate the scalability and versatility of Omni-Mol.",
      "arxiv_url": "https://openreview.net/forum?id=dw9H08UxJb",
      "pdf_url": "https://openreview.net/pdf/a19b59a0b74a8cc994889dcbe336d68e6754a976.pdf",
      "primary_category": "Large Language Models, Molecular Science, Multi-modal Learning",
      "categories": [
        "Large Language Models",
        "Molecular Science",
        "Multi-modal Learning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "yGOytgjurF",
      "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems",
      "authors": [
        "Hancheng Ye",
        "Zhengqi Gao",
        "Mingyuan Ma",
        "Qinsi Wang",
        "Yuzhe Fu",
        "Ming-Yu Chung",
        "Yueqian Lin",
        "Zhijian Liu",
        "Jianyi Zhang",
        "Danyang Zhuo",
        "Yiran Chen"
      ],
      "abstract": "Multi-agent large language model (LLM) systems are increasingly adopted for complex language processing tasks that require communication and coordination among agents. However, these systems often suffer substantial overhead from repeated reprocessing of overlapping contexts across agents. In typical pipelines, once an agent receives a message from its predecessor, the full context-including prior turns-must be reprocessed from scratch, leading to inefficient processing. While key-value (KV) caching is an effective solution for avoiding redundant computation in single-agent settings where prefixes remain unchanged, it cannot be directly reused in multi-agent scenarios due to diverging prefixes introduced by agent-specific context extensions. We identify that the core challenge lies in the offset variance of KV-caches across agents. To address this, we propose **KVCOMM**, a training-free framework that enables efficient prefilling in multi-agent inference by reusing KV-caches and aligning cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM estimates and adjusts KV-caches for shared content by referencing a pool of cached examples—termed *anchors*—that store observed cache deviations under varying prefixes. The anchor pool is maintained and updated online, allowing dynamic adaptation to distinct user requests and context structures. KVCOMM achieves over 70% reuse rate across diverse multi- agent workloads, including retrieval-augmented generation, math reasoning, and collaborative coding tasks, all without quality degradation. Particularly, when each fully-connected agent receives 1K input tokens with 512 prefix tokens and 512 output tokens under a five-agent setting, KVCOMM achieves up to 7.8× speedup compared to the standard prefill pipeline, reducing TTFT from ∼430ms to ∼55ms. Code is available at https://github.com/FastMAS/KVCOMM.",
      "arxiv_url": "https://openreview.net/forum?id=yGOytgjurF",
      "pdf_url": "https://openreview.net/pdf/81561154949bf17e7f12ee6dc0485c10a2415686.pdf",
      "primary_category": "LLM-based Multi-agent System; KV-cache reuse; Multi-context Agent Communication",
      "categories": [
        "LLM-based Multi-agent System; KV-cache reuse; Multi-context Agent Communication"
      ],
      "tags": [
        "LLM",
        "RAG",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "hTGqC1h8Ig",
      "title": "Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs",
      "authors": [
        "Yifan Wei",
        "Xiaoyan Yu",
        "Tengfei Pan",
        "Angsheng Li",
        "Li Du"
      ],
      "abstract": "Large language models (LLMs) have achieved unprecedented performance by leveraging vast pretraining corpora, yet their performance remains suboptimal in knowledge-intensive domains such as medicine and scientific research, where high factual precision is required.\nWhile synthetic data provides a promising avenue for augmenting domain knowledge, \nexisting methods frequently generate redundant samples that do not align with the model’s true knowledge gaps. \nTo overcome this limitation, \nwe propose a novel Structural Entropy-guided Knowledge Navigator (SENATOR) framework that addresses the intrinsic knowledge deficiencies of LLMs. \nOur approach employs the Structure Entropy (SE) metric to quantify uncertainty along knowledge graph paths and leverages Monte Carlo Tree Search (MCTS) to selectively explore regions where the model lacks domain-specific knowledge. \nGuided by these insights, the framework generates targeted synthetic data for supervised fine-tuning, enabling continuous self-improvement. \nExperimental results on LLaMA-3 and Qwen2 across multiple domain-specific benchmarks show that SENATOR effectively detects and repairs knowledge deficiencies, achieving notable performance improvements.",
      "arxiv_url": "https://openreview.net/forum?id=hTGqC1h8Ig",
      "pdf_url": "https://openreview.net/pdf/9d7794ec03b519d23816b2e75862acc50acda2da.pdf",
      "primary_category": "Synthetic Data Generation, llm-agents, self-inprovement",
      "categories": [
        "Synthetic Data Generation",
        "llm-agents",
        "self-inprovement",
        "MCTS"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "To7Rs2wsTd",
      "title": "VideoLucy: Deep Memory Backtracking for Long Video Understanding",
      "authors": [
        "Jialong Zuo",
        "Yongtai Deng",
        "Lingdong Kong",
        "Jingkang Yang",
        "Rui Jin",
        "Yiwei Zhang",
        "Nong Sang",
        "Liang Pan",
        "Ziwei Liu",
        "Changxin Gao"
      ],
      "abstract": "Recent studies have shown that agent-based systems leveraging large language models (LLMs) for key information retrieval and integration have emerged as a promising approach for long video understanding. However, these systems face two major challenges. First, they typically perform modeling and reasoning on individual frames, struggling to capture the temporal context of consecutive frames. Second, to reduce the cost of dense frame-level captioning, they adopt sparse frame sampling, which risks discarding crucial information. To overcome these limitations, we propose VideoLucy, a deep memory backtracking framework for long video understanding. Inspired by the human recollection process from coarse to fine, VideoLucy employs a hierarchical memory structure with progressive granularity. This structure explicitly defines the detail level and temporal scope of memory at different hierarchical depths. Through an agent-based iterative backtracking mechanism, VideoLucy systematically mines video-wide, question-relevant deep memories until sufficient information is gathered to provide a confident answer. This design enables effective temporal understanding of consecutive frames while preserving critical details. In addition, we introduce EgoMem, a new benchmark for long video understanding. EgoMem is designed to comprehensively evaluate a model's ability to understand complex events that unfold over time and capture fine-grained details in extremely long videos. Extensive experiments demonstrate the superiority of VideoLucy. Built on open-source models, VideoLucy significantly outperforms state-of-the-art methods on multiple long video understanding benchmarks, achieving performance even surpassing the latest proprietary models such as GPT-4o. Our code and dataset will be made publicly available.",
      "arxiv_url": "https://openreview.net/forum?id=To7Rs2wsTd",
      "pdf_url": "https://openreview.net/pdf/161829c4b6a3e37d14c4d9cdf381480eef4aeab5.pdf",
      "primary_category": "Long Video Understanding, Agent-based System",
      "categories": [
        "Long Video Understanding",
        "Agent-based System"
      ],
      "tags": [
        "LLM",
        "Information Retrieval"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "Tf9eoTIIjh",
      "title": "Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization",
      "authors": [
        "Bowei He",
        "Lihao Yin",
        "Huiling Zhen",
        "Shuqi LIU",
        "Han Wu",
        "Xiaokun Zhang",
        "Mingxuan Yuan",
        "Chen Ma"
      ],
      "abstract": "Post-training compression has been a widely employed approach to scale down large language model (LLM) and facilitate efficient inference. In various proposed compression methods, including pruning and quantization, calibration data plays a vital role by informing the weight importance and activation dynamic ranges. However, how calibration data impacts the LLM capability after compression is less explored. Few of the existing works, though recognizing the significance of this study, only investigate the language modeling or commonsense reasoning performance degradation from limited angles, like the data sources or sample amounts. More systematic research is still needed to examine the impacts on  different LLM capabilities in terms of compositional properties and domain correspondence of calibration data. In this work, we aim at bridging this gap and further analyze underlying influencing mechanisms from the activation pattern perspective. Especially, we explore the calibration data's impacts on high-level complex reasoning capabilities, like math problem solving and code generation. Delving into the underlying mechanism, we find that the representativeness and diversity in activation space more fundamentally determine the quality of calibration data. Finally, we propose a calibration data curation framework based on such observations and analysis, enhancing the performance of existing post-training compression methods on preserving critical LLM capabilities. Our code is provided in [Link](https://github.com/BokwaiHo/COLA.git).",
      "arxiv_url": "https://openreview.net/forum?id=Tf9eoTIIjh",
      "pdf_url": "https://openreview.net/pdf/451c6f73dc2b0953cd5c2ad72ff3e1cd9bca4c41.pdf",
      "primary_category": "Calibration Data, Large Language Model, Model Compression",
      "categories": [
        "Calibration Data",
        "Large Language Model",
        "Model Compression",
        "Capability Preservation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "f4pvPNf9ox",
      "title": "MURKA: Multi-Reward Reinforcement Learning with Knowledge Alignment for Optimization Tasks",
      "authors": [
        "Wantong Xie",
        "Yi-Xiang Hu",
        "Jieyang Xu",
        "Feng Wu",
        "Xiangyang Li"
      ],
      "abstract": "Optimization plays a central role in Operations Research (OR) and numerous industrial applications, yet automating the end-to-end process of translating natural language descriptions into executable optimization programs remains a formidable challenge. While recent efforts have applied Large Language Models (LLMs) to this task, existing approaches are hindered by high inference costs, limited robustness across domains, and weak verification mechanisms. In this work, we propose MURKA, a reinforcement learning and knowledge distillation-based framework that enhances LLM-driven optimization modeling via collaborative agent alignment. MURKA orchestrates three specialized agents---Extractor, Solver, and Checker---to achieve accurate problem understanding, robust formulation, and verifiable execution. The Extractor is trained using group relative policy optimization with a composite reward function that incorporates semantic correctness and execution fidelity. The Solver benefits from knowledge distillation from a powerful teacher model, yielding structurally valid and executable formulations in AMPL. The Checker iteratively verifies solution correctness via solver feedback. \nWe validate MURKA's generalizability through extensive experiments across diverse OR benchmarks, demonstrating its robustness and scalability.\nExperimental results on eight diverse OR benchmarks, including NLP4LP, ComplexOR, and NL4Opt, demonstrate that MURKA, built on the LLaMa3-8B backbone, achieves a 5.9\\% absolute improvement in solution accuracy and a 5.1\\% increase in execution success rate compared to leading baselines. These results establish MURKA as an effective and scalable paradigm for LLM-driven optimization, with strong potential for deployment in real-world OR applications.",
      "arxiv_url": "https://openreview.net/forum?id=f4pvPNf9ox",
      "pdf_url": "https://openreview.net/pdf/dbf84e2669183c96fe40caac76d12d8331acc0e2.pdf",
      "primary_category": "Large Language Models, Reinforcement Learning, Optimization Modeling",
      "categories": [
        "Large Language Models",
        "Reinforcement Learning",
        "Optimization Modeling",
        "Multi-Agent Systems",
        "Knowledge Distillation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "3naHyE5klE",
      "title": "Revolutionizing Training-Free NAS: Towards Efficient Automatic Proxy Discovery via Large Language Models",
      "authors": [
        "Haidong Kang",
        "Lihong Lin",
        "Hanling Wang"
      ],
      "abstract": "The success of computer vision tasks is mainly attributed to the architectural design of neural networks. This highlights the need to automatically design high-performance architectures via Neural Architecture Search (NAS). To accelerate the search process, training-free NAS is proposed, which aims to search high-performance architectures at initialization via zero-cost proxies (ZCPs). However, existing zero-cost proxies heavily rely on manual design, which is often labor-intensive and requires extensive expert knowledge. In addition, these crafted proxies often suffer from poor correlation with final model performance and high computational complexity, severely limiting NAS efficiency in real-world applications. To address those issues, this paper proposes a novel Large Language Models (LLMs)-driven $\\underline{A}$utomatic $\\underline{P}$roxy $\\underline{D}$iscovery ($\\textbf{APD}$) framework, which revolutionizes the design paradigm of ZCPs by leveraging LLMs to automatically discover optimal ZCPs for Training-Free NAS. Moreover, we utilize actor-critic based reinforcement learning to optimize prompts, enabling to generate better ZCPs in the next generation. We conduct extensive experiments on mainstream NAS benchmarks, demonstrating APD excels in both performance and efficiency. Besides, we firmly believe that our APD will dramatically benefit the deep learning community through providing novel paradigm of design algorithms via LLMs.",
      "arxiv_url": "https://openreview.net/forum?id=3naHyE5klE",
      "pdf_url": "https://openreview.net/pdf/0b0593977bd8bee631d9e36b8dadbf79f485a92c.pdf",
      "primary_category": "Large Language Models, Training-Free NAS, Automatic Proxy Discovery",
      "categories": [
        "Large Language Models",
        "Training-Free NAS",
        "Automatic Proxy Discovery"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "PjbpL4brUb",
      "title": "DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs",
      "authors": [
        "Ruokai Yin",
        "Yuhang Li",
        "Donghyun Lee",
        "Priyadarshini Panda"
      ],
      "abstract": "Large language models (LLMs) deliver strong performance but are difficult to deploy due to high memory and compute costs. While pruning reduces these demands, most methods ignore activation sparsity observed at runtime. We reinterpret activation sparsity as dynamic structured weight sparsity and propose DuoGPT, a unified framework that constructs dual-sparse (spMspV) workloads by combining unstructured weight pruning with activation sparsity. To preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with activation-aware calibration and introduce output residuals from the dense model as correction terms. We further optimize the solution for efficient GPU execution, enabling scalability to billion-parameter LLMs. Evaluations on LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured pruning methods by up to 9.17\\% accuracy at an iso-speedup of 1.39$\\times$ compared to the baseline dense model. Code is available at GitHub.",
      "arxiv_url": "https://openreview.net/forum?id=PjbpL4brUb",
      "pdf_url": "https://openreview.net/pdf/6827bccec4b62c1789e879b8f3c71d807f2a2f02.pdf",
      "primary_category": "Weight Pruning, Dual-sparsity, Activation Sparsity",
      "categories": [
        "Weight Pruning",
        "Dual-sparsity",
        "Activation Sparsity",
        "Efficient Large Language Models",
        "Post-training Calibration",
        "Model Compression"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "aEAbRPXV37",
      "title": "Majority of the Bests: Improving Best-of-N via Bootstrapping",
      "authors": [
        "Amin Rakhsha",
        "Kanika Madan",
        "Tianyu Zhang",
        "Amir-massoud Farahmand",
        "Amir Khasahmadi"
      ],
      "abstract": "Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN’s outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms.",
      "arxiv_url": "https://openreview.net/forum?id=aEAbRPXV37",
      "pdf_url": "https://openreview.net/pdf/ca9ebb18a39a760e5a9f060777deae8bcd6f08de.pdf",
      "primary_category": "llms, test-time compute, best-of-n",
      "categories": [
        "llms",
        "test-time compute",
        "best-of-n",
        "bootstrap",
        "inference-time computation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "HbTxc6U1fO",
      "title": "Video World Models with Long-term Spatial Memory",
      "authors": [
        "Tong Wu",
        "Shuai Yang",
        "Ryan Po",
        "Yinghao Xu",
        "Ziwei Liu",
        "Dahua Lin",
        "Gordon Wetzstein"
      ],
      "abstract": "Emerging world models autoregressively generate video frames in response to actions, such as camera movements and text prompts, among other control signals. Due to limited temporal context window sizes, these models often struggle to maintain scene consistency during revisits, leading to severe forgetting of previously generated environments. Inspired by the mechanisms of human memory, we introduce a novel framework to enhancing long-term consistency of video world models through a geometry-grounded long-term spatial memory. Our framework includes mechanisms to store and retrieve information from the long-term spatial memory and we curate custom datasets to train and evaluate world models with explicitly stored 3D memory mechanisms. Our evaluations show improved quality, consistency, and context length compared to relevant baselines, paving the way towards long-term consistent world generation.",
      "arxiv_url": "https://openreview.net/forum?id=HbTxc6U1fO",
      "pdf_url": "https://openreview.net/pdf/e578989914ac5f7dc5e642668aea0a4453f5591b.pdf",
      "primary_category": "diffusion model, world model",
      "categories": [
        "diffusion model",
        "world model"
      ],
      "tags": [
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1v0ULVJOZ9",
      "title": "RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility",
      "authors": [
        "Haoyu He",
        "Haozheng Luo",
        "Yan Chen",
        "Qi Wang"
      ],
      "abstract": "Predicting human mobility is inherently challenging due to  complex long-range dependencies and multi-scale periodic behaviors. To address this, we introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), a unified framework that leverages large language models (LLMs) as general-purpose spatio-temporal predictors and trajectory reasoners. Methodologically, RHYTHM employs temporal tokenization to partition each trajectory into daily segments and encode them as discrete tokens with hierarchical attention that captures both daily and weekly dependencies, thereby quadratically reducing the sequence length while preserving cyclical information. Additionally, we enrich token representations by adding pre-computed prompt embeddings for trajectory segments and prediction targets via a frozen LLM, and feeding these combined embeddings back into the LLM backbone to capture complex interdependencies. Computationally, RHYTHM keeps the pretrained LLM backbone frozen, yielding faster training and lower memory usage. We evaluate our model against state-of-the-art methods using three real-world datasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a 5.0% increase on weekends, and a 24.6% reduction in training time. Code is publicly available at https://github.com/he-h/rhythm.",
      "arxiv_url": "https://openreview.net/forum?id=1v0ULVJOZ9",
      "pdf_url": "https://openreview.net/pdf/60a315332ed5827b8055e70ce1d5519c0d6132fb.pdf",
      "primary_category": "Human mobility prediction, large language models (LLMs), spatio-temporal forecasting",
      "categories": [
        "Human mobility prediction",
        "large language models (LLMs)",
        "spatio-temporal forecasting"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "cy6MGBwToV",
      "title": "FedRW: Efficient Privacy-Preserving Data Reweighting for Enhancing Federated Learning of Language Models",
      "authors": [
        "Pukang Ye",
        "Junwei Luo",
        "Jiachen Shen",
        "Saipan Zhou",
        "Shangmin Dou",
        "Zhenfu Cao",
        "Hanzhe Yao",
        "Xiaolei Dong",
        "Yunbo Yang"
      ],
      "abstract": "Data duplication within large-scale corpora often impedes large language models' (LLMs) performance and privacy. In privacy-concerned federated learning scenarios, conventional deduplication methods typically rely on trusted third parties to perform uniform deletion, risking loss of informative samples while introducing privacy vulnerabilities. To address these gaps, we propose Federated ReWeighting (FedRW), the first privacy-preserving framework, to the best of our knowledge, that performs soft deduplication via sample reweighting instead of deletion in federated LLM training, without assuming a trusted third party. At its core, FedRW proposes a secure, frequency-aware reweighting protocol through secure multi-party computation, coupled with a parallel orchestration strategy to ensure efficiency and scalability. During training, FedRW utilizes an adaptive reweighting mechanism with global sample frequencies to adjust individual loss contributions, effectively improving generalization and robustness. Empirical results demonstrate that FedRW outperforms the state-of-the-art method by achieving up to $28.78\\times$ speedup in preprocessing and approximately $11.42$\\% improvement in perplexity, while offering enhanced security guarantees. FedRW thus establishes a new paradigm for managing duplication in federated LLM training.",
      "arxiv_url": "https://openreview.net/forum?id=cy6MGBwToV",
      "pdf_url": "https://openreview.net/pdf/5e8830d6b39af1aa9c33e81d71c96f18ae837808.pdf",
      "primary_category": "LLM Privacy, Data Deduplication, Federated Learning",
      "categories": [
        "LLM Privacy",
        "Data Deduplication",
        "Federated Learning",
        "Secure Multi-Party Computation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "eCElREEUsr",
      "title": "GoT: Unleashing Reasoning Capability of MLLM for Visual Generation and Editing",
      "authors": [
        "Rongyao Fang",
        "Chengqi Duan",
        "Kun Wang",
        "Linjiang Huang",
        "Hao Li",
        "Hao Tian",
        "Shilin Yan",
        "Weihao Yu",
        "Xingyu Zeng",
        "Jifeng Dai",
        "Xihui Liu",
        "Hongsheng Li"
      ],
      "abstract": "Current image generation and editing methods primarily process textual prompts as direct inputs without explicit reasoning about visual composition or operational steps. We present Generation Chain-of-Thought (GoT), a novel paradigm that empowers a Multimodal Large Language Model (MLLM) to first generate an explicit, structured reasoning chain in natural language—detailing semantic relationships, object attributes, and, crucially, precise spatial coordinates—before any image synthesis occurs. This intermediate reasoning output directly guides the subsequent visual generation or editing process. This approach transforms conventional text-to-image generation and editing into a reasoning-guided framework that analyzes semantic relationships and spatial arrangements. We define the formulation of GoT and construct large-scale GoT datasets containing over \\textbf{9M} samples with detailed reasoning chains capturing semantic-spatial relationships. To leverage the advantages of GoT, we implement a unified framework that integrates Qwen2.5-VL for reasoning chain generation with an end-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance Module. Experiments show our GoT framework achieves excellent performance on both generation and editing tasks, with significant improvements over baselines. Additionally, our approach enables interactive visual generation, allowing users to explicitly modify reasoning steps for precise image adjustments. GoT pioneers a new direction for reasoning-driven visual generation and editing, producing images that better align with human intent. We will release our datasets and models to facilitate future research.",
      "arxiv_url": "https://openreview.net/forum?id=eCElREEUsr",
      "pdf_url": "https://openreview.net/pdf/88332ed6ca1dea81759b8d07d12b66ab6da63abf.pdf",
      "primary_category": "Reasoning-based Visual Generation and Editing, MLLM",
      "categories": [
        "Reasoning-based Visual Generation and Editing",
        "MLLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "e7jNIna1eP",
      "title": "NaDRO: Leveraging Dual-Reward Strategies for LLMs Training on Noisy Data",
      "authors": [
        "Haolong Qian",
        "Xianliang Yang",
        "Ling Zhang",
        "Lei Song",
        "Jiang Bian",
        "Chun Yuan"
      ],
      "abstract": "Group Relative Policy Optimization (GRPO) fine-tuning has demonstrated significant enhancements in reasoning tasks. However, it often relies on high quality labeled dataset, which is typically difficult to obtain. To address this challenge, we introduce \\textbf{N}oise-\\textbf{A}ware \\textbf{D}ual-\\textbf{R}eward \\textbf{O}ptimization (\\textbf{NaDRO}) to  effectively enhances the training of Large Language Models (LLMs) under noisy or ambiguous supervision. NaDRO operates through two key components: \\textbf{(1) Preference-based Outcome Reward (POR)},which makes a principled bias-variance tradeoff, reducing training variance by learning from robust preference rankings instead of overfitting to single-best estimates; and \\textbf{(2) Context Perception Reward (CPR) mechanism}, which ensures that LLMs conduct necessary qualitative assessment of the current problem state to foster deeper situational understanding prior to decision-making. To validate our approach in a realistic decision-making testbed, we model classic combinatorial optimization problems like the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) as Markov Decision Processes, generating training data via cost-limited exploration. Our results demonstrate that the fine-tuned Qwen 7B and Llama 3.1-8B models achieve statistically robust performance, significantly outperforming leading LLM baselines and standard fine-tuning methods on these complex benchmarks. Code is released at \\url{https://github.com/microsoft/HeurAgenix/tree/NaDRO}.",
      "arxiv_url": "https://openreview.net/forum?id=e7jNIna1eP",
      "pdf_url": "https://openreview.net/pdf/dd2f07983c5691dcf31358eee4d88de3e1c8f90b.pdf",
      "primary_category": "Large Language Models, Combinatorial Optimization, Preference Learning",
      "categories": [
        "Large Language Models",
        "Combinatorial Optimization",
        "Preference Learning",
        "Process Reward"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "7nTWoceJGK",
      "title": "Guard Me If You Know Me: Protecting Specific Face-Identity from Deepfakes",
      "authors": [
        "Kaiqing Lin",
        "Zhiyuan Yan",
        "Ke-Yue Zhang",
        "Li Hao",
        "Yue Zhou",
        "Yuzhen Lin",
        "Weixiang Li",
        "Taiping Yao",
        "Shouhong Ding",
        "Bin Li"
      ],
      "abstract": "Securing personal identity against deepfake attacks is increasingly critical in the digital age, especially for celebrities and political figures whose faces are easily accessible and frequently targeted.\nMost existing deepfake detection methods focus on general-purpose scenarios and often ignore the valuable prior knowledge of known facial identities, e.g., \"VIP individuals\" whose authentic facial data are already available. \nIn this paper, we propose **VIPGuard**, a unified multimodal framework designed to capture fine-grained and comprehensive facial representations of a given identity, compare them against potentially fake or similar-looking faces, and reason over these comparisons to make accurate and explainable predictions.\nSpecifically, our framework consists of three main stages. First, we fine-tune a multimodal large language model (MLLM) to learn detailed and structural facial attributes. \nSecond, we perform identity-level discriminative learning to enable the model to distinguish subtle differences between highly similar faces, including real and fake variations. Finally, we introduce user-specific customization, where we model the unique characteristics of the target face identity and perform semantic reasoning via MLLM to enable personalized and explainable deepfake detection.\nOur framework shows clear advantages over previous detection works, where traditional detectors mainly rely on low-level visual cues and provide no human-understandable explanations, while other MLLM-based models often lack a detailed understanding of specific face identities.\nTo facilitate the evaluation of our method, we build a comprehensive identity-aware benchmark called **VIPBench** for personalized deepfake detection, involving the latest 7 face-swapping and 7 entire face synthesis techniques for generation. \nExtensive experiments show that our model outperforms existing methods in both detection and explanation.\nThe code is available at https://github.com/KQL11/VIPGuard .",
      "arxiv_url": "https://openreview.net/forum?id=7nTWoceJGK",
      "pdf_url": "https://openreview.net/pdf/752fe685c62910243f7cf195e6f366d2b1240762.pdf",
      "primary_category": "Personalized Deepfake Detection, Multi-media Forensic, Face Recognition",
      "categories": [
        "Personalized Deepfake Detection",
        "Multi-media Forensic",
        "Face Recognition",
        "MLLMs"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "pXXKKBx8G0",
      "title": "CoIDO: Efficient Data Selection for Visual Instruction Tuning via Coupled Importance-Diversity Optimization",
      "authors": [
        "Yichen Yan",
        "Ming Zhong",
        "Qi Zhu",
        "Xiaoling Gu",
        "Jinpeng Chen",
        "Huan Li"
      ],
      "abstract": "Multimodal large language models (MLLMs) rely heavily on instruction tuning to align vision and language capabilities, yet the computational cost of training on large-scale datasets remains a major bottleneck. Existing data selection methods aim to mitigate this by selecting important and diverse subsets, but they often suffer from two critical drawbacks: high computational overhead from processing the entire dataset and suboptimal data selection due to separate treatment of importance and diversity.\n\nWe introduce CoIDO, a novel dual-objective framework that jointly optimizes data importance and diversity to overcome these challenges. Unlike existing approaches that require costly evaluations across the whole dataset, CoIDO employs a lightweight plug-in scorer. This scorer is trained on just a small random sample of data to learn the distribution of the candidate set, drastically reducing computational demands. By leveraging a homoscedastic uncertainty-based formulation, CoIDO effectively balances importance and diversity during training, enabling the scorer to assign CoIDO scores to all data points. This unified scoring approach allows for direct ranking and selection of the most valuable subsets, completely bypassing the need for specialized algorithms.\n\nIn our experiments, we trained the CoIDO Scorer using only 20% of randomly sampled data. Once trained, CoIDO was applied to the entire dataset to select a 20% subset for instruction tuning. On the widely used LLaVA-1.5-7B model across ten downstream tasks, this selected subset achieved an impressive 98.2% of the performance of full-data fine-tuning, on average. Moreover, CoIDO outperforms all competitors in terms of both efficiency (lowest training FLOPs) and aggregated accuracy. Our code is available at: https://github.com/SuDIS-ZJU/CoIDO",
      "arxiv_url": "https://openreview.net/forum?id=pXXKKBx8G0",
      "pdf_url": "https://openreview.net/pdf/5c537ac88c42105f6b60e30777e3460d66f749e4.pdf",
      "primary_category": "Large Language Model, Visual Instruction Tuning, Data Selection",
      "categories": [
        "Large Language Model",
        "Visual Instruction Tuning",
        "Data Selection"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0VDmWjW456",
      "title": "L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models",
      "authors": [
        "Xiaohao Liu",
        "Xiaobo Xia",
        "Weixiang Zhao",
        "Manyi Zhang",
        "Xianzhi Yu",
        "Xiu Su",
        "Shuo Yang",
        "See-Kiong Ng",
        "Tat-Seng Chua"
      ],
      "abstract": "Large language models (LLMs) have achieved notable progress. Despite their success, next-token prediction (NTP), the dominant method for LLM training and inference, is constrained in both contextual coverage and inference efficiency due to its inherently sequential process. To overcome these challenges, we propose leap multi-token prediction~(L-MTP), an innovative token prediction method that extends the capabilities of multi-token prediction (MTP) by introducing a leap-based mechanism. Unlike conventional MTP, which generates multiple tokens at adjacent positions, L-MTP strategically skips over intermediate tokens, predicting non-sequential ones in a single forward pass. This structured leap not only enhances the model's ability to capture long-range dependencies but also enables a decoding strategy specially optimized for non-sequential leap token generation, effectively accelerating inference. We theoretically demonstrate the benefit of L-MTP in improving inference efficiency. Experiments across diverse benchmarks validate its merit in boosting both LLM performance and inference speed. The source code is available at https://github.com/Xiaohao-Liu/L-MTP.",
      "arxiv_url": "https://openreview.net/forum?id=0VDmWjW456",
      "pdf_url": "https://openreview.net/pdf/8c15e4335df0d4cce7dc8ac9dbefc5257bbbcd83.pdf",
      "primary_category": "large language models, multi-token prediction, inference acceleration",
      "categories": [
        "large language models",
        "multi-token prediction",
        "inference acceleration"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "ZnrM5RGrgR",
      "title": "OptiScene: LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization",
      "authors": [
        "Yixuan Yang",
        "Zhen Luo",
        "Tongsheng Ding",
        "Junru Lu",
        "Mingqi Gao",
        "Jinyu Yang",
        "Victor Sanchez",
        "Feng Zheng"
      ],
      "abstract": "Automatic indoor layout generation has attracted increasing attention due to its potential in interior design, virtual environment construction, and embodied AI. Existing methods fall into two categories: prompt-driven approaches that leverage proprietary LLM services (e.g., GPT APIs), and learning-based methods trained on layout data upon diffusion-based models. Prompt-driven methods often suffer from spatial inconsistency and high computational costs, while learning-based methods are typically constrained by coarse relational graphs and limited datasets, restricting their generalization to diverse room categories. In this paper, we revisit LLM-based indoor layout generation and present 3D-SynthPlace, a large-scale dataset that combines synthetic layouts generated via a `GPT synthesize, Human inspect' pipeline, upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000 scenes, covering four common room types—bedroom, living room, kitchen, and bathroom—enriched with diverse objects and high-level spatial annotations. We further introduce OptiScene, a strong open-source LLM optimized for indoor layout generation, fine-tuned based on our 3D-SynthPlace dataset through our two-stage training. For the warum-up stage I, we adopt supervised fine-tuning (SFT), which is taught to first generate high-level spatial descriptions then conditionally predict concrete object placements. For the reinforcing stage II, to better align the generated layouts with human design preferences, we apply multi-turn direct preference optimization (DPO), which significantly improving layout quality and generation success rates. Extensive experiments demonstrate that OptiScene outperforms traditional prompt-driven and learning-based baselines. Moreover, OptiScene shows promising potential in interactive tasks such as scene editing and robot navigation, highlighting its applicability beyond static layout generation.",
      "arxiv_url": "https://openreview.net/forum?id=ZnrM5RGrgR",
      "pdf_url": "https://openreview.net/pdf/006d74d8622bdd923a664d247091cceaf995db43.pdf",
      "primary_category": "Indoor scene layout generation, LLM-based, DPO",
      "categories": [
        "Indoor scene layout generation",
        "LLM-based",
        "DPO",
        "3D",
        "Synthetic Data"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "U88JlpY0vR",
      "title": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning",
      "authors": [
        "Jinkun Hao",
        "Naifu Liang",
        "Zhen Luo",
        "Xudong XU",
        "Weipeng Zhong",
        "Ran Yi",
        "Yichen Jin",
        "Zhaoyang Lyu",
        "Feng Zheng",
        "Lizhuang Ma",
        "Jiangmiao Pang"
      ],
      "abstract": "The ability of robots to interpret human instructions and execute manipulation tasks necessitates the availability of task-relevant tabletop scenes for training. However, traditional methods for creating these scenes rely on time-consuming manual layout design or purely randomized layouts, which are limited in terms of plausibility or alignment with the tasks. In this paper, we formulate a novel task, namely task-oriented tabletop scene generation, which poses significant challenges due to the substantial gap between high-level task instructions and the tabletop scenes. To support research on such a challenging task, we introduce \\textbf{MesaTask-10K}, a large-scale dataset comprising approximately 10,700 synthetic tabletop scenes with \\emph{manually crafted layouts} that ensure realistic layouts and intricate inter-object relations. To bridge the gap between tasks and scenes, we propose a \\textbf{Spatial Reasoning Chain} that decomposes the generation process into object inference, spatial interrelation reasoning, and scene graph construction for the final 3D layout. We present \\textbf{MesaTask}, an LLM-based framework that utilizes this reasoning chain and is further enhanced with DPO algorithms to generate physically plausible tabletop scenes that align well with given task descriptions. Exhaustive experiments demonstrate the superior performance of MesaTask compared to baselines in generating task-conforming tabletop scenes with realistic layouts.",
      "arxiv_url": "https://openreview.net/forum?id=U88JlpY0vR",
      "pdf_url": "https://openreview.net/pdf/094b02e3ef77b10aa88b9a149588ac30f57b6cee.pdf",
      "primary_category": "3D scene generation, 3D, Tabletop Scene Generation",
      "categories": [
        "3D scene generation",
        "3D",
        "Tabletop Scene Generation",
        "Tabletop Dataset"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "E8adS5srds",
      "title": "Anchored Diffusion Language Model",
      "authors": [
        "Litu Rout",
        "Constantine Caramanis",
        "Sanjay Shakkottai"
      ],
      "abstract": "Diffusion Language Models (DLMs) promise parallel generation and bidirectional context, yet they underperform autoregressive (AR) models in both *likelihood modeling* and *generated text quality*. We identify that this performance gap arises when important tokens (e.g., key words or low-frequency words that anchor a sentence) are masked early in the forward process, limiting contextual information for accurate reconstruction. To address this, we introduce the *Anchored Diffusion Language Model (ADLM)*, a novel two-stage framework that first predicts distributions over important tokens via an anchor network, and then predicts the likelihoods of missing tokens conditioned on the anchored predictions. ADLM significantly improves test perplexity on LM1B and OpenWebText, achieving up to 25.4\\% gains over prior DLMs, and narrows the gap with strong AR baselines. It also achieves state-of-the-art zero-shot generalization across seven benchmarks and surpasses AR models in MAUVE score, which marks the first time a DLM generates better human-like text than an AR model. Theoretically, we derive an Anchored Negative Evidence Lower Bound (ANELBO) objective and show that anchoring improves sample complexity and likelihood modeling. Beyond diffusion, anchoring boosts performance in AR models and enhances reasoning in math and logic tasks, outperforming existing chain-of-thought approaches. Please see our project page: [anchored-diffusion-llm.github.io](https://anchored-diffusion-llm.github.io/) for code and demo.",
      "arxiv_url": "https://openreview.net/forum?id=E8adS5srds",
      "pdf_url": "https://openreview.net/pdf/3382d1c97cabea9ba146422066b3f69a4a2fd5f3.pdf",
      "primary_category": "discrete diffusion, diffusion language models, large-language models",
      "categories": [
        "discrete diffusion",
        "diffusion language models",
        "large-language models",
        "generative modeling",
        "planning",
        "reasoning",
        "chain-of-thought"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "4ULtNYHc5T",
      "title": "Exploring Tradeoffs through Mode Connectivity for Multi-Task Learning",
      "authors": [
        "Zhipeng Zhou",
        "Ziqiao Meng",
        "Pengcheng Wu",
        "Peilin Zhao",
        "Chunyan Miao"
      ],
      "abstract": "Nowadays deep models are required to be versatile due to the increasing realistic needs. Multi-task learning (MTL) offers an efficient way for this purpose to learn multiple tasks simultaneously with a single model. However, prior MTL solutions often focus on resolving conflicts and imbalances during optimization, which may not outperform simple linear scalarization strategies~\\citep{xin2022current}. Instead of altering the optimization trajectory, this paper leverages mode connectivity to efficiently approach the Pareto front and identify the desired trade-off point. Unlike Pareto Front Learning (PFL), which aims to align with the entire Pareto front, we focus on effectively and efficiently exploring optimal trade-offs. However, three challenges persist: (1) the low-loss path can neither fully traverse trade-offs nor align with user preference due to its randomness, (2) commonly adopted Bézier curves in mode connectivity are ill-suited to navigating the complex loss landscapes of deep models, and (3) poor scalability to large-scale task scenarios. To address these challenges, we adopt non-uniform rational B-Splines (NURBS) to model mode connectivity, allowing for more flexible and precise curve optimization. Additionally, we introduce an order-aware objective to explore task loss trade-offs and employ a task grouping strategy to enhance scalability under massive task scenarios. Extensive experiments on key MTL datasets demonstrate that our proposed method, *EXTRA* (EXplore TRAde-offs), effectively identifies the desired point on the Pareto front and achieves state-of-the-art performance. *EXTRA* is also validated as a plug-and-play solution for mainstream MTL approaches.",
      "arxiv_url": "https://openreview.net/forum?id=4ULtNYHc5T",
      "pdf_url": "https://openreview.net/pdf/7270729acb82c91739635deec06645f8f9d79115.pdf",
      "primary_category": "Multi-Task Learning, Mode Connectivity",
      "categories": [
        "Multi-Task Learning",
        "Mode Connectivity"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nf8PKQKtl2",
      "title": "NavBench: Probing Multimodal Large Language Models for Embodied Navigation",
      "authors": [
        "Yanyuan Qiao",
        "Haodong Hong",
        "Wenqi Lyu",
        "Dong An",
        "Siqi Zhang",
        "Yutong Xie",
        "Xinyu Wang",
        "Qi Wu"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated strong generalization in vision-language tasks, yet their ability to understand and act within embodied environments remains underexplored. We present NavBench, a benchmark to evaluate the embodied navigation capabilities of MLLMs under zero-shot settings. NavBench consists of two components: (1) navigation comprehension, assessed through three cognitively grounded tasks including global instruction alignment, temporal progress estimation, and local observation-action reasoning, covering 3,200 question-answer pairs; and (2) step-by-step execution in 432 episodes across 72 indoor scenes, stratified by spatial, cognitive, and execution complexity. To support real-world deployment, we introduce a pipeline that converts MLLMs' outputs into robotic actions. We evaluate both proprietary and open-source models, finding that GPT-4o performs well across tasks, while lighter open-source models succeed in simpler cases. Results also show that models with higher comprehension scores tend to achieve better execution performance. Providing map-based context improves decision accuracy, especially in medium-difficulty scenarios. However, most models struggle with temporal understanding, particularly in estimating progress during navigation, which may pose a key challenge.",
      "arxiv_url": "https://openreview.net/forum?id=nf8PKQKtl2",
      "pdf_url": "https://openreview.net/pdf/1ef1a313c6a3eea3eea8cfe4ac568866df673dec.pdf",
      "primary_category": "Embodied Navigation, Multimodal Large Language Models",
      "categories": [
        "Embodied Navigation",
        "Multimodal Large Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tXqLxHlb8Z",
      "title": "AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems",
      "authors": [
        "Yingxuan Yang",
        "Huacan Chai",
        "Shuai Shao",
        "Yuanyi Song",
        "Siyuan Qi",
        "Renting Rui",
        "Weinan Zhang"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of multi-agent systems, where multiple LLM-based agents collaborate to solve complex tasks.   However, existing systems predominantly rely on centralized coordination, which introduces scalability bottlenecks, limits adaptability, and creates single points of failure.   Additionally, concerns over privacy and proprietary knowledge sharing hinder cross-organizational collaboration, leading to siloed expertise.   To address these challenges, we propose AgentNet, a decentralized, Retrieval-Augmented Generation (RAG)-based framework that enables LLM-based agents to autonomously evolve their capabilities and collaborate efficiently in a Directed Acyclic Graph (DAG)-structured network.   Unlike traditional multi-agent systems that depend on static role assignments or centralized control, AgentNet allows agents to specialize dynamically, adjust their connectivity, and route tasks without relying on predefined workflows.\nAgentNet’s core design is built upon several key innovations: (1) Fully Decentralized Paradigm: Removing the central orchestrator, allowing agents to coordinate and specialize autonomously, fostering fault tolerance and emergent collective intelligence. (2) Dynamically Evolving Graph Topology: Real-time adaptation of agent connections based on task demands, ensuring scalability and resilience.\n(3) Adaptive Learning for Expertise Refinement: A retrieval-based memory system that enables agents to continuously update and refine their specialized skills.\nBy eliminating centralized control, AgentNet enhances fault tolerance, promotes scalable specialization, and enables privacy-preserving collaboration across organizations.      Through decentralized coordination and minimal data exchange, agents can leverage diverse knowledge sources while safeguarding sensitive information.      Experimental results demonstrate that AgentNet outperforms traditional centralized multi-agent systems, significantly improving efficiency, adaptability, and scalability in dynamic environments, making it a promising foundation for next-generation autonomous, privacy-respecting multi-agent ecosystems.",
      "arxiv_url": "https://openreview.net/forum?id=tXqLxHlb8Z",
      "pdf_url": "https://openreview.net/pdf/38bdf6e7191adba7391b1fde1ad37e27887b2bac.pdf",
      "primary_category": "LLM Agent, MAS, RAG",
      "categories": [
        "LLM Agent",
        "MAS",
        "RAG",
        "DAG"
      ],
      "tags": [
        "LLM",
        "RAG",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "h60y6zlPyl",
      "title": "TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets",
      "authors": [
        "Yuzhe YANG",
        "Yifei Zhang",
        "Minghao Wu",
        "Kaidi Zhang",
        "Yunmiao Zhang",
        "Honghai Yu",
        "Yan Hu",
        "Benyou Wang"
      ],
      "abstract": "The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.",
      "arxiv_url": "https://openreview.net/forum?id=h60y6zlPyl",
      "pdf_url": "https://openreview.net/pdf/5b6ec77501598abe4058a2428f83582bd78bc0e9.pdf",
      "primary_category": "LLM Agent, Multi-Agent System, Human-Behavior",
      "categories": [
        "LLM Agent",
        "Multi-Agent System",
        "Human-Behavior",
        "Computational Social Science",
        "Computational Finance",
        "Market Impact",
        "Financial Market Simulation"
      ],
      "tags": [
        "LLM",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "btm5Z5Vu8G",
      "title": "ALINE: Joint Amortization for Bayesian Inference and Active Data Acquisition",
      "authors": [
        "Daolang Huang",
        "Xinyi Wen",
        "Ayush Bharti",
        "Samuel Kaski",
        "Luigi Acerbi"
      ],
      "abstract": "Many critical applications, from autonomous scientific discovery to personalized medicine, demand systems that can both strategically acquire the most informative data and instantaneously perform inference based upon it. While amortized methods for Bayesian inference and experimental design offer part of the solution, neither approach is optimal in the most general and challenging task, where new data needs to be collected for instant inference. To tackle this issue, we introduce the Amortized Active Learning and Inference Engine (ALINE), a unified framework for amortized Bayesian inference and active data acquisition. ALINE leverages a transformer architecture trained via reinforcement learning with a reward based on self-estimated information gain provided by its own integrated inference component. This allows it to strategically query informative data points while simultaneously refining its predictions. Moreover, ALINE can selectively direct its querying strategy towards specific subsets of model parameters or designated predictive tasks, optimizing for posterior estimation, data prediction, or a mixture thereof. Empirical results on regression-based active learning, classical Bayesian experimental design benchmarks, and a psychometric model with selectively targeted parameters demonstrate that ALINE delivers both instant and accurate inference along with efficient selection of informative points.",
      "arxiv_url": "https://openreview.net/forum?id=btm5Z5Vu8G",
      "pdf_url": "https://openreview.net/pdf/48fe0e4bc3fa5b604da39cefa4d294425652fca6.pdf",
      "primary_category": "Amortized inference, Bayesian experimental design, Active learning",
      "categories": [
        "Amortized inference",
        "Bayesian experimental design",
        "Active learning",
        "Bayesian inference",
        "Neural processes"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "vMpvtSmtXY",
      "title": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning",
      "authors": [
        "Xinyan Chen",
        "Renrui Zhang",
        "Dongzhi Jiang",
        "Aojun Zhou",
        "Shilin Yan",
        "Weifeng Lin",
        "Hongsheng Li"
      ],
      "abstract": "Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical CoT. However, they face three key limitations for math problem-solving: *reliance on coarse-grained box-shaped image regions, limited perception of vision encoders on math content, and dependence on external capabilities for visual modification*.  In this paper, we propose **MINT-CoT**, introducing **M**athematical **IN**terleaved **T**okens for **C**hain-**o**f-**T**hought visual reasoning. MINT-CoT adaptively interleaves relevant visual tokens into textual reasoning steps via an Interleave Token, which dynamically selects visual regions of any shapes within math figures. To empower this capability, we construct the MINT-CoT dataset, containing 54K mathematical problems aligning each reasoning step with visual regions at the token level, accompanied by a rigorous data generation pipeline. We further present a three-stage MINT-CoT training strategy, progressively combining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which derives our MINT-CoT-7B model. Extensive experiments demonstrate the effectiveness of our method for effective visual interleaved reasoning in mathematical domains, where MINT-CoT-7B outperforms the baseline model by +34.08% on MathVista and +28.78% on GeoQA, respectively.",
      "arxiv_url": "https://openreview.net/forum?id=vMpvtSmtXY",
      "pdf_url": "https://openreview.net/pdf/82857fa25f2da29b5e82ec3b3093429dd593aab4.pdf",
      "primary_category": "Multimodal Large Laguage Models, Chain-of-Thought, Mathematics",
      "categories": [
        "Multimodal Large Laguage Models",
        "Chain-of-Thought",
        "Mathematics"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "RbdLnwEEjk",
      "title": "Enhancing LLM Watermark Resilience Against Both Scrubbing and  Spoofing Attacks",
      "authors": [
        "Huanming Shen",
        "Baizhou Huang",
        "Xiaojun Wan"
      ],
      "abstract": "Watermarking is widely regarded as a promising defense against the misuse of large language models (LLMs); however, existing methods are fundamentally constrained by their vulnerability to scrubbing and spoofing attacks. This vulnerability stems from an inherent trade-off governed by watermark window size: smaller windows resist scrubbing better but are easier to reverse-engineer, enabling low-cost statistics-based spoofing attacks. This work expands the trade-off boundary by introducing a novel mechanism, equivalent texture keys, where multiple tokens within a watermark window can independently support the detection.  Based on the redundancy, we propose a watermark scheme with **S**ub-vocabulary decomposed **E**quivalent t**E**xture **K**ey (**SEEK**). SEEK achieves a Pareto improvement, enhancing robustness to scrubbing attacks without sacrificing resistance to spoofing.",
      "arxiv_url": "https://openreview.net/forum?id=RbdLnwEEjk",
      "pdf_url": "https://openreview.net/pdf/71e04f986b85cf8e63c1965c0b494639a22ac089.pdf",
      "primary_category": "machine learning security, llm watermarking",
      "categories": [
        "machine learning security",
        "llm watermarking"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "quKHZ3fcgx",
      "title": "Poison as Cure: Visual Noise for Mitigating Object Hallucinations in LVMs",
      "authors": [
        "Kejia Zhang",
        "Keda TAO",
        "Jiasheng Tang",
        "Huan Wang"
      ],
      "abstract": "Large vision-language models (LVMs) extend large language models (LLMs) with visual perception capabilities, enabling them to process and interpret visual information. A major challenge compromising their reliability is object hallucination that LVMs may generate plausible but factually inaccurate information. We propose a novel \\textit{visual adversarial perturbation (VAP)} method to mitigate this hallucination issue. VAP alleviates LVM hallucination by applying strategically optimized visual noise without altering the base model. Our approach formulates hallucination suppression as an optimization problem, leveraging adversarial strategies to generate beneficial visual perturbations that enhance the model's factual grounding and reduce parametric knowledge bias. Extensive experimental results demonstrate that our method consistently reduces object hallucinations across 8 state-of-the-art LVMs, validating its efficacy across diverse evaluations.",
      "arxiv_url": "https://openreview.net/forum?id=quKHZ3fcgx",
      "pdf_url": "https://openreview.net/pdf/b3f409e30b3f9fd8f5178bb1fc6eef27b79ff546.pdf",
      "primary_category": "Large Vision-Langugae Model, Object Hallucination",
      "categories": [
        "Large Vision-Langugae Model",
        "Object Hallucination"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "nfxTpNiSMH",
      "title": "SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding",
      "authors": [
        "Zhao Jin",
        "Rong-Cheng Tu",
        "Jingyi Liao",
        "Wenhao Sun",
        "Xiao Luo",
        "Shunyu Liu",
        "Dacheng Tao"
      ],
      "abstract": "3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene based on natural language queries. To alleviate the reliance on costly 3D training data, recent studies have explored zero-shot 3DVG by leveraging the extensive knowledge and powerful reasoning capabilities of pre-trained LLMs and VLMs. However, existing paradigms tend to emphasize either spatial (3D-based) or semantic (2D-based) understanding, limiting their effectiveness in complex real-world applications. In this work, we introduce SPAZER — a VLM-driven agent that combines both modalities in a progressive reasoning framework. It first holistically analyzes the scene and produces a 3D rendering from the optimal  viewpoint. Based on this, anchor-guided candidate screening is conducted to perform a coarse-level localization of potential objects. Furthermore, leveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is efficiently performed to determine the best-matching object. By bridging spatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot grounding without training on 3D-labeled data. Extensive experiments on ScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms previous state-of-the-art zero-shot methods, achieving notable gains of $\\mathbf{9.0\\}$% and $\\mathbf{10.9\\}$% in accuracy.",
      "arxiv_url": "https://openreview.net/forum?id=nfxTpNiSMH",
      "pdf_url": "https://openreview.net/pdf/b0522ba8737e40b1714bc1adc199fcfa47a3dd5d.pdf",
      "primary_category": "3D Visual Grounding, Multi-modal Agent, Zero-shot",
      "categories": [
        "3D Visual Grounding",
        "Multi-modal Agent",
        "Zero-shot"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "6hvaQTKkpF",
      "title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models",
      "authors": [
        "Kele Shao",
        "Keda TAO",
        "Can Qin",
        "Haoxuan You",
        "Yang Sui",
        "Huan Wang"
      ],
      "abstract": "Video large language models (video LLMs) excel at video comprehension but face significant computational inefficiency due to redundant video tokens. Existing token pruning methods offer solutions. However, approaches operating within the LLM (inner-LLM pruning), such as FastV, incur intrinsic computational overhead in shallow layers. In contrast, methods performing token pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy within individual frames or limited temporal windows, neglecting the crucial global temporal dynamics and correlations across longer video sequences. This leads to sub-optimal spatio-temporal reduction and does not leverage video compressibility fully. Crucially, the synergistic potential and mutual influence of combining these strategies remain unexplored. To further reduce redundancy, we introduce HoliTom, a novel training-free holistic token merging framework. HoliTom employs outer-LLM pruning through global redundancy-aware temporal segmentation, followed by spatial-temporal merging to reduce visual tokens by over 90%, significantly alleviating the LLM's computational burden. Complementing this, we introduce a robust inner-LLM token similarity-based merging approach, designed for superior performance and compatibility with outer-LLM pruning. Evaluations demonstrate our method's promising efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational costs to 6.9% of FLOPs while maintaining 99.1% of the original performance. Furthermore, we achieve a 2.28× reduction in Time-To-First-Token (TTFT) and a 1.32× acceleration in decoding throughput, highlighting the practical benefits of our integrated pruning approach for efficient video LLMs inference.",
      "arxiv_url": "https://openreview.net/forum?id=6hvaQTKkpF",
      "pdf_url": "https://openreview.net/pdf/dca93197a3992ee4019883e6e3da71d93210c9e3.pdf",
      "primary_category": "Video LLMs, Token Compression, Video Understanding",
      "categories": [
        "Video LLMs",
        "Token Compression",
        "Video Understanding"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QaZxGWlbgO",
      "title": "Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension",
      "authors": [
        "Yongdong Luo",
        "Xiawu Zheng",
        "Guilin Li",
        "Shukang Yin",
        "Haojia Lin",
        "Chaoyou Fu",
        "Jinfa Huang",
        "Jiayi Ji",
        "Fei Chao",
        "Jiebo Luo",
        "Rongrong Ji"
      ],
      "abstract": "Existing large video-language models (LVLMs) struggle to comprehend long videos correctly due to limited context. To address this problem, fine-tuning long-context LVLMs and employing GPT-based agents have emerged as promising solutions. However, fine-tuning LVLMs would require extensive high-quality data and substantial GPU resources, while GPT-based agents would rely on proprietary models (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented Generation (Video-RAG), a training-free and cost-effective pipeline that employs visually-aligned auxiliary texts to help facilitate cross-modality alignment while providing additional information beyond the visual content. Specifically, we leverage open-source external tools to extract visually-aligned information from pure video data (e.g., audio, optical character, and object detection), and incorporate the extracted information into an existing LVLM as auxiliary texts, alongside video frames and queries, in a plug-and-play manner. Our Video-RAG offers several key advantages: (i) lightweight with low computing overhead due to single-turn retrieval; (ii) easy implementation and compatibility with any LVLM; and (iii) significant, consistent performance gains across long video understanding benchmarks, including Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates superior performance over proprietary models like Gemini-1.5-Pro and GPT-4o when utilized with a 72B model.",
      "arxiv_url": "https://openreview.net/forum?id=QaZxGWlbgO",
      "pdf_url": "https://openreview.net/pdf/a01226196a493609572c35a772d14e8a4a392c9b.pdf",
      "primary_category": "retrieval-augmented generation, video large language model, long video understanding",
      "categories": [
        "retrieval-augmented generation",
        "video large language model",
        "long video understanding"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "uwUkETPIJN",
      "title": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay",
      "authors": [
        "Yifan Sun",
        "Jingyan Shen",
        "Yibin Wang",
        "Tianyu Chen",
        "Zhendong Wang",
        "Mingyuan Zhou",
        "Huan Zhang"
      ],
      "abstract": "Reinforcement learning (RL) has become an effective approach for fine-tuning large language models (LLMs), particularly to enhance their reasoning capabilities. However, RL fine-tuning remains highly resource-intensive, and existing work has largely overlooked the problem of data efficiency. In this paper, we propose two techniques to improve data efficiency in LLM RL fine-tuning: difficulty-targeted online data selection and rollout replay. We introduce the notion of adaptive difficulty to guide online data selection, prioritizing questions of moderate difficulty that are more likely to yield informative learning signals. To estimate adaptive difficulty efficiently, we develop an attention-based framework that requires rollouts for only a small reference set of questions. The adaptive difficulty of the remaining questions is then estimated based on their similarity to this set. To further reduce rollout cost, we introduce a rollout replay mechanism inspired by experience replay in traditional RL. This technique reuses recent rollouts, lowering per-step computation while maintaining stable updates. Experiments across 6 LLM-dataset combinations show that our method reduces RL fine-tuning time by 23% to 62% while reaching the same level of performance as the original GRPO algorithm. \nOur code repository is available at https://github.com/ASTRAL-Group/data-efficient-llm-rl/.",
      "arxiv_url": "https://openreview.net/forum?id=uwUkETPIJN",
      "pdf_url": "https://openreview.net/pdf/cee70a5f023ee5700a73e5416e83d7c68523b3a3.pdf",
      "primary_category": "Data Efficiency, Online Data Selection, Experience Replay",
      "categories": [
        "Data Efficiency",
        "Online Data Selection",
        "Experience Replay",
        "Reinforcement Learning",
        "Large Language Model"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "At8OUlyTOu",
      "title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization",
      "authors": [
        "Mingzhe Du",
        "Anh Tuan Luu",
        "Yue Liu",
        "Yuhao QING",
        "Dong HUANG",
        "Xinyi He",
        "Qian Liu",
        "Zejun MA",
        "See-Kiong Ng"
      ],
      "abstract": "Large Language Models (LLMs) generate functionally correct solutions but often fall short in code efficiency, a critical bottleneck for real-world deployment. In this paper, we introduce a novel test-time iterative optimization framework to address this, employing a closed-loop system where LLMs iteratively refine code based on empirical performance feedback from an execution sandbox. We explore three training strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark show that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO, using reinforcement learning (RL) with execution feedback, continuously optimizes code performance, significantly boosting both pass@1 (from 47% to 62%) and the likelihood of outperforming human submissions in efficiency (from 31% to 45%). Our work demonstrates effective test-time code efficiency improvement and critically reveals the power of RL in teaching LLMs to truly self-improve code efficiency. We released our code and data at https://github.com/Elfsong/Afterburner.",
      "arxiv_url": "https://openreview.net/forum?id=At8OUlyTOu",
      "pdf_url": "https://openreview.net/pdf/e7b6558411a26ba61f67fc195459f743e845fabd.pdf",
      "primary_category": "Code Generation, Software Engineering",
      "categories": [
        "Code Generation",
        "Software Engineering"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "84dnGT4ajt",
      "title": "CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment",
      "authors": [
        "Qinfeng Li",
        "Tianyue Luo",
        "Xuhong Zhang",
        "Yangfan Xie",
        "Zhiqiang Shen",
        "Lijun Zhang",
        "Yier Jin",
        "Hao Peng",
        "Xinkui Zhao",
        "XianWei Zhu",
        "Jianwei Yin"
      ],
      "abstract": "Proprietary large language models (LLMs) exhibit strong generalization capabilities across diverse tasks and are increasingly deployed on edge devices for efficiency and privacy reasons. However, deploying proprietary LLMs at the edge without adequate protection introduces critical security threats. Attackers can extract model weights and architectures, enabling unauthorized copying and misuse. Even when protective measures prevent full extraction of model weights, attackers may still perform advanced attacks, such as fine-tuning, to further exploit the model. Existing defenses against these threats typically incur significant computational and communication overhead, making them impractical for edge deployment.\nTo safeguard the edge-deployed LLMs, we introduce CoreGuard, a computation- and communication-efficient protection method. CoreGuard employs an efficient protection protocol to reduce computational overhead and minimize communication overhead via a propagation protocol. Extensive experiments show that CoreGuard achieves upper-bound security protection with negligible overhead.",
      "arxiv_url": "https://openreview.net/forum?id=84dnGT4ajt",
      "pdf_url": "https://openreview.net/pdf/30a5b12f6335c73134adfe1bf4221fa1e473b5c1.pdf",
      "primary_category": "Intellectual Property Protection, Large Language Model, Model Stealing",
      "categories": [
        "Intellectual Property Protection",
        "Large Language Model",
        "Model Stealing",
        "Proactive Defense",
        "Trusted Execution Environment"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "QXEhBMNrCW",
      "title": "Group-in-Group Policy Optimization for LLM Agent Training",
      "authors": [
        "Lang Feng",
        "Zhenghai Xue",
        "Tingcong Liu",
        "Bo An"
      ],
      "abstract": "Recent advances in group-based reinforcement learning (RL) have driven frontier large language models (LLMs) in single-turn tasks like mathematical reasoning. However, their scalability to multi-turn LLM agent training remains limited. Unlike static tasks, agent-environment interactions unfold over many steps and often yield sparse or delayed rewards, making credit assignment across individual steps significantly more challenging. In this work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence. GiGPO introduces a two-level structure for estimating relative advantage: (i) At the episode-level, GiGPO computes macro relative advantages based on groups of complete trajectories; (ii) At the step-level, GiGPO introduces an anchor state grouping mechanism that retroactively constructs step-level groups by identifying repeated environment states across trajectories. Actions stemming from the same state are grouped together, enabling micro relative advantage estimation. This hierarchical structure effectively captures both global trajectory quality and local step effectiveness without relying on auxiliary models or additional rollouts. We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Crucially, GiGPO delivers fine-grained per-step credit signals, achieves performance gains of > 12\\% on ALFWorld and > 9\\% on WebShop over GRPO, and obtains superior performance on QA tasks (42.1\\% on 3B and 47.2\\% on 7B): all while maintaining the same GPU memory overhead, identical LLM rollout, and incurring little to no additional time cost.",
      "arxiv_url": "https://openreview.net/forum?id=QXEhBMNrCW",
      "pdf_url": "https://openreview.net/pdf/c28e200ee92ae5eef9869fe35dbd6fc859cd04cf.pdf",
      "primary_category": "Large Language Model, Agent, Group-Based Reinforcement Learning",
      "categories": [
        "Large Language Model",
        "Agent",
        "Group-Based Reinforcement Learning"
      ],
      "tags": [
        "LLM",
        "Agentic AI",
        "Search Agent"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "h6xQClTm4W",
      "title": "Exploring the Limits of Vision-Language-Action Manipulation in Cross-task Generalization",
      "authors": [
        "Jiaming Zhou",
        "Ke Ye",
        "Jiayi LIU",
        "Teli Ma",
        "Zifan Wang",
        "Ronghe Qiu",
        "Kun-Yu Lin",
        "Zhilin Zhao",
        "Junwei Liang"
      ],
      "abstract": "The generalization capabilities of vision-language-action (VLA) models to unseen tasks are crucial to achieving general-purpose robotic manipulation in open-world settings.\nHowever, the cross-task generalization capabilities of existing VLA models remain significantly underexplored.\nTo address this gap, we introduce **AGNOSTOS**, a novel simulation benchmark designed to rigorously evaluate cross-task zero-shot generalization in manipulation. \nAGNOSTOS comprises 23 unseen manipulation tasks for test—distinct from common training task distributions—and incorporates two levels of generalization difficulty to assess robustness. \nOur systematic evaluation reveals that current VLA models, despite being trained on diverse datasets, struggle to generalize effectively to these unseen tasks. \nTo overcome this limitation, we propose **Cross-Task In-Context Manipulation (X-ICM)**, \na method that conditions large language models (LLMs) on in-context demonstrations from seen tasks to predict action sequences for unseen tasks.\nAdditionally, we introduce a **dynamics-guided sample selection** strategy that identifies relevant demonstrations by capturing cross-task dynamics. \nOn AGNOSTOS, X-ICM significantly improves cross-task zero-shot generalization performance over leading VLAs, achieving improvements of 6.0\\% over $\\pi_0$ and 7.9\\% over VoxPoser.\nWe believe AGNOSTOS and X-ICM will serve as valuable tools for advancing general-purpose robotic manipulation.",
      "arxiv_url": "https://openreview.net/forum?id=h6xQClTm4W",
      "pdf_url": "https://openreview.net/pdf/ba782206825d0f1f5219718d27497e7e2d001e66.pdf",
      "primary_category": "Vision-Language-Action, Cross-task Generalizable Manipulation",
      "categories": [
        "Vision-Language-Action",
        "Cross-task Generalizable Manipulation"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8Ounc8L4F7",
      "title": "RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models",
      "authors": [
        "Zukang Xu",
        "Xing Hu",
        "Qiang Wu",
        "Dawei Yang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their exponentially increasing parameters pose significant challenges for deployment on resource-constrained devices.  Vector Quantization (VQ) shows great promise for low-bit quantization (e.g., 2 to 4 bits), but existing work faces two key challenges: unconstrained direction error and suboptimal bit allocation. In this paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit quantization for LLMs. RSAVQ introduces two geometry-driven innovations that effectively mitigate above limitations: (1) Error Direction Sensitivity Guidance (EDSG), which leverages the Fisher information matrix (FIM)-induced Riemannian metric to project quantization errors onto low-sensitivity directions in the parameter space. Specifically, this projection is performed along the negative natural gradient direction, which effectively suppresses error expansion.  (2) Weight Channel Sensitivity Guidance (WCSG) , which constructs a channel-wise sensitivity metric via FIM curvature analysis to dynamically guide bit resource allocation. The approach facilitates a globally optimal quantization solution within prescribed bit constraints.  Experiments demonstrate that RSAVQ outperforms existing methods for LLMs. For example, in 2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP\\# by 0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy.  This work offers a practical solution for constrained environments and a theoretical bridge between information geometry and the quantization of neural networks, advancing efficient deep learning.",
      "arxiv_url": "https://openreview.net/forum?id=8Ounc8L4F7",
      "pdf_url": "https://openreview.net/pdf/684dcd140be8db16ec0d72efbe12713c0c092d52.pdf",
      "primary_category": "Efficient AI, LLM Quantization, Large Language Model",
      "categories": [
        "Efficient AI",
        "LLM Quantization",
        "Large Language Model",
        "Model Compression"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "V8sDJ1oMEy",
      "title": "VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning",
      "authors": [
        "Wenhao Li",
        "Qiangchang Wang",
        "Xianjing Meng",
        "Zhibin Wu",
        "Yilong Yin"
      ],
      "abstract": "Few-shot learning (FSL) aims to recognize novel concepts from only a few labeled support samples. Recent studies enhance support features by incorporating additional semantic information  (e.g., class descriptions) or designing complex semantic fusion modules. However, these methods still suffer from hallucinating semantics that contradict the visual evidence due to the lack of grounding in actual instances, resulting in noisy guidance and costly corrections. To address these issues, we propose a novel framework, bridging Vision and Text with LLMs for Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts conditioned on Large Language Models (LLMs) and support images, seamlessly integrating them through a geometry-aware alignment mechanism. It mainly consists of Cross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment (CGA). Specifically, the CIP conditions an LLM on both class names and support images to generate precise class descriptions iteratively in a single  structured reasoning pass. These descriptions not only enrich the semantic understanding of novel classes but also enable the zero-shot synthesis of semantically consistent images. The descriptions and synthetic images act respectively as complementary textual and visual prompts, providing high-level class semantics and low-level intra-class diversity to compensate for limited support data. Furthermore, the CGA jointly aligns the fused textual, support, and synthetic visual representations by minimizing the kernelized volume of the 3-dimensional parallelotope they span. It captures global and nonlinear relationships among all representations, enabling structured and consistent multimodal integration. The proposed VT-FSL method establishes new state-of-the-art performance across ten diverse benchmarks, including standard, cross-domain, and fine-grained few-shot learning scenarios. Code is available at https://github.com/peacelwh/VT-FSL.",
      "arxiv_url": "https://openreview.net/forum?id=V8sDJ1oMEy",
      "pdf_url": "https://openreview.net/pdf/521c2d2f24d89aa54641a2bb0276e4fd1a5cebf4.pdf",
      "primary_category": "Few-Shot Learning, Multimodal Fusion, LLM",
      "categories": [
        "Few-Shot Learning",
        "Multimodal Fusion",
        "LLM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "1BAiQmAFsx",
      "title": "Walking the Tightrope: Autonomous Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning",
      "authors": [
        "Xiaoyu Yang",
        "Jie Lu",
        "En Yu"
      ],
      "abstract": "This paper uncovers a critical yet overlooked phenomenon in multi-modal large language models (MLLMs), especially for chest diagnosis: detrimental concept drift within chain-of-thought (CoT) reasoning during non-stationary reinforcement fine-tuning (RFT), where reasoning token distributions evolve unpredictably, thereby introducing significant biases in final predictions. To address this, we are pioneers in establishing the theoretical bridge between concept drift theory and RFT processes by formalizing CoT's autoregressive token streams as non-stationary distributions undergoing arbitrary temporal shifts. Leveraging this framework, we propose a novel autonomous counterfact-aware RFT that systematically decouples beneficial distribution adaptation from harmful concept drift through concept graph-empowered LLM experts generating counterfactual reasoning trajectories. Our solution, Counterfactual Preference Optimization (CPO), enables autonomous and stable RFT in non-stationary environments, particularly within the medical domain, through custom-tuning of counterfactual-aware preference alignment. Extensive experiments demonstrate our superior performance of robustness, generalization and coordination within RFT. Besides, we also contribute a large-scale dataset CXR-CounterFact (CCF), comprising 320,416 meticulously curated counterfactual reasoning trajectories derived from MIMIC-CXR. Our code and data are public at: https://github.com/XiaoyuYoung/CPO.",
      "arxiv_url": "https://openreview.net/forum?id=1BAiQmAFsx",
      "pdf_url": "https://openreview.net/pdf/89b8ebdad9f5e33501c6ae5382a930207650c41c.pdf",
      "primary_category": "Concept Drift, Reinforced Fine-tuning, MLLMs",
      "categories": [
        "Concept Drift",
        "Reinforced Fine-tuning",
        "MLLMs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "oEgybA04dY",
      "title": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning",
      "authors": [
        "Yana Wei",
        "Liang Zhao",
        "Jianjian Sun",
        "Kangheng Lin",
        "jisheng yin",
        "Jingcheng Hu",
        "Yinmin Zhang",
        "En Yu",
        "Haoran Lv",
        "Zejia Weng",
        "Jia Wang",
        "Qi Han",
        "Zheng Ge",
        "Xiangyu Zhang",
        "Daxin Jiang",
        "Vishal M. Patel"
      ],
      "abstract": "The remarkable reasoning capability of large language models (LLMs) stems from cognitive behaviors that emerge through reinforcement with verifiable rewards. This work investigates how to transfer this principle to Multimodal LLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage paradigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning, \nfollowed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps—surpassing all previous open-source efforts in scale.\nThis pioneering work reveals three fundamental insights: 1) Behavior transfer emerges surprisingly early in cold start due to linguistic mental imagery. 2) Cold start broadly memorizes visual behaviors, while RL critically discerns and scales up effective patterns. 3) Transfer strategically favors high-utility behaviors such as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR), achieves state-of-the-art performance on a suite of reasoning benchmarks, including 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We release our model, data, and training dynamics to catalyze the development of more capable, behavior-aligned multimodal reasoners.",
      "arxiv_url": "https://openreview.net/forum?id=oEgybA04dY",
      "pdf_url": "https://openreview.net/pdf/aa64c75f269da233428bef98108f402866b88aff.pdf",
      "primary_category": "Multimodal LLM, Visual Reasoning, Cognitive Behavior Transfer",
      "categories": [
        "Multimodal LLM",
        "Visual Reasoning",
        "Cognitive Behavior Transfer"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "8Flpo0zaaO",
      "title": "SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations",
      "authors": [
        "Buyun Liang",
        "Liangzu Peng",
        "Jinqi Luo",
        "Darshan Thaker",
        "Kwan Ho Ryan Chan",
        "Rene Vidal"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in high-risk domains. However, state-of-the-art LLMs often exhibit hallucinations, raising serious concerns about their reliability. Prior work has explored adversarial attacks to elicit hallucinations in LLMs, but these methods often rely on unrealistic prompts, either by inserting nonsensical tokens or by altering the original semantic intent. Consequently, such approaches provide limited insight into how hallucinations arise in real-world settings. In contrast, adversarial attacks in computer vision typically involve realistic modifications to input images. However, the problem of identifying realistic adversarial prompts for eliciting LLM hallucinations remains largely underexplored. To address this gap, we propose Semantically Equivalent and Coherent Attacks (SECA), which elicit hallucinations via realistic modifications to the prompt that preserve its meaning while maintaining semantic coherence. Our contributions are threefold: (i) we formulate finding realistic attacks for hallucination elicitation as a constrained optimization problem over the input prompt space under semantic equivalence and coherence constraints; (ii) we introduce a constraint-preserving zeroth-order method to effectively search for adversarial yet feasible prompts; and (iii) we demonstrate through experiments on open-ended multiple-choice question answering tasks that SECA achieves higher attack success rates while incurring almost no semantic equivalence or semantic coherence errors compared to existing methods. SECA highlights the sensitivity of both open-source and commercial gradient-inaccessible LLMs to realistic and plausible prompt variations. Code is available at https://github.com/Buyun-Liang/SECA.",
      "arxiv_url": "https://openreview.net/forum?id=8Flpo0zaaO",
      "pdf_url": "https://openreview.net/pdf/af8cc4135dd10bca1fc8bb463ecbd8a8ce1a47c2.pdf",
      "primary_category": "Large Language Models; Adversarial Attack; Red Teaming; LLM Hallucination; Robustness Evaluation; Trustworthy AI",
      "categories": [
        "Large Language Models; Adversarial Attack; Red Teaming; LLM Hallucination; Robustness Evaluation; Trustworthy AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "zIFuLxUAu9",
      "title": "Semi-off-Policy Reinforcement Learning for Vision-Language Slow-Thinking Reasoning",
      "authors": [
        "Junhao Shen",
        "Haiteng Zhao",
        "Yuzhe Gu",
        "Songyang Gao",
        "Kuikun Liu",
        "Haian Huang",
        "Jianfei Gao",
        "Dahua Lin",
        "Wenwei Zhang",
        "Kai Chen"
      ],
      "abstract": "Enhancing large vision-language models (LVLMs) with visual slow-thinking reasoning is crucial for solving complex multimodal tasks. However, since LVLMs are mainly trained with vision-language alignment, it is difficult to adopt on-policy reinforcement learning (RL) to develop the slow thinking ability because the rollout space is restricted by its initial abilities. Off-policy RL offers a way to go beyond the current policy, but directly distilling trajectories from external models may cause visual hallucinations due to mismatched visual perception abilities across models. To address these issues, this paper proposes **SOPHIA**, a simple and scalable **S**emi-**O**ff-**P**olicy RL for vision-language slow-t**HI**nking re**A**soning. SOPHIA builds a semi-off-policy behavior model by combining on-policy visual understanding from a trainable LVLM with off-policy slow-thinking reasoning from a language model, assigns outcome-based rewards to reasoning, and propagates visual rewards backward. Then LVLM learns slow-thinking reasoning ability from the obtained reasoning trajectories using propagated rewards via off-policy RL algorithms. Extensive experiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the effectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50\\% in average, reaching state-of-the-art performance among open-source LVLMs on multiple multimodal reasoning benchmarks, and even outperforms some closed-source models (e.g., GPT-4.1) on the challenging MathVision and OlympiadBench, achieving 49.08\\% and 49.95\\% pass@1 accuracy, respectively. Analysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy RL methods, offering a better policy initialization for further on-policy training.",
      "arxiv_url": "https://openreview.net/forum?id=zIFuLxUAu9",
      "pdf_url": "https://openreview.net/pdf/7f0c2533fad4e8c7e1c59751a9d982dda9611eba.pdf",
      "primary_category": "Large vision-language model, Slow-thinking reasoning",
      "categories": [
        "Large vision-language model",
        "Slow-thinking reasoning"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "EKJhU5ioSo",
      "title": "GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning",
      "authors": [
        "Jusheng Zhang",
        "Yijia Fan",
        "Wenjun Lin",
        "Ruiqi Chen",
        "Haoyi Jiang",
        "Wenhao Chai",
        "Jian Wang",
        "Keze Wang"
      ],
      "abstract": "We propose **GAM-Agent**, a game-theoretic multi-agent framework for enhancing vision-language reasoning. Unlike prior single-agent or monolithic models, GAM-Agent formulates the reasoning process as a non-zero-sum game between base agents—each specializing in visual perception subtasks—and a critical agent that verifies logic consistency and factual correctness. Agents communicate via structured claims, evidence, and uncertainty estimates. The framework introduces an uncertainty-aware controller to dynamically adjust agent collaboration, triggering multi-round debates when disagreement or ambiguity is detected. This process yields more robust and interpretable predictions. Experiments on four challenging benchmarks—MMMU, MMBench, MVBench, and V*Bench—demonstrate that GAM-Agent significantly improves performance across various VLM backbones. Notably, GAM-Agent boosts the accuracy of small-to-mid scale models (e.g., Qwen2.5-VL-7B, InternVL3-14B) by 5–6\\%, and still enhances strong models like GPT-4o by up to 2–3\\%. Our approach is modular, scalable, and generalizable, offering a path toward reliable and explainable multi-agent multimodal reasoning.",
      "arxiv_url": "https://openreview.net/forum?id=EKJhU5ioSo",
      "pdf_url": "https://openreview.net/pdf/d481b7328d742813a7ca25c21d876761789f0bfe.pdf",
      "primary_category": "Multi-Agent; Uncertainty; Visual Language Model",
      "categories": [
        "Multi-Agent; Uncertainty; Visual Language Model"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG",
        "Agentic AI"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "h3dbocj7po",
      "title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments",
      "authors": [
        "Enjun Du",
        "Xunkai Li",
        "Tian Jin",
        "Zhihan Zhang",
        "Rong-Hua Li",
        "Guoren Wang"
      ],
      "abstract": "The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes—a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce \\textbf{GraphMaster}—the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited “Sub” variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments.",
      "arxiv_url": "https://openreview.net/forum?id=h3dbocj7po",
      "pdf_url": "https://openreview.net/pdf/b832aae73478d8c09f69301e2887f00510d8e187.pdf",
      "primary_category": "Graph data synthesis, text-attributed graphs, multi-agent framework",
      "categories": [
        "Graph data synthesis",
        "text-attributed graphs",
        "multi-agent framework",
        "large language models",
        "retrieval-augmented generation",
        "data-limited environments"
      ],
      "tags": [
        "LLM",
        "Agentic AI",
        "Context Compression"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "0lNwIIHWhZ",
      "title": "ComPO: Preference Alignment via Comparison Oracles",
      "authors": [
        "Peter Chen",
        "Xi Chen",
        "Wotao Yin",
        "Tianyi Lin"
      ],
      "abstract": "Direct alignment methods are increasingly used for aligning large language models (LLMs) with human preferences. However, these methods suffer from the issues of verbosity and likelihood displacement, which can be driven by the noisy preference pairs that induce similar likelihood for preferred and dispreferred responses. The contributions of this paper are two-fold. First, we propose a new preference alignment method based on zeroth-order, comparison-based optimization via comparison oracles and provide convergence guarantees for its basic scheme. Second, we improve our method using some heuristics and conduct the experiments to demonstrate the flexibility and compatibility of practical scheme in improving the performance of LLMs using noisy preference pairs. Evaluations are conducted across multiple base and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with benchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show the effectiveness of our method as an alternative to addressing the limitations of existing direct alignment methods. A highlight of our work is that we evidence the importance of designing specialized methods for preference pairs with distinct likelihood margin, which complements the recent findings in Razin et al (2025).",
      "arxiv_url": "https://openreview.net/forum?id=0lNwIIHWhZ",
      "pdf_url": "https://openreview.net/pdf/98bc95b6044d12b1e129aed8dd961af675cc17a6.pdf",
      "primary_category": "LLM Preference Alignment, Zeroth-order Optimization, Comparison-based Optimization",
      "categories": [
        "LLM Preference Alignment",
        "Zeroth-order Optimization",
        "Comparison-based Optimization",
        "Noisy Preference Pairs"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "epZTfZF7JC",
      "title": "VITA-Audio: Fast Interleaved Audio-Text Token Generation for Efficient Large Speech-Language Model",
      "authors": [
        "Zuwei Long",
        "Yunhang Shen",
        "Chaoyou Fu",
        "Heting Gao",
        "lijiang Li",
        "Peixian Chen",
        "Mengdan Zhang",
        "Hang Shao",
        "Jian Li",
        "Jinlong Peng",
        "Haoyu Cao",
        "Ke Li",
        "Rongrong Ji",
        "Xing Sun"
      ],
      "abstract": "With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses a significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, a four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3~5x at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.",
      "arxiv_url": "https://openreview.net/forum?id=epZTfZF7JC",
      "pdf_url": "https://openreview.net/pdf/9045b043695e1d38794557e2d092c84adae305ab.pdf",
      "primary_category": "Multimodal Large Language Models, Large Speech Language Models",
      "categories": [
        "Multimodal Large Language Models",
        "Large Speech Language Models"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "tjXtcZjIgQ",
      "title": "EvolvedGRPO: Unlocking Reasoning in LVLMs via Progressive Instruction Evolution",
      "authors": [
        "Zhebei Shen",
        "Qifan Yu",
        "Juncheng Li",
        "Wei Ji",
        "Qizhi Chen",
        "Siliang Tang",
        "Yueting Zhuang"
      ],
      "abstract": "Recent advances in reinforcement learning (RL) methods such as Grouped Relative Policy Optimization (GRPO) have strengthened the reasoning capabilities of Large Vision-Language Models (LVLMs). However, due to the inherent entanglement between visual and textual modalities, applying GRPO to LVLMs often leads to reward convergence across different responses to the same sample as training progresses, hindering effective gradient updates and causing the enhancement of chain-of-thought reasoning to stagnate or even collapse.\nTo address this issue, we propose a progressive instruction evolution framework, EvolvedGRPO, to gradually generate more complex questions via editing instructions in an adversarial way, progressively aligned with the model’s evolving capabilities. Specifically, we design two instruction editing strategies across modalities, incorporating incrementally increasing editing instructions and RL-based adversarial data augmentation to improve the effectiveness of model training. To address GRPO's limitations on overly difficult problems, we first train on basic subproblem versions of complex multi-modal questions in both the visual and textual modalities, progressively increasing difficulty to enable prefix-style process rewards, effectively combining the strengths of both process rewards and group-wise relative rewards. Finally, EvolvedGRPO achieves state-of-the-art performance among open-source RL models on multi-modal reasoning tasks, even approaching the closed-source GPT-4o in reasoning capabilities, and demonstrates better performance on unseen LVLM general benchmarks. The Code for EvolvedGRPO is available at https://github.com/SHENZHEBEI/EvolvedGRPO.",
      "arxiv_url": "https://openreview.net/forum?id=tjXtcZjIgQ",
      "pdf_url": "https://openreview.net/pdf/2148cf3a9c0f9e8369c30c853ee0a21f8fceec80.pdf",
      "primary_category": "multi-modal reasoning, reinforcement learning, self-improvement",
      "categories": [
        "multi-modal reasoning",
        "reinforcement learning",
        "self-improvement"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "mnIox70nhO",
      "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring",
      "authors": [
        "Yang Li",
        "Qiang Sheng",
        "Yehan Yang",
        "Xueyao Zhang",
        "Juan Cao"
      ],
      "abstract": "Though safety alignment has been applied to most large language models (LLMs), LLM service providers generally deploy a subsequent moderation as the external safety guardrail in real-world products. Existing moderators mainly practice a conventional full detection, which determines the harmfulness based on the complete LLM output, causing high service latency. Recent works pay more attention to partial detection where moderators oversee the generation midway and early stop the output if harmfulness is detected, but they directly apply moderators trained with the full detection paradigm to incomplete outputs, introducing a training-inference gap that lowers the performance. In this paper, we explore how to form a data-and-model solution that natively supports partial detection. For the data, we construct **FineHarm**, a dataset consisting of 29K prompt-response pairs with fine-grained token-level annotations to provide reasonable supervision for token-level training. Then, we propose the **Streaming Content Monitor (SCM)**, which is trained with dual supervision of response- and token-level labels and can follow the output stream of LLM to make a timely judgment of harmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is comparable to full-detection, by only seeing the first 18% of tokens in responses on average. Moreover, the SCM can serve as a pseudo-harmfulness annotator for improving safety alignment and lead to a higher harmlessness score than DPO.",
      "arxiv_url": "https://openreview.net/forum?id=mnIox70nhO",
      "pdf_url": "https://openreview.net/pdf/39d2422b8a0675c8bd1af424425125f2d1bfc016.pdf",
      "primary_category": "harmful content detection, streaming content moderation, LLM safety",
      "categories": [
        "harmful content detection",
        "streaming content moderation",
        "LLM safety"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    },
    {
      "arxiv_id": "rJ5ky9C3ue",
      "title": "CrossSpectra: Exploiting Cross-Layer Smoothness for Parameter-Efficient Fine-Tuning",
      "authors": [
        "Yifei Zhang",
        "Hao Zhu",
        "Junhao Dong",
        "Haoran Shi",
        "Ziqiao Meng",
        "Piotr Koniusz",
        "Han Yu"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) is essential for adapting large foundation models without excessive storage cost. However, current approaches such as LoRA treat each layer’s adaptation independently, overlooking correlations across layers. This independence causes the number of trainable parameters to grow linearly with model depth. We provide theoretical and empirical evidence that skip connections in transformers create smooth gradient propagation across layers. This smoothness leads to weight adaptations that concentrate most of their energy in low-frequency spectral components, especially along the layer dimension. Empirical analysis confirms this effect, showing that most of adaptation energy lies in low frequencies. Building on this insight, we propose CrossSpectra, which parameterizes all attention-weight adaptations $(Q, K, V)$ across layers as a single 3D tensor and represents them with sparse spectral coefficients ($\\kappa_1, \\kappa_2$). Using $\\kappa_{1}$ non-zero coefficients within each layer’s frequency space and truncating to $\\kappa_{2}$ frequencies across layers, CrossSpectra requires $\\mathcal{O}(\\kappa_{1}\\kappa_{2})$ parameters instead of LoRA’s $\\mathcal{O}(Lrd)$, where $L$ is the number of layers and $r$ the rank. Across natural-language and vision benchmarks, \\methodname{} matches or surpasses baseline performance while using fewer parameters than LoRA, achieving only $0.36\\%$ of LoRA’s parameter count when fine-tuning LLaMA-7B on instruction-following tasks. These results show that exploiting the \\textbf{architectural smoothness of transformers} through spectral analysis yields major efficiency gains in PEFT.",
      "arxiv_url": "https://openreview.net/forum?id=rJ5ky9C3ue",
      "pdf_url": "https://openreview.net/pdf/9e8bcf05a37a330cbd98c5b3651e94ede0177adc.pdf",
      "primary_category": "Parameter-efficient fine-tuning (PEFT)",
      "categories": [
        "Parameter-efficient fine-tuning (PEFT)"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "NeurIPS 2025",
      "source": "conference",
      "conference": "NeurIPS 2025"
    }
  ],
  "available_tags": [
    "Agentic AI",
    "Context Compression",
    "Information Retrieval",
    "LLM",
    "Multi-Modal RAG",
    "Personalization",
    "RAG",
    "Search Agent"
  ]
}