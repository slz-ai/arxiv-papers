{
  "date": "2026-02-23",
  "paper_count": 87,
  "papers": [
    {
      "arxiv_id": "2602.20156v1",
      "title": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks",
      "authors": [
        "David Schmotz",
        "Luca Beurer-Kellner",
        "Sahar Abdelnabi",
        "Maksym Andriushchenko"
      ],
      "abstract": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.",
      "arxiv_url": "http://arxiv.org/abs/2602.20156v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20156v1",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20144v1",
      "title": "Agentic AI for Scalable and Robust Optical Systems Control",
      "authors": [
        "Zehao Wang",
        "Mingzhe Han",
        "Wei Cheng",
        "Yue-Kai Huang",
        "Philip Ji",
        "Denton Wu",
        "Mahdi Safari",
        "Flemming Holtorf",
        "Kenaish AlQubaisi",
        "Norbert M. Linke",
        "Danyang Zhuo",
        "Yiran Chen",
        "Ting Wang",
        "Dirk Englund",
        "Tingjun Chen"
      ],
      "abstract": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems.",
      "arxiv_url": "http://arxiv.org/abs/2602.20144v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20144v1",
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.NI"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20135v1",
      "title": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration",
      "authors": [
        "Mohammad Amanlou",
        "Erfan Shafiee Moghaddam",
        "Yasaman Amou Jafari",
        "Mahdi Noori",
        "Farhan Farsi",
        "Behnam Bahrak"
      ],
      "abstract": "With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.",
      "arxiv_url": "http://arxiv.org/abs/2602.20135v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20135v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20133v1",
      "title": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization",
      "authors": [
        "Mert Cemri",
        "Shubham Agrawal",
        "Akshat Gupta",
        "Shu Liu",
        "Audrey Cheng",
        "Qiuyang Mang",
        "Ashwin Naren",
        "Lutfi Eren Erdogan",
        "Koushik Sen",
        "Matei Zaharia",
        "Alex Dimakis",
        "Ion Stoica"
      ],
      "abstract": "The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an \"accumulated improvement signal\" to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems.",
      "arxiv_url": "http://arxiv.org/abs/2602.20133v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20133v1",
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20132v1",
      "title": "LAD: Learning Advantage Distribution for Reasoning",
      "authors": [
        "Wendi Li",
        "Sharon Li"
      ],
      "abstract": "Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity.",
      "arxiv_url": "http://arxiv.org/abs/2602.20132v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20132v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20130v1",
      "title": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering",
      "authors": [
        "Zaifu Zhan",
        "Min Zeng",
        "Shuang Zhou",
        "Yiran Song",
        "Xiaoyi Chen",
        "Yu Hou",
        "Yifan Wu",
        "Yang Ruan",
        "Rui Zhang"
      ],
      "abstract": "Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy.   Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time.   Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\\leq$4\\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost.   Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability.   Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems.",
      "arxiv_url": "http://arxiv.org/abs/2602.20130v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20130v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20122v1",
      "title": "NanoKnow: How to Know What Your Language Model Knows",
      "authors": [
        "Lingwei Gu",
        "Nour Jedidi",
        "Jimmy Lin"
      ],
      "abstract": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.",
      "arxiv_url": "http://arxiv.org/abs/2602.20122v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20122v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20119v1",
      "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning",
      "authors": [
        "Jiahui Fu",
        "Junyu Nan",
        "Lingfeng Sun",
        "Hongyu Li",
        "Jianing Qian",
        "Jennifer L. Barry",
        "Kris Kitani",
        "George Konidaris"
      ],
      "abstract": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/",
      "arxiv_url": "http://arxiv.org/abs/2602.20119v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20119v1",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "tags": [
        "Multi-Modal RAG"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20114v1",
      "title": "Benchmarking Unlearning for Vision Transformers",
      "authors": [
        "Kairan Zhao",
        "Iurie Luca",
        "Peter Triantafillou"
      ],
      "abstract": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline.",
      "arxiv_url": "http://arxiv.org/abs/2602.20114v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20114v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20104v1",
      "title": "Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration",
      "authors": [
        "Hasan Amin",
        "Ming Yin",
        "Rajiv Khanna"
      ],
      "abstract": "In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance.",
      "arxiv_url": "http://arxiv.org/abs/2602.20104v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20104v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20102v1",
      "title": "BarrierSteer: LLM Safety via Learning Barrier Steering",
      "authors": [
        "Thanh Q. Tran",
        "Arun Verma",
        "Kiwan Wong",
        "Bryan Kian Hsiang Low",
        "Daniela Rus",
        "Wei Xiao"
      ],
      "abstract": "Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints directly into the model's latent representation space. BarrierSteer employs a steering mechanism based on Control Barrier Functions (CBFs) to efficiently detect and prevent unsafe response trajectories during inference with high precision. By enforcing multiple safety constraints through efficient constraint merging, without modifying the underlying LLM parameters, BarrierSteer preserves the model's original capabilities and performance. We provide theoretical results establishing that applying CBFs in latent space offers a principled and computationally efficient approach to enforcing safety. Our experiments across multiple models and datasets show that BarrierSteer substantially reduces adversarial success rates, decreases unsafe generations, and outperforms existing methods.",
      "arxiv_url": "http://arxiv.org/abs/2602.20102v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20102v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20094v1",
      "title": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching",
      "authors": [
        "Yuzhe Wang",
        "Yaochen Zhu",
        "Jundong Li"
      ],
      "abstract": "As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs.",
      "arxiv_url": "http://arxiv.org/abs/2602.20094v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20094v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20091v1",
      "title": "How Retrieved Context Shapes Internal Representations in RAG",
      "authors": [
        "Samuel Yeh",
        "Sharon Li"
      ],
      "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design.",
      "arxiv_url": "http://arxiv.org/abs/2602.20091v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20091v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20089v1",
      "title": "StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues",
      "authors": [
        "Zanxi Ruan",
        "Qiuyu Kong",
        "Songqun Gao",
        "Yiming Wang",
        "Marco Cristani"
      ],
      "abstract": "Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StructXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them as proxies for the visual structure of an image, and filters the corresponding captions to emphasize structural cues, making them \"structure-centric\". Fine-tuning augments the standard alignment loss with three structure-centric losses: (i) aligning edge maps with structural text, (ii) matching local edge regions to textual chunks, and (iii) connecting edge maps to color images to prevent representation drift. From a theoretical standpoint, while standard CLIP maximizes the mutual information between visual and textual embeddings, StructXLIP additionally maximizes the mutual information between multimodal structural representations. This auxiliary optimization is intrinsically harder, guiding the model toward more robust and semantically stable minima, enhancing vision-language alignment. Beyond outperforming current competitors on cross-modal retrieval in both general and specialized domains, our method serves as a general boosting recipe that can be integrated into future approaches in a plug-and-play manner. Code and pretrained models are publicly available at: https://github.com/intelligolabs/StructXLIP.",
      "arxiv_url": "http://arxiv.org/abs/2602.20089v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20089v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20078v1",
      "title": "Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning",
      "authors": [
        "Shan Yang",
        "Yang Liu"
      ],
      "abstract": "Scaling cooperative multi-agent reinforcement learning (MARL) is fundamentally limited by cross-agent noise: when agents share a common reward, the actions of all $N$ agents jointly determine each agent's learning signal, so cross-agent noise grows with $N$. In the policy gradient setting, per-agent gradient estimate variance scales as $Θ(N)$, yielding sample complexity $\\mathcal{O}(N/ε)$. We observe that many domains -- cloud computing, transportation, power systems -- have differentiable analytical models that prescribe efficient system states. In this work, we propose Descent-Guided Policy Gradient (DG-PG), a framework that constructs noise-free per-agent guidance gradients from these analytical models, decoupling each agent's gradient from the actions of all others. We prove that DG-PG reduces gradient variance from $Θ(N)$ to $\\mathcal{O}(1)$, preserves the equilibria of the cooperative game, and achieves agent-independent sample complexity $\\mathcal{O}(1/ε)$. On a heterogeneous cloud scheduling task with up to 200 agents, DG-PG converges within 10 episodes at every tested scale -- from $N=5$ to $N=200$ -- directly confirming the predicted scale-invariant complexity, while MAPPO and IPPO fail to converge under identical architectures.",
      "arxiv_url": "http://arxiv.org/abs/2602.20078v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20078v1",
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20065v1",
      "title": "Multilingual Large Language Models do not comprehend all natural languages to equal degrees",
      "authors": [
        "Natalia Moskvina",
        "Raquel Montero",
        "Masaya Yoshida",
        "Ferdy Hubers",
        "Paolo Morosi",
        "Walid Irhaymi",
        "Jin Yan",
        "Tamara Serrano",
        "Elena Pagliarini",
        "Fritz Günther",
        "Evelina Leivada"
      ],
      "abstract": "Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities.",
      "arxiv_url": "http://arxiv.org/abs/2602.20065v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20065v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20064v1",
      "title": "The LLMbda Calculus: AI Agents, Conversations, and Information Flow",
      "authors": [
        "Zac Garby",
        "Andrew D. Gordon",
        "David Sands"
      ],
      "abstract": "A conversation with a large language model (LLM) is a sequence of prompts and responses, with each response generated from the preceding conversation. AI agents build such conversations automatically: given an initial human prompt, a planner loop interleaves LLM calls with tool invocations and code execution. This tight coupling creates a new and poorly understood attack surface. A malicious prompt injected into a conversation can compromise later reasoning, trigger dangerous tool calls, or distort final outputs. Despite the centrality of such systems, we currently lack a principled semantic foundation for reasoning about their behaviour and safety. We address this gap by introducing an untyped call-by-value lambda calculus enriched with dynamic information-flow control and a small number of primitives for constructing prompt-response conversations. Our language includes a primitive that invokes an LLM: it serializes a value, sends it to the model as a prompt, and parses the response as a new term. This calculus faithfully represents planner loops and their vulnerabilities, including the mechanisms by which prompt injection alters subsequent computation. The semantics explicitly captures conversations, and so supports reasoning about defenses such as quarantined sub-conversations, isolation of generated code, and information-flow restrictions on what may influence an LLM call. A termination-insensitive noninterference theorem establishes integrity and confidentiality guarantees, demonstrating that a formal calculus can provide rigorous foundations for safe agentic programming.",
      "arxiv_url": "http://arxiv.org/abs/2602.20064v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20064v1",
      "primary_category": "cs.PL",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.CR"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20062v1",
      "title": "A Theory of How Pretraining Shapes Inductive Bias in Fine-Tuning",
      "authors": [
        "Nicolas Anguita",
        "Francesco Locatello",
        "Andrew M. Saxe",
        "Marco Mondelli",
        "Flavia Mancini",
        "Samuel Lippl",
        "Clementine Domine"
      ],
      "abstract": "Pretraining and fine-tuning are central stages in modern machine learning systems. In practice, feature learning plays an important role across both stages: deep neural networks learn a broad range of useful features during pretraining and further refine those features during fine-tuning. However, an end-to-end theoretical understanding of how choices of initialization impact the ability to reuse and refine features during fine-tuning has remained elusive. Here we develop an analytical theory of the pretraining-fine-tuning pipeline in diagonal linear networks, deriving exact expressions for the generalization error as a function of initialization parameters and task statistics. We find that different initialization choices place the network into four distinct fine-tuning regimes that are distinguished by their ability to support feature learning and reuse, and therefore by the task statistics for which they are beneficial. In particular, a smaller initialization scale in earlier layers enables the network to both reuse and refine its features, leading to superior generalization on fine-tuning tasks that rely on a subset of pretraining features. We demonstrate empirically that the same initialization parameters impact generalization in nonlinear networks trained on CIFAR-100. Overall, our results demonstrate analytically how data and network initialization interact to shape fine-tuning generalization, highlighting an important role for the relative scale of initialization across different layers in enabling continued feature learning during fine-tuning.",
      "arxiv_url": "http://arxiv.org/abs/2602.20062v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20062v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20059v1",
      "title": "Interaction Theater: A case of LLM Agents Interacting at Scale",
      "authors": [
        "Sarath Shekkizhar",
        "Adam Earle"
      ],
      "abstract": "As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts, 3.5M comments, and 78K agent profiles. We combine lexical metrics (Jaccard specificity), embedding-based semantic similarity, and LLM-as-judge validation to characterize agent interaction quality. Our findings reveal agents produce diverse, well-formed text that creates the surface appearance of active discussion, but the substance is largely absent. Specifically, while most agents ($67.5\\%$) vary their output across contexts, $65\\%$ of comments share no distinguishing content vocabulary with the post they appear under, and information gain from additional comments decays rapidly. LLM judge based metrics classify the dominant comment types as spam ($28\\%$) and off-topic content ($22\\%$). Embedding-based semantic analysis confirms that lexically generic comments are also semantically generic. Agents rarely engage in threaded conversation ($5\\%$ of comments), defaulting instead to independent top-level responses. We discuss implications for multi-agent interaction design, arguing that coordination mechanisms must be explicitly designed; without them, even large populations of capable agents produce parallel output rather than productive exchange.",
      "arxiv_url": "http://arxiv.org/abs/2602.20059v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20059v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20055v1",
      "title": "To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation",
      "authors": [
        "Apoorva Vashisth",
        "Manav Kulshrestha",
        "Pranav Bakshi",
        "Damon Conover",
        "Guillaume Sartoretti",
        "Aniket Bera"
      ],
      "abstract": "Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot. However, in real-world scenarios, such as home environments and warehouses, clutter can block all routes. Targeted at such cases, we introduce the Lifelong Interactive Navigation problem, where a mobile robot with manipulation abilities can move clutter to forge its own path to complete sequential object- placement tasks - each involving placing an given object (eg. Alarm clock, Pillow) onto a target object (eg. Dining table, Desk, Bed). To address this lifelong setting - where effects of environment changes accumulate and have long-term effects - we propose an LLM-driven, constraint-based planning framework with active perception. Our framework allows the LLM to reason over a structured scene graph of discovered objects and obstacles, deciding which object to move, where to place it, and where to look next to discover task-relevant information. This coupling of reasoning and active perception allows the agent to explore the regions expected to contribute to task completion rather than exhaustively mapping the environment. A standard motion planner then executes the corresponding navigate-pick-place, or detour sequence, ensuring reliable low-level control. Evaluated in physics-enabled ProcTHOR-10k simulator, our approach outperforms non-learning and learning-based baselines. We further demonstrate our approach qualitatively on real-world hardware.",
      "arxiv_url": "http://arxiv.org/abs/2602.20055v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20055v1",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20052v1",
      "title": "Entropy in Large Language Models",
      "authors": [
        "Marco Scharringhausen"
      ],
      "abstract": "In this study, the output of large language models (LLM) is considered an information source generating an unlimited sequence of symbols drawn from a finite alphabet. Given the probabilistic nature of modern LLMs, we assume a probabilistic model for these LLMs, following a constant random distribution and the source itself thus being stationary. We compare this source entropy (per word) to that of natural language (written or spoken) as represented by the Open American National Corpus (OANC). Our results indicate that the word entropy of such LLMs is lower than the word entropy of natural speech both in written or spoken form. The long-term goal of such studies is to formalize the intuitions of information and uncertainty in large language training to assess the impact of training an LLM from LLM generated training data. This refers to texts from the world wide web in particular.",
      "arxiv_url": "http://arxiv.org/abs/2602.20052v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20052v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20048v1",
      "title": "CodeCompass: Navigating the Navigation Paradox in Agentic Code Intelligence",
      "authors": [
        "Tarakanath Paipuru"
      ],
      "abstract": "Modern code intelligence agents operate in contexts exceeding 1 million tokens--far beyond the scale where humans manually locate relevant files. Yet agents consistently fail to discover architecturally critical files when solving real-world coding tasks. We identify the Navigation Paradox: agents perform poorly not due to context limits, but because navigation and retrieval are fundamentally distinct problems. Through 258 automated trials across 30 benchmark tasks on a production FastAPI repository, we demonstrate that graph-based structural navigation via CodeCompass--a Model Context Protocol server exposing dependency graphs--achieves 99.4% task completion on hidden-dependency tasks, a 23.2 percentage-point improvement over vanilla agents (76.2%) and 21.2 points over BM25 retrieval (78.2%).However, we uncover a critical adoption gap: 58% of trials with graph access made zero tool calls, and agents required explicit prompt engineering to adopt the tool consistently. Our findings reveal that the bottleneck is not tool availability but behavioral alignment--agents must be explicitly guided to leverage structural context over lexical heuristics. We contribute: (1) a task taxonomy distinguishing semantic-search, structural, and hidden-dependency scenarios; (2) empirical evidence that graph navigation outperforms retrieval when dependencies lack lexical overlap; and (3) open-source infrastructure for reproducible evaluation of navigation tools.",
      "arxiv_url": "http://arxiv.org/abs/2602.20048v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20048v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "tags": [
        "Agentic AI",
        "Information Retrieval",
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20046v1",
      "title": "Closing the gap in multimodal medical representation alignment",
      "authors": [
        "Eleonora Grassucci",
        "Giordano Cicchetti",
        "Danilo Comminiello"
      ],
      "abstract": "In multimodal learning, CLIP has emerged as the de-facto approach for mapping different modalities into a shared latent space by bringing semantically similar representations closer while pushing apart dissimilar ones. However, CLIP-based contrastive losses exhibit unintended behaviors that negatively impact true semantic alignment, leading to sparse and fragmented latent spaces. This phenomenon, known as the modality gap, has been partially mitigated for standard text and image pairs but remains unknown and unresolved in more complex multimodal settings, such as the medical domain. In this work, we study this phenomenon in the latter case, revealing that the modality gap is present also in medical alignment, and we propose a modality-agnostic framework that closes this gap, ensuring that semantically related representations are more aligned, regardless of their source modality. Our method enhances alignment between radiology images and clinical text, improving cross-modal retrieval and image captioning.",
      "arxiv_url": "http://arxiv.org/abs/2602.20046v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20046v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20042v1",
      "title": "Position: General Alignment Has Hit a Ceiling; Edge Alignment Must Be Taken Seriously",
      "authors": [
        "Han Bao",
        "Yue Huang",
        "Xiaoda Wang",
        "Zheyuan Zhang",
        "Yujun Zhou",
        "Carl Yang",
        "Xiangliang Zhang",
        "Yanfang Ye"
      ],
      "abstract": "Large language models are being deployed in complex socio-technical systems, which exposes limits in current alignment practice. We take the position that the dominant paradigm of General Alignment, which compresses diverse human values into a single scalar reward, reaches a structural ceiling in settings with conflicting values, plural stakeholders, and irreducible uncertainty. These failures follow from the mathematics and incentives of scalarization and lead to \\textbf{structural} value flattening, \\textbf{normative} representation loss, and \\textbf{cognitive} uncertainty blindness. We introduce Edge Alignment as a distinct approach in which systems preserve multi dimensional value structure, support plural and democratic representation, and incorporate epistemic mechanisms for interaction and clarification. To make this approach practical, we propose seven interdependent pillars organized into three phases. We identify key challenges in data collection, training objectives, and evaluation, outlining complementary technical and governance directions. Taken together, these measures reframe alignment as a lifecycle problem of dynamic normative governance rather than as a single instance optimization task.",
      "arxiv_url": "http://arxiv.org/abs/2602.20042v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20042v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20040v1",
      "title": "AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization",
      "authors": [
        "Fahmida Liza Piya",
        "Rahmatollah Beheshti"
      ],
      "abstract": "Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic framework that separates context selection, generation, verification, and targeted correction to reduce hallucinated content. The framework decomposes summarization into coordinated stages that compress task-relevant context, generate an initial draft, identify weakly supported spans using internal attention grounding signals, and selectively revise flagged content under supervisory control. We evaluate AgenticSum on two public datasets, using reference-based metrics, LLM-as-a-judge assessment, and human evaluation. Across various measures, AgenticSum demonstrates consistent improvements compared to vanilla LLMs and other strong baselines. Our results indicate that structured, agentic design with targeted correction offers an effective inference time solution to improve clinical note summarization using LLMs.",
      "arxiv_url": "http://arxiv.org/abs/2602.20040v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20040v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20021v1",
      "title": "Agents of Chaos",
      "authors": [
        "Natalie Shapira",
        "Chris Wendler",
        "Avery Yen",
        "Gabriele Sarti",
        "Koyena Pal",
        "Olivia Floody",
        "Adam Belfki",
        "Alex Loftus",
        "Aditya Ratan Jannali",
        "Nikhil Prakash",
        "Jasmine Cui",
        "Giordano Rogers",
        "Jannik Brinkmann",
        "Can Rager",
        "Amir Zur",
        "Michael Ripa",
        "Aruna Sankaranarayanan",
        "David Atkinson",
        "Rohit Gandikota",
        "Jaden Fiotto-Kaufman",
        "EunJeong Hwang",
        "Hadas Orgad",
        "P Sam Sahil",
        "Negev Taglicht",
        "Tomer Shabtay",
        "Atai Ambus",
        "Nitay Alon",
        "Shiri Oron",
        "Ayelet Gordon-Tapiero",
        "Yotam Kaplan",
        "Vered Shwartz",
        "Tamar Rott Shaham",
        "Christoph Riedl",
        "Reuth Mirsky",
        "Maarten Sap",
        "David Manheim",
        "Tomer Ullman",
        "David Bau"
      ],
      "abstract": "We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.",
      "arxiv_url": "http://arxiv.org/abs/2602.20021v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20021v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.20020v1",
      "title": "gencat: Generative computerized adaptive testing",
      "authors": [
        "Wanyong Feng",
        "Andrew Lan"
      ],
      "abstract": "Existing computerized Adaptive Testing (CAT) frameworks are typically built on predicting the correctness of a student response to a question. Although effective, this approach fails to leverage textual information in questions and responses, especially for open-ended questions. In this work, we propose GENCAT (\\textbf{GEN}erative \\textbf{CAT}), a novel CAT framework that leverages Large Language Models for knowledge estimate and question selection. First, we develop a Generative Item Response Theory (GIRT) model that enables us to estimate student knowledge from their open-ended responses and predict responses to unseen questions. We train the model in a two-step process, first via Supervised Fine-Tuning and then via preference optimization for knowledge-response alignment. Second, we introduce three question selection algorithms that leverage the generative capabilities of the GIRT model, based on the uncertainty, linguistic diversity, and information of sampled student responses. Third, we conduct experiments on two real-world programming datasets and demonstrate that GENCAT outperforms existing CAT baselines, achieving an AUC improvement of up to 4.32\\% in the key early testing stages.",
      "arxiv_url": "http://arxiv.org/abs/2602.20020v1",
      "pdf_url": "https://arxiv.org/pdf/2602.20020v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19987v1",
      "title": "Counterfactual Understanding via Retrieval-aware Multimodal Modeling for Time-to-Event Survival Prediction",
      "authors": [
        "Ha-Anh Hoang Nguyen",
        "Tri-Duc Phan Le",
        "Duc-Hoang Pham",
        "Huy-Son Nguyen",
        "Cam-Van Thi Nguyen",
        "Duc-Trong Le",
        "Hoang-Quynh Le"
      ],
      "abstract": "This paper tackles the problem of time-to-event counterfactual survival prediction, aiming to optimize individualized survival outcomes in the presence of heterogeneity and censored data. We propose CURE, a framework that advances counterfactual survival modeling via comprehensive multimodal embedding and latent subgroup retrieval. CURE integrates clinical, paraclinical, demographic, and multi-omics information, which are aligned and fused through cross-attention mechanisms. Complex multi-omics signals can be adaptively refined using a mixture-of-experts architecture, emphasizing the most informative omics components. Building upon this representation, CURE implicitly retrieves patient-specific latent subgroups that capture both baseline survival dynamics and treatment-dependent variations. Experimental results on METABRIC and TCGA-LUAD datasets demonstrate that proposed CURE model consistently outperforms strong baselines in survival analysis, evaluated using the Time-dependent Concordance Index ($C^{td}$) and Integrated Brier Score (IBS). These findings highlight the potential of CURE to enhance multimodal understanding and serve as a foundation for future treatment recommendation models. All code and related resources are publicly available to facilitate the reproducibility https://github.com/L2R-UET/CURE.",
      "arxiv_url": "http://arxiv.org/abs/2602.19987v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19987v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IR"
      ],
      "tags": [
        "Multi-Modal RAG"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19983v1",
      "title": "Contextual Safety Reasoning and Grounding for Open-World Robots",
      "authors": [
        "Zachary Ravichadran",
        "David Snyder",
        "Alexander Robey",
        "Hamed Hassani",
        "Vijay Kumar",
        "George J. Pappas"
      ],
      "abstract": "Robots are increasingly operating in open-world environments where safe behavior depends on context: the same hallway may require different navigation strategies when crowded versus empty, or during an emergency versus normal operations. Traditional safety approaches enforce fixed constraints in user-specified contexts, limiting their ability to handle the open-ended contextual variability of real-world deployment. We address this gap via CORE, a safety framework that enables online contextual reasoning, grounding, and enforcement without prior knowledge of the environment (e.g., maps or safety specifications). CORE uses a vision-language model (VLM) to continuously reason about context-dependent safety rules directly from visual observations, grounds these rules in the physical environment, and enforces the resulting spatially-defined safe sets via control barrier functions. We provide probabilistic safety guarantees for CORE that account for perceptual uncertainty, and we demonstrate through simulation and real-world experiments that CORE enforces contextually appropriate behavior in unseen environments, significantly outperforming prior semantic safety methods that lack online contextual reasoning. Ablation studies validate our theoretical guarantees and underscore the importance of both VLM-based reasoning and spatial grounding for enforcing contextual safety in novel settings. We provide additional resources at https://zacravichandran.github.io/CORE.",
      "arxiv_url": "http://arxiv.org/abs/2602.19983v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19983v1",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "tags": [
        "Multi-Modal RAG"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19969v1",
      "title": "ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting",
      "authors": [
        "Yuxing Tian",
        "Fengran Mo",
        "Weixu Zhang",
        "Yiyan Qi",
        "Jian-Yun Nie"
      ],
      "abstract": "The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods. However, they still face two major limitations. First, attention signals are highly concentrated a small subset of tokens within a few documents, making others indistinguishable. Second, attention often overemphasizes phrases lexically similar to the query, yielding biased rankings that irrelevant documents with mere lexical resemblance are regarded as relevant. In this paper, we propose \\textbf{ReAttn}, a post-hoc re-weighting strategy for attention-based re-ranking methods. It first compute the cross-document IDF weighting to down-weight attention on query-overlapping tokens that frequently appear across the candidate documents, reducing lexical bias and emphasizing distinctive terms. It then employs entropy-based regularization to mitigate over-concentrated attention, encouraging a more balanced distribution across informative tokens. Both adjustments operate directly on existing attention weights without additional training or supervision. Extensive experiments demonstrate the effectiveness of our method.",
      "arxiv_url": "http://arxiv.org/abs/2602.19969v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19969v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "tags": [
        "Information Retrieval",
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19961v1",
      "title": "Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval",
      "authors": [
        "Yibo Yan",
        "Jiahao Huo",
        "Guanbo Feng",
        "Mingdong Ou",
        "Yi Cao",
        "Xin Zou",
        "Shuliang Liu",
        "Yuanhuiyi Lyu",
        "Yu Huang",
        "Jungang Li",
        "Kening Zheng",
        "Xu Zheng",
        "Philip S. Yu",
        "James Kwok",
        "Xuming Hu"
      ],
      "abstract": "With the rapid proliferation of multimodal information, Visual Document Retrieval (VDR) has emerged as a critical frontier in bridging the gap between unstructured visually rich data and precise information acquisition. Unlike traditional natural image retrieval, visual documents exhibit unique characteristics defined by dense textual content, intricate layouts, and fine-grained semantic dependencies. This paper presents the first comprehensive survey of the VDR landscape, specifically through the lens of the Multimodal Large Language Model (MLLM) era. We begin by examining the benchmark landscape, and subsequently dive into the methodological evolution, categorizing approaches into three primary aspects: multimodal embedding models, multimodal reranker models, and the integration of Retrieval-Augmented Generation (RAG) and Agentic systems for complex document intelligence. Finally, we identify persistent challenges and outline promising future directions, aiming to provide a clear roadmap for future multimodal document intelligence.",
      "arxiv_url": "http://arxiv.org/abs/2602.19961v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19961v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "tags": [
        "Agentic AI",
        "Information Retrieval",
        "LLM",
        "Multi-Modal RAG",
        "RAG"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19948v1",
      "title": "Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming",
      "authors": [
        "Ian Steenstra",
        "Paola Pedrelli",
        "Weiyan Shi",
        "Stacy Marsella",
        "Timothy W. Bickmore"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with simulated patient agents equipped with dynamic cognitive-affective models and assesses therapy session simulations against a comprehensive quality of care and risk ontology. We apply this framework to a high-impact test case, Alcohol Use Disorder, evaluating six AI agents (including ChatGPT, Gemini, and Character.AI) against a clinically-validated cohort of 15 patient personas representing diverse clinical phenotypes.   Our large-scale simulation (N=369 sessions) reveals critical safety gaps in the use of AI for mental health support. We identify specific iatrogenic risks, including the validation of patient delusions (\"AI Psychosis\") and failure to de-escalate suicide risk. Finally, we validate an interactive data visualization dashboard with diverse stakeholders, including AI engineers and red teamers, mental health professionals, and policy experts (N=9), demonstrating that this framework effectively enables stakeholders to audit the \"black box\" of AI psychotherapy. These findings underscore the critical safety risks of AI-provided mental health support and the necessity of simulation-based clinical red teaming before deployment.",
      "arxiv_url": "http://arxiv.org/abs/2602.19948v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19948v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.MA"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19946v1",
      "title": "When Pretty Isn't Useful: Investigating Why Modern Text-to-Image Models Fail as Reliable Training Data Generators",
      "authors": [
        "Krzysztof Adamkiewicz",
        "Brian Moser",
        "Stanislav Frolov",
        "Tobias Christian Nauen",
        "Federico Raue",
        "Andreas Dengel"
      ],
      "abstract": "Recent text-to-image (T2I) diffusion models produce visually stunning images and demonstrate excellent prompt following. But do they perform well as synthetic vision data generators? In this work, we revisit the promise of synthetic data as a scalable substitute for real training sets and uncover a surprising performance regression. We generate large-scale synthetic datasets using state-of-the-art T2I models released between 2022 and 2025, train standard classifiers solely on this synthetic data, and evaluate them on real test data. Despite observable advances in visual fidelity and prompt adherence, classification accuracy on real test data consistently declines with newer T2I models as training data generators. Our analysis reveals a hidden trend: These models collapse to a narrow, aesthetic-centric distribution that undermines diversity and label-image alignment. Overall, our findings challenge a growing assumption in vision research, namely that progress in generative realism implies progress in data realism. We thus highlight an urgent need to rethink the capabilities of modern T2I models as reliable training data generators.",
      "arxiv_url": "http://arxiv.org/abs/2602.19946v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19946v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19945v1",
      "title": "DP-FedAdamW: An Efficient Optimizer for Differentially Private Federated Large Models",
      "authors": [
        "Jin Liu",
        "Yinbin Miao",
        "Ning Xi",
        "Junkang Liu"
      ],
      "abstract": "Balancing convergence efficiency and robustness under Differential Privacy (DP) is a central challenge in Federated Learning (FL). While AdamW accelerates training and fine-tuning in large-scale models, we find that directly applying it to Differentially Private FL (DPFL) suffers from three major issues: (i) data heterogeneity and privacy noise jointly amplify the variance of second-moment estimator, (ii) DP perturbations bias the second-moment estimator, and (iii) DP amplify AdamW sensitivity to local overfitting, worsening client drift. We propose DP-FedAdamW, the first AdamW-based optimizer for DPFL. It restores AdamW under DP by stabilizing second-moment variance, removing DP-induced bias, and aligning local updates to the global descent to curb client drift. Theoretically, we establish an unbiased second-moment estimator and prove a linearly accelerated convergence rate without any heterogeneity assumption, while providing tighter $(\\varepsilon,δ)$-DP guarantees. Our empirical results demonstrate the effectiveness of DP-FedAdamW across language and vision Transformers and ResNet-18. On Tiny-ImageNet (Swin-Base, $\\varepsilon=1$), DP-FedAdamW outperforms the state-of-the-art (SOTA) by 5.83\\%. The code is available in Appendix.",
      "arxiv_url": "http://arxiv.org/abs/2602.19945v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19945v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19938v1",
      "title": "A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs",
      "authors": [
        "Zijie Liu",
        "Jie Peng",
        "Jinhao Duan",
        "Zirui Liu",
        "Kaixiong Zhou",
        "Mingfu Liang",
        "Luke Simon",
        "Xi Liu",
        "Zhaozhuo Xu",
        "Tianlong Chen"
      ],
      "abstract": "Sparse Mixture-of-Experts (SMoE) architectures are increasingly used to scale large language models efficiently, delivering strong accuracy under fixed compute budgets. However, SMoE models often suffer from severe load imbalance across experts, where a small subset of experts receives most tokens while others are underutilized. Prior work has focused mainly on training-time solutions such as routing regularization or auxiliary losses, leaving inference-time behavior, which is critical for deployment, less explored.   We present a systematic analysis of expert routing during inference and identify three findings: (i) load imbalance persists and worsens with larger batch sizes, (ii) selection frequency does not reliably reflect expert importance, and (iii) overall expert workload and importance can be estimated using a small calibration set. These insights motivate inference-time mechanisms that rebalance workloads without retraining or router modification.   We propose Replicate-and-Quantize (R&Q), a training-free and near-lossless framework for dynamic workload rebalancing. In each layer, heavy-hitter experts are replicated to increase parallel capacity, while less critical experts and replicas are quantized to remain within the original memory budget. We also introduce a Load-Imbalance Score (LIS) to measure routing skew by comparing heavy-hitter load to an equal allocation baseline. Experiments across representative SMoE models and benchmarks show up to 1.4x reduction in imbalance with accuracy maintained within +/-0.6%, enabling more predictable and efficient inference.",
      "arxiv_url": "http://arxiv.org/abs/2602.19938v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19938v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19926v1",
      "title": "Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models",
      "authors": [
        "Jin Liu",
        "Yinbin Miao",
        "Ning Xi",
        "Junkang Liu"
      ],
      "abstract": "Fine-tuning large vision models (LVMs) and large language models (LLMs) under differentially private federated learning (DPFL) is hindered by a fundamental privacy-utility trade-off. Low-Rank Adaptation (LoRA), a promising parameter-efficient fine-tuning (PEFT) method, reduces computational and communication costs by introducing two trainable low-rank matrices while freezing pre-trained weights. However, directly applying LoRA in DPFL settings leads to performance degradation, especially in LVMs. Our analysis reveals three previously underexplored challenges: (1) gradient coupling caused by the simultaneous update of two asymmetric low-rank matrices, (2) compounded noise amplification under differential privacy, and (3) sharpness of the global aggregated model in the parameter space. To address these issues, we propose LA-LoRA (\\textbf{L}ocal \\textbf{A}lternating \\textbf{LoRA}), a novel approach that decouples gradient interactions and aligns update directions across clients to enhance robustness under stringent privacy constraints. Theoretically, LA-LoRA strengthens convergence guarantees in noisy federated environments. Extensive experiments demonstrate that LA-LoRA achieves state-of-the-art (SOTA) performance on Swin Transformer and RoBERTa models, showcasing robustness to DP noise and broad applicability across both LVMs and LLMs. For example, when fine-tuning the Swin-B model on the Tiny-ImageNet dataset under a strict privacy budget ($ε= 1$), LA-LoRA outperforms the best baseline, RoLoRA, by 16.83\\% in test accuracy. Code is provided in \\repolink.",
      "arxiv_url": "http://arxiv.org/abs/2602.19926v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19926v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19919v1",
      "title": "Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling",
      "authors": [
        "Xiang Li",
        "Zikai Wei",
        "Yiyan Qi",
        "Wanyun Zhou",
        "Xiang Liu",
        "Penglei Sun",
        "Yongqi Zhang",
        "Xiaowen Chu"
      ],
      "abstract": "Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual information as the primary source of trading signals in learning-based systems. Two key challenges hinder existing approaches: (1) the absence of large-scale, event-centric datasets that jointly model news semantics and statistically grounded market reactions, and (2) the misalignment between language model reasoning and financially valid trading behavior under dynamic market conditions. To address these challenges, we propose Janus-Q, an end-to-end event-driven trading framework that elevates financial news events from auxiliary signals to primary decision units. Janus-Q unifies event-centric data construction and model optimization under a two-stage paradigm. Stage I focuses on event-centric data construction, building a large-scale financial news event dataset comprising 62,400 articles annotated with 10 fine-grained event types, associated stocks, sentiment labels, and event-driven cumulative abnormal return (CAR). Stage II performs decision-oriented fine-tuning, combining supervised learning with reinforcement learning guided by a Hierarchical Gated Reward Model (HGRM), which explicitly captures trade-offs among multiple trading objectives. Extensive experiments demonstrate that Janus-Q achieves more consistent, interpretable, and profitable trading decisions than market indices and LLM baselines, improving the Sharpe Ratio by up to 102.0% while increasing direction accuracy by over 17.5% compared to the strongest competing strategies.",
      "arxiv_url": "http://arxiv.org/abs/2602.19919v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19919v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19914v1",
      "title": "Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning",
      "authors": [
        "Thatchawin Leelawat",
        "Lewis D Griffin"
      ],
      "abstract": "Existing benchmarks for AI reasoning provide limited insight into how closely these capabilities resemble human reasoning in naturalistic contexts. We present an adaptation of the Watson & Holmes detective tabletop game as a new benchmark designed to evaluate reasoning performance using incrementally presented narrative evidence, open-ended questions and unconstrained language responses. An automated grading system was developed and validated against human assessors to enable scalable and replicable performance evaluation. Results show a clear improvement in AI model performance over time. Over nine months of 2025, model performance rose from the lower quartile of the human comparison group to approximately the top 5%. Around half of this improvement reflects steady advancement across successive model releases, while the remainder corresponds to a marked step change associated with reasoning-oriented model architectures. Systematic differences in the performance of AI models compared to humans, dependent on features of the specific detection puzzle, were mostly absent with the exception of a fall in performance for models when solving longer cases (case lengths being in the range of 1900-4000 words), and an advantage at inductive reasoning for reasoning models at early stages of case solving when evidence was scant.",
      "arxiv_url": "http://arxiv.org/abs/2602.19914v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19914v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19895v1",
      "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning",
      "authors": [
        "Zhongwei Wan",
        "Yun Shen",
        "Zhihao Dou",
        "Donghao Zhou",
        "Yu Zhang",
        "Xin Wang",
        "Hui Shen",
        "Jing Xiong",
        "Chaofan Tao",
        "Zixuan Zhong",
        "Peizhou Huang",
        "Mi Zhang"
      ],
      "abstract": "Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.",
      "arxiv_url": "http://arxiv.org/abs/2602.19895v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19895v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19872v1",
      "title": "GOAL: Geometrically Optimal Alignment for Continual Generalized Category Discovery",
      "authors": [
        "Jizhou Han",
        "Chenhao Ding",
        "SongLin Dong",
        "Yuhang He",
        "Shaokun Wang",
        "Qiang Wang",
        "Yihong Gong"
      ],
      "abstract": "Continual Generalized Category Discovery (C-GCD) requires identifying novel classes from unlabeled data while retaining knowledge of known classes over time. Existing methods typically update classifier weights dynamically, resulting in forgetting and inconsistent feature alignment. We propose GOAL, a unified framework that introduces a fixed Equiangular Tight Frame (ETF) classifier to impose a consistent geometric structure throughout learning. GOAL conducts supervised alignment for labeled samples and confidence-guided alignment for novel samples, enabling stable integration of new classes without disrupting old ones. Experiments on four benchmarks show that GOAL outperforms the prior method Happy, reducing forgetting by 16.1% and boosting novel class discovery by 3.2%, establishing a strong solution for long-horizon continual discovery.",
      "arxiv_url": "http://arxiv.org/abs/2602.19872v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19872v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19844v1",
      "title": "LLM-enabled Applications Require System-Level Threat Monitoring",
      "authors": [
        "Yedi Zhang",
        "Haoyu Wang",
        "Xianglin Yang",
        "Jin Song Dong",
        "Jun Sun"
      ],
      "abstract": "LLM-enabled applications are rapidly reshaping the software ecosystem by using large language models as core reasoning components for complex task execution. This paradigm shift, however, introduces fundamentally new reliability challenges and significantly expands the security attack surface, due to the non-deterministic, learning-driven, and difficult-to-verify nature of LLM behavior. In light of these emerging and unavoidable safety challenges, we argue that such risks should be treated as expected operational conditions rather than exceptional events, necessitating a dedicated incident-response perspective. Consequently, the primary barrier to trustworthy deployment is not further improving model capability but establishing system-level threat monitoring mechanisms that can detect and contextualize security-relevant anomalies after deployment -- an aspect largely underexplored beyond testing or guardrail-based defenses. Accordingly, this position paper advocates systematic and comprehensive monitoring of security threats in LLM-enabled applications as a prerequisite for reliable operation and a foundation for dedicated incident-response frameworks.",
      "arxiv_url": "http://arxiv.org/abs/2602.19844v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19844v1",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19843v1",
      "title": "MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems",
      "authors": [
        "Jin Jia",
        "Zhiling Deng",
        "Zhuangbin Chen",
        "Yingqi Wang",
        "Zibin Zheng"
      ],
      "abstract": "As LLM-based Multi-Agent Systems (MAS) are increasingly deployed for complex tasks, ensuring their reliability has become a pressing challenge. Since MAS coordinate through unstructured natural language rather than rigid protocols, they are prone to semantic failures (e.g., hallucinations, misinterpreted instructions, and reasoning drift) that propagate silently without raising runtime exceptions. Prevailing evaluation approaches, which measure only end-to-end task success, offer limited insight into how these failures arise or how effectively agents recover from them. To bridge this gap, we propose MAS-FIRE, a systematic framework for fault injection and reliability evaluation of MAS. We define a taxonomy of 15 fault types covering intra-agent cognitive errors and inter-agent coordination failures, and inject them via three non-invasive mechanisms: prompt modification, response rewriting, and message routing manipulation. Applying MAS-FIRE to three representative MAS architectures, we uncover a rich set of fault-tolerant behaviors that we organize into four tiers: mechanism, rule, prompt, and reasoning. This tiered view enables fine-grained diagnosis of where and why systems succeed or fail. Our findings reveal that stronger foundation models do not uniformly improve robustness. We further show that architectural topology plays an equally decisive role, with iterative, closed-loop designs neutralizing over 40% of faults that cause catastrophic collapse in linear workflows. MAS-FIRE provides the process-level observability and actionable guidance needed to systematically improve multi-agent systems.",
      "arxiv_url": "http://arxiv.org/abs/2602.19843v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19843v1",
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19840v1",
      "title": "SAMAS: A Spectrum-Guided Multi-Agent System for Achieving Style Fidelity in Literary Translation",
      "authors": [
        "Jingzhuo Wu",
        "Jiajun Zhang",
        "Keyan Jin",
        "Dehua Ma",
        "Junbo Wang"
      ],
      "abstract": "Modern large language models (LLMs) excel at generating fluent and faithful translations. However, they struggle to preserve an author's unique literary style, often producing semantically correct but generic outputs. This limitation stems from the inability of current single-model and static multi-agent systems to perceive and adapt to stylistic variations. To address this, we introduce the Style-Adaptive Multi-Agent System (SAMAS), a novel framework that treats style preservation as a signal processing task. Specifically, our method quantifies literary style into a Stylistic Feature Spectrum (SFS) using the wavelet packet transform. This SFS serves as a control signal to dynamically assemble a tailored workflow of specialized translation agents based on the source text's structural patterns. Extensive experiments on translation benchmarks show that SAMAS achieves competitive semantic accuracy against strong baselines, primarily by leveraging its statistically significant advantage in style fidelity.",
      "arxiv_url": "http://arxiv.org/abs/2602.19840v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19840v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19810v1",
      "title": "OpenClaw, Moltbook, and ClawdLab: From Agent-Only Social Networks to Autonomous Scientific Research",
      "authors": [
        "Lukas Weidener",
        "Marko Brkić",
        "Mihailo Jovanović",
        "Ritvik Singh",
        "Emre Ulgac",
        "Aakaash Meduri"
      ],
      "abstract": "In January 2026, the open-source agent framework OpenClaw and the agent-only social network Moltbook produced a large-scale dataset of autonomous AI-to-AI interaction, attracting six academic publications within fourteen days. This study conducts a multivocal literature review of that ecosystem and presents ClawdLab, an open-source platform for autonomous scientific research, as a design science response to the architectural failure modes identified. The literature documents emergent collective phenomena, security vulnerabilities spanning 131 agent skills and over 15,200 exposed control panels, and five recurring architectural patterns. ClawdLab addresses these failure modes through hard role restrictions, structured adversarial critique, PI-led governance, multi-model orchestration, and domain-specific evidence requirements encoded as protocol constraints that ground validation in computational tool outputs rather than social consensus; the architecture provides emergent Sybil resistance as a structural consequence. A three-tier taxonomy distinguishes single-agent pipelines, predetermined multi-agent workflows, and fully decentralised systems, analysing why leading AI co-scientist platforms remain confined to the first two tiers. ClawdLab's composable third-tier architecture, in which foundation models, capabilities, governance, and evidence requirements are independently modifiable, enables compounding improvement as the broader AI ecosystem advances.",
      "arxiv_url": "http://arxiv.org/abs/2602.19810v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19810v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19799v1",
      "title": "Path-conditioned training: a principled way to rescale ReLU neural networks",
      "authors": [
        "Arthur Lebeurrier",
        "Titouan Vayer",
        "Rémi Gribonval"
      ],
      "abstract": "Despite recent algorithmic advances, we still lack principled ways to leverage the well-documented rescaling symmetries in ReLU neural network parameters. While two properly rescaled weights implement the same function, the training dynamics can be dramatically different. To offer a fresh perspective on exploiting this phenomenon, we build on the recent path-lifting framework, which provides a compact factorization of ReLU networks. We introduce a geometrically motivated criterion to rescale neural network parameters which minimization leads to a conditioning strategy that aligns a kernel in the path-lifting space with a chosen reference. We derive an efficient algorithm to perform this alignment. In the context of random network initialization, we analyze how the architecture and the initialization scale jointly impact the output of the proposed method. Numerical experiments illustrate its potential to speed up training.",
      "arxiv_url": "http://arxiv.org/abs/2602.19799v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19799v1",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.OC"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19743v1",
      "title": "NILE: Formalizing Natural-Language Descriptions of Formal Languages",
      "authors": [
        "Tristan Kneisel",
        "Marko Schmellenkamp",
        "Fabian Vehlken",
        "Thomas Zeume"
      ],
      "abstract": "This paper explores how natural-language descriptions of formal languages can be compared to their formal representations and how semantic differences can be explained. This is motivated from educational scenarios where learners describe a formal language (presented, e.g., by a finite state automaton, regular expression, pushdown automaton, context-free grammar or in set notation) in natural language, and an educational support system has to (1) judge whether the natural-language description accurately describes the formal language, and to (2) provide explanations why descriptions are not accurate.   To address this question, we introduce a representation language for formal languages, Nile, which is designed so that Nile expressions can mirror the syntactic structure of natural-language descriptions of formal languages. Nile is sufficiently expressive to cover a broad variety of formal languages, including all regular languages and fragments of context-free languages typically used in educational contexts. Generating Nile expressions that are syntactically close to natural-language descriptions then allows to provide explanations for inaccuracies in the descriptions algorithmically.   In experiments on an educational data set, we show that LLMs can translate natural-language descriptions into equivalent, syntactically close Nile expressions with high accuracy - allowing to algorithmically provide explanations for incorrect natural-language descriptions. Our experiments also show that while natural-language descriptions can also be translated into regular expressions (but not context-free grammars), the expressions are often not syntactically close and thus not suitable for providing explanations.",
      "arxiv_url": "http://arxiv.org/abs/2602.19743v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19743v1",
      "primary_category": "cs.FL",
      "categories": [
        "cs.FL",
        "cs.CL",
        "cs.LO"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19710v1",
      "title": "Universal Pose Pretraining for Generalizable Vision-Language-Action Policies",
      "authors": [
        "Haitao Lin",
        "Hanyang Yu",
        "Jingshun Huang",
        "He Zhang",
        "Yonggen Ling",
        "Ping Tan",
        "Xiangyang Xue",
        "Yanwei Fu"
      ],
      "abstract": "Existing Vision-Language-Action (VLA) models often suffer from feature collapse and low training efficiency because they entangle high-level perception with sparse, embodiment-specific action supervision. Since these models typically rely on VLM backbones optimized for Visual Question Answering (VQA), they excel at semantic identification but often overlook subtle 3D state variations that dictate distinct action patterns.   To resolve these misalignments, we propose Pose-VLA, a decoupled paradigm that separates VLA training into a pre-training phase for extracting universal 3D spatial priors in a unified camera-centric space, and a post-training phase for efficient embodiment alignment within robot-specific action space. By introducing discrete pose tokens as a universal representation, Pose-VLA seamlessly integrates spatial grounding from diverse 3D datasets with geometry-level trajectories from robotic demonstrations. Our framework follows a two-stage pre-training pipeline, establishing fundamental spatial grounding via poses followed by motion alignment through trajectory supervision.   Extensive evaluations demonstrate that Pose-VLA achieves state-of-the-art results on RoboTwin 2.0 with a 79.5% average success rate and competitive performance on LIBERO at 96.0%. Real-world experiments further showcase robust generalization across diverse objects using only 100 demonstrations per task, validating the efficiency of our pre-training paradigm.",
      "arxiv_url": "http://arxiv.org/abs/2602.19710v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19710v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19702v1",
      "title": "DReX: An Explainable Deep Learning-based Multimodal Recommendation Framework",
      "authors": [
        "Adamya Shyam",
        "Venkateswara Rao Kagita",
        "Bharti Rana",
        "Vikas Kumar"
      ],
      "abstract": "Multimodal recommender systems leverage diverse data sources, such as user interactions, content features, and contextual information, to address challenges like cold-start and data sparsity. However, existing methods often suffer from one or more key limitations: processing different modalities in isolation, requiring complete multimodal data for each interaction during training, or independent learning of user and item representations. These factors contribute to increased complexity and potential misalignment between user and item embeddings. To address these challenges, we propose DReX, a unified multimodal recommendation framework that incrementally refines user and item representations by leveraging interaction-level features from multimodal feedback. Our model employs gated recurrent units to selectively integrate these fine-grained features into global representations. This incremental update mechanism provides three key advantages: (1) simultaneous modeling of both nuanced interaction details and broader preference patterns, (2) eliminates the need for separate user and item feature extraction processes, leading to enhanced alignment in their learned representation, and (3) inherent robustness to varying or missing modalities. We evaluate the performance of the proposed approach on three real-world datasets containing reviews and ratings as interaction modalities. By considering review text as a modality, our approach automatically generates interpretable keyword profiles for both users and items, which supplement the recommendation process with interpretable preference indicators. Experiment results demonstrate that our approach outperforms state-of-the-art methods across all evaluated datasets.",
      "arxiv_url": "http://arxiv.org/abs/2602.19702v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19702v1",
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19679v1",
      "title": "TeHOR: Text-Guided 3D Human and Object Reconstruction with Textures",
      "authors": [
        "Hyeongjin Nam",
        "Daniel Sungho Jung",
        "Kyoung Mu Lee"
      ],
      "abstract": "Joint reconstruction of 3D human and object from a single image is an active research area, with pivotal applications in robotics and digital content creation. Despite recent advances, existing approaches suffer from two fundamental limitations. First, their reconstructions rely heavily on physical contact information, which inherently cannot capture non-contact human-object interactions, such as gazing at or pointing toward an object. Second, the reconstruction process is primarily driven by local geometric proximity, neglecting the human and object appearances that provide global context crucial for understanding holistic interactions. To address these issues, we introduce TeHOR, a framework built upon two core designs. First, beyond contact information, our framework leverages text descriptions of human-object interactions to enforce semantic alignment between the 3D reconstruction and its textual cues, enabling reasoning over a wider spectrum of interactions, including non-contact cases. Second, we incorporate appearance cues of the 3D human and object into the alignment process to capture holistic contextual information, thereby ensuring visually plausible reconstructions. As a result, our framework produces accurate and semantically coherent reconstructions, achieving state-of-the-art performance.",
      "arxiv_url": "http://arxiv.org/abs/2602.19679v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19679v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19668v1",
      "title": "Personalized Longitudinal Medical Report Generation via Temporally-Aware Federated Adaptation",
      "authors": [
        "He Zhu",
        "Ren Togo",
        "Takahiro Ogawa",
        "Kenji Hirata",
        "Minghui Tang",
        "Takaaki Yoshimura",
        "Hiroyuki Sugimori",
        "Noriko Nishioka",
        "Yukie Shimizu",
        "Kohsuke Kudo",
        "Miki Haseyama"
      ],
      "abstract": "Longitudinal medical report generation is clinically important yet remains challenging due to strict privacy constraints and the evolving nature of disease progression. Although federated learning (FL) enables collaborative training without data sharing, existing FL methods largely overlook longitudinal dynamics by assuming stationary client distributions, making them unable to model temporal shifts across visits or patient-specific heterogeneity-ultimately leading to unstable optimization and suboptimal report generation.   We introduce Federated Temporal Adaptation (FTA), a federated setting that explicitly accounts for the temporal evolution of client data. Building upon this setting, we propose FedTAR, a framework that integrates demographic-driven personalization with time-aware global aggregation. FedTAR generates lightweight LoRA adapters from demographic embeddings and performs temporal residual aggregation, where updates from different visits are weighted by a meta-learned temporal policy optimized via first-order MAML.   Experiments on J-MID (1M exams) and MIMIC-CXR demonstrate consistent improvements in linguistic accuracy, temporal coherence, and cross-site generalization, establishing FedTAR as a robust and privacy-preserving paradigm for federated longitudinal modeling.",
      "arxiv_url": "http://arxiv.org/abs/2602.19668v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19668v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19661v1",
      "title": "PaReGTA: An LLM-based EHR Data Encoding Approach to Capture Temporal Information",
      "authors": [
        "Kihyuk Yoon",
        "Lingchao Mao",
        "Catherine Chong",
        "Todd J. Schwedt",
        "Chia-Chun Chiang",
        "Jing Li"
      ],
      "abstract": "Temporal information in structured electronic health records (EHRs) is often lost in sparse one-hot or count-based representations, while sequence models can be costly and data-hungry. We propose PaReGTA, an LLM-based encoding framework that (i) converts longitudinal EHR events into visit-level templated text with explicit temporal cues, (ii) learns domain-adapted visit embeddings via lightweight contrastive fine-tuning of a sentence-embedding model, and (iii) aggregates visit embeddings into a fixed-dimensional patient representation using hybrid temporal pooling that captures both recency and globally informative visits. Because PaReGTA does not require training from scratch but instead utilizes a pre-trained LLM, it can perform well even in data-limited cohorts. Furthermore, PaReGTA is model-agnostic and can benefit from future EHR-specialized sentence-embedding models. For interpretability, we introduce PaReGTA-RSS (Representation Shift Score), which quantifies clinically defined factor importance by recomputing representations after targeted factor removal and projecting representation shifts through a machine learning model. On 39,088 migraine patients from the All of Us Research Program, PaReGTA outperforms sparse baselines for migraine type classification while deep sequential models were unstable in our cohort.",
      "arxiv_url": "http://arxiv.org/abs/2602.19661v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19661v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19643v1",
      "title": "KGHaluBench: A Knowledge Graph-Based Hallucination Benchmark for Evaluating the Breadth and Depth of LLM Knowledge",
      "authors": [
        "Alex Robertson",
        "Huizhi Liang",
        "Mahbub Gani",
        "Rohit Kumar",
        "Srijith Rajamohan"
      ],
      "abstract": "Large Language Models (LLMs) possess a remarkable capacity to generate persuasive and intelligible language. However, coherence does not equate to truthfulness, as the responses often contain subtle hallucinations. Existing benchmarks are limited by static and narrow questions, leading to limited coverage and misleading evaluations. We present KGHaluBench, a Knowledge Graph-based hallucination benchmark that assesses LLMs across the breadth and depth of their knowledge, providing a fairer and more comprehensive insight into LLM truthfulness. Our framework utilises the KG to dynamically construct challenging, multifaceted questions, whose difficulty is then statistically estimated to address popularity bias. Our automated verification pipeline detects abstentions and verifies the LLM's response at both conceptual and correctness levels to identify different types of hallucinations. We evaluate 25 frontier models, using novel accuracy and hallucination metrics. The results provide a more interpretable insight into the knowledge factors that cause hallucinations across different model sizes. KGHaluBench is publicly available to support future developments in hallucination mitigation.",
      "arxiv_url": "http://arxiv.org/abs/2602.19643v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19643v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19641v1",
      "title": "Evaluating the Impact of Data Anonymization on Image Retrieval",
      "authors": [
        "Marvin Chen",
        "Manuel Eberhardinger",
        "Johannes Maucher"
      ],
      "abstract": "With the growing importance of privacy regulations such as the General Data Protection Regulation, anonymizing visual data is becoming increasingly relevant across institutions. However, anonymization can negatively affect the performance of Computer Vision systems that rely on visual features, such as Content-Based Image Retrieval (CBIR). Despite this, the impact of anonymization on CBIR has not been systematically studied. This work addresses this gap, motivated by the DOKIQ project, an artificial intelligence-based system for document verification actively used by the State Criminal Police Office Baden-Württemberg. We propose a simple evaluation framework: retrieval results after anonymization should match those obtained before anonymization as closely as possible. To this end, we systematically assess the impact of anonymization using two public datasets and the internal DOKIQ dataset. Our experiments span three anonymization methods, four anonymization degrees, and four training strategies, all based on the state of the art backbone Self-Distillation with No Labels (DINO)v2. Our results reveal a pronounced retrieval bias in favor of models trained on original data, which produce the most similar retrievals after anonymization. The findings of this paper offer practical insights for developing privacy-compliant CBIR systems while preserving performance.",
      "arxiv_url": "http://arxiv.org/abs/2602.19641v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19641v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "tags": [
        "Multi-Modal RAG"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19631v1",
      "title": "Localized Concept Erasure in Text-to-Image Diffusion Models via High-Level Representation Misdirection",
      "authors": [
        "Uichan Lee",
        "Jeonghyeon Kim",
        "Sangheum Hwang"
      ],
      "abstract": "Recent advances in text-to-image (T2I) diffusion models have seen rapid and widespread adoption. However, their powerful generative capabilities raise concerns about potential misuse for synthesizing harmful, private, or copyrighted content. To mitigate such risks, concept erasure techniques have emerged as a promising solution. Prior works have primarily focused on fine-tuning the denoising component (e.g., the U-Net backbone). However, recent causal tracing studies suggest that visual attribute information is localized in the early self-attention layers of the text encoder, indicating a potential alternative for concept erasing. Building on this insight, we conduct preliminary experiments and find that directly fine-tuning early layers can suppress target concepts but often degrades the generation quality of non-target concepts. To overcome this limitation, we propose High-Level Representation Misdirection (HiRM), which misdirects high-level semantic representations of target concepts in the text encoder toward designated vectors such as random directions or semantically defined directions (e.g., supercategories), while updating only early layers that contain causal states of visual attributes. Our decoupling strategy enables precise concept removal with minimal impact on unrelated concepts, as demonstrated by strong results on UnlearnCanvas and NSFW benchmarks across diverse targets (e.g., objects, styles, nudity). HiRM also preserves generative utility at low training cost, transfers to state-of-the-art architectures such as Flux without additional training, and shows synergistic effects with denoiser-based concept erasing methods.",
      "arxiv_url": "http://arxiv.org/abs/2602.19631v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19631v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19626v1",
      "title": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding",
      "authors": [
        "Roberto Tacconelli"
      ],
      "abstract": "We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs.   On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.",
      "arxiv_url": "http://arxiv.org/abs/2602.19626v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19626v1",
      "primary_category": "cs.IT",
      "categories": [
        "cs.IT",
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19620v1",
      "title": "Rules or Weights? Comparing User Understanding of Explainable AI Techniques with the Cognitive XAI-Adaptive Model",
      "authors": [
        "Louth Bin Rawshan",
        "Zhuoyu Wang",
        "Brian Y Lim"
      ],
      "abstract": "Rules and Weights are popular XAI techniques for explaining AI decisions. Yet, it remains unclear how to choose between them, lacking a cognitive framework to compare their interpretability. In an elicitation user study on forward and counterfactual decision tasks, we identified 7 reasoning strategies of interpreting three XAI Schemas - weights, rules, and their hybrid. To analyze their capabilities, we propose CoXAM, a Cognitive XAI-Adaptive Model with shared memory representation to encode instance attributes, linear weights, and decision rules. CoXAM employs computational rationality to choose among reasoning processes based on the trade-off in utility and reasoning time, separately for forward or counterfactual decision tasks. In a validation study, CoXAM demonstrated a stronger alignment with human decision-making compared to baseline machine learning proxy models. The model successfully replicated and explained several key empirical findings, including that counterfactual tasks are inherently harder than forward tasks, decision tree rules are harder to recall and apply than linear weights, and the helpfulness of XAI depends on the application data context, alongside identifying which underlying reasoning strategies were most effective. With CoXAM, we contribute a cognitive basis to accelerate debugging and benchmarking disparate XAI techniques.",
      "arxiv_url": "http://arxiv.org/abs/2602.19620v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19620v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19614v1",
      "title": "Workflow-Level Design Principles for Trustworthy GenAI in Automotive System Engineering",
      "authors": [
        "Chih-Hong Cheng",
        "Brian Hsuan-Cheng Liao",
        "Adam Molin",
        "Hasan Esen"
      ],
      "abstract": "The adoption of large language models in safety-critical system engineering is constrained by trustworthiness, traceability, and alignment with established verification practices. We propose workflow-level design principles for trustworthy GenAI integration and demonstrate them in an end-to-end automotive pipeline, from requirement delta identification to SysML v2 architecture update and re-testing. First, we show that monolithic (\"big-bang\") prompting misses critical changes in large specifications, while section-wise decomposition with diversity sampling and lightweight NLP sanity checks improves completeness and correctness. Then, we propagate requirement deltas into SysML v2 models and validate updates via compilation and static analysis. Additionally, we ensure traceable regression testing by generating test cases through explicit mappings from specification variables to architectural ports and states, providing practical safeguards for GenAI used in safety-critical automotive engineering.",
      "arxiv_url": "http://arxiv.org/abs/2602.19614v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19614v1",
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19612v1",
      "title": "Anatomy of Unlearning: The Dual Impact of Fact Salience and Model Fine-Tuning",
      "authors": [
        "Borisiuk Anna",
        "Andrey Savchenko",
        "Alexander Panchecko",
        "Elena Tutubalina"
      ],
      "abstract": "Machine Unlearning (MU) enables Large Language Models (LLMs) to remove unsafe or outdated information. However, existing work assumes that all facts are equally forgettable and largely ignores whether the forgotten knowledge originates from pretraining or supervised fine-tuning (SFT). In this paper, we introduce DUAL (Dual Unlearning Evaluation across Training Stages), a benchmark of 28.6k Wikidata-derived triplets annotated with fact popularity using Wikipedia link counts and LLM-based salience scores. Our experiments show that pretrained and SFT models respond differently to unlearning. An SFT step on the forget data yields smoother forgetting, more stable tuning, and 10-50% higher retention, while direct unlearning on pretrained models remains unstable and prone to relearning or catastrophic forgetting.",
      "arxiv_url": "http://arxiv.org/abs/2602.19612v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19612v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19608v1",
      "title": "Satellite-Based Detection of Looted Archaeological Sites Using Machine Learning",
      "authors": [
        "Girmaw Abebe Tadesse",
        "Titien Bartette",
        "Andrew Hassanali",
        "Allen Kim",
        "Jonathan Chemla",
        "Andrew Zolli",
        "Yves Ubelmann",
        "Caleb Robinson",
        "Inbal Becker-Reshef",
        "Juan Lavista Ferres"
      ],
      "abstract": "Looting at archaeological sites poses a severe risk to cultural heritage, yet monitoring thousands of remote locations remains operationally difficult. We present a scalable and satellite-based pipeline to detect looted archaeological sites, using PlanetScope monthly mosaics (4.7m/pixel) and a curated dataset of 1,943 archaeological sites in Afghanistan (898 looted, 1,045 preserved) with multi-year imagery (2016--2023) and site-footprint masks. We compare (i) end-to-end CNN classifiers trained on raw RGB patches and (ii) traditional machine learning (ML) trained on handcrafted spectral/texture features and embeddings from recent remote-sensing foundation models. Results indicate that ImageNet-pretrained CNNs combined with spatial masking reach an F1 score of 0.926, clearly surpassing the strongest traditional ML setup, which attains an F1 score of 0.710 using SatCLIP-V+RF+Mean, i.e., location and vision embeddings fed into a Random Forest with mean-based temporal aggregation. Ablation studies demonstrate that ImageNet pretraining (even in the presence of domain shift) and spatial masking enhance performance. In contrast, geospatial foundation model embeddings perform competitively with handcrafted features, suggesting that looting signatures are extremely localized. The repository is available at https://github.com/microsoft/looted_site_detection.",
      "arxiv_url": "http://arxiv.org/abs/2602.19608v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19608v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19594v1",
      "title": "ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?",
      "authors": [
        "Ayush Nangia",
        "Shikhar Mishra",
        "Aman Gokrani",
        "Paras Chopra"
      ],
      "abstract": "We introduce ISO-Bench, a benchmark for coding agents to test their capabilities on real-world inference optimization tasks. These tasks were taken from vLLM and SGLang, two of the most popular LLM serving frameworks. Each task provides an agent with a codebase and bottleneck description, whereby the agent must produce an optimization patch evaluated against expert human solutions. We curated 54 tasks from merged pull requests with measurable performance improvements. While existing benchmarks heavily use runtime-based metrics, such approaches can be gamed to pass tests without capturing the actual intent of the code changes. Therefore, we combine both hard (execution-based) and soft (LLM-based) metrics to show that both are necessary for complete evaluation. While evaluating both closed and open-source coding agents, we find no single agent dominates across codebases. Surprisingly, agents often identify correct bottlenecks but fail to execute working solutions. We also show that agents with identical underlying models differ substantially, suggesting scaffolding is as important as the model.",
      "arxiv_url": "http://arxiv.org/abs/2602.19594v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19594v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19580v1",
      "title": "Leap+Verify: Regime-Adaptive Speculative Weight Prediction for Accelerating Neural Network Training",
      "authors": [
        "Jeremy McEntire"
      ],
      "abstract": "We introduce Leap+Verify, a framework that applies speculative execution -- predicting future model weights and validating predictions before acceptance -- to accelerate neural network training. Inspired by speculative decoding in language model inference and by the Automatically Scalable Computation (ASC) architecture for program execution, Leap+Verify decomposes training into three dynamically detected regimes (chaotic, transition, stable) using activation-space cosine similarity as a real-time Lyapunov proxy signal. Within each regime, analytic weight predictors (momentum, linear, quadratic extrapolation) attempt to forecast model parameters K training steps ahead; predictions are accepted only when validated against a held-out loss criterion. We evaluate Leap+Verify on GPT-2 124M and Qwen 2.5-1.5B trained on WikiText-103 across five random seeds, sweeping prediction depth K in {5, 10, 25, 50, 75, 100}. Momentum-based prediction (Adam moment extrapolation) fails catastrophically at both scales, with predicted losses exceeding actuals by 100-10,000x -- a universal norm explosion in optimizer-state extrapolation. Finite-difference predictors (linear, quadratic) succeed where momentum fails: at 124M, they achieve 24% strict acceptance at K=5 in stable regimes; at 1.5B, they achieve 37% strict acceptance in transition regimes. The scale-dependent finding is in regime distribution: GPT-2 124M spends 34% of training in stable regime, while Qwen 1.5B spends 64% in chaotic regime and reaches stable in only 0-2 of 40 checkpoints. Larger models are more predictable when predictable, but less often predictable -- the practical bottleneck shifts from predictor accuracy to regime availability. Cross-seed results are highly consistent (less than 1% validation loss variance), and the three-regime framework produces identical phase boundaries (plus or minus 50 steps) across seeds.",
      "arxiv_url": "http://arxiv.org/abs/2602.19580v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19580v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "econ.GN"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19578v1",
      "title": "Goal-Oriented Influence-Maximizing Data Acquisition for Learning and Optimization",
      "authors": [
        "Weichi Yao",
        "Bianca Dumitrascu",
        "Bryan R. Goldsmith",
        "Yixin Wang"
      ],
      "abstract": "Active data acquisition is central to many learning and optimization tasks in deep neural networks, yet remains challenging because most approaches rely on predictive uncertainty estimates that are difficult to obtain reliably. To this end, we propose Goal-Oriented Influence- Maximizing Data Acquisition (GOIMDA), an active acquisition algorithm that avoids explicit posterior inference while remaining uncertainty-aware through inverse curvature. GOIMDA selects inputs by maximizing their expected influence on a user-specified goal functional, such as test loss, predictive entropy, or the value of an optimizer-recommended design. Leveraging first-order influence functions, we derive a tractable acquisition rule that combines the goal gradient, training-loss curvature, and candidate sensitivity to model parameters. We show theoretically that, for generalized linear models, GOIMDA approximates predictive-entropy minimization up to a correction term accounting for goal alignment and prediction bias, thereby, yielding uncertainty-aware behavior without maintaining a Bayesian posterior. Empirically, across learning tasks (including image and text classification) and optimization tasks (including noisy global optimization benchmarks and neural-network hyperparameter tuning), GOIMDA consistently reaches target performance with substantially fewer labeled samples or function evaluations than uncertainty-based active learning and Gaussian-process Bayesian optimization baselines.",
      "arxiv_url": "http://arxiv.org/abs/2602.19578v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19578v1",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19574v1",
      "title": "CTC-TTS: LLM-based dual-streaming text-to-speech with CTC alignment",
      "authors": [
        "Hanwen Liu",
        "Saierdaer Yusuyin",
        "Hao Huang",
        "Zhijian Ou"
      ],
      "abstract": "Large-language-model (LLM)-based text-to-speech (TTS) systems can generate natural speech, but most are not designed for low-latency dual-streaming synthesis. High-quality dual-streaming TTS depends on accurate text--speech alignment and well-designed training sequences that balance synthesis quality and latency. Prior work often relies on GMM-HMM based forced-alignment toolkits (e.g., MFA), which are pipeline-heavy and less flexible than neural aligners; fixed-ratio interleaving of text and speech tokens struggles to capture text--speech alignment regularities. We propose CTC-TTS, which replaces MFA with a CTC based aligner and introduces a bi-word based interleaving strategy. Two variants are designed: CTC-TTS-L (token concatenation along the sequence length) for higher quality and CTC-TTS-F (embedding stacking along the feature dimension) for lower latency. Experiments show that CTC-TTS outperforms fixed-ratio interleaving and MFA-based baselines on streaming synthesis and zero-shot tasks. Speech samples are available at https://ctctts.github.io/.",
      "arxiv_url": "http://arxiv.org/abs/2602.19574v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19574v1",
      "primary_category": "eess.AS",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19562v1",
      "title": "A Multimodal Framework for Aligning Human Linguistic Descriptions with Visual Perceptual Data",
      "authors": [
        "Joseph Bingham"
      ],
      "abstract": "Establishing stable mappings between natural language expressions and visual percepts is a foundational problem for both cognitive science and artificial intelligence. Humans routinely ground linguistic reference in noisy, ambiguous perceptual contexts, yet the mechanisms supporting such cross-modal alignment remain poorly understood. In this work, we introduce a computational framework designed to model core aspects of human referential interpretation by integrating linguistic utterances with perceptual representations derived from large-scale, crowd-sourced imagery. The system approximates human perceptual categorization by combining scale-invariant feature transform (SIFT) alignment with the Universal Quality Index (UQI) to quantify similarity in a cognitively plausible feature space, while a set of linguistic preprocessing and query-transformation operations captures pragmatic variability in referring expressions. We evaluate the model on the Stanford Repeated Reference Game corpus (15,000 utterances paired with tangram stimuli), a paradigm explicitly developed to probe human-level perceptual ambiguity and coordination. Our framework achieves robust referential grounding. It requires 65\\% fewer utterances than human interlocutors to reach stable mappings and can correctly identify target objects from single referring expressions 41.66\\% of the time (versus 20\\% for humans).These results suggest that relatively simple perceptual-linguistic alignment mechanisms can yield human-competitive behavior on a classic cognitive benchmark, and offers insights into models of grounded communication, perceptual inference, and cross-modal concept formation. Code is available at https://anonymous.4open.science/r/metasequoia-9D13/README.md .",
      "arxiv_url": "http://arxiv.org/abs/2602.19562v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19562v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19555v1",
      "title": "Agentic AI as a Cybersecurity Attack Surface: Threats, Exploits, and Defenses in Runtime Supply Chains",
      "authors": [
        "Xiaochong Jiang",
        "Shiqi Yang",
        "Wenting Yang",
        "Yichen Liu",
        "Cheng Ji"
      ],
      "abstract": "Agentic systems built on large language models (LLMs) extend beyond text generation to autonomously retrieve information and invoke tools. This runtime execution model shifts the attack surface from build-time artifacts to inference-time dependencies, exposing agents to manipulation through untrusted data and probabilistic capability resolution. While prior work has focused on model-level vulnerabilities, security risks emerging from cyclic and interdependent runtime behavior remain fragmented. We systematize these risks within a unified runtime framework, categorizing threats into data supply chain attacks (transient context injection and persistent memory poisoning) and tool supply chain attacks (discovery, implementation, and invocation). We further identify the Viral Agent Loop, in which agents act as vectors for self-propagating generative worms without exploiting code-level flaws. Finally, we advocate a Zero-Trust Runtime Architecture that treats context as untrusted control flow and constrains tool execution through cryptographic provenance rather than semantic inference.",
      "arxiv_url": "http://arxiv.org/abs/2602.19555v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19555v1",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19549v1",
      "title": "Sculpting the Vector Space: Towards Efficient Multi-Vector Visual Document Retrieval via Prune-then-Merge Framework",
      "authors": [
        "Yibo Yan",
        "Mingdong Ou",
        "Yi Cao",
        "Xin Zou",
        "Jiahao Huo",
        "Shuliang Liu",
        "James Kwok",
        "Xuming Hu"
      ],
      "abstract": "Visual Document Retrieval (VDR), which aims to retrieve relevant pages within vast corpora of visually-rich documents, is of significance in current multimodal retrieval applications. The state-of-the-art multi-vector paradigm excels in performance but suffers from prohibitive overhead, a problem that current efficiency methods like pruning and merging address imperfectly, creating a difficult trade-off between compression rate and feature fidelity. To overcome this dilemma, we introduce Prune-then-Merge, a novel two-stage framework that synergizes these complementary approaches. Our method first employs an adaptive pruning stage to filter out low-information patches, creating a refined, high-signal set of embeddings. Subsequently, a hierarchical merging stage compresses this pre-filtered set, effectively summarizing semantic content without the noise-induced feature dilution seen in single-stage methods. Extensive experiments on 29 VDR datasets demonstrate that our framework consistently outperforms existing methods, significantly extending the near-lossless compression range and providing robust performance at high compression ratios.",
      "arxiv_url": "http://arxiv.org/abs/2602.19549v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19549v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.IR"
      ],
      "tags": [
        "Information Retrieval"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19548v1",
      "title": "Beyond a Single Extractor: Re-thinking HTML-to-Text Extraction for LLM Pretraining",
      "authors": [
        "Jeffrey Li",
        "Josh Gardner",
        "Doug Kang",
        "Fangping Shi",
        "Karanjeet Singh",
        "Chun-Liang Li",
        "Herumb Shandilya",
        "David Hall",
        "Oncel Tuzel",
        "Percy Liang",
        "Ludwig Schmidt",
        "Hadi Pour Ansari",
        "Fartash Faghri"
      ],
      "abstract": "One of the first pre-processing steps for constructing web-scale LLM pretraining datasets involves extracting text from HTML. Despite the immense diversity of web content, existing open-source datasets predominantly apply a single fixed extractor to all webpages. In this work, we investigate whether this practice leads to suboptimal coverage and utilization of Internet data. We first show that while different extractors may lead to similar model performance on standard language understanding tasks, the pages surviving a fixed filtering pipeline can differ substantially. This suggests a simple intervention: by taking a Union over different extractors, we can increase the token yield of DCLM-Baseline by up to 71% while maintaining benchmark performance. We further show that for structured content such as tables and code blocks, extractor choice can significantly impact downstream task performance, with differences of up to 10 percentage points (p.p.) on WikiTQ and 3 p.p. on HumanEval.",
      "arxiv_url": "http://arxiv.org/abs/2602.19548v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19548v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19539v1",
      "title": "Can a Teenager Fool an AI? Evaluating Low-Cost Cosmetic Attacks on Age Estimation Systems",
      "authors": [
        "Xingyu Shen",
        "Tommy Duong",
        "Xiaodong An",
        "Zengqi Zhao",
        "Zebang Hu",
        "Haoyu Hu",
        "Ziyou Wang",
        "Finn Guo",
        "Simiao Ren"
      ],
      "abstract": "Age estimation systems are increasingly deployed as gatekeepers for age-restricted online content, yet their robustness to cosmetic modifications has not been systematically evaluated. We investigate whether simple, household-accessible cosmetic changes, including beards, grey hair, makeup, and simulated wrinkles, can cause AI age estimators to classify minors as adults. To study this threat at scale without ethical concerns, we simulate these physical attacks on 329 facial images of individuals aged 10 to 21 using a VLM image editor (Gemini 2.5 Flash Image). We then evaluate eight models from our prior benchmark: five specialized architectures (MiVOLO, Custom-Best, Herosan, MiViaLab, DEX) and three vision-language models (Gemini 3 Flash, Gemini 2.5 Flash, GPT-5-Nano). We introduce the Attack Conversion Rate (ACR), defined as the fraction of images predicted as minor at baseline that flip to adult after attack, a population-agnostic metric that does not depend on the ratio of minors to adults in the test set. Our results reveal that a synthetic beard alone achieves 28 to 69 percent ACR across all eight models; combining all four attacks shifts predicted age by +7.7 years on average across all 329 subjects and reaches up to 83 percent ACR; and vision-language models exhibit lower ACR (59 to 71 percent) than specialized models (63 to 83 percent) under the full attack, although the ACR ranges overlap and the difference is not statistically tested. These findings highlight a critical vulnerability in deployed age-verification pipelines and call for adversarial robustness evaluation as a mandatory criterion for model selection.",
      "arxiv_url": "http://arxiv.org/abs/2602.19539v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19539v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.CR",
        "cs.LG"
      ],
      "tags": [
        "LLM",
        "Multi-Modal RAG"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19538v1",
      "title": "Cost-Aware Diffusion Active Search",
      "authors": [
        "Arundhati Banerjee",
        "Jeff Schneider"
      ],
      "abstract": "Active search for recovering objects of interest through online, adaptive decision making with autonomous agents requires trading off exploration of unknown environments with exploitation of prior observations in the search space. Prior work has proposed information gain and Thompson sampling based myopic, greedy approaches for agents to actively decide query or search locations when the number of targets is unknown. Decision making algorithms in such partially observable environments have also shown that agents capable of lookahead over a finite horizon outperform myopic policies for active search. Unfortunately, lookahead algorithms typically rely on building a computationally expensive search tree that is simulated and updated based on the agent's observations and a model of the environment dynamics. Instead, in this work, we leverage the sequence modeling abilities of diffusion models to sample lookahead action sequences that balance the exploration-exploitation trade-off for active search without building an exhaustive search tree. We identify the optimism bias in prior diffusion based reinforcement learning approaches when applied to the active search setting and propose mitigating solutions for efficient cost-aware decision making with both single and multi-agent teams. Our proposed algorithm outperforms standard baselines in offline reinforcement learning in terms of full recovery rate and is computationally more efficient than tree search in cost-aware active decision making.",
      "arxiv_url": "http://arxiv.org/abs/2602.19538v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19538v1",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19534v1",
      "title": "Large Language Model-Assisted UAV Operations and Communications: A Multifaceted Survey and Tutorial",
      "authors": [
        "Yousef Emami",
        "Hao Zhou",
        "Radha Reddy",
        "Atefeh Hajijamali Arani",
        "Biliang Wang",
        "Kai Li",
        "Luis Almeida",
        "Zhu Han"
      ],
      "abstract": "Uncrewed Aerial Vehicles (UAVs) are widely deployed across diverse applications due to their mobility and agility. Recent advances in Large Language Models (LLMs) offer a transformative opportunity to enhance UAV intelligence beyond conventional optimization-based and learning-based approaches. By integrating LLMs into UAV systems, advanced environmental understanding, swarm coordination, mobility optimization, and high-level task reasoning can be achieved, thereby allowing more adaptive and context-aware aerial operations. This survey systematically explores the intersection of LLMs and UAV technologies and proposes a unified framework that consolidates existing architectures, methodologies, and applications for UAVs. We first present a structured taxonomy of LLM adaptation techniques for UAVs, including pretraining, fine-tuning, Retrieval-Augmented Generation (RAG), and prompt engineering, along with key reasoning capabilities such as Chain-of-Thought (CoT) and In-Context Learning (ICL). We then examine LLM-assisted UAV communications and operations, covering navigation, mission planning, swarm control, safety, autonomy, and network management. After that, the survey further discusses Multimodal LLMs (MLLMs) for human-swarm interaction, perception-driven navigation, and collaborative control. Finally, we address ethical considerations, including bias, transparency, accountability, and Human-in-the-Loop (HITL) strategies, and outline future research directions. Overall, this work positions LLM-assisted UAVs as a foundation for intelligent and adaptive aerial systems.",
      "arxiv_url": "http://arxiv.org/abs/2602.19534v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19534v1",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19519v1",
      "title": "Ada-RS: Adaptive Rejection Sampling for Selective Thinking",
      "authors": [
        "Yirou Ge",
        "Yixi Li",
        "Alec Chiu",
        "Shivani Shekhar",
        "Zijie Pan",
        "Avinash Thangali",
        "Yun-Shiuan Chuang",
        "Chaitanya Kulkarni",
        "Uma Kona",
        "Linsey Pang",
        "Prakhar Mehrotra"
      ],
      "abstract": "Large language models (LLMs) are increasingly being deployed in cost and latency-sensitive settings. While chain-of-thought improves reasoning, it can waste tokens on simple requests. We study selective thinking for tool-using LLMs and introduce Adaptive Rejection Sampling (Ada-RS), an algorithm-agnostic sample filtering framework for learning selective and efficient reasoning. For each given context, Ada-RS scores multiple sampled completions with an adaptive length-penalized reward then applies stochastic rejection sampling to retain only high-reward candidates (or preference pairs) for downstream optimization. We demonstrate how Ada-RS plugs into both preference pair (e.g. DPO) or grouped policy optimization strategies (e.g. DAPO). Using Qwen3-8B with LoRA on a synthetic tool call-oriented e-commerce benchmark, Ada-RS improves the accuracy-efficiency frontier over standard algorithms by reducing average output tokens by up to 80% and reducing thinking rate by up to 95% while maintaining or improving tool call accuracy. These results highlight that training-signal selection is a powerful lever for efficient reasoning in latency-sensitive deployments.",
      "arxiv_url": "http://arxiv.org/abs/2602.19519v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19519v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19517v1",
      "title": "Classroom Final Exam: An Instructor-Tested Reasoning Benchmark",
      "authors": [
        "Chongyang Gao",
        "Diji Yang",
        "Shuyan Zhou",
        "Xichen Yan",
        "Luchuan Song",
        "Shuo Li",
        "Kezhen Chen"
      ],
      "abstract": "We introduce \\CFE{} (\\textbf{C}lassroom \\textbf{F}inal \\textbf{E}xam), a multimodal benchmark for evaluating the reasoning capabilities of large language models across more than 20 STEM domains. \\CFE{} is curated from repeatedly used, authentic university homework and exam problems, together with reference solutions provided by course instructors. \\CFE{} presents a significant challenge even for frontier models: the newly released Gemini-3.1-pro-preview achieves an overall accuracy of 59.69\\%, while the second-best model, Gemini-3-flash-preview, reaches 55.46\\%, leaving considerable room for improvement. Beyond leaderboard results, we perform a diagnostic analysis by decomposing reference solutions into reasoning flows. We find that although frontier models can often answer intermediate sub-questions correctly, they struggle to reliably derive and maintain correct intermediate states throughout multi-step solutions. We further observe that model-generated solutions typically have more reasoning steps than those provided by the instructor, indicating suboptimal step efficiency and a higher risk of error accumulation. The data and code are available at https://github.com/Analogy-AI/CFE_Bench.",
      "arxiv_url": "http://arxiv.org/abs/2602.19517v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19517v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.CL",
        "cs.CV"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19509v1",
      "title": "Pyramid MoA: A Probabilistic Framework for Cost-Optimized Anytime Inference",
      "authors": [
        "Arindam Khaled"
      ],
      "abstract": "Large Language Models (LLMs) face a persistent trade-off between inference cost and reasoning capability. While \"Oracle\" models (e.g., Llama-3-70B) achieve state-of-the-art accuracy, they are prohibitively expensive for high-volume deployment. Smaller models (e.g., 8B parameters) are cost-effective but struggle with complex tasks. In this work, we propose \"Pyramid MoA\", a hierarchical Mixture-of-Agents architecture that uses a lightweight Router to dynamically escalate queries only when necessary. By leveraging semantic agreement and confidence calibration among an ensemble of small models, our Router identifies \"hard\" problems with high precision. On the GSM8K benchmark, our system achieves 93.0% accuracy, effectively matching the Oracle baseline (98.0%) while reducing compute costs by 61%. We demonstrate that the system introduces negligible latency overhead (+0.82s) and allows for a tunable trade-off between performance and budget.",
      "arxiv_url": "http://arxiv.org/abs/2602.19509v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19509v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19502v1",
      "title": "Human-Guided Agentic AI for Multimodal Clinical Prediction: Lessons from the AgentDS Healthcare Benchmark",
      "authors": [
        "Lalitha Pranathi Pulavarthy",
        "Raajitha Muthyala",
        "Aravind V Kuruvikkattil",
        "Zhenan Yin",
        "Rashmita Kudamala",
        "Saptarshi Purkayastha"
      ],
      "abstract": "Agentic AI systems are increasingly capable of autonomous data science workflows, yet clinical prediction tasks demand domain expertise that purely automated approaches struggle to provide. We investigate how human guidance of agentic AI can improve multimodal clinical prediction, presenting our approach to all three AgentDS Healthcare benchmark challenges: 30-day hospital readmission prediction (Macro-F1 = 0.8986), emergency department cost forecasting (MAE = $465.13), and discharge readiness assessment (Macro-F1 = 0.7939). Across these tasks, human analysts directed the agentic workflow at key decision points, multimodal feature engineering from clinical notes, scanned PDF billing receipts, and time-series vital signs; task-appropriate model selection; and clinically informed validation strategies. Our approach ranked 5th overall in the healthcare domain, with a 3rd-place finish on the discharge readiness task. Ablation studies reveal that human-guided decisions compounded to a cumulative gain of +0.065 F1 over automated baselines, with multimodal feature extraction contributing the largest single improvement (+0.041 F1). We distill three generalizable lessons: (1) domain-informed feature engineering at each pipeline stage yields compounding gains that outperform extensive automated search; (2) multimodal data integration requires task-specific human judgment that no single extraction strategy generalizes across clinical text, PDFs, and time-series; and (3) deliberate ensemble diversity with clinically motivated model configurations outperforms random hyperparameter search. These findings offer practical guidance for teams deploying agentic AI in healthcare settings where interpretability, reproducibility, and clinical validity are essential.",
      "arxiv_url": "http://arxiv.org/abs/2602.19502v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19502v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19491v1",
      "title": "Botson: An Accessible and Low-Cost Platform for Social Robotics Research",
      "authors": [
        "Samuel Bellaire",
        "Abdalmalek Abu-raddaha",
        "Natalie Kim",
        "Nathan Morhan",
        "William Elliott",
        "Samir Rawashdeh"
      ],
      "abstract": "Trust remains a critical barrier to the effective integration of Artificial Intelligence (AI) into human-centric domains. Disembodied agents, such as voice assistants, often fail to establish trust due to their inability to convey non-verbal social cues. This paper introduces the architecture of Botson: an anthropomorphic social robot powered by a large language model (LLM). Botson was created as a low-cost and accessible platform for social robotics research.",
      "arxiv_url": "http://arxiv.org/abs/2602.19491v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19491v1",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19483v1",
      "title": "Making Conformal Predictors Robust in Healthcare Settings: a Case Study on EEG Classification",
      "authors": [
        "Arjun Chatterjee",
        "Sayeed Sajjad Razin",
        "John Wu",
        "Siddhartha Laghuvarapu",
        "Jathurshan Pradeepkumar",
        "Jimeng Sun"
      ],
      "abstract": "Quantifying uncertainty in clinical predictions is critical for high-stakes diagnosis tasks. Conformal prediction offers a principled approach by providing prediction sets with theoretical coverage guarantees. However, in practice, patient distribution shifts violate the i.i.d. assumptions underlying standard conformal methods, leading to poor coverage in healthcare settings. In this work, we evaluate several conformal prediction approaches on EEG seizure classification, a task with known distribution shift challenges and label uncertainty. We demonstrate that personalized calibration strategies can improve coverage by over 20 percentage points while maintaining comparable prediction set sizes. Our implementation is available via PyHealth, an open-source healthcare AI framework: https://github.com/sunlabuiuc/PyHealth.",
      "arxiv_url": "http://arxiv.org/abs/2602.19483v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19483v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19467v1",
      "title": "Can Large Language Models Replace Human Coders? Introducing ContentBench",
      "authors": [
        "Michael Haman"
      ],
      "abstract": "Can low-cost large language models (LLMs) take over the interpretive coding work that still anchors much of empirical content analysis? This paper introduces ContentBench, a public benchmark suite that helps answer this replacement question by tracking how much agreement low-cost LLMs achieve and what they cost on the same interpretive coding tasks. The suite uses versioned tracks that invite researchers to contribute new benchmark datasets. I report results from the first track, ContentBench-ResearchTalk v1.0: 1,000 synthetic, social-media-style posts about academic research labeled into five categories spanning praise, critique, sarcasm, questions, and procedural remarks. Reference labels are assigned only when three state-of-the-art reasoning models (GPT-5, Gemini 2.5 Pro, and Claude Opus 4.1) agree unanimously, and all final labels are checked by the author as a quality-control audit. Among the 59 evaluated models, the best low-cost LLMs reach roughly 97-99% agreement with these jury labels, far above GPT-3.5 Turbo, the model behind early ChatGPT and the initial wave of LLM-based text annotation. Several top models can code 50,000 posts for only a few dollars, pushing large-scale interpretive coding from a labor bottleneck toward questions of validation, reporting, and governance. At the same time, small open-weight models that run locally still struggle on sarcasm-heavy items (for example, Llama 3.2 3B reaches only 4% agreement on hard-sarcasm). ContentBench is released with data, documentation, and an interactive quiz at contentbench.github.io to support comparable evaluations over time and to invite community extensions.",
      "arxiv_url": "http://arxiv.org/abs/2602.19467v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19467v1",
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19463v1",
      "title": "PuppetChat: Fostering Intimate Communication through Bidirectional Actions and Micronarratives",
      "authors": [
        "Emma Jiren Wang",
        "Siying Hu",
        "Zhicong Lu"
      ],
      "abstract": "As a primary channel for sustaining modern intimate relationships, instant messaging facilitates frequent connection across distances. However, today's tools often dilute care; they favor single tap reactions and vague emojis that do not support two way action responses, do not preserve the feeling that the exchange keeps going without breaking, and are weakly tied to who we are and what we share. To address this challenge, we present PuppetChat, a dyadic messaging prototype that restores this expressive depth through embodied interaction. PuppetChat uses a reciprocity aware recommender to encourage responsive actions and generates personalized micronarratives from user stories to ground interactions in personal history. Our 10-day field study with 11 dyads of close partners or friends revealed that this approach enhanced social presence, supported more expressive self disclosure, and sustained continuity and shared memories.",
      "arxiv_url": "http://arxiv.org/abs/2602.19463v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19463v1",
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19458v1",
      "title": "ComplLLM: Fine-tuning LLMs to Discover Complementary Signals for Decision-making",
      "authors": [
        "Ziyang Guo",
        "Yifan Wu",
        "Jason Hartline",
        "Kenneth Holstein",
        "Jessica Hullman"
      ],
      "abstract": "Multi-agent decision pipelines can outperform single agent workflows when complementarity holds, i.e., different agents bring unique information to the table to inform a final decision. We propose ComplLLM, a post-training framework based on decision theory that fine-tunes a decision-assistant LLM using complementary information as reward to output signals that complement existing agent decisions. We validate ComplLLM on synthetic and real-world tasks involving domain experts, demonstrating how the approach recovers known complementary information and produces plausible explanations of complementary signals to support downstream decision-makers.",
      "arxiv_url": "http://arxiv.org/abs/2602.19458v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19458v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19455v1",
      "title": "SenTSR-Bench: Thinking with Injected Knowledge for Time-Series Reasoning",
      "authors": [
        "Zelin He",
        "Boran Han",
        "Xiyuan Zhang",
        "Shuai Zhang",
        "Haotian Lin",
        "Qi Zhu",
        "Haoyang Fang",
        "Danielle C. Maddix",
        "Abdul Fatir Ansari",
        "Akash Chandrayan",
        "Abhinav Pradhan",
        "Bernie Wang",
        "Matthew Reimherr"
      ],
      "abstract": "Time-series diagnostic reasoning is essential for many applications, yet existing solutions face a persistent gap: general reasoning large language models (GRLMs) possess strong reasoning skills but lack the domain-specific knowledge to understand complex time-series patterns. Conversely, fine-tuned time-series LLMs (TSLMs) understand these patterns but lack the capacity to generalize reasoning for more complicated questions. To bridge this gap, we propose a hybrid knowledge-injection framework that injects TSLM-generated insights directly into GRLM's reasoning trace, thereby achieving strong time-series reasoning with in-domain knowledge. As collecting data for knowledge injection fine-tuning is costly, we further leverage a reinforcement learning-based approach with verifiable rewards (RLVR) to elicit knowledge-rich traces without human supervision, then transfer such an in-domain thinking trace into GRLM for efficient knowledge injection. We further release SenTSR-Bench, a multivariate time-series-based diagnostic reasoning benchmark collected from real-world industrial operations. Across SenTSR-Bench and other public datasets, our method consistently surpasses TSLMs by 9.1%-26.1% and GRLMs by 7.9%-22.4%, delivering robust, context-aware time-series diagnostic insights.",
      "arxiv_url": "http://arxiv.org/abs/2602.19455v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19455v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19450v1",
      "title": "Red-Teaming Claude Opus and ChatGPT-based Security Advisors for Trusted Execution Environments",
      "authors": [
        "Kunal Mukherjee"
      ],
      "abstract": "Trusted Execution Environments (TEEs) (e.g., Intel SGX and ArmTrustZone) aim to protect sensitive computation from a compromised operating system, yet real deployments remain vulnerable to microarchitectural leakage, side-channel attacks, and fault injection. In parallel, security teams increasingly rely on Large Language Model (LLM) assistants as security advisors for TEE architecture review, mitigation planning, and vulnerability triage. This creates a socio-technical risk surface: assistants may hallucinate TEE mechanisms, overclaim guarantees (e.g., what attestation does and does not establish), or behave unsafely under adversarial prompting.   We present a red-teaming study of two prevalently deployed LLM assistants in the role of TEE security advisors: ChatGPT-5.2 and Claude Opus-4.6, focusing on the inherent limitations and transferability of prompt-induced failures across LLMs. We introduce TEE-RedBench, a TEE-grounded evaluation methodology comprising (i) a TEE-specific threat model for LLM-mediated security work, (ii) a structured prompt suite spanning SGX and TrustZone architecture, attestation and key management, threat modeling, and non-operational mitigation guidance, along with policy-bound misuse probes, and (iii) an annotation rubric that jointly measures technical correctness, groundedness, uncertainty calibration, refusal quality, and safe helpfulness. We find that some failures are not purely idiosyncratic, transferring up to 12.02% across LLM assistants, and we connect these outcomes to secure architecture by outlining an \"LLM-in-the-loop\" evaluation pipeline: policy gating, retrieval grounding, structured templates, and lightweight verification checks that, when combined, reduce failures by 80.62%.",
      "arxiv_url": "http://arxiv.org/abs/2602.19450v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19450v1",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19441v1",
      "title": "When AI Teammates Meet Code Review: Collaboration Signals Shaping the Integration of Agent-Authored Pull Requests",
      "authors": [
        "Costain Nachuma",
        "Minhaz Zibran"
      ],
      "abstract": "Autonomous coding agents increasingly contribute to software development by submitting pull requests on GitHub; yet, little is known about how these contributions integrate into human-driven review workflows. We present a large empirical study of agent-authored pull requests using the public AIDev dataset, examining integration outcomes, resolution speed, and review-time collaboration signals. Using logistic regression with repository-clustered standard errors, we find that reviewer engagement has the strongest correlation with successful integration, whereas larger change sizes and coordination-disrupting actions, such as force pushes, are associated with a lower likelihood of merging. In contrast, iteration intensity alone provides limited explanatory power once collaboration signals are considered. A qualitative analysis further shows that successful integration occurs when agents engage in actionable review loops that converge toward reviewer expectations. Overall, our results highlight that the effective integration of agent-authored pull requests depends not only on code quality but also on alignment with established review and coordination practices.",
      "arxiv_url": "http://arxiv.org/abs/2602.19441v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19441v1",
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19439v1",
      "title": "OptiRepair: Closed-Loop Diagnosis and Repair of Supply Chain Optimization Models with LLM Agents",
      "authors": [
        "Ruicheng Ao",
        "David Simchi-Levi",
        "Xinshang Wang"
      ],
      "abstract": "Problem Definition. Supply chain optimization models frequently become infeasible because of modeling errors. Diagnosis and repair require scarce OR expertise: analysts must interpret solver diagnostics, trace root causes across echelons, and fix formulations without sacrificing operational soundness. Whether AI agents can perform this task remains untested.   Methodology/Results. OptiRepair splits this task into a domain-agnostic feasibility phase (iterative IIS-guided repair of any LP) and a domain-specific validation phase (five rationality checks grounded in inventory theory). We test 22 API models from 7 families on 976 multi-echelon supply chain problems and train two 8B-parameter models using self-taught reasoning with solver-verified rewards. The trained models reach 81.7% Rational Recovery Rate (RRR) -- the fraction of problems resolved to both feasibility and operational rationality -- versus 42.2% for the best API model and 21.3% on average. The gap concentrates in Phase 1 repair: API models average 27.6% recovery rate versus 97.2% for trained models.   Managerial Implications. Two gaps separate current AI from reliable model repair: solver interaction (API models restore only 27.6% of infeasible formulations) and operational rationale (roughly one in four feasible repairs violate supply chain theory). Each requires a different intervention: solver interaction responds to targeted training; operational rationale requires explicit specification as solver-verifiable checks. For organizations adopting AI in operational planning, formalizing what \"rational\" means in their context is the higher-return investment.",
      "arxiv_url": "http://arxiv.org/abs/2602.19439v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19439v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.OC"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19416v1",
      "title": "IR$^3$: Contrastive Inverse Reinforcement Learning for Interpretable Detection and Mitigation of Reward Hacking",
      "authors": [
        "Mohammad Beigi",
        "Ming Jin",
        "Junshan Zhang",
        "Jiaxin Zhang",
        "Qifan Wang",
        "Lifu Huang"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) enables powerful LLM alignment but can introduce reward hacking - models exploit spurious correlations in proxy rewards without genuine alignment. Compounding this, the objectives internalized during RLHF remain opaque, making hacking behaviors difficult to detect or correct. We introduce IR3 (Interpretable Reward Reconstruction and Rectification), a framework that reverse-engineers, interprets, and surgically repairs the implicit objectives driving RLHF-tuned models. We propose Contrastive Inverse Reinforcement Learning (C-IRL), which reconstructs the implicit reward function by contrasting paired responses from post-alignment and baseline policies to explain behavioral shifts during RLHF. We then decompose the reconstructed reward via sparse autoencoders into interpretable features, enabling identification of hacking signatures through contribution analysis. Finally, we propose mitigation strategies - clean reward optimization, adversarial shaping, constrained optimization, and feature-guided distillation - that target problematic features while preserving beneficial alignment. Experiments across multiple reward model configurations show that IR3 achieves 0.89 correlation with ground-truth rewards, identifies hacking features with over 90% precision, and significantly reduces hacking behaviors while maintaining capabilities within 3% of the original model.",
      "arxiv_url": "http://arxiv.org/abs/2602.19416v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19416v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19411v1",
      "title": "MACE-POLAR-1: A Polarisable Electrostatic Foundation Model for Molecular Chemistry",
      "authors": [
        "Ilyes Batatia",
        "William J. Baldwin",
        "Domantas Kuryla",
        "Joseph Hart",
        "Elliott Kasoar",
        "Alin M. Elena",
        "Harry Moore",
        "Mikołaj J. Gawkowski",
        "Benjamin X. Shi",
        "Venkat Kapil",
        "Panagiotis Kourtis",
        "Ioan-Bogdan Magdău",
        "Gábor Csányi"
      ],
      "abstract": "Accurate modelling of electrostatic interactions and charge transfer is fundamental to computational chemistry, yet most machine learning interatomic potentials (MLIPs) rely on local atomic descriptors that cannot capture long-range electrostatic effects. We present a new electrostatic foundation model for molecular chemistry that extends the MACE architecture with explicit treatment of long-range interactions and electrostatic induction. Our approach combines local many-body geometric features with a non-self-consistent field formalism that updates learnable charge and spin densities through polarisable iterations to model induction, followed by global charge equilibration via learnable Fukui functions to control total charge and total spin. This design enables an accurate and physical description of systems with varying charge and spin states while maintaining computational efficiency. Trained on the OMol25 dataset of 100 million hybrid DFT calculations, our models achieve chemical accuracy across diverse benchmarks, with accuracy competitive with hybrid DFT on thermochemistry, reaction barriers, conformational energies, and transition metal complexes. Notably, we demonstrate that the inclusion of long-range electrostatics leads to a large improvement in the description of non-covalent interactions and supramolecular complexes over non-electrostatic models, including sub-kcal/mol prediction of molecular crystal formation energy in the X23-DMC dataset and a fourfold improvement over short-ranged models on protein-ligand interactions. The model's ability to handle variable charge and spin states, respond to external fields, provide interpretable spin-resolved charge densities, and maintain accuracy from small molecules to protein-ligand complexes positions it as a versatile tool for computational molecular chemistry and drug discovery.",
      "arxiv_url": "http://arxiv.org/abs/2602.19411v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19411v1",
      "primary_category": "physics.chem-ph",
      "categories": [
        "physics.chem-ph",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19403v1",
      "title": "Personalized Prediction of Perceived Message Effectiveness Using Large Language Model Based Digital Twins",
      "authors": [
        "Jasmin Han",
        "Janardan Devkota",
        "Joseph Waring",
        "Amanda Luken",
        "Felix Naughton",
        "Roger Vilardaga",
        "Jonathan Bricker",
        "Carl Latkin",
        "Meghan Moran",
        "Yiqun Chen",
        "Johannes Thrul"
      ],
      "abstract": "Perceived message effectiveness (PME) by potential intervention end-users is important for selecting and optimizing personalized smoking cessation intervention messages for mobile health (mHealth) platform delivery. This study evaluates whether large language models (LLMs) can accurately predict PME for smoking cessation messages.   We evaluated multiple models for predicting PME across three domains: content quality, coping support, and quitting support. The dataset comprised 3010 message ratings (5-point Likert scale) from 301 young adult smokers. We compared (1) supervised learning models trained on labeled data, (2) zero and few-shot LLMs prompted without task-specific fine-tuning, and (3) LLM-based digital twins that incorporate individual characteristics and prior PME histories to generate personalized predictions. Model performance was assessed on three held-out messages per participant using accuracy, Cohen's kappa, and F1.   LLM-based digital twins outperformed zero and few-shot LLMs (12 percentage points on average) and supervised baselines (13 percentage points), achieving accuracies of 0.49 (content), 0.45 (coping), and 0.49 (quitting), with directional accuracies of 0.75, 0.66, and 0.70 on a simplified 3-point scale. Digital twin predictions showed greater dispersion across rating categories, indicating improved sensitivity to individual differences.   Integrating personal profiles with LLMs captures person-specific differences in PME and outperforms supervised and zero and few-shot approaches. Improved PME prediction may enable more tailored intervention content in mHealth. LLM-based digital twins show potential for supporting personalization of mobile smoking cessation and other health behavior change interventions.",
      "arxiv_url": "http://arxiv.org/abs/2602.19403v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19403v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "stat.AP"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.19396v1",
      "title": "Hiding in Plain Text: Detecting Concealed Jailbreaks via Activation Disentanglement",
      "authors": [
        "Amirhossein Farzam",
        "Majid Behabahani",
        "Mani Malek",
        "Yuriy Nevmyvaka",
        "Guillermo Sapiro"
      ],
      "abstract": "Large language models (LLMs) remain vulnerable to jailbreak prompts that are fluent and semantically coherent, and therefore difficult to detect with standard heuristics. A particularly challenging failure mode occurs when an attacker tries to hide the malicious goal of their request by manipulating its framing to induce compliance. Because these attacks maintain malicious intent through a flexible presentation, defenses that rely on structural artifacts or goal-specific signatures can fail. Motivated by this, we introduce a self-supervised framework for disentangling semantic factor pairs in LLM activations at inference. We instantiate the framework for goal and framing and construct GoalFrameBench, a corpus of prompts with controlled goal and framing variations, which we use to train Representation Disentanglement on Activations (ReDAct) module to extract disentangled representations in a frozen LLM. We then propose FrameShield, an anomaly detector operating on the framing representations, which improves model-agnostic detection across multiple LLM families with minimal computational overhead. Theoretical guarantees for ReDAct and extensive empirical validations show that its disentanglement effectively powers FrameShield. Finally, we use disentanglement as an interpretability probe, revealing distinct profiles for goal and framing signals and positioning semantic disentanglement as a building block for both LLM safety and mechanistic interpretability.",
      "arxiv_url": "http://arxiv.org/abs/2602.19396v1",
      "pdf_url": "https://arxiv.org/pdf/2602.19396v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-23",
      "source": "arxiv",
      "conference": null
    }
  ],
  "available_tags": [
    "Agentic AI",
    "Information Retrieval",
    "LLM",
    "Multi-Modal RAG",
    "Personalization",
    "RAG"
  ]
}