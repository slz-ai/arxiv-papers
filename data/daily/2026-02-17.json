{
  "date": "2026-02-17",
  "paper_count": 72,
  "papers": [
    {
      "arxiv_id": "2602.16093v1",
      "title": "Updating Parametric Knowledge with Context Distillation Retains Post-Training Capabilities",
      "authors": [
        "Shankar Padmanabhan",
        "Mustafa Omer Gul",
        "Tanya Goyal"
      ],
      "abstract": "Post-training endows pretrained LLMs with a variety of desirable skills, including instruction-following, reasoning, and others. However, these post-trained LLMs only encode knowledge up to a cut-off date, necessitating continual adaptation. Unfortunately, existing solutions cannot simultaneously learn new knowledge from an adaptation document corpora and mitigate the forgetting of earlier learned capabilities. To address this, we introduce Distillation via Split Contexts (DiSC), a simple context-distillation based approach for continual knowledge adaptation. \\methodname~derives student and teacher distributions by conditioning on distinct segments of the training example and minimizes the KL divergence between the shared tokens. This allows us to efficiently apply context-distillation without requiring explicit generation steps during training. We run experiments on four post-trained models and two adaptation domains. Compared to prior finetuning and distillation methods for continual adaptation, DiSC consistently reports the best trade-off between learning new knowledge and mitigating forgetting of previously learned skills like instruction-following, reasoning, and factual knowledge.",
      "arxiv_url": "http://arxiv.org/abs/2602.16093v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16093v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16069v1",
      "title": "The Limits of Long-Context Reasoning in Automated Bug Fixing",
      "authors": [
        "Ravi Raju",
        "Mengmeng Ji",
        "Shubhangi Upasani",
        "Bo Li",
        "Urmish Thakker"
      ],
      "abstract": "Rapidly increasing context lengths have led to the assumption that large language models (LLMs) can directly reason over entire codebases. Concurrently, recent advances in LLMs have enabled strong performance on software engineering benchmarks, particularly when paired with agentic workflows. In this work, we systematically evaluate whether current LLMs can reliably perform long-context code debugging and patch generation. Using SWE-bench Verified as a controlled experimental setting, we first evaluate state-of-the-art models within an agentic harness (mini-SWE-agent), where performance improves substantially: GPT-5-nano achieves up to a 31\\% resolve rate on 100 samples, and open-source models such as Deepseek-R1-0528 obtain competitive results. However, token-level analysis shows that successful agentic trajectories typically remain under 20k tokens, and that longer accumulated contexts correlate with lower success rates, indicating that agentic success primarily arises from task decomposition into short-context steps rather than effective long-context reasoning. To directly test long-context capability, we construct a data pipeline where we artificially inflate the context length of the input by placing the relevant files into the context (ensuring perfect retrieval recall); we then study single-shot patch generation under genuinely long contexts (64k-128k tokens). Despite this setup, performance degrades sharply: Qwen3-Coder-30B-A3B achieves only a 7\\% resolve rate at 64k context, while GPT-5-nano solves none of the tasks. Qualitative analysis reveals systematic failure modes, including hallucinated diffs, incorrect file targets, and malformed patch headers. Overall, our findings highlight a significant gap between nominal context length and usable context capacity in current LLMs, and suggest that existing agentic coding benchmarks do not meaningfully evaluate long-context reasoning.",
      "arxiv_url": "http://arxiv.org/abs/2602.16069v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16069v1",
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.LG"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16066v1",
      "title": "Improving Interactive In-Context Learning from Natural Language Feedback",
      "authors": [
        "Martin Klissarov",
        "Jonathan Cook",
        "Diego Antognini",
        "Hao Sun",
        "Jingling Li",
        "Natasha Jaques",
        "Claudiu Musat",
        "Edward Grefenstette"
      ],
      "abstract": "Adapting one's thought process based on corrective feedback is an essential ability in human learning, particularly in collaborative settings. In contrast, the current large language model training paradigm relies heavily on modeling vast, static corpora. While effective for knowledge acquisition, it overlooks the interactive feedback loops essential for models to adapt dynamically to their context. In this work, we propose a framework that treats this interactive in-context learning ability not as an emergent property, but as a distinct, trainable skill. We introduce a scalable method that transforms single-turn verifiable tasks into multi-turn didactic interactions driven by information asymmetry. We first show that current flagship models struggle to integrate corrective feedback on hard reasoning tasks. We then demonstrate that models trained with our approach dramatically improve the ability to interactively learn from language feedback. More specifically, the multi-turn performance of a smaller model nearly reaches that of a model an order of magnitude larger. We also observe robust out-of-distribution generalization: interactive training on math problems transfers to diverse domains like coding, puzzles and maze navigation. Our qualitative analysis suggests that this improvement is due to an enhanced in-context plasticity. Finally, we show that this paradigm offers a unified path to self-improvement. By training the model to predict the teacher's critiques, effectively modeling the feedback environment, we convert this external signal into an internal capability, allowing the model to self-correct even without a teacher.",
      "arxiv_url": "http://arxiv.org/abs/2602.16066v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16066v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16065v1",
      "title": "Can Generative Artificial Intelligence Survive Data Contamination? Theoretical Guarantees under Contaminated Recursive Training",
      "authors": [
        "Kevin Wang",
        "Hongqian Niu",
        "Didong Li"
      ],
      "abstract": "Generative Artificial Intelligence (AI), such as large language models (LLMs), has become a transformative force across science, industry, and society. As these systems grow in popularity, web data becomes increasingly interwoven with this AI-generated material and it is increasingly difficult to separate them from naturally generated content. As generative models are updated regularly, later models will inevitably be trained on mixtures of human-generated data and AI-generated data from earlier versions, creating a recursive training process with data contamination. Existing theoretical work has examined only highly simplified settings, where both the real data and the generative model are discrete or Gaussian, where it has been shown that such recursive training leads to model collapse. However, real data distributions are far more complex, and modern generative models are far more flexible than Gaussian and linear mechanisms. To fill this gap, we study recursive training in a general framework with minimal assumptions on the real data distribution and allow the underlying generative model to be a general universal approximator. In this framework, we show that contaminated recursive training still converges, with a convergence rate equal to the minimum of the baseline model's convergence rate and the fraction of real data used in each iteration. To the best of our knowledge, this is the first (positive) theoretical result on recursive training without distributional assumptions on the data. We further extend the analysis to settings where sampling bias is present in data collection and support all theoretical results with empirical studies.",
      "arxiv_url": "http://arxiv.org/abs/2602.16065v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16065v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.ML"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16061v1",
      "title": "Partial Identification under Missing Data Using Weak Shadow Variables from Pretrained Models",
      "authors": [
        "Hongyu Chen",
        "David Simchi-Levi",
        "Ruoxuan Xiong"
      ],
      "abstract": "Estimating population quantities such as mean outcomes from user feedback is fundamental to platform evaluation and social science, yet feedback is often missing not at random (MNAR): users with stronger opinions are more likely to respond, so standard estimators are biased and the estimand is not identified without additional assumptions. Existing approaches typically rely on strong parametric assumptions or bespoke auxiliary variables that may be unavailable in practice. In this paper, we develop a partial identification framework in which sharp bounds on the estimand are obtained by solving a pair of linear programs whose constraints encode the observed data structure. This formulation naturally incorporates outcome predictions from pretrained models, including large language models (LLMs), as additional linear constraints that tighten the feasible set. We call these predictions weak shadow variables: they satisfy a conditional independence assumption with respect to missingness but need not meet the completeness conditions required by classical shadow-variable methods. When predictions are sufficiently informative, the bounds collapse to a point, recovering standard identification as a special case. In finite samples, to provide valid coverage of the identified set, we propose a set-expansion estimator that achieves slower-than-$\\sqrt{n}$ convergence rate in the set-identified regime and the standard $\\sqrt{n}$ rate under point identification. In simulations and semi-synthetic experiments on customer-service dialogues, we find that LLM predictions are often ill-conditioned for classical shadow-variable methods yet remain highly effective in our framework. They shrink identification intervals by 75--83\\% while maintaining valid coverage under realistic MNAR mechanisms.",
      "arxiv_url": "http://arxiv.org/abs/2602.16061v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16061v1",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "econ.EM",
        "stat.ME"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16054v1",
      "title": "CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill",
      "authors": [
        "Bradley McDanel",
        "Steven Li",
        "Harshit Khaitan"
      ],
      "abstract": "The prefill stage in long-context LLM inference remains a computational bottleneck. Recent token-ranking heuristics accelerate inference by selectively processing a subset of semantically relevant tokens. However, existing methods suffer from unstable token importance estimation, often varying between layers. Evaluating token-ranking quality independently from heuristic-specific architectures is challenging. To address this, we introduce an Answer-Informed Oracle, which defines ground-truth token importance by measuring attention from generated answers back to the prompt. This oracle reveals that existing heuristics exhibit high variance across layers: rankings can degrade sharply at specific layers, a failure mode invisible to end-to-end benchmarks. The diagnosis suggests a simple fix: aggregate scores across layers rather than relying on any single one. We implement this as Cross-Layer Attention Aggregation (CLAA), which closes the gap to the oracle upper bound and reduces Time-to-First-Token (TTFT) by up to 39\\% compared to the Full KV Cache baseline.",
      "arxiv_url": "http://arxiv.org/abs/2602.16054v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16054v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16053v1",
      "title": "Multi-Objective Alignment of Language Models for Personalized Psychotherapy",
      "authors": [
        "Mehrab Beikzadeh",
        "Yasaman Asadollah Salmanpour",
        "Ashima Suvarna",
        "Sriram Sankararaman",
        "Matteo Malgaroli",
        "Majid Sarrafzadeh",
        "Saadia Gabriel"
      ],
      "abstract": "Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct preference optimization. We train reward models for six criteria -- empathy, safety, active listening, self-motivated change, trust/rapport, and patient autonomy -- and systematically compare multi-objective approaches against single-objective optimization, supervised fine-tuning, and parameter merging. Multi-objective DPO (MODPO) achieves superior balance (77.6% empathy, 62.6% safety) compared to single-objective optimization (93.6% empathy, 47.8% safety), and therapeutic criteria outperform general communication principles by 17.2%. Blinded clinician evaluation confirms MODPO is consistently preferred, with LLM-evaluator agreement comparable to inter-clinician reliability.",
      "arxiv_url": "http://arxiv.org/abs/2602.16053v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16053v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16052v1",
      "title": "MoE-Spec: Expert Budgeting for Efficient Speculative Decoding",
      "authors": [
        "Bradley McDanel",
        "Steven Li",
        "Sruthikesh Surineni",
        "Harshit Khaitan"
      ],
      "abstract": "Speculative decoding accelerates Large Language Model (LLM) inference by verifying multiple drafted tokens in parallel. However, for Mixture-of-Experts (MoE) models, this parallelism introduces a severe bottleneck: large draft trees activate many unique experts, significantly increasing memory pressure and diminishing speedups from speculative decoding relative to autoregressive decoding. Prior methods reduce speculation depth when MoE verification becomes expensive. We propose MoE-Spec, a training-free verification-time expert budgeting method that decouples speculation depth from memory cost by enforcing a fixed expert capacity limit at each layer, loading only the experts that contribute most to verification and dropping the long tail of rarely used experts that drive bandwidth overhead. Experiments across multiple model scales and datasets show that this method yields 10--30\\% higher throughput than state-of-the-art speculative decoding baselines (EAGLE-3) at comparable quality, with flexibility to trade accuracy for further latency reductions through tighter budgets.",
      "arxiv_url": "http://arxiv.org/abs/2602.16052v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16052v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16050v1",
      "title": "Evidence-Grounded Subspecialty Reasoning: Evaluating a Curated Clinical Intelligence Layer on the 2025 Endocrinology Board-Style Examination",
      "authors": [
        "Amir Hosseinian",
        "MohammadReza Zare Shahneh",
        "Umer Mansoor",
        "Gilbert Szeto",
        "Kirill Karlin",
        "Nima Aghaeepour"
      ],
      "abstract": "Background: Large language models have demonstrated strong performance on general medical examinations, but subspecialty clinical reasoning remains challenging due to rapidly evolving guidelines and nuanced evidence hierarchies. Methods: We evaluated January Mirror, an evidence-grounded clinical reasoning system, against frontier LLMs (GPT-5, GPT-5.2, Gemini-3-Pro) on a 120-question endocrinology board-style examination. Mirror integrates a curated endocrinology and cardiometabolic evidence corpus with a structured reasoning architecture to generate evidence-linked outputs. Mirror operated under a closed-evidence constraint without external retrieval. Comparator LLMs had real-time web access to guidelines and primary literature. Results: Mirror achieved 87.5% accuracy (105/120; 95% CI: 80.4-92.3%), exceeding a human reference of 62.3% and frontier LLMs including GPT-5.2 (74.6%), GPT-5 (74.0%), and Gemini-3-Pro (69.8%). On the 30 most difficult questions (human accuracy less than 50%), Mirror achieved 76.7% accuracy. Top-2 accuracy was 92.5% for Mirror versus 85.25% for GPT-5.2. Conclusions: Mirror provided evidence traceability: 74.2% of outputs cited at least one guideline-tier source, with 100% citation accuracy on manual verification. Curated evidence with explicit provenance can outperform unconstrained web retrieval for subspecialty clinical reasoning and supports auditability for clinical deployment.",
      "arxiv_url": "http://arxiv.org/abs/2602.16050v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16050v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16039v1",
      "title": "How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment",
      "authors": [
        "Hang Li",
        "Kaiqi Yang",
        "Xianxuan Long",
        "Fedor Filippov",
        "Yucheng Chu",
        "Yasemin Copur-Gencturk",
        "Peng He",
        "Cory Miller",
        "Namsoo Shin",
        "Joseph Krajcik",
        "Hui Liu",
        "Jiliang Tang"
      ],
      "abstract": "The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncertainty, stemming from the inherently probabilistic nature of LLMs. Output uncertainty is an inescapable challenge in automatic assessment, as assessment results often play a critical role in informing subsequent pedagogical actions, such as providing feedback to students or guiding instructional decisions. Unreliable or poorly calibrated uncertainty estimates can lead to unstable downstream interventions, potentially disrupting students' learning processes and resulting in unintended negative consequences. To systematically understand this challenge and inform future research, we benchmark a broad range of uncertainty quantification methods in the context of LLM-based automatic assessment. Although the effectiveness of these methods has been demonstrated in many tasks across other domains, their applicability and reliability in educational settings, particularly for automatic grading, remain underexplored. Through comprehensive analyses of uncertainty behaviors across multiple assessment datasets, LLM families, and generation control settings, we characterize the uncertainty patterns exhibited by LLMs in grading scenarios. Based on these findings, we evaluate the strengths and limitations of different uncertainty metrics and analyze the influence of key factors, including model families, assessment tasks, and decoding strategies, on uncertainty estimates. Our study provides actionable insights into the characteristics of uncertainty in LLM-based automatic assessment and lays the groundwork for developing more reliable and effective uncertainty-aware grading systems in the future.",
      "arxiv_url": "http://arxiv.org/abs/2602.16039v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16039v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16038v1",
      "title": "Heuristic Search as Language-Guided Program Optimization",
      "authors": [
        "Mingxin Yu",
        "Ruixiao Yang",
        "Chuchu Fan"
      ],
      "abstract": "Large Language Models (LLMs) have advanced Automated Heuristic Design (AHD) in combinatorial optimization (CO) in the past few years. However, existing discovery pipelines often require extensive manual trial-and-error or reliance on domain expertise to adapt to new or complex problems. This stems from tightly coupled internal mechanisms that limit systematic improvement of the LLM-driven design process. To address this challenge, we propose a structured framework for LLM-driven AHD that explicitly decomposes the heuristic discovery process into modular stages: a forward pass for evaluation, a backward pass for analytical feedback, and an update step for program refinement. This separation provides a clear abstraction for iterative refinement and enables principled improvements of individual components. We validate our framework across four diverse real-world CO domains, where it consistently outperforms baselines, achieving up to $0.17$ improvement in QYI on unseen test sets. Finally, we show that several popular AHD methods are restricted instantiations of our framework. By integrating them in our structured pipeline, we can upgrade the components modularly and significantly improve their performance.",
      "arxiv_url": "http://arxiv.org/abs/2602.16038v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16038v1",
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16037v1",
      "title": "Optimization Instability in Autonomous Agentic Workflows for Clinical Symptom Detection",
      "authors": [
        "Cameron Cagan",
        "Pedram Fard",
        "Jiazi Tian",
        "Jingya Cheng",
        "Shawn N. Murphy",
        "Hossein Estiri"
      ],
      "abstract": "Autonomous agentic workflows that iteratively refine their own behavior hold considerable promise, yet their failure modes remain poorly characterized. We investigate optimization instability, a phenomenon in which continued autonomous improvement paradoxically degrades classifier performance, using Pythia, an open-source framework for automated prompt optimization. Evaluating three clinical symptoms with varying prevalence (shortness of breath at 23%, chest pain at 12%, and Long COVID brain fog at 3%), we observed that validation sensitivity oscillated between 1.0 and 0.0 across iterations, with severity inversely proportional to class prevalence. At 3% prevalence, the system achieved 95% accuracy while detecting zero positive cases, a failure mode obscured by standard evaluation metrics. We evaluated two interventions: a guiding agent that actively redirected optimization, amplifying overfitting rather than correcting it, and a selector agent that retrospectively identified the best-performing iteration successfully prevented catastrophic failure. With selector agent oversight, the system outperformed expert-curated lexicons on brain fog detection by 331% (F1) and chest pain by 7%, despite requiring only a single natural language term as input. These findings characterize a critical failure mode of autonomous AI systems and demonstrate that retrospective selection outperforms active intervention for stabilization in low-prevalence classification tasks.",
      "arxiv_url": "http://arxiv.org/abs/2602.16037v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16037v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16034v1",
      "title": "FeDecider: An LLM-Based Framework for Federated Cross-Domain Recommendation",
      "authors": [
        "Xinrui He",
        "Ting-Wei Li",
        "Tianxin Wei",
        "Xuying Ning",
        "Xinyu He",
        "Wenxuan Bao",
        "Hanghang Tong",
        "Jingrui He"
      ],
      "abstract": "Federated cross-domain recommendation (Federated CDR) aims to collaboratively learn personalized recommendation models across heterogeneous domains while preserving data privacy. Recently, large language model (LLM)-based recommendation models have demonstrated impressive performance by leveraging LLMs' strong reasoning capabilities and broad knowledge. However, adopting LLM-based recommendation models in Federated CDR scenarios introduces new challenges. First, there exists a risk of overfitting with domain-specific local adapters. The magnitudes of locally optimized parameter updates often vary across domains, causing biased aggregation and overfitting toward domain-specific distributions. Second, unlike traditional recommendation models (e.g., collaborative filtering, bipartite graph-based methods) that learn explicit and comparable user/item representations, LLMs encode knowledge implicitly through autoregressive text generation training. This poses additional challenges for effectively measuring the cross-domain similarities under heterogeneity. To address these challenges, we propose an LLM-based framework for federated cross-domain recommendation, FeDecider. Specifically, FeDecider tackles the challenge of scale-specific noise by disentangling each client's low-rank updates and sharing only their directional components. To handle the need for flexible and effective integration, each client further learns personalized weights that achieve the data-aware integration of updates from other domains. Extensive experiments across diverse datasets validate the effectiveness of our proposed FeDecider.",
      "arxiv_url": "http://arxiv.org/abs/2602.16034v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16034v1",
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "tags": [
        "LLM",
        "Personalization"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15983v1",
      "title": "ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization",
      "authors": [
        "Junbo Jacob Lian",
        "Yujun Sun",
        "Huiling Chen",
        "Chaoyu Zhang",
        "Chung-Piaw Teo"
      ],
      "abstract": "Large language models (LLMs) can translate natural language into optimization code, but silent failures pose a critical risk: code that executes and returns solver-feasible solutions may encode semantically incorrect formulations, creating a feasibility-correctness gap of up to 90 percentage points on compositional problems. We introduce ReLoop, addressing silent failures from two complementary directions. Structured generation decomposes code production into a four-stage reasoning chain (understand, formalize, synthesize, verify) that mirrors expert modeling practice, with explicit variable-type reasoning and self-verification to prevent formulation errors at their source. Behavioral verification detects errors that survive generation by testing whether the formulation responds correctly to solver-based parameter perturbation, without requiring ground truth -- an external semantic signal that bypasses the self-consistency problem inherent in LLM-based code review. The two mechanisms are complementary: structured generation dominates on complex compositional problems, while behavioral verification becomes the largest single contributor on problems with localized formulation defects. Together with execution recovery via IIS-enhanced diagnostics, ReLoop raises correctness from 22.6% to 31.1% and execution from 72.1% to 100.0% on the strongest model, with consistent gains across five models spanning three paradigms (foundation, SFT, RL) and three benchmarks. We additionally release RetailOpt-190, 190 compositional retail optimization scenarios targeting the multi-constraint interactions where LLMs most frequently fail.",
      "arxiv_url": "http://arxiv.org/abs/2602.15983v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15983v1",
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "math.OC"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15958v1",
      "title": "DocSplit: A Comprehensive Benchmark Dataset and Evaluation Approach for Document Packet Recognition and Splitting",
      "authors": [
        "Md Mofijul Islam",
        "Md Sirajus Salekin",
        "Nivedha Balakrishnan",
        "Vincil C. Bishop",
        "Niharika Jain",
        "Spencer Romo",
        "Bob Strahan",
        "Boyi Xie",
        "Diego A. Socolinsky"
      ],
      "abstract": "Document understanding in real-world applications often requires processing heterogeneous, multi-page document packets containing multiple documents stitched together. Despite recent advances in visual document understanding, the fundamental task of document packet splitting, which involves separating a document packet into individual units, remains largely unaddressed. We present the first comprehensive benchmark dataset, DocSplit, along with novel evaluation metrics for assessing the document packet splitting capabilities of large language models. DocSplit comprises five datasets of varying complexity, covering diverse document types, layouts, and multimodal settings. We formalize the DocSplit task, which requires models to identify document boundaries, classify document types, and maintain correct page ordering within a document packet. The benchmark addresses real-world challenges, including out-of-order pages, interleaved documents, and documents lacking clear demarcations. We conduct extensive experiments evaluating multimodal LLMs on our datasets, revealing significant performance gaps in current models' ability to handle complex document splitting tasks. The DocSplit benchmark datasets and proposed novel evaluation metrics provide a systematic framework for advancing document understanding capabilities essential for legal, financial, healthcare, and other document-intensive domains. We release the datasets to facilitate future research in document packet processing.",
      "arxiv_url": "http://arxiv.org/abs/2602.15958v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15958v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15950v1",
      "title": "Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families",
      "authors": [
        "Yuval Levental"
      ],
      "abstract": "We present a simple experiment that exposes a fundamental limitation in vision-language models (VLMs): the inability to accurately localize filled cells in binary grids when those cells lack textual identity. We generate fifteen 15x15 grids with varying density (10.7%-41.8% filled cells) and render each as two image types -- text symbols (. and #) and filled squares without gridlines -- then ask three frontier VLMs (Claude Opus, ChatGPT 5.2, and Gemini 3 Thinking) to transcribe them. In the text-symbol condition, Claude and ChatGPT achieve approximately 91% cell accuracy and 84% F1, while Gemini achieves 84% accuracy and 63% F1. In the filled-squares condition, all three models collapse to 60-73% accuracy and 29-39% F1. Critically, all conditions pass through the same visual encoder -- the text symbols are images, not tokenized text. The text-vs-squares F1 gap ranges from 34 to 54 points across models, demonstrating that VLMs behave as if they possess a high-fidelity text-recognition pathway for spatial reasoning that dramatically outperforms their native visual pathway. Each model exhibits a distinct failure mode in the squares condition -- systematic under-counting (Claude), massive over-counting (ChatGPT), and template hallucination (Gemini) -- but all share the same underlying deficit: severely degraded spatial localization for non-textual visual elements.",
      "arxiv_url": "http://arxiv.org/abs/2602.15950v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15950v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15945v1",
      "title": "From Tool Orchestration to Code Execution: A Study of MCP Design Choices",
      "authors": [
        "Yuval Felendler",
        "Parth A. Gandhi",
        "Idan Habler",
        "Yuval Elovici",
        "Asaf Shabtai"
      ],
      "abstract": "Model Context Protocols (MCPs) provide a unified platform for agent systems to discover, select, and orchestrate tools across heterogeneous execution environments. As MCP-based systems scale to incorporate larger tool catalogs and multiple concurrently connected MCP servers, traditional tool-by-tool invocation increases coordination overhead, fragments state management, and limits support for wide-context operations. To address these scalability challenges, recent MCP designs have incorporated code execution as a first-class capability, an approach called Code Execution MCP (CE-MCP). This enables agents to consolidate complex workflows, such as SQL querying, file analysis, and multi-step data transformations, into a single program that executes within an isolated runtime environment. In this work, we formalize the architectural distinction between context-coupled (traditional) and context-decoupled (CE-MCP) models, analyzing their fundamental scalability trade-offs. Using the MCP-Bench framework across 10 representative servers, we empirically evaluate task behavior, tool utilization patterns, execution latency, and protocol efficiency as the scale of connected MCP servers and available tools increases, demonstrating that while CE-MCP significantly reduces token usage and execution latency, it introduces a vastly expanded attack surface. We address this security gap by applying the MAESTRO framework, identifying sixteen attack classes across five execution phases-including specific code execution threats such as exception-mediated code injection and unsafe capability synthesis. We validate these vulnerabilities through adversarial scenarios across multiple LLMs and propose a layered defense architecture comprising containerized sandboxing and semantic gating. Our findings provide a rigorous roadmap for balancing scalability and security in production-ready executable agent workflows.",
      "arxiv_url": "http://arxiv.org/abs/2602.15945v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15945v1",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15823v1",
      "title": "CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing",
      "authors": [
        "Zarif Ikram",
        "Arad Firouzkouhi",
        "Stephen Tu",
        "Mahdi Soltanolkotabi",
        "Paria Rashidinejad"
      ],
      "abstract": "A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.",
      "arxiv_url": "http://arxiv.org/abs/2602.15823v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15823v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15816v1",
      "title": "Developing AI Agents with Simulated Data: Why, what, and how?",
      "authors": [
        "Xiaoran Liu",
        "Istvan David"
      ],
      "abstract": "As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.",
      "arxiv_url": "http://arxiv.org/abs/2602.15816v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15816v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.ET"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15809v1",
      "title": "Decision Quality Evaluation Framework at Pinterest",
      "authors": [
        "Yuqi Tian",
        "Robert Paine",
        "Attila Dobi",
        "Kevin O'Sullivan",
        "Aravindh Manickavasagam",
        "Faisal Farooq"
      ],
      "abstract": "Online platforms require robust systems to enforce content safety policies at scale. A critical component of these systems is the ability to evaluate the quality of moderation decisions made by both human agents and Large Language Models (LLMs). However, this evaluation is challenging due to the inherent trade-offs between cost, scale, and trustworthiness, along with the complexity of evolving policies. To address this, we present a comprehensive Decision Quality Evaluation Framework developed and deployed at Pinterest. The framework is centered on a high-trust Golden Set (GDS) curated by subject matter experts (SMEs), which serves as a ground truth benchmark. We introduce an automated intelligent sampling pipeline that uses propensity scores to efficiently expand dataset coverage. We demonstrate the framework's practical application in several key areas: benchmarking the cost-performance trade-offs of various LLM agents, establishing a rigorous methodology for data-driven prompt optimization, managing complex policy evolution, and ensuring the integrity of policy content prevalence metrics via continuous validation. The framework enables a shift from subjective assessments to a data-driven and quantitative practice for managing content safety systems.",
      "arxiv_url": "http://arxiv.org/abs/2602.15809v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15809v1",
      "primary_category": "stat.AP",
      "categories": [
        "stat.AP",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15791v1",
      "title": "Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings",
      "authors": [
        "Suhyung Jang",
        "Ghang Lee",
        "Jaekun Lee",
        "Hyunjun Lee"
      ],
      "abstract": "Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.",
      "arxiv_url": "http://arxiv.org/abs/2602.15791v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15791v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15785v1",
      "title": "This human study did not involve human subjects: Validating LLM simulations as behavioral evidence",
      "authors": [
        "Jessica Hullman",
        "David Broska",
        "Huaman Sun",
        "Aaron Shaw"
      ],
      "abstract": "A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.",
      "arxiv_url": "http://arxiv.org/abs/2602.15785v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15785v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15781v1",
      "title": "Neural Scaling Laws for Boosted Jet Tagging",
      "authors": [
        "Matthias Vigl",
        "Nicole Hartman",
        "Michael Kagan",
        "Lukas Heinrich"
      ],
      "abstract": "The success of Large Language Models (LLMs) has established that scaling compute, through joint increases in model capacity and dataset size, is the primary driver of performance in modern machine learning. While machine learning has long been an integral component of High Energy Physics (HEP) data analysis workflows, the compute used to train state-of-the-art HEP models remains orders of magnitude below that of industry foundation models. With scaling laws only beginning to be studied in the field, we investigate neural scaling laws for boosted jet classification using the public JetClass dataset. We derive compute optimal scaling laws and identify an effective performance limit that can be consistently approached through increased compute. We study how data repetition, common in HEP where simulation is expensive, modifies the scaling yielding a quantifiable effective dataset size gain. We then study how the scaling coefficients and asymptotic performance limits vary with the choice of input features and particle multiplicity, demonstrating that increased compute reliably drives performance toward an asymptotic limit, and that more expressive, lower-level features can raise the performance limit and improve results at fixed dataset size.",
      "arxiv_url": "http://arxiv.org/abs/2602.15781v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15781v1",
      "primary_category": "hep-ex",
      "categories": [
        "hep-ex",
        "cs.LG",
        "hep-ph",
        "physics.data-an"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15778v1",
      "title": "*-PLUIE: Personalisable metric with Llm Used for Improved Evaluation",
      "authors": [
        "Quentin Lemesle",
        "LÃ©ane Jourdan",
        "Daisy Munson",
        "Pierre Alain",
        "Jonathan Chevelu",
        "Arnaud Delhay",
        "Damien Lolive"
      ],
      "abstract": "Evaluating the quality of automatically generated text often relies on LLM-as-a-judge (LLM-judge) methods. While effective, these approaches are computationally expensive and require post-processing. To address these limitations, we build upon ParaPLUIE, a perplexity-based LLM-judge metric that estimates confidence over ``Yes/No'' answers without generating text. We introduce *-PLUIE, task specific prompting variants of ParaPLUIE and evaluate their alignment with human judgement. Our experiments show that personalised *-PLUIE achieves stronger correlations with human ratings while maintaining low computational cost.",
      "arxiv_url": "http://arxiv.org/abs/2602.15778v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15778v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15763v1",
      "title": "GLM-5: from Vibe Coding to Agentic Engineering",
      "authors": [
        "GLM-5 Team",
        ":",
        "Aohan Zeng",
        "Xin Lv",
        "Zhenyu Hou",
        "Zhengxiao Du",
        "Qinkai Zheng",
        "Bin Chen",
        "Da Yin",
        "Chendi Ge",
        "Chengxing Xie",
        "Cunxiang Wang",
        "Gengzheng Pan",
        "Hao Zeng",
        "Haoke Zhang",
        "Haoran Wang",
        "Huilong Chen",
        "Jiajie Zhang",
        "Jian Jiao",
        "Jiaqi Guo",
        "Jingsen Wang",
        "Jingzhao Du",
        "Jinzhu Wu",
        "Kedong Wang",
        "Lei Li",
        "Lin Fan",
        "Lucen Zhong",
        "Mingdao Liu",
        "Mingming Zhao",
        "Pengfan Du",
        "Qian Dong",
        "Rui Lu",
        "Shuang-Li",
        "Shulin Cao",
        "Song Liu",
        "Ting Jiang",
        "Xiaodong Chen",
        "Xiaohan Zhang",
        "Xuancheng Huang",
        "Xuezhen Dong",
        "Yabo Xu",
        "Yao Wei",
        "Yifan An",
        "Yilin Niu",
        "Yitong Zhu",
        "Yuanhao Wen",
        "Yukuo Cen",
        "Yushi Bai",
        "Zhongpei Qiao",
        "Zihan Wang",
        "Zikang Wang",
        "Zilin Zhu",
        "Ziqiang Liu",
        "Zixuan Li",
        "Bojie Wang",
        "Bosi Wen",
        "Can Huang",
        "Changpeng Cai",
        "Chao Yu",
        "Chen Li",
        "Chen Li",
        "Chenghua Huang",
        "Chengwei Hu",
        "Chenhui Zhang",
        "Chenzheng Zhu",
        "Congfeng Yin",
        "Daoyan Lin",
        "Dayong Yang",
        "Di Wang",
        "Ding Ai",
        "Erle Zhu",
        "Fangzhou Yi",
        "Feiyu Chen",
        "Guohong Wen",
        "Hailong Sun",
        "Haisha Zhao",
        "Haiyi Hu",
        "Hanchen Zhang",
        "Hanrui Liu",
        "Hanyu Zhang",
        "Hao Peng",
        "Hao Tai",
        "Haobo Zhang",
        "He Liu",
        "Hongwei Wang",
        "Hongxi Yan",
        "Hongyu Ge",
        "Huan Liu",
        "Huan Liu",
        "Huanpeng Chu",
        "Jia'ni Zhao",
        "Jiachen Wang",
        "Jiajing Zhao",
        "Jiamin Ren",
        "Jiapeng Wang",
        "Jiaxin Zhang",
        "Jiayi Gui",
        "Jiayue Zhao",
        "Jijie Li",
        "Jing An",
        "Jing Li",
        "Jingwei Yuan",
        "Jinhua Du",
        "Jinxin Liu",
        "Junkai Zhi",
        "Junwen Duan",
        "Kaiyue Zhou",
        "Kangjian Wei",
        "Ke Wang",
        "Keyun Luo",
        "Laiqiang Zhang",
        "Leigang Sha",
        "Liang Xu",
        "Lindong Wu",
        "Lintao Ding",
        "Lu Chen",
        "Minghao Li",
        "Nianyi Lin",
        "Pan Ta",
        "Qiang Zou",
        "Rongjun Song",
        "Ruiqi Yang",
        "Shangqing Tu",
        "Shangtong Yang",
        "Shaoxiang Wu",
        "Shengyan Zhang",
        "Shijie Li",
        "Shuang Li",
        "Shuyi Fan",
        "Wei Qin",
        "Wei Tian",
        "Weining Zhang",
        "Wenbo Yu",
        "Wenjie Liang",
        "Xiang Kuang",
        "Xiangmeng Cheng",
        "Xiangyang Li",
        "Xiaoquan Yan",
        "Xiaowei Hu",
        "Xiaoying Ling",
        "Xing Fan",
        "Xingye Xia",
        "Xinyuan Zhang",
        "Xinze Zhang",
        "Xirui Pan",
        "Xunkai Zhang",
        "Yandong Wu",
        "Yanfu Li",
        "Yidong Wang",
        "Yifan Zhu",
        "Yijun Tan",
        "Yilin Zhou",
        "Yiming Pan",
        "Ying Zhang",
        "Yinpei Su",
        "Yipeng Geng",
        "Yipeng Geng",
        "Yong Yan",
        "Yonglin Tan",
        "Yuean Bi",
        "Yuhan Shen",
        "Yuhao Yang",
        "Yujiang Li",
        "Yunan Liu",
        "Yunqing Wang",
        "Yuntao Li",
        "Yurong Wu",
        "Yutao Zhang",
        "Yuxi Duan",
        "Yuxuan Zhang",
        "Zezhen Liu",
        "Zhengtao Jiang",
        "Zhenhe Yan",
        "Zheyu Zhang",
        "Zhixiang Wei",
        "Zhuo Chen",
        "Zhuoer Feng",
        "Zijun Yao",
        "Ziwei Chai",
        "Ziyuan Wang",
        "Zuzhou Zhang",
        "Bin Xu",
        "Minlie Huang",
        "Hongning Wang",
        "Juanzi Li",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "abstract": "We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.",
      "arxiv_url": "http://arxiv.org/abs/2602.15763v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15763v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15758v1",
      "title": "ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models",
      "authors": [
        "Manav Nitin Kapadnis",
        "Lawanya Baghel",
        "Atharva Naik",
        "Carolyn RosÃ©"
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common ground, tracking prior edits, and adapting to evolving preferences. We introduce ChartEditBench, a benchmark for incremental, visually grounded chart editing via code, comprising 5,000 difficulty-controlled modification chains and a rigorously human-verified subset. Unlike prior one-shot benchmarks, ChartEditBench evaluates sustained, context-aware editing. We further propose a robust evaluation framework that mitigates limitations of LLM-as-a-Judge metrics by integrating execution-based fidelity checks, pixel-level visual similarity, and logical code verification. Experiments with state-of-the-art MLLMs reveal substantial degradation in multi-turn settings due to error accumulation and breakdowns in shared context, with strong performance on stylistic edits but frequent execution failures on data-centric transformations. ChartEditBench, establishes a challenging testbed for grounded, intent-aware multimodal programming.",
      "arxiv_url": "http://arxiv.org/abs/2602.15758v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15758v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15757v1",
      "title": "Beyond Binary Classification: Detecting Fine-Grained Sexism in Social Media Videos",
      "authors": [
        "Laura De Grazia",
        "Danae SÃ¡nchez Villegas",
        "Desmond Elliott",
        "Mireia FarrÃºs",
        "Mariona TaulÃ©"
      ],
      "abstract": "Online sexism appears in various forms, which makes its detection challenging. Although automated tools can enhance the identification of sexist content, they are often restricted to binary classification. Consequently, more subtle manifestations of sexism may remain undetected due to the lack of fine-grained, context-sensitive labels. To address this issue, we make the following contributions: (1) we present FineMuSe, a new multimodal sexism detection dataset in Spanish that includes both binary and fine-grained annotations; (2) we introduce a comprehensive hierarchical taxonomy that encompasses forms of sexism, non-sexism, and rhetorical devices of irony and humor; and (3) we evaluate a wide range of LLMs for both binary and fine-grained sexism detection. Our findings indicate that multimodal LLMs perform competitively with human annotators in identifying nuanced forms of sexism; however, they struggle to capture co-occurring sexist types when these are conveyed through visual cues.",
      "arxiv_url": "http://arxiv.org/abs/2602.15757v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15757v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15753v1",
      "title": "Under-resourced studies of under-resourced languages: lemmatization and POS-tagging with LLM annotators for historical Armenian, Georgian, Greek and Syriac",
      "authors": [
        "Chahan Vidal-GorÃ¨ne",
        "Bastien Kindt",
        "Florian Cafiero"
      ],
      "abstract": "Low-resource languages pose persistent challenges for Natural Language Processing tasks such as lemmatization and part-of-speech (POS) tagging. This paper investigates the capacity of recent large language models (LLMs), including GPT-4 variants and open-weight Mistral models, to address these tasks in few-shot and zero-shot settings for four historically and linguistically diverse under-resourced languages: Ancient Greek, Classical Armenian, Old Georgian, and Syriac. Using a novel benchmark comprising aligned training and out-of-domain test corpora, we evaluate the performance of foundation models across lemmatization and POS-tagging, and compare them with PIE, a task-specific RNN baseline. Our results demonstrate that LLMs, even without fine-tuning, achieve competitive or superior performance in POS-tagging and lemmatization across most languages in few-shot settings. Significant challenges persist for languages characterized by complex morphology and non-Latin scripts, but we demonstrate that LLMs are a credible and relevant option for initiating linguistic annotation tasks in the absence of data, serving as an effective aid for annotation.",
      "arxiv_url": "http://arxiv.org/abs/2602.15753v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15753v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15752v1",
      "title": "Beyond Match Maximization and Fairness: Retention-Optimized Two-Sided Matching",
      "authors": [
        "Ren Kishimoto",
        "Rikiya Takehi",
        "Koichi Tanaka",
        "Masahiro Nomura",
        "Riku Togashi",
        "Yoji Tomita",
        "Yuta Saito"
      ],
      "abstract": "On two-sided matching platforms such as online dating and recruiting, recommendation algorithms often aim to maximize the total number of matches. However, this objective creates an imbalance, where some users receive far too many matches while many others receive very few and eventually abandon the platform. Retaining users is crucial for many platforms, such as those that depend heavily on subscriptions. Some may use fairness objectives to solve the problem of match maximization. However, fairness in itself is not the ultimate objective for many platforms, as users do not suddenly reward the platform simply because exposure is equalized. In practice, where user retention is often the ultimate goal, casually relying on fairness will leave the optimization of retention up to luck.   In this work, instead of maximizing matches or axiomatically defining fairness, we formally define the new problem setting of maximizing user retention in two-sided matching platforms. To this end, we introduce a dynamic learning-to-rank (LTR) algorithm called Matching for Retention (MRet). Unlike conventional algorithms for two-sided matching, our approach models user retention by learning personalized retention curves from each user's profile and interaction history. Based on these curves, MRet dynamically adapts recommendations by jointly considering the retention gains of both the user receiving recommendations and those who are being recommended, so that limited matching opportunities can be allocated where they most improve overall retention. Naturally but importantly, empirical evaluations on synthetic and real-world datasets from a major online dating platform show that MRet achieves higher user retention, since conventional methods optimize matches or fairness rather than retention.",
      "arxiv_url": "http://arxiv.org/abs/2602.15752v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15752v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15730v1",
      "title": "Causal Effect Estimation with Latent Textual Treatments",
      "authors": [
        "Omri Feldman",
        "Amar Venugopal",
        "Jann Spiess",
        "Amir Feder"
      ],
      "abstract": "Understanding the causal effects of text on downstream outcomes is a central task in many applications. Estimating such effects requires researchers to run controlled experiments that systematically vary textual features. While large language models (LLMs) hold promise for generating text, producing and evaluating controlled variation requires more careful attention. In this paper, we present an end-to-end pipeline for the generation and causal estimation of latent textual interventions. Our work first performs hypothesis generation and steering via sparse autoencoders (SAEs), followed by robust causal estimation. Our pipeline addresses both computational and statistical challenges in text-as-treatment experiments. We demonstrate that naive estimation of causal effects suffers from significant bias as text inherently conflates treatment and covariate information. We describe the estimation bias induced in this setting and propose a solution based on covariate residualization. Our empirical results show that our pipeline effectively induces variation in target features and mitigates estimation error, providing a robust foundation for causal effect estimation in text-as-treatment settings.",
      "arxiv_url": "http://arxiv.org/abs/2602.15730v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15730v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "econ.EM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15724v1",
      "title": "Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation",
      "authors": [
        "Shutian Gu",
        "Chengkai Huang",
        "Ruoyu Wang",
        "Lina Yao"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.",
      "arxiv_url": "http://arxiv.org/abs/2602.15724v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15724v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15689v2",
      "title": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models",
      "authors": [
        "Noa Linder",
        "Meirav Segal",
        "Omer Antverg",
        "Gil Gekker",
        "Tomer Fichman",
        "Omri Bodenheimer",
        "Edan Maor",
        "Omer Nevo"
      ],
      "abstract": "Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework characterizes requests along five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users, grounded in the technical substance of the request rather than stated intent. We demonstrate that this content-grounded approach resolves inconsistencies in current frontier model behavior and allows organizations to construct tunable, risk-aware refusal policies.",
      "arxiv_url": "http://arxiv.org/abs/2602.15689v2",
      "pdf_url": "https://arxiv.org/pdf/2602.15689v2",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15682v1",
      "title": "The Next Paradigm Is User-Centric Agent, Not Platform-Centric Service",
      "authors": [
        "Luankang Zhang",
        "Hang Lv",
        "Qiushi Pan",
        "Kefen Wang",
        "Yonghao Huang",
        "Xinrui Miao",
        "Yin Xu",
        "Wei Guo",
        "Yong Liu",
        "Hao Wang",
        "Enhong Chen"
      ],
      "abstract": "Modern digital services have evolved into indispensable tools, driving the present large-scale information systems. Yet, the prevailing platform-centric model, where services are optimized for platform-driven metrics such as engagement and conversion, often fails to align with users' true needs. While platform technologies have advanced significantly-especially with the integration of large language models (LLMs)-we argue that improvements in platform service quality do not necessarily translate to genuine user benefit. Instead, platform-centric services prioritize provider objectives over user welfare, resulting in conflicts against user interests. This paper argues that the future of digital services should shift from a platform-centric to a user-centric agent. These user-centric agents prioritize privacy, align with user-defined goals, and grant users control over their preferences and actions. With advancements in LLMs and on-device intelligence, the realization of this vision is now feasible. This paper explores the opportunities and challenges in transitioning to user-centric intelligence, presents a practical device-cloud pipeline for its implementation, and discusses the necessary governance and ecosystem structures for its adoption.",
      "arxiv_url": "http://arxiv.org/abs/2602.15682v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15682v1",
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15678v1",
      "title": "Revisiting Northrop Frye's Four Myths Theory with Large Language Models",
      "authors": [
        "Edirlei Soares de Lima",
        "Marco A. Casanova",
        "Antonio L. Furtado"
      ],
      "abstract": "Northrop Frye's theory of four fundamental narrative genres (comedy, romance, tragedy, satire) has profoundly influenced literary criticism, yet computational approaches to his framework have focused primarily on narrative patterns rather than character functions. In this paper, we present a new character function framework that complements pattern-based analysis by examining how archetypal roles manifest differently across Frye's genres. Drawing on Jungian archetype theory, we derive four universal character functions (protagonist, mentor, antagonist, companion) by mapping them to Jung's psychic structure components. These functions are then specialized into sixteen genre-specific roles based on prototypical works. To validate this framework, we conducted a multi-model study using six state-of-the-art Large Language Models (LLMs) to evaluate character-role correspondences across 40 narrative works. The validation employed both positive samples (160 valid correspondences) and negative samples (30 invalid correspondences) to evaluate whether models both recognize valid correspondences and reject invalid ones. LLMs achieved substantial performance (mean balanced accuracy of 82.5%) with strong inter-model agreement (Fleiss' $Îº$ = 0.600), demonstrating that the proposed correspondences capture systematic structural patterns. Performance varied by genre (ranging from 72.7% to 89.9%) and role (52.5% to 99.2%), with qualitative analysis revealing that variations reflect genuine narrative properties, including functional distribution in romance and deliberate archetypal subversion in satire. This character-based approach demonstrates the potential of LLM-supported methods for computational narratology and provides a foundation for future development of narrative generation methods and interactive storytelling applications.",
      "arxiv_url": "http://arxiv.org/abs/2602.15678v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15678v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15677v1",
      "title": "CAMEL: An ECG Language Model for Forecasting Cardiac Events",
      "authors": [
        "Neelay Velingker",
        "Alaia Solko-Breslin",
        "Mayank Keoliya",
        "Seewon Choi",
        "Jiayi Xin",
        "Anika Marathe",
        "Alireza Oraii",
        "Rajat Deo",
        "Sameed Khatana",
        "Rajeev Alur",
        "Mayur Naik",
        "Eric Wong"
      ],
      "abstract": "Electrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing cardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising framework for ECG classification accompanied by report generation. However, current models cannot forecast future cardiac events despite the immense clinical value for planning earlier intervention. To address this gap, we propose CAMEL, the first ELM that is capable of inference over longer signal durations which enables its forecasting capability. Our key insight is a specialized ECG encoder which enables cross-understanding of ECG signals with text. We train CAMEL using established LLM training procedures, combining LoRA adaptation with a curriculum learning pipeline. Our curriculum includes ECG classification, metrics calculations, and multi-turn conversations to elicit reasoning. CAMEL demonstrates strong zero-shot performance across 6 tasks and 9 datasets, including ECGForecastBench, a new benchmark that we introduce for forecasting arrhythmias. CAMEL is on par with or surpasses ELMs and fully supervised baselines both in- and out-of-distribution, achieving SOTA results on ECGBench (+7.0% absolute average gain) as well as ECGForecastBench (+12.4% over fully supervised models and +21.1% over zero-shot ELMs).",
      "arxiv_url": "http://arxiv.org/abs/2602.15677v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15677v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15675v1",
      "title": "LLM-to-Speech: A Synthetic Data Pipeline for Training Dialectal Text-to-Speech Models",
      "authors": [
        "Ahmed Khaled Khamis",
        "Hesham Ali"
      ],
      "abstract": "Despite the advances in neural text to speech (TTS), many Arabic dialectal varieties remain marginally addressed, with most resources concentrated on Modern Spoken Arabic (MSA) and Gulf dialects, leaving Egyptian Arabic -- the most widely understood Arabic dialect -- severely under-resourced. We address this gap by introducing NileTTS: 38 hours of transcribed speech from two speakers across diverse domains including medical, sales, and general conversations. We construct this dataset using a novel synthetic pipeline: large language models (LLM) generate Egyptian Arabic content, which is then converted to natural speech using audio synthesis tools, followed by automatic transcription and speaker diarization with manual quality verification. We fine-tune XTTS v2, a state-of-the-art multilingual TTS model, on our dataset and evaluate against the baseline model trained on other Arabic dialects. Our contributions include: (1) the first publicly available Egyptian Arabic TTS dataset, (2) a reproducible synthetic data generation pipeline for dialectal TTS, and (3) an open-source fine-tuned model. All resources are released to advance Egyptian Arabic speech synthesis research.",
      "arxiv_url": "http://arxiv.org/abs/2602.15675v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15675v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15669v1",
      "title": "PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra",
      "authors": [
        "Xiachong Feng",
        "Liang Zhao",
        "Weihong Zhong",
        "Yichong Huang",
        "Yuxuan Gu",
        "Lingpeng Kong",
        "Xiaocheng Feng",
        "Bing Qin"
      ],
      "abstract": "Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.",
      "arxiv_url": "http://arxiv.org/abs/2602.15669v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15669v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15654v1",
      "title": "Zombie Agents: Persistent Control of Self-Evolving LLM Agents via Self-Reinforcing Injections",
      "authors": [
        "Xianglin Yang",
        "Yufei He",
        "Shuo Ji",
        "Bryan Hooi",
        "Jin Song Dong"
      ],
      "abstract": "Self-evolving LLM agents update their internal state across sessions, often by writing and reusing long-term memory. This design improves performance on long-horizon tasks but creates a security risk: untrusted external content observed during a benign session can be stored as memory and later treated as instruction. We study this risk and formalize a persistent attack we call a Zombie Agent, where an attacker covertly implants a payload that survives across sessions, effectively turning the agent into a puppet of the attacker.   We present a black-box attack framework that uses only indirect exposure through attacker-controlled web content. The attack has two phases. During infection, the agent reads a poisoned source while completing a benign task and writes the payload into long-term memory through its normal update process. During trigger, the payload is retrieved or carried forward and causes unauthorized tool behavior. We design mechanism-specific persistence strategies for common memory implementations, including sliding-window and retrieval-augmented memory, to resist truncation and relevance filtering. We evaluate the attack on representative agent setups and tasks, measuring both persistence over time and the ability to induce unauthorized actions while preserving benign task quality. Our results show that memory evolution can convert one-time indirect injection into persistent compromise, which suggests that defenses focused only on per-session prompt filtering are not sufficient for self-evolving agents.",
      "arxiv_url": "http://arxiv.org/abs/2602.15654v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15654v1",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15620v2",
      "title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens",
      "authors": [
        "Shiqi Liu",
        "Zeyu He",
        "Guojian Zhan",
        "Letian Tao",
        "Zhilong Zheng",
        "Jiang Wu",
        "Yinuo Wang",
        "Yang Guan",
        "Kehua Sheng",
        "Bo Zhang",
        "Keqiang Li",
        "Jingliang Duan",
        "Shengbo Eben Li"
      ],
      "abstract": "Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often suffer from late-stage performance collapse, leading to degraded reasoning quality and unstable training. Our analysis shows that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. We find that training instability can be caused by a tiny fraction of tokens, approximately 0.01\\%, which we term \\emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. To mitigate this instability, we design S2T (silencing spurious tokens) mechanism to efficiently identify spurious tokens through characteristic signals with low probability, low entropy, and positive advantage, and then to suppress their gradient perturbations during optimization. Incorporating this mechanism into a group-based objective, we propose Spurious-Token-Aware Policy Optimization (STAPO), which promotes stable and effective large-scale model refinement. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% ($Ï_{\\mathrm{T}}$=1.0, top-p=1.0) and 3.69\\% ($Ï_{\\mathrm{T}}$=0.7, top-p=0.9) over GRPO, 20-Entropy and JustRL.",
      "arxiv_url": "http://arxiv.org/abs/2602.15620v2",
      "pdf_url": "https://arxiv.org/pdf/2602.15620v2",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15600v1",
      "title": "The geometry of online conversations and the causal antecedents of conflictual discourse",
      "authors": [
        "Carlo Santagiustina",
        "Caterina Cruciani"
      ],
      "abstract": "This article investigates the causal antecedents of conflictual language and the geometry of interaction in online threaded conversations related to climate change. We employ three annotation dimensions, inferred through LLM prompting and averaging, to capture complementary aspects of discursive conflict (such as stance: agreement vs disagreement; tone: attacking vs respectful; and emotional versus factual framing) and use data from a threaded online forum to examine how these dimensions respond to temporal, conversational, and arborescent structural features of discussions. We show that, as suggested by the literature, longer delays between successive posts in a thread are associated with replies that are, on average, more respectful, whereas longer delays relative to the parent post are associated with slightly less disagreement but more emotional (less factual) language. Second, we characterize alignment with the local conversational environment and find strong convergence both toward the average stance, tone and emotional framing of older sibling posts replying to the same parent and toward those of the parent post itself, with parent post effects generally stronger than sibling effects. We further show that early branch-level responses condition these alignment dynamics, such that parent-child stance alignment is amplified or attenuated depending on whether a branch is initiated in agreement or disagreement with the discussion's root message. These influences are largely additive for civility-related dimensions (attacking vs respectful, disagree vs agree), whereas for emotional versus factual framing there is a significant interaction: alignment with the parent's emotionality is amplified when older siblings are similarly aligned.",
      "arxiv_url": "http://arxiv.org/abs/2602.15600v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15600v1",
      "primary_category": "cs.SI",
      "categories": [
        "cs.SI",
        "cs.AI",
        "econ.EM",
        "stat.AP"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15564v1",
      "title": "Beyond Static Pipelines: Learning Dynamic Workflows for Text-to-SQL",
      "authors": [
        "Yihan Wang",
        "Peiyu Liu",
        "Runyu Chen",
        "Wei Xu"
      ],
      "abstract": "Text-to-SQL has recently achieved impressive progress, yet remains difficult to apply effectively in real-world scenarios. This gap stems from the reliance on single static workflows, fundamentally limiting scalability to out-of-distribution and long-tail scenarios. Instead of requiring users to select suitable methods through extensive experimentation, we attempt to enable systems to adaptively construct workflows at inference time. Through theoretical and empirical analysis, we demonstrate that optimal dynamic policies consistently outperform the best static workflow, with performance gains fundamentally driven by heterogeneity across candidate workflows. Motivated by this, we propose SquRL, a reinforcement learning framework that enhances LLMs' reasoning capability in adaptive workflow construction. We design a rule-based reward function and introduce two effective training mechanisms: dynamic actor masking to encourage broader exploration, and pseudo rewards to improve training efficiency. Experiments on widely-used Text-to-SQL benchmarks demonstrate that dynamic workflow construction consistently outperforms the best static workflow methods, with especially pronounced gains on complex and out-of-distribution queries. The codes are available at https://github.com/Satissss/SquRL",
      "arxiv_url": "http://arxiv.org/abs/2602.15564v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15564v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15563v1",
      "title": "1-Bit Wonder: Improving QAT Performance in the Low-Bit Regime through K-Means Quantization",
      "authors": [
        "Sohir Maskey",
        "Constantin Eichenberg",
        "Johannes Messner",
        "Douglas Orr"
      ],
      "abstract": "Quantization-aware training (QAT) is an effective method to drastically reduce the memory footprint of LLMs while keeping performance degradation at an acceptable level. However, the optimal choice of quantization format and bit-width presents a challenge in practice. The full design space of quantization is not fully explored in the context of QAT, and the precise trade-off between quantization and downstream performance is poorly understood, as comparisons often rely solely on perplexity-based evaluations. In this work, we address these shortcomings with an empirical study of QAT in the low-bit regime. We show that k-means based weight quantization outperforms integer formats and can be implemented efficiently on standard hardware. Furthermore, we find that, under a fixed inference memory budget, the best performance on generative downstream tasks is achieved with $1$-bit quantized weights.",
      "arxiv_url": "http://arxiv.org/abs/2602.15563v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15563v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15553v1",
      "title": "RUVA: Personalized Transparent On-Device Graph Reasoning",
      "authors": [
        "Gabriele Conte",
        "Alessio Mattiace",
        "Gianni Carmosino",
        "Potito Aghilar",
        "Giovanni Servedio",
        "Francesco Musicco",
        "Vito Walter Anelli",
        "Tommaso Di Noia",
        "Francesco Maria Donini"
      ],
      "abstract": "The Personal AI landscape is currently dominated by \"Black Box\" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, \"deleting\" a concept from a vector space is mathematically imprecise, leaving behind probabilistic \"ghosts\" that violate true privacy. We propose Ruva, the first \"Glass Box\" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the \"Right to be Forgotten.\" Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at http://sisinf00.poliba.it/ruva/.",
      "arxiv_url": "http://arxiv.org/abs/2602.15553v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15553v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "tags": [
        "Personalization",
        "RAG"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15532v1",
      "title": "Quantifying construct validity in large language model evaluations",
      "authors": [
        "Ryan Othniel Kearns"
      ],
      "abstract": "The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance.   Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity. Latent factor models ignore scaling laws, and as a result, the capabilities they extract often proxy model size. Scaling laws ignore measurement error, and as a result, the capabilities they extract are both uninterpretable and overfit to the observed benchmarks.   This thesis presents the structured capabilities model, the first model to extract interpretable and generalisable capabilities from a large collection of LLM benchmark results. I fit this model and its two alternatives on a large sample of results from the OpenLLM Leaderboard. Structured capabilities outperform latent factor models on parsimonious fit indices, and exhibit better out-of-distribution benchmark prediction than scaling laws. These improvements are possible because neither existing approach separates model scale from capabilities in the appropriate way. Model scale should inform capabilities, as in scaling laws, and these capabilities should inform observed results up to measurement error, as in latent factor models. In combining these two insights, structured capabilities demonstrate better explanatory and predictive power for quantifying construct validity in LLM evaluations.",
      "arxiv_url": "http://arxiv.org/abs/2602.15532v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15532v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15531v1",
      "title": "GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway",
      "authors": [
        "Javier Irigoyen",
        "Roberto Daza",
        "Aythami Morales",
        "Julian Fierrez",
        "Francisco Jurado",
        "Alvaro Ortigosa",
        "Ruben Tolosana"
      ],
      "abstract": "This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.",
      "arxiv_url": "http://arxiv.org/abs/2602.15531v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15531v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15521v1",
      "title": "ExpertWeaver: Unlocking the Inherent MoE in Dense LLMs with GLU Activation Patterns",
      "authors": [
        "Ziyu Zhao",
        "Tong Zhu",
        "Zhi Zhang",
        "Tiantian Fan",
        "Jinluan Yang",
        "Kun Kuang",
        "Zhongyu Wei",
        "Fei Wu",
        "Yu Cheng"
      ],
      "abstract": "Mixture-of-Experts (MoE) effectively scales model capacity while preserving computational efficiency through sparse expert activation. However, training high-quality MoEs from scratch is prohibitively expensive. A promising alternative is to convert pretrained dense models into sparse MoEs. Existing dense-to-MoE methods fall into two categories: \\textbf{dynamic structural pruning} that converts dense models into MoE architectures with moderate sparsity to balance performance and inference efficiency, and \\textbf{downcycling} approaches that use pretrained dense models to initialize highly sparse MoE architectures. However, existing methods break the intrinsic activation patterns within dense models, leading to suboptimal expert construction. In this work, we argue that the Gated Linear Unit (GLU) mechanism provides a natural blueprint for dense-to-MoE conversion. We show that the fine-grained neural-wise activation patterns of GLU reveal a coarse-grained structure, uncovering an inherent MoE architecture composed of consistently activated universal neurons and dynamically activated specialized neurons. Leveraging this discovery, we introduce ExpertWeaver, a training-free framework that partitions neurons according to their activation patterns and constructs shared experts and specialized routed experts with layer-adaptive configurations. Our experiments demonstrate that ExpertWeaver significantly outperforms existing methods, both as a training-free dynamic structural pruning technique and as a downcycling strategy for superior MoE initialization.",
      "arxiv_url": "http://arxiv.org/abs/2602.15521v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15521v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15514v1",
      "title": "DependencyAI: Detecting AI Generated Text through Dependency Parsing",
      "authors": [
        "Sara Ahmed",
        "Tracy Hammond"
      ],
      "abstract": "As large language models (LLMs) become increasingly prevalent, reliable methods for detecting AI-generated text are critical for mitigating potential risks. We introduce DependencyAI, a simple and interpretable approach for detecting AI-generated text using only the labels of linguistic dependency relations. Our method achieves competitive performance across monolingual, multi-generator, and multilingual settings. To increase interpretability, we analyze feature importance to reveal syntactic structures that distinguish AI-generated from human-written text. We also observe a systematic overprediction of certain models on unseen domains, suggesting that generator-specific writing styles may affect cross-domain generalization. Overall, our results demonstrate that dependency relations alone provide a robust signal for AI-generated text detection, establishing DependencyAI as a strong linguistically grounded, interpretable, and non-neural network baseline.",
      "arxiv_url": "http://arxiv.org/abs/2602.15514v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15514v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15513v1",
      "title": "Improving MLLMs in Embodied Exploration and Question Answering with Human-Inspired Memory Modeling",
      "authors": [
        "Ji Li",
        "Jing Xia",
        "Mingyi Li",
        "Shiyan Hu"
      ],
      "abstract": "Deploying Multimodal Large Language Models as the brain of embodied agents remains challenging, particularly under long-horizon observations and limited context budgets. Existing memory assisted methods often rely on textual summaries, which discard rich visual and spatial details and remain brittle in non-stationary environments. In this work, we propose a non-parametric memory framework that explicitly disentangles episodic and semantic memory for embodied exploration and question answering. Our retrieval-first, reasoning-assisted paradigm recalls episodic experiences via semantic similarity and verifies them through visual reasoning, enabling robust reuse of past observations without rigid geometric alignment. In parallel, we introduce a program-style rule extraction mechanism that converts experiences into structured, reusable semantic memory, facilitating cross-environment generalization. Extensive experiments demonstrate state-of-the-art performance on embodied question answering and exploration benchmarks, yielding a 7.3% gain in LLM-Match and an 11.4% gain in LLM MatchXSPL on A-EQA, as well as +7.7% success rate and +6.8% SPL on GOAT-Bench. Analyses reveal that our episodic memory primarily improves exploration efficiency, while semantic memory strengthens complex reasoning of embodied agents.",
      "arxiv_url": "http://arxiv.org/abs/2602.15513v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15513v1",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15509v1",
      "title": "Fine-Refine: Iterative Fine-grained Refinement for Mitigating Dialogue Hallucination",
      "authors": [
        "Xiangyan Chen",
        "Yujian Gan",
        "Matthew Purver"
      ],
      "abstract": "The tendency for hallucination in current large language models (LLMs) negatively impacts dialogue systems. Such hallucinations produce factually incorrect responses that may mislead users and undermine system trust. Existing refinement methods for dialogue systems typically operate at the response level, overlooking the fact that a single response may contain multiple verifiable or unverifiable facts. To address this gap, we propose Fine-Refine, a fine-grained refinement framework that decomposes responses into atomic units, verifies each unit using external knowledge, assesses fluency via perplexity, and iteratively corrects granular errors. We evaluate factuality across the HybriDialogue and OpendialKG datasets in terms of factual accuracy (fact score) and coverage (Not Enough Information Proportion), and experiments show that Fine-Refine substantially improves factuality, achieving up to a 7.63-point gain in dialogue fact score, with a small trade-off in dialogue quality.",
      "arxiv_url": "http://arxiv.org/abs/2602.15509v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15509v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15508v1",
      "title": "Eco-Amazon: Enriching E-commerce Datasets with Product Carbon Footprint for Sustainable Recommendations",
      "authors": [
        "Giuseppe Spillo",
        "Allegra De Filippo",
        "Cataldo Musto",
        "Michela Milano",
        "Giovanni Semeraro"
      ],
      "abstract": "In the era of responsible and sustainable AI, information retrieval and recommender systems must expand their scope beyond traditional accuracy metrics to incorporate environmental sustainability. However, this research line is severely limited by the lack of item-level environmental impact data in standard benchmarks. This paper introduces Eco-Amazon, a novel resource designed to bridge this gap. Our resource consists of an enriched version of three widely used Amazon datasets (i.e., Home, Clothing, and Electronics) augmented with Product Carbon Footprint (PCF) metadata. CO2e emission scores were generated using a zero-shot framework that leverages Large Language Models (LLMs) to estimate item-level PCF based on product attributes. Our contribution is three-fold: (i) the release of the Eco-Amazon datasets, enriching item metadata with PCF signals; (ii) the LLM-based PCF estimation script, which allows researchers to enrich any product catalogue and reproduce our results; (iii) a use case demonstrating how PCF estimates can be exploited to promote more sustainable products. By providing these environmental signals, Eco-Amazon enables the community to develop, benchmark, and evaluate the next generation of sustainable retrieval and recommendation models. Our resource is available at https://doi.org/10.5281/zenodo.18549130, while our source code is available at: http://github.com/giuspillo/EcoAmazon/.",
      "arxiv_url": "http://arxiv.org/abs/2602.15508v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15508v1",
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15504v1",
      "title": "Towards Expectation Detection in Language: A Case Study on Treatment Expectations in Reddit",
      "authors": [
        "Aswathy Velutharambath",
        "Amelie WÃ¼hrl"
      ],
      "abstract": "Patients' expectations towards their treatment have a substantial effect on the treatments' success. While primarily studied in clinical settings, online patient platforms like medical subreddits may hold complementary insights: treatment expectations that patients feel unnecessary or uncomfortable to share elsewhere. Despite this, no studies examine what type of expectations users discuss online and how they express them. Presumably this is because expectations have not been studied in natural language processing (NLP) before. Therefore, we introduce the task of Expectation Detection, arguing that expectations are relevant for many applications, including opinion mining and product design. Subsequently, we present a case study for the medical domain, where expectations are particularly crucial to extract. We contribute RedHOTExpect, a corpus of Reddit posts (4.5K posts) to study expectations in this context. We use a large language model (LLM) to silver-label the data and validate its quality manually (label accuracy ~78%). Based on this, we analyze which linguistic patterns characterize expectations and explore what patients expect and why. We find that optimism and proactive framing are more pronounced in posts about physical or treatment-related illnesses compared to mental-health contexts, and that in our dataset, patients mostly discuss benefits rather than negative outcomes. The RedHOTExpect corpus can be obtained from https://www.ims.uni-stuttgart.de/data/RedHOTExpect",
      "arxiv_url": "http://arxiv.org/abs/2602.15504v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15504v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15485v2",
      "title": "SecCodeBench-V2 Technical Report",
      "authors": [
        "Longfei Chen",
        "Ji Zhao",
        "Lanxiao Cui",
        "Tong Su",
        "Xingbo Pan",
        "Ziyang Li",
        "Yongxing Wu",
        "Qijiang Cao",
        "Qiyao Cai",
        "Jing Zhang",
        "Yuandong Ni",
        "Junyao He",
        "Zeyu Zhang",
        "Chao Ge",
        "Xuhuai Lu",
        "Zeyu Gao",
        "Yuxin Cui",
        "Weisen Chen",
        "Yuxuan Peng",
        "Shengping Wang",
        "Qi Li",
        "Yukai Huang",
        "Yukun Liu",
        "Tuo Zhou",
        "Terry Yue Zhuo",
        "Junyang Lin",
        "Chao Zhang"
      ],
      "abstract": "We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and JavaScript. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.",
      "arxiv_url": "http://arxiv.org/abs/2602.15485v2",
      "pdf_url": "https://arxiv.org/pdf/2602.15485v2",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15481v1",
      "title": "LLM-as-Judge on a Budget",
      "authors": [
        "Aadirupa Saha",
        "Aniket Wagde",
        "Branislav Kveton"
      ],
      "abstract": "LLM-as-a-judge has emerged as a cornerstone technique for evaluating large language models by leveraging LLM reasoning to score prompt-response pairs. Since LLM judgments are stochastic, practitioners commonly query each pair multiple times to estimate mean scores accurately. This raises a critical challenge: given a fixed computational budget $B$, how to optimally allocate queries across $K$ prompt-response pairs to minimize estimation error? % We present a principled variance-adaptive approach leveraging multi-armed bandit theory and concentration inequalities. Our method dynamically allocates queries based on estimated score variances, concentrating resources where uncertainty is highest. Further, our algorithm is shown to achieve a worst-case score-estimation error of $\\tilde{O}\\left(\\sqrt{\\frac{\\sum_{i=1}^K Ï_i^2}{B}}\\right)$, $Ï_i^2$ being the unknown score variance for pair $i \\in [K]$ with near-optimal budget allocation. % Experiments on \\emph{Summarize-From-Feedback} and \\emph{HelpSteer2} demonstrate that our method significantly outperforms uniform allocation, reducing worst-case estimation error while maintaining identical budgets. Our work establishes a theoretical foundation for efficient LLM evaluation with practical implications for AI safety, model alignment, and automated assessment at scale.",
      "arxiv_url": "http://arxiv.org/abs/2602.15481v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15481v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15478v1",
      "title": "Evaluating Federated Learning for Cross-Country Mood Inference from Smartphone Sensing Data",
      "authors": [
        "Sharmad Kalpande",
        "Saurabh Shirke",
        "Haroon R. Lone"
      ],
      "abstract": "Mood instability is a key behavioral indicator of mental health, yet traditional assessments rely on infrequent and retrospective reports that fail to capture its continuous nature. Smartphone-based mobile sensing enables passive, in-the-wild mood inference from everyday behaviors; however, deploying such systems at scale remains challenging due to privacy constraints, uneven sensing availability, and substantial variability in behavioral patterns.   In this work, we study mood inference using smartphone sensing data in a cross-country federated learning setting, where each country participates as an independent client while retaining local data. We introduce FedFAP, a feature-aware personalized federated framework designed to accommodate heterogeneous sensing modalities across regions. Evaluations across geographically and culturally diverse populations show that FedFAP achieves an AUROC of 0.744, outperforming both centralized approaches and existing personalized federated baselines. Beyond inference, our results offer design insights for mood-aware systems, demonstrating how population-aware personalization and privacy-preserving learning can enable scalable and mood-aware mobile sensing technologies.",
      "arxiv_url": "http://arxiv.org/abs/2602.15478v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15478v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15460v1",
      "title": "On the Out-of-Distribution Generalization of Reasoning in Multimodal LLMs for Simple Visual Planning Tasks",
      "authors": [
        "Yannic Neuhaus",
        "Nicolas Flammarion",
        "Matthias Hein",
        "Francesco Croce"
      ],
      "abstract": "Integrating reasoning in large language models and large vision-language models has recently led to significant improvement of their capabilities. However, the generalization of reasoning models is still vaguely defined and poorly understood. In this work, we present an evaluation framework to rigorously examine how well chain-of-thought (CoT) approaches generalize on a simple planning task. Specifically, we consider a grid-based navigation task in which a model is provided with a map and must output a sequence of moves that guides a player from a start position to a goal while avoiding obstacles. The versatility of the task and its data allows us to fine-tune model variants using different input representations (visual and textual) and CoT reasoning strategies, and systematically evaluate them under both in-distribution (ID) and out-of-distribution (OOD) test conditions. Our experiments show that, while CoT reasoning improves in-distribution generalization across all representations, out-of-distribution generalization (e.g., to larger maps) remains very limited in most cases when controlling for trivial matches with the ID data. Surprisingly, we find that reasoning traces which combine multiple text formats yield the best (and non-trivial) OOD generalization. Finally, purely text-based models consistently outperform those utilizing image-based inputs, including a recently proposed approach relying on latent space reasoning.",
      "arxiv_url": "http://arxiv.org/abs/2602.15460v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15460v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15456v1",
      "title": "In Agents We Trust, but Who Do Agents Trust? Latent Source Preferences Steer LLM Generations",
      "authors": [
        "Mohammad Aflah Khan",
        "Mahsa Amani",
        "Soumi Das",
        "Bishwamittra Ghosh",
        "Qinyuan Wu",
        "Krishna P. Gummadi",
        "Manish Gupta",
        "Abhilasha Ravichander"
      ],
      "abstract": "Agents based on Large Language Models (LLMs) are increasingly being deployed as interfaces to information on online platforms. These agents filter, prioritize, and synthesize information retrieved from the platforms' back-end databases or via web search. In these scenarios, LLM agents govern the information users receive, by drawing users' attention to particular instances of retrieved information at the expense of others. While much prior work has focused on biases in the information LLMs themselves generate, less attention has been paid to the factors that influence what information LLMs select and present to users. We hypothesize that when information is attributed to specific sources (e.g., particular publishers, journals, or platforms), current LLMs exhibit systematic latent source preferences- that is, they prioritize information from some sources over others. Through controlled experiments on twelve LLMs from six model providers, spanning both synthetic and real-world tasks, we find that several models consistently exhibit strong and predictable source preferences. These preferences are sensitive to contextual framing, can outweigh the influence of content itself, and persist despite explicit prompting to avoid them. They also help explain phenomena such as the observed left-leaning skew in news recommendations in prior work. Our findings advocate for deeper investigation into the origins of these preferences, as well as for mechanisms that provide users with transparency and control over the biases guiding LLM-powered agents.",
      "arxiv_url": "http://arxiv.org/abs/2602.15456v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15456v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15449v1",
      "title": "TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models",
      "authors": [
        "Chansung Park",
        "Juyong Jiang",
        "Fan Wang",
        "Sayak Paul",
        "Jiasi Shen",
        "Jing Tang",
        "Jianguo Li"
      ],
      "abstract": "Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.",
      "arxiv_url": "http://arxiv.org/abs/2602.15449v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15449v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SE"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15436v1",
      "title": "Measuring Social Integration Through Participation: Categorizing Organizations and Leisure Activities in the Displaced Karelians Interview Archive using LLMs",
      "authors": [
        "Joonatan Laato",
        "Veera Schroderus",
        "Jenna Kanerva",
        "Jenni Kauppi",
        "Virpi Lummaa",
        "Filip Ginter"
      ],
      "abstract": "Digitized historical archives make it possible to study everyday social life on a large scale, but the information extracted directly from text often does not directly allow one to answer the research questions posed by historians or sociologists in a quantitative manner. We address this problem in a large collection of Finnish World War II Karelian evacuee family interviews. Prior work extracted more than 350K mentions of leisure time activities and organizational memberships from these interviews, yielding 71K unique activity and organization names -- far too many to analyze directly.   We develop a categorization framework that captures key aspects of participation (the kind of activity/organization, how social it typically is, how regularly it happens, and how physically demanding it is). We annotate a gold-standard set to allow for a reliable evaluation, and then test whether large language models can apply the same schema at scale. Using a simple voting approach across multiple model runs, we find that an open-weight LLM can closely match expert judgments. Finally, we apply the method to label the 350K entities, producing a structured resource for downstream studies of social integration and related outcomes.",
      "arxiv_url": "http://arxiv.org/abs/2602.15436v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15436v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15391v1",
      "title": "Improving LLM Reliability through Hybrid Abstention and Adaptive Detection",
      "authors": [
        "Ankit Sharma",
        "Nachiket Tapas",
        "Jyotiprakash Patra"
      ],
      "abstract": "Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.",
      "arxiv_url": "http://arxiv.org/abs/2602.15391v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15391v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15378v1",
      "title": "Making Large Language Models Speak Tulu: Structured Prompting for an Extremely Low-Resource Language",
      "authors": [
        "Prathamesh Devadiga",
        "Paras Chopra"
      ],
      "abstract": "Can large language models converse in languages virtually absent from their training data? We investigate this question through a case study on Tulu, a Dravidian language with over 2 million speakers but minimal digital presence. Rather than fine-tuning an LLM, we examine whether structured prompts alone can elicit basic conversational ability under controlled prompting. We systematically tackle various challenges posed by absence of training data for Tulu by combining explicit grammar documentation, negative constraints to suppress high-probability tokens from related languages, romanization standardization, and quality-controlled synthetic data generation via self-play. Evaluated on a manually curated held-out set across three LLMs (Gemini 2.0 Flash, GPT-4o, Llama 3.1 70B) and validated by native speakers, our approach reduces vocabulary contamination from 80% to 5% while achieving 85% grammatical accuracy. Cross-model analysis reveals that negative constraints provide consistent improvements (12--18 percentage points), while grammar documentation effects vary by model architecture (8--22 points).",
      "arxiv_url": "http://arxiv.org/abs/2602.15378v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15378v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15918v1",
      "title": "EarthSpatialBench: Benchmarking Spatial Reasoning Capabilities of Multimodal LLMs on Earth Imagery",
      "authors": [
        "Zelin Xu",
        "Yupu Zhang",
        "Saugat Adhikari",
        "Saiful Islam",
        "Tingsong Xiao",
        "Zibo Liu",
        "Shigang Chen",
        "Da Yan",
        "Zhe Jiang"
      ],
      "abstract": "Benchmarking spatial reasoning in multimodal large language models (MLLMs) has attracted growing interest in computer vision due to its importance for embodied AI and other agentic systems that require precise interaction with the physical world. However, spatial reasoning on Earth imagery has lagged behind, as it uniquely involves grounding objects in georeferenced images and quantitatively reasoning about distances, directions, and topological relations using both visual cues and vector geometry coordinates (e.g., 2D bounding boxes, polylines, and polygons). Existing benchmarks for Earth imagery primarily focus on 2D spatial grounding, image captioning, and coarse spatial relations (e.g., simple directional or proximity cues). They lack support for quantitative direction and distance reasoning, systematic topological relations, and complex object geometries beyond bounding boxes. To fill this gap, we propose \\textbf{EarthSpatialBench}, a comprehensive benchmark for evaluating spatial reasoning in MLLMs on Earth imagery. The benchmark contains over 325K question-answer pairs spanning: (1) qualitative and quantitative reasoning about spatial distance and direction; (2) systematic topological relations; (3) single-object queries, object-pair queries, and compositional aggregate group queries; and (4) object references expressed via textual descriptions, visual overlays, and explicit geometry coordinates, including 2D bounding boxes, polylines, and polygons. We conducted extensive experiments on both open-source and proprietary models to identify limitations in the spatial reasoning of MLLMs.",
      "arxiv_url": "http://arxiv.org/abs/2602.15918v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15918v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15350v1",
      "title": "Fine-Tuning LLMs to Generate Economical and Reliable Actions for the Power Grid",
      "authors": [
        "Mohamad Chehade",
        "Hao Zhu"
      ],
      "abstract": "Public Safety Power Shutoffs (PSPS) force rapid topology changes that can render standard operating points infeasible, requiring operators to quickly identify corrective transmission switching actions that reduce load shedding while maintaining acceptable voltage behavior. We present a verifiable, multi-stage adaptation pipeline that fine-tunes an instruction-tuned large language model (LLM) to generate \\emph{open-only} corrective switching plans from compact PSPS scenario summaries under an explicit switching budget. First, supervised fine-tuning distills a DC-OPF MILP oracle into a constrained action grammar that enables reliable parsing and feasibility checks. Second, direct preference optimization refines the policy using AC-evaluated preference pairs ranked by a voltage-penalty metric, injecting voltage-awareness beyond DC imitation. Finally, best-of-$N$ selection provides an inference-time addition by choosing the best feasible candidate under the target metric. On IEEE 118-bus PSPS scenarios, fine-tuning substantially improves DC objective values versus zero-shot generation, reduces AC power-flow failure from 50\\% to single digits, and improves voltage-penalty outcomes on the common-success set. Code and data-generation scripts are released to support reproducibility.",
      "arxiv_url": "http://arxiv.org/abs/2602.15350v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15350v1",
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15344v1",
      "title": "ER-MIA: Black-Box Adversarial Memory Injection Attacks on Long-Term Memory-Augmented Large Language Models",
      "authors": [
        "Mitchell Piehl",
        "Zhaohan Xi",
        "Zuobin Xiong",
        "Pan He",
        "Muchao Ye"
      ],
      "abstract": "Large language models (LLMs) are increasingly augmented with long-term memory systems to overcome finite context windows and enable persistent reasoning across interactions. However, recent research finds that LLMs become more vulnerable because memory provides extra attack surfaces. In this paper, we present the first systematic study of black-box adversarial memory injection attacks that target the similarity-based retrieval mechanism in long-term memory-augmented LLMs. We introduce ER-MIA, a unified framework that exposes this vulnerability and formalizes two realistic attack settings: content-based attacks and question-targeted attacks. In these settings, ER-MIA includes an arsenal of composable attack primitives and ensemble attacks that achieve high success rates under minimal attacker assumptions. Extensive experiments across multiple LLMs and long-term memory systems demonstrate that similarity-based retrieval constitutes a fundamental and system-level vulnerability, revealing security risks that persist across memory designs and application scenarios.",
      "arxiv_url": "http://arxiv.org/abs/2602.15344v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15344v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15338v1",
      "title": "Discovering Implicit Large Language Model Alignment Objectives",
      "authors": [
        "Edward Chen",
        "Sanmi Koyejo",
        "Carlos Guestrin"
      ],
      "abstract": "Large language model (LLM) alignment relies on complex reward signals that often obscure the specific behaviors being incentivized, creating critical risks of misalignment and reward hacking. Existing interpretation methods typically rely on pre-defined rubrics, risking the omission of \"unknown unknowns\", or fail to identify objectives that comprehensively cover and are causal to the model behavior. To address these limitations, we introduce Obj-Disco, a framework that automatically decomposes an alignment reward signal into a sparse, weighted combination of human-interpretable natural language objectives. Our approach utilizes an iterative greedy algorithm to analyze behavioral changes across training checkpoints, identifying and validating candidate objectives that best explain the residual reward signal. Extensive evaluations across diverse tasks, model sizes, and alignment algorithms demonstrate the framework's robustness. Experiments with popular open-source reward models show that the framework consistently captures > 90% of reward behavior, a finding further corroborated by human evaluation. Additionally, a case study on alignment with an open-source reward model reveals that Obj-Disco can successfully identify latent misaligned incentives that emerge alongside intended behaviors. Our work provides a crucial tool for uncovering the implicit objectives in LLM alignment, paving the way for more transparent and safer AI development.",
      "arxiv_url": "http://arxiv.org/abs/2602.15338v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15338v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15325v1",
      "title": "AgriWorld:A World Tools Protocol Framework for Verifiable Agricultural Reasoning with Code-Executing LLM Agents",
      "authors": [
        "Zhixing Zhang",
        "Jesen Zhang",
        "Hao Liu",
        "Qinhan Lv",
        "Jing Yang",
        "Kaitong Cai",
        "Keze Wang"
      ],
      "abstract": "Foundation models for agriculture are increasingly trained on massive spatiotemporal data (e.g., multi-spectral remote sensing, soil grids, and field-level management logs) and achieve strong performance on forecasting and monitoring. However, these models lack language-based reasoning and interactive capabilities, limiting their usefulness in real-world agronomic workflows. Meanwhile, large language models (LLMs) excel at interpreting and generating text, but cannot directly reason over high-dimensional, heterogeneous agricultural datasets. We bridge this gap with an agentic framework for agricultural science. It provides a Python execution environment, AgriWorld, exposing unified tools for geospatial queries over field parcels, remote-sensing time-series analytics, crop growth simulation, and task-specific predictors (e.g., yield, stress, and disease risk). On top of this environment, we design a multi-turn LLM agent, Agro-Reflective, that iteratively writes code, observes execution results, and refines its analysis via an execute-observe-refine loop. We introduce AgroBench, with scalable data generation for diverse agricultural QA spanning lookups, forecasting, anomaly detection, and counterfactual \"what-if\" analysis. Experiments outperform text-only and direct tool-use baselines, validating execution-driven reflection for reliable agricultural reasoning.",
      "arxiv_url": "http://arxiv.org/abs/2602.15325v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15325v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15322v1",
      "title": "On Surprising Effectiveness of Masking Updates in Adaptive Optimizers",
      "authors": [
        "Taejong Joo",
        "Wenhan Xia",
        "Cheolmin Kim",
        "Ming Zhang",
        "Eugene Ie"
      ],
      "abstract": "Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\\% and 9\\% compared to Adam and Muon, respectively.",
      "arxiv_url": "http://arxiv.org/abs/2602.15322v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15322v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15318v1",
      "title": "Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs",
      "authors": [
        "Libo Zhang",
        "Zhaoning Zhang",
        "Wangyang Hong",
        "Peng Qiao",
        "Dongsheng Li"
      ],
      "abstract": "Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks.",
      "arxiv_url": "http://arxiv.org/abs/2602.15318v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15318v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "tags": [
        "Context Compression",
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15313v1",
      "title": "Mnemis: Dual-Route Retrieval on Hierarchical Graphs for Long-Term LLM Memory",
      "authors": [
        "Zihao Tang",
        "Xin Yu",
        "Ziyu Xiao",
        "Zengxuan Wen",
        "Zelin Li",
        "Jiaxi Zhou",
        "Hualei Wang",
        "Haohua Wang",
        "Haizhen Huang",
        "Weiwei Deng",
        "Feng Sun",
        "Qi Zhang"
      ],
      "abstract": "AI Memory, specifically how models organizes and retrieves historical messages, becomes increasingly valuable to Large Language Models (LLMs), yet existing methods (RAG and Graph-RAG) primarily retrieve memory through similarity-based mechanisms. While efficient, such System-1-style retrieval struggles with scenarios that require global reasoning or comprehensive coverage of all relevant information. In this work, We propose Mnemis, a novel memory framework that integrates System-1 similarity search with a complementary System-2 mechanism, termed Global Selection. Mnemis organizes memory into a base graph for similarity retrieval and a hierarchical graph that enables top-down, deliberate traversal over semantic hierarchies. By combining the complementary strength from both retrieval routes, Mnemis retrieves memory items that are both semantically and structurally relevant. Mnemis achieves state-of-the-art performance across all compared methods on long-term memory benchmarks, scoring 93.9 on LoCoMo and 91.6 on LongMemEval-S using GPT-4.1-mini.",
      "arxiv_url": "http://arxiv.org/abs/2602.15313v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15313v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15312v1",
      "title": "Extracting Consumer Insight from Text: A Large Language Model Approach to Emotion and Evaluation Measurement",
      "authors": [
        "Stephan Ludwig",
        "Peter J. Danaher",
        "Xiaohao Yang",
        "Yu-Ting Lin",
        "Ehsan Abedin",
        "Dhruv Grewal",
        "Lan Du"
      ],
      "abstract": "Accurately measuring consumer emotions and evaluations from unstructured text remains a core challenge for marketing research and practice. This study introduces the Linguistic eXtractor (LX), a fine-tuned, large language model trained on consumer-authored text that also has been labeled with consumers' self-reported ratings of 16 consumption-related emotions and four evaluation constructs: trust, commitment, recommendation, and sentiment. LX consistently outperforms leading models, including GPT-4 Turbo, RoBERTa, and DeepSeek, achieving 81% macro-F1 accuracy on open-ended survey responses and greater than 95% accuracy on third-party-annotated Amazon and Yelp reviews. An application of LX to online retail data, using seemingly unrelated regression, affirms that review-expressed emotions predict product ratings, which in turn predict purchase behavior. Most emotional effects are mediated by product ratings, though some emotions, such as discontent and peacefulness, influence purchase directly, indicating that emotional tone provides meaningful signals beyond star ratings. To support its use, a no-code, cost-free, LX web application is available, enabling scalable analyses of consumer-authored text. In establishing a new methodological foundation for consumer perception measurement, this research demonstrates new methods for leveraging large language models to advance marketing research and practice, thereby achieving validated detection of marketing constructs from consumer data.",
      "arxiv_url": "http://arxiv.org/abs/2602.15312v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15312v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "econ.EM"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15294v1",
      "title": "EAA: Automating materials characterization with vision language model agents",
      "authors": [
        "Ming Du",
        "Yanqi Luo",
        "Srutarshi Banerjee",
        "Michael Wojcik",
        "Jelena Popovic",
        "Mathew J. Cherukara"
      ],
      "abstract": "We present Experiment Automation Agents (EAA), a vision-language-model-driven agentic system designed to automate complex experimental microscopy workflows. EAA integrates multimodal reasoning, tool-augmented action, and optional long-term memory to support both autonomous procedures and interactive user-guided measurements. Built on a flexible task-manager architecture, the system enables workflows ranging from fully agent-driven automation to logic-defined routines that embed localized LLM queries. EAA further provides a modern tool ecosystem with two-way compatibility for Model Context Protocol (MCP), allowing instrument-control tools to be consumed or served across applications. We demonstrate EAA at an imaging beamline at the Advanced Photon Source, including automated zone plate focusing, natural language-described feature search, and interactive data acquisition. These results illustrate how vision-capable agents can enhance beamline efficiency, reduce operational burden, and lower the expertise barrier for users.",
      "arxiv_url": "http://arxiv.org/abs/2602.15294v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15294v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15278v1",
      "title": "Visual Persuasion: What Influences Decisions of Vision-Language Models?",
      "authors": [
        "Manuel Cherep",
        "Pranav M R",
        "Pattie Maes",
        "Nikhil Singh"
      ],
      "abstract": "The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference: choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization, adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities, safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.",
      "arxiv_url": "http://arxiv.org/abs/2602.15278v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15278v1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.15273v1",
      "title": "FrameRef: A Framing Dataset and Simulation Testbed for Modeling Bounded Rational Information Health",
      "authors": [
        "Victor De Lima",
        "Jiqun Liu",
        "Grace Hui Yang"
      ],
      "abstract": "Information ecosystems increasingly shape how people internalize exposure to adverse digital experiences, raising concerns about the long-term consequences for information health. In modern search and recommendation systems, ranking and personalization policies play a central role in shaping such exposure and its long-term effects on users. To study these effects in a controlled setting, we present FrameRef, a large-scale dataset of 1,073,740 systematically reframed claims across five framing dimensions: authoritative, consensus, emotional, prestige, and sensationalist, and propose a simulation-based framework for modeling sequential information exposure and reinforcement dynamics characteristic of ranking and recommendation systems. Within this framework, we construct framing-sensitive agent personas by fine-tuning language models with framing-conditioned loss attenuation, inducing targeted biases while preserving overall task competence. Using Monte Carlo trajectory sampling, we show that small, systematic shifts in acceptance and confidence can compound over time, producing substantial divergence in cumulative information health trajectories. Human evaluation further confirms that FrameRef's generated framings measurably affect human judgment. Together, our dataset and framework provide a foundation for systematic information health research through simulation, complementing and informing responsible human-centered research. We release FrameRef, code, documentation, human evaluation data, and persona adapter models at https://github.com/infosenselab/frameref.",
      "arxiv_url": "http://arxiv.org/abs/2602.15273v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15273v1",
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY",
        "cs.CL"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "2026-02-17",
      "source": "arxiv",
      "conference": null
    }
  ],
  "available_tags": [
    "Agentic AI",
    "Context Compression",
    "LLM",
    "Personalization",
    "RAG"
  ]
}