{
  "date": "2026-02-18",
  "paper_count": 52,
  "papers": [
    {
      "arxiv_id": "2602.16708v1",
      "title": "Policy Compiler for Secure Agentic Systems",
      "authors": [
        "Nils Palumbo",
        "Sarthak Choudhary",
        "Jihye Choi",
        "Prasad Chalasani",
        "Mihai Christodorescu",
        "Somesh Jha"
      ],
      "abstract": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.   Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.   PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
      "arxiv_url": "http://arxiv.org/abs/2602.16708v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16708v1",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16703v1",
      "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
      "authors": [
        "Shen Zhou Hong",
        "Alex Kleinman",
        "Alyssa Mathiowetz",
        "Adam Howes",
        "Julian Cohen",
        "Suveer Ganta",
        "Alex Letizia",
        "Dora Liao",
        "Deepika Pahari",
        "Xavier Roberts-Gaal",
        "Luca Righetti",
        "Joe Torres"
      ],
      "abstract": "Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a \"typical\" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.",
      "arxiv_url": "http://arxiv.org/abs/2602.16703v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16703v1",
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16699v1",
      "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
      "authors": [
        "Wenxuan Ding",
        "Nicholas Tomlin",
        "Greg Durrett"
      ],
      "abstract": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
      "arxiv_url": "http://arxiv.org/abs/2602.16699v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16699v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16698v1",
      "title": "Causality is Key for Interpretability Claims to Generalise",
      "authors": [
        "Shruti Joshi",
        "Aaron Mueller",
        "David Klindt",
        "Wieland Brendel",
        "Patrik Reizinger",
        "Dhanya Sridhar"
      ],
      "abstract": "Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.",
      "arxiv_url": "http://arxiv.org/abs/2602.16698v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16698v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16687v1",
      "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens",
      "authors": [
        "Potsawee Manakul",
        "Woody Haosheng Gan",
        "Martijn Bartelds",
        "Guangzhi Sun",
        "William Held",
        "Diyi Yang"
      ],
      "abstract": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture.",
      "arxiv_url": "http://arxiv.org/abs/2602.16687v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16687v1",
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16684v1",
      "title": "Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition",
      "authors": [
        "Bo Pan",
        "Peter Zhiping Zhang",
        "Hao-Wei Pang",
        "Alex Zhu",
        "Xiang Yu",
        "Liying Zhang",
        "Liang Zhao"
      ],
      "abstract": "Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with limited edit controllability or learn MMP-style edits from restricted settings and small models. We propose a variable-to-variable formulation of analog generation and train a foundation model on large-scale MMP transformations (MMPTs) to generate diverse variables conditioned on an input variable. To enable practical control, we develop prompting mechanisms that let the users specify preferred transformation patterns during generation. We further introduce MMPT-RAG, a retrieval-augmented framework that uses external reference analogs as contextual guidance to steer generation and generalize from project-specific series. Experiments on general chemical corpora and patent-specific datasets demonstrate improved diversity, novelty, and controllability, and show that our method recovers realistic analog structures in practical discovery scenarios.",
      "arxiv_url": "http://arxiv.org/abs/2602.16684v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16684v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "tags": [
        "RAG"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16671v1",
      "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
      "authors": [
        "Jaid Monwar Chowdhury",
        "Chi-An Fu",
        "Reyhaneh Jabbarvand"
      ],
      "abstract": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.",
      "arxiv_url": "http://arxiv.org/abs/2602.16671v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16671v1",
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16666v1",
      "title": "Towards a Science of AI Agent Reliability",
      "authors": [
        "Stephan Rabanser",
        "Sayash Kapoor",
        "Peter Kirgis",
        "Kangheng Liu",
        "Saiteja Utpala",
        "Arvind Narayanan"
      ],
      "abstract": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
      "arxiv_url": "http://arxiv.org/abs/2602.16666v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16666v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16662v1",
      "title": "Evaluating Collective Behaviour of Hundreds of LLM Agents",
      "authors": [
        "Richard Willis",
        "Jianing Zhao",
        "Yali Du",
        "Joel Z. Leibo"
      ],
      "abstract": "As autonomous agents powered by LLM are increasingly deployed in society, understanding their collective behaviour in social dilemmas becomes critical. We introduce an evaluation framework where LLMs generate strategies encoded as algorithms, enabling inspection prior to deployment and scaling to populations of hundreds of agents -- substantially larger than in previous work. We find that more recent models tend to produce worse societal outcomes compared to older models when agents prioritise individual gain over collective benefits. Using cultural evolution to model user selection of agents, our simulations reveal a significant risk of convergence to poor societal equilibria, particularly when the relative benefit of cooperation diminishes and population sizes increase. We release our code as an evaluation suite for developers to assess the emergent collective behaviour of their models.",
      "arxiv_url": "http://arxiv.org/abs/2602.16662v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16662v1",
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16660v1",
      "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
      "authors": [
        "Yuyan Bu",
        "Xiaohao Liu",
        "ZhaoXing Ren",
        "Yaodong Yang",
        "Juntao Dai"
      ],
      "abstract": "The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.",
      "arxiv_url": "http://arxiv.org/abs/2602.16660v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16660v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16650v1",
      "title": "Retrieval Augmented Generation of Literature-derived Polymer Knowledge: The Example of a Biodegradable Polymer Expert System",
      "authors": [
        "Sonakshi Gupta",
        "Akhlak Mahmood",
        "Wei Xiong",
        "Rampi Ramprasad"
      ],
      "abstract": "Polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning difficult. Existing tools typically extract narrow, study-specific facts in isolation, failing to preserve the cross-study context required to answer broader scientific questions. Retrieval-augmented generation (RAG) offers a promising way to overcome this limitation by combining large language models (LLMs) with external retrieval, but its effectiveness depends strongly on how domain knowledge is represented. In this work, we develop two retrieval pipelines: a dense semantic vector-based approach (VectorRAG) and a graph-based approach (GraphRAG). Using over 1,000 polyhydroxyalkanoate (PHA) papers, we construct context-preserving paragraph embeddings and a canonicalized structured knowledge graph supporting entity disambiguation and multi-hop reasoning. We evaluate these pipelines through standard retrieval metrics, comparisons with general state-of-the-art systems such as GPT and Gemini, and qualitative validation by a domain chemist. The results show that GraphRAG achieves higher precision and interpretability, while VectorRAG provides broader recall, highlighting complementary trade-offs. Expert validation further confirms that the tailored pipelines, particularly GraphRAG, produce well-grounded, citation-reliable responses with strong domain relevance. By grounding every statement in evidence, these systems enable researchers to navigate the literature, compare findings across studies, and uncover patterns that are difficult to extract manually. More broadly, this work establishes a practical framework for building materials science assistants using curated corpora and retrieval design, reducing reliance on proprietary models while enabling trustworthy literature analysis at scale.",
      "arxiv_url": "http://arxiv.org/abs/2602.16650v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16650v1",
      "primary_category": "cs.CE",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16640v1",
      "title": "Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval",
      "authors": [
        "Subrit Dikshit"
      ],
      "abstract": "The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a \"resource divide.\" State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered to democratize access to Indian legal intelligence. Built upon a custom configuration of the GPT-2 architecture (124 million parameters), Quecto-V1 was trained from scratch exclusively on a corpus of Indian statutes, including the Indian Penal Code (IPC), the Code of Criminal Procedure (CrPC), and the Constitution of India. Unlike generalist models, which prioritize broad world knowledge, our approach maximizes \"lexical density\" within the legal domain. Furthermore, we address the deployment bottleneck by applying post-training 8-bit quantization (GGUF format), compressing the model to a memory footprint of under 150 MB. Our empirical analysis demonstrates that Quecto-V1 achieves high fidelity in retrieving statutory definitions and penal provisions, outperforming general-purpose SLMs in domain-specific exact match tasks while running entirely offline on consumer-grade CPUs. We further present an ablation study showing that 8-bit quantization yields a 74% reduction in model size with less than 3.5% degradation in retrieval accuracy compared to full-precision baselines. These findings suggest that for specialized, high-stakes domains like law, domain-specific training coupled with aggressive quantization offers a viable, privacy-preserving alternative to monolithic cloud models.",
      "arxiv_url": "http://arxiv.org/abs/2602.16640v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16640v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16639v1",
      "title": "AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models",
      "authors": [
        "Adib Sakhawat",
        "Fardeen Sadab"
      ],
      "abstract": "Evaluating the social intelligence of Large Language Models (LLMs) increasingly requires moving beyond static text generation toward dynamic, adversarial interaction. We introduce the Adversarial Resource Extraction Game (AREG), a benchmark that operationalizes persuasion and resistance as a multi-turn, zero-sum negotiation over financial resources. Using a round-robin tournament across frontier models, AREG enables joint evaluation of offensive (persuasion) and defensive (resistance) capabilities within a single interactional framework. Our analysis provides evidence that these capabilities are weakly correlated ($ρ= 0.33$) and empirically dissociated: strong persuasive performance does not reliably predict strong resistance, and vice versa. Across all evaluated models, resistance scores exceed persuasion scores, indicating a systematic defensive advantage in adversarial dialogue settings. Further linguistic analysis suggests that interaction structure plays a central role in these outcomes. Incremental commitment-seeking strategies are associated with higher extraction success, while verification-seeking responses are more prevalent in successful defenses than explicit refusal. Together, these findings indicate that social influence in LLMs is not a monolithic capability and that evaluation frameworks focusing on persuasion alone may overlook asymmetric behavioral vulnerabilities.",
      "arxiv_url": "http://arxiv.org/abs/2602.16639v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16639v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16610v1",
      "title": "Who can we trust? LLM-as-a-jury for Comparative Assessment",
      "authors": [
        "Mengjie Qian",
        "Guangzhi Sun",
        "Mark J. F. Gales",
        "Kate M. Knill"
      ],
      "abstract": "Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.",
      "arxiv_url": "http://arxiv.org/abs/2602.16610v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16610v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16607v1",
      "title": "CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes",
      "authors": [
        "Miguel Marques",
        "Ana Luísa Fernandes",
        "Ana Filipa Pacheco",
        "Rute Rebouças",
        "Inês Cantante",
        "José Isidro",
        "Luís Filipe Cunha",
        "Alípio Jorge",
        "Nuno Guimarães",
        "Sérgio Nunes",
        "António Leal",
        "Purificação Silvano",
        "Ricardo Campos"
      ],
      "abstract": "Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts.",
      "arxiv_url": "http://arxiv.org/abs/2602.16607v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16607v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16603v1",
      "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving",
      "authors": [
        "Chia-chi Hsieh",
        "Zan Zong",
        "Xinyang Chen",
        "Jianjiang Li",
        "Jidong Zhai",
        "Lijie Wen"
      ],
      "abstract": "The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.   In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs.",
      "arxiv_url": "http://arxiv.org/abs/2602.16603v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16603v1",
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16585v1",
      "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
      "authors": [
        "Dimitri Yatsenko",
        "Thinh T. Nguyen"
      ],
      "abstract": "Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the relational workflow model: tables represent workflow steps, rows represent artifacts, foreign keys prescribe execution order. The schema specifies not only what data exists but how it is derived -- a single formal system where data structure, computational dependencies, and integrity constraints are all queryable, enforceable, and machine-readable. Four technical innovations extend this foundation: object-augmented schemas integrating relational metadata with scalable object storage, semantic matching using attribute lineage to prevent erroneous joins, an extensible type system for domain-specific formats, and distributed job coordination designed for composability with external orchestration. By unifying data structure, data, and computational transformations, DataJoint creates a substrate for SciOps where agents can participate in scientific workflows without risking data corruption.",
      "arxiv_url": "http://arxiv.org/abs/2602.16585v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16585v1",
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16578v1",
      "title": "Creating a digital poet",
      "authors": [
        "Vered Tohar",
        "Tsahi Hayat",
        "Amir Leshem"
      ],
      "abstract": "Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.",
      "arxiv_url": "http://arxiv.org/abs/2602.16578v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16578v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16571v1",
      "title": "Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset",
      "authors": [
        "Zhuqian Zhou",
        "Kirk Vanacore",
        "Bakhtawar Ahtisham",
        "Jinsook Lee",
        "Doug Pietrzak",
        "Daryl Hedley",
        "Jorge Dias",
        "Chris Shaw",
        "Ruth Schäfer",
        "René F. Kizilcec"
      ],
      "abstract": "Large-scale sharing of dialogue-based data is instrumental for advancing the science of teaching and learning, yet rigorous de-identification remains a major barrier. In mathematics tutoring transcripts, numeric expressions frequently resemble structured identifiers (e.g., dates or IDs), leading generic Personally Identifiable Information (PII) detection systems to over-redact core instructional content and reduce dataset utility. This work asks how PII can be detected in math tutoring transcripts while preserving their educational utility. To address this challenge, we investigate the \"numeric ambiguity\" problem and introduce MathEd-PII, the first benchmark dataset for PII detection in math tutoring dialogues, created through a human-in-the-loop LLM workflow that audits upstream redactions and generates privacy-preserving surrogates. The dataset contains 1,000 tutoring sessions (115,620 messages; 769,628 tokens) with validated PII annotations. Using a density-based segmentation method, we show that false PII redactions are disproportionately concentrated in math-dense regions, confirming numeric ambiguity as a key failure mode. We then compare four detection strategies: a Presidio baseline and LLM-based approaches with basic, math-aware, and segment-aware prompting. Math-aware prompting substantially improves performance over the baseline (F1: 0.821 vs. 0.379) while reducing numeric false positives, demonstrating that de-identification must incorporate domain context to preserve analytic utility. This work provides both a new benchmark and evidence that utility-preserving de-identification for tutoring data requires domain-aware modeling.",
      "arxiv_url": "http://arxiv.org/abs/2602.16571v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16571v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16554v1",
      "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
      "authors": [
        "Yuanjie Ren",
        "Jinzheng Li",
        "Yidi Qi"
      ],
      "abstract": "We introduce MerLean, a fully automated agentic framework for autoformalization in quantum computation. MerLean extracts mathematical statements from \\LaTeX{} source files, formalizes them into verified Lean~4 code built on Mathlib, and translates the result back into human-readable \\LaTeX{} for semantic review. We evaluate MerLean on three theoretical quantum computing papers producing 2,050 Lean declarations from 114 statements in total. MerLean achieves end-to-end formalization on all three papers, reducing the verification burden to only the newly introduced definitions and axioms. Our results demonstrate that agentic autoformalization can scale to frontier research, offering both a practical tool for machine-verified peer review and a scalable engine for mining high-quality synthetic data to train future reasoning models. Our approach can also be generalized to any other rigorous research in mathematics and theoretical physics.",
      "arxiv_url": "http://arxiv.org/abs/2602.16554v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16554v1",
      "primary_category": "cs.LO",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.ET",
        "quant-ph"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16520v1",
      "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
      "authors": [
        "Doron Shavit"
      ],
      "abstract": "Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than a one-shot classification: it normalizes and de-obfuscates suspicious inputs, chunks text to reduce context dilution and guarantee coverage, performs parallel chunk screening, and composes cross-chunk signals to recover split-payload attacks. On AutoDAN-style adversarial inputs, RLM-JB achieves high detection effectiveness across three LLM backends (ASR/Recall 92.5-98.0%) while maintaining very high precision (98.99-100%) and low false positive rates (0.0-2.0%), highlighting a practical sensitivity-specificity trade-off as the screening backend changes.",
      "arxiv_url": "http://arxiv.org/abs/2602.16520v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16520v1",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16516v1",
      "title": "Supercharging Agenda Setting Research: The ParlaCAP Dataset of 28 European Parliaments and a Scalable Multilingual LLM-Based Classification",
      "authors": [
        "Taja Kuzman Pungeršek",
        "Peter Rupnik",
        "Daniela Širinić",
        "Nikola Ljubešić"
      ],
      "abstract": "This paper introduces ParlaCAP, a large-scale dataset for analyzing parliamentary agenda setting across Europe, and proposes a cost-effective method for building domain-specific policy topic classifiers. Applying the Comparative Agendas Project (CAP) schema to the multilingual ParlaMint corpus of over 8 million speeches from 28 parliaments of European countries and autonomous regions, we follow a teacher-student framework in which a high-performing large language model (LLM) annotates in-domain training data and a multilingual encoder model is fine-tuned on these annotations for scalable data annotation. We show that this approach produces a classifier tailored to the target domain. Agreement between the LLM and human annotators is comparable to inter-annotator agreement among humans, and the resulting model outperforms existing CAP classifiers trained on manually-annotated but out-of-domain data. In addition to the CAP annotations, the ParlaCAP dataset offers rich speaker and party metadata, as well as sentiment predictions coming from the ParlaSent multilingual transformer model, enabling comparative research on political attention and representation across countries. We illustrate the analytical potential of the dataset with three use cases, examining the distribution of parliamentary attention across policy topics, sentiment patterns in parliamentary speech, and gender differences in policy attention.",
      "arxiv_url": "http://arxiv.org/abs/2602.16516v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16516v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16500v1",
      "title": "Optimizing Soft Prompt Tuning via Structural Evolution",
      "authors": [
        "Zhenzhen Huang",
        "Chaoning Zhang",
        "Haoyu Bian",
        "Songbo Zhang",
        "Chi-lok Andy Tai",
        "Jiaquan Zhang",
        "Caiyan Qin",
        "Jingjing Qu",
        "Yalan Ye",
        "Yang Yang",
        "Heng Tao Shen"
      ],
      "abstract": "Soft prompt tuning leverages continuous embeddings to capture task-specific information in large pre-trained language models (LLMs), achieving competitive performance in few-shot settings. However, soft prompts rely on high-dimensional, implicit representations and lack explicit semantics and traceable training behaviors, which limits their interpretability. To address this limitation, we propose a soft prompt tuning optimization method based on topological morphological evolution. Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution. Quantitative analysis shows that topologically stable and compact soft prompts achieve better downstream performance. Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss guides the model to learn structurally stable adaptations by quantifying inter-parameter connectivity and redundancy. Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives.",
      "arxiv_url": "http://arxiv.org/abs/2602.16500v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16500v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16490v1",
      "title": "From Growing to Looping: A Unified View of Iterative Computation in LLMs",
      "authors": [
        "Ferdinand Kapl",
        "Emmanouil Angelis",
        "Kaitlin Maile",
        "Johannes von Oswald",
        "Stefan Bauer"
      ],
      "abstract": "Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from a common form of iterative computation. Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\\times$, despite the model never being trained to loop. Both approaches also adapt better than the baseline when given more in-context examples or additional supervised fine-tuning data. Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning.",
      "arxiv_url": "http://arxiv.org/abs/2602.16490v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16490v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16488v1",
      "title": "Learning to Learn from Language Feedback with Social Meta-Learning",
      "authors": [
        "Jonathan Cook",
        "Diego Antognini",
        "Martin Klissarov",
        "Claudiu Musat",
        "Edward Grefenstette"
      ],
      "abstract": "Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback.",
      "arxiv_url": "http://arxiv.org/abs/2602.16488v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16488v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16485v1",
      "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
      "authors": [
        "Jeffrey T. H. Wong",
        "Zixi Zhang",
        "Junyi Liu",
        "Yiren Zhao"
      ],
      "abstract": "Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models with superior coordination capabilities, and (2) a self-assessment protocol where tool agents profile their own domain expertise to account for variations in post-training skills. During inference, the orchestrator dynamically activates the most suitable tool agents based on these proficiency profiles. Experiments on five reasoning and code generation benchmarks show that Team-of-Thoughts delivers consistently superior task performance. Notably, on AIME24 and LiveCodeBench, our approach achieves accuracies of 96.67% and 72.53%, respectively, substantially outperforming homogeneous role-play baselines, which score 80% and 65.93%.",
      "arxiv_url": "http://arxiv.org/abs/2602.16485v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16485v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16481v1",
      "title": "Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach",
      "authors": [
        "Zihao Li",
        "Fabrizio Russo"
      ],
      "abstract": "Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.",
      "arxiv_url": "http://arxiv.org/abs/2602.16481v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16481v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16467v1",
      "title": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models",
      "authors": [
        "Saurabh Bharti",
        "Gaurav Azad",
        "Abhinaw Jagtap",
        "Nachiket Tapas"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.",
      "arxiv_url": "http://arxiv.org/abs/2602.16467v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16467v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16444v1",
      "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation",
      "authors": [
        "Yixue Zhang",
        "Kun Wu",
        "Zhi Gao",
        "Zhen Zhao",
        "Pei Ren",
        "Zhiyuan Xu",
        "Fei Liao",
        "Xinhua Wang",
        "Shichao Fan",
        "Di Wu",
        "Qiuxuan Feng",
        "Meng Li",
        "Zhengping Che",
        "Chang Liu",
        "Jian Tang"
      ],
      "abstract": "The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io.",
      "arxiv_url": "http://arxiv.org/abs/2602.16444v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16444v1",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16438v1",
      "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
      "authors": [
        "Eva Paraschou",
        "Line Harder Clemmensen",
        "Sneha Das"
      ],
      "abstract": "Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p< 0.001$ across all models), sexual orientation, and disability status. We demonstrate that improving fairness along one attribute can inadvertently worsen disparities in others under uncertainty, highlighting the necessity of context-aware, multi-attribute fairness evaluation frameworks.",
      "arxiv_url": "http://arxiv.org/abs/2602.16438v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16438v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16429v1",
      "title": "TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers",
      "authors": [
        "Ido Levy",
        "Eilam Shapira",
        "Yinon Goldshtein",
        "Avi Yaeli",
        "Nir Mashkif",
        "Segev Shlomov"
      ],
      "abstract": "Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures.",
      "arxiv_url": "http://arxiv.org/abs/2602.16429v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16429v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16379v1",
      "title": "Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents",
      "authors": [
        "Mohammad H. A. Monfared",
        "Lucie Flek",
        "Akbar Karimi"
      ],
      "abstract": "We propose an agentic data augmentation method for Aspect-Based Sentiment Analysis (ABSA) that uses iterative generation and verification to produce high quality synthetic training examples. To isolate the effect of agentic structure, we also develop a closely matched prompting-based baseline using the same model and instructions. Both methods are evaluated across three ABSA subtasks (Aspect Term Extraction (ATE), Aspect Sentiment Classification (ATSC), and Aspect Sentiment Pair Extraction (ASPE)), four SemEval datasets, and two encoder-decoder models: T5-Base and Tk-Instruct. Our results show that the agentic augmentation outperforms raw prompting in label preservation of the augmented data, especially when the tasks require aspect term generation. In addition, when combined with real data, agentic augmentation provides higher gains, consistently outperforming prompting-based generation. These benefits are most pronounced for T5-Base, while the more heavily pretrained Tk-Instruct exhibits smaller improvements. As a result, augmented data helps T5-Base achieve comparable performance with its counterpart.",
      "arxiv_url": "http://arxiv.org/abs/2602.16379v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16379v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16346v1",
      "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
      "authors": [
        "Nivya Talokar",
        "Ayush K Tarun",
        "Murari Mandal",
        "Maksym Andriushchenko",
        "Antoine Bosselut"
      ],
      "abstract": "LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.",
      "arxiv_url": "http://arxiv.org/abs/2602.16346v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16346v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16313v1",
      "title": "MemoryArena: Benchmarking Agent Memory in Interdependent Multi-Session Agentic Tasks",
      "authors": [
        "Zexue He",
        "Yu Wang",
        "Churan Zhi",
        "Yuanzhe Hu",
        "Tzu-Ping Chen",
        "Lang Yin",
        "Ze Chen",
        "Tong Arthur Wu",
        "Siru Ouyang",
        "Zihan Wang",
        "Jiaxin Pei",
        "Julian McAuley",
        "Yejin Choi",
        "Alex Pentland"
      ],
      "abstract": "Existing evaluations of agents with memory typically assess memorization and action in isolation. One class of benchmarks evaluates memorization by testing recall of past conversations or text but fails to capture how memory is used to guide future decisions. Another class focuses on agents acting in single-session tasks without the need for long-term memory. However, in realistic settings, memorization and action are tightly coupled: agents acquire memory while interacting with the environment, and subsequently rely on that memory to solve future tasks. To capture this setting, we introduce MemoryArena, a unified evaluation gym for benchmarking agent memory in multi-session Memory-Agent-Environment loops. The benchmark consists of human-crafted agentic tasks with explicitly interdependent subtasks, where agents must learn from earlier actions and feedback by distilling experiences into memory, and subsequently use that memory to guide later actions to solve the overall task. MemoryArena supports evaluation across web navigation, preference-constrained planning, progressive information search, and sequential formal reasoning, and reveals that agents with near-saturated performance on existing long-context memory benchmarks like LoCoMo perform poorly in our agentic setting, exposing a gap in current evaluations for agents with memory.",
      "arxiv_url": "http://arxiv.org/abs/2602.16313v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16313v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "Agentic AI"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16298v1",
      "title": "MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models",
      "authors": [
        "Martin Hyben",
        "Sebastian Kula",
        "Jan Cegin",
        "Jakub Simko",
        "Ivan Srba",
        "Robert Moro"
      ],
      "abstract": "Large Language Models (LLMs) are beginning to reshape how media professionals verify information, yet automated support for detecting check-worthy claims a key step in the fact-checking process remains limited. We introduce the Multi-Check-Worthy (MultiCW) dataset, a balanced multilingual benchmark for check-worthy claim detection spanning 16 languages, 7 topical domains, and 2 writing styles. It consists of 123,722 samples, evenly distributed between noisy (informal) and structured (formal) texts, with balanced representation of check-worthy and non-check-worthy classes across all languages. To probe robustness, we also introduce an equally balanced out-of-distribution evaluation set of 27,761 samples in 4 additional languages. To provide baselines, we benchmark 3 common fine-tuned multilingual transformers against a diverse set of 15 commercial and open LLMs under zero-shot settings. Our findings show that fine-tuned models consistently outperform zero-shot LLMs on claim classification and show strong out-of-distribution generalization across languages, domains, and styles. MultiCW provides a rigorous multilingual resource for advancing automated fact-checking and enables systematic comparisons between fine-tuned models and cutting-edge LLMs on the check-worthy claim detection task.",
      "arxiv_url": "http://arxiv.org/abs/2602.16298v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16298v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16290v1",
      "title": "Aladdin-FTI @ AMIYA Three Wishes for Arabic NLP: Fidelity, Diglossia, and Multidialectal Generation",
      "authors": [
        "Jonathan Mutal",
        "Perla Al Almaoui",
        "Simon Hengchen",
        "Pierrette Bouillon"
      ],
      "abstract": "Arabic dialects have long been under-represented in Natural Language Processing (NLP) research due to their non-standardization and high variability, which pose challenges for computational modeling. Recent advances in the field, such as Large Language Models (LLMs), offer promising avenues to address this gap by enabling Arabic to be modeled as a pluricentric language rather than a monolithic system. This paper presents Aladdin-FTI, our submission to the AMIYA shared task. The proposed system is designed to both generate and translate dialectal Arabic (DA). Specifically, the model supports text generation in Moroccan, Egyptian, Palestinian, Syrian, and Saudi dialects, as well as bidirectional translation between these dialects, Modern Standard Arabic (MSA), and English. The code and trained model are publicly available.",
      "arxiv_url": "http://arxiv.org/abs/2602.16290v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16290v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16246v1",
      "title": "Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents",
      "authors": [
        "Yun-Shiuan Chuang",
        "Chaitanya Kulkarni",
        "Alec Chiu",
        "Avinash Thangali",
        "Zijie Pan",
        "Shivani Shekhar",
        "Yirou Ge",
        "Yixi Li",
        "Uma Kona",
        "Linsey Pang",
        "Prakhar Mehrotra"
      ],
      "abstract": "Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWorld) rely on fully deterministic backends, which are costly to build and iterate. We propose Proxy State-Based Evaluation, an LLM-driven simulation framework that preserves final state-based evaluation without a deterministic database. Specifically, a scenario specifies the user goal, user/system facts, expected final state, and expected agent behavior, and an LLM state tracker infers a structured proxy state from the full interaction trace. LLM judges then verify goal completion and detect tool/user hallucinations against scenario constraints. Empirically, our benchmark produces stable, model-differentiating rankings across families and inference-time reasoning efforts, and its on-/off-policy rollouts provide supervision that transfers to unseen scenarios. Careful scenario specification yields near-zero simulator hallucination rates as supported by ablation studies. The framework also supports sensitivity analyses over user personas. Human-LLM judge agreement exceeds 90%, indicating reliable automated evaluation. Overall, proxy state-based evaluation offers a practical, scalable alternative to deterministic agentic benchmarks for industrial LLM agents.",
      "arxiv_url": "http://arxiv.org/abs/2602.16246v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16246v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16241v1",
      "title": "Are LLMs Ready to Replace Bangla Annotators?",
      "authors": [
        "Md. Najib Hasan",
        "Touseef Hasan",
        "Souvika Sarkar"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences. We conduct a systematic benchmark of 17 LLMs using a unified evaluation framework. Our analysis uncovers annotator bias and substantial instability in model judgments. Surprisingly, increased model scale does not guarantee improved annotation quality--smaller, more task-aligned models frequently exhibit more consistent behavior than their larger counterparts. These results highlight important limitations of current LLMs for sensitive annotation tasks in low-resource languages and underscore the need for careful evaluation before deployment.",
      "arxiv_url": "http://arxiv.org/abs/2602.16241v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16241v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16201v1",
      "title": "Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications",
      "authors": [
        "Sanket Badhe",
        "Deep Shah",
        "Nehal Kathrotia"
      ],
      "abstract": "Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work across technical and sociotechnical perspectives.   We introduce a structured analytical framework that synthesizes prior work across four complementary axes: how long-Tail Knowledge is defined, the mechanisms by which it is lost or distorted during training and inference, the technical interventions proposed to mitigate these failures, and the implications of these failures for fairness, accountability, transparency, and user trust. We further examine how existing evaluation practices obscure tail behavior and complicate accountability for rare but consequential failures. The paper concludes by identifying open challenges related to privacy, sustainability, and governance that constrain long-Tail Knowledge representation. Taken together, this paper provides a unifying conceptual framework for understanding how long-Tail Knowledge is defined, lost, evaluated, and manifested in deployed language model systems.",
      "arxiv_url": "http://arxiv.org/abs/2602.16201v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16201v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16188v1",
      "title": "Deep TPC: Temporal-Prior Conditioning for Time Series Forecasting",
      "authors": [
        "Filippos Bellos",
        "NaveenJohn Premkumar",
        "Yannis Avrithis",
        "Nam H. Nguyen",
        "Jason J. Corso"
      ],
      "abstract": "LLM-for-time series (TS) methods typically treat time shallowly, injecting positional or prompt-based cues once at the input of a largely frozen decoder, which limits temporal reasoning as this information degrades through the layers. We introduce Temporal-Prior Conditioning (TPC), which elevates time to a first-class modality that conditions the model at multiple depths. TPC attaches a small set of learnable time series tokens to the patch stream; at selected layers these tokens cross-attend to temporal embeddings derived from compact, human-readable temporal descriptors encoded by the same frozen LLM, then feed temporal context back via self-attention. This disentangles time series signal and temporal information while maintaining a low parameter budget. We show that by training only the cross-attention modules and explicitly disentangling time series signal and temporal information, TPC consistently outperforms both full fine-tuning and shallow conditioning strategies, achieving state-of-the-art performance in long-term forecasting across diverse datasets. Code available at: https://github.com/fil-mp/Deep_tpc",
      "arxiv_url": "http://arxiv.org/abs/2602.16188v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16188v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16179v1",
      "title": "EnterpriseGym Corecraft: Training Generalizable Agents on High-Fidelity RL Environments",
      "authors": [
        "Sushant Mehta",
        "Logan Ritchie",
        "Suhaas Garre",
        "Nick Heiner",
        "Edwin Chen"
      ],
      "abstract": "We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce \\corecraft{}, the first environment in \\textsc{EnterpriseGym}, Surge AI's suite of agentic RL environments. \\corecraft{} is a fully operational enterprise simulation of a customer support organization, comprising over 2,500 entities across 14 entity types with 23 unique tools, designed to measure whether AI agents can perform the multi-step, domain-specific work that real jobs demand. Frontier models such as GPT-5.2 and Claude Opus 4.6 solve fewer than 30\\% of tasks when all expert-authored rubric criteria must be satisfied. Using this environment, we train GLM~4.6 with Group Relative Policy Optimization (GRPO) and adaptive clipping. After a single epoch of training, the model improves from 25.37\\% to 36.76\\% task pass rate on held-out evaluation tasks. More importantly, these gains transfer to out-of-distribution benchmarks: +4.5\\% on BFCL Parallel, +7.4\\% on $τ^2$-Bench Retail, and +6.8\\% on Toolathlon (Pass@1). We believe three environment properties are consistent with the observed transfer: task-centric world building that optimizes for diverse, challenging tasks; expert-authored rubrics enabling reliable reward computation; and enterprise workflows that reflect realistic professional patterns. Our results suggest that environment quality, diversity, and realism are key factors enabling generalizable agent capabilities.",
      "arxiv_url": "http://arxiv.org/abs/2602.16179v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16179v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16173v1",
      "title": "Learning Personalized Agents from Human Feedback",
      "authors": [
        "Kaiqu Liang",
        "Julia Kruk",
        "Shengyi Qian",
        "Xianjun Yang",
        "Shengjie Bi",
        "Yuanshun Yao",
        "Shaoliang Nie",
        "Mingyang Zhang",
        "Lijuan Liu",
        "Jaime Fernández Fisac",
        "Shuyan Zhou",
        "Saghar Hosseini"
      ],
      "abstract": "Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.",
      "arxiv_url": "http://arxiv.org/abs/2602.16173v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16173v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "tags": [
        "Agentic AI",
        "Personalization"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16165v1",
      "title": "HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents",
      "authors": [
        "Jiangweizhi Peng",
        "Yuanxin Liu",
        "Ruida Zhou",
        "Charles Fleming",
        "Zhaoran Wang",
        "Alfredo Garcia",
        "Mingyi Hong"
      ],
      "abstract": "Training LLMs as interactive agents for multi-turn decision-making remains challenging, particularly in long-horizon tasks with sparse and delayed rewards, where agents must execute extended sequences of actions before receiving meaningful feedback. Most existing reinforcement learning (RL) approaches model LLM agents as flat policies operating at a single time scale, selecting one action at each turn. In sparse-reward settings, such flat policies must propagate credit across the entire trajectory without explicit temporal abstraction, which often leads to unstable optimization and inefficient credit assignment.   We propose HiPER, a novel Hierarchical Plan-Execute RL framework that explicitly separates high-level planning from low-level execution. HiPER factorizes the policy into a high-level planner that proposes subgoals and a low-level executor that carries them out over multiple action steps. To align optimization with this structure, we introduce a key technique called hierarchical advantage estimation (HAE), which carefully assigns credit at both the planning and execution levels. By aggregating returns over the execution of each subgoal and coordinating updates across the two levels, HAE provides an unbiased gradient estimator and provably reduces variance compared to flat generalized advantage estimation.   Empirically, HiPER achieves state-of-the-art performance on challenging interactive benchmarks, reaching 97.4\\% success on ALFWorld and 83.3\\% on WebShop with Qwen2.5-7B-Instruct (+6.6\\% and +8.3\\% over the best prior method), with especially large gains on long-horizon tasks requiring multiple dependent subtasks. These results highlight the importance of explicit hierarchical decomposition for scalable RL training of multi-turn LLM agents.",
      "arxiv_url": "http://arxiv.org/abs/2602.16165v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16165v1",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16162v1",
      "title": "LLMs Exhibit Significantly Lower Uncertainty in Creative Writing Than Professional Writers",
      "authors": [
        "Peiqi Sui"
      ],
      "abstract": "We argue that uncertainty is a key and understudied limitation of LLMs' performance in creative writing, which is often characterized as trite and cliché-ridden. Literary theory identifies uncertainty as a necessary condition for creative expression, while current alignment strategies steer models away from uncertain outputs to ensure factuality and reduce hallucination. We formalize this tension by quantifying the \"uncertainty gap\" between human-authored stories and model-generated continuations. Through a controlled information-theoretic analysis of 28 LLMs on high-quality storytelling datasets, we demonstrate that human writing consistently exhibits significantly higher uncertainty than model outputs. We find that instruction-tuned and reasoning models exacerbate this trend compared to their base counterparts; furthermore, the gap is more pronounced in creative writing than in functional domains, and strongly correlates to writing quality. Achieving human-level creativity requires new uncertainty-aware alignment paradigms that can distinguish between destructive hallucinations and the constructive ambiguity required for literary richness.",
      "arxiv_url": "http://arxiv.org/abs/2602.16162v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16162v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16154v1",
      "title": "Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution",
      "authors": [
        "Nithin Sivakumaran",
        "Shoubin Yu",
        "Hyunji Lee",
        "Yue Zhang",
        "Ali Payani",
        "Mohit Bansal",
        "Elias Stengel-Eskin"
      ],
      "abstract": "Chain-of-thought (CoT) reasoning sometimes fails to faithfully reflect the true computation of a large language model (LLM), hampering its utility in explaining how LLMs arrive at their answers. Moreover, optimizing for faithfulness and interpretability in reasoning often degrades task performance. To address this tradeoff and improve CoT faithfulness, we propose Reasoning Execution by Multiple Listeners (REMUL), a multi-party reinforcement learning approach. REMUL builds on the hypothesis that reasoning traces which other parties can follow will be more faithful. A speaker model generates a reasoning trace, which is truncated and passed to a pool of listener models who \"execute\" the trace, continuing the trace to an answer. Speakers are rewarded for producing reasoning that is clear to listeners, with additional correctness regularization via masked supervised finetuning to counter the tradeoff between faithfulness and performance. On multiple reasoning benchmarks (BIG-Bench Extra Hard, MuSR, ZebraLogicBench, and FOLIO), REMUL consistently and substantially improves three measures of faithfulness -- hint attribution, early answering area over the curve (AOC), and mistake injection AOC -- while also improving accuracy. Our analysis finds that these gains are robust across training domains, translate to legibility gains, and are associated with shorter and more direct CoTs.",
      "arxiv_url": "http://arxiv.org/abs/2602.16154v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16154v1",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16140v1",
      "title": "Human-AI Collaboration in Large Language Model-Integrated Building Energy Management Systems: The Role of User Domain Knowledge and AI Literacy",
      "authors": [
        "Wooyoung Jung",
        "Kahyun Jeon",
        "Prosper Babon-Ayeng"
      ],
      "abstract": "This study aimed to comprehend how user domain knowledge and artificial intelligence (AI) literacy impact the effective use of human-AI interactive building energy management system (BEMS). While prior studies have investigated the potential of integrating large language models (LLMs) into BEMS or building energy modeling, very few studies have examined how user interact with such systems. We conducted a systematic role-playing experiment, where 85 human subjects interacted with an advanced generative pre-trained transformer (OpenAI GPT-4o). Participants were tasked with identifying the top five behavioral changes that could reduce home energy use with the GPT model that functioned as an LLM-integrated BEMS. Then, the collected prompt-response data and participant conclusions were analyzed using an analytical framework that hierarchically assessed and scored human-AI interactions and their home energy analysis approaches. Also, participants were classified into four groups based on their self-evaluated domain knowledge of building energy use and AI literacy, and Kruskal-Wallis H tests with post-hoc pairwise comparisons were conducted across 20 quantifiable metrics. Key takeaways include: most participants employed concise prompts (median: 16.2 words) and relied heavily on GPT's analytical capabilities; and notably, only 1 of 20 metrics, appliance identification rate, showed statistically significant group differences (p=0.037), driven by AI literacy rather than domain knowledge, suggesting an equalizing effect of LLMs across expertise levels. This study provides foundational insights into human-AI collaboration dynamics and promising development directions in the context of LLM-integrated BEMS and contributes to realizing human-centric LLM-integrated energy systems.",
      "arxiv_url": "http://arxiv.org/abs/2602.16140v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16140v1",
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16136v1",
      "title": "Retrieval Collapses When AI Pollutes the Web",
      "authors": [
        "Hongyeon Yu",
        "Dongchan Kim",
        "Young-Bum Kim"
      ],
      "abstract": "The rapid proliferation of AI-generated content on the Web presents a structural risk to information retrieval, as search engines and Retrieval-Augmented Generation (RAG) systems increasingly consume evidence produced by the Large Language Models (LLMs). We characterize this ecosystem-level failure mode as Retrieval Collapse, a two-stage process where (1) AI-generated content dominates search results, eroding source diversity, and (2) low-quality or adversarial content infiltrates the retrieval pipeline. We analyzed this dynamic through controlled experiments involving both high-quality SEO-style content and adversarially crafted content. In the SEO scenario, a 67\\% pool contamination led to over 80\\% exposure contamination, creating a homogenized yet deceptively healthy state where answer accuracy remains stable despite the reliance on synthetic sources. Conversely, under adversarial contamination, baselines like BM25 exposed $\\sim$19\\% of harmful content, whereas LLM-based rankers demonstrated stronger suppression capabilities. These findings highlight the risk of retrieval pipelines quietly shifting toward synthetic evidence and the need for retrieval-aware strategies to prevent a self-reinforcing cycle of quality decline in Web-grounded systems.",
      "arxiv_url": "http://arxiv.org/abs/2602.16136v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16136v1",
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "tags": [
        "LLM",
        "RAG"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16131v1",
      "title": "Empirical Cumulative Distribution Function Clustering for LLM-based Agent System Analysis",
      "authors": [
        "Chihiro Watanabe",
        "Jingyu Sun"
      ],
      "abstract": "Large language models (LLMs) are increasingly used as agents to solve complex tasks such as question answering (QA), scientific debate, and software development. A standard evaluation procedure aggregates multiple responses from LLM agents into a single final answer, often via majority voting, and compares it against reference answers. However, this process can obscure the quality and distributional characteristics of the original responses. In this paper, we propose a novel evaluation framework based on the empirical cumulative distribution function (ECDF) of cosine similarities between generated responses and reference answers. This enables a more nuanced assessment of response quality beyond exact match metrics. To analyze the response distributions across different agent configurations, we further introduce a clustering method for ECDFs using their distances and the $k$-medoids algorithm. Our experiments on a QA dataset demonstrate that ECDFs can distinguish between agent settings with similar final accuracies but different quality distributions. The clustering analysis also reveals interpretable group structures in the responses, offering insights into the impact of temperature, persona, and question topics.",
      "arxiv_url": "http://arxiv.org/abs/2602.16131v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16131v1",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16124v1",
      "title": "Rethinking ANN-based Retrieval: Multifaceted Learnable Index for Large-scale Recommendation System",
      "authors": [
        "Jiang Zhang",
        "Yubo Wang",
        "Wei Chang",
        "Lu Han",
        "Xingying Cheng",
        "Feng Zhang",
        "Min Li",
        "Songhao Jiang",
        "Wei Zheng",
        "Harry Tran",
        "Zhen Wang",
        "Lei Chen",
        "Yueming Wang",
        "Benyu Zhang",
        "Xiangjun Fan",
        "Bi Xue",
        "Qifan Wang"
      ],
      "abstract": "Approximate nearest neighbor (ANN) search is widely used in the retrieval stage of large-scale recommendation systems. In this stage, candidate items are indexed using their learned embedding vectors, and ANN search is executed for each user (or item) query to retrieve a set of relevant items. However, ANN-based retrieval has two key limitations. First, item embeddings and their indices are typically learned in separate stages: indexing is often performed offline after embeddings are trained, which can yield suboptimal retrieval quality-especially for newly created items. Second, although ANN offers sublinear query time, it must still be run for every request, incurring substantial computation cost at industry scale. In this paper, we propose MultiFaceted Learnable Index (MFLI), a scalable, real-time retrieval paradigm that learns multifaceted item embeddings and indices within a unified framework and eliminates ANN search at serving time. Specifically, we construct a multifaceted hierarchical codebook via residual quantization of item embeddings and co-train the codebook with the embeddings. We further introduce an efficient multifaceted indexing structure and mechanisms that support real-time updates. At serving time, the learned hierarchical indices are used directly to identify relevant items, avoiding ANN search altogether. Extensive experiments on real-world data with billions of users show that MFLI improves recall on engagement tasks by up to 11.8\\%, cold-content delivery by up to 57.29\\%, and semantic relevance by 13.5\\% compared with prior state-of-the-art methods. We also deploy MFLI in the system and report online experimental results demonstrating improved engagement, less popularity bias, and higher serving efficiency.",
      "arxiv_url": "http://arxiv.org/abs/2602.16124v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16124v1",
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "tags": [
        "Personalization"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16113v1",
      "title": "Evolutionary Context Search for Automated Skill Acquisition",
      "authors": [
        "Qi Sun",
        "Stefan Nielsen",
        "Rio Yokota",
        "Yujin Tang"
      ],
      "abstract": "Large Language Models cannot reliably acquire new knowledge post-deployment -- even when relevant text resources exist, models fail to transform them into actionable knowledge without retraining. Retrieval-Augmented Generation attempts to bridge this gap by surfacing relevant documents at inference time, yet similarity-based retrieval often fails to identify context that actually improves task performance. We introduce Evolutionary Context Search (ECS), an evolutionary method that searches context combinations using accuracy on a small development set, requiring only inference calls without weight updates. ECS moves beyond semantic similarity to discover non-obvious context pairings that significantly boost performance. Our empirical results show that ECS improves BackendBench by 27\\% and $τ$-bench airline by 7\\%. The evolved contexts are model-agnostic, as those evolved with Gemini-3-Flash transfer effectively to Claude Sonnet and DeepSeek. This suggests that ECS opens a path toward automated context discovery for skill acquisition -- an efficient alternative to manual prompt engineering or costly fine-tuning.",
      "arxiv_url": "http://arxiv.org/abs/2602.16113v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16113v1",
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "tags": [
        "RAG"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16111v1",
      "title": "Surrogate-Based Prevalence Measurement for Large-Scale A/B Testing",
      "authors": [
        "Zehao Xu",
        "Tony Paek",
        "Kevin O'Sullivan",
        "Attila Dobi"
      ],
      "abstract": "Online media platforms often need to measure how frequently users are exposed to specific content attributes in order to evaluate trade-offs in A/B experiments. A direct approach is to sample content, label it using a high-quality rubric (e.g., an expert-reviewed LLM prompt), and estimate impression-weighted prevalence. However, repeatedly running such labeling for every experiment arm and segment is too costly and slow to serve as a default measurement at scale.   We present a scalable \\emph{surrogate-based prevalence measurement} framework that decouples expensive labeling from per-experiment evaluation. The framework calibrates a surrogate signal to reference labels offline and then uses only impression logs to estimate prevalence for arbitrary experiment arms and segments. We instantiate this framework using \\emph{score bucketing} as the surrogate: we discretize a model score into buckets, estimate bucket-level prevalences from an offline labeled sample, and combine these calibrated bucket level prevalences with the bucket distribution of impressions in each arm to obtain fast, log-based estimates.   Across multiple large-scale A/B tests, we validate that the surrogate estimates closely match the reference estimates for both arm-level prevalence and treatment--control deltas. This enables scalable, low-latency prevalence measurement in experimentation without requiring per-experiment labeling jobs.",
      "arxiv_url": "http://arxiv.org/abs/2602.16111v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16111v1",
      "primary_category": "stat.AP",
      "categories": [
        "stat.AP",
        "cs.AI"
      ],
      "tags": [
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    },
    {
      "arxiv_id": "2602.16105v1",
      "title": "GPSBench: Do Large Language Models Understand GPS Coordinates?",
      "authors": [
        "Thinh Hung Truong",
        "Jey Han Lau",
        "Jianzhong Qi"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench",
      "arxiv_url": "http://arxiv.org/abs/2602.16105v1",
      "pdf_url": "https://arxiv.org/pdf/2602.16105v1",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "tags": [
        "Agentic AI",
        "LLM"
      ],
      "published_date": "2026-02-18",
      "source": "arxiv",
      "conference": null
    }
  ],
  "available_tags": [
    "Agentic AI",
    "LLM",
    "Personalization",
    "RAG"
  ]
}